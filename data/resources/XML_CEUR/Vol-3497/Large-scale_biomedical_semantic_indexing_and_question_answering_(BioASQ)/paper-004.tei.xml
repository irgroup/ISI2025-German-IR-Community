<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,403.12,15.42;1,88.78,106.66,339.92,15.42">BIT.UA at BioASQ 11B: Two-Stage IR with Synthetic Training and Zero-Shot Answer Generation</title>
				<funder ref="#_NM2jMWA">
					<orgName type="full">EC</orgName>
				</funder>
				<funder ref="#_9GUF7Xw">
					<orgName type="full">FCT</orgName>
				</funder>
				<funder ref="#_7ZXR3BW">
					<orgName type="full">Foundation for Science and Technology (FCT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.93,134.97,71.84,11.96"><forename type="first">Tiago</forename><surname>Almeida</surname></persName>
							<email>tiagomeloalmeida@ua.pt</email>
							<affiliation key="aff0">
								<orgName type="department">IEETA/DETI</orgName>
								<orgName type="institution" key="instit1">LASI</orgName>
								<orgName type="institution" key="instit2">University of Aveiro</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,173.42,134.97,100.78,11.96"><forename type="first">Richard</forename><forename type="middle">A A</forename><surname>Jonker</surname></persName>
							<email>richard.jonker@ua.pt</email>
							<affiliation key="aff0">
								<orgName type="department">IEETA/DETI</orgName>
								<orgName type="institution" key="instit1">LASI</orgName>
								<orgName type="institution" key="instit2">University of Aveiro</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,286.84,134.97,72.46,11.96"><forename type="first">Roshan</forename><surname>Poudel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IEETA/DETI</orgName>
								<orgName type="institution" key="instit1">LASI</orgName>
								<orgName type="institution" key="instit2">University of Aveiro</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.95,134.97,67.88,11.96"><forename type="first">Jorge</forename><forename type="middle">M</forename><surname>Silva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IEETA/DETI</orgName>
								<orgName type="institution" key="instit1">LASI</orgName>
								<orgName type="institution" key="instit2">University of Aveiro</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,63.78,11.96"><forename type="first">S√©rgio</forename><surname>Matos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IEETA/DETI</orgName>
								<orgName type="institution" key="instit1">LASI</orgName>
								<orgName type="institution" key="instit2">University of Aveiro</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,403.12,15.42;1,88.78,106.66,339.92,15.42">BIT.UA at BioASQ 11B: Two-Stage IR with Synthetic Training and Zero-Shot Answer Generation</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">4E29D677B7576FA3934F5395C8FDE307</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Retrieval</term>
					<term>Dense Retrieval</term>
					<term>Language model</term>
					<term>Answer Generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the efforts of the Biomedical Informatics and Technologies (BIT) group at the University of Aveiro in the eleventh edition of the BioASQ challenge. This paper presents our efforts in the eleventh edition of the BioASQ challenge. We addressed Task B in its two phases: document retrieval (phase A) and question answering (phase B). In phase A, we utilized a sparse retrieval method for initial document retrieval, implemented using Anserini, followed by a re-ranking step using transformer models, including monoT5 and PubMedBERT. Phase B featured the application of large language models (LLMs) to generate answers to questions based on a relevant article, with models such as Alpaca-LoRA, OA-Pythia, and OA-LLaMA. We also explored a variety of prompts and question types, as well as different generation strategies to optimize our system's performance. Our systems, in phase A, achieved competitive results scoring at the top and close to the top for all the batches, and achieving the best results in terms of F1 for all the batches. Regarding the phase B, our systems underperformed according to the automatic measures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The realm of biomedical literature has been experiencing an exponential increase, predominantly driven by the rise in open-access and peer-reviewed publications. This rapid expansion results in an information overload, posing a significant challenge to researchers, physicians, and other healthcare practitioners <ref type="bibr" coords="1,200.56,477.87,11.58,10.91" target="#b0">[1]</ref>. As delineated by Klerings et al. <ref type="bibr" coords="1,365.05,477.87,11.47,10.91" target="#b0">[1]</ref>, the primary concern stems not from the abundance of information but the scarcity of sophisticated information retrieval systems proficient in managing this growing body of literature. To mitigate this, the BioASQ challenge is a yearly competition that stimulates the creation of intelligent retrieval systems. In its eleventh year, the BioASQ challenge <ref type="bibr" coords="1,263.77,532.07,11.23,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,277.71,532.07,8.88,10.91" target="#b2">3]</ref> comprises several tasks targeting unique facets of information retrieval and text mining within the biomedical domain.</p><p>Task B and the Synergy task emphasises information retrieval and question-answering. Task B bifurcates into phases A and B. Phase A involves identifying relevant documents or snippets that answer a biomedical question, while phase B addresses the extraction and generation of responses. These tasks collectively aim at advancing systems that provide evidence or answers to open-ended biomedical queries. In contrast, the Synergy task seeks to resolve open-ended questions about COVID-19 by leveraging IR and QA systems.</p><p>This paper describes our participation in Task B phase A and ideal answer in phase B of the BioASQ challenge. During phase A, we utilized the traditional BM25 <ref type="bibr" coords="2,405.62,353.21,12.99,10.91" target="#b3">[4]</ref> for base document retrieval, followed by document re-ranking executed via a variety of transformer models, including monoT5 <ref type="bibr" coords="2,175.79,380.31,12.99,10.91" target="#b4">[5]</ref> and PubMedBERT <ref type="bibr" coords="2,278.31,380.31,11.58,10.91" target="#b5">[6]</ref>. These models were fine-tuned on prior years' data, and synthetic data generation was employed to mitigate the constraints of a small dataset size. During phase B, we adopted a naive unsupervised approach where language models were prompted to generate answers to a question provided with a article as context. The approach involved exploring various models and prompts along with differing context selections. Figure <ref type="figure" coords="2,501.27,434.50,4.97,10.91" target="#fig_0">1</ref> shows an illustration of an end-to-end pipeline for information retrieval and answering system.</p><p>Following this introduction, Section 2 explains the related work. Section 3 is the methodological section, where we explore the used datasets and corpora and thoroughly illustrates the employed methodologies. Section 4 shows our results and section 5 discusses them. The paper concludes in Section 6, summarising the key findings of our participation, with a brief discussion of future work in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The BioASQ challenge has consistently catalyzed significant advancements in biomedical information retrieval and question-answering. Task B, in particular, encapsulates the essence of these complex processes, focusing on two fundamental fields: Information Retrieval (IR) and Question Answering (QA).</p><p>Fundamentally, IR (phase A) aims to identify and retrieve relevant documents or snippets that align with a posed biomedical question, thereby addressing the issue of locating pertinent information within the vast corpus of biomedical literature <ref type="bibr" coords="2,362.94,655.72,11.58,10.91" target="#b1">[2]</ref>. QA (phase B), on the other hand, is concerned with extracting and generating comprehensive answers from the retrieved information. This intricate process requires understanding the question at hand and determining the most suitable answer by leveraging the context provided by the retrieved documents.</p><p>In the latest competition, the state-of-the-art performances were achieved by systems that utilized a two-step process: an initial sparse retrieval system followed by a Transformer-based re-ranking model <ref type="bibr" coords="3,171.03,141.16,11.57,10.91" target="#b6">[7]</ref>. This approach was not unique to a single submission but was rather a common thread among various entries. Our previous work Almeida et al. <ref type="bibr" coords="3,418.83,154.71,12.84,10.91" target="#b7">[8]</ref> also employed a similar pipeline, that used BM25 as first-stage, and in the second stage, employing powerful models such as PubMedBERT <ref type="bibr" coords="3,231.03,181.81,12.99,10.91" target="#b5">[6]</ref> and UPWM <ref type="bibr" coords="3,306.04,181.81,11.58,10.91" target="#b8">[9]</ref>. These models have shown remarkable proficiency in interpreting intricate biomedical queries and matching it to a relevant article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Information Retrieval</head><p>Information Retrieval (IR) involves identifying relevant documents that match a specific query. IR can be broadly categorized into sparse retrieval and dense retrieval. Sparse retrieval, usually associated with more traditional approaches, involves converting text into an inverted index to enable fast searching. An inverted index stores a mapping of terms to documents. Sparse retrieval has the advantage that it is fast and explainable. The simpler approach of sparse retrieval includes Bag-of-Words and term frequency-inverse document frequency (tf-idf). There are also sparse retrieval techniques that are enhanced by transformer-based models such as DeepCT <ref type="bibr" coords="3,127.91,339.81,17.76,10.91" target="#b9">[10]</ref> and HDCT <ref type="bibr" coords="3,198.30,339.81,17.76,10.91" target="#b10">[11]</ref> which produces contextualized term weights that can be stored in traditional inverted indexes. Nevertheless, one of the most relevant and well-known algorithms used in sparse retrieval is BM25 <ref type="bibr" coords="3,233.45,366.90,11.43,10.91" target="#b3">[4]</ref>.</p><formula xml:id="formula_0" coords="3,129.48,388.82,211.60,38.27">BM25 = ‚àëÔ∏Å ‚éõ ‚éù tf(ùë°, ùê∑) ‚Ä¢ (ùëò 1 + 1) tf(ùë°, ùê∑) + ùëò 1 ‚Ä¢ (1 -ùëè + ùëè ‚Ä¢ |ùê∑| avgdl )</formula><p>‚Ä¢ ln</p><formula xml:id="formula_1" coords="3,361.06,388.82,104.73,32.57">(Ô∏Ç ùëÅ -df(ùë°) + 0.5 df(ùë°) + 0.5 )Ô∏Ç ‚éû ‚é† .</formula><p>Where tf(ùë°, ùê∑) represents the term frequency of term (ùë°) in the document (ùê∑), |ùê∑| represents the length of the documents, avgdl is the average length of a document in the collection, ùëÅ is the number of documents in the collection and tf(ùë°) is the number of documents containing term ùë°. ùëò 1 and ùëè are hyperparameters that can be tuned.</p><p>On the other hand, a more recent approach called dense retrieval has emerged, utilizing transformer models to convert both documents and queries into the same dimensional space <ref type="bibr" coords="3,487.56,506.99,16.08,10.91" target="#b11">[12]</ref>. In this approach, the query is transformed into a vector representation by the dense retrieval model. The search process involves comparing the similarity of the query vector against all the document vectors that have been previously encoded. Prominent approaches in this domain include DPR <ref type="bibr" coords="3,149.43,561.19,18.06,10.91" target="#b12">[13]</ref> and ANCE <ref type="bibr" coords="3,222.93,561.19,16.41,10.91" target="#b13">[14]</ref>, which employ transformer-based models to learn a joint dimensional space for projecting queries and documents in a meaningful way. This enables queries to be closer in dimensional space to their relevant documents. To facilitate efficient execution of this type of search, libraries like Facebook's FAISS <ref type="bibr" coords="3,383.20,601.84,18.07,10.91" target="#b14">[15]</ref> offer a comprehensive framework designed specifically for this purpose.</p><p>Both the dense retrieval and sparse retrieval techniques can be broadly classified as representation-based approaches. In this approach, the document and query are encoded separately, and the search is performed based on either similarity measures (dense retrieval) or cumulative scores (sparse retrieval). In contrast, interaction-based approaches jointly score the query and document, allowing for the extraction of more intricate matching patterns and potentially improving retrieval results. However, due to the need to score the query against every document in the collection, interaction-based approaches are not practical for searching the entire document corpus. Therefore, representation-based approaches are commonly adopted as first-stage retrieval techniques to reduce the search space. Subsequently, more powerful interaction-based techniques can be employed to further refine the ranking order, a process known as re-ranking in the literature. These models are typically trained using pointwise and pairwise techniques <ref type="bibr" coords="4,180.09,181.81,16.16,10.91" target="#b15">[16]</ref>. Pointwise learning involves assigning a score to each document, and the ranking is then performed by sorting these scores. On the other hand, pairwise learning involves comparing pairs of documents and enforcing a margin between positive and negative document pairs, leading to a more discriminative learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Question Answering</head><p>Question Answering(QA) aims to provide accurate and relevant answers to various questions. QA tasks can be generally divided into two main categories:</p><p>‚Ä¢ Extractive QA involves identifying and extracting an answer from the given context.</p><p>‚Ä¢ Generative QA requires the model to generate an answer freely, sometimes requiring a context.</p><p>Generative QA can further be divided into open and closed generative QA. In open generative QA, the text is generated using a context provided. This is not to be confused with open-domain QA. Closed generative QA has no context, and the model entirely generates the answer.</p><p>More recent generative QA approaches leverage large language models (LLMs) for zero-shot answer generation. In this setup, the model is provided with a query containing the context and asked to generate an answer. This approach is relatively new in the literature. GPT-3 <ref type="bibr" coords="4,470.36,432.94,17.94,10.91" target="#b16">[17]</ref> is a powerful autoregressive language model that uses deep learning to produce human-like text. It has 175 billion parameters and has been applied successfully in zero-shot tasks that require a deep understanding of context, making it a suitable choice for generative QA tasks.</p><p>Other recent LLMs have surfaced, such as LLaMA <ref type="bibr" coords="4,323.02,487.14,16.18,10.91" target="#b17">[18]</ref>, Alpaca <ref type="bibr" coords="4,379.19,487.14,17.84,10.91" target="#b18">[19]</ref> and Pythia <ref type="bibr" coords="4,450.01,487.14,16.18,10.91" target="#b19">[20]</ref>. LLaMA is a foundation LLM that is based on various transformer-based architectures, namely GPT-3 <ref type="bibr" coords="4,89.29,514.24,16.38,10.91" target="#b16">[17]</ref>, PaLM <ref type="bibr" coords="4,141.43,514.24,16.38,10.91" target="#b20">[21]</ref>, and GPTNeo <ref type="bibr" coords="4,225.33,514.24,16.38,10.91" target="#b21">[22]</ref>. Alpaca is a LLM based on LLaMA that was fine-tuned on the text generated by OpenAi's GPT-3.5. Using this technique of knowledge distillation, LLMs can be made much smaller without sacrificing too much performance. Alpaca-LoRA<ref type="foot" coords="4,461.92,539.58,3.71,7.97" target="#foot_0">1</ref> employs an approach known as Low-Rank Adaptation <ref type="bibr" coords="4,291.86,554.89,16.16,10.91" target="#b22">[23]</ref>, which keeps the pre-trained model weights constant and introduces trainable rank decomposition matrices at each layer of the Transformer architecture. This significantly reduces the number of trainable parameters for downstream tasks. Pythia is a library for Transformers, providing various pre-trained models, which are also GPT based. OpenAssistant <ref type="bibr" coords="4,231.17,609.08,17.94,10.91" target="#b23">[24]</ref> fine-tuned Pythia and LLaMA models on human-labelled datasets to boost the models' performance and create an open-source competitor to ChatGPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The methodology section commences with a comprehensive overview of the corpora and the dataset used in each task. Subsequently, it details the methods employed for each task we participated in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Corpora and Dataset</head><p>For Task B, we were provided with a dataset containing data from the first ten editions of the challenge. The dataset included 4719 questions, categorized as 1417 'factoid', 1271 'yesno', 1130 'summaries', and 901 'lists'. Each question was accompanied by its relevant documents, snippets, concepts, RDF triples, and exact and ideal answers. To construct our corpus, we utilized the PubMed annual baseline document collections spanning from 2013 to 2023. This corpus consisted of the abstracts and titles of all documents. The most recent PubMed baseline collection (2023) contains approximately 35 million documents. However, we encountered a challenge due to the dynamic nature of the documents. Each year, documents are updated or removed, which means that the relevant documents for a question in the first edition may no longer be present in the document collection for the current edition. This posed a problem when relying solely on the latest baseline collection to extract the title and abstract for accurate querying. To address this issue, we augmented each question with the year it appeared in, enabling us to query the relevant documents more precisely.</p><p>Additionally, we encountered some documents that were missing titles, abstracts, or both. This could be due to licensing or linguistic issues. We addressed this by removing these incomplete documents from the collection. Afterwards, we created sparse Anserini <ref type="bibr" coords="5,452.70,391.34,17.75,10.91" target="#b24">[25]</ref> indexes for each year. Having yearly indexes proved advantageous as it allowed us to search for relevant documents specific to the year in which a question appeared. This approach enhanced the accuracy of retrieving pertinent information for each question.</p><p>Regarding the question dataset, there were cases where questions were repeated or were very similar although having a different set of relevant documents. Due to this fact, we decided to merge similar questions by merging the set of relevant articles to enrich the training data. To accomplish this, we leveraged the pre-trained SimCSE <ref type="bibr" coords="5,338.85,486.19,18.07,10.91" target="#b25">[26]</ref> model to compute the similarity between questions. Then, questions with a similarity score above 99% were automatically merged, while questions with a similarity score between 90% and 99% were manually reviewed. Another additional step was to remove the questions before BioASQ 4. During these years of the challenge, the systems were able to use the full-text article from PubMed Central (PMC) to make judgments. This will lead to situations where the model does not have the necessary content to make a correct prediction for these document pairs. At the end of this process, the number of resulting question were 3465 (-30%) totalling, 25781 question-documents positive pairs. In order to build a training dataset for neural relevance models, we need to also gather negative question-document pairs, such that the model can learn how to correctly score relevant and irrelevant documents. To accomplish this, we performed random sampling over the list of documents provided by the BM25 that were not positives for a given question. This should result in a list of strong negative documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Synthetic Question Generation</head><p>Data quality and quantity are crucial for developing strong, effective models in deep learning for information retrieval and relevance determination. In the previous section, we describe our pre-processing steps to increase the quality of the gold standard data. However, we are still missing in terms of data quantity. We propose generating questions by transformer-based language models to create a synthetic dataset that can be used to pre-train the relevance models to first learn basic retrieval patterns.</p><p>To synthetically generate a question for a given article, we fed an engineered prompt that tries to condition a language model to generate a question based on the information contained in the article. More formally, we empirically built the prompt, ùëù = {ùëù 1 , ..., ùëù ùëÄ }, such that a language model would maximize the probability of a question, ùë¶ = {ùë¶ 1 , ..., ùë¶ ùëÅ }, being sampled according to Equation <ref type="formula" coords="6,190.21,243.03,3.74,10.91" target="#formula_2">1</ref>.</p><formula xml:id="formula_2" coords="6,216.61,266.85,290.03,33.71">ùë¶ ‚àº ùëÅ ‚àèÔ∏Å ùëñ=1 ùëÉ (ùë¶ ùëñ |ùëù 1 , ..., ùëù ùëÄ , ùë¶ 1 , ..., ùë¶ ùëñ-1 )<label>(1)</label></formula><p>In this work, we mainly used zero-shot question generation since we did not explore training the language models to generate questions based on the BioASQ data. To further guide the language model into generating useful questions, we also included a question starting word as part of the prompt, such that the model will be forced to pick the following words conditioned on that starting word. Some examples of words that start a question are {What, Which, Is, List, Are, Does}<ref type="foot" coords="6,158.73,373.04,3.71,7.97" target="#foot_1">2</ref> , Prompt 1 shows the prompt that we adopted for generating a question in a zero-shot fashion with OA-pythia 12B model. &lt;|prompter|&gt;Given the following context \"{article}\", generate a question that can be answered by the information provided in the context: &lt;|endoftext|&gt;&lt;|assistant|&gt;What Prompt 1: Example of the last prompt used to generate synthetic questions with OA-pythia model</p><p>Regarding the language models that we used, we tried with small language models like, GPT-Neo-125M <ref type="bibr" coords="6,161.35,529.28,17.95,10.91" target="#b21">[22]</ref> and also with the larger ones such as OA-pythia-12B <ref type="bibr" coords="6,419.55,529.28,16.46,10.91" target="#b19">[20,</ref><ref type="bibr" coords="6,438.73,529.28,14.05,10.91" target="#b23">24]</ref> model. The synthetic dataset contained 79855 questions that were generated from 15971 randomly sampled articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Phase A</head><p>Our approach for the phase A of the challenge involved the development of a two-stage retrieval pipeline designed to handle the large volume of biomedical literature with efficiency. Figure <ref type="figure" coords="6,501.16,619.66,5.08,10.91" target="#fig_0">1</ref> presents the overview of our two-stage retrieval pipeline. At first, we utilized a sparse retrieval method. To accomplish this, we constructed an inverted index, a commonly used data structure in information retrieval that maps terms to the documents that contain them, using Anserini, a powerful retrieval toolkit built on Lucene <ref type="bibr" coords="7,467.93,262.28,16.22,10.91" target="#b26">[27]</ref>. For compatibility with our Python-based pipeline, we used Pyserini, Anserini's Python wrapper <ref type="bibr" coords="7,89.29,289.38,16.25,10.91" target="#b27">[28]</ref>.</p><p>For document retrieval, we adopted the BM25 ranking function, which is widely recognized for its effectiveness <ref type="bibr" coords="7,175.74,316.48,16.09,10.91" target="#b28">[29]</ref>. We selected the top 100 documents based on BM25 scores as the initial retrieval result and occasionally extended them to the top 1000 for broader coverage. Figure <ref type="figure" coords="7,89.29,343.58,5.12,10.91" target="#fig_2">3</ref> illustrates that extending from the top 100 to the top 1000 documents increases the number of expected documents in the set by 20% (from 71% to 91% recall). This extension provides a higher chance of retrieving more relevant documents. However, it comes with a trade-off in speed, as the neural retrieval system needs to process ten times more documents. It is worth noting that if the top 100 documents already contain a sufficient number of positive documents, using the top 1000 may not yield significant gains in metrics. This observation will be later addressed in the discussion section. The parameters for the BM25, specifically ùëè and ùëò 1 , were selected through a preliminary hyperparameter tuning process. Figure <ref type="figure" coords="7,264.22,642.25,5.02,10.91" target="#fig_3">4</ref> shows a summary of all the runs and their respective parameters. Based on this we adopted the parameters ùëò 1 = 0.5 and ùëè = 0.3.</p><p>In the second-stage, we utilized re-ranking models, which includes state-of-the-art transformer-based models such as PubMedbert <ref type="bibr" coords="8,302.78,254.29,12.91,10.91" target="#b5">[6]</ref> and monoT5 <ref type="bibr" coords="8,377.36,254.29,12.91,10.91" target="#b4">[5]</ref> (both base and large variantes). We also considered the BioGPT <ref type="bibr" coords="8,266.76,267.84,18.03,10.91" target="#b29">[30]</ref> and Pegasus <ref type="bibr" coords="8,345.72,267.84,18.03,10.91" target="#b30">[31]</ref> models, but due to their higher computational cost, they were discarded. These models were trained using both pointwise and pairwise approaches to evaluate their effectiveness in differing scenarios. To expand upon the limited availability of training data, we also experimented with including synthetic data in our training regimen as a pretraining mechanism. Finally, to consolidate the output from several models, we used the reciprocal rank fusion (RRF) <ref type="bibr" coords="8,116.32,349.14,16.26,10.91" target="#b31">[32]</ref>. This approach acts as an ensemble technique to improve the overall ranking order of the relevant documents by considering the judgment of multiple models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Submissions</head><p>The runs submitted to the phase A challenge were ensembles of various trained models with different checkpoints. The various systems submitted are briefly described in Table <ref type="table" coords="8,461.40,425.56,3.74,10.91">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Summary table of the configuration of the submitted system for each evaluation batch. The 'x' means that the system was used in that specific batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System Synthetic Training # Models Submissions BERT T5-L T5-B B1 B2 B3 B4</head><formula xml:id="formula_3" coords="8,137.75,535.40,316.48,135.09">System-0 False Pointwise 3 - - x x x x System-1 False Pointwise 5 - - x x x x System-2 False Pointwise - 2 2 x - - - True Pointwise 7 - - - x x x System-3 False Pointwise 2 2 - x - - - True Pairwise 7 - - - x x - Mixed Pointwise 5 - - - - - x System-4 False Pointwise - 4 - x - - - Mixed Mixed 22 3 - - x x x</formula><p>With more detail:</p><p>‚Ä¢ System-0: This system contained 3 PubMedBERT models that re-ranked 1000 documents fetched from BM25. ‚Ä¢ System-1: This system contained 5 PubMedBERT models that re-ranked 100 documents fetched from BM25. ‚Ä¢ System-2: For the first batch, the system contained 2 T5-base and 2 T5-Large models that re-ranked 100 documents from BM25. For the rest of the batches, the system used an ensemble of models trained on synthetic data and then fine-tuned on the challenge data. The models ensembled were 7 PubMedBERT models, 5 of which re-ranked 1000 documents, and the remaining 2 re-ranked 200 documents. ‚Ä¢ System-3: In the first batch, an ensemble of 2 T5-base models and 2 PubMedBERT models were used to re-rank 100 documents. The following 2 batches investigated pairwise training with synthetic data, where 7 PubMedBERT models were trained using a pairwise loss function. Among them, 4 models re-ranked 100 documents, and 3 re-ranked 500 documents. In the final batch, some models were removed and replaced with models from the first system. ‚Ä¢ System-4: In the first batch, the system contained 2 T5-Large models and 2 T5-Base models. The Large models re-ranked 1000 documents, and the Base models re-ranked 100 documents. In the remaining submissions, we ensembled most of our trained models, reaching a total of 25 models. However, it should be noted that only 24 models were used in the last batch. To provide a natural language answer to a question, we adopted an exploratory approach, testing various prompts and models to gauge their effectiveness in generating precise and meaningful answers. Recognizing the varying complexities inherent to different question types, we also experimented with per-question type prompting. This approach considers the nature of the question-be it factoid, list, or summary-and tailors the model's prompt accordingly, enabling more accurate and contextually relevant responses, see Prompt 2 as reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Phase B</head><p>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: {instruction} ### Input: ABSTRACT: {text} QUESTION: {question} ### Response:</p><p>Prompt 2: Example of a zero-shot prompt for generation answers with the Alpaca-LoRa model.</p><p>The text within brackets correspond to placehorders for the instruction, question and article text. The default instruction was "Given the ABSTRACT, answer the QUESTION". For yesno type of question we used the following "Given the input ABSTRACT produce a yes or no answer to QUESTION", while for the summary type we used "Given the input ABSTRACT produce a short and concise answer to QUESTION".</p><p>Context selection should play a large part in the quality of the text generation. We tested this using our top retrieved article from phase A, the top gold standard article, or a combination of both as context for the model. The latter was accomplished by selection the gold standard article that was ranked higher according to our model. A key focus of our experimentation was the application of various advanced language models such as ALPACA-LoRA (13 billion), OA-Pythia (12 billion) and OA-LLaMA (30 billion) models. Furthermore, we dabbled with different answer-generation strategies, including random sampling, beam search and contrastive search. In random search, a random token is selected for the next token following the probability distribution of the model. In beam search, multiple possible continuations at each step are explored based on a predefined beam width, aiming to find the most probable and coherent sequence of words. Contrastive search involves searching for alternative continuations or completions by contrasting different options and selecting the most distinctive or interesting one. We also extensively tested different hyperparameters for model generation, including temperature and the maximum token length. This experimentation allowed us to fine-tune our models' performance, leading to more precise and informative answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Submissions</head><p>For Phase B, our submissions consisted of various instruction transformer-based models, each described concisely in Table <ref type="table" coords="10,213.50,614.98,3.66,10.91">2</ref>. The "Document Source" column specifies the origin of the article used as context for answer generation. Specifically, "System-0"and "System-4" correspond to the highest scoring documents outputted by the respective phase A system. On the other hand, "Gold" indicates that the document was obtained from the provided gold standard.</p><p>More precisely, due to time constraints, for the first batch we selected only a single model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Summary table of the configuration of the submitted system for each evaluation batch for phase B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch System</head><p>Model Document Source LoRA-Alpaca OA-Pythia OA-LLaMA System-0 System-4 Gold</p><formula xml:id="formula_4" coords="11,108.09,150.73,381.04,176.82">1 System-0 7b - - x - - 2 System-0 7b - - - x - System-1 7b - - - x x System-2 30b - - - x - System-3 30b - - - x x System-4 30b - - - x x 3 System-0 - 12b - x - x System-2 - 12b - - x x System-3 - 12b - - x - 4 System-0 - - 30b x - - System-1 - - 30b - x x System-2 - 12b - - x x System-3 - - 30b - x - System-4 - 12b - - x -</formula><p>for submission using the top ranked document from our best performing model in phase A. This was seen as a naive approach, which is why in further batches we tested with both our models and the gold standard document to answer a question. In the second batch we used the same Alpaca-LoRA model, with the addition of the 30 billion parameter version, tested on the best performing model of the batch and also using the gold standard documents. For the third batch, we were unable to submit 5 submissions due to technical problems, however in this submission we changed the model to OpenAssistant's Pythia 12 billion parameter model. In the final batch, we additionally tested the OpenAssistant LLaMA model with 30 billion parameters.</p><p>Regarding the generation strategies, we adopted contrastive search for the Alpaca-LoRA and random sample with high confidence for the OpenAssistant variantes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>This section starts by addressing our validation results measured over a subset of the training data. Then we show the official preliminary results of the BioASQ challenge for phase A and B. Note that the preliminary results are the results available at time of writing and are due to changes after the reevaluation period. To see the official results, use the BioASQ 11B official leaderboard <ref type="foot" coords="11,141.95,587.19,3.71,7.97" target="#foot_2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Validation results</head><p>The validation of our models was conducted to assess their performance and gain valuable insights in their configuration. In this section, we summarize the validation results obtained over a subset of the training data. More precisely, we performed a stratified train/test split of 95/5 of the dataset, which corresponds to 3292 questions for training and 173 for validation.</p><p>Taking into consideration that the official evaluation batch only contains 90 questions, we believe that our split was representative. Table <ref type="table" coords="12,127.60,188.83,5.17,10.91" target="#tab_0">3</ref> summarizes the best validation results of various neural relevance models trained on different subsets of data and also the BM25 baseline. The models were evaluated based on their Mean Average Precision at 10 (MAP@10) score, which measures the average precision of the top 10 retrieved documents for each query. Each neural model was trained using different combinations of training data, including synthetic and gold standard datasets. Overall, when training with the gold standard data, all the reranking methods are capable to improve upon the baseline, reinforcing the idea that it is beneficial to adopt a reranking method as a second-stage mechanism of a retrieval pipeline. Regarding the architectures, the PubMedBERT and monoT5-large architectures managed to achieve comparable performances, whilst the monoT5-base architecture achieved considerably poor results. This disparity may be attributed to the fact that monoT5 is a sequence-to-sequence model that directly learns the retrieval task using natural language, placing greater emphasis on the quality of the underlying language model, and, therefore, their size.</p><p>Notably, the best configuration we obtained involved training the PubMedBERT model with synthetically generated data and subsequently fine-tuning it with the gold data. This outcome highlights the beneficial impact of incorporating synthetic data.</p><p>Furthermore, an unexpected result emerged when comparing the performance of models that were only trained with synthetic data against the BM25 baseline. It was surprising to observe that using only synthetic data yielded improvements over the performance of BM25. This suggests that it is indeed possible to train models without relying on gold data and still achieve superior performance compared to traditional baselines such as BM25. This finding opens up new possibilities for model training and highlights the potential of synthetic data as a valuable resource for retrieval tasks where no labelled data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Phase A</head><p>The preliminary results of our submissions are displayed in Table <ref type="table" coords="13,384.53,135.97,3.76,10.91" target="#tab_1">4</ref>, showcasing the rankings based on Mean Average Precision at 10 (MAP@10). Additionally, we provide the results regarding F1-score at 10, offering insights into the trade-off between precision and recall across the systems. The Top Competitor represents the most successful system among all competitor systems. Overall, we achieved highly competitive results, achieving the best-performing system in the first and second batches in MAP@10 and the best-performing system in all the batches in the F1-score. Significantly, the systems that attained these high F1-scores were relevance models, designed to discard documents if the likelihood of their relevance fell below 1%. Consequently, for questions with less than 10 positive documents, these systems were capable of outputting fewer than 10 documents, thus increasing precision compared to a ranking model that consistently outputs 10 documents regardless of their scores. Comparing now the performance between the systems, the initial two, namely System-0 and System-1, were employed to study the difference between re-ranking 1,000 and 100 documents. An analysis of these models' results across various batches revealed that the performance was not significantly affected by the increase in re-ranked documents. This observation will be further revisited in the subsequent discussion section.</p><p>Upon evaluating the remaining systems for the first batch, it was discerned that the utilization of T5 models did not significantly enhance performance compared to the BERT models. This observation carries substantial importance, especially given that the inference time for T5 models exceeded that of the BERT-based models. Consequently, the decision was taken to cease the deployment of T5 models in subsequent submissions, favouring instead the more efficient BERT models, which delivered adequate performance. Furthermore, System-4 for the initial batch demonstrated an unexpectedly lower Mean Average Precision (MAP) compared to the outcomes of other ensemble methods. This indicates that the specific configuration or ensemble of models in System-4 did not yield the anticipated results.</p><p>Furthermore, upon comparing System-2 and System-3, the distinctive variance can be traced back to the training technique deployed. It was deduced that pairwise training slightly under-performed compared to pointwise training methods. As a consequence, only pointwise training was used in the final batch.</p><p>Turning to the final system, System-4, it was observed that ensembling more models consistently outperformed the other systems in all instances. Again, this is an anticipated result corroborating existing literature <ref type="bibr" coords="14,235.39,141.16,16.25,10.91" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Phase B</head><p>The preliminary, automatically generated results regarding the phase B are displayed in Table <ref type="table" coords="14,89.29,204.44,3.81,10.91" target="#tab_2">5</ref>. Before analysis of the results, it is important to note that the metrics used to evaluate the systems in the competition is a manual evaluation of the ideal answers, rather than these automatic metrics. Overall, our systems showed a reasonable performance on the automatic metrics, at best placing 9th, and the remainder of the submission are mostly below the median position of the submissions. The metrics used in the competition Rougue-2 and Rogue-SU4, in the results presented, we show Rogue-2(F1). Given our approach used to generate the text was from an unsupervised nature, this is not surprising, as our system is not guided to generate expected BioASQ answers. Nevertheless, the answers can be correct and therefore missed by the automatic metrics. In Appendix A we showcase some examples of answerers that were generated by the OA-LLAMA-30B model and the OA-Pythia-12B model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Throughout phase A, we observed that our reranking methods consistently enhanced the baseline ranking order, which is known to be a challenging task, as mentioned in <ref type="bibr" coords="14,438.63,615.26,11.57,10.91" target="#b8">[9]</ref>. To provide a more tangible visualization of these improvements, we present in Figure <ref type="figure" coords="14,430.31,628.81,5.17,10.91" target="#fig_5">6</ref> the ratio of improvement achieved by our reranking models in comparison to the BM25 baseline. Remarkably, across all batches, our reranking models achieved an average improvement of 30%, and in some cases, even nearing 40%. We attribute these notable gains to two primary factors. Firstly, the quality of our training data played a crucial role, as we focused on meticulous cleaning of the gold standard data prior to training our models. Additionally, the availability of more advanced training algorithms enabled efficient fine-tuning of entire transformer-based models, further contributing to the model's performance. Next, we delve into a detailed discussion of various factors that impact the performance of our systems, namely model architecture, loss function, the number of reranked documents, and the utilization of synthetic data during pretraining. To facilitate this analysis, we present parallel plots in Figures <ref type="figure" coords="15,196.49,376.95,5.07,10.91" target="#fig_6">7</ref> and<ref type="figure" coords="15,223.38,376.95,3.73,10.91" target="#fig_7">8</ref>, showcasing these variables for the models used in the second and fourth batches, respectively. Although we focus on these two batches for clarity, it is worth noting that the first and third batches follow similar patterns.</p><p>Upon examining both figures, it becomes evident that the preferred architecture and loss function for optimal performance are PubMedBERT and pointwise, respectively, as these models achieved the highest MAP@10 scores according to the plots. Furthermore, in terms of the number of documents used for reranking, it appears that increasing the count does not lead to improved metrics. This observation may be attributed to the fact that the evaluation metrics only consider the top ten documents. This consideration arises from the fact that the BioASQ team evaluates the system's performance based on the top 10 documents only. Therefore, when there are already enough positive documents among the top 100, reranking a larger number of documents may not result in noticeable improvements.</p><p>Finally, the impact of synthetic data yields contradictory results. In the case of the second batch (Figure <ref type="figure" coords="15,152.94,553.09,3.65,10.91" target="#fig_6">7</ref>), incorporating synthetic data did not contribute to an overall performance improvement. However, for the fourth batch, it did exhibit a positive effect. We speculate that this discrepancy may be attributed to the quantity and coverage of the synthetic questions generated. Specifically, for the fourth batch, the test set questions may have been closer to those synthetically generated, particularly in terms of the documents used for their generation. Further investigation is needed to validate this hypothesis.</p><p>The generation phase (Phase B) of our system presented several insightful findings. Notably, we observed a positive correlation between the size of the language model and the quality of generated answers, which aligns with previous findings that larger models generally tend to    perform better <ref type="bibr" coords="16,157.17,411.49,16.43,10.91" target="#b33">[34,</ref><ref type="bibr" coords="16,176.32,411.49,12.32,10.91" target="#b34">35]</ref>. Additionally, we found that small modifications to the prompt significantly impacted the system's output, suggesting that the models may struggle with generalization. This effect was more pronounced in smaller models, indicating that fine-tuning may be necessary to achieve optimal results <ref type="bibr" coords="16,158.66,465.69,16.30,10.91" target="#b35">[36]</ref>. In contrast, for larger models, the quality of generation was less affected by the prompt variation, showcasing their robustness.</p><p>Overall, the text generation quality was satisfactory, demonstrating coherence and relevance to the biomedical questions. The employment of different prompts for various question types particularly enhanced the performance of smaller models, aligning them more closely with the inherent intricacies of each question category.</p><p>We also explored ensembling multiple contexts to improve answer diversity and depth. Unfortunately, our attempts were not fruitful, suggesting that this method might require further refinement for it to be effective in this specific task.</p><p>Finally, we hypothesize that with some pre-training or domain-specific training, the models might perform even better. Such training could enhance their ability to generate precise and contextually accurate answers for biomedical questions, further increasing their utility in real-world applications <ref type="bibr" coords="16,195.03,628.28,16.25,10.91" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we detailed our participation on tasks B phase A and B of the eleventh edition of the BioASQ challenge. For phase A, we adopted a two-stage retrieval pipeline comprising the Anserini BM25 as the initial stage, followed by reranker models based on PubMedBERT and monoT5 transformer-based models. In order to effectively train the reranker models effectively, we enhanced the quality of the gold standard data through careful cleaning and also explored synthetic data augmentation techniques through question generation. By using these methods, we achieved significant improvements over the baseline ranking order. Our systems, were able to place first in various batches of the competition.</p><p>For phase B, our approach involved leveraging instruction transformer-based models to generate answers conditioned on the articles retrieved during phase A in a zero-shot setting. We observed a positive correlation between the size of the language model and the quality of the generated answers. Smaller models were more sensitive to prompt variations, indicating the need for fine-tuning to enhance their performance. Larger models, on the other hand, exhibited greater robustness and generated coherent and relevant answers. The employment of different prompts for various question types improved the performance of smaller models, aligning them more closely with the specific intricacies of each question category. Overall our performance on the phase B, according to the automatic metrics, was mediocre. However, we believe that further manual analysis is required for a fair evaluation given the unsupervised nature of our generation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Future Work</head><p>In terms of the direction for future work, several promising avenues appear worthy of exploration, particularly for Phase B of our system.</p><p>First, while our initial attempts to join multiple contexts (ensembling) did not yield the anticipated results, we believe this approach still holds considerable potential. Therefore, refining our ensembling techniques to effectively integrate different contexts into the questionanswering process will be an area of interest. This could potentially enhance both the diversity and depth of our generated answers.</p><p>Second, the incorporation of snippet extraction as an intermediary step in our approach might serve to enhance the precision of our answer generation. Extracting relevant snippets from the retrieved documents could refine the context that is fed into our models, potentially leading to more accurate and relevant answers. Several recent works have reported success using such techniques <ref type="bibr" coords="17,190.65,562.83,16.25,10.91" target="#b37">[38]</ref>.</p><p>Lastly, fine-tuning the models specifically for the ideal answers in Phase B of Task B could further boost performance. As we observed that prompt changes significantly impacted the system's output, especially for smaller models, task-specific fine-tuning might increase the models' robustness against these changes and enhance their overall performance. In fact, recent studies have shown that fine-tuning large-scale pre-trained models on downstream tasks can lead to substantial improvements in task performance <ref type="bibr" coords="17,331.65,644.13,16.43,10.91" target="#b34">[35,</ref><ref type="bibr" coords="17,350.80,644.13,12.32,10.91" target="#b35">36]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Examples of answers generation.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question: Which a r e t h e t y p e s o f C h a r c o t -Marie</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,247.86,243.83,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: High level overview of the entire system pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,197.58,379.46,8.93"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the proposed two-stage retrieval pipeline for participating in phase A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,89.29,596.60,316.17,8.93"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Recall value of the retrieved documents at different result set sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,89.29,204.52,416.69,9.65;8,89.29,216.74,181.83,8.87"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Parallel coordinate plot showing impact of ùëè and ùëò 1 hyperparameters in determining recall score during hyperparameter tuning process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,89.29,561.63,400.18,8.93"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Overview of our methodology for participating in the phase B of the BioASQ challenge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="15,89.29,296.92,416.69,8.93;15,89.29,308.93,72.21,8.87"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Improvement ratio over the baseline BM25 in terms of MAP@10 of all the submitted systems in all the batches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="16,89.29,205.88,416.70,8.93;16,89.29,217.88,132.51,8.87"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Parallel plot showing the impact of different hyperparameters in the MAP score of the neural retrieval models used in batch 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="16,89.29,364.16,416.70,8.93;16,89.29,376.16,132.51,8.87"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Parallel plot showing the impact of different hyperparameters in the MAP score of the neural retrieval models used in batch 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="22,395.69,117.17,34.72,9.96;22,152.26,129.13,81.49,9.96;22,130.96,153.04,328.22,9.96;22,151.14,164.99,290.76,9.96;22,130.96,188.90,326.98,9.96;22,158.37,200.86,278.32,9.96;22,152.06,212.81,290.59,9.96;22,152.49,224.77,296.23,9.96;22,151.68,236.72,308.39,9.96;22,157.85,248.68,284.34,9.96;22,152.35,260.63,267.29,9.96;22,151.79,272.59,309.34,9.96;22,152.39,284.54,310.20,9.96;22,152.70,296.50,307.51,9.96;22,151.69,308.45,308.80,9.96;22,158.20,320.41,284.31,9.96;22,151.91,332.36,290.38,9.96;22,152.65,344.32,295.37,9.96;22,151.97,356.27,116.47,9.96;22,159.00,379.39,277.28,10.91;22,130.96,407.80,173.99,9.96;22,130.96,431.71,274.57,9.96;22,152.56,443.67,289.69,9.96;22,152.16,455.62,272.58,9.96;22,152.43,467.58,74.27,9.96;22,130.96,491.49,326.74,9.96;22,152.39,503.44,308.18,9.96;22,158.13,515.40,302.48,9.96;22,157.71,527.35,291.34,9.96;22,152.39,539.31,289.27,9.96;22,152.09,551.26,145.34,9.96;22,159.13,574.38,277.01,10.91;22,130.96,602.79,323.43,9.96;22,130.96,626.70,317.50,9.96;22,152.23,638.65,303.27,9.96;22,150.89,650.61,46.41,9.96;23,130.96,87.68,308.26,9.96;23,152.04,99.63,170.30,9.96;23,158.95,122.75,277.39,10.91;23,130.96,151.16,299.21,9.96;23,152.35,163.11,93.36,9.96;23,130.96,187.02,311.32,9.96;23,152.52,198.98,284.04,9.96;23,152.85,210.93,295.06,9.96;23,152.17,222.89,260.31,9.96;23,152.53,234.84,193.83,9.96;23,130.96,258.75,308.52,9.96;23,152.23,270.71,308.25,9.96;23,158.83,282.67,284.17,9.96;23,152.71,294.62,295.59,9.96;23,152.17,306.58,296.16,9.96;23,152.15,318.53,302.07,9.96;23,152.30,330.49,308.40,9.96;23,158.04,342.44,302.40,9.96;23,158.20,354.40,302.02,9.96;23,156.87,366.35,279.19,9.96;23,151.99,378.31,266.11,9.96;23,152.38,390.26,308.08,9.96;23,157.86,402.22,218.89,9.96;23,152.30,414.17,266.22,9.96;23,152.66,426.13,272.47,9.96;23,152.40,438.08,272.22,9.96;23,152.36,450.04,308.12,9.96;23,158.71,461.99,295.57,9.96;23,152.19,473.95,272.06,9.96;23,152.23,485.90,277.71,9.96;23,152.53,497.86,284.00,9.96;23,152.64,509.81,150.77,9.96;23,158.94,532.93,277.41,10.91"><head>6 ]generated 1 : 2 .Answer generated 3 :</head><label>6123</label><figDesc>-Tooth D i s e a s e (CMT) ? OA-LLaMA-30B: T h e re a r e f o u r main t y p e s o f C h a r c o t -Marie -Tooth D i s e a s e (CMT) : CMT1 , CMT2 , CMT3 , and CMT4 . Pythia-12B: The f o u r t y p e s o f CHARCOT-MARIE-TOOTH DISEASE (OMIM # 3 0 1 5 0 0 ) a r e c a u s e d by m u t a t i o n s i n g e n e s e n c o d i n g components o f p a r a n o d a l a x o h e m i s e g m e n t a l j u n c t i o n s ( AHJs ) . They i n c l u d e d e m y e l i n a t i n g f o r m s due t o l o s s -of -f u n c t i o n m u t a t i o n s i n NEURL4 ( Type 1 ;Oculomotor A p r a x i a L a t e r a l i s 4 ) ; D e j e r i n e -S o t t a s D y s a r t h r i a ( DSD ; Type 2A ; MOCS2A ) ; K u g e l b e r g -Welander S p a s t i c P a r a p l e g i a ( KWSPRAP ; Type 2B ; SPG11 ) and H e r e d i t a r y D i f f u s e N e u r o n a l D e g e n e r a t i o n (HDND ; Type 3 ; VPS13A ) . I n h e r i t e d Axon G u a r i n o n i J u n t o s ( IAGJ ; Type 4 ; UNC5H1 ) i s c h a r a c t e r i z e d by o n i o n b u l b f o r m a t i o n a t AHJs due t o i n c r e a s e d e x p r e s s i o n o f C a s p r and c o n t a c t i n -a s s o c i a t e d p r o t e i n ( CTDPNNP1 ) r e s u l t i n g from g a i n -of -f u n c t i o n m u t a t i o n s i n t h e i r p r o m o t e r r e g i o n s . [ Answer Question id 6446caac57b1c7a315000074Question: What a r e L u m i n o p s i n s ? OA-LLaMA-30B: L u m i n o p s i n s a r e c h i m e r i c p r o t e i n s c o n s i s t i n g o f a l u c i f e r a s e f u s e d t o an o p s i n t h a t a l l o w f o r c o n t r o l o f n e u r o n a l a c t i v i t y t h r o u g h o p t o g e n e t i c s . Pythia-12B: L u m i n o p s i n s ( a b b r e v i a t e d a s LMOs o r Lumisomes ) a r e g e n e t i c a l l y e n c o d e d p h o t o s w i t c h e s t h a t r e s p o n d t o b l u e l i g h t by c h a n g i n g t h e i r a b s o r p t i o n s p e c t r u m from 5 0 0 nm t o 600 -700nm . [ 1 ] [ 2 ] They c o n s i s t o f a f u s i o n p r o t e i n b e t w e e n l u c i f e r a s e s s u c h a s f i r e f l y L u c i n a p e l l u c i d a [ b ] ( . . . ) Answer generated 2: Question id 6419cd02690f196b5100003a Question: I s SARS-CoV-2 t r a n s m i t t e d t h r o u g h b r e a s t m i l k ? OA-LLaMA-30B: No , a c c o r d i n g t o t h i s s t u d y , t h e r e i s no e v i d e n c e t h a t b r e a s t m i l k c o n t a i n s i n f e c t i o u s SARS-COVID -Pythia-12B: No , a c c o r d i n g t o t h i s s t u d y which a n a l y z e d o v e r 1 0 0 women ' s b r e a s t m i l k . Question id 641791ed690f196b51000032 Question: Which amino a c i d i n i m p l i c a t e d i n t h e B l u e d i a p e r syndrome ? OA-LLaMA-30B: The B l u e d i a p e r syndrome i s c a u s e d by a f r a m e s h i f t m u t a t i o n i n t h e P r o p r o t e i n C o n v e r t a s e S u b t i l i s i n / K e x i n Type 1 ( PCSK1 ) gene which r e s u l t s i n t h e s u b s t i t u t i o n o f V a l i n e f o r L e u c i n e a t p o s i t i o n 2 2 7 ( p . [ V a l 2 2 7 L e u f s * 1 2 ] ) Pythia-12B: The p a t i e n t ' s symptoms a r e c o n s i s t e n t w i t h t h o s e s e e n i n i n d i v i d u a l s w i t h P r o p r o t e i n C o n v e r t a s e S u b t i l i s i n / K e x i n Type 1 ( PCSK1 ) m u t a t i o n s , which r e s u l t i n d e c r e a s e d a c t i v i t y o r l a c k o f e x p r e s s i o n o f t h e p r o t e i n . S p e c i f i c a l l y , t h e s e m u t a t i o n s l e a d t o a d e c r e a s e i n p r o t e o l y t i c p r o c e s s i n g o f p e p t i d e s i n v o l v e d i n n u t r i e n t t r a n s p o r t a c r o s s e p i t h e l i a s u c h a s j e j u n u m and i le um , r e s u l t i n g i n m a l a b s o r p t i o n o f c a r b o h y d r a t e s , f a t s , and p r o t e i n s . T h i s may e x p l a i n why t h e p a t i e n t e x p e r i e n c e d e p i s o d e s o f f a s t i n g h y p o k a l e m i a s e c o n d a r y t o c a r b o h y d r a t e and f a t m a l d i g e s t i o n w h i l e a l s o e x p e r i e n c i n g d i a r r h e a due t o p o o r d i g e s t i o n and m a l a s s i m i l a t i o n o f m a c r o n u t r i e n t s . I t s h o u l d be n o t e d t h a t o t h e r f a c t o r s s u c h a s i m b a l a n c e s i n e l e c t r o l y t e s and b i c a r b o n a t e p r o d u c t i o n c o u l d c o n t r i b u t e t o t h e p a t i e n t ' s p h e n o t y p e a s w e l l . A d d i t i o n a l l y , g i v e n t h e r a r i t y o f t h e c o n d i t i o n , i t i s p o s s i b l e t h a t t h e r e a r e u n d i s c o v e r e d m o d i f i e r g e n e s t h a t i n f l u e n c e p e n e t r a n c e and e x p r e s s i v i t y o f PCSK1 m u t a t i o n s , f u r t h e r c o m p l i c a t i n g d i a g n o s i s and management o f a f f e c t e d i n d i v i d u a l s ( . . . ) Answer generated 4: Question id 63f57d9b33942b094c000004</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="12,88.99,270.78,417.00,164.51"><head>Table 3</head><label>3</label><figDesc>Summary of the best validation results for various relevance models and different combinations of training data.</figDesc><table coords="12,180.17,314.30,234.93,120.99"><row><cell>Model type</cell><cell cols="2">Training data Synthetic Gold standard</cell><cell>MAP@10</cell></row><row><cell>PubMedBERT</cell><cell>x</cell><cell>x</cell><cell>58.06</cell></row><row><cell>PubMedBERT</cell><cell></cell><cell>x</cell><cell>57.75</cell></row><row><cell>PubMedBERT</cell><cell>x</cell><cell></cell><cell>51.74</cell></row><row><cell>monoT5-base</cell><cell>x</cell><cell></cell><cell>49.89</cell></row><row><cell>monoT5-base</cell><cell></cell><cell>x</cell><cell>51.90</cell></row><row><cell>monoT5-large</cell><cell></cell><cell>x</cell><cell>57.36</cell></row><row><cell>BM25</cell><cell></cell><cell></cell><cell>43.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="13,88.99,298.94,410.07,150.32"><head>Table 4</head><label>4</label><figDesc>Preliminary results made available by the BioASQ team for all the batches for phase A.</figDesc><table coords="13,98.28,330.85,400.78,118.41"><row><cell>System</cell><cell>F1</cell><cell cols="2">Batch 1 MAP Rank</cell><cell>F1</cell><cell cols="2">Batch 2 MAP Rank</cell><cell>F1</cell><cell cols="2">Batch 3 MAP Rank</cell><cell>F1</cell><cell cols="2">Batch 4 MAP Rank</cell></row><row><cell>System-0</cell><cell cols="2">27.74 45.90</cell><cell>1</cell><cell cols="2">20.21 35.80</cell><cell>9</cell><cell cols="2">19.11 28.39</cell><cell>13</cell><cell cols="2">18.09 25.80</cell><cell>10</cell></row><row><cell>System-1</cell><cell cols="2">27.69 45.31</cell><cell>2</cell><cell cols="2">20.88 38.40</cell><cell>2</cell><cell cols="2">19.09 28.94</cell><cell>12</cell><cell cols="2">19.25 27.03</cell><cell>8</cell></row><row><cell>System-2</cell><cell cols="2">21.92 45.22</cell><cell>3</cell><cell cols="2">16.32 35.35</cell><cell>11</cell><cell cols="2">17.83 28.96</cell><cell>11</cell><cell cols="2">18.13 27.01</cell><cell>9</cell></row><row><cell>System-3</cell><cell cols="2">24.18 44.99</cell><cell>4</cell><cell cols="2">14.97 34.61</cell><cell>12</cell><cell cols="2">12.64 27.20</cell><cell>14</cell><cell cols="2">12.81 27.63</cell><cell>6</cell></row><row><cell>System-4</cell><cell cols="2">21.83 42.75</cell><cell>10</cell><cell cols="2">16.18 38.52</cell><cell>1</cell><cell cols="2">12.77 30.42</cell><cell>4</cell><cell cols="2">12.73 27.70</cell><cell>5</cell></row><row><cell>Baseline</cell><cell cols="2">12.00 34.50</cell><cell>-</cell><cell>5.10</cell><cell>29.96</cell><cell>-</cell><cell>5.91</cell><cell>22.42</cell><cell>-</cell><cell>5.35</cell><cell>19.96</cell><cell>-</cell></row><row><cell cols="3">Top Competitor 18.23 44.62</cell><cell>4</cell><cell cols="2">15.98 37.42</cell><cell>3</cell><cell cols="2">13.20 31.85</cell><cell>1</cell><cell cols="2">14.25 32.24</cell><cell>1</cell></row><row><cell>Median</cell><cell cols="2">16.78 37.32</cell><cell>17</cell><cell cols="2">13.72 27.81</cell><cell>17</cell><cell cols="2">11.10 23.26</cell><cell>18</cell><cell cols="2">10.42 21.47</cell><cell>14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="14,88.98,355.39,388.75,59.41"><head>Table 5</head><label>5</label><figDesc>Automatic evaluation made available by the BioASQ team for all the batches for phase B.</figDesc><table coords="14,101.29,389.33,376.44,25.48"><row><cell>System</cell><cell>R-2</cell><cell>Batch 1</cell><cell>Batch 2</cell><cell>Batch 3</cell><cell>Batch 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="14,101.29,405.03,395.45,131.69"><head>(F1) Rank R-2(F1) Rank R-2(F1) Rank R-2(F1) Rank</head><label></label><figDesc></figDesc><table coords="14,101.29,426.79,387.52,109.94"><row><cell>System-0</cell><cell>29.07</cell><cell>12</cell><cell>17.84</cell><cell>21</cell><cell>12.908</cell><cell>26</cell><cell>15.86</cell><cell>26</cell></row><row><cell>System-1</cell><cell>-</cell><cell>-</cell><cell>22.57</cell><cell>18</cell><cell>-</cell><cell>-</cell><cell>24.33</cell><cell>24</cell></row><row><cell>System-2</cell><cell>-</cell><cell>-</cell><cell>28.53</cell><cell>16</cell><cell>12.14</cell><cell>25</cell><cell>10.76</cell><cell>33</cell></row><row><cell>System-3</cell><cell>-</cell><cell>-</cell><cell>32.42</cell><cell>9</cell><cell>08.91</cell><cell>30</cell><cell>15.82</cell><cell>27</cell></row><row><cell>System-4</cell><cell>-</cell><cell>-</cell><cell>31.69</cell><cell>10</cell><cell>-</cell><cell>-</cell><cell>08.92</cell><cell>35</cell></row><row><cell>Median</cell><cell>31.03</cell><cell>10</cell><cell>28.53</cell><cell>16</cell><cell>20.54</cell><cell>18</cell><cell>32.12</cell><cell>19</cell></row><row><cell>Best</cell><cell>40.63</cell><cell>1</cell><cell>32.90</cell><cell>1</cell><cell>37.41</cell><cell>1</cell><cell>40.84</cell><cell>1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,92.57,670.98,132.98,8.97"><p>https://github.com/tloen/alpaca-lora</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,92.30,671.00,338.10,8.97"><p>These words follow the distribution of the starting words that appear in the BioASQ dataset.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="11,92.57,659.97,414.00,8.97;11,92.57,670.93,72.03,8.97"><p>Phase A: http://participants-area.bioasq.org/results/11b/phaseA/, Phase B: http://participants-area.bioasq.org/ results/11b/phaseB/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially supported by national funds through the <rs type="funder">Foundation for Science and Technology (FCT)</rs> in the context of the project <rs type="grantNumber">UIDB/00127/2020</rs>. <rs type="person">Tiago Almeida</rs> is funded by <rs type="funder">FCT</rs> under the grant <rs type="grantNumber">2020.05784</rs>.BD. <rs type="person">Jorge Miguel Silva</rs> has received funding from the <rs type="funder">EC</rs> under grant agreement <rs type="grantNumber">101081813</rs>, <rs type="projectName">Genomic Data Infrastructure</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7ZXR3BW">
					<idno type="grant-number">UIDB/00127/2020</idno>
				</org>
				<org type="funding" xml:id="_9GUF7Xw">
					<idno type="grant-number">2020.05784</idno>
				</org>
				<org type="funded-project" xml:id="_NM2jMWA">
					<idno type="grant-number">101081813</idno>
					<orgName type="project" subtype="full">Genomic Data Infrastructure</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="18,112.66,210.55,393.33,10.91;18,112.66,224.10,393.33,10.91;18,112.33,237.65,68.33,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="18,293.58,210.55,212.41,10.91;18,112.66,224.10,57.31,10.91">Information overload in healthcare: too much of a good thing?</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Klerings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Weinhandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Thaler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,177.99,224.10,310.43,10.91">Zeitschrift f√ºr Evidenz, Fortbildung und Qualit√§t im Gesundheitswesen</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="285" to="290" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,251.20,394.52,10.91;18,112.66,264.75,395.17,10.91;18,112.66,278.30,394.61,10.91;18,112.66,291.85,394.53,10.91;18,112.66,305.40,22.69,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="18,371.39,264.75,136.44,10.91;18,112.66,278.30,265.47,10.91">BioASQ: A challenge on largescale biomedical semantic indexing and question answering</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Almirantis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Artieres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Alvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zschunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,406.30,278.30,100.97,10.91;18,112.66,291.85,284.53,10.91">AAAI fall symposium: Information retrieval and knowledge discovery in biomedical text</title>
		<meeting><address><addrLine>Arlington, VA</addrLine></address></meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,318.95,394.53,10.91;18,112.66,332.50,393.33,10.91;18,112.66,346.05,393.33,10.91;18,112.66,359.59,393.33,10.91;18,112.66,373.14,306.11,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="18,234.48,332.50,271.51,10.91;18,112.66,346.05,307.20,10.91">Overview of BioASQ 2023: The eleventh BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lima-L√≥pez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Farr√©-Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,445.02,346.05,60.97,10.91;18,112.66,359.59,393.33,10.91;18,112.66,373.14,252.02,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="18,112.66,386.69,394.53,10.91;18,112.66,400.24,395.00,10.91;18,112.66,413.79,323.71,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="18,236.38,386.69,265.55,10.91">The Probabilistic Relevance Framework: BM25 and Beyond</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000019</idno>
		<ptr target="https://www.nowpublishers.com/article/Details/INR-019.doi:10.1561/1500000019" />
	</analytic>
	<monogr>
		<title level="j" coord="18,112.66,400.24,228.16,10.91">Foundations and Trends¬Æ in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,427.34,393.33,10.91;18,112.66,440.89,395.17,10.91;18,112.66,454.44,394.53,10.91;18,112.41,467.99,394.76,10.91;18,112.66,483.98,133.46,7.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="18,325.80,427.34,180.18,10.91;18,112.66,440.89,131.29,10.91">Document Ranking with a Pretrained Sequence-to-Sequence Model</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.63</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.findings-emnlp.63.doi:10.18653/v1/2020.findings-emnlp.63" />
	</analytic>
	<monogr>
		<title level="m" coord="18,271.89,440.89,235.94,10.91;18,112.66,454.44,289.18,10.91">Findings of the Association for Computational Linguistics: EMNLP 2020, Association for Computational Linguistics</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="708" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,495.09,394.53,10.91;18,112.66,508.64,394.53,10.91;18,112.28,522.18,332.27,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="18,112.66,508.64,390.01,10.91">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,112.28,522.18,268.63,10.91">ACM Transactions on Computing for Healthcare (HEALTH)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,535.73,393.33,10.91;18,112.66,549.28,395.17,10.91;18,112.66,562.83,395.17,10.91;18,112.66,576.38,394.53,10.91;18,112.39,589.93,394.88,10.91;18,112.66,603.48,191.86,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="18,447.57,535.73,58.42,10.91;18,112.66,549.28,173.81,10.91">Overview of BioASQ tasks 10a, 10b and Synergy10</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vandorou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-3180/paper-10.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="18,302.17,549.28,42.49,10.91;18,231.00,562.83,276.84,10.91;18,112.66,576.38,177.25,10.91">Proceedings of the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="18,188.14,589.93,158.53,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 5th -to -8th, 2022. 2022</date>
			<biblScope unit="volume">3180</biblScope>
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
	<note>CLEF2022</note>
</biblStruct>

<biblStruct coords="18,112.66,617.03,395.17,10.91;18,112.66,630.58,394.61,10.91;18,112.66,644.13,393.32,10.91;18,112.66,657.68,393.53,10.91;19,112.66,86.97,395.01,10.91;19,112.66,100.52,258.08,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="18,299.49,617.03,208.34,10.91;18,112.66,630.58,374.64,10.91">Deep Learning solutions based on fixed contextualized embeddings from PubMedBERT on BioASQ 10b and traditional IR in Synergy</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pinho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Matos</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-3180/paper-12.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="18,351.19,644.13,154.80,10.91;18,112.66,657.68,271.14,10.91">Proceedings of the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="19,270.11,86.97,151.83,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 5th -to -8th, 2022. 2022</date>
			<biblScope unit="volume">3180</biblScope>
			<biblScope unit="page" from="204" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,114.06,394.53,10.91;19,112.66,127.61,393.33,10.91;19,112.66,141.16,394.53,10.91;19,112.66,154.71,394.53,10.91;19,112.66,168.26,299.84,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="19,215.99,114.06,286.90,10.91">Universal passage weighting mecanism (UPWM) in BioASQ 9b</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Matos</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-2936/paper-13.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="19,379.22,127.61,126.76,10.91;19,112.66,141.16,296.02,10.91">Proceedings of the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="19,318.03,154.71,146.46,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum<address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">September 21st -to -24th, 2021. 2936. 2021</date>
			<biblScope unit="page" from="196" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,181.81,393.33,10.91;19,112.66,195.36,397.48,10.91;19,112.36,208.91,131.68,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="19,187.28,181.81,318.71,10.91;19,112.66,195.36,64.29,10.91">Context-Aware Sentence/Passage Term Importance Estimation For First Stage Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1910.10687</idno>
		<idno type="arXiv">arXiv:1910.10687</idno>
		<ptr target="http://arxiv.org/abs/1910.10687.doi:10.48550/arXiv.1910.10687" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,222.46,395.17,10.91;19,112.66,236.01,394.52,10.91;19,112.66,249.56,395.01,10.91;19,112.66,263.11,196.08,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="19,188.92,222.46,276.60,10.91">Context-Aware Document Term Weighting for Ad-Hoc Search</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380258</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/3366423.3380258.doi:10.1145/3366423.3380258" />
	</analytic>
	<monogr>
		<title level="m" coord="19,488.74,222.46,19.10,10.91;19,112.66,236.01,217.28,10.91">Proceedings of The Web Conference 2020, WWW &apos;20</title>
		<meeting>The Web Conference 2020, WWW &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1897" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,276.66,393.33,10.91;19,112.66,290.20,107.17,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="19,187.93,276.66,248.11,10.91">Condenser: a pre-training architecture for dense retrieval</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08253</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="19,112.66,303.75,393.33,10.91;19,112.66,317.30,394.61,10.91;19,112.31,330.85,218.89,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="19,438.63,303.75,67.35,10.91;19,112.66,317.30,204.92,10.91">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2004.04906" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,344.40,395.17,10.91;19,112.66,357.95,393.33,10.91;19,112.66,371.50,369.67,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="19,490.56,344.40,17.27,10.91;19,112.66,357.95,359.87,10.91">Approximate nearest neighbor negative contrastive learning for dense text retrieval</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Overwijk</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2007.00808" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,385.05,395.17,10.91;19,112.66,398.60,156.41,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="19,255.31,385.05,179.93,10.91">Billion-scale similarity search with GPUs</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>J√©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,444.73,385.05,63.11,10.91;19,112.66,398.60,77.55,10.91">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,412.15,393.33,10.91;19,112.26,425.70,393.72,10.91;19,112.66,439.25,327.57,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="19,322.00,412.15,183.99,10.91;19,112.26,425.70,85.46,10.91">Optimizing dense retrieval model training with hard negatives</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,220.57,425.70,285.41,10.91;19,112.66,439.25,229.90,10.91">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1503" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,452.79,394.53,10.91;19,112.66,466.34,393.32,10.91;19,112.66,479.89,266.90,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="19,274.48,466.34,169.72,10.91">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,452.29,466.34,53.69,10.91;19,112.66,479.89,172.82,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,493.44,394.53,10.91;19,112.66,506.99,393.33,10.91;19,112.66,520.54,211.84,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Rozi√®re</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m" coord="19,288.90,506.99,217.08,10.91;19,112.66,520.54,29.25,10.91">Llama: Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="19,112.66,534.09,394.53,10.91;19,112.66,547.64,394.04,10.91;19,112.66,561.19,98.26,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<title level="m" coord="19,112.66,547.64,254.96,10.91">Stanford alpaca: An instruction-following LLaMA model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,574.74,395.01,10.91;19,112.66,588.29,394.53,10.91;19,112.66,601.84,395.01,10.91;19,112.66,617.83,97.35,7.90" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">S</forename><surname>Prashanth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Raff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Van Der Wal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01373</idno>
		<title level="m" coord="19,112.66,601.84,364.10,10.91">Pythia: A suite for analyzing large language models across training and scaling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,628.93,395.01,10.91;19,112.66,642.48,394.53,10.91;19,112.66,656.03,394.53,10.91;19,112.48,669.58,394.70,10.91;20,112.66,86.97,394.53,10.91;20,112.66,100.52,394.53,10.91;20,112.28,114.06,394.91,10.91;20,112.66,127.61,394.53,10.91;20,112.66,141.16,394.53,10.91;20,112.66,154.71,122.77,10.91" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Fiedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m" coord="20,278.76,141.16,223.29,10.91">PaLM: Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,168.26,395.17,10.91;20,112.66,181.81,395.01,10.91;20,112.66,195.36,395.17,10.91;20,112.66,208.91,21.34,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="20,327.48,168.26,180.35,10.91;20,112.66,181.81,165.75,10.91">GPT-Neo: Large scale autoregressive language modeling with mesh-tensorflow</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5297715</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5297715.doi:10.5281/zenodo.5297715,Ifyouusethissoftware,pleaseciteitusingthesemeta-data" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,222.46,394.62,10.91;20,112.66,236.01,393.33,10.91;20,112.66,249.56,339.95,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="20,478.90,222.46,28.37,10.91;20,112.66,236.01,203.20,10.91">LoRA: Low-rank adaptation of large language models</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=nZeVKeeFYf9" />
	</analytic>
	<monogr>
		<title level="m" coord="20,339.11,236.01,166.88,10.91;20,112.66,249.56,69.13,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,263.11,394.53,10.91;20,112.66,276.66,394.53,10.91;20,112.66,290.20,393.33,10.91;20,112.66,303.75,271.19,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>K√∂pf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Von R√ºtte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Anagnostidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-R</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barhoum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">M</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nagyfi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Es</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Glushkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dantuluri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mattick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.07327</idno>
		<title level="m" coord="20,295.62,290.20,210.36,10.91;20,112.66,303.75,140.95,10.91">OpenAssistant conversations -democratizing large language model alignment</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,317.30,393.33,10.91;20,112.66,330.85,393.33,10.91;20,112.66,344.40,394.52,10.91;20,112.66,357.95,395.01,10.91;20,112.66,371.50,155.44,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="20,223.20,317.30,282.79,10.91;20,112.66,330.85,34.73,10.91">Anserini: Enabling the use of Lucene for information retrieval research</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080721</idno>
		<ptr target="https://doi.org/10.1145/3077136.3080721.doi:10.1145/3077136.3080721" />
	</analytic>
	<monogr>
		<title level="m" coord="20,168.74,330.85,337.24,10.91;20,112.66,344.40,214.72,10.91">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1253" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,385.05,394.62,10.91;20,112.66,398.60,394.53,10.91;20,112.28,412.15,394.91,10.91;20,112.66,425.70,394.51,10.91;20,112.36,441.69,133.46,7.90" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="20,221.21,385.05,265.81,10.91">SimCSE: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.552</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.552.doi:10.18653/v1/2021.emnlp-main.552" />
	</analytic>
	<monogr>
		<title level="m" coord="20,112.66,398.60,394.53,10.91;20,112.28,412.15,242.54,10.91">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, Online and</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, Online and<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6894" to="6910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,452.79,393.33,10.91;20,112.66,466.34,393.33,10.91;20,112.66,479.89,280.09,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="20,223.82,452.79,282.17,10.91;20,112.66,466.34,36.15,10.91">Anserini: enabling the use of Lucene for information retrieval research</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,172.99,466.34,333.00,10.91;20,112.66,479.89,182.66,10.91">Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 40th international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1253" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,493.44,393.33,10.91;20,112.66,506.99,394.53,10.91;20,112.66,520.54,173.79,10.91" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10073</idno>
		<title level="m" coord="20,392.48,493.44,113.51,10.91;20,112.66,506.99,389.98,10.91">Pyserini: An easy-to-use python toolkit to support replicable ir research with sparse and dense representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="20,112.66,534.09,394.53,10.91;20,112.66,547.64,304.60,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="20,254.69,534.09,247.58,10.91">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,112.66,547.64,225.74,10.91">Foundations and Trends¬Æ in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,561.19,393.33,10.91;20,112.66,574.74,393.33,10.91;20,112.33,588.29,349.87,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="20,368.40,561.19,137.58,10.91;20,112.66,574.74,247.50,10.91">BioGPT: generative pre-trained transformer for biomedical text generation and mining</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1093/bib/bbac409</idno>
		<ptr target="https://doi.org/10.1093/bib/bbac409.doi:10.1093/bib/bbac409" />
	</analytic>
	<monogr>
		<title level="j" coord="20,369.97,574.74,122.71,10.91">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,601.84,393.33,10.91;20,112.66,615.39,393.33,10.91;20,112.66,628.93,199.65,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="20,273.96,601.84,232.02,10.91;20,112.66,615.39,132.82,10.91">PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,269.21,615.39,236.78,10.91;20,112.66,628.93,169.27,10.91">Proceedings of the 37th International Conference on Machine Learning, ICML&apos;20, JMLR.org</title>
		<meeting>the 37th International Conference on Machine Learning, ICML&apos;20, JMLR.org</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,642.48,393.32,10.91;20,112.66,656.03,393.33,10.91;20,112.66,669.58,395.01,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="20,299.93,642.48,206.05,10.91;20,112.66,656.03,169.38,10.91">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buettcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,307.48,656.03,198.51,10.91;20,112.66,669.58,310.26,10.91">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,86.97,393.33,10.91;21,112.66,100.52,393.33,10.91;21,112.66,114.06,394.53,10.91;21,112.28,127.61,395.00,10.91;21,112.31,141.16,312.30,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="21,305.76,86.97,200.23,10.91;21,112.66,100.52,169.31,10.91">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buettcher</surname></persName>
		</author>
		<idno type="DOI">10.1145/1571941.1572114</idno>
		<ptr target="https://doi.org/10.1145/1571941.1572114.doi:10.1145/1571941.1572114" />
	</analytic>
	<monogr>
		<title level="m" coord="21,307.29,100.52,198.70,10.91;21,112.66,114.06,390.58,10.91">Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09</title>
		<meeting>the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,154.71,394.53,10.91;21,112.66,168.26,393.32,10.91;21,112.66,181.81,266.90,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="21,274.48,168.26,169.72,10.91">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,452.29,168.26,53.69,10.91;21,112.66,181.81,172.82,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,195.36,394.53,10.91;21,112.66,208.91,393.33,10.91;21,112.48,222.46,264.75,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="21,112.66,208.91,363.77,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,488.38,208.91,17.60,10.91;21,112.48,222.46,170.67,10.91">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,236.01,395.17,10.91;21,112.66,249.56,394.53,10.91;21,112.66,263.11,321.31,10.91" xml:id="b35">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<title level="m" coord="21,145.01,249.56,362.18,10.91;21,112.66,263.11,138.52,10.91">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="21,112.66,276.66,393.33,10.91;21,112.66,290.20,322.16,10.91" xml:id="b36">
	<monogr>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08836</idno>
		<title level="m" coord="21,320.98,276.66,185.01,10.91;21,112.66,290.20,139.86,10.91">Sample efficient text summarization using a single pre-trained transformer</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="21,112.66,303.75,393.33,10.91;21,112.26,317.30,290.75,10.91" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07129</idno>
		<title level="m" coord="21,322.93,303.75,183.06,10.91;21,112.26,317.30,109.13,10.91">ERNIE: Enhanced language representation with informative entities</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
