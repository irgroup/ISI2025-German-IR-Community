<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,380.70,15.42;1,89.29,106.66,310.81,15.42;1,89.29,129.00,212.38,11.96">Improving Biomedical Question Answering with Sentence-based Ranking at BioASQ-11b Notebook for the BioASQ Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,154.90,77.98,11.96"><forename type="first">Anna</forename><surname>Aksenova</surname></persName>
							<email>anna.aksenova@ontotext.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ontotext</orgName>
								<address>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,179.50,154.90,80.73,11.96"><forename type="first">Tsvetan</forename><surname>Asamov</surname></persName>
							<email>tsvetan.asamov@ontotext.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ontotext</orgName>
								<address>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.88,154.90,61.81,11.96"><forename type="first">Petar</forename><surname>Ivanov</surname></persName>
							<email>petar.ivanov@ontotext.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ontotext</orgName>
								<address>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,365.68,154.90,83.49,11.96"><forename type="first">Svetla</forename><surname>Boytcheva</surname></persName>
							<email>svetla.boytcheva@ontotext.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ontotext</orgName>
								<address>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,380.70,15.42;1,89.29,106.66,310.81,15.42;1,89.29,129.00,212.38,11.96">Improving Biomedical Question Answering with Sentence-based Ranking at BioASQ-11b Notebook for the BioASQ Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">BEBB1865A23E4BBF8071C68D059946A5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ELECTRA</term>
					<term>Extractive Question Answering</term>
					<term>Sentence BERT</term>
					<term>Information Retrieval</term>
					<term>Biomedical NLP</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper presents a solution of BioASQ 2023 11b question answering task (part of the Conference and Labs of the Evaluation Forum -CLEF). Our team participated in Phase B, submitting the system for factoid and yesno types of questions in English based on extractive question answering and text classification respectively. In this work, we outline our Question Answering (QA) approach based on sentence embedding ranking coupled with biomedical ELECTRA model [1] fine-tuning. The approach showed the third-best accuracy score for yesno (0.8571) questions and the fourth-best accuracy score for factoid questions (0.5161) on the final test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper describes a pipeline proposed by our team for solving BioASQ-11b <ref type="bibr" coords="1,434.13,388.55,12.77,10.91" target="#b1">[2]</ref> Phase B task. We propose a simple yet efficient approach to biomedical question answering that may further be scaled and used for industrial purposes.</p><p>The BioASQ 2023 task 11b phase B consisted of several subtypes of questions in English to be resolved, namely factoid, yesno or list questions. For each of the types, the participants were encouraged to provide not only an "exact answer" (span of text, binary value and list of entities respectively), but also a comprehensive "ideal answer", which would answer the question in a natural way. Our team focused on retrieving answers for factoid and yesno questions also providing ideal answers corresponding to those tasks.</p><p>In this work, we present our approach which is based on 2 steps: sentence embedding cosine similarity ranking based on biomedical sentence BERT <ref type="bibr" coords="1,336.83,524.04,12.90,10.91" target="#b2">[3]</ref> and a fine-tuning BioM-ELECTRA model <ref type="bibr" coords="1,119.49,537.59,12.84,10.91" target="#b0">[1]</ref> on text and token classification.</p><p>The paper is organized as follows: in Section 2 we present a brief outline of the previous research on question answering task; Section 3 introduces task description; Section 6 describes our approach and experimental set up; in Section 4 we overview the challenge dataset and additional sources used for model fine-tuning; in Section 5 the submitted system is explained, Section 7 discusses our main findings; finally, Section 8 draws some conclusions and outlooks for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Early research in extractive QA focused on knowledge-based approaches that relied on structured data sources, such as biomedical ontologies and databases. Systems like the first BioASQ solutions <ref type="bibr" coords="2,131.86,199.79,12.71,10.91" target="#b3">[4]</ref> and AskHERMES <ref type="bibr" coords="2,226.23,199.79,12.72,10.91" target="#b4">[5]</ref> employed a combination of information retrieval techniques and named entity recognition to extract relevant information from curated resources. These approaches provided accurate answers by leveraging domain-specific knowledge, but their performance was limited by the availability and coverage of structured resources.</p><p>To overcome the limitations of knowledge-based approaches, researchers explored corpusbased approaches that utilized large-scale text corpora. Extractive QA in this setting has been solved quite well for the common domain after the introduction of the SQuAD dataset <ref type="bibr" coords="2,470.68,281.08,11.32,10.91" target="#b5">[6]</ref>. The top-ranked solutions rely on transformer-based models such as BERT <ref type="bibr" coords="2,396.36,294.63,12.71,10.91" target="#b6">[7]</ref> and XLNet <ref type="bibr" coords="2,461.80,294.63,11.30,10.91" target="#b7">[8]</ref>. These approaches leveraged the abundance of textual data, enabling broader coverage and adaptability to different question types.</p><p>As for the biomedical domain, the number of pre-trained language models of different architectures is quite limited. Biomedical transformer models follow the core architecture of the original transformer model, consisting of multi-head self-attention mechanisms and feedforward neural networks. However, several model variants have been introduced to enhance their performance in the biomedical domain. For instance, models like BioBERT <ref type="bibr" coords="2,449.77,389.48,12.89,10.91" target="#b8">[9]</ref> and SciB-ERT <ref type="bibr" coords="2,111.71,403.03,18.07,10.91" target="#b9">[10]</ref> are transformer models pre-trained on biomedical text, providing domain-specific embeddings. Other variants include BlueBERT <ref type="bibr" coords="2,296.65,416.58,16.14,10.91" target="#b10">[11]</ref>, ClinicalBERT <ref type="bibr" coords="2,381.17,416.58,16.15,10.91" target="#b11">[12]</ref>, and PubMedBERT <ref type="bibr" coords="2,487.01,416.58,16.14,10.91" target="#b12">[13]</ref>, which cater to specific subdomains or incorporate additional contextual information. These models provide a robust foundation for processing biomedical text and extracting valuable information from vast amounts of biomedical literature. According to the recent results, ELECTRA model <ref type="bibr" coords="2,119.78,470.77,18.00,10.91" target="#b13">[14]</ref> pre-trained on PubMed and fine-tuned on SQuAD showed state-of-the-art results on BioASQ-7 for base-scale models <ref type="bibr" coords="2,248.14,484.32,16.28,10.91" target="#b14">[15]</ref>, therefore we focused on ELECTRA-based models for tackling BioASQ-11 challenge.</p><p>Providing a comprehensive and elaborated answer to a question could be approached as a natural language generation task. Leveraging the power of GPT-based architectures and pre-training techniques, BioGPT <ref type="bibr" coords="2,235.06,538.52,17.82,10.91" target="#b15">[16]</ref> has been developed to be applied for language modelling in the specific domain. BioGPT benefits from its pre-training on biomedical literature, which equips it with a strong foundation of domain-specific knowledge. It can understand and handle the technical terminology, abbreviations, and concepts prevalent in the biomedical field. This specialized knowledge allows BioGPT to effectively tackle complex biomedical questions that may require a deep understanding of the domain, enabling it to provide accurate and informative answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task formulation</head><p>For each of the subtasks that we tackled, each data point consisted of a number abstracts extracted from PubMed scientific medical publications in English<ref type="foot" coords="3,377.97,123.07,3.71,7.97" target="#foot_0">1</ref> and a question. The answer was supposed to be inferred from one or more of the given paragraphs.</p><p>• Factoid question answering Given a set of text paragraphs and an open question, return a short span of text containing the entity. Usually, it is a symptom, disease or numerical value. • Yesno question answering Given a set of text paragraphs and a general question, return either "yes" or "no".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Ideal answers formulation</head><p>Given a set of text paragraphs and a general question, return a sentence in a natural language that will answer the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data</head><p>The BioASQ dataset <ref type="bibr" coords="3,178.37,320.06,17.76,10.91" target="#b16">[17]</ref> was manually annotated for Question Answering (QA) task by medical experts. Originally the training set for factoid questions consisted of 1417 samples and the training set for yesno questions consisted of 1271 samples.</p><p>For the purposes of building different experimental systems, we have introduced several adjustments for those sets.</p><p>1. For internal evaluation purposes, original BioASQ-11b training datasets were split into train and development subsets, which comprised 80% and 20% of the full challenge data respectively. 2. For the factoid question set we checked whether the exact answer (or its non-capitalized version) was present in any of the reference abstracts. If so, we kept the question and extracted the position of the answer in the text, otherwise, we omitted the sample. <ref type="foot" coords="3,479.52,464.92,3.71,7.97" target="#foot_1">2</ref> As a result, we obtained a dataset in standard SQuAD format <ref type="bibr" coords="3,368.25,480.22,11.43,10.91" target="#b5">[6]</ref>. 3. For the factoid question training set we also employed additional training data based on BioASQ 7b. The dataset was transformed to SQuAD format and introduced by Jeong et al. <ref type="bibr" coords="3,116.56,522.22,16.27,10.91" target="#b17">[18]</ref>. The original dataset comprised 3231 questions. Duplicate questions were omitted. This particular dataset was used as, to the extent of our knowledge, it is the only open source biomedical QA dataset that has required SQuAD formatting. 4. For the yesno QA task we experimented on adopting PubMedQA <ref type="bibr" coords="3,417.46,564.23,18.07,10.91" target="#b18">[19]</ref> data to enlarge the training set. It included 1000 questions.</p><p>As a result we obtained several train sets. The data distribution is presented in Table <ref type="table" coords="3,494.86,591.33,3.74,10.91" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Factoid answers</head><p>As it was already mentioned in Section 4, we decided to narrow down the factoid QA task to extractive question answering, i.e. we assume that the exact answer is explicitly present in at least one of the reference paragraphs. In short, the backbone model is fine-tuned for token classification task and predicts whether each token holds answer_start, answer_end or other position. Such approach is unable to predict the answers that are not explicitly present in the context, however such QA systems are much easier to train and control. As it was already discussed in Section 2, ELECTRA-based models proved to be efficient for biomedical QA, therefore we use those as a backbone for our further fine-tuning experiments. Some of the reference paragraphs in the training set are longer than the input sequence length for transformer model that we used. To avoid loosing the information, we split such examples into parts with an overlap of 128 tokens following the approach suggested by Huggingface Transformers tutorial <ref type="bibr" coords="4,184.69,405.30,16.09,10.91" target="#b19">[20]</ref>. In addition, we adapted answer_start and answer_end candidate ranking procedure suggested by the same resource.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Yesno answers</head><p>The binary type of questions was tackled as a binary classification task with the same backbone transformer model. Question and context separated by <ref type="bibr" coords="4,366.90,484.57,29.67,7.90">[SEP]</ref> token were fed into the transformer with a linear layer on top of the pooled output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ideal answers</head><p>We focused on fine-tuning open-source BioGPT model <ref type="bibr" coords="4,334.86,545.40,17.91,10.91" target="#b15">[16]</ref> for generating ideal answers.</p><p>We have performed BioGPT fine-tuning in prefix-tuning setting by introducing additional tokens [QUESTION], [CONTEXT], and [ANSWER], which should prompt the model to generate answers after the input questions and contexts. The schema of the training input is presented below.</p><p>[ </p><formula xml:id="formula_0" coords="4,106.13,614.16,397.94,9.72">BOS] [QUESTION] Question text [CONTEXT] Context text [ANSWER] Answer text [EOS]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ranking</head><p>Typical end-to-end question answering pipelines include document retrieval step before the information extraction itself. We adopted this idea to the competition setting.</p><p>As the majority of answers in BioASQ can be given based on a single sentence from all of the given PubMed paragraphs, we decided to introduce sentence ranking step before QA.</p><p>Basic information retrieval pipeline consists of three steps: calculating document and query vectors, calculating similarity between query and documents, sorting the documents based on their similarity to the query. As we use sentences as items for ranking, for preprocessing sentence tokenization step we used SciSpacy small scientific model <ref type="bibr" coords="5,400.63,476.59,18.07,10.91" target="#b20">[21]</ref> as both rule-based and NLTK-based sentence splitting <ref type="bibr" coords="5,252.16,490.14,18.07,10.91" target="#b21">[22]</ref> were not able to preserve entities that the final QA system was supposed to extract. For instance, some abbreviations and measures with dots were identified as sentence borders. For embedding extraction we used sentence transformers <ref type="bibr" coords="5,488.01,517.24,17.98,10.91" target="#b22">[23]</ref> which are widely used as a basic architecture for ranking tasks. As such models are trained to increase cosine similarity between semantically close sentences, we sort all the sentences in reference passages by cosine similarity between sentence and question vector. Circa 96% of the answers were located in top-5 ranked sentences, therefore after ranking we reduce the contexts to top-5 sentences. Apparently, not only does this procedure reduce required training resources, but also improves the accuracy of the whole pipeline. Figure <ref type="figure" coords="5,357.15,598.54,5.00,10.91" target="#fig_0">1</ref> presents all steps in the pipeline of the best submitted system for factoid questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Backbone models</head><p>Four publicly available biomedical transformer models were used in our experiments, namely BioGPT <ref type="bibr" coords="6,126.77,121.08,16.25,10.91" target="#b15">[16]</ref>, BioM-ELECTRA <ref type="bibr" coords="6,225.71,121.08,16.25,10.91" target="#b14">[15]</ref>, ELECTRAMed <ref type="bibr" coords="6,316.56,121.08,16.25,10.91" target="#b23">[24]</ref>, S-PubMedBERT <ref type="bibr" coords="6,413.41,121.08,11.43,10.91" target="#b2">[3]</ref>.</p><p>• BioGPT<ref type="foot" coords="6,149.72,142.08,3.71,7.97" target="#foot_3">3</ref> : GPT-2-based model pre-trained on PubMed abstracts with custom-built vocabulary. • BioM-ELECTRA<ref type="foot" coords="6,186.33,169.87,3.71,7.97" target="#foot_4">4</ref> : pre-trained on PubMed Abstracts with PubMedBERT vocabulary <ref type="bibr" coords="6,488.05,171.63,17.94,10.91" target="#b24">[25]</ref> and fine-tuned on SQuAD 2.0. • ELECTRAMed<ref type="foot" coords="6,178.88,197.67,3.71,7.97" target="#foot_5">5</ref> : pre-trained on PubMed Abstracts with SciVocab vocabulary <ref type="bibr" coords="6,457.77,199.42,16.25,10.91" target="#b9">[10]</ref>.</p><p>• S-PubMedBERT<ref type="foot" coords="6,186.39,211.91,3.71,7.97" target="#foot_6">6</ref> : initialised as PubMedBERT and fine-tuned on MS-MARCO dataset <ref type="bibr" coords="6,488.22,213.66,17.76,10.91" target="#b25">[26]</ref> using sentence-transformers framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>The performance of our methods is reported on development set that we described in Section 4. Evaluation for all the tested systems is done with the target metrics of BioASQ challenge, i.e. accuracy for yesno questions, strict accuracy or exact match for factoid questions and ROUGE <ref type="bibr" coords="6,126.84,326.12,17.91,10.91" target="#b26">[27]</ref> for ideal answers. All the pipelines were implemented using HuggingFace Transformers (Extractive QA, Text Classification and Language Modeling tutorials).</p><p>Table <ref type="table" coords="6,127.63,366.76,5.17,10.91" target="#tab_1">2</ref> and Table <ref type="table" coords="6,182.63,366.76,5.17,10.91" target="#tab_2">3</ref> report our experimental results. Initially, for obtaining exact answers, we fine-tuned both ELECTRA models without any updates on the data to establish a solid baseline. Then we experimented with adding more samples to the training sets for both types of questions. In particular, 2753 questions were added to factoid dataset and 1000 questions were added to yesno dataset. The models fine-tuned on the updated datasets are marked with "+" sign. Given that additional data improved the performance of extractive QA system, but did not help in terms of yesno QA, we took the best datasets for further experiments with ranking.</p><p>All the experiments were conducted on a single NVIDIA RTX A5000 GPU. For fine-tuning the models we used the following hyper parameter settings:</p><p>• learning rate: Initialized to 1e-5 and 5e-5. The latter is suggested as the best rate in the original BioM-ELECTRA paper by Alrowili and Vijay-Shanker <ref type="bibr" coords="6,433.26,511.46,11.47,10.91" target="#b0">[1]</ref>. The smaller learning rate appeared to be more beneficial in our case. • number of epochs: We tested 3, 5 and 10 epochs for each of the settings. The best performance was achieved on 5 epochs, therefore we report these results. • batch size: It was set to 16 due to the limitations of GPU memory.</p><p>As we were mostly focusing on exact answers, the number of experiments performed on ideal answer generation was limited. They are presented in Table <ref type="table" coords="6,379.95,603.73,3.70,10.91" target="#tab_3">4</ref>. The model was fine-tuned on language modeling task with the following hyper parameters: • learning rate: Initialized to 1e-5 as a default suggested for the fine-tuning.</p><p>• number of epochs: We tested 3 and 5 epochs for each of the settings. As the models after 5 epochs fine-tuning provided better performance, we report the metrics for those. • batch size: It was set to 4 due to the limitations of GPU memory.</p><p>• generation temperature: Initialized to 0.7 as a default parameter. We wanted our model to have some variability in generation, however we did not aim to allow it to generate very creative responses. • maximum prediction length: Initialized to 100 tokens as an approximation of ideal answers in the training set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>In terms of obtaining exact answers, re-ranking the context sentences improved the performance of both models for both types of questions. Between the two ELECTRA models, BioM-ELECTRA-SQuAD showed slightly better results.</p><p>As for the ideal answers, limiting the context with sentence-ranking technique did not improve the target metric. Further research should be conducted in that direction.</p><p>The proposed system for exact answers presented second-best accuracy score on Test batch 3 leaderboard and third-best accuracy score on Test batch 4. The difference in scores on our development set and on the leaderboard is caused by a set of answers that is not explicitly present in any of the reference PubMed abstracts as the system cannot predict those by design. Although our system is inferior of top-1 GPT-3.5-based pipeline for factoid questions due to this limitation, it scores the same as GPT-4-based system for the same set of questions. In addition, our pipeline scores higher in lenient accuracy (top-5 accuracy), meaning that it is more beneficial for medical suggestions systems. Overall, the pipeline is quite flexible and not demanding in terms of computational resources. It can be easily customised for different datasets, domains and languages as soon as there exist relevant backbone transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper we have presented simple yet efficient pipeline for building a question-answering system for biomedical domain based on pre-trained biomedical transformer models. The system showed good results in BioASQ 2023, proving that it could be used for further development and could be adapted to real-world medical question answering tasks.</p><p>As a direction for future development we can suggest focusing more on answer candidate selection step as the top-5 accuracy of our system is 20% higher compared to top-1 accuracy. In addition, we are aiming to scale this approach to multilingual question answering task as multilinguality still remains a challenge for QA systems, especially in biomedical domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,323.58,185.88,8.93;5,89.29,84.19,416.70,226.83"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Proposed pipeline for factoid QA</figDesc><graphic coords="5,89.29,84.19,416.70,226.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,102.63,90.49,321.74,93.78"><head>Table 1</head><label>1</label><figDesc>Data distribution for the obtained train and development sets</figDesc><table coords="4,170.91,122.10,253.46,62.16"><row><cell>Data</cell><cell cols="2">Train examples Dev examples</cell></row><row><cell>Factoid cleaned</cell><cell>892</cell><cell>222</cell></row><row><cell cols="2">Factoid cleaned + BioASQ7 2891</cell><cell>222</cell></row><row><cell>Yesno</cell><cell>889</cell><cell>382</cell></row><row><cell>Yesno + PubMedQA</cell><cell>1889</cell><cell>382</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,90.49,330.96,117.69"><head>Table 2</head><label>2</label><figDesc>Evaluation of the tested approaches for factoid QA on dev set</figDesc><table coords="7,175.32,122.10,244.63,86.07"><row><cell>Pipeline</cell><cell cols="2">Strict accuracy F1</cell></row><row><cell>BioM-ELECTRA-SQuAD</cell><cell>85.3</cell><cell>88.6</cell></row><row><cell>BioM-ELECTRA-SQuAD+</cell><cell>87.0</cell><cell>90.1</cell></row><row><cell cols="2">BioM-ELECTRA-SQuAD+_ranking 88.5</cell><cell>91.9</cell></row><row><cell>ELECTRAMed</cell><cell>83.0</cell><cell>86.4</cell></row><row><cell>ELECTRAMed+</cell><cell>84.9</cell><cell>88.9</cell></row><row><cell>ELECTRAMed+_ranking</cell><cell>86.7</cell><cell>90.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,229.82,301.78,117.69"><head>Table 3</head><label>3</label><figDesc>Evaluation of the tested approaches for yesno QA on dev set</figDesc><table coords="7,204.51,261.44,186.26,86.08"><row><cell>Pipeline</cell><cell>Accuracy</cell></row><row><cell>BioM-ELECTRA-SQuAD</cell><cell>0.94</cell></row><row><cell>BioM-ELECTRA-SQuAD+</cell><cell>0.92</cell></row><row><cell cols="2">BioM-ELECTRA-SQuAD_ranking 0.96</cell></row><row><cell>ELECTRAMed</cell><cell>0.90</cell></row><row><cell>ELECTRAMed+</cell><cell>0.85</cell></row><row><cell>ELECTRAMed_ranking</cell><cell>0.92</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,523.23,265.14,69.87"><head>Table 4</head><label>4</label><figDesc>Evaluation of BioGPT for ideal answer generation</figDesc><table coords="7,241.14,554.85,113.00,38.25"><row><cell>Model</cell><cell>ROUGE</cell></row><row><cell>BioGPT</cell><cell>0.42</cell></row><row><cell cols="2">BioGPT_ranking 0.40</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,92.57,659.97,122.23,8.97"><p>https://pubmed.ncbi.nlm.nih.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,92.57,670.93,19.42,8.97"><p>Circa</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2" coords="3,114.23,670.93,256.04,8.97"><p>30% of the datapoints did not contain exact answer that was expected.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="6,92.57,638.16,145.73,8.97"><p>https://huggingface.co/microsoft/biogpt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4" coords="6,92.57,649.12,230.16,8.97"><p>https://huggingface.co/sultan/BioM-ELECTRA-Large-SQuAD2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5" coords="6,92.57,660.08,251.63,8.97"><p>https://huggingface.co/giacomomiolo/electramed_base_scivocab_1M</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6" coords="6,92.57,671.04,232.82,8.97"><p>https://huggingface.co/pritamdeka/S-PubMedBert-MS-MARCO</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,461.65,393.33,10.91;8,112.66,475.20,393.33,10.91;8,112.66,488.75,179.48,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,252.53,461.65,253.46,10.91;8,112.66,475.20,154.27,10.91">Biom-transformers: building large biomedical language models with bert, albert and electra</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alrowili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,289.27,475.20,216.71,10.91;8,112.66,488.75,91.61,10.91">Proceedings of the 20th Workshop on Biomedical Language Processing</title>
		<meeting>the 20th Workshop on Biomedical Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="221" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,502.30,394.53,10.91;8,112.66,515.85,393.33,10.91;8,112.66,529.40,393.33,10.91;8,112.66,542.95,394.04,10.91;8,112.66,556.50,173.99,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,112.66,515.85,393.33,10.91;8,112.66,529.40,146.07,10.91">The eleventh edition of the large-scale biomedical semantic indexing and question answering challenge</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Farré-Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lima-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-28241-6_66</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-3-031-28241-6_66" />
	</analytic>
	<monogr>
		<title level="m" coord="8,281.01,529.40,148.17,10.91">Advances in Information Retrieval</title>
		<meeting><address><addrLine>Switzerland, Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Bioasq at clef</note>
</biblStruct>

<biblStruct coords="8,112.66,570.05,393.33,10.91;8,112.66,583.60,393.98,10.91;8,112.41,597.15,38.81,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,327.02,570.05,178.97,10.91;8,112.66,583.60,227.03,10.91">Improved methods to aid unsupervised evidence-based fact checking for online health news</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Deka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jurek-Loughrey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Deepak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,348.64,583.60,121.13,10.91">Journal of Data Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="474" to="504" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,610.69,393.32,10.91;8,112.66,624.24,305.86,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,414.78,610.69,91.20,10.91;8,112.66,624.24,207.75,10.91">Results of the bioasq tasks of the question answering lab at clef 2015</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kosmopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,343.51,624.24,45.05,10.91">CLEF 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,637.79,393.33,10.91;8,112.66,651.34,393.33,10.91;8,112.66,664.89,134.67,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,438.34,637.79,67.65,10.91;8,112.66,651.34,288.51,10.91">Askhermes: An online question answering system for complex clinical questions</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antieau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Cimino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ely</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,409.62,651.34,96.37,10.91;8,112.66,664.89,50.74,10.91">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="277" to="288" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,86.97,393.33,10.91;9,112.66,100.52,280.07,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<title level="m" coord="9,324.88,86.97,181.11,10.91;9,112.66,100.52,98.46,10.91">Squad: 100,000+ questions for machine comprehension of text</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,114.06,393.33,10.91;9,112.66,127.61,311.37,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="9,326.58,114.06,179.40,10.91;9,112.66,127.61,181.08,10.91">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,141.16,393.33,10.91;9,112.66,154.71,371.43,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m" coord="9,419.10,141.16,86.88,10.91;9,112.66,154.71,241.15,10.91">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,168.26,393.33,10.91;9,112.66,181.81,393.98,10.91;9,112.41,195.36,48.96,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,361.64,168.26,144.35,10.91;9,112.66,181.81,268.25,10.91">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,394.29,181.81,66.92,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,208.91,393.60,10.91;9,112.66,222.46,146.44,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676</idno>
		<title level="m" coord="9,234.39,208.91,239.83,10.91">Scibert: A pretrained language model for scientific text</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,236.01,393.33,10.91;9,112.66,249.56,387.24,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="9,212.93,236.01,293.06,10.91;9,112.66,249.56,257.50,10.91">Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05474</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,263.11,393.33,10.91;9,112.66,276.66,272.20,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05342</idno>
		<title level="m" coord="9,276.78,263.11,229.21,10.91;9,112.66,276.66,89.80,10.91">Clinicalbert: Modeling clinical notes and predicting hospital readmission</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,290.20,394.53,10.91;9,112.66,303.75,394.53,10.91;9,112.28,317.30,332.27,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,112.66,303.75,390.01,10.91">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,112.28,317.30,268.63,10.91">ACM Transactions on Computing for Healthcare (HEALTH)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,330.85,393.33,10.91;9,112.66,344.40,347.38,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m" coord="9,335.14,330.85,170.85,10.91;9,112.66,344.40,165.13,10.91">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,357.95,393.32,10.91;9,112.26,371.50,393.73,10.91;9,112.66,385.05,395.01,10.91;9,112.66,398.60,395.00,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,219.86,357.95,286.12,10.91;9,112.26,371.50,153.47,10.91">BioM-transformers: Building large biomedical language models with BERT, ALBERT and ELECTRA</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alrowili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Shanker</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.bionlp-1.24</idno>
		<ptr target="https://aclanthology.org/2021.bionlp-1.24.doi:10.18653/v1/2021.bionlp-1.24" />
	</analytic>
	<monogr>
		<title level="m" coord="9,289.99,371.50,216.00,10.91;9,112.66,385.05,276.72,10.91">Proceedings of the 20th Workshop on Biomedical Language Processing, Association for Computational Linguistics</title>
		<meeting>the 20th Workshop on Biomedical Language Processing, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="221" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,412.15,393.33,10.91;9,112.66,425.70,395.17,10.91;9,112.66,439.25,395.00,10.91;9,112.66,455.24,477.12,7.90;9,112.66,466.34,38.03,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,368.40,412.15,137.58,10.91;9,112.66,425.70,259.37,10.91">BioGPT: generative pre-trained transformer for biomedical text generation and mining</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1093/bib/bbac409</idno>
		<ptr target="https://academic.oup.com/bib/article-pdf/23/6/bbac409/47144271/bbac409.pdf,bbac409" />
	</analytic>
	<monogr>
		<title level="j" coord="9,389.52,425.70,118.32,10.91;9,112.66,439.25,12.12,10.91">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,479.89,393.33,10.91;9,112.66,493.44,394.62,10.91;9,112.31,506.99,168.55,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,363.95,479.89,142.04,10.91;9,112.66,493.44,192.05,10.91">Bioasq-qa: A manually curated corpus for biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-023-02068-4</idno>
		<ptr target="https://doi.org/10.1038/s41597-023-02068-4" />
	</analytic>
	<monogr>
		<title level="j" coord="9,317.67,493.44,66.96,10.91">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">170</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,520.54,393.33,10.91;9,112.66,534.09,393.57,10.91;9,112.33,547.64,29.19,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="9,390.94,520.54,115.05,10.91;9,112.66,534.09,239.70,10.91">Transferability of natural language inference to biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00217</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,561.19,393.33,10.91;9,112.66,574.74,393.33,10.91;9,112.66,588.29,393.33,10.91;9,112.66,601.84,270.14,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,305.73,561.19,200.26,10.91;9,112.66,574.74,87.85,10.91">Pubmedqa: A dataset for biomedical research question answering</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,231.73,574.74,274.25,10.91;9,112.66,588.29,393.33,10.91;9,112.66,601.84,171.35,10.91">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2567" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,615.39,394.53,10.91;9,112.66,628.93,394.53,10.91;9,112.66,642.48,395.17,10.91;9,112.66,656.03,393.33,10.91;9,112.66,669.58,393.33,10.91;10,112.66,86.97,395.00,10.91;10,112.66,100.52,197.48,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,311.38,642.48,196.45,10.91;9,112.66,656.03,77.28,10.91">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-demos.6.doi:10.18653/v1/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m" coord="9,219.91,656.03,286.07,10.91;9,112.66,669.58,393.33,10.91;10,112.66,86.97,47.14,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,114.06,393.54,10.91;10,112.66,127.61,393.33,10.91;10,112.66,141.16,394.53,10.91;10,112.66,154.71,395.00,10.91;10,112.66,170.70,132.96,7.90" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,329.93,114.06,176.27,10.91;10,112.66,127.61,180.09,10.91">ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5034</idno>
		<idno type="arXiv">arXiv:1902.07669</idno>
		<ptr target="https://www.aclweb.org/anthology/W19-5034.doi:10.18653/v1/W19-5034" />
	</analytic>
	<monogr>
		<title level="m" coord="10,315.83,127.61,190.15,10.91;10,112.66,141.16,259.53,10.91">Proceedings of the 18th BioNLP Workshop and Shared Task, Association for Computational Linguistics</title>
		<meeting>the 18th BioNLP Workshop and Shared Task, Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="319" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,181.81,393.33,10.91;10,112.66,195.36,253.14,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="10,228.64,181.81,277.34,10.91;10,112.66,195.36,122.59,10.91">Natural language processing with Python: analyzing text with the natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,208.91,394.53,10.91;10,112.66,222.46,122.77,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m" coord="10,219.42,208.91,283.17,10.91">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,236.01,393.33,10.91;10,112.66,249.56,239.25,10.91" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="10,266.73,236.01,239.26,10.91;10,112.66,249.56,109.56,10.91">Electramed: a new pre-trained language representation model for biomedical nlp</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mantoan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Orsenigo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09585</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,263.11,394.53,10.91;10,112.66,276.66,394.53,10.91;10,112.28,290.20,395.39,10.91;10,112.41,303.75,186.21,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="10,112.66,276.66,390.01,10.91">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3458754</idno>
		<ptr target="https://doi.org/10.1145%2F3458754.doi:10.1145/3458754" />
	</analytic>
	<monogr>
		<title level="j" coord="10,112.28,290.20,221.53,10.91">ACM Transactions on Computing for Healthcare</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,317.30,394.53,10.91;10,112.66,330.85,393.71,10.91;10,112.66,344.40,388.45,10.91" xml:id="b25">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<title level="m" coord="10,448.57,330.85,57.79,10.91;10,112.66,344.40,258.71,10.91">Ms marco: A human generated machine reading comprehension dataset</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,357.95,395.17,10.91;10,112.66,371.50,395.01,10.91;10,112.41,385.05,262.56,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="10,156.31,357.95,253.99,10.91">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W04-1013" />
	</analytic>
	<monogr>
		<title level="m" coord="10,433.56,357.95,74.27,10.91;10,112.66,371.50,270.67,10.91">Text Summarization Branches Out, Association for Computational Linguistics</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
