<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.71,85.05,277.77,15.39;1,88.78,106.97,344.12,15.39;1,89.29,128.89,315.71,15.39;1,89.29,150.80,308.00,15.39">VICOMTECH at MedProcNER 2023: Transformers-based Sequence-labelling and Cross-encoding for Entity Detection and Normalisation in Spanish Clinical Texts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,181.26,64.46,10.68"><forename type="first">Elena</forename><surname>Zotova</surname></persName>
							<email>ezotova@vicomtech.org</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SNLT group at Vicomtech Foundation</orgName>
								<orgName type="department" key="dep2">Basque Research and Technology Alliance (BRTA)</orgName>
								<address>
									<addrLine>Mikeletegi Pasealekua 57, Donostia/San-Sebastián</addrLine>
									<postCode>20009</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Languages and Computer Systems</orgName>
								<orgName type="department" key="dep2">Basque Country (UPV-EHU)</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<addrLine>Paseo Manuel de Lardizabal, 1, Donostia/San-Sebastián</addrLine>
									<postCode>20018</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,178.14,181.26,96.32,10.68"><forename type="first">Aitor</forename><surname>García-Pablos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SNLT group at Vicomtech Foundation</orgName>
								<orgName type="department" key="dep2">Basque Research and Technology Alliance (BRTA)</orgName>
								<address>
									<addrLine>Mikeletegi Pasealekua 57, Donostia/San-Sebastián</addrLine>
									<postCode>20009</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.56,181.26,79.93,10.68"><forename type="first">Montse</forename><surname>Cuadros</surname></persName>
							<email>mcuadros@vicomtech.org</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SNLT group at Vicomtech Foundation</orgName>
								<orgName type="department" key="dep2">Basque Research and Technology Alliance (BRTA)</orgName>
								<address>
									<addrLine>Mikeletegi Pasealekua 57, Donostia/San-Sebastián</addrLine>
									<postCode>20009</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,410.95,181.26,70.40,10.68"><forename type="first">German</forename><surname>Rigau</surname></persName>
							<email>german.rigau@ehu.eus</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Languages and Computer Systems</orgName>
								<orgName type="department" key="dep2">Basque Country (UPV-EHU)</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<addrLine>Paseo Manuel de Lardizabal, 1, Donostia/San-Sebastián</addrLine>
									<postCode>20018</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">HiTZ Basque Center for Language Technologies</orgName>
								<address>
									<addrLine>Paseo Manuel de Lardizabal, 1, Donostia/San-Sebastián</addrLine>
									<postCode>20018</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.71,85.05,277.77,15.39;1,88.78,106.97,344.12,15.39;1,89.29,128.89,315.71,15.39;1,89.29,150.80,308.00,15.39">VICOMTECH at MedProcNER 2023: Transformers-based Sequence-labelling and Cross-encoding for Entity Detection and Normalisation in Spanish Clinical Texts</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">9720C036D9D17E24667891B0ED60CBF2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Named Entity Recognition</term>
					<term>Entity Linking</term>
					<term>Entity Normalisation</term>
					<term>Clinical Coding</term>
					<term>Document Indexing</term>
					<term>SNOMED CT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the Vicomtech NLP team in the MedProcNER 2023 shared task about detecting mentions of procedures in clinical texts written in Spanish and normalising them to SNOMED CT codes. We participate in each of the three tasks, combining multiple approaches and strategies. For Task 1 (NER) we use a Transformer-based model to perform sequence labelling. For Task 2 (Normalisation) we use Semantic Text Search approaches to relate entity mentions to their codes. The solution for Task 3 (Indexing) is built on top of the two first tasks. For Task 1 our system obtained 77.96% of F1-score. Our approaches for Task 2 and Task 3 achieved the highest F1 scores in the official evaluation results-57.07% and 62.42%, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper describes Vicomtech's participation in MedProcNER 2023 shared task <ref type="bibr" coords="1,459.52,482.52,11.59,9.74" target="#b0">[1]</ref>, which is part of the BioASQ Workshop in the CLEF 2023 conference <ref type="bibr" coords="1,372.02,496.07,11.59,9.74" target="#b1">[2]</ref>. This challenge is focused on the detection, normalisation and indexing of clinical procedures in clinical documents in Spanish. It is split in three tasks:</p><p>• Task 1. Clinical Procedure Recognition. In this subtask, participants are challenged to automatically detect mentions of clinical procedures in clinical reports in Spanish. Using the MedProcNER corpus as training data, they must build systems capable of retrieving the start and end position of clinical procedures mentioned in the text. • Task 2. Clinical Procedure Normalisation. The challenge of this subtask is to automatically normalise mentions of clinical procedures in published clinical reports in Spanish. The proposed systems should assign SNOMED CT codes to the mentions retrieved in Task 1. • Task 3. Clinical Procedure-based Document Indexing. The subtask aims to automatically assign clinical procedure codes to the full clinical case report texts. Using the MedProcNER corpus as training data, participants must create systems that can assign SNOMED CT codes to the full case report so that they can be indexed.</p><p>We refer the reader to the overview article <ref type="bibr" coords="2,298.53,224.70,13.00,9.74" target="#b0">[1]</ref> and the competition's official website <ref type="foot" coords="2,485.38,222.02,4.06,7.79" target="#foot_0">1</ref> for detailed information about MedProcNER 2023. Vicomtech's NLP team has implemented several tools to address the different stages of the task incrementally: entity mention detection has been addressed with a Transformer-based sequence labelling system; the entity normalisation has been tackled with Semantic Text Similarity (STS) techniques; finally, both methods were applied to resolve the clinical indexing problem.</p><p>This paper is organised as follows. Section 2 describes the annotated corpus provided by the organisers, the custom train and validation split used for the experiments, the SNOMED CT gazetteer database and the method for the terminology enrichment. Section 3 presents the systems implemented to tackle Task 1. Section 4 describes the approach for Task 2 (normalisation). Section 5 briefly explains how we treated the task of clinical indexing. Section 6 shows the challenge's official results and discusses the presented systems' performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data Description</head><p>The official dataset consists of 1,000 manually annotated text clinical reports in Spanish from which 750 documents are prepared for training purposes and 250 documents are reserved for the participants' systems' testing. Clinical case reports are a type of textual genre in medicine that describe a patient's medical history, symptoms, diagnosis, and treatment in detail. Task 1 annotations consist of 11,065 spans of entity mentions with their corresponding label PROCEDIMIENTO (procedure), Task 2 train set is annotated with 4,857 SNOMED CT codes, and Task 3 contains 250 documents, where each document is annotated with a set of SNOMED CT codes.</p><p>The majority of the entities for the normalisation task are single code annotations; nevertheless, about 2.6% (125) of all spans are annotated with a composite code, formed with two or more SNOMED CT IDs, concatenated with a "+" sign. We also check if all annotated codes correspond to the procedure semantic tag in SNOMED CT gazetteer. 4,602 codes from the train set (94.75%) have procedure tag; there are also 255 items with the other tags; for instance, 101 codes are labelled as regime/therapy in SNOMED CT, 51 code is marked as CODE_NOT_IN_DICT, 25 codes have the tag physical object, 24 codes are tagged as product, 12 codes are tagged as substance, etc.</p><p>To perform the experiments, we randomly split the documents into training (90%) and validation (10%) sets. We can see the details of the split in Table <ref type="table" coords="2,375.16,649.15,3.74,9.74" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">SNOMED CT Dictionary Enrichment</head><p>The shared task organisers provided a TSV file containing the SNOMED CT codes, their definitions, and the training and validation data. The SNOMED CT taxonomy contains 242,228 entries of 130,219 unique concepts, which means that some of the concepts have various synonyms (up to 32 entries with the same code). These codes must be assigned to each entity in the input texts as part of Task 2. As a data pre-processing step, we have extended the provided dictionary entries using the manually labelled terms from the training set of our train-validation split. This adds 2,697 unique terms and synonyms, so the final number of entries in the SNOMED CT taxonomy becomes 244,924. If a complex code occurs, we treat it as a single atomic code. We refer to complex code when an entity is assigned multiple SNOMED CT codes. Example: Al examen físico las mucosas son húmedas y normocoloreadas, la auscultación cardio-respiratoria es normal [...] (On physical examination the mucous membranes are moist and normal-coloured, cardio-respiratory auscultation is normal <ref type="bibr" coords="3,433.93,383.36,8.93,9.74">[..</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.])</head><p>This phrase is annotated with composite SNOMED CT code 449263002+449264008 "auscultación del corazón" + "auscultación del tracto respiratorio inferior" ("auscultation of the heart" + "auscultation of the lower respiratory tract"), which means that both codes occur in the marked span. In the enriched SNOMED CT taxonomy it is defined as 449263002+449264008 "auscultación cardio-respiratoria".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task 1: Clinical Procedure Recognition</head><p>In the task 1, participants are requested to automatically detect mentions of clinical procedures in the provided clinical reports. In other words, it is a Named Entity Recognition task. We have faced the task as a regular sequence labelling task, using IOB-tagging to emit one of B-TAG, I-TAG or O, where the only possible TAG type is "PROCEDIMIENTO".</p><p>The sequence labelling is performed by a Transformer-based model, which encodes each input token into its contextual word embedding. These word embeddings pass through a classification head that projects the word embedding into the output label space. We have experimented with several Transformer models.</p><p>Since the MedProcNER documents are generally too long to fit in one piece into a Transformer model, we have applied the sliding windows technique, as described elsewhere <ref type="bibr" coords="3,450.26,643.61,11.58,9.74" target="#b2">[3]</ref>. In a few words, we surround each window with a number of context tokens. These tokens are ignored when rebuilding the original document; they provide valuable information to resolve the central window by avoiding hard, meaningless segmentation cuts. We have applied this sliding windows technique with the xlm-roberta-large<ref type="foot" coords="4,252.01,301.06,4.06,7.79" target="#foot_1">2</ref> model and with the roberta-base-biomedical-es from the BSC <ref type="foot" coords="4,108.04,314.61,4.06,7.79" target="#foot_2">3</ref> as they are listed in the HuggingFace model hub <ref type="bibr" coords="4,333.04,317.29,11.43,9.74" target="#b3">[4]</ref>.</p><p>As an alternative to the sliding windows approach, we have tried a longformer model, which allows a large enough sequence-length encoding up to 4096 to accommodate any MedProcNER clinical procedures in one shot. We select longformer-base-4096-bne-es <ref type="foot" coords="4,396.31,355.26,4.06,7.79" target="#foot_3">4</ref> which is the longformer version of the RoBERTa model for the Spanish language <ref type="bibr" coords="4,328.47,371.49,11.28,9.74" target="#b4">[5]</ref>. It allows us to process larger contexts as input without additional aggregation strategies. We split the clinical reports documents into paragraphs following the line-break characters and tokenize them into words using SpaCy<ref type="foot" coords="4,501.46,395.91,4.06,7.79" target="#foot_4">5</ref> to obtain token-label pairs. The resulting corpus is shown in Table <ref type="table" coords="4,388.24,412.13,3.74,9.74" target="#tab_1">2</ref>, we get 3,821 paragraphs where the longest one is 711 tokens. Transformer encoding tokens do not correspond to the grammatical word and punctuation tokens, so we assume that the maximum sequence length of 2,048 will be enough to encode all the paragraphs.</p><p>Table <ref type="table" coords="4,127.04,466.33,5.08,9.74" target="#tab_2">3</ref> shows the models' performance where the best-performing model is roberta-bio-es, trained on biomedical domain corpora, the second best model is xlm-roberta-large which shows close to the best performance due to its size and parameters number. The longformer strategy is not the best, which might be because of the size of the training corpus reduces with the paragraph split method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Task 2: Clinical Procedure Normalization</head><p>Our primary approach is Semantic Text Similarity (STS) techniques. STS determines how similar two pieces of text are by measuring their degree of semantic closeness. Semantic search is based on STS, allowing retrieval of relevant text results beyond mere lexical matching. The main concepts of semantic search are query, collection of documents, and degree of relevance between a query and retrieved documents. There are different methods of measuring the degree of relevance and relatedness of two pieces of text-cosine distance, inner product, etc.</p><p>As a baseline model for resolving the normalisation problem, we implement similar document retrieval with the BM25 ranking function <ref type="bibr" coords="5,273.94,142.29,11.35,9.74" target="#b5">[6]</ref>. This function ranks a set of documents based on the query terms appearing in each document, regardless of their proximity, and it works on the concept of bag-of-words and TF-IDF. We use it as the simplest statistical method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Transformer-based Semantic Search</head><p>The semantic search involves embedding all entries (sentences, documents, or, in this case, taxonomy descriptions) into a vector space. At search time, the query, represented in this task by the detected entity mention, is also embedded into the same vector space. This allows a direct comparison of vectors using distance. Nowadays, the most extended method to encode text is to use a pre-trained Transformer model <ref type="bibr" coords="5,304.73,272.99,13.00,9.74" target="#b6">[7]</ref> to obtain the corresponding embeddings (multidimensional vectors) and compute the similarity score using a similarity metric (e.g., in this case, it is the inner product of the normalised vectors).</p><p>We encode the entity words and SNOMED CT code descriptions with a SapBERT-XLMR-large model <ref type="bibr" coords="5,119.24,327.19,11.36,9.74" target="#b7">[8]</ref>. This model is pretrained with UMLS database <ref type="bibr" coords="5,342.29,327.19,12.78,9.74" target="#b8">[9]</ref> using XLM-RoBERTa-large as the base model. We find injecting UMLS knowledge of multilingual clinical terminology into a pretrained language model especially helpful for the normalisation task; an embedding dimension of 1024 is enough to encode all the terminology and corpus entities without truncation. [CLS] token of the transformer's architecture is used for the vector representation of a text.</p><p>Next, each corpus entity's closest candidate from the SNOMED CT database is retrieved. The code of the most similar taxonomy entry is used as the predicted code for each given entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Cross-Encoders</head><p>We also experiment with a cross-encoder model <ref type="bibr" coords="5,302.90,457.89,17.83,9.74" target="#b9">[10]</ref> training. Cross-encoders handle sentence pair scoring and classification tasks <ref type="bibr" coords="5,256.14,471.44,16.42,9.74" target="#b10">[11]</ref>. They have been proven successful in the clinical domain also <ref type="bibr" coords="5,149.48,484.99,16.42,9.74" target="#b11">[12]</ref>. In contrast to an unsupervised semantic similarity function, the crossencoder is trained by encoding both sentences simultaneously and produces a value between 0 and 1 that indicates the similarity or relatedness of the input sentence pair (see Figure <ref type="figure" coords="5,491.24,512.09,8.21,9.74" target="#fig_0">1b</ref>). Cross-encoders are trained using a set of text pairs labelled as similar/related (i.e., positive) or dissimilar/unrelated (negative).</p><p>We have used the same train and evaluation split from the dataset to retrieve positive examples from the SNOMED CT file. For each entity labelled in the corpus, we have created pairs with the entity's text and the SNOMED CT description corresponding to the entity's SNOMED CT code. To obtain negative examples, we have used negative sampling by choosing pairs that are not related. We implement three types of corpus preparation with negative sampling.</p><p>• NS1. Semantic search with SapBERT-XLMR-large model <ref type="bibr" coords="5,377.39,630.06,11.59,9.74" target="#b7">[8]</ref>. We take the first 64 candidates, and for each of them, we assign the value 1.0 to the query-candidate pair if the query has the same code as a candidate, and the value 0.0 if the code is different. The column SNOMED CT Description in Table <ref type="table" coords="5,308.11,670.70,5.07,9.74" target="#tab_3">4</ref> contains ten pairs for a query "exploración ginecológica" (SNOMED CT code 83607001). We hypothesise that labelled semantically similar pairs will help to discriminate the correct term. • NS2. Semantic search with SapBERT-XLMR-large model <ref type="bibr" coords="6,364.87,441.15,12.69,9.74" target="#b7">[8]</ref> adding UMLS synonyms. We enrich the negative sampling corpus with definitions from different clinical taxonomies presented in UMLS, such as ICD-10, CUI, etc. We use the ClinIDMap mapping tool <ref type="bibr" coords="6,488.04,468.25,17.95,9.74" target="#b12">[13]</ref> to get new synonyms for all corpus codes. The number of positive pairs increases more than three times. The column UMLS Synonym shows some examples for the same query in Table <ref type="table" coords="6,154.99,508.89,3.74,9.74" target="#tab_4">5</ref>. • NS3. We get all composite codes from the train set and make composite descriptions concatenating the terms of these codes from SNOMED CT. With this, the positive pair will be:</p><p>corpus span: auscultación cardio-respiratoria (cardio-respiratory auscultation) SNOMED CT description: auscultación del corazón; auscultación del tracto respiratorio inferior (auscultation of the heart; auscultation of the lower respiratory tract)</p><p>Negative examples are obtained with the same method as described above. We add the composite code examples to the NS2 corpus.</p><p>Table <ref type="table" coords="6,127.87,670.70,5.17,9.74" target="#tab_4">5</ref> describes the datasets obtained with the methods of negative sampling. We have  trained three cross-encoder models with these datasets on top of the model RoBERTa base pre-trained with data from the National Library of Spain (BNE) <ref type="bibr" coords="7,382.81,479.19,13.00,9.74" target="#b4">[5]</ref> as it performed well on Spanish texts. We use the Sentence-Transformers <ref type="bibr" coords="7,308.17,492.74,17.78,9.74" target="#b9">[10]</ref> framework to train the models. The best checkpoint within 20 training epochs was chosen according to the macro F1-score calculated on the validation dataset-F1=0.6843 for NS1 method, F1=0.9118 for NS2 method and F1=0.9046 for NS3 method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Approaches to SNOMED CT code prediction</head><p>Based on the described semantic text search techniques and models, we have experimented with three approaches to predict the correct set of codes given to an entity. We also implement the search in SNOMED CT database only and in the enriched SNOMED CT+train database. We also try to filter SNOMED CT database to procedure semantic tag only, but, as was commented in Section 2, some of the annotated codes are not procedures, the performance drops, so we have not considered it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Search (SS)</head><p>We use two types of semantic search: (1) with BM25 ranking and (2) transformer-based semantic search. For the second type we encode both the entity words and SNOMED CT with SapBERT model and retrieve the closest candidate from the SNOMED CT using the cosine similarity function (see Figure <ref type="figure" coords="8,306.49,128.74,8.01,9.74" target="#fig_0">1a</ref>). The code of the most similar taxonomy entry is used as the predicted code for each given entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Search and Rerank (SS-R)</head><p>With this approach the prediction of the SNOMED CT codes consists on the following steps:</p><p>1. Retrieve 64 candidates from SNOMED CT with the semantic search or with BM25 approaches described above. 2. Rerank the retrieved documents with the cross-encoder model described in Section 4.2.</p><p>The cross-encoder model re-scores the candidates retrieved in the first step. 3. Get the candidate with the highest score from the cross-encoder and pick its corresponding SNOMED CT code for the entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Search and Conditional Post-Processing (SS-C)</head><p>Examining the top most similar items retrieved from semantic search, we observe that sometimes the correct answer occurs at the second position, increasing the evaluation score of Accuracy@K where K=2 by 5 points. Low scores in the first position suggest the retrieved term is incorrect, and the second following may be correct. We experiment with the similarity score threshold on the development set and select the threshold with slightly higher accuracy (see SS-C in Table <ref type="table" coords="8,393.38,372.00,3.56,9.74" target="#tab_5">6</ref>). If the score of the first retrieved is less than the threshold, we choose the second item.</p><p>All semantic similarity experiments are implemented using FAISS <ref type="bibr" coords="8,403.05,413.27,18.07,9.74" target="#b13">[14]</ref> and the reranking process is implemented in Sentence-Transformers <ref type="bibr" coords="8,318.96,426.82,18.07,9.74" target="#b9">[10]</ref> framework. We used the normalised inner product to calculate the similarity metric. The performance is calculated over the Task 2 validation set. As depicted in Table <ref type="table" coords="8,263.93,453.92,3.81,9.74" target="#tab_5">6</ref>, the best-performing system is the semantic search with SapBERT model over SNOMED CT dictionary enriched with the entities from the training corpus. The performance of any semantic search system depends not only on the semantic relatedness but also on lexical similarity, which is essential for a short text search like in this case. 112 unique entities from the validation set precisely match the entities in the training set but not in SNOMED CT ("oligoelementos", "nutrición parenteral", 'coronariografía", "acto quirúrgico", "fluidoterapia", "diagnóstico anatomopatológico" etc), making predictions easy. For this reason, we have selected this system for our submission to the task competition. To test our ideas, we also submit the cross-encoder models on top of our best NER model (see Table <ref type="table" coords="8,497.08,562.31,3.53,9.74" target="#tab_2">3</ref>).</p><p>Since we were limited to submit only five runs, we decided not to use BM25 method due to its lower scores. But it should be noted that the cross-encoder model helps to re-rank the retrieved with BM25 system, BM25+cross-encoder performs 5 points better than BM25 only.</p><p>The sampling methods for cross-encoder training show very similar behaviour. As for the transformer-based semantic search and cross-encoder, it does not help in the overall evaluation. We manually examine the errors of the cross-encoder and see that in some cases it rescores incorrect predictions of the semantic search system to the correct ones. So, it could be used in an ensemble with the SS system. For instance, corpus entity "anatomía patológica del lavado" (pathological anatomy of lavage) is normalised to the code 67889009 "lavado" (lavage), while reranking model assigns the correct term "anatomía patológica" (pathological anatomy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Task 3: Clinical Procedure-based Document Indexing</head><p>Our approach for the MedProcNER Task 3 is directly based on the previous tasks. The Task 1 and 2 detect and normalise mentions to clinical procedures, obtaining the exact span in which the occur, together with their SNOMED CT code. For the Task 3 we just gather the codes for each document, retaining the set of unique codes per document, and using that as the outcome for the Task 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and Discussion</head><p>This section describes the official results obtained by our submitted systems in the MedProcNER 2023 Shared Task. At the time of this writing, the results from all the other participants have not been disclosed by the organisers, and the only information we have to compare our systems is the best score for each task. The results are shown in Table <ref type="table" coords="10,365.38,418.92,3.73,9.74" target="#tab_6">7</ref>, where the scores provided by the organisers are marked in bold. According to this, in the Task 1 our best system is 2 points below the best scoring participant, while our best system for Task 2 and Task 3 has obtained the highest score.</p><p>Our models exhibit a similar behaviour on our custom validation set and in the official test set. The precision of xlm-roberta-large model is notably higher than in the other models both in the validation and test set, which might affect the performance and robustness of the model and might be helpful in situations when the precision metric is more important than recall.</p><p>We examined the errors in the development set and concluded that it is difficult for the models to distinguish between semanticaly or lexically close corpus entity and its code definition. For instance, corpus entity "Gammagrafía ósea con Tc99m-MDP" (Bone scintigraphy with Tc99m-MDP) is manually annotated as code 418832007 which has the definition "gammagrafía ósea de cuerpo entero" (whole body bone scintigraphy). The system assigns to the entity code 425559005 with the definition "resonancia magnética nuclear de hueso" (bone magnetic resonance imaging). We suppose that the model calculates high relatedness because of the words "ósea" and "hueso" (which both have meaning bone). Also the word "gammagrafía" is related to nuclear medicine, which is semantically close to "resonancia magnética nuclear".</p><p>There are also some other challenging points. For instance, a type of error is related to the strict match of the entity and SNOMED CT definition. There are cases where the corpus word is equal to the SNOMED CT definition but they are annotated with different codes. This is an example from the corpus: Pruebas complementarias: hemograma con ligera leucocitosis sin desviación de fórmula leucocitaria siendo el resto normal, bioquímica y coagulación normales. (Complementary tests: hemogram with slight leukocytosis without deviation of the leukocyte formula, the rest being normal, normal biochemistry and coagulation.)</p><p>In this context, the entity "hemograma" (blood count) is manually annotated with code 26604007, definition "recuento sanguíneo completo" (complete blood count). The predicted code is 43789009 with definition "hemograma" (blood count). The code 43789009 has two definitions in SNOMED CT: "hemograma completo sin fórmula diferencial" (complete blood count without differential formula) and "hemograma" (blood count). We rely on the similarity model in the whole database, and it always matches two identical words if they occur. In this case, the possible solution might be additional word sense disambiguation using the context of the corpus entity.</p><p>Composite codes are difficult to predict also, for instance, code 363679005+182770003 "estudios de imagen y preanestésicos" (imaging and pre-anaesthetic studies) is composed of two definition-"procedimiento de estudio por imágenes" (imaging procedure) and "evaluación preanestésica" (pre-anaesthetic assessment). The predicted code is 182770003, which is only one part of the code. A possible method to tackle this problem could be a specific classifier to distinguish between simple and composite codes. Further, as a composite code may consist of more than two codes, in case of using a similarity search approach it would be difficult to guess the number of top most-similar codes to select.</p><p>Again, it must be noted that the results of each task depend on the results from the previous tasks. Any error in the Task 1 impacts the results for the Task 2 and Task 3, and that must be taken into account when examining the overall results for the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper we have described our participation in the MedProcNER 2023 Shared Task. We presented three runs for Task 1 (NER), which requires finding mentions of procedure entities in the provided clinical texts. For this first task we have trained several sequence labelling models based on multilingual and Spanish pre-trained Transformer models. For Task 2 (Normalisation), which requires assigning specific SNOMED CT codes to each detected entity, we have implemented a system based on Semantic Text Similarity and cross-encoders. Our approach for Task 3 (document indexing) is directly based on the systems for the previous two tasks; we detect procedure entities, normalise them, and then get a set of unique codes for each document. Our submissions for Task 2 and 3 have achieved the highest scores for the competition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,89.29,405.36,418.35,8.91;7,89.29,417.32,416.69,8.91;7,89.29,429.27,99.91,8.91;7,89.29,205.58,416.67,192.31"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Semantic models. Bi-encoder encodes two text pieces separately and measures their relatedness with semantic similarity function. Cross-encoder model is trained with pairs of text and produces a score between 0 and 1.</figDesc><graphic coords="7,89.29,205.58,416.67,192.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.67,366.97,69.89"><head>Table 1</head><label>1</label><figDesc>Training dataset for tasks 1 and 2</figDesc><table coords="3,139.31,122.26,316.65,38.30"><row><cell></cell><cell cols="3">Documents PROCEDIMIENTO Unique SNOMED CT codes</cell></row><row><cell>Train</cell><cell>675</cell><cell>4,346</cell><cell>1,538</cell></row><row><cell>Validation</cell><cell>75</cell><cell>511</cell><cell>291</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.99,90.67,366.68,69.89"><head>Table 2</head><label>2</label><figDesc>Corpus for Longformer-es model</figDesc><table coords="4,139.60,122.26,316.07,38.30"><row><cell></cell><cell cols="3">Paragraphs Max Tokens/Paragraph Mean Token/Paragraph</cell></row><row><cell>Train</cell><cell>3,432</cell><cell>711</cell><cell>79.96</cell></row><row><cell>Validation</cell><cell>389</cell><cell>533</cell><cell>78.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,88.99,182.88,306.44,79.60"><head>Table 3</head><label>3</label><figDesc>Results of the NER models on our validation set.</figDesc><table coords="4,199.84,212.22,195.60,50.25"><row><cell>System</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>xlm-roberta-large</cell><cell cols="3">0.7872 0.7514 0.7689</cell></row><row><cell>roberta-bio-es</cell><cell cols="3">0.7649 0.7755 0.7702</cell></row><row><cell cols="4">longformer-bne-es 0.7402 0.7560 0.7480</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,88.99,90.67,417.00,295.83"><head>Table 4</head><label>4</label><figDesc>Examples for training the cross-encoder created with negative sampling of the query "exploración ginecológica" (gynecologic examination), SNOMED CT code 83607001. The label 1 indicates that the retrieved entry matches the code assigned in the training data to "exploración ginecológica". The UMLS Synonym column shows the definitions of the term in the clinical taxonomies included in UMLS .</figDesc><table coords="6,199.57,158.13,201.64,228.37"><row><cell cols="2">Label SNOMED CT Description</cell></row><row><cell>1</cell><cell>examen ginecológico</cell></row><row><cell>0</cell><cell>exploración del aparato genital femenino</cell></row><row><cell>0</cell><cell>examen ginecológico endoscópico</cell></row><row><cell>0</cell><cell>endoscopia ginecológica</cell></row><row><cell>0</cell><cell>examen ginecológico de rutina</cell></row><row><cell>0</cell><cell>exploración de vagina</cell></row><row><cell>0</cell><cell>examen vaginal</cell></row><row><cell>0</cell><cell>exploración del aparato genitourinario</cell></row><row><cell>0</cell><cell>incisión y exploración de la vagina</cell></row><row><cell>0</cell><cell>pruebas uroginecológicas</cell></row><row><cell cols="2">Label UMLS Synonym</cell></row><row><cell>1</cell><cell>gynecologic examination</cell></row><row><cell>1</cell><cell>gynaecologic examination</cell></row><row><cell>1</cell><cell>female genital examination</cell></row><row><cell>1</cell><cell>examination of female genitals</cell></row><row><cell>1</cell><cell>gynecologic examination (procedure)</cell></row><row><cell>1</cell><cell>examen ginecológico (procedimiento)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,88.99,90.67,371.38,99.07"><head>Table 5</head><label>5</label><figDesc>Size of training and development corpus for cross-encoder training.</figDesc><table coords="7,134.90,122.26,325.47,67.47"><row><cell>Method</cell><cell></cell><cell>Train</cell><cell></cell><cell></cell><cell>Validation</cell><cell></cell></row><row><cell></cell><cell>Total</cell><cell cols="5">Negative Positive Total Negative Positive</cell></row><row><cell>NS1</cell><cell>173,430</cell><cell>168,565</cell><cell>4,865</cell><cell>25,771</cell><cell>24,957</cell><cell>814</cell></row><row><cell>NS2</cell><cell>186,179</cell><cell>168,565</cell><cell>17,614</cell><cell>27,759</cell><cell>24,957</cell><cell>2,802</cell></row><row><cell>NS3</cell><cell>186,294</cell><cell>168,578</cell><cell>17,716</cell><cell>27,776</cell><cell>24,955</cell><cell>2,821</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,88.99,90.67,334.09,336.39"><head>Table 6</head><label>6</label><figDesc></figDesc><table coords="9,89.29,102.62,333.79,324.43"><row><cell>Experiment Results</cell><cell></cell><cell></cell><cell></cell></row><row><cell>System</cell><cell>Database</cell><cell cols="2">Accuracy@1 F1 macro</cell></row><row><cell>SS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM25</cell><cell>SNOMED CT</cell><cell>0.1996</cell><cell>0.0965</cell></row><row><cell>BM25</cell><cell>SNOMED CT+train</cell><cell>0.4149</cell><cell>0.2152</cell></row><row><cell>SapBERT</cell><cell>SNOMED CT</cell><cell>0.4344</cell><cell>0.2554</cell></row><row><cell>SapBERT</cell><cell>SNOMED CT+train</cell><cell>0.6810</cell><cell>0.4095</cell></row><row><cell>SS-R</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">BM25+NS1 SNOMED CT+train</cell><cell>0.4775</cell><cell>0.2579</cell></row><row><cell cols="2">BM25+NS2 SNOMED CT+train</cell><cell>0.4775</cell><cell>0.2604</cell></row><row><cell cols="2">BM25+NS3 SNOMED CT+train</cell><cell>0.4716</cell><cell>0.2604</cell></row><row><cell>SS+NS1</cell><cell>SNOMED CT+train</cell><cell>0.6458</cell><cell>0.3703</cell></row><row><cell>SS+NS2</cell><cell>SNOMED CT+train</cell><cell>0.6438</cell><cell>0.3814</cell></row><row><cell>SS+NS3</cell><cell>SNOMED CT+train</cell><cell>0.6360</cell><cell>0.3646</cell></row><row><cell>SS-C</cell><cell>Threshold</cell><cell></cell><cell></cell></row><row><cell></cell><cell>14</cell><cell>0.6810</cell><cell>0.4095</cell></row><row><cell></cell><cell>16</cell><cell>0.6830</cell><cell>0.4113</cell></row><row><cell></cell><cell>18</cell><cell>0.6830</cell><cell>0.4104</cell></row><row><cell></cell><cell>20</cell><cell>0.6810</cell><cell>0.4071</cell></row><row><cell></cell><cell>22</cell><cell>0.6732</cell><cell>0.3939</cell></row><row><cell></cell><cell>24</cell><cell>0.6810</cell><cell>0.4017</cell></row><row><cell></cell><cell>26</cell><cell>0.6693</cell><cell>0.3892</cell></row><row><cell></cell><cell>28</cell><cell>0.6556</cell><cell>0.3738</cell></row><row><cell></cell><cell>30</cell><cell>0.6478</cell><cell>0.3631</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,88.99,90.67,333.48,300.54"><head>Table 7</head><label>7</label><figDesc>Test results, provided by the organisers; the bold font refers to the best scores.</figDesc><table coords="10,172.80,122.26,249.68,268.95"><row><cell>System</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Task 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run 1: xlm-roberta-large</cell><cell cols="3">0.8054 0.7535 0.7786</cell></row><row><cell>Run 2: roberta-bio-es</cell><cell cols="3">0.7679 0.7629 0.7653</cell></row><row><cell>Run 3: longformer-bne-es</cell><cell cols="3">0.7478 0.7588 0.7533</cell></row><row><cell>Best</cell><cell></cell><cell></cell><cell>0.7985</cell></row><row><cell>Task 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run 1: xlm-roberta-large-SS</cell><cell cols="3">0.5902 0.5525 0.5707</cell></row><row><cell>Run 2: roberta-bio-es-SS</cell><cell cols="3">0.5665 0.5627 0.5646</cell></row><row><cell>Run 3: roberta-bio-es-SS-C</cell><cell cols="3">0.5662 0.5625 0.5643</cell></row><row><cell cols="4">Run 4: roberta-bio-es-SS-R-NS2 0.5248 0.5213 0.5230</cell></row><row><cell>Run 5: longformer-bne-es-SS</cell><cell cols="3">0.5498 0.5580 0.5539</cell></row><row><cell>Best</cell><cell></cell><cell></cell><cell>0.5707</cell></row><row><cell>Task 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run 1: roberta-bio-es-SS</cell><cell cols="3">0.6182 0.6295 0.6238</cell></row><row><cell cols="4">Run 2: roberta-bio-es-SS-R-NS2 0.5885 0.5917 0.5901</cell></row><row><cell>Run 3: longformer-bne-es-SS</cell><cell cols="3">0.6039 0.6288 0.6161</cell></row><row><cell>Run 4: xlm-roberta-large-SS</cell><cell cols="3">0.6371 0.6109 0.6239</cell></row><row><cell>Run 5: roberta-bio-es-SS-C</cell><cell cols="3">0.6190 0.6295 0.6242</cell></row><row><cell>Best</cell><cell></cell><cell></cell><cell>0.6242</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,92.46,671.96,118.41,8.01"><p>https://temu.bsc.es/medprocner/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,92.46,639.06,150.34,8.01"><p>https://huggingface.co/xlm-roberta-large</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,92.46,650.02,247.92,8.01"><p>https://huggingface.co/PlanTL-GOB-ES/roberta-base-biomedical-es</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,92.46,660.98,256.31,8.01"><p>https://huggingface.co/PlanTL-GOB-ES/longformer-base-4096-bne-es</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,92.46,671.94,59.07,8.01"><p>https://spacy.io/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,112.66,657.16,394.53,9.74;11,112.66,670.70,395.16,9.74;12,112.66,88.09,393.33,9.74;12,112.66,101.64,178.54,9.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,236.08,670.70,271.74,9.74;12,112.66,88.09,155.91,9.74">Overview of MedProcNER Task on Medical Procedure Detec-tion and Entity Linking at BioASQ</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lima-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Farré-Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gascó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,313.62,88.09,192.37,9.74;12,112.66,101.64,147.84,9.74">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,115.19,394.53,9.74;12,112.66,128.74,393.32,9.74;12,112.66,142.29,393.32,9.74;12,112.66,155.84,393.32,9.74;12,112.66,169.38,306.11,9.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,234.29,128.74,271.69,9.74;12,112.66,142.29,306.98,9.74">Overview of BioASQ 2023: The eleventh BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lima-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Farré-Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,445.01,142.29,60.97,9.74;12,112.66,155.84,393.32,9.74;12,112.66,169.38,252.02,9.74">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="12,112.66,182.93,393.32,9.74;12,112.66,196.48,393.33,9.74;12,112.66,210.03,395.00,9.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,289.41,182.93,119.38,9.74">Vicomtech at CANTEMIST</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García-Pablos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cuadros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,452.67,182.93,53.31,9.74;12,112.66,196.48,393.33,9.74;12,112.66,210.03,246.73,9.74">Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020) co-located with 36th Conference of the Spanish Society for Natural Language Processing</title>
		<meeting>the Iberian Languages Evaluation Forum (IberLEF 2020) co-located with 36th Conference of the Spanish Society for Natural Language Processing<address><addrLine>SEPLN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020. 2020</date>
			<biblScope unit="page" from="489" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,223.58,394.52,9.74;12,112.66,237.13,393.32,9.74;12,112.66,250.68,185.25,9.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,216.95,237.13,289.03,9.74;12,112.66,250.68,45.66,9.74">HuggingFace&apos;s Transformers: State-of-the-art Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,264.23,394.53,9.74;12,112.66,277.78,393.32,9.74;12,112.66,291.33,137.92,9.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,288.91,277.78,144.34,9.74">MarIA: Spanish Language Models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Fandiño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Estapé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pàmies</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Palao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Ocampo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">P</forename><surname>Carrino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">A</forename><surname>Oller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Penagos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,441.75,277.78,64.23,9.74;12,112.66,291.33,93.13,9.74">Procesamiento del Lenguaje Natural</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,304.88,394.52,9.74;12,112.66,318.43,394.53,9.74;12,112.33,331.98,393.65,9.74;12,112.66,345.52,395.01,9.74;12,112.41,359.07,38.81,9.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,337.31,304.88,169.87,9.74;12,112.66,318.43,128.94,9.74">Okapi at TREC-7: Automatic Ad Hoc, Filtering, VLC and Interactive</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,264.54,318.43,242.65,9.74;12,112.33,331.98,49.16,9.74">Proceedings of The Seventh Text REtrieval Conference, TREC 1998</title>
		<meeting>The Seventh Text REtrieval Conference, TREC 1998<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="1998">November 9-11, 1998. 1998</date>
			<biblScope unit="page" from="199" to="210" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,372.62,394.53,9.74;12,112.66,386.17,394.53,9.74;12,112.66,399.72,393.32,9.74;12,112.66,413.27,222.96,9.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,175.25,386.17,108.07,9.74">Attention is All you Need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="12,314.47,399.72,191.52,9.74;12,112.66,413.27,33.92,9.74">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,426.82,393.53,9.74;12,112.66,440.37,395.01,9.74;12,112.66,453.92,38.81,9.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,287.93,426.82,218.26,9.74;12,112.66,440.37,179.46,9.74">Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,315.68,440.37,145.79,9.74">Proceedings of ACL-IJCNLP 2021</title>
		<meeting>ACL-IJCNLP 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="565" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,467.47,393.32,9.74;12,112.66,481.02,226.14,9.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,186.62,467.47,319.36,9.74;12,112.66,481.02,51.86,9.74">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,173.16,481.02,81.71,9.74">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,494.57,395.17,9.74;12,112.66,508.11,393.32,9.74;12,112.66,521.66,393.32,9.74;12,112.66,535.21,394.53,9.74;12,112.66,548.76,122.62,9.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,231.25,494.57,276.58,9.74;12,112.66,508.11,41.53,9.74">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,183.76,508.11,322.21,9.74;12,112.66,521.66,393.32,9.74;12,112.66,535.21,330.76,9.74">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,562.31,366.87,9.74" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno>ArXiv abs/1901.04085</idno>
		<title level="m" coord="12,207.27,562.31,134.85,9.74">Passage Re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,575.86,395.18,9.74;12,112.66,589.41,395.17,9.74;12,112.66,602.96,394.53,9.74;12,112.66,616.51,159.97,9.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,274.33,575.86,233.51,9.74;12,112.66,589.41,100.52,9.74">WikiUMLS: Aligning UMLS to Wikipedia via Crosslingual Neural Ranking</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Verspoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,237.02,589.41,270.81,9.74;12,112.66,602.96,339.95,9.74">Proceedings of the 28th International Conference on Computational Linguistics, International Committee on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics, International Committee on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5957" to="5962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,630.06,393.32,9.74;12,112.66,643.61,395.16,9.74;12,112.66,657.16,395.01,9.74;12,112.66,670.70,48.96,9.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,263.68,630.06,242.30,9.74;12,112.66,643.61,68.73,9.74">ClinIDMap: Towards a Clinical IDs Mapping for Data Interoperability</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zotova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cuadros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rigau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,211.95,643.61,295.87,9.74;12,112.66,657.16,69.49,9.74">Proceedings of the Thirteenth Language Resources and Evaluation Conference</title>
		<meeting>the Thirteenth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3661" to="3669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,88.09,395.17,9.74;13,112.66,101.64,156.41,9.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,254.59,88.09,181.01,9.74">Billion-scale Similarity Search with GPUs</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,445.08,88.09,62.75,9.74;13,112.66,101.64,77.55,9.74">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
