<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,350.20,15.42;1,89.29,106.66,266.79,15.42;1,89.29,129.00,210.20,11.96">Object Detection Pipeline Using YOLOv8 for Document Information Extraction Notebook for the DocILE Lab at CLEF 2023</title>
				<funder ref="#_KaHJSmh">
					<orgName type="full">Ministry of Education, Youth, and Sports of the Czech Republic</orgName>
				</funder>
				<funder ref="#_26SCG8U">
					<orgName type="full">University of West Bohemia</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.10,154.90,61.74,11.96"><forename type="first">Jakub</forename><surname>Straka</surname></persName>
							<email>strakajk@kky.zcu.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Cybernetics and New Technologies for the Information Society</orgName>
								<address>
									<addrLine>Technická 8</addrLine>
									<postCode>301 00</postCode>
									<settlement>Plzeň</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,163.48,154.90,59.21,11.96"><forename type="first">Ivan</forename><surname>Gruber</surname></persName>
							<email>grubiv@ntis.zcu.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Cybernetics and New Technologies for the Information Society</orgName>
								<address>
									<addrLine>Technická 8</addrLine>
									<postCode>301 00</postCode>
									<settlement>Plzeň</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,350.20,15.42;1,89.29,106.66,266.79,15.42;1,89.29,129.00,210.20,11.96">Object Detection Pipeline Using YOLOv8 for Document Information Extraction Notebook for the DocILE Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">7EFF89DBA95EC976B2360FEC1E2E2C62</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Document Understanding</term>
					<term>Document Information Extraction</term>
					<term>Object Detection</term>
					<term>YOLOv8</term>
					<term>Line Item Recognition</term>
					<term>Key Information Localization and Extraction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The extraction of information from semi-structured documents is an ongoing problem. This task is often approached from the perspective of NLP and large transformer-based models are employed. In our work, we successfully demonstrated that the Key Information Localization and Extraction (KILE) and Line Item Recognition (LIR) tasks can be effectively addressed as object detection problems using a convolutional neural network (CNN) model. We utilized a relatively small and fast YOLOv8 model for which we conducted a series of experiments to explore the impact of different factors on model training. With YOLOv8, we were able to achieve AP 0.716 on the KILE task and 0.638 on the LIR task. Our code is available at https://github.com/strakaj/YOLOv8-for-document-understanding.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Companies engage in daily communication often through semi-structured documents. Upon receiving such documents, the initial step involves manually extracting the data before it can be processed. This manual process is repetitive and time-consuming, leading to the question of whether the information from these documents could be obtained automatically. contain tables where items can span multiple lines. The objective of LIR is to detect and classify all the information in the table and group related information into line items. KILE is a relatively simpler task that only aims to detect information and classify them into a predefined set of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data</head><p>The DocILE dataset <ref type="bibr" coords="2,182.03,186.24,11.48,10.91" target="#b0">[1,</ref><ref type="bibr" coords="2,196.75,186.24,9.03,10.91" target="#b1">2]</ref> is composed of three subsets. The main subset is a set of around 6700 annotated documents. All documents were chosen to have an invoice-like structure. The second subset is a set of around 100 000 synthetically generated documents and the last is a set of around 900 000 unlabeled documents. In our work, we primarily focused on the annotated subset.</p><p>Annotated subset contains 5180 training documents, 500 validation documents, and 1000 test documents. Each document can have multiple pages resulting in 6759 training pages, 635 validation pages, and 1321 test pages. The KILE task contains 36 classes while the LIR task contains 19 classes. In both tasks, information that needs to be extracted is referred to as fields. These fields contain text and a bounding box that represents their location. Additionally, in the LIR task, they also contain the id of the line item to which they belong. The documents in the dataset are divided into layout clusters based on the types of fields in the document and their position. This means that all documents in one layout cluster have fields with the same types on the same positions.</p><p>The dataset exhibits a significant diversity of layouts, as demonstrated by the validation set consisting of 204 distinct layout clusters and the training set containing 1063 clusters. This is one of the main challenges of the document understanding task. In Figure <ref type="figure" coords="2,430.30,403.03,5.17,10.91" target="#fig_0">1</ref> are shown two documents from two layout clusters with similar classes but very different layouts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Evaluation metric</head><p>For both tasks, it is only necessary to detect the position of all fields, their class (field type), and in the case of the LIR task, to assign the field to the correct line item. Since the task is specified as a detection task, the authors of the competition chose a standard metric for object detection. The primary metric used for the KILE task is average precision (AP), and for the LIR task is used micro F1 score (F1). To make the metric more suitable for the tasks, Pseudo-Character Centers (PCC) of letters are used instead of Intersection-over-Union to determine true positives. This choice was inspired by <ref type="bibr" coords="2,193.01,547.60,11.41,10.91" target="#b2">[3]</ref>. PCCs are generated by splitting the field uniformly by the number of its character. The detected field should contain only PCCs corresponding to that field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>In this work, we approached both tasks as object detection tasks. Because of its speed and small size, we decided to utilize the YOLOv8<ref type="foot" coords="2,257.33,631.57,3.71,7.97" target="#foot_0">1</ref>  <ref type="bibr" coords="2,264.27,633.32,12.71,10.91" target="#b3">[4]</ref> model. YOLOv8 is a one-stage, anchor-free detector based on a convolutional neural network (CNN). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">LIR post-processing</head><p>Post-processing of detected LIR fields involves only a grouping procedure. Objects are grouped into line items based on detected line_item fields. To determine the line item to which each LIR field belongs, we find the line_item field that has the largest overlap in the y-axis with the LIR field. The overlap must exceed a chosen threshold, which, based on experiments, see Table <ref type="table" coords="3,499.63,604.72,3.78,10.91" target="#tab_0">1</ref>, we set at 20% of the height of the line_item field. Tables usually span across the entire width of the page, therefore we used only overlap in the y-axis and omitted the x-axis completely. The grouping can be also approached in different ways, it may be beneficial to explore different approaches in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Chargrid</head><p>Chargrid was presented in <ref type="bibr" coords="4,213.36,193.46,13.00,10.91" target="#b4">[5]</ref> as a novel representation of the documents. Chargrid is constructed from character bounding boxes. In the first step, a new image is initialized with the same dimensions as the original document. In the second step, each character is assigned a unique numeric value. In the last step, the area of the box corresponding to a character is filled in the new image by the value assigned to this character. We adopted this procedure but with some modifications. Instead of replacing the original image, we concatenated chargrid representation with the original image. Since we did not have character bounding boxes, we used word bounding boxes from OCR <ref type="bibr" coords="4,406.03,288.30,11.58,10.91" target="#b5">[6]</ref>, which we divided uniformly by the number of characters. Furthermore, instead of encoding characters into one numeric value, we used three numbers for each character, which led to slightly better results than using one number only. To encode characters into 3 channels, we first select a number base for which there exists a mapping between characters and numbers in this base, such that each character has a value with a maximum of 3 digits. We obtain the base using ⌈︁ An example of a chargrid is in Figure <ref type="figure" coords="4,267.18,428.55,3.71,10.91" target="#fig_3">2</ref>. In order to use this representation, it was necessary to increase the number of kernels of the input convolutional layer of the YOLOv8 model.    Additional augmentations provided by the YOLOv8 framework are rotation, up-down flip, shear, perspective, and mixup. We did not use these augmentations in our experiments. Rotation, shear, up-down flip, and perspective are similar to the other geometric augmentations that we used. Finally, we believe that the mixup is not relevant to our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Augmentations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we will describe experiments that were conducted in order to determine the best parameters of the model and other factors that can influence the training procedure. Our aim is to get a better understanding of this task and what is beneficial for it.</p><p>Experimental Setup. If not stated otherwise, we trained models for 100 epochs with an initial learning rate 1 × 10 -3 using optimizer AdamW <ref type="bibr" coords="6,343.15,255.65,11.58,10.91" target="#b6">[7]</ref>. An input image size was set to 640 × 640 pixels and batch size to 16. In experiments, where the input image size is equal to 1280 × 1280, there was the batch size decreased to 8 due to memory requirements. All models were trained from randomly initialized weights. If not specified otherwise, models were trained on annotated train set and evaluated on annotated validation set. Synthetic and unlabeled datasets were not used. All the provided results are averages over three runs initialized from different seeds and the last checkpoint is always used for evaluation.</p><p>The model expects the input image to be a square. Instead of a standard resizing of the image, which would distort the aspect ratio, the shorter side of the image is padded from both sides with a neutral background color.</p><p>Augmentations. We decided to explore possible data augmentations because the model trained without them showed clear signs of overfitting. Data augmentation is a common practice, but the specific augmentations used can vary depending on the task. In this experiment, we aimed to determine the most beneficial augmentations for KILE and LIR tasks. We conducted experiments with augmentations illustrated in Figure <ref type="figure" coords="6,323.77,460.55,3.66,10.91" target="#fig_1">3</ref>. The two most beneficial augmentations were mosaic and translate augmentations. Their comparison is in Table <ref type="table" coords="6,394.05,474.09,3.66,10.91" target="#tab_1">2</ref>. The scale augmentation also showed some improvements, although not as much as the previous two. Unfortunately, a combination of translate and scale did not improve the results compared to translate alone. On the other hand, the two remaining augmentations did not help with overfitting and in some cases even worsened the results.</p><p>It should be noted that all augmentations that have been beneficial are geometric augmentations that alter the position of objects in images. We argue that the model trained without these augmentations strongly relies on the position of the individual classes in the image. However, when the augmentations are applied, the model is forced to obtain information about the object from other visual clues, such as the size, length, and formatting of the text. The inferior performance observed during training with the HSV augmentation can be attributed to the loss of details after applying this augmentation. Despite the left-right flip being a geometric augmentation that alters the position of objects in the image, there was no improvement in the results. This is expected as this augmentation does not represent any real-world scenario. Additionally, since the augmentation is applied with a probability of 0.5, the model receives mixed information about the structure and appearance of the text. This indicates that model can distinguish the appearance and context of the fields. This is further supported by the fact that the model trained with left-right flip augmentation with the probability of 1.0 achieved comparable results with the model trained without it.</p><p>In our final setup, the chosen augmentations were applied with the default setting defined in the YOLOv8 config file. For translate probability was 1.0 and the image could be translated by a maximum of 10% of its size in each axis. Mosaic augmentation was also applied with probability 1.0. Although the mosaic augmentation itself achieved the best results, we used translate in all subsequent experiments, because training with this augmentation was approximately four times faster than with the mosaic augmentation. Chargrid In <ref type="bibr" coords="7,155.23,384.77,12.69,10.91" target="#b4">[5]</ref> was as an input for the model used only chargrid generated from the document. The advantage of chargrid is that it can capture even small characters that could be lost in a standard image at low resolution. On the other hand, chargrid does not convey information about specific formatting of the text and other visual features on the page e.g. borders of the table. However, these features are presented in the original image.</p><p>We decided to compare a model trained only on images, a model trained on chargrids only, and a model trained on both. The results of the experiment are in Table <ref type="table" coords="7,390.93,466.07,3.74,10.91" target="#tab_2">3</ref>. The model trained only with chargrids achieved better results than the model trained only on images. This indicates that the semantic information contained within the text is more useful than the visual information. However, when both representations were used, the model achieved better results than when using the representations separately. This means that each representation provides unique information. Model size. YOLOv8 has several variants that differ in the number of parameters. We observed that models with a higher number of parameters were more prone to overfitting, as expected. This tendency was more prominent in the KILE task compared to the LIR task. This behavior can be attributed to the larger number of objects in the LIR task, where tables consist of multiple lines that can differ in appearance. Better results with larger models could be probably achieved with more data and longer training. In Figure <ref type="figure" coords="8,300.24,154.71,5.17,10.91" target="#fig_7">4</ref> are compared YOLOv8 model variants with different numbers of parameters with baseline methods proposed by competition organizers in <ref type="bibr" coords="8,89.29,181.81,11.45,10.91" target="#b0">[1]</ref>. Interestingly, even the smallest model with 3.2 M parameters achieved better results than transformer-based models with more than 80 M parameters on the KILE task. This indicates that model size is not the only important factor in successfully addressing this task.  In Table <ref type="table" coords="8,138.38,412.27,5.00,10.91" target="#tab_3">4</ref> are summarized numbers of parameters and FLOPs of YOLOv8 variants, RoBERTa and LayotLMv3. All variants YOLOv8 have a lower number of parameters compared to RoBERTa and LayotLMv3. However, only the smallest variants of YOLOv8 have a lower number of FLOPs than RoBERTa and LayotLMv3. Other. For completeness, we also verified the obvious parameters for which we expected an improvement in the results. First, we verified the impact of the size of the input image. From the results in Table <ref type="table" coords="9,180.97,114.06,3.81,10.91" target="#tab_4">5</ref>, it can be seen that the increase in size greatly improved results. We believe it is caused by the fact that details that are lost in low resolution images are important for detection. It is common that models trained from weights pre-trained on different dataset learn faster and have better results. YOLOv8 provides weights pre-trained on the COCO dataset <ref type="bibr" coords="9,465.33,294.04,11.44,10.91" target="#b7">[8]</ref>. Even though objects in the COCO dataset and images of documents are very different, starting from the pre-trained model was beneficial as indicated by results in Table <ref type="table" coords="9,394.72,321.13,3.74,10.91" target="#tab_5">6</ref>. The dataset contains a large set of syntactic documents and unlabeled documents. It is often the case that the more data, the better the result. In Table <ref type="table" coords="9,353.84,474.01,3.81,10.91">7</ref>, there are the results of a model trained on a train set only and of a model trained on the train set together with a synthetic set. Unexpectedly, the model trained on the train and synthetic set performed worse on the validation set than the model trained on the train set only. When trained only on synthetic data model showed signs of overfitting and the results for the validation set were close to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head><p>Results of a model trained on a synthetic dataset. All models were evaluated on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KILE LIR Model</head><p>Training set AP F1 AP F1</p><p>YOLOv8n train 0.476 0.600 0.371 0.557 YOLOv8n train+synthetic 0.417 0.552 0.354 0.527</p><p>In some cases, we observed that the documents in the synthetic set are visually different from the documents in the validation set, and train set, even when they belong to the same cluster. Examples can be viewed in Figure <ref type="figure" coords="10,274.45,114.06,3.66,10.91" target="#fig_8">5</ref>. This is not an issue for baseline methods RoBERTa and LayoutLMv3 presented in <ref type="bibr" coords="10,226.40,127.61,12.91,10.91" target="#b0">[1]</ref> that both use textual information, but it can be an issue for YOLOv8 which mainly relies on visual information. The results indicate that YOLOv8 is not able to sufficiently generalize information from documents that are visually different from the validation set documents, even when they contain similar information at similar positions in the document.</p><p>In object detection, there are no commonly used techniques for pre-training of the model on unlabeled data, therefore we did not utilize unlabeled dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>The final training setup was based on the experiments conducted in the previous section. We used the largest variant of the model, YOLOv8x pre-trained on the COCO dataset. Input image had resolution 1280 × 1280 and was concatenated with chargrid. As augmentation was used only translate augmentation. The initial learning rate was set to 1 × 10 -3 , the batch size was 8, and the AdamW optimizer was used. The model used for the KILE task was trained for 30 epochs and the model for LIR was trained for 50 epochs. Results on the validation set are presented in Table <ref type="table" coords="11,332.05,371.82,3.78,10.91" target="#tab_6">8</ref>. The model achieved superior results compared to the baseline methods on the KILE task. However, the model performed suboptimally on the LIR task. This could be attributed to various factors, one of which is the high number of false positive detections. In most tables not all columns are relevant, but the model was always not able to effectively distinguish which columns are important for the given table and which are not. As a result, there were excessive detections of fields in the tables, leading to a large number of false positives. Another common error that contributed to the high number of false positives, was the miss-classification of fields. An example of excessive detections is shown in Figure <ref type="figure" coords="11,165.49,480.21,5.16,10.91" target="#fig_10">6</ref> and an example of miss-classifications is shown in Figure <ref type="figure" coords="11,435.09,480.21,3.80,10.91" target="#fig_11">7</ref>. In the case of miss-classified fields, the error often occurred with semantically similar classes e.g. gross, net, etc.   In Table <ref type="table" coords="12,138.44,484.71,3.68,10.91" target="#tab_7">9</ref>, there are presented results on the test set. Results the KILE task are consistent with the validation set. Nevertheless, the YOLOv8 results on the LIR task decreased more than the results of the baseline methods. This fact motivates us to analyze the results in more detail. Both validation and test sets can be divided into three subsets based on the number of documents from the same cluster appearing in the train set. The results for each subset can be found in Table <ref type="table" coords="13,418.08,114.06,8.20,10.91" target="#tab_8">10</ref>. It is obvious that YOLOv8 compared to baseline methods performed worse on the zero-shot subset as the relative decrease in score is greater for YOLOv8. On many-shot and few-shot subsets, the relative change for all models is similar. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Future Work</head><p>The YOLOv8 model proved to be useful for the extraction of information from documents, but we believe that there is still room for improvement. We have not sufficiently explored the possibilities of using synthetic data and unlabeled data for training, because our initial experiments did not show any significant differences while using them but the training was substantially longer. While pre-training models on unlabeled data is a crucial step in NLP tasks, it is not a common practice in object detection. However, there could be potential opportunities to explore and adapt this procedure for improving the performance of the object detection model as well.</p><p>Augmentations present another area with potential opportunities for improvement. We observed that geometric augmentations had a positive impact on model training. However, we used standard augmentations used in object detection. In the future, it may be beneficial to design augmentations specifically tailored for this task.</p><p>Last but not least the post-processing of the detected objects can be possibly improved given that we did not use any sophisticated methods. Especially in the LIR task, we observed a high number of false positive detections. The use of a filtration method based on a priori occurrences of different object classes in one table could potentially lead to better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Document information extraction using transformer-based models is a common approach, typically treating the task as an NLP problem. However, in our work, we demonstrated that the KILE and LIR tasks can be effectively addressed as object detection tasks using the CNN model.</p><p>We mainly focused on exploring various factors that can impact the training process, such as image size and different augmentations. Additionally, we successfully demonstrated that chargrid representation concatenated with input image is beneficial for training.</p><p>Furthermore, we compared the results of YOLOv8 with baseline methods, analyzed the results, and provided suggestions for future work. To evaluate the performance of our proposed approach, we compared the results of YOLOv8, with baseline methods. On the KILE task, YOLOv8 surpassed the baseline methods, achieving an 0.716 AP. However, for the LIR task, YOLOv8 did not outperform the baseline methods, the best achieved F1 score was 0.638.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,395.45,344.79,8.93;3,102.48,86.92,190.48,246.65"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of two documents with similar information but different layouts.</figDesc><graphic coords="3,102.48,86.92,190.48,246.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,98.08,372.14,3.65,5.24;4,95.67,370.58,10.91,6.98;4,106.58,372.90,88.24,9.57;4,194.82,367.80,5.76,6.98;4,200.58,372.17,305.40,10.91;4,89.29,387.91,416.69,10.91;4,89.29,401.45,416.69,10.91;4,89.02,415.00,162.99,10.91"><head>3 √︀</head><label>3</label><figDesc>𝑙𝑒𝑛(characters + 1) ⌉︁ . Then, we assigned each character value in the chosen base. At the first position in the set of characters, we added an empty character that represents the document background. Finally, we normalize the encoded values by the base value so that the encoded values fall within the range of ⟨0, 1⟩.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,89.29,656.94,258.41,8.96;4,166.88,466.50,121.58,158.23"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of chargrid representation of the document.</figDesc><graphic coords="4,166.88,466.50,121.58,158.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,89.29,107.54,416.69,10.91;5,89.29,121.08,416.69,10.91;5,89.29,134.63,416.69,10.91;5,88.98,148.18,418.21,10.91;5,88.89,161.73,417.09,10.91;5,89.29,175.28,253.61,10.91"><head></head><label></label><figDesc>Data augmentation is one of the most common practices used during the training of the model to prevent overfitting and improve its ability to generalize by increasing the variability of the input data while simulating real data examples that are missing from the training set. The original YOLOv8 implementation uses several commonly used augmentations. In our experiments, we explored the usage of some of these augmentations, in Figure3you can see examples of individual augmentations that were used in experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,89.29,461.27,211.34,8.93;5,160.33,326.49,78.06,101.64"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of the explored augmentations.</figDesc><graphic coords="5,160.33,326.49,78.06,101.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,89.29,372.89,416.69,8.93;8,89.29,384.89,35.86,8.87"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of performance of models with the different number of parameters on KILE and LIR task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="10,89.29,619.29,417.05,8.93;10,89.29,631.29,380.73,8.87;10,112.35,440.02,115.49,153.15"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of documents from the same layout clusters from different datasets. The first row represents documents from the cluster: 293 and second row documents from the cluster: 554.</figDesc><graphic coords="10,112.35,440.02,115.49,153.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="12,89.29,264.39,215.11,8.93;12,106.59,162.06,187.98,69.96"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example of excessive predictions in tables.</figDesc><graphic coords="12,106.59,162.06,187.98,69.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="12,89.29,452.61,319.47,8.93"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Example of miss-classified fields and excessive predictions in tables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,393.84,55.70"><head>Table 1</head><label>1</label><figDesc>Results of a model for different thresholds between line item field and LIR fields.</figDesc><table coords="4,112.44,119.83,370.39,26.36"><row><cell cols="2">Threshold 0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell></row><row><cell>F1</cell><cell cols="10">0.598 0.601 0.602 0.595 0.565 0.559 0.560 0.560 0.558 0.538</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,236.87,417.00,115.63"><head>Table 2</head><label>2</label><figDesc>Comparison of the influence of the two most beneficial augmentations. All models were evaluated on the validation set.</figDesc><table coords="7,324.57,278.22,87.04,8.87"><row><cell>KILE</cell><cell>LIR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,559.33,417.00,103.67"><head>Table 3</head><label>3</label><figDesc>Comparison of different training setups -using images, chargrids, or both. All models were evaluated on the validation set.</figDesc><table coords="7,321.94,600.68,87.04,8.87"><row><cell>KILE</cell><cell>LIR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,481.86,417.00,171.02"><head>Table 4</head><label>4</label><figDesc>Comparison of a number of parameters and FLOPs of the models. Values provided for YOLOv8 are calculated for a model with image input size 1280 × 1280 and values provided for RoBERTa and LayotLMv3 are with 512 tokens at the input. The number of parameters for RoBERTa and LayotLMv3 is in the format: model parameters + embedding parameters.</figDesc><table coords="8,206.80,549.38,181.68,103.51"><row><cell>Model</cell><cell cols="2">Parameters (M) FLOPs (B)</cell></row><row><cell>YOLOv8n</cell><cell>3.157</cell><cell>35.430</cell></row><row><cell>YOLOv8s</cell><cell>11.167</cell><cell>115.267</cell></row><row><cell>YOLOv8m</cell><cell>25.903</cell><cell>317.282</cell></row><row><cell>YOLOv8l</cell><cell>43.692</cell><cell>662.971</cell></row><row><cell>YOLOv8x</cell><cell>68.230</cell><cell>1034.189</cell></row><row><cell>RoBERTa</cell><cell cols="2">86.131 + 91.812 87.747</cell></row><row><cell cols="3">LayoutLMv3 87.402 + 91.812 123.407</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,88.99,167.72,417.00,91.57"><head>Table 5</head><label>5</label><figDesc>Comparison of the effect of input image size on the performance. All models were evaluated on the validation set.</figDesc><table coords="9,188.85,209.07,217.58,50.22"><row><cell></cell><cell>Image</cell><cell></cell><cell>KILE</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Size</cell><cell>AP</cell><cell>F1</cell><cell>AP</cell><cell>F1</cell></row><row><cell cols="2">YOLOv8n 640</cell><cell cols="4">0.476 0.600 0.334 0.557</cell></row><row><cell cols="2">YOLOv8n 1280</cell><cell cols="4">0.594 0.672 0.345 0.587</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,88.99,350.14,417.00,91.57"><head>Table 6</head><label>6</label><figDesc>Comparison of the effect of weight initialization on the results. All models were evaluated on the validation set.</figDesc><table coords="9,184.25,391.49,226.78,50.22"><row><cell></cell><cell></cell><cell></cell><cell>KILE</cell><cell></cell><cell>LIR</cell></row><row><cell>Model</cell><cell cols="2">Weights AP</cell><cell>F1</cell><cell>AP</cell><cell>F1</cell></row><row><cell cols="6">YOLOv8n Random 0.474 0.601 0.378 0.561</cell></row><row><cell cols="2">YOLOv8n COCO</cell><cell cols="4">0.513 0.632 0.395 0.576</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,88.99,208.03,416.99,145.04"><head>Table 8</head><label>8</label><figDesc>Comparison of results of baseline models presented in<ref type="bibr" coords="11,325.88,220.04,11.97,8.87" target="#b0">[1]</ref> and YOLOv8x on KILE &amp; LIR tasks on validation set.</figDesc><table coords="11,284.64,249.38,158.53,8.87"><row><cell>KILE</cell><cell>LIR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="12,88.99,538.36,418.53,135.31"><head>Table 9</head><label>9</label><figDesc>Comparison of results of baseline models presented in<ref type="bibr" coords="12,308.71,550.37,11.68,8.87" target="#b0">[1]</ref> and YOLOv8x on KILE &amp; LIR tasks on test set.</figDesc><table coords="12,284.64,569.98,158.53,8.87"><row><cell>KILE</cell><cell>LIR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="13,88.99,180.45,417.00,158.93"><head>Table 10</head><label>10</label><figDesc>Comparison of results on subsets of the validation set in the first half of the table and on subsets of the test set in the second half. Values in brackets indicate a relative change in the percentage of the corresponding metrics compared to the results on all documents. AP values are the results for the KILE task and F1 values are the results for the LIR task. The results with the largest relative increase or the smallest decrease are highlighted in bold.</figDesc><table coords="13,95.27,259.45,405.48,79.93"><row><cell></cell><cell>All</cell><cell></cell><cell cols="2">Many-shot</cell><cell cols="2">Few-shot</cell><cell cols="2">Zero-shot</cell></row><row><cell>Model</cell><cell>AP</cell><cell>F1</cell><cell>AP</cell><cell>F1</cell><cell>AP</cell><cell>F1</cell><cell>AP</cell><cell>F1</cell></row><row><cell>YOLOv8x</cell><cell cols="2">0.716 0.646</cell><cell>0.800 (+12)</cell><cell>0.703 (+9)</cell><cell>0.730 (+2)</cell><cell>0.613 (-5)</cell><cell>0.488 (-32)</cell><cell>0.384 (-41)</cell></row><row><cell>RoBERTa BASE+SYNTH</cell><cell cols="2">0.566 0.701</cell><cell>0.624 (+10)</cell><cell>0.792 (+13)</cell><cell>0.566 (+0)</cell><cell>0.608 (-13)</cell><cell>0.406 (-28)</cell><cell>0.465 (-34)</cell></row><row><cell>LayoutLMv3 OURS+SYNTH</cell><cell cols="2">0.532 0.681</cell><cell cols="2">0.800 (+50) 0.599 (-12)</cell><cell>0.524 (-2)</cell><cell>0.530 (-22)</cell><cell>0.365 (-31)</cell><cell>0.431 (-37)</cell></row><row><cell>YOLOv8x</cell><cell cols="2">0.680 0.597</cell><cell cols="2">0.814 (+20) 0.683 (+15)</cell><cell>0.646 (-5)</cell><cell>0.489 (-18)</cell><cell>0.393 (-42)</cell><cell>0.402 (-33)</cell></row><row><cell>RoBERTa BASE+SYNTH</cell><cell cols="2">0.539 0.698</cell><cell>0.615 (+14)</cell><cell>0.760 (+9)</cell><cell>0.499 (-7)</cell><cell>0.568 (-19)</cell><cell cols="2">0.384 (-29) 0.631 (-10)</cell></row><row><cell>LayoutLMv3 OURS+SYNTH</cell><cell cols="2">0.512 0.691</cell><cell>0.601 (+17)</cell><cell>0.773 (+12)</cell><cell>0.465 (-9)</cell><cell>0.538 (-22)</cell><cell>0.338 (-34)</cell><cell>0.586 (-15)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,92.57,671.03,210.32,8.97"><p>Implementation: https://github.com/ultralytics/ultralytics</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Computational resources were provided by the e-<rs type="projectName">INFRA CZ</rs> project (ID:<rs type="grantNumber">90254</rs>), supported by the <rs type="funder">Ministry of Education, Youth, and Sports of the Czech Republic</rs>. The work was supported by the <rs type="funder">University of West Bohemia</rs>, project No. <rs type="grantNumber">SGS-2022-017</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_KaHJSmh">
					<idno type="grant-number">90254</idno>
					<orgName type="project" subtype="full">INFRA CZ</orgName>
				</org>
				<org type="funding" xml:id="_26SCG8U">
					<idno type="grant-number">SGS-2022-017</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,112.66,339.71,394.53,10.91;14,112.66,353.26,393.32,10.91;14,112.66,366.81,394.53,10.91;14,112.66,380.36,393.53,10.91;14,112.66,393.91,103.82,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,226.61,353.26,279.37,10.91;14,112.66,366.81,45.25,10.91">DocILE Benchmark for Document Information Localization and Extraction</title>
		<author>
			<persName coords=""><forename type="first">Š</forename><surname>Šimsa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Uřičář</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kocián</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Skalický</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Coustaty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,182.79,366.81,324.39,10.91;14,112.66,380.36,53.03,10.91">17th International Conference on Document Analysis and Recognition, ICDAR 2021</title>
		<title level="s" coord="14,383.74,380.36,122.45,10.91;14,112.66,393.91,31.19,10.91">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>San José, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">August 21-26, 2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,407.46,394.53,10.91;14,112.66,421.01,393.32,10.91;14,112.66,434.55,394.53,10.91;14,112.66,448.10,393.33,10.91;14,112.66,461.65,393.33,10.91;14,112.66,475.20,267.69,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,227.92,421.01,278.06,10.91;14,112.66,434.55,64.76,10.91">Overview of DocILE 2023: Document Information Localization and Extraction</title>
		<author>
			<persName coords=""><forename type="first">Š</forename><surname>Šimsa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Uřičář</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kocián</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Skalický</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Coustaty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,375.83,448.10,130.16,10.91;14,112.66,461.65,258.02,10.91;14,403.07,461.65,102.92,10.91;14,112.66,475.20,233.99,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>LNCS Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="14,112.66,488.75,393.32,10.91;14,112.66,502.30,393.58,10.91;14,112.66,515.85,393.81,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,403.96,488.75,102.02,10.91;14,112.66,502.30,231.47,10.91">Cleval: Character-level evaluation for text detection and recognition tasks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,371.69,502.30,134.55,10.91;14,112.66,515.85,305.25,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="564" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,529.40,394.04,10.91;14,112.66,542.95,95.90,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jocher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<ptr target="https://github.com/ultralytics/ultralytics" />
		<title level="m" coord="14,263.79,529.40,92.78,10.91">YOLO by Ultralytics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,556.50,395.17,10.91;14,112.66,570.05,393.33,10.91;14,112.66,583.60,395.16,10.91;14,112.66,597.15,395.01,10.91;14,112.66,610.69,138.14,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,481.67,556.50,26.16,10.91;14,112.66,570.05,194.77,10.91">Chargrid: Towards understanding 2D documents</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Reisswig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brarda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Höhne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Faddoul</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1476</idno>
		<ptr target="https://aclanthology.org/D18-1476.doi:10.18653/v1/D18-1476" />
	</analytic>
	<monogr>
		<title level="m" coord="14,331.20,570.05,174.79,10.91;14,112.66,583.60,395.16,10.91;14,112.66,597.15,32.07,10.91">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4459" to="4469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,624.24,365.95,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><surname>Mindee</surname></persName>
		</author>
		<ptr target="https://github.com/mindee/doctr" />
		<title level="m" coord="14,151.05,624.24,147.22,10.91">doctr: Document text recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,637.79,393.33,10.91;14,112.66,651.34,107.17,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m" coord="14,238.15,637.79,182.94,10.91">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,86.97,394.53,10.91;15,112.66,100.52,393.33,10.91;15,112.66,114.06,394.52,10.91;15,112.66,127.61,123.33,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,112.66,100.52,200.81,10.91">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,341.73,100.52,164.26,10.91;15,112.66,114.06,93.50,10.91">Computer Vision-ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,141.16,394.53,10.91;15,112.30,154.71,393.68,10.91;15,112.66,168.26,107.17,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="15,173.53,154.71,256.77,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,181.81,393.33,10.91;15,112.66,195.36,393.33,10.91;15,112.66,208.91,161.25,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,289.23,181.81,216.76,10.91;15,112.66,195.36,133.13,10.91">Layoutlmv3: Pre-training for document ai with unified text and image masking</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,267.98,195.36,238.01,10.91;15,112.66,208.91,62.92,10.91">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4083" to="4091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,222.46,393.33,10.91;15,112.66,236.01,393.53,10.91;15,112.30,249.56,271.01,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,260.70,222.46,245.29,10.91;15,112.66,236.01,128.47,10.91">PubTables-1M: Towards comprehensive table extraction from unstructured documents</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Smock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pesala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,264.50,236.01,241.69,10.91;15,112.30,249.56,172.53,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4634" to="4642" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
