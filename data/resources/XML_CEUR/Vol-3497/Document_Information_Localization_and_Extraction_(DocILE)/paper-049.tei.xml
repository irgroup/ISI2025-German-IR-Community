<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,367.69,15.42;1,89.29,106.66,315.04,15.42">Extended Overview of DocILE 2023: Document Information Localization and Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,64.66,11.96"><forename type="first">Štěpán</forename><surname>Šimsa</surname></persName>
							<email>stepan.simsa@rossum.ai</email>
						</author>
						<author>
							<persName coords="1,178.11,134.97,67.05,11.96"><forename type="first">Michal</forename><surname>Uřičář</surname></persName>
							<email>michal.uricar@rossum.ai</email>
						</author>
						<author>
							<persName coords="1,269.30,134.97,51.78,11.96"><forename type="first">Milan</forename><surname>Šulc</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Second Foundation</orgName>
								<address>
									<addrLine>Na Florenci 15</addrLine>
									<postCode>110 00</postCode>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,333.72,134.97,49.91,11.96"><forename type="first">Yash</forename><surname>Patel</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Visual Recognition Group</orgName>
								<orgName type="institution">CTU in Prague</orgName>
								<address>
									<addrLine>Karlovo náměstí 13</addrLine>
									<postCode>121 35</postCode>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,396.28,134.97,71.58,11.96"><forename type="first">Ahmed</forename><surname>Hamdi</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of La Rochelle</orgName>
								<address>
									<addrLine>23 Avenue Albert Einstein</addrLine>
									<postCode>17031</postCode>
									<settlement>La Rochelle</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,64.64,11.96"><forename type="first">Matěj</forename><surname>Kocián</surname></persName>
						</author>
						<author>
							<persName coords="1,166.58,148.92,79.72,11.96"><forename type="first">Matyáš</forename><surname>Skalický</surname></persName>
						</author>
						<author>
							<persName coords="1,258.94,148.92,47.16,11.96"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Visual Recognition Group</orgName>
								<orgName type="institution">CTU in Prague</orgName>
								<address>
									<addrLine>Karlovo náměstí 13</addrLine>
									<postCode>121 35</postCode>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.75,148.92,77.64,11.96"><forename type="first">Antoine</forename><surname>Doucet</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of La Rochelle</orgName>
								<address>
									<addrLine>23 Avenue Albert Einstein</addrLine>
									<postCode>17031</postCode>
									<settlement>La Rochelle</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,409.04,148.92,85.40,11.96"><forename type="first">Mickaël</forename><surname>Coustaty</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of La Rochelle</orgName>
								<address>
									<addrLine>23 Avenue Albert Einstein</addrLine>
									<postCode>17031</postCode>
									<settlement>La Rochelle</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,113.26,162.87,106.46,11.96"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Computer Vision Center</orgName>
								<orgName type="institution">Universitat Autónoma de Barcelona</orgName>
								<address>
									<addrLine>Cerdanyola del Vallès</addrLine>
									<postCode>08193</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>1 Rossum, Křižíkova 148/34</addrLine>
									<postCode>186 00</postCode>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,367.69,15.42;1,89.29,106.66,315.04,15.42">Extended Overview of DocILE 2023: Document Information Localization and Extraction</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">C835F83C976D0160B4A3A723BB9483A3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Extraction</term>
					<term>Computer Vision</term>
					<term>Natural Language Processing</term>
					<term>Optical Character Recognition</term>
					<term>Document Understanding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper provides an overview of the DocILE 2023 Competition, its tasks, participant submissions, the competition results and possible future research directions. This first edition of the competition focused on two Information Extraction tasks, Key Information Localization and Extraction (KILE) and Line Item Recognition (LIR). Both of these tasks require detection of pre-defined categories of information in business documents. The second task additionally requires correctly grouping the information into tuples, capturing the structure laid out in the document. The competition used the recently published DocILE dataset and benchmark that stays open to new submissions. The diversity of the participant solutions indicates the potential of the dataset as the submissions included pure Computer Vision, pure Natural Language Processing, as well as multi-modal solutions and utilized all of the parts of the dataset, including the annotated, synthetic and unlabeled subsets. This is an extended version of the condensed overview paper <ref type="bibr" coords="1,190.11,390.47,9.39,8.97" target="#b0">[1]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Documents, such as invoices, purchase orders, contracts, and financial statements, are a major form of communication between businesses. Extraction of the key information from such</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data</head><p>The competition was based on the DocILE <ref type="bibr" coords="2,279.88,403.03,17.91,10.91" target="#b23">[24]</ref> dataset of business documents, which consists of three distinct subsets: annotated, unlabeled, and synthetic. The annotated set comprises 6, 680 real business documents sourced from publicly available platforms, which have been carefully annotated. The unlabeled set consists of a massive collection of 932, 467 real business documents also obtained from publicly available sources, intended for unsupervised pre-training purposes. The dataset draws its documents from two public data sources: UCSF Industry Documents Library <ref type="bibr" coords="2,126.47,484.32,18.07,10.91" target="#b25">[26]</ref> and Public Inspection Files (PIF) <ref type="bibr" coords="2,300.17,484.32,16.41,10.91" target="#b26">[27]</ref>. UCSF Industry Documents Library is a digitalized archive of documents created by industries that impact public health, while PIF consists of public files of American broadcast stations, specifically focusing on political campaign ads. The documents were retrieved in a PDF format, and various selection criteria were applied to ensure the quality and relevance of the dataset. The synthetic set comprises 100, 000 documents generated using a proprietary document generator. These synthetic documents are designed to mimic the layout and structure of 100 fully annotated real business documents from the annotated set.</p><p>Participants were allowed to use the 5, 180 training samples, 500 validation samples and the full synthetic and unlabeled dataset. The remaining 1, 000 documents form the test set. Usage of external document datasets or models pre-trained on such datasets was forbidden in the competition, while datasets and pre-trained models from other domains -such as images from ImageNet <ref type="bibr" coords="2,135.14,646.91,17.91,10.91" target="#b27">[28]</ref> or texts from BooksCorpus <ref type="bibr" coords="2,278.65,646.91,17.91,10.91" target="#b28">[29]</ref> -were allowed.</p><p>For each document, the dataset contains the original PDF file and OCR pre-computed using the DocTR <ref type="bibr" coords="3,138.61,86.97,17.75,10.91" target="#b29">[30]</ref> library achieving excellent recognition scores in <ref type="bibr" coords="3,371.04,86.97,16.08,10.91" target="#b30">[31]</ref>. Annotations are provided for documents in the annotated and synthetic sets and include field annotations for the two competition tasks, KILE and LIR, as well as additional metadata: original source of the document, layout cluster ID <ref type="foot" coords="3,163.17,125.86,3.71,7.97" target="#foot_0">1</ref> , table grid annotation, document type, currency, page count and page image sizes. Annotations for the test set are not publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Tasks and Evaluation</head><p>The competition had two tracks, one for each of the two tasks, KILE and LIR, respectively. The goal of both of these tasks is to detect semantic fields in the document, i.e., for each category (field type) localize all the text boxes that have this semantic meaning and extract the corresponding text. For LIR, fields have to be additionally grouped into Line Items, i.e., tuples representing a single item. For a more formal definition, refer to <ref type="bibr" coords="3,370.88,253.99,16.09,10.91" target="#b24">[25]</ref>, where the tasks were first defined. An example document with annotations for KILE and LIR is illustrated in Figure <ref type="figure" coords="3,487.91,267.54,3.74,10.91" target="#fig_0">1</ref>.</p><p>The DocILE benchmark is hosted on the Robust Reading Challenge portal <ref type="foot" coords="3,432.67,279.33,3.71,7.97" target="#foot_1">2</ref> . As the test set annotations remain private, the only way to compare the solutions on the test set is to make a submission to the benchmark. During the competition, participants did not see the results or even their own score, so they had to select the final solution without gathering any info about the test set.</p><p>To focus the competition on the most important part of the two tasks, which is the semantic understanding of the values in the documents, only the localization part was evaluated. This means the tasks can be framed as object detection tasks, with LIR additionally requiring the grouping of the detected objects into Line Items. Therefore, standard object detection metrics are employed, with Average Precision (AP) as the main metric for KILE and F1 as the main metric for LIR. A predicted and a ground truth field are matching if they have the same field type and if they cover the same text in the document, as explained in detail in Figure <ref type="figure" coords="3,463.03,430.13,3.67,10.91" target="#fig_1">2</ref>. For LIR the fields also need to belong to corresponding Line Items, where this correspondence is found with a matching that maximizes the total number of matched fields, as shown in Figure <ref type="figure" coords="3,480.34,457.22,3.74,10.91">3</ref>.</p><p>Extracting the text of the localized fields is an obvious extension of the two tasks whose precision is also important. Therefore, both tracks in the benchmark have a separate leaderboard, where the extracted text is compared with the annotated text for each matched field pair and an exact match is required to count the pair as a true positive pair.</p><p>The benchmark also contains additional leaderboards for zero-shot, few-shot and many-shot evaluation. This is the same evaluation as in the main leaderboard but evaluated only on a subset of the test documents. Specifically, it is evaluated on documents from layout clusters that have zero (zero-shot), one to three (few-shot) or four and more (many-shot) samples available for training (i.e., in the training or validation set). These test subsets contain roughly 250, 250 and 500 documents, respectively. This enables a more detailed analysis of the methods and helps to understand which methods generalize better to new document layouts and which can better overfit to clusters with many examples available for training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Submissions</head><p>The competitions received contributions from 5 teams for the KILE task and 4 teams in the LIR task. See Figure <ref type="figure" coords="4,180.01,504.22,5.06,10.91">4</ref> to compare this with the number of dataset downloads and competition registrations. We briefly present all the submitted methods in an alphabetical order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">GraphDoc -USTC-iFLYTEK, China</head><p>The team from the University of Science and Technology of China and iFLYTEK AI Research, China submitted a method <ref type="bibr" coords="4,209.99,581.04,16.23,10.91" target="#b32">[33]</ref>, which jointly solves both KILE and LIR tasks. Their approach is based on an ensemble of a modified GraphDoc <ref type="bibr" coords="4,312.77,594.59,18.05,10.91" target="#b10">[11]</ref> tailored for the purpose of the DocILE competition, pre-trained on the DocILE unlabeled set and consequently fine-tuned on the training set. Both competition tasks are handled like Named Entity Recognition (NER), followed by a special Merger module, which operates on the attention layer from the GraphDoc model and the merging strategy is therefore learned, unlike in the baseline method. The authors noticed the inherent nature of the KILE and LIR task and exploited it naturally -the word A predicted field matches the location of a ground truth field if their bounding boxes cover the same text. More precisely, the fields must contain exactly the same Pseudo-Character Centers defined in 2a. Note: in 2b, only one of the predictions would be considered correct if all three boxes were predicted. Images are taken from <ref type="bibr" coords="5,249.36,355.21,14.92,8.87" target="#b31">[32]</ref>.</p><p>tokens are merged to instances by the first level Merger module and then the second Merger module operates on these instances for the line item classes and merges them into final line items. The proposed method still uses some level of a rule-based post-processing, which is based on the observation of data: 1) some field annotations contain only part of the detected text boxes from DocTR and need to be manually split (such as currency_code_amount_due fields that usually contains only the symbol '$'); 2) some symbols are frequently detected as part of the OCR word box, but excluded from the annotations (such as the symbol '#'); 3) Text boxes that are far apart rarely belong to the same instance, or to the same line item.</p><p>Besides the contribution on the model side, the authors also devoted some effort to improve the OCR detections provided, by removing the detections with low confidence and by running DocTR <ref type="bibr" coords="5,124.13,528.26,18.07,10.91" target="#b29">[30]</ref> on scaled-up images (1.25×, 1.5×, and 1.75×) and aggregating the found text boxes to improve the recall of the OCR detections. The OCR detections are also re-ordered, similarly as in the baseline methods, in the top-down left-right reading order.</p><p>Since the proposed method uses multi-modal input (text, layout, vision), we can put it into a category of combination of NLP and CV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">LiLT -University of Information Technology, Vietnam</head><p>The team from University of Information Technology, Vietnam submitted a method based on the baselines with a layout-aware backbone LiLT <ref type="bibr" coords="5,319.77,645.73,16.42,10.91" target="#b33">[34]</ref>. The authors decided to re-split the provided dataset to 80% for training and 20% for validation (original ratio was 90% and 10%, Based on the feedback from a few of these teams, we attribute this to the tight schedule and to the competitiveness of the baselines, as they were not so easy to beat.</p><p>respectively), arguing that the original split was leading to a poor generalization. Another contribution was filtering out low-confident OCR detections. There is no mention of the usage of either the synthetic or the unlabeled sets of the DocILE dataset in the manuscript. Unfortunately, despite competing in both KILE and LIR tasks, the authors submitted a manuscript describing only the solution for the LIR task. Since the backbone LiLT uses a combination of text and layout input, we categorize it as a pure NLP solution.</p><p>The review process of the authors' manuscript discovered a violation of the benchmark rules due to the usage of the prohibited pre-trained checkpoint for the LiLT backbone. The authors used the checkpoint from training on the IIT-CDIP <ref type="bibr" coords="7,320.00,86.97,17.98,10.91" target="#b34">[35]</ref> dataset, which is a document dataset. Therefore we had to remove this method from the official leaderboard of the competition and the benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Union-RoBERTa -University of Information Technology, Vietnam</head><p>The team from University of Information Technology, Vietnam submitted a method <ref type="bibr" coords="7,458.38,163.75,17.79,10.91" target="#b35">[36]</ref> which is heavily based on the provided baselines. Their method, coined as Union-RoBERTa, is an ensemble of two provided baselines <ref type="bibr" coords="7,255.27,190.84,18.07,10.91" target="#b23">[24]</ref> with a plain RoBERTa trained from scratch on the synthetic and training data using Fast Gradient Method. They use the affirmative strategy for the ensemble (hence the Union in the name) and follow it by an additional merging of fields based on distance with a threshold tuned on the validation set. This ensemble is then used to generate pseudo-labels for 10, 000 samples from the unlabeled set which are then used for additional pre-training of the three models followed by an additional training on the training set. Although there is not much novelty in the proposed method, it is a nice example how well-established practices can yield significant improvements.</p><p>The proposed method participated in the KILE task only. Since the method is based on RoBERTa models, we put it into a pure NLP category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">ViBERTGrid -Ricoh Software Research Center, China</head><p>The team from Ricoh Software Research Center, China submitted a method based on token classification with ViBERTGrid <ref type="bibr" coords="7,235.06,376.02,16.41,10.91" target="#b36">[37]</ref>, followed by a distance-based merging procedure. The team participated in both KILE and LIR tasks. However, the results were below baselines for both tasks and the authors decided not to submit a manuscript with further details. We can only guess, based on the provided description with ViBERTGrid, that the method was a combination of NLP and CV.</p><p>We noticed that the method probably suffers from not using the adequate score (all detections were using the same score 1.0) which could explain why AP is significantly lower compared to the other methods, while F1 measure on the KILE task is in the middle of the ranking, as seen in Figure <ref type="figure" coords="7,131.96,484.41,10.06,10.91" target="#fig_4">5a</ref> and discussed more in Section 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">YOLOv8 -University of West Bohemia, Czech Republic</head><p>The team from University of West Bohemia, Czech Republic submitted a method <ref type="bibr" coords="7,446.65,534.09,17.81,10.91" target="#b37">[38]</ref> based on the combination of YOLOv8 <ref type="bibr" coords="7,218.68,547.64,18.04,10.91" target="#b38">[39]</ref> and CharGrid <ref type="bibr" coords="7,304.42,547.64,18.04,10.91" target="#b39">[40]</ref> with modifications, such as splitting the word boxes to pseudo-characters, not using the one number encoding of a character directly but a three numbers encoding instead, and concatenating the image with the CharGrid representation. The authors did not leverage synthetic nor unlabeled parts of the dataset, but they used augmentations during training. Due to the faster training procedure, they decided to use just random translation for augmentation, even though the best results in ablation study were observed when mosaicking was applied. The method works quite well on the KILE task (where it even achieves the highest F1) but falls behind on the LIR task. The latter is attributed to the increased number of false positive detections.</p><p>This contribution is purely based on computer vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head><p>The results for the KILE and LIR tasks, including the baselines from the DocILE dataset paper <ref type="bibr" coords="8,487.07,111.28,16.09,10.91" target="#b23">[24]</ref>, are displayed in Figures <ref type="figure" coords="8,204.39,124.83,10.26,10.91" target="#fig_4">5a</ref> and<ref type="figure" coords="8,239.19,124.83,8.74,10.91" target="#fig_4">5b</ref>, respectively. We can see that while on the KILE task participants approaches clearly outperform the provided baseline by a large margin on the main evaluation metric (AP), on the LIR task, there is not such a big improvement, except for the GraphDoc based approach. The baseline methods are marked with ⊟ symbol. Interestingly, for the KILE task, the secondary metric (F1) does not seem to be correlated with the primary metric (AP) and several of the methods, including the baselines, are comparatively much better on F1 than on AP. In fact, the YOLOv8 based approach outperforms the otherwise winning GraphDoc in F1 metric. This might be related to the fact that AP takes into account the score assigned to individual predictions, while F1 does not, and that some teams focused on assigning good scores to predictions more than others, as discussed in Section 5.5.</p><p>In the LIR task, there is some correlation between the primary metric, which in this task is F1, and the secondary metric (AP), with a slight violation for the GraphDoc based method.</p><p>Considering the achieved metric values, we can say that the DocILE benchmark poses very challenging tasks, because the best results on both KILE and LIR tasks are below 80% of the respective quality metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Text Extraction Evaluation</head><p>Figure <ref type="figure" coords="8,119.84,363.33,4.98,10.91" target="#fig_7">6</ref> summarizes the results when text extractions are checked in the evaluation. Note, that this was intentionally not done in the main evaluation, which focuses more on the localization part, so that participants do not have to focus on optimizing the OCR solution for text read out. However, in a real-world system, this would likely be the main metric for evaluation and therefore we present results of all of the competing methods when this strict text comparison is employed. By definition, all methods are performing worse on both KILE and LIR task, compared to the main localization-only evaluation. Also both AP and F1 metrics show less variance for all competing methods. Unfortunately, the YOLOv8 based method did not provide the text outputs (which was not required for the competition), so we cannot evaluate this method properly.</p><p>The KILE task, summarized in Figure <ref type="figure" coords="8,268.61,485.28,8.33,10.91" target="#fig_7">6a</ref>, shows that the GraphDoc still outperforms all the other competitors. However, the margin is not as big as in the final evaluation.</p><p>The LIR task is summarized in Figure <ref type="figure" coords="8,269.68,512.37,8.60,10.91" target="#fig_7">6b</ref>. Surprisingly, the GraphDoc based method, which was winning in the main evaluation, and which kept its position for the KILE task, is now lagging behind quite significantly. We believe this might be attributed to the lack of effort invested to the text read-out after merging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation on zero/few/many-shot layouts</head><p>In this section, we present a break-down of the evaluation with respect to the document layouts seen/unseen during training, hence providing hints about how the particular method generalizes. We have three distinct categories for this evaluation: 1) zero-shot, formed by document layouts that were not in the training nor validation sets; 2) few-shot, which is formed by document layouts that have 1-3 samples in the training and validation subset of the DocILE dataset; 3) many-shot, with 4 or more samples in the training and validation subset.     In Figure <ref type="figure" coords="11,143.24,86.97,3.74,10.91" target="#fig_10">7</ref>, we show the results of the first category -zero-shot. For the KILE task (Figure <ref type="figure" coords="11,89.04,100.52,7.99,10.91" target="#fig_10">7a</ref>), we can see that GraphDoc is still a clear winner with a relatively high margin. However, interestingly, YOLOv8 performs much worse, compared to the overall results. This might be attributed to the fact that this method did not leverage the unlabeled part of the DocILE dataset and therefore is more prone to overfitting. The RoBERTa baseline performs better than RoBERTa with supervised pre-training on synthetic data, which might be caused by the fact that synthetic documents are based on selected layouts from the training set and these layouts are not present in the zero-shot test subset, although we do not see the same effect in the case of LayoutLMv3 or the LIR task. Union-RoBERTa gets to the second place; considering it is basically an ensemble of the baselines, this might be an indicator that ensembling can also improve generalization properties. It is also worth mentioning that ViBERTGrid is very good in generalization when the F1 measure is concerned.</p><formula xml:id="formula_0" coords="9,139.81,259.25,308.62,85.29">G r a p h D o c [ 3 3 ] Y O L O v 8 [ 3 8 ] U n i o n -R o B E R T a [ 3 6 ] L i L T ⊟ R o B E R T a + s y n t h ⊟ R o B E R T a ⊟ L a y o u t L M v 3 + s y n t h ⊟ L a y o u t L M v 3 V i B E R T G r i d<label>45</label></formula><formula xml:id="formula_1" coords="9,138.83,524.04,304.68,93.22">G r a p h D o c [ 3 3 ] L i L T ⊟ R o B E R T a + s y n t h ⊟ L a y o u t L M v 3 + s y n t h ⊟ R o B E R T a ⊟ L a y o u t L M v 3 Y O L O v 8 [ 3 8 ] V i B E R T G r i d<label>20</label></formula><formula xml:id="formula_2" coords="10,157.63,289.66,288.34,60.86">U n i o n -R o B E R T a [ 3 6 ] ⊟ R o B E R T a + s y n t h L i L T ⊟ R o B E R T a ⊟ L a y o u t L M v 3 + s y n t h ⊟ L a y o u t L M v 3 V i B E R T G r i d Y O L O v 8 [<label>3 8</label></formula><formula xml:id="formula_3" coords="10,166.95,562.37,274.09,60.86">L i L T ⊟ R o B E R T a + s y n t h ⊟ L a y o u t L M v 3 + s y n t h ⊟ R o B E R T a ⊟ L a y o u t L M v 3 G r a p h D o c [ 3 3 ] V i B E R T G r i d Y O L O v 8 [<label>3 8</label></formula><p>The LIR task (Figure <ref type="figure" coords="11,189.43,249.56,8.95,10.91" target="#fig_10">7b</ref>) shows similar results -GraphDoc remains on the first place, LiLT lost its second position to RoBERTa with supervised pre-training on synthetic data and LayoutLMv3 baseline pre-trained on synthetic data swapped its position with RoBERTa baseline. Note, that for both tasks, the results are significantly worse for the zero-shot setup compared to the overall results, showing a room for improvement with respect to generalization of all competing methods.</p><p>The results of the few-shot evaluation are in Figure <ref type="figure" coords="11,342.40,330.85,3.81,10.91" target="#fig_12">8</ref>. The KILE task (Figure <ref type="figure" coords="11,460.40,330.85,9.05,10.91" target="#fig_12">8a</ref>) shows that only a few similar layouts during training can help significantly. We see, that YOLOv8 gets back to the second place, RoBERTa+synth baseline improves significantly. It is also worth mentioning that all methods improve both the AP and F1 metrics by roughly 10%, compared to the zero-shot setup, with some exceptions with even a better improvement, and ViBERTGrid, which has a lower improvement.</p><p>In the LIR task (Figure <ref type="figure" coords="11,200.02,412.15,7.89,10.91" target="#fig_12">8b</ref>), we can see that all methods get closer to each other, similarly as it was in the overall evaluation. However, what is really surprising is that the results for zero-shot variant were actually slightly better than the results for few-shot. Also, the LiLT benefits from seeing at least a few similar layouts during training much more than GraphDoc and overtakes its first position. Also RoBERTa baseline is slightly better than RoBERTa+synth.</p><p>In Figure <ref type="figure" coords="11,142.75,479.89,3.70,10.91" target="#fig_14">9</ref>, we show the results for the many-shot scenario. For the KILE task (Figure <ref type="figure" coords="11,481.38,479.89,7.77,10.91" target="#fig_14">9a</ref>), it can be seen that the order of competing methods converges to the same one as for the overall results, with the only exception of LayoutLMv3 and LayoutLMv3+synth baselines, which are swapped. We can also see, that the results are roughly 10% better than for the overall case, which is not surprising, since the overall case contains also unseen layout examples. For the LIR task (Figure <ref type="figure" coords="11,182.64,547.64,8.21,10.91" target="#fig_14">9b</ref>), we see a similar trend, but the improvement is not that significant. Interestingly, the LayoutLMv3+synth baseline gets to the second place outperforming both LiLT and RoBERTa+synth baselines. However, we should point out that the results of these methods are very close.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Breakdown based on document source</head><p>The number of documents of each layout cluster in the training, validation and unlabeled subsets are depicted in Figure <ref type="figure" coords="11,191.28,651.56,10.35,10.91" target="#fig_0">13</ref> and Figure <ref type="figure" coords="11,255.97,651.56,10.35,10.91" target="#fig_20">14</ref> for the UCSF and PIF document source type subsets, respectively.       The distribution of the number of document pages in the training, validation and unlabeled sets are depicted in Figure <ref type="figure" coords="15,210.89,100.52,10.35,10.91" target="#fig_17">11</ref> and Figure <ref type="figure" coords="15,275.31,100.52,10.35,10.91" target="#fig_1">12</ref> for the UCSF and PIF document sources subsets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G r a p h D o c [ 3 3 ]</head><formula xml:id="formula_4" coords="12,157.63,284.02,290.80,60.86">U n i o n -R o B E R T a [ 3 6 ] ⊟ R o B E R T a Y O L O v 8 [ 3 8 ] L i L T ⊟ R o B E R T a + s y n t h ⊟ L a y o u t L M v 3 + s y n t h ⊟ L a y o u t L M v 3 V i B E R T G r i d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G r a p h D o c [ 3 3 ]</head><formula xml:id="formula_5" coords="12,138.83,539.10,304.68,78.50">⊟ R o B E R T a + s y n t h L i L T ⊟ R o B E R T a ⊟ L a y o u t L M v 3 + s y n t h Y O L O v 8 [ 3 8 ] ⊟ L a y o u t L M v 3 V i B E R T G r i d<label>10</label></formula><formula xml:id="formula_6" coords="13,140.07,243.43,308.36,101.11">Y O L O v 8 [ 3 8 ] U n i o n -R o B E R T a [ 3 6 ] ⊟ R o B E R T a + s y n t h L i L T ⊟ R o B E R T a ⊟ L a y o u t L M v 3 + s y n t h ⊟ L a y o u t L M v 3 V i B E R T G r i d<label>40</label></formula><formula xml:id="formula_7" coords="13,138.83,533.37,304.68,83.88">L i L T G r a p h D o c [ 3 3 ] ⊟ R o B E R T a ⊟ R o B E R T a + s y n t h ⊟ L a y o u t L M v 3 + s y n t h Y O L O v 8 [ 3 8 ] ⊟ L a y o u t L M v 3 V i B E R T G r i d<label>10</label></formula><formula xml:id="formula_8" coords="14,177.98,283.68,270.45,60.86">Y O L O v 8 [ 3 8 ] U n i o n -R o B E R T a [ 3 6 ] L i L T ⊟ R o B E R T a + s y n t h ⊟ R o B E R T a ⊟ L a y o u t L M v 3 ⊟ L a y o u t L M v 3 + s y n t h V i B E R T G r i d</formula><formula xml:id="formula_9" coords="14,138.83,535.46,304.68,81.80">⊟ L a y o u t L M v 3 + s y n t h L i L T ⊟ R o B E R T a + s y n t h ⊟ R o B E R T a ⊟ L a y o u t L M v 3 Y O L O v 8 [ 3 8 ] V i B E R T G r i d<label>20</label></formula><p>Figure <ref type="figure" coords="15,132.59,127.61,10.35,10.91" target="#fig_16">10</ref> depicts the breakdown of results based on the document source type and also in combination with zero/few/many-shot layout analysis for both KILE and LIR tasks of the winning solution <ref type="bibr" coords="15,169.61,154.71,16.41,10.91" target="#b32">[33]</ref>. From the graphs, we can see that documents from the PIF source are posing a bigger problem to the method which is interesting since UCSF has bigger variance in the number of different layouts. This noticable difference might be attributed to the fact that PIF documents are more frequently multi-paged. There might be some non-trivial changes in document layout thanks to the transition from one page to another, especially when tables are concerned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Using synthetic and unlabeled data</head><p>According to the submitted participant papers, only GraphDoc and partly also Union-RoBERTa (they used only 10, 000 samples) leveraged the unlabeled part of the DocILE dataset. We believe that the reason for not using the unlabeled data was mainly relatively tight time constraints. It is visible that GraphDoc-based method wins in almost all comparisons with the exception of the few-shot (Figure <ref type="figure" coords="15,180.38,326.38,8.95,10.91" target="#fig_12">8b</ref>) and text extraction (Figure <ref type="figure" coords="15,314.97,326.38,8.95,10.91" target="#fig_7">6b</ref>) LIR tasks. However, it is hard to judge if this could be attributed to the usage of the unlabeled data.</p><p>Only the authors of Union-RoBERTa report the usage of the synthetic part of the DocILE dataset. Again, the reason for not using the provided synthetic data might be time constraints. From the baselines point of view, we see that using the synthetic data helps in most situations, with a few exceptions like the zero-shot KILE task (Figure <ref type="figure" coords="15,367.11,394.13,9.05,10.91" target="#fig_10">7a</ref>) and the few-shot LIR task (Figure <ref type="figure" coords="15,123.44,407.68,8.09,10.91" target="#fig_12">8b</ref>), where RoBERTa performs better than RoBERTa+synth. However, simultaneously, the LayoutLMv3+synth outperforms LayoutLMv3. But we should point out that in these cases the differences are not very big.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Importance of score for Average Precision</head><p>While for the F1 metric the score assigned to individual predictions is ignored, it plays an important role for the AP metric. In AP, predicted fields are first sorted by the score, then the precision-recall pairs are computed iteratively and finally the metric itself is the average precision achieved for different recall thresholds. Therefore, if we can ensure that there are more true positives among the predictions with higher score than among the predictions with lower score, the precision will increase for lower recall thresholds and remain similar for higher recall thresholds, when compared to the case when scores are random.</p><p>To prove this point, we can look at two examples. The ViBERTGrid method used the same score for all predictions and it achieves very poor results on AP compared to its results on F1, as can be seen in Figure <ref type="figure" coords="15,195.28,606.45,3.66,10.91" target="#fig_4">5</ref>. On the other hand, in the participant paper of the GraphDoc method, they argue that the prediction score is important for the AP metric and they show that by using a carefully selected score they achieve a 13.6% higher result on AP on the validation set compared to using the same score for all predictions. We can see in Figure <ref type="figure" coords="15,422.97,647.09,5.08,10.91" target="#fig_4">5</ref> that for the KILE task GraphDoc has the smallest difference between the AP and F1 metrics of all the methods.       This is not the case for LIR, maybe because here AP was not the main evaluation metric and so less focus might have been given to assigning a correct score to the predictions in this case.</p><p>Since there is a noticeable difference between the behaviour of the AP and F1 metrics, benchmark submissions are allowed to mark some fields with the flag use_only_for_ap to include it only for the AP computation, while excluding it from the F1 computation. Unfortunately, no submission have utilized this feature so we cannot evaluate its effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Potential gain from hand-crafted post-processing rules</head><p>In the GraphDoc <ref type="bibr" coords="19,168.57,204.44,18.06,10.91" target="#b32">[33]</ref> submission the authors use multiple post-processing rules to mitigate some common errors. Two of these rules deal with the problem that granularity of the input OCR words is not always good enough for some specific field types:</p><p>($) When a word box has a predicted field type currency_code_amount_due and contains the symbol '$', return just the value '$' and split the bounding box. (#) For field types with the suffix _id, when the predicted text starts with the symbol '#', remove this symbol and split the bounding box.</p><p>In this section, we explore what is the potential impact of such heuristic rules and verify whether we see a gap between GraphDoc <ref type="bibr" coords="19,282.05,338.10,18.07,10.91" target="#b32">[33]</ref> and the other methods on the impacted field types. In the following analysis, we will consider a method that uses the OCR provided with the dataset and that creates final fields by taking a union of bounding boxes of several of the input OCR word (their snapped version). Although this is not true for YOLOv8 <ref type="bibr" coords="19,427.46,378.75,18.07,10.91" target="#b37">[38]</ref> that predicts bounding boxes directly, the other methods more or less follow these criteria.</p><p>First, let us generalize the two rules above. We say a field is a split field if there is no word token that matches the field location, i.e., that covers the same set of Pseudo-Character-Centers (PCCs) as defined in Figure <ref type="figure" coords="19,209.17,432.94,3.66,10.91" target="#fig_1">2</ref>, and if there exists a word token that covers a superset of the PCCs covered by the field. The number of split fields for each field type in the training and validation sets are listed in Tables <ref type="table" coords="19,192.24,460.04,4.97,10.91" target="#tab_1">1</ref> and<ref type="table" coords="19,218.47,460.04,3.66,10.91" target="#tab_2">2</ref>. For KILE, a total of 7.5 % of fields are split fields, while for LIR it is 3.3 % of all fields. Since handling these cases usually affects both precision and recall, it has the potential to improve the final metrics by several percentage points. Now let us focus specifically on the rules ($) and (#) listed above. We say a split field follows the rule ($) if its text is equal to just the symbol '$' and it is covered by a word that contains this symbol in its text. In the training and validation set, the affected field types are only currency_code_amount_due and line_item_currency as shown in Table <ref type="table" coords="19,473.96,541.34,3.81,10.91" target="#tab_3">3</ref>. This represents 4.9 % of all KILE fields and less than 0.1 % of all LIR fields.</p><p>We say a split field follows the rule (#) if there is a word covering this field that has exactly the same text with an additional symbol '#' prepended at the beginning. The number of split fields satisfying the rule (#) for each affected field type is listed in Table <ref type="table" coords="19,390.04,595.53,3.81,10.91" target="#tab_4">4</ref>. In total, this represents 0.5 % of all KILE fields and less than 0.1 % of all LIR fields and as noticed by GraphDoc <ref type="bibr" coords="19,486.67,609.08,16.41,10.91" target="#b32">[33]</ref>, most of these fields have a type with the suffix _id.</p><p>Let us now verify whether we see the impact of the GraphDoc <ref type="bibr" coords="19,377.65,636.18,17.83,10.91" target="#b32">[33]</ref> post-processing rules on the test set predictions. In Figure <ref type="figure" coords="19,240.76,649.73,10.31,10.91" target="#fig_22">15</ref> we compare all of the methods on the whole test subset, on the currency_code_amount_due field type (only for KILE) and on field types with the suffix _id. As expected, we see that rule ($) gives GraphDoc a big edge over most of the other methods. Exceptions are YOLOv8 <ref type="bibr" coords="20,240.13,536.75,16.18,10.91" target="#b37">[38]</ref>, which does not have the same limitations connected to the OCR input, and ViBERTGrid, which also demonstrates decent performance on this field type, but the reasons behind its success are unknown to us, as we have not received a paper describing this method in more detail. For (#) we do not see GraphDoc outperforming the other methods (when compared to the results on all field types) on either of the two tasks, which matches the observations from the analysis on the training and validation set above. From these results it is apparent that the small trick of extracting just the symbol '$' out of the word boxes predicted to have the class currency_code_amount_due, pushed the GraphDoc <ref type="bibr" coords="20,139.16,645.15,18.07,10.91" target="#b32">[33]</ref> results on KILE several percentage points up compared to most of the other methods.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented the first edition of the DocILE 2023 competition, which consisted of two tracks: KILE and LIR. Both tasks consist of detection of pre-defined categories of information in business documents. The latter task additionally requires groupping the information into tuples. In the end, we obtained 5 submissions for KILE and 4 submissions for LIR. The diversity of the chosen approaches shows the potential of the DocILE dataset and benchmark, which spans the domains of computer vision, layout analysis, and natural language processing. Unsurprisingly, some of the submissions used a multi-modal approach. The values of the respective error metrics indicate that the benchmark is non-trivial and the problems are far from being solved.</p><p>The benchmark remains open to new submissions, leaving it as a springboard for future research and for the document understanding community. To point out just a few possible research questions for this benchmark: 1) How to best use the unlabeled and synthetic datasets (as most of the solutions did not focus on these parts of the dataset)? 2) Is it possible to better utilize the fact that many documents share the same layout and push the performance on the few-shot subset closer to the performance on the many-shot subset? 3) Which parts of the tasks are better solved by pure NLP solutions (such as the baselines), which are better solved by pure CV solutions (such as YOLOv8) and do the multi-modal solutions (such as GraphDoc) already utilize both of the modalities to their full potential or is one of the modalities still under-utilized?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,411.30,416.69,8.93;4,89.29,423.31,416.69,8.87;4,89.29,435.26,239.14,8.87;4,80.17,60.63,220.58,292.65"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example DocILE document with KILE and LIR annotations emphasized. Left: field annotations. Right: Line Item areas displayed by alternating blue and green . Bottom: color legend for KILE and LIR field types. The image is taken from [24].</figDesc><graphic coords="4,80.17,60.63,220.58,292.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,307.34,416.69,8.93;5,89.29,319.35,416.69,8.87;5,89.29,331.30,416.69,8.87;5,89.29,343.26,416.70,8.87;5,88.93,355.21,179.08,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2:Correct and incorrect bounding box predictions of the phone number are shown in 2b and in 2c, respectively. A predicted field matches the location of a ground truth field if their bounding boxes cover the same text. More precisely, the fields must contain exactly the same Pseudo-Character Centers defined in 2a. Note: in 2b, only one of the predictions would be considered correct if all three boxes were predicted. Images are taken from<ref type="bibr" coords="5,249.36,355.21,14.92,8.87" target="#b31">[32]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,257.71,416.70,8.93;6,89.29,269.72,416.69,8.87;6,89.29,281.67,416.70,8.87;6,89.29,293.63,416.87,8.87;6,89.29,305.58,92.26,8.87;6,89.29,84.19,416.69,166.10"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Visualization of Line Item (LI) matching. Both the annotations and predictions consist of three line items where LI 3 and LI 0 are clearly matched together. The two fields of "Item B" are detected both as part of LI 4 and LI 5, so greedy assignment might assign LI 4 to LI 1, leading to only three matched fields in total. Instead, maximum matching assigns LI 4 to LI 2 and LI 5 to LI 1, leading to four matched fields overall.</figDesc><graphic coords="6,89.29,84.19,416.69,166.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,89.29,652.03,385.08,8.93"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Final results of the DocILE'23 competition for Task 1: KILE (5a) and Task 2: LIR (5b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,89.29,658.00,303.11,8.93"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Text extraction results for Task 1: KILE (6a) and Task 2: LIR (6b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="12,89.29,652.37,340.28,8.93"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Results on the zero-shot subset for Task 1: KILE (7a) and Task 2: LIR (7b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="13,89.29,652.03,337.95,8.93"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Results on the few-shot subset for Task 1: KILE (8a) and Task 2: LIR (8b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="14,89.29,652.03,346.43,8.93"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Results on the many-shot subset for Task 1: KILE (9a) and Task 2: LIR (9b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="16,168.94,274.60,16.39,10.56;16,179.36,303.69,7.98,4.39;16,183.75,299.40,7.98,4.29;16,188.04,292.99,11.19,6.40;16,196.03,274.76,22.47,16.65;16,218.86,292.99,13.39,8.60;16,229.05,274.76,22.47,16.65;16,213.22,335.87,7.98,4.39;16,217.60,331.58,7.98,4.29;16,221.89,325.17,11.19,6.40;16,229.88,306.94,22.47,16.65;16,248.12,300.65,7.98,4.70;16,254.41,274.76,30.13,24.31;16,252.71,325.17,13.39,8.60;16,262.90,306.94,22.47,16.65;16,281.14,300.65,7.98,4.70;16,287.43,274.76,30.13,24.31;16,280.74,334.39,7.98,4.39;16,285.12,330.09,7.98,4.29;16,289.42,323.69,11.19,6.40;16,297.40,305.46,22.47,16.65;16,315.64,299.17,7.98,4.70;16,321.93,292.72,9.97,4.86;16,326.75,288.19,7.98,4.58;16,331.32,274.76,19.25,13.43;16,320.24,323.69,13.39,8.60;16,330.42,305.46,22.47,16.65;16,348.66,299.17,7.98,4.70;16,354.95,292.72,9.97,4.86;16,359.77,288.19,7.98,4.58;16,364.34,274.76,19.25,13.43;16,340.67,339.08,7.98,4.39;16,345.06,334.78,7.98,4.29;16,349.35,328.38,11.19,6.40;16,357.34,310.15,22.47,16.65;16,375.57,303.86,7.98,4.70;16,381.86,297.15,7.98,5.12;16,386.98,274.05,28.93,23.10;16,380.17,328.38,13.39,8.60;16,390.36,310.15,22.47,16.65;16,408.59,303.86,7.98,4.70;16,414.88,297.15,7.98,5.12;16,420.00,274.05,28.93,23.10;16,140.07,261.25,9.22,7.86;16,140.07,232.40,9.22,7.86;16,140.07,203.55,9.22,7.86;16,140.07,174.70,9.22,7.86;16,140.07,145.86,9.22,7.86;16,140.07,117.01,9.22,7.86;16,140.07,88.16,9.22,7.86;16,113.95,174.10,8.87,24.71;16,113.95,157.65,8.87,13.97;16,515.49,94.83,11.84,8.87;16,516.58,106.84,9.65,8.87;16,109.82,354.61,377.01,9.96;16,123.82,366.56,119.33,9.96;16,167.69,569.03,16.39,10.56;16,178.12,598.12,7.98,4.39;16,182.50,593.82,7.98,4.29;16,186.79,587.42,11.19,6.40;16,194.78,569.19,22.47,16.65;16,217.61,587.42,13.39,8.60;16,227.80,569.19,22.47,16.65;16,211.97,630.30,7.98,4.39;16,216.36,626.01,7.98,4.29;16,220.65,619.60,11.19,6.40;16,228.64,601.37,22.47,16.65;16,246.87,595.08,7.98,4.70;16,253.16,569.19,30.13,24.31;16,251.47,619.60,13.39,8.60;16,261.66,601.37,22.47,16.65;16,279.89,595.08,7.98,4.70;16,286.18,569.19,30.13,24.31;16,279.49,628.82,7.98,4.39;16,283.88,624.52,7.98,4.29;16,288.17,618.12,11.19,6.40;16,296.16,599.89,22.47,16.65;16,314.39,593.60,7.98,4.70;16,320.68,587.15,9.97,4.86;16,325.50,582.62,7.98,4.58;16,330.08,569.19,19.25,13.43;16,318.99,618.12,13.39,8.60;16,329.18,599.89,22.47,16.65;16,347.41,593.60,7.98,4.70;16,353.70,587.15,9.97,4.86;16,358.52,582.61,7.98,4.58;16,363.10,569.19,19.25,13.43;16,339.43,633.50,7.98,4.39;16,343.81,629.21,7.98,4.29;16,348.11,622.81,11.19,6.40;16,356.10,604.57,22.47,16.65;16,374.33,598.28,7.98,4.70;16,380.62,591.58,7.98,5.12;16,385.74,568.48,28.93,23.10;16,378.93,622.81,13.39,8.60;16,389.11,604.57,22.47,16.65;16,407.35,598.28,7.98,4.70;16,413.64,591.58,7.98,5.12;16,418.75,568.48,28.93,23.10;16,138.83,555.68,9.22,7.86;16,138.83,526.83,9.22,7.86;16,138.83,497.98,9.22,7.86;16,138.83,469.13,9.22,7.86;16,138.83,440.29,9.22,7.86;16,138.83,411.44,9.22,7.86;16,138.83,382.59,9.22,7.86;16,112.71,468.53,8.87,24.71;16,112.71,452.07,8.87,13.97;16,514.25,389.26,11.84,8.87;16,515.34,401.27,9.65,8.87;16,108.57,649.04,377.01,9.96;16,123.17,660.99,113.28,9.96"><head></head><label></label><figDesc>GraphDoc<ref type="bibr" coords="16,168.44,354.61,16.44,9.96" target="#b32">[33]</ref> breakdown of results based on the document source and zero/few/manyshot layout for the KILE task. GraphDoc<ref type="bibr" coords="16,167.71,649.04,16.43,9.96" target="#b32">[33]</ref> breakdown of results based on the document source and zero/few/manyshot layout for the LIR task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16" coords="16,89.29,679.48,416.69,8.93;16,89.29,691.48,416.69,8.87;16,89.02,703.44,69.43,8.87"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Breakdown of results based on the document source type and in combination with zero/few/many-shot layouts of the winning solution GraphDoc [33] for Task 1: KILE (10a) and Task 2: LIR (10b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17" coords="17,89.29,294.62,416.69,8.93;17,89.29,306.57,370.39,8.96"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Distribution of the number of document pages in the training, validation and unlabeled sets for the UCSF source type subset. The numbers of documents are displayed above the bars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19" coords="17,89.29,594.09,416.69,8.93;17,89.29,606.04,360.72,8.96"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Figure 12: Distribution of the number of document pages in the training, validation and unlabeled sets for the PIF source type subset. The numbers of documents are displayed above the bars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20" coords="18,89.29,615.34,416.69,8.93;18,89.29,627.29,418.23,8.96"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: The number of documents of each layout cluster in the training, validation and unlabeled subsets with PIF document source type, on a logarithmic scale. Some clusters have up to 100k documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21" coords="22,139.81,308.20,7.98,4.55;22,144.36,296.10,16.60,12.10;22,156.46,291.60,7.98,4.50;22,160.96,285.67,11.19,5.93;22,168.48,273.61,16.17,10.47;22,177.98,304.16,7.98,3.69;22,181.44,299.70,7.98,4.70;22,186.13,296.43,7.98,3.27;22,189.40,291.73,7.98,4.70;22,194.16,285.79,10.91,5.88;22,201.62,273.73,16.16,10.47;22,190.65,323.82,7.98,4.39;22,195.03,285.79,42.95,38.02;22,234.64,273.73,16.16,10.47;22,266.97,274.17,16.16,11.60;22,259.92,320.74,7.17,4.93;22,265.49,294.89,29.19,24.26;22,291.33,289.97,7.98,3.34;22,296.25,273.61,19.27,14.77;22,314.83,300.06,7.17,4.93;22,320.40,274.22,29.19,24.26;22,317.25,329.44,7.17,4.93;22,322.82,306.39,26.18,21.47;22,344.29,300.77,7.98,5.62;22,349.90,294.89,10.91,5.88;22,357.37,289.97,7.98,3.34;22,362.29,273.61,19.27,14.77;22,371.46,308.06,7.17,4.93;22,377.02,285.00,26.18,21.47;22,398.49,279.39,7.98,5.62;22,404.11,273.51,10.91,5.88;22,411.84,302.28,7.98,4.00;22,415.84,286.09,20.75,16.19;22,432.00,281.58,7.98,4.55;22,436.55,274.32,11.88,7.26;22,144.68,259.99,4.61,7.86;22,137.51,217.03,11.77,7.86;22,137.51,174.07,11.77,7.86;22,137.51,131.12,11.77,7.86;22,137.51,88.16,11.77,7.86;22,113.95,180.10,8.87,11.84;22,113.95,163.64,8.87,13.97;22,522.43,94.39,11.84,8.87;22,515.49,106.91,25.72,8.87;22,515.49,119.42,25.72,8.87;22,109.82,345.52,375.33,9.96;22,123.82,357.47,330.49,9.96;22,142.24,592.87,7.98,4.55;22,146.78,580.77,16.60,12.10;22,158.88,576.27,7.98,4.50;22,163.38,570.33,11.19,5.93;22,170.90,558.27,16.17,10.47;22,207.02,558.84,16.16,11.60;22,203.64,605.40,7.17,4.93;22,209.21,579.56,29.19,24.26;22,235.05,574.63,7.98,3.34;22,239.97,558.27,19.27,14.77;22,231.62,614.11,7.17,4.93;22,237.19,591.05,26.18,21.47;22,258.65,585.44,7.98,5.62;22,264.27,579.56,10.91,5.88;22,271.73,574.63,7.98,3.34;22,276.66,558.27,19.27,14.77;22,298.90,584.72,7.17,4.93;22,304.47,558.88,29.19,24.26;22,326.18,592.72,7.17,4.93;22,331.74,569.67,26.18,21.47;22,353.21,564.05,7.98,5.62;22,358.83,558.17,10.91,5.88;22,367.50,588.83,7.98,3.69;22,370.96,584.36,7.98,4.70;22,375.66,581.10,7.98,3.27;22,378.92,576.40,7.98,4.70;22,383.68,570.46,10.91,5.88;22,391.14,558.40,16.17,10.47;22,406.92,586.95,7.98,4.00;22,410.92,570.75,20.75,16.19;22,427.08,566.24,7.98,4.55;22,431.63,558.98,11.88,7.26;22,143.44,544.73,4.61,7.86;22,136.27,503.33,11.77,7.86;22,136.27,461.94,11.77,7.86;22,136.27,420.54,11.77,7.86;22,136.27,379.14,11.77,7.86;22,112.71,465.86,8.87,9.65;22,112.71,449.40,8.87,13.97;22,521.18,379.06,9.65,8.87;22,514.23,391.58,23.54,8.87;22,108.57,630.18,375.33,9.96;22,123.17,642.14,120.21,9.96"><head></head><label></label><figDesc>KILE evaluated with AP on different subsets of field types. AP: all field types, AP ($): field type currency_code_amount_due, AP (#): all KILE field types with suffix _id. LIR evaluated with F1 on different subsets of field types. F1: all field types, F1 (#): field type line_item_order_id.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22" coords="22,89.29,659.94,416.87,8.93;22,89.02,671.95,162.93,8.87"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Evaluation of the methods on field types affected by the GraphDoc splitting heuristics for Task 1: KILE (15a) and Task 2: LIR (15b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="20,88.99,90.49,315.47,408.36"><head>Table 1</head><label>1</label><figDesc>List of all split fields in the training and validation sets for the KILE task.</figDesc><table coords="20,188.33,118.58,216.13,380.27"><row><cell>field type</cell><cell>split fields</cell></row><row><cell cols="2">currency_code_amount_due 89.8 % (3593/4000)</cell></row><row><cell>vendor_tax_id</cell><cell>26.6 % (202/760)</cell></row><row><cell>bank_num</cell><cell>21.0 % (22/105)</cell></row><row><cell>tax_detail_rate</cell><cell>20.3 % (15/74)</cell></row><row><cell>vendor_registration_id</cell><cell>13.5 % (7/52)</cell></row><row><cell>bic</cell><cell>12.9 % (4/31)</cell></row><row><cell>account_num</cell><cell>8.9 % (12/135)</cell></row><row><cell>customer_tax_id</cell><cell>7.5 % (3/40)</cell></row><row><cell>customer_id</cell><cell>7.4 % (155/2108)</cell></row><row><cell>customer_order_id</cell><cell>7.3 % (46/626)</cell></row><row><cell>order_id</cell><cell>7.2 % (244/3374)</cell></row><row><cell>document_id</cell><cell>6.5 % (401/6141)</cell></row><row><cell>vendor_order_id</cell><cell>4.3 % (10/233)</cell></row><row><cell>date_issue</cell><cell>4.0 % (251/6214)</cell></row><row><cell>date_due</cell><cell>3.3 % (29/884)</cell></row><row><cell>amount_paid</cell><cell>3.2 % (14/432)</cell></row><row><cell>payment_reference</cell><cell>3.2 % (6/187)</cell></row><row><cell>vendor_email</cell><cell>2.8 % (18/648)</cell></row><row><cell>tax_detail_gross</cell><cell>1.8 % (10/542)</cell></row><row><cell>amount_due</cell><cell>1.8 % (113/6125)</cell></row><row><cell>tax_detail_net</cell><cell>1.7 % (9/519)</cell></row><row><cell>amount_total_gross</cell><cell>1.7 % (102/5966)</cell></row><row><cell>amount_total_tax</cell><cell>1.7 % (11/654)</cell></row><row><cell>tax_detail_tax</cell><cell>1.7 % (10/601)</cell></row><row><cell>amount_total_net</cell><cell>1.4 % (12/838)</cell></row><row><cell>payment_terms</cell><cell>0.7 % (15/2295)</cell></row><row><cell>vendor_name</cell><cell>0.4 % (26/7354)</cell></row><row><cell>customer_billing_name</cell><cell>0.1 % (9/6142)</cell></row><row><cell>customer_other_name</cell><cell>0.1 % (1/1463)</cell></row><row><cell>vendor_address</cell><cell>0.0 % (2/6634)</cell></row><row><cell>micro accuracy</cell><cell>7.5 % (5352/71513)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="21,88.99,96.16,323.20,229.04"><head>Table 2</head><label>2</label><figDesc>List of all split fields in the training and validation sets for the LIR task.</figDesc><table coords="21,180.59,124.25,231.60,200.95"><row><cell>field type</cell><cell>split fields</cell></row><row><cell>line_item_order_id</cell><cell>24.4 % (69/283)</cell></row><row><cell cols="2">line_item_units_of_measure 20.4 % (312/1533)</cell></row><row><cell>line_item_position</cell><cell>8.9 % (905/10189)</cell></row><row><cell>line_item_date</cell><cell>5.7 % (1842/32405)</cell></row><row><cell>line_item_currency</cell><cell>5.2 % (62/1196)</cell></row><row><cell>line_item_weight</cell><cell>4.4 % (4/91)</cell></row><row><cell>line_item_discount_amount</cell><cell>3.5 % (2/57)</cell></row><row><cell cols="2">line_item_unit_price_gross 3.3 % (636/19324)</cell></row><row><cell>line_item_code</cell><cell>2.3 % (204/8904)</cell></row><row><cell>line_item_unit_price_net</cell><cell>2.2 % (67/3009)</cell></row><row><cell>line_item_amount_net</cell><cell>2.1 % (90/4235)</cell></row><row><cell>line_item_quantity</cell><cell>1.6 % (359/22993)</cell></row><row><cell>line_item_amount_gross</cell><cell>1.5 % (345/23734)</cell></row><row><cell>line_item_description</cell><cell>1.0 % (273/28617)</cell></row><row><cell>line_item_person_name</cell><cell>0.2 % (1/496)</cell></row><row><cell>micro accuracy</cell><cell>3.3 % (5171/157469)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="21,88.99,352.47,331.29,85.57"><head>Table 3</head><label>3</label><figDesc>List of all split fields in the training and validation sets satisfying rule ($).</figDesc><table coords="21,172.51,380.56,247.77,57.48"><row><cell cols="2">task field type</cell><cell>split fields</cell></row><row><cell cols="3">KILE currency_code_amount_due 86.8 % (3470/4000)</cell></row><row><cell>LIR</cell><cell>line_item_currency</cell><cell>5.1 % (61/1196)</cell></row><row><cell cols="2">KILE micro accuracy</cell><cell>4.9 % (3470/71513)</cell></row><row><cell>LIR</cell><cell>micro accuracy</cell><cell>0.0 % (61/157469)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="21,88.99,465.31,323.55,205.13"><head>Table 4</head><label>4</label><figDesc>List of all split fields in the training and validation sets satisfying rule (#).</figDesc><table coords="21,180.24,493.40,232.30,177.04"><row><cell cols="2">task field type</cell><cell>split fields (#)</cell></row><row><cell cols="2">KILE vendor_tax_id</cell><cell>10.9 % (83/760)</cell></row><row><cell cols="3">KILE vendor_registration_id 5.8 % (3/52)</cell></row><row><cell cols="2">KILE bank_num</cell><cell>5.7 % (6/105)</cell></row><row><cell cols="2">KILE customer_tax_id</cell><cell>5.0 % (2/40)</cell></row><row><cell>LIR</cell><cell>line_item_order_id</cell><cell>3.2 % (9/283)</cell></row><row><cell cols="2">KILE document_id</cell><cell>2.6 % (159/6141)</cell></row><row><cell cols="2">KILE account_num</cell><cell>1.5 % (2/135)</cell></row><row><cell cols="2">KILE customer_id</cell><cell>1.4 % (29/2108)</cell></row><row><cell cols="2">KILE order_id</cell><cell>1.4 % (46/3374)</cell></row><row><cell cols="2">KILE customer_order_id</cell><cell>1.3 % (8/626)</cell></row><row><cell cols="2">KILE payment_reference</cell><cell>1.1 % (2/187)</cell></row><row><cell>LIR</cell><cell>line_item_position</cell><cell>0.0 % (2/10189)</cell></row><row><cell cols="2">KILE micro accuracy</cell><cell>0.5 % (340/71513)</cell></row><row><cell>LIR</cell><cell>micro accuracy</cell><cell>0.0 % (11/157469)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,92.57,649.10,413.41,8.97;3,92.57,660.06,25.10,8.97"><p>Clusters are formed by documents that have similar visual layout and placement of semantic information in this layout.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,92.57,671.02,103.66,8.97"><p>https://rrc.cvc.uab.es/?ch=26</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="23,112.66,400.24,394.53,10.91;23,112.66,413.79,393.32,10.91;23,112.66,427.34,394.53,10.91;23,112.66,440.89,393.33,10.91;23,112.66,454.44,393.33,10.91;23,112.66,467.99,267.69,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="23,227.92,413.79,278.06,10.91;23,112.66,427.34,64.76,10.91">Overview of DocILE 2023: Document Information Localization and Extraction</title>
		<author>
			<persName coords=""><forename type="first">Š</forename><surname>Šimsa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Uřičář</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kocián</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Skalický</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Coustaty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,375.83,440.89,130.16,10.91;23,112.66,454.44,258.02,10.91;23,403.07,454.44,102.92,10.91;23,112.66,467.99,233.99,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>LNCS Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="23,112.66,481.54,393.33,10.91;23,112.66,495.09,394.52,10.91;23,112.66,508.64,74.05,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="23,378.51,481.54,127.47,10.91;23,112.66,495.09,389.48,10.91">Vibertgrid: a jointly trained multi-modal 2d document representation for key information extraction from documents</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,127.29,508.64,28.34,10.91">ICDAR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,522.18,394.61,10.91;23,112.33,535.73,253.20,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="23,464.02,522.18,43.25,10.91;23,112.33,535.73,167.46,10.91">Chargrid: Towards understanding 2d documents</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Reisswig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brarda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Höhne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Faddoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,303.46,535.73,30.55,10.91">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,549.28,393.32,10.91;23,112.66,562.83,227.73,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="23,323.22,549.28,182.76,10.91;23,112.66,562.83,154.64,10.91">Layoutlm: Pre-training of text and layout for document image understanding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>KDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,576.38,394.53,10.91;23,112.66,589.93,393.60,10.91;23,112.33,603.48,29.19,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="23,112.66,589.93,364.31,10.91">Layoutlmv2: Multi-modal pre-training for visually-rich document understanding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Florencio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,485.85,589.93,20.42,10.91">ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,617.03,393.33,10.91;23,112.66,630.58,234.45,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="23,289.23,617.03,216.76,10.91;23,112.66,630.58,137.65,10.91">Layoutlmv3: Pre-training for document ai with unified text and image masking</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ACM-MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,644.13,393.33,10.91;24,112.66,86.97,394.62,10.91;24,112.28,100.52,53.80,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="23,345.32,644.13,160.67,10.91;24,112.66,86.97,372.43,10.91">Bros: A pre-trained language model focusing on text and layout for better key information extraction from documents</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,112.28,100.52,22.71,10.91">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,114.06,393.33,10.91;24,112.66,127.61,105.90,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="24,256.36,114.06,249.63,10.91;24,112.66,127.61,28.75,10.91">Visualmrc: Machine reading comprehension on document images</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yoshida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,141.16,393.33,10.91;24,112.66,154.71,394.53,10.91;24,112.66,168.26,22.69,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Powalski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Borchmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurkiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietruszka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pałka</surname></persName>
		</author>
		<title level="m" coord="24,478.15,141.16,27.84,10.91;24,112.66,154.71,339.77,10.91;24,473.86,154.71,27.77,10.91">Going full-tilt boogie on document understanding with text-image-layout transformer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>ICDAR</note>
</biblStruct>

<biblStruct coords="24,112.66,181.81,393.33,10.91;24,112.30,195.36,332.07,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m" coord="24,467.37,181.81,38.62,10.91;24,112.30,195.36,267.04,10.91">Unifying Vision, Text, and Layout for Universal Document Processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,208.91,393.33,10.91;24,112.66,222.46,355.37,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="24,290.61,208.91,215.38,10.91;24,112.66,222.46,165.19,10.91">Multimodal pre-training based on graph attention network for document understanding</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,286.56,222.46,149.55,10.91">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,236.01,393.33,10.91;24,112.66,249.56,113.66,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="24,362.60,236.01,143.39,10.91;24,112.66,249.56,31.22,10.91">Deep visual template-free form parsing</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tensmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,166.90,249.56,28.34,10.91">ICDAR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,263.11,393.33,10.91;24,112.66,276.66,238.44,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="24,349.04,263.11,156.95,10.91;24,112.66,276.66,155.41,10.91">One-shot field spotting on colored forms using subgraph isomorphism</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hammami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Héroux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,291.68,276.66,28.34,10.91">ICDAR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,290.20,394.52,10.91;24,112.66,303.75,70.41,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="24,284.01,290.20,218.24,10.91">irmp: From printed forms to relational data model</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>HPCC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,317.30,393.33,10.91;24,112.66,330.85,310.77,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<title level="m" coord="24,400.01,317.30,105.98,10.91;24,112.66,330.85,228.47,10.91">ICDAR2019 competition on scanned receipt OCR and information extraction</title>
		<imprint>
			<publisher>ICDAR</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,344.40,393.33,10.91;24,112.66,357.95,193.14,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">K</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Eisenschlos</surname></persName>
		</author>
		<title level="m" coord="24,391.86,344.40,114.13,10.91;24,112.66,357.95,128.35,10.91">Tapas: Weakly supervised table parsing via pre-training</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct coords="24,112.66,371.50,393.33,10.91;24,112.66,385.05,326.55,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="24,336.23,371.50,169.76,10.91;24,112.66,385.05,243.78,10.91">Deepdesrt: Deep learning for detection and structure recognition of tables in document images</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,379.79,385.05,28.34,10.91">ICDAR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,398.60,393.32,10.91;24,112.66,412.15,116.22,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="24,274.11,398.60,231.87,10.91;24,112.66,412.15,34.00,10.91">Publaynet: Largest dataset ever for document layout analysis</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jimeno-Yepes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,169.46,412.15,28.34,10.91">ICDAR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,425.70,393.33,10.91;24,112.66,439.25,166.10,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="24,254.72,425.70,251.26,10.91;24,112.66,439.25,34.86,10.91">An invoice reading system using a graph convolutional network</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lohani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Belaïd</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Belaïd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,171.06,439.25,77.17,10.91">ACCV workshops</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,452.79,393.32,10.91;24,112.66,466.34,305.39,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="24,401.12,452.79,104.87,10.91;24,112.66,466.34,233.89,10.91">Representation learning for information extraction from form-like documents</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">P</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Potti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Wendt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,370.21,466.34,16.81,10.91">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,479.89,393.33,10.91;24,112.66,493.44,247.18,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="24,391.90,479.89,114.08,10.91;24,112.66,493.44,164.30,10.91">Table detection in invoice documents by graph neural networks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goldmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fornés</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lladós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,300.42,493.44,28.34,10.91">ICDAR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,506.99,394.61,10.91;24,112.14,520.54,57.38,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="24,276.09,506.99,211.23,10.91">DocVQA: A dataset for vqa on document images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,534.09,394.62,10.91;24,112.14,547.64,57.38,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bagal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<title level="m" coord="24,413.77,534.09,72.13,10.91">InfographicVQA</title>
		<imprint>
			<publisher>WACV</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,561.19,394.53,10.91;24,112.66,574.74,393.32,10.91;24,112.66,588.29,394.53,10.91;24,112.66,601.84,393.53,10.91;24,112.66,615.39,103.82,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="24,226.61,574.74,279.37,10.91;24,112.66,588.29,45.25,10.91">DocILE Benchmark for Document Information Localization and Extraction</title>
		<author>
			<persName coords=""><forename type="first">Š</forename><surname>Šimsa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Uřičář</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kocián</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Skalický</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Coustaty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,182.79,588.29,324.39,10.91;24,112.66,601.84,53.03,10.91">17th International Conference on Document Analysis and Recognition, ICDAR 2021</title>
		<title level="s" coord="24,383.74,601.84,122.45,10.91;24,112.66,615.39,31.19,10.91">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>San José, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">August 21-26, 2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,628.93,394.62,10.91;24,112.33,642.48,208.90,10.91" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="24,309.27,628.93,198.01,10.91;24,112.33,642.48,133.45,10.91">Business document information extraction: Towards practical benchmarks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Skalický</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Š</forename><surname>Šimsa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Uřičář</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,656.03,395.17,10.91;24,112.66,669.58,84.64,10.91" xml:id="b25">
	<monogr>
		<ptr target="https://www.industrydocuments.ucsf.edu/,????" />
		<title level="m" coord="24,112.66,656.03,151.56,10.91">Web, Industry Documents Library</title>
		<imprint>
			<date type="published" when="2022-10-20">2022-10-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,86.97,370.77,10.91" xml:id="b26">
	<analytic>
		<title/>
		<ptr target="https://publicfiles.fcc.gov/,????" />
	</analytic>
	<monogr>
		<title level="j" coord="25,112.66,86.97,123.16,10.91">Web, Public Inspection Files</title>
		<imprint>
			<date type="published" when="2022-10-20">2022-10-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,100.52,394.53,10.91;25,112.28,114.06,394.07,10.91;25,112.33,127.61,29.19,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="25,254.27,114.06,221.61,10.91">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="25,485.02,114.06,21.33,10.91">IJCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,141.16,393.32,10.91;25,112.66,154.71,393.33,10.91;25,112.66,168.26,98.66,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="25,466.25,141.16,39.73,10.91;25,112.66,154.71,393.33,10.91;25,112.66,168.26,24.00,10.91">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,160.02,168.26,20.71,10.91">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,181.81,375.71,10.91" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="25,151.05,181.81,156.78,10.91">docTR: Document Text Recognition</title>
		<author>
			<persName coords=""><surname>Mindee</surname></persName>
		</author>
		<ptr target="https://github.com/mindee/doctr" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,195.36,387.97,10.91" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="25,216.22,195.36,197.69,10.91">Text Detection Forgot About Document OCR</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Olejniczak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>CVWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,208.91,393.33,10.91;25,112.66,222.46,394.53,10.91;25,112.66,236.01,393.33,10.91;25,112.66,249.56,394.53,10.91;25,112.28,263.11,394.91,10.91;25,112.66,276.66,319.10,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="25,358.33,208.91,147.66,10.91;25,112.66,222.46,173.06,10.91">DocILE 2023 Teaser: Document Information Localization and Extraction</title>
		<author>
			<persName coords=""><forename type="first">Š</forename><surname>Šimsa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Skalický</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hamdi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-28241-6_69</idno>
	</analytic>
	<monogr>
		<title level="m" coord="25,392.71,236.01,113.28,10.91;25,112.66,249.56,319.85,10.91;25,183.06,263.11,90.21,10.91">Advances in Information Retrieval -45th European Conference on Information Retrieval, ECIR 2023</title>
		<title level="s" coord="25,355.06,264.12,147.93,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">April 2-6, 2023. 2023</date>
			<biblScope unit="volume">13982</biblScope>
			<biblScope unit="page" from="600" to="608" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct coords="25,112.66,290.20,393.33,10.91;25,112.66,303.75,394.53,10.91;25,112.66,317.30,393.33,10.91;25,112.66,330.85,394.53,10.91;25,112.66,344.40,90.78,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="25,324.97,290.20,181.02,10.91;25,112.66,303.75,195.46,10.91">USTC-iFLYTEK at DocILE: a Multi-modal approach using Domain-specific GraphDoc</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="25,194.95,317.30,311.04,10.91;25,112.66,330.85,26.91,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="25,369.74,330.85,137.44,10.91;25,112.66,344.40,21.79,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">September 18th -to -21st. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,357.95,395.17,10.91;25,112.66,371.50,267.52,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="25,223.19,357.95,284.65,10.91;25,112.66,371.50,206.87,10.91">Lilt: A simple yet effective language-independent layout transformer for structured document understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="25,328.24,371.50,20.02,10.91">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,385.05,393.33,10.91;25,112.66,398.60,280.96,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="25,399.36,385.05,106.63,10.91;25,112.66,398.60,204.40,10.91">Building a test collection for complex document information processing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Agam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,340.16,398.60,23.37,10.91">SIGIR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,412.15,395.17,10.91;25,112.66,425.70,393.33,10.91;25,112.66,439.25,394.52,10.91;25,112.14,452.79,395.05,10.91;25,112.66,466.34,383.65,10.91" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="25,476.03,412.15,31.80,10.91;25,112.66,425.70,393.33,10.91;25,112.66,439.25,122.43,10.91">Union-RoBERTa: RoBERTas Ensemble Technique for Competition on Document Information Localization and Extraction</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">G</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-N</forename><forename type="middle">M</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">G</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">V</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="25,112.14,452.79,329.65,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="25,268.16,466.34,159.16,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">September 18th -to -21st. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,479.89,393.61,10.91;25,112.33,493.44,393.65,10.91;25,112.66,506.99,393.33,10.91;25,112.66,520.54,395.17,10.91;25,112.66,534.09,394.42,10.91;25,112.66,547.64,395.00,10.91;25,112.66,561.19,193.04,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="25,400.07,479.89,106.20,10.91;25,112.33,493.44,393.65,10.91;25,112.66,506.99,48.83,10.91">ViBERTgrid: A Jointly Trained Multi-modal 2D Document Representation for Key Information Extraction from Documents</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Huo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-86549-8_35</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-86549-8_35.doi:10.1007/978-3-030-86549-8\_35" />
	</analytic>
	<monogr>
		<title level="m" coord="25,368.12,506.99,137.87,10.91;25,112.66,520.54,241.83,10.91">16th International Conference on Document Analysis and Recognition, ICDAR 2021</title>
		<title level="s" coord="25,364.28,535.10,142.80,9.72;25,112.66,548.65,17.17,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Lladós</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Lopresti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Uchida</surname></persName>
		</editor>
		<meeting><address><addrLine>Lausanne, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">September 5-10, 2021. 2021</date>
			<biblScope unit="volume">12821</biblScope>
			<biblScope unit="page" from="548" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,574.74,393.32,10.91;25,112.66,588.29,393.33,10.91;25,112.66,601.84,394.53,10.91;25,112.66,615.39,347.74,10.91" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="25,200.87,574.74,305.11,10.91;25,112.66,588.29,45.25,10.91">Object Detection Pipeline Using YOLOv8 for Document Information Extraction</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gruber</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="25,437.05,588.29,68.93,10.91;25,112.66,601.84,286.65,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="25,232.24,615.39,159.16,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">September 18th -to -21st. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,628.93,394.04,10.91;25,112.66,642.48,95.90,10.91" xml:id="b38">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jocher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<ptr target="https://github.com/ultralytics/ultralytics" />
		<title level="m" coord="25,263.79,628.93,92.78,10.91">YOLO by Ultralytics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,656.03,394.61,10.91;25,112.33,669.58,393.65,10.91;26,112.33,86.97,395.50,10.91;26,112.66,100.52,393.33,10.91;26,112.66,114.06,330.15,10.91" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="25,462.13,656.03,45.14,10.91;25,112.33,669.58,172.89,10.91">CharGrid: Towards Understanding 2D Documents</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Reisswig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brarda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Höhne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Faddoul</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D18-1476/" />
	</analytic>
	<monogr>
		<title level="m" coord="26,141.44,86.97,366.39,10.91;26,112.66,100.52,29.82,10.91">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Riloff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Chiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</editor>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11-04">October 31 -November 4, 2018. 2018</date>
			<biblScope unit="page" from="4459" to="4469" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
