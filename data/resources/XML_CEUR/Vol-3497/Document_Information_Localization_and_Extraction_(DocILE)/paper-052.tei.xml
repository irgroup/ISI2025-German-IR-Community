<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,403.46,15.42;1,89.29,106.66,262.52,15.42;1,89.29,129.00,210.20,11.96">USTC-iFLYTEK at DocILE: A Multi-modal Approach Using Domain-specific GraphDoc Notebook for the DocILE Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.95,154.90,49.66,11.96"><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<email>yanwangsa@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">NERC-SLIP</orgName>
								<orgName type="institution">University of Science and Technology of China (USTC)</orgName>
								<address>
									<addrLine>No. 96, JinZhai Road</addrLine>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,151.25,154.90,34.40,11.96"><forename type="first">Jun</forename><surname>Du</surname></persName>
							<email>jundu@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">NERC-SLIP</orgName>
								<orgName type="institution">University of Science and Technology of China (USTC)</orgName>
								<address>
									<addrLine>No. 96, JinZhai Road</addrLine>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,204.28,154.90,52.42,11.96"><forename type="first">Jiefeng</forename><surname>Ma</surname></persName>
							<email>jfma@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">NERC-SLIP</orgName>
								<orgName type="institution">University of Science and Technology of China (USTC)</orgName>
								<address>
									<addrLine>No. 96, JinZhai Road</addrLine>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.35,154.90,54.62,11.96"><forename type="first">Pengfei</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NERC-SLIP</orgName>
								<orgName type="institution">University of Science and Technology of China (USTC)</orgName>
								<address>
									<addrLine>No. 96, JinZhai Road</addrLine>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.61,154.90,82.87,11.96"><forename type="first">Zhenrong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NERC-SLIP</orgName>
								<orgName type="institution">University of Science and Technology of China (USTC)</orgName>
								<address>
									<addrLine>No. 96, JinZhai Road</addrLine>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.10,168.85,71.04,11.96"><forename type="first">Jianshu</forename><surname>Zhang</surname></persName>
							<email>jszhang6@iflytek.com</email>
							<affiliation key="aff1">
								<orgName type="department">iFLYTEK Research</orgName>
								<address>
									<addrLine>No.666 West Wangjiang Road</addrLine>
									<settlement>Hefei, Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">https</orgName>
								<orgName type="department" key="dep2">github.com</orgName>
								<orgName type="department" key="dep3">DocILE-Competition CLEF</orgName>
								<orgName type="institution">SPRATeam-USTC</orgName>
								<address>
									<postCode>2023</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,403.46,15.42;1,89.29,106.66,262.52,15.42;1,89.29,129.00,210.20,11.96">USTC-iFLYTEK at DocILE: A Multi-modal Approach Using Domain-specific GraphDoc Notebook for the DocILE Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">51F8DC54089490051316E4B601801260</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Document intelligence</term>
					<term>Multi-modal</term>
					<term>Domain-specific pre-train</term>
					<term>Model ensemble</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the development of digitalization in business, the automatic extraction of information from semistructured business documents is becoming increasingly important. This paper introduces the methods we employed in two tasks of the DocILE competition. We utilize the GraphDoc model firstly pre-trained on provided invoice-domain documents to extract embeddings for text boxes and perform classification for each of them. Based on the classification results, we feed the embeddings into our proposed Merger module to aggregate separate text boxes into semantic instances and line items. Our approach achieved an AP result of 71.25% on the test set of Task 1 and a micro-F1 result of 75.93% on the test set of Task 2. Scripts and pre-trained models used in our experiments have been made publicly available at here 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Business documents are files that provide details related to a company's internal and external transactions. Despite the widespread adoption of digital business practices, many of them are still presented in unstructured or semi-structured formats, such as contracts, invoices, and reports, making it difficult to process automatically. And businesses often rely on manual data entry and processing, which can be time-consuming, error-prone, and expensive.</p><p>Automatic information extraction from business documents is a highly challenging task and the most immediate obstacle is the lack of suitable datasets, as mentioned by several authors <ref type="bibr" coords="1,89.29,521.25,11.23,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,102.86,521.25,7.49,10.91" target="#b1">2]</ref>. Although several publicly available datasets exist for document understanding, only a few of them are tailored to extracting information from business documents. And they are typically limited in size <ref type="bibr" coords="1,154.69,548.35,11.39,10.91" target="#b2">[3,</ref><ref type="bibr" coords="1,168.81,548.35,7.49,10.91" target="#b3">4,</ref><ref type="bibr" coords="1,179.03,548.35,8.98,10.91" target="#b4">5]</ref> and lack annotations for field-level locations <ref type="bibr" coords="1,392.38,548.35,11.39,10.91" target="#b5">[6,</ref><ref type="bibr" coords="1,406.50,548.35,7.48,10.91" target="#b6">7,</ref><ref type="bibr" coords="1,416.72,548.35,7.59,10.91" target="#b7">8]</ref>. Additionally, the variability in document structures and the scattered nature of the information make it difficult to develop models that can accurately and efficiently extract key information from different types of documents.</p><p>To address these challenges, the Document Information Localization and Extraction (DocILE) competition <ref type="bibr" coords="2,146.90,127.61,13.00,10.91" target="#b8">[9]</ref> presents the DocILE dataset and benchmark <ref type="bibr" coords="2,366.41,127.61,16.41,10.91" target="#b9">[10]</ref>, which includes two tasks: Key Information Localization and Extraction (KILE) <ref type="bibr" coords="2,321.95,141.16,18.07,10.91" target="#b10">[11]</ref> and Line Item Recognition (LIR) <ref type="bibr" coords="2,487.15,141.16,16.41,10.91" target="#b10">[11]</ref>. Unlike Key Information Extraction (KIE) <ref type="bibr" coords="2,270.04,154.71,11.44,10.91" target="#b6">[7,</ref><ref type="bibr" coords="2,284.20,154.71,7.63,10.91" target="#b7">8]</ref>, which focuses on extracting information from documents, KILE requires additionally precise localization of the extracted information. The goal of LIR task is to extract a list of line items <ref type="bibr" coords="2,310.11,181.81,16.56,10.91" target="#b11">[12,</ref><ref type="bibr" coords="2,330.07,181.81,12.42,10.91" target="#b12">13]</ref>, such as those commonly found in invoice goods and service tables and each item is represented by a set of key information such as name, quantity, and price. Therefore, for Task 1, the main objective of the experiment is to accurately localize key information (instances) of pre-defined categories in the document, while Task 2 builds on Task 1 to further group instances into line items. The two benchmark tasks use different primary metrics for evaluation. For Task 1, the Average Precision metric (AP) is used as the official evaluation metric, while for Task 2, the primary evaluation metric is the micro F1 score over all line item fields.</p><p>In this paper, we present our systems in the DocILE competition in both tasks. We employ the pre-trained GraphDoc <ref type="bibr" coords="2,187.39,303.75,17.76,10.91" target="#b13">[14]</ref> model to extract embeddings for text boxes and perform classification for each of them. Based on the classification results, the embeddings are fed into the proposed Merger module to aggregate the instances. And for Task 2, another Merger module is employed to group instances into line items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset</head><p>This competition provides three subsets, which are derived from the UCSF Industry Documents Library<ref type="foot" coords="2,122.93,414.82,3.71,7.97" target="#foot_0">1</ref> and Public Inspection Files<ref type="foot" coords="2,251.84,414.82,3.71,7.97" target="#foot_1">2</ref> : an annotated set of 6,680 real business documents, an unlabeled set of 932k real business documents, and a synthetic set of 100k documents with full task labels generated using proprietary document generation techniques. The annotated set is further split into training <ref type="bibr" coords="2,201.53,457.22,11.61,10.91" target="#b4">(5,</ref><ref type="bibr" coords="2,213.14,457.22,15.48,10.91">180)</ref>, validation (500), and test (1,000) sets, and includes annotations for 36 pre-defined categories in the KILE task and 19 others in the LIR task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Due to the competition's prohibition on using external document datasets for training, we pre-trained several GraphDoc models under different configurations on the provided unlabeled set before fine-tuning the annotated set for both tasks. For both KILE and LIR tasks, we first classify text boxes into various categories using the output embeddings of the GraphDoc model, followed by the instance aggregation process handled by the proposed Merger module. And for Task 2, we further employ another Merger module to gather instances into line items. There are also some pre/post-processings according to the text contents and distances between text boxes. Finally, we also adopt a model ensemble strategy to further enhance the system performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">GraphDoc pre-training</head><p>GraphDoc is a document understanding model that applies a multimodal graph attentionbased approach. Unlike previous pre-training models such as RoBERTa <ref type="bibr" coords="3,418.11,292.47,18.06,10.91" target="#b14">[15]</ref> and LayoutLM <ref type="bibr" coords="3,89.29,306.02,16.20,10.91" target="#b15">[16]</ref>, GraphDoc treats the semantic regions of document images extracted by optical character recognition (OCR) as fundamental input elements instead of individual words. This novel approach allows the model to better capture the semantic information in documents and generalize across different types of business documents.</p><p>The pre-training process is illustrated in Figure <ref type="figure" coords="3,320.70,360.22,3.81,10.91" target="#fig_0">1</ref>. We rendered the unlabelled 932k PDF documents into images for separate pages. To guarantee the quality of the pre-training corpus, we remove those words whose confidence is lower than 0.5 and text lines whose average confidence is lower than 0.85. After that, only those pages which have more than 5 text lines and 50 characters will be reserved for pre-training. Finally, we can get 2,736,766 valid pages in total. The Masked Sentence Modeling (MSM) is used as the pre-training task for GraphDoc. Each sentence is randomly and independently masked, while its corresponding layout information is preserved. For the masked sentence, its text content is replaced with a special symbol named <ref type="bibr" coords="3,89.29,468.61,34.12,10.91">[MASK]</ref>. The training target is to predict the sentence embeddings of masked ones based on the sentence embeddings and the visual embeddings of others.</p><p>During pre-training, we freeze the parameters of Sentence-BERT <ref type="bibr" coords="3,394.84,495.71,18.01,10.91" target="#b16">[17]</ref> and jointly train the visual backbone and GraphDoc in an end-to-end fashion. GraphDoc contains 12 layers of graph attention blocks, with the hidden size set to 768 and the number of heads to 12. We pre-train the GraphDoc using Adam optimizer, with the learning rate of 5 Ã— 10 -5 . The learning rate is linearly warmed up over the first 10% steps and then linearly decayed. As for the MSM task, 15% of all input sentences are masked among which 80% are replaced by the [MASK] symbol, 10% are replaced by random sentences from other documents, and 10% remain the same.</p><p>We pre-train two different GraphDoc models with the parameter top-ğ‘˜ for the graph attention layer set to 36 and 60, respectively. The pre-training is conducted on 8 Telsa A100 48GB GPUs with a batch size of 240. When top-ğ‘˜ is set to 36, it takes around 13 hours to pre-train on 2.  </p><note type="other">Swin Transformer FPN Pooling Feature Aggregation Visual Embedding Graph Attention Layer MSM Pretrain Ã—N</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data pre-processing</head><p>Data pre-processing is one of the important steps in both tasks. When it comes to models such as RoBERTa, LayoutLM, and GraphDoc, OCR results are essential for accurately locating key information in documents. The competition organizers provide pre-computed OCR results using the DocTR library <ref type="bibr" coords="4,203.65,324.47,18.07,10.91" target="#b17">[18]</ref> with the DBNet <ref type="bibr" coords="4,300.54,324.47,18.07,10.91" target="#b18">[19]</ref> detector and the CRNN <ref type="bibr" coords="4,432.44,324.47,18.06,10.91" target="#b19">[20]</ref> recognition model, which are further post-processed by snapping word boxes to text to obtain snapped text boxes (ğµ snap ).</p><p>To improve the recall rate of instances in the detection process, we convert the document PDF into image ğ¼ and scale it by 1.25, 1.5, and 1.75 times the original size before text detection and recognition.</p><p>ğµ</p><formula xml:id="formula_0" coords="4,198.51,404.00,308.13,13.58">DocTR (ğ¼ â€² ) = ğ‘“ DocTR (ğ¼ â€² ), ğ¼ â€² âˆˆ {ğ¼ 1.25 , ğ¼ 1.5 , ğ¼ 1.75 }<label>(1)</label></formula><p>Then, we apply non-maximum suppression (NMS) with an IoU threshold of 0.3 to generate an intermediate set of text boxes ğµ nms :</p><formula xml:id="formula_1" coords="4,230.34,456.40,276.30,13.58">ğµ nms = NMS(ğµ DocTR (ğ¼ â€² ), 0.3)<label>(2)</label></formula><p>Lastly, we incorporate missed text boxes into the snapped text boxes provided by the competition organizers and eliminate erroneous large-sized text boxes generated by DocTR. Specifically, we add approximately 2% more text boxes to the existing set, while the percentage of removed text boxes is roughly 0.1% of the total count.</p><formula xml:id="formula_2" coords="4,148.40,538.48,358.24,13.77">ğµ final = Filter(ğµ snap âˆª {ğ‘ | ğ‘ âˆˆ ğµ nms , IoS(ğ‘, ğ‘ â€² ) &lt; 0.3, ğ‘ â€² âˆˆ ğµ snap })<label>(3)</label></formula><p>where Filter is a filtering operation that removes erroneous text boxes. Intersection over Smaller (IoS) is similar to IoU, but the score between two text boxes ğ‘ ğ‘– and ğ‘ ğ‘— is defined as:</p><formula xml:id="formula_3" coords="4,209.45,593.52,297.19,26.23">IoS(ğ‘ ğ‘– , ğ‘ ğ‘— ) = area of intersection min(area of ğ‘ ğ‘– , area of ğ‘ ğ‘— )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model</head><p>The overview of our model is illustrated in Figure <ref type="figure" coords="4,318.68,656.03,3.81,10.91" target="#fig_2">2</ref>. Binary cross-entropy loss (BCELoss) is used for classification due to overlaps between instances and the possibility of one text box belonging to multiple categories, which is consistent with the published baselines.</p><formula xml:id="formula_4" coords="5,190.73,110.77,315.91,33.71">â„’ cls = - 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘–=1 (ğ‘ ğ‘– log ğ‘ Ë†ğ‘– + (1 -ğ‘ ğ‘– ) log(1 -ğ‘ Ë†ğ‘–))<label>(5)</label></formula><p>where ğ‘ is the number of classes, ğ‘ ğ‘– is the true label for each class, and ğ‘ Ë†ğ‘– is the classification logit for each class. To prevent memory errors caused by a large number of text boxes in some document images, we group the text boxes in each image after sorting them from left to right and top to bottom and feed them into the model in batches. Additionally, to avoid losing instances that may be split between different groups, a sliding window strategy is also adopted.</p><p>In contrast to the rule-based approach used in the baselines <ref type="bibr" coords="5,371.51,223.20,16.38,10.91" target="#b9">[10]</ref>, we introduce a learnable Merger module to automatically aggregate text boxes into semantic instances based on their embeddings extracted by GraphDoc, as depicted in Figure <ref type="figure" coords="5,352.79,250.30,3.79,10.91" target="#fig_3">3</ref>. In Task 1, for each category, we calculate the attention score between text boxes of the same category to determine which text boxes should be merged into an instance. The attention score between the ğ‘–-th text box and the ğ‘—-th text box is calculated as follow:</p><formula xml:id="formula_5" coords="5,241.71,316.28,264.93,13.13">ğ‘ ğ‘–ğ‘— = (E ğ‘– W ğ‘˜ )(E ğ‘— W ğ‘ ) âŠ¤<label>(6)</label></formula><p>where</p><formula xml:id="formula_6" coords="5,120.34,336.85,213.51,12.58">E ğ‘– âˆˆ R ğ‘‘ , E ğ‘— âˆˆ R ğ‘‘ , W ğ‘˜ âˆˆ R ğ‘‘Ã—ğ‘‘ , W ğ‘ âˆˆ R ğ‘‘Ã—ğ‘‘ .</formula><p>ğ‘‘ is the dimension of the embedding extracted by GraphDoc. The loss function for instance merging is as follow:</p><formula xml:id="formula_7" coords="5,141.48,375.43,365.16,33.71">â„’ inst = - 1 ğ‘Š * ğ» ğ‘Š âˆ‘ï¸ ğ‘–=1 ğ» âˆ‘ï¸ ğ‘—=1 (ï¸ ğ‘€ ğ‘–,ğ‘— log ğ‘€ Ë†ğ‘–,ğ‘— + (1 -ğ‘€ ğ‘–,ğ‘— ) log(1 -ğ‘€ Ë†ğ‘–,ğ‘— ) )ï¸<label>(7)</label></formula><p>where ğ‘Š and ğ» represent the width and height of the attention map, respectively, ğ‘€ ğ‘–,ğ‘— is the true label at position (ğ‘–, ğ‘—), and ğ‘€ Ë†ğ‘–,ğ‘— is the attention score at position (ğ‘–, ğ‘—). Moreover, to avoid conflicts, we employ a method similar to NMS by using the mean attention scores of text boxes in the merged instance (NMS score). Specifically, if there are overlapping text boxes between two instances, the instance with the higher score will be selected. Differing from Task 1, the input instances in Task 2 are restricted to a single class and are non-overlapping. Therefore, it's not necessary to merge instances separately for each class. All text boxes with pre-defined classes are input into the Merger module with the background texts excluded. The inputs of the second merge stage are instance-level embeddings using the average of all text boxes' embeddings in each instance, which can be formulated as:</p><formula xml:id="formula_8" coords="5,259.06,564.32,247.58,27.55">E Inst ğ‘— = âˆ‘ï¸€ ğ‘› ğ‘–=1 E ğ‘– ğ‘›<label>(8)</label></formula><p>where E Inst ğ‘— represents the embedding of the ğ‘—-th instance, and ğ‘› is the number of text boxes contained in the ğ‘—-th instance. And â„’ item , the loss function for line item merging, is similar to â„’ inst .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boxes Classification Instance Merge</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Line-item Merge</head><formula xml:id="formula_9" coords="6,111.20,156.80,177.08,22.43">ğµ ! ğµ " ğµ # ğµ $ ğµ % ğµ &amp; ğµ ' ğµ ( ğµ )</formula><p>Instance Merger Attention Line-item Merger Attention </p><formula xml:id="formula_10" coords="6,150.34,152.24,195.83,186.53">ğ¼ ! ğ¼ " ğ¼ # ğ¼ % ğ¼ $ ğ¼ &amp; Boxes Instances Instances Line-items</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model ensembling</head><p>In order to further enhance model performance, we ensemble several models using different GraphDoc models to improve the text box classification accuracy. Specifically, after the finetuning stage, we select the top best-performing models under two different configurations as described in Section 3.1 for ensembling. There are four ensemble strategies depending on the ensemble method and the evaluation metric for model performance. Moreover, the micro F1 score for text box classification under each strategy is also calculated. The number of models under different top-ğ‘˜ configurations used for the ensemble in each strategy is determined based on the micro F1 score for text box classification of the ensemble models. As for the final selection of the four strategies, for Task 1, it is determined based on the micro F1 score of the ensemble models for text box classification. For Task 2, the official evaluation metric on the task is used to determine the optimal strategy. Lastly, the ensemble classification results are fed into the Merger module of the best-performing model prior to an ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Post-processing</head><p>Although our classification and Merger modules have achieved impressive results in both tasks, there still exists some challenges that cannot be addressed by the models alone. There are mainly two types of post-processing strategies performed: text box splitting and instance splitting. For example, instances of the "currency_code_amount_due" class are typically represented by boxes only containing the character "$", which are below the detection threshold of DocTR due to their small scales. We extract the text box containing the "$" symbol as the location of the instance if the text box belongs to this class and contains "$" in the text. Moreover, we observe that text box belonging to certain classes with "id" in their class names usually has the "#" symbol in its text, which is absent in the text of the corresponding instance. We remove the character "#" from the text and split the text box.</p><p>Furthermore, to prevent occasional errors in the Merger module, we devise rules to keep each text box as a separate instance if the distance between the text boxes in a single instance exceeds the predefined threshold. Specifically, for Task 1, we apply instance splitting when the minimum horizontal distance between text boxes is greater than 1.5 times the minimum width of text boxes and the maximum vertical distance is greater than 2 times the maximum height. For Task 2, instance splitting is applied when the minimum horizontal distance between text boxes is greater than twice the maximum width.</p><p>Additionally, for Task 1 whose official evaluation metric is AP, the score (confidence) in the final predicted result is critical since predictions are sorted by score from the highest to the lowest. And there are also three different score selection methods evaluated as described below:</p><p>â€¢ NMS score : As introduced in section 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Resources</head><p>We use BCELoss with the transformers 3 framework to train the classification, instance merge, and line item merge tasks in a joint manner. The loss functions for two tasks are as follows:</p><formula xml:id="formula_11" coords="8,253.25,147.60,253.38,12.01">â„’ KILE = â„’ cls + â„’ inst<label>(9)</label></formula><formula xml:id="formula_12" coords="8,237.73,170.36,268.91,12.01">â„’ LIR = â„’ cls + â„’ inst + â„’ item<label>(10)</label></formula><p>The initial learning rate is set to 5 Ã— 10 -5 and is linearly warmed up over the first 10% steps and then linearly decayed. Adam is used as the optimizer. The training is conducted on 2 Telsa V100 24GB GPUs with a batch size of 8. For Task 1, we train the model for 300-500 epochs, while for Task 2, we train the model for 500-1000 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>For Task 1 and Task 2, we fine-tuned the model with different top-ğ‘˜ configurations, and the variation curves of losses used in each task are shown in Figure <ref type="figure" coords="8,374.73,302.07,3.74,10.91" target="#fig_6">4</ref>. Firstly, the instance detection recall rates on the validation set are compared in Table <ref type="table" coords="8,469.10,315.62,3.66,10.91" target="#tab_0">1</ref>. When the IoS between a text box and an instance exceeds the threshold, we regard the text box as a part of the instance and assign it the same class label as the instance, indicating that the instance is detected. It can be observed that our data pre-processing significantly improves the recall rate of instance detection. Specifically, it achieves a recall rate of 0.995 at the threshold of 0.3, while the official method only achieves a recall rate of 0.979. Secondly, for the selection of the classification threshold during the text box classification stage, Table <ref type="table" coords="8,142.31,649.95,4.97,10.91" target="#tab_1">2</ref> presents the performance of both tasks on the validation set at different thresholds. It can be seen that the selection of the classification threshold can exert a substantial influence and the results of Task 1 can differ by up to 7 percent when the threshold changes from 0.1 to 0.9. For Task 1, a threshold of 0.1 is considered optimal, while for Task 2, a threshold of 0.5 is deemed appropriate. Then, as discussed in Section 3.4, we ensemble multiple models using four schemes to enhance the text box classification accuracy and improve the model's performance. Based on the observations for Task 1, the accuracy of text box classification is positively correlated with the official evaluation metric. The Merger module performs well when the classification is correct. As shown in Table <ref type="table" coords="9,227.85,575.04,3.81,10.91" target="#tab_2">3</ref>, when strategy 4 is chosen and the ensemble number is 11, the micro F1 score for text box classification on the validation set is the highest. For Task 2, two Merger modules are employed, making it difficult to ascertain the performance of each module. To address this issue, the selection of the final ensemble strategy is based on the official evaluation metric of Task 2 at the optimal ensemble quantity for each of the four strategies. As indicated in Table <ref type="table" coords="9,171.31,642.79,3.73,10.91" target="#tab_2">3</ref>, strategy 1 is the final ensemble approach and the number of models used for ensembling is 11. Besides, Table <ref type="table" coords="10,165.74,288.18,5.11,10.91" target="#tab_3">4</ref> presents the performance of our final ensemble model with different score selection methods on the AP for Task 1 on the validation set. And Table <ref type="table" coords="10,431.53,301.73,5.17,10.91" target="#tab_4">5</ref> showcases the performance of our final ensemble model with NMS classification score in different postprocessing approaches. Additionally, there are approximately 6% of instances generated through text box splitting in Task 1 and the percentage is around 2% in Task 2. Furthermore, ablation experiments are also conducted to summarize and justify each process as shown in Table <ref type="table" coords="10,173.00,611.29,3.79,10.91" target="#tab_5">6</ref>. Finally, we evaluate the performance of our ensemble model on the test sets and the model achieves an AP of 71.25% for Task 1 and a micro F1 score of 75.93% for Task 2 as shown in Table <ref type="table" coords="10,203.02,638.39,3.74,10.91">7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>This paper presents the model and methods we used in the DocILE competition. We utilize the GraphDoc model to extract embeddings for each text box and perform classification for each of them. Based on the classification results, we feed the embeddings into our proposed Merger module to aggregate the instances and line items. To further improve the model performance, we also conduct model ensembling and some post-processing operations. The results of the validation and test sets demonstrate the effectiveness of our approach in both KILE and LIR tasks. As mentioned, our approach still requires manual rule-making to address some errors. In the future, we will continue to explore ways to automatically solve these issues.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,220.75,355.39,8.93;3,91.20,115.60,57.47,71.98"><head>UnlabelledFigure 1 :</head><label>1</label><figDesc>Figure 1: The pre-training schema. MSM is short for Masked Sentence Modeling task.</figDesc><graphic coords="3,91.20,115.60,57.47,71.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,501.97,617.65,4.27,10.91;3,89.29,631.20,416.70,10.91;3,89.04,644.75,416.94,10.91;3,89.29,658.30,139.22,10.91;4,0.15,24.40,29.03,7.91;4,106.43,198.40,72.26,7.91;4,218.62,91.06,37.00,7.03;4,219.46,100.18,35.37,7.03;4,229.01,110.26,16.32,7.03;4,250.46,159.22,29.63,7.03;4,247.45,168.34,35.66,7.03;4,249.27,189.70,31.91,7.03;4,247.39,198.82,35.66,7.03;4,228.10,128.26,74.36,7.03;4,224.84,137.38,80.88,7.03;4,345.22,107.14,34.53,7.03;4,351.71,117.22,21.62,7.03;4,427.81,121.78,35.77,7.03;4,422.97,131.62,45.37,7.03;4,428.06,158.26,35.27,7.03;4,420.05,168.34,51.23,7.03;4,428.06,189.46,35.27,7.03;4,417.77,199.54,55.83,7.03;4,281.21,110.02,30.91,7.03;4,283.19,92.98,26.96,7.03;4,350.32,173.86,24.34,7.03;4,351.71,183.94,21.62,7.03;4,421.14,108.82,48.99,7.03;4,424.25,93.70,42.75,7.03"><head></head><label></label><figDesc>7 million unlabelled pages for 3 epochs. When changing the parameter top-ğ‘˜ to 60, it takes around 19.5 hours to complete the pre-training for 4 epochs. More choices of top-k are not explored further due to time constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,228.18,138.53,8.93;4,106.13,108.57,67.53,82.54"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The finetuning schema.</figDesc><graphic coords="4,106.13,108.57,67.53,82.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,89.29,418.69,416.69,8.93;6,89.29,430.70,416.69,8.87;6,89.29,442.65,295.31,8.87;6,111.76,302.86,193.20,105.40"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The merge results of instance merging and line item merging. The red, green, and blue boxes represent text boxes, instances, and line items, respectively. The marks near boxes are the indexes. And the attention maps on the right indicate which boxes should be merged.</figDesc><graphic coords="6,111.76,302.86,193.20,105.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,107.28,591.04,398.70,10.91;6,116.56,604.58,389.42,10.91;6,116.56,618.13,314.65,10.91;6,107.28,633.04,398.70,10.91;6,116.56,646.59,389.63,10.91;6,116.56,660.14,98.73,10.91;7,107.28,86.97,398.71,10.91;7,116.56,100.52,389.42,10.91;7,116.56,114.06,43.80,10.91;7,107.28,128.97,398.71,10.91;7,116.56,142.52,391.11,10.91"><head>â€¢ strategy 1 :</head><label>1</label><figDesc>Averaging + official. The final probability scores used for classification are generated by averaging the probability scores of various models for each text box. Model selection criteria is based on the official evaluation metric of each task. â€¢ strategy 2 : Averaging + F1. Employ the Averaging method as a model ensemble technique as strategy 1. But the selection criteria for models is based on their micro F1 score for text box classification. â€¢ strategy 3 : Voting + official. The final classification results are voted by various models for each text box. Model selection criteria is based on the official evaluation metrics of each task. â€¢ strategy 4 : Voting + F1. Employ the Voting method as a model ensemble technique and the selection criteria for models is based on their micro F1 scores for text box classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,288.74,576.01,217.24,10.91;7,116.56,589.56,173.19,10.91;7,107.28,604.46,398.71,10.91;7,116.56,618.01,56.23,10.91;7,107.28,632.91,335.12,10.91"><head>2 ,</head><label>2</label><figDesc>NMS score refers to the average attention score of text boxes within the same instance. â€¢ Classification score : The average probability scores for the text boxes contained within the instance. â€¢ NMS classification score : Average of Classification score and NMS score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,89.29,417.09,416.69,8.93;9,89.29,429.10,99.97,8.87;9,94.29,247.73,190.00,142.50"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The loss curves for Task 1 and Task 2 under different top-ğ‘˜ configurations, trained using annotated training data.</figDesc><graphic coords="9,94.29,247.73,190.00,142.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,88.99,411.73,322.62,81.84"><head>Table 1</head><label>1</label><figDesc>The instance detection recall rates for the official OCR and our processed OCR</figDesc><table coords="8,191.83,443.35,211.62,50.22"><row><cell>Config</cell><cell>0.1</cell><cell>0.3</cell><cell>Threshold 0.5</cell><cell>0.7</cell><cell>0.9</cell></row><row><cell>Official</cell><cell cols="5">0.982 0.979 0.968 0.944 0.885</cell></row><row><cell cols="6">Multi-scale 0.998 0.995 0.983 0.957 0.891</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,88.99,526.61,416.99,91.57"><head>Table 2</head><label>2</label><figDesc>Impact of different classification thresholds in the text box classification stage on the official evaluation metrics</figDesc><table coords="8,199.75,567.96,195.79,50.22"><row><cell>Task</cell><cell>0.1</cell><cell>0.3</cell><cell>Threshold 0.5</cell><cell>0.7</cell><cell>0.9</cell></row><row><cell cols="6">Task 1 0.714 0.701 0.690 0.676 0.643</cell></row><row><cell cols="6">Task 2 0.782 0.783 0.784 0.784 0.782</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,88.99,90.49,423.53,170.99"><head>Table 3</head><label>3</label><figDesc>The micro F1 scores for text box classification under different model ensembling strategies and model numbers and the official evaluation metric at the optimal ensemble quantity</figDesc><table coords="10,95.27,134.06,417.25,127.42"><row><cell>Task</cell><cell>Strategy</cell><cell>1</cell><cell>Model Number 5 11</cell><cell>20</cell><cell>Official Evaluation Metric</cell></row><row><cell></cell><cell cols="4">Strategy 1 (Averaging + official) 0.912 0.914 0.914 0.914</cell><cell>-</cell></row><row><cell>Task 1</cell><cell>Strategy 2 (Averaging + F1) Strategy 3 (Voting + official)</cell><cell cols="3">0.916 0.916 0.915 0.915 0.906 0.918 0.918 0.918</cell><cell>--</cell></row><row><cell></cell><cell>Strategy 4 (Voting + F1)</cell><cell cols="3">0.914 0.919 0.921 0.920</cell><cell>-</cell></row><row><cell></cell><cell cols="4">Strategy 1 (Averaging + official) 0.872 0.872 0.874 0.871</cell><cell>0.785</cell></row><row><cell>Task 2</cell><cell>Strategy 2 (Averaging + F1) Strategy 3 (Voting + official)</cell><cell cols="3">0.882 0.887 0.884 0.885 0.892 0.880 0.875 0.873</cell><cell>0.784 0.784</cell></row><row><cell></cell><cell>Strategy 4 (Voting + F1)</cell><cell cols="3">0.902 0.891 0.888 0.887</cell><cell>0.761</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,88.99,371.39,289.90,91.57"><head>Table 4</head><label>4</label><figDesc>The influence of different score selection methods on the AP for Task 1</figDesc><table coords="10,230.02,400.78,135.23,62.17"><row><cell>Score Selection</cell><cell>AP</cell></row><row><cell>None</cell><cell>0.606</cell></row><row><cell>NMS score</cell><cell>0.625</cell></row><row><cell>Classification score</cell><cell>0.740</cell></row><row><cell cols="2">NMS classification score 0.740</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,88.99,497.16,324.77,81.99"><head>Table 5</head><label>5</label><figDesc>Post-processing results on official evaluation metrics on the validation set</figDesc><table coords="10,181.51,528.78,232.26,50.37"><row><cell cols="4">Instance Splitting Text Box Splitting Task 1 Task 2</cell></row><row><cell>-</cell><cell>-</cell><cell>0.672</cell><cell>0.769</cell></row><row><cell>âœ“</cell><cell>-</cell><cell>0.677</cell><cell>0.770</cell></row><row><cell>âœ“</cell><cell>âœ“</cell><cell cols="2">0.740 0.785</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,88.99,90.49,417.00,205.39"><head>Table 6</head><label>6</label><figDesc>The results of ablation experiments on pre-processing, post-processing and model ensembling on the validation set</figDesc><table coords="11,88.99,131.84,388.70,164.04"><row><cell cols="5">Pre-processing Post-processing Model Ensembling</cell><cell>Task 1 AP F1</cell><cell>Task 2 AP F1</cell></row><row><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>0.638 0.700 0.577 0.755</cell></row><row><cell>âœ“</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>0.643 0.702 0.594 0.766</cell></row><row><cell>âœ“</cell><cell>âœ“</cell><cell></cell><cell>-</cell><cell></cell><cell>0.714 0.750 0.615 0.784</cell></row><row><cell>âœ“</cell><cell>âœ“</cell><cell></cell><cell>âœ“</cell><cell></cell><cell>0.740 0.769 0.619 0.785</cell></row><row><cell>Table 7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>The results on the test set</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Task</cell><cell>AP</cell><cell>F1</cell><cell>P</cell><cell>R</cell></row><row><cell></cell><cell cols="5">Task 1 71.25% 74.25% 71.41% 77.31%</cell></row><row><cell></cell><cell cols="5">Task 2 57.89% 75.93% 80.82% 71.60%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,92.57,660.05,154.37,8.97"><p>https://www.industrydocuments.ucsf.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,92.57,671.01,94.60,8.97"><p>https://publicfiles.fcc.gov/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,112.66,500.36,393.33,10.91;11,112.66,513.91,394.53,10.91;11,112.39,527.45,132.81,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,264.45,500.36,241.54,10.91;11,112.66,513.91,51.68,10.91">One-shot template matching for automatic document data capture</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhakal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Munikar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,208.98,513.91,293.61,10.91">Artificial Intelligence for Transforming Business and Society (AITB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,541.00,393.33,10.91;11,112.66,554.55,393.32,10.91;11,112.66,568.10,394.53,10.91;11,112.66,581.65,108.11,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,297.15,541.00,208.84,10.91;11,112.66,554.55,278.83,10.91">Information extraction from invoices: a graph neural network approach for datasets with high layout variety</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Drews</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wobbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,414.43,554.55,91.56,10.91;11,112.66,568.10,92.05,10.91;11,262.14,568.10,240.81,10.91">A Collection of Latest Research on Technology Issues</title>
		<imprint>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="5" to="20" />
			<date type="published" when="2021">2021</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Innovation Through Information Systems</note>
</biblStruct>

<biblStruct coords="11,112.66,595.20,393.61,10.91;11,112.66,608.75,281.19,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,305.30,595.20,200.97,10.91;11,112.66,608.75,99.15,10.91">Spatial dual-modality graph reasoning for key information extraction</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14470</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,622.30,395.17,10.91;11,112.66,635.85,393.97,10.91;11,112.66,649.40,38.81,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,269.66,622.30,238.17,10.91;11,112.66,635.85,35.91,10.91">A probabilistic approach to printed document understanding</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Medvet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Davanzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,156.98,635.85,307.67,10.91">International Journal on Document Analysis and Recognition (IJDAR)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="335" to="347" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,662.95,393.33,10.91;12,112.39,86.97,393.60,10.91;12,112.66,100.52,366.22,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,438.52,662.95,67.47,10.91;12,112.39,86.97,319.80,10.91">Towards robust visual information extraction in real world: New dataset and novel solution</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,452.95,86.97,53.04,10.91;12,112.66,100.52,217.60,10.91">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2738" to="2745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,114.06,395.17,10.91;12,112.66,127.61,393.32,10.91;12,112.66,141.16,394.53,10.91;12,112.66,154.71,22.69,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,140.54,127.61,237.51,10.91">Due: End-to-end document understanding benchmark</title>
		<author>
			<persName coords=""><forename type="first">Å</forename><surname>Borchmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietruszka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Stanislawek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurkiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Turski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Szyndler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>GraliÅ„ski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,401.78,127.61,104.20,10.91;12,112.66,141.16,390.88,10.91">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,168.26,393.33,10.91;12,112.66,181.81,393.33,10.91;12,112.66,195.36,326.52,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,404.74,168.26,101.25,10.91;12,112.66,181.81,221.50,10.91">Icdar2019 competition on scanned receipt ocr and information extraction</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,357.08,181.81,148.90,10.91;12,112.66,195.36,201.41,10.91">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1516" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,208.91,394.53,10.91;12,112.66,222.46,393.32,10.91;12,112.66,236.01,393.33,10.91;12,112.66,249.56,394.52,10.91;12,112.66,263.11,80.57,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,159.90,222.46,346.08,10.91;12,112.66,236.01,68.98,10.91">Kleister: key information extraction datasets involving long documents with complex layouts</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>StanisÅ‚awek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>GraliÅ„ski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>WrÃ³blewska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>LipiÅ„ski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kaliska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosalska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Topolski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Biecek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,203.59,236.01,302.40,10.91;12,112.66,249.56,48.81,10.91">Document Analysis and Recognition-ICDAR 2021: 16th International Conference</title>
		<meeting><address><addrLine>Lausanne, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">September 5-10, 2021. 2021</date>
			<biblScope unit="page" from="564" to="579" />
		</imprint>
	</monogr>
	<note>Part I</note>
</biblStruct>

<biblStruct coords="12,112.66,276.66,394.53,10.91;12,112.66,290.20,393.32,10.91;12,112.66,303.75,394.53,10.91;12,112.66,317.30,393.33,10.91;12,112.66,330.85,393.33,10.91;12,112.66,344.40,267.69,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,227.92,290.20,278.06,10.91;12,112.66,303.75,64.76,10.91">Overview of DocILE 2023: Document Information Localization and Extraction</title>
		<author>
			<persName coords=""><forename type="first">Å </forename><surname>Å imsa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>UÅ™iÄÃ¡Å™</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>KociÃ¡n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>SkalickÃ½</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Coustaty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,375.83,317.30,130.16,10.91;12,112.66,330.85,258.02,10.91;12,403.07,330.85,102.92,10.91;12,112.66,344.40,233.99,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>LNCS Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="12,112.66,357.95,394.53,10.91;12,112.66,371.50,393.32,10.91;12,112.66,385.05,394.53,10.91;12,112.66,398.60,393.53,10.91;12,112.66,412.15,103.82,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,226.61,371.50,279.37,10.91;12,112.66,385.05,45.25,10.91">DocILE Benchmark for Document Information Localization and Extraction</title>
		<author>
			<persName coords=""><forename type="first">Å </forename><surname>Å imsa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>UÅ™iÄÃ¡Å™</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>KociÃ¡n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>SkalickÃ½</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Coustaty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,182.79,385.05,324.39,10.91;12,112.66,398.60,53.03,10.91">17th International Conference on Document Analysis and Recognition, ICDAR 2021</title>
		<title level="s" coord="12,383.74,398.60,122.45,10.91;12,112.66,412.15,31.19,10.91">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>San JosÃ©, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">August 21-26, 2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,425.70,394.62,10.91;12,112.33,439.25,394.86,10.91;12,112.66,452.79,394.53,10.91;12,112.66,466.34,345.59,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,309.27,425.70,198.01,10.91;12,112.33,439.25,132.16,10.91">Business document information extraction: Towards practical benchmarks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Skalická»³</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Å </forename><surname>Å imsa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>UÅ™iÄÃ¡Å™</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,268.05,439.25,239.14,10.91;12,112.66,452.79,389.90,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 13th International Conference of the CLEF Association, CLEF 2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 5-8, 2022. 2022</date>
			<biblScope unit="page" from="105" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,479.89,393.33,10.91;12,112.66,493.44,222.64,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="12,243.56,479.89,262.43,10.91;12,112.66,493.44,40.43,10.91">Key information extraction from documents: evaluation and generator</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bensch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14624</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,506.99,395.17,10.91;12,112.66,520.54,284.41,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">I</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Reisswig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04948</idno>
		<title level="m" coord="12,218.35,506.99,289.48,10.91;12,112.66,520.54,101.90,10.91">Bertgrid: Contextualized embedding for 2d document representation and understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,534.09,393.33,10.91;12,112.66,547.64,355.37,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,290.61,534.09,215.38,10.91;12,112.66,547.64,165.19,10.91">Multimodal pre-training based on graph attention network for document understanding</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,286.56,547.64,149.55,10.91">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,561.19,394.53,10.91;12,112.30,574.74,393.68,10.91;12,112.66,588.29,107.17,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="12,173.53,574.74,256.77,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,601.84,393.53,10.91;12,112.66,615.39,393.33,10.91;12,112.66,628.93,330.86,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,313.71,601.84,192.48,10.91;12,112.66,615.39,138.31,10.91">Layoutlm: Pre-training of text and layout for document image understanding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,274.25,615.39,231.73,10.91;12,112.66,628.93,232.45,10.91">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1192" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,642.48,394.53,10.91;12,112.66,656.03,173.79,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m" coord="12,219.74,642.48,282.85,10.91">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,669.58,365.95,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><surname>Mindee</surname></persName>
		</author>
		<ptr target="https://github.com/mindee/doctr" />
		<title level="m" coord="12,151.05,669.58,147.22,10.91">doctr: Document text recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,86.97,393.33,10.91;13,112.66,100.52,394.52,10.91;13,112.66,114.06,100.87,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,291.84,86.97,214.15,10.91;13,112.66,100.52,51.14,10.91">Real-time scene text detection with differentiable binarization</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,186.47,100.52,266.17,10.91">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11474" to="11481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,127.61,393.33,10.91;13,112.66,141.16,393.33,10.91;13,112.66,154.71,244.11,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,209.68,127.61,296.30,10.91;13,112.66,141.16,252.97,10.91">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,375.04,141.16,130.95,10.91;13,112.66,154.71,150.03,10.91">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
