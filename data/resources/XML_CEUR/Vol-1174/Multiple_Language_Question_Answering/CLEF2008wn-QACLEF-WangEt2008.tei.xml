<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,168.36,75.29,269.94,12.58">Information Synthesis for Answer Validation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,235.62,114.54,40.83,9.02"><forename type="first">Rui</forename><surname>Wang</surname></persName>
							<email>rwang@coli.uni-sb.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.92,114.54,69.18,9.02"><forename type="first">Günter</forename><surname>Neumann</surname></persName>
							<email>neumann@dfki.de</email>
							<affiliation key="aff1">
								<orgName type="department">LT-Lab</orgName>
								<orgName type="institution">DFKI Stuhlsatzenhausweg</orgName>
								<address>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,168.36,75.29,269.94,12.58">Information Synthesis for Answer Validation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7F3D84FE1DE96704E1ECC8DA4278AE0F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Answer Validation</term>
					<term>Recognizing Textual Entailment</term>
					<term>Information Synthesis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report is about our participation in the Answer Validation Exercise (AVE2008). Our system casts the AVE task into a Recognizing Textual Entailment (RTE) problem and uses an existing RTE system to validate answers. Additional information from named-entity (NE) recognizer, question analysis component, and so on, is also considered as assistances to make the final decision. In all, we have submitted two runs, one run for English and the other for German. They have achieved f-measures of 0.64 and 0.61 respectively. Compared with our system last year, which purely depends on the output of the RTE system, the extra information does show its effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Related Work</head><p>Answer Validation is an important step for Question Answering (QA) systems, which aims to validate the answers extracted from natural language texts, and select the most proper answers for the final output.</p><p>Using Recognizing Textual Entailment (RTE-1 - <ref type="bibr" coords="1,285.63,415.32,74.16,9.02" target="#b2">Dagan et al., 2006</ref>; RTE-2 - <ref type="bibr" coords="1,406.85,415.32,88.29,9.02" target="#b0">Bar-Haim et al., 2006</ref>) to do answer validation has shown a great success <ref type="bibr" coords="1,255.18,426.84,77.14,9.02" target="#b8">(Peñas et al., 2007)</ref>. We also developed our own RTE system and participated in AVE2007. The RTE system proposed a new sentence representation extracted from the dependency structure, and utilized the Subsequence Kernel method <ref type="bibr" coords="1,353.59,449.82,123.14,9.02" target="#b1">(Bunescu and Mooney, 2006)</ref> to perform machine learning. We have achieved fairly high results on both the RTE-2 data set <ref type="bibr" coords="1,405.97,461.34,118.66,9.02">(Wang and Neumann, 2007a)</ref> and the RTE-3 data set <ref type="bibr" coords="1,165.17,472.86,116.87,9.02">(Wang and Neumann, 2007b)</ref>, especially on Information Extraction (IE) and QA pairs. However, on the AVE data sets, we still found much space for the improvement. Therefore, based on the system we developed last year, our motivation this year is to see whether using extra information, e.g. namedentity (NE) recognition, question analysis, etc., can make further improvement on the final results. This report will start with a brief introduction of our RTE system and then followed by the whole AVE system. The results of our two submission runs will be shown in section 4, and in the end, we will summarize our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The RTE System</head><p>The RTE system <ref type="bibr" coords="1,142.35,607.14,115.00,9.02">(Wang and Neumann, 2007a</ref>; <ref type="bibr" coords="1,264.87,607.14,117.25,9.02">Wang and Neumann, 2007b)</ref> is developed for RTE-3 Challenge <ref type="bibr" coords="1,70.92,618.60,103.74,9.02" target="#b4">(Giampiccolo et al., 2007)</ref>. The system contains a main approach with two backup strategies. The main approach extracts parts of the dependency structures to form a new representation, named Tree Skeleton, as the feature space and then applies Subsequence Kernels to represent TSs and perform Machine Learning. The backup strategies will deal with the T-H pairs which cannot be solved by the main approach. One backup strategy is called Triple Matcher, as it calculates the overlapping ratio on top of the dependency structures in a triple representation; the other is simply a Bag-of-Words (BoW) method, which calculates the overlapping ratio of words in T and H.</p><p>The main approach starts with processing H, since it is usually textually shorter than T, and the dependency structure also simpler. Tree skeletons are extracted based on the dependency structures derived by Minipar <ref type="bibr" coords="1,504.87,710.64,19.69,9.02;1,70.92,722.10,23.36,9.02" target="#b6">(Lin, 1998)</ref> for English and SMES <ref type="bibr" coords="1,192.41,722.10,128.87,9.02" target="#b7">(Neumann and Piskorski, 2002)</ref> for German. There are nouns in the lower part of the parse tree, and they share a common parent node, which is (usually) a verb in the upper part. Since content words usually convey most of the meaning of the sentence, we will mark the nouns as Topic Words and the verb as the Root Node. Together with the dependency paths in between, they form a subtree of the original dependency structure, which can be viewed as an extended version of Predicate-Argument Structure <ref type="bibr" coords="2,477.36,84.72,47.02,9.02;2,70.92,96.24,54.90,9.02" target="#b5">(Gildea and Palmer, 2002)</ref>. We call the subtree Tree Skeleton, the topic words Foot Nodes, and the dependency path from the noun to the root node Spine. If there are two foot nodes, the corresponding spines will be the Left Spine and the Right Spine.</p><p>On top of the tree skeleton of H, the tree skeleton of T can also be extracted. We assume that if the entailment holds from T to H, at least, they will share the same topics. Since in practice, there are different expressions for the same entity, we have applied some fuzzy matching techniques to correspond the topic words in T and H, like initialism, partial matching, etc. Once we successfully identify the topic words in T, we trace up along the dependency parse tree to find the lowest common parent node, which will be marked as the root node of the tree skeleton of T<ref type="foot" coords="2,124.26,187.96,3.00,5.40" target="#foot_0">1</ref> .</p><p>After some generalizations, we merge the two tree skeletons by 1) excluding the longest common prefixes for left spines and 2) excluding the longest common suffixes for right spines. Finally, we will get the dissimilarity of the two tree skeletons and we call it Spine Differences, i.e. Left Spine Difference (LSD) and Right Spine Difference (RSD). Then, since all the remaining symbols are POS tags and (generalized) dependency relation tags, they altogether form a Closed-Class Symbol (CCS) set. The spine difference is thus a sequence of CCSs. To represent it, we have utilized a Subsequence Kernel and a Collocation Kernel <ref type="bibr" coords="2,382.74,257.22,116.52,9.02">(Wang and Neumann, 2007a)</ref>.</p><p>We have also considered the comparison between root nodes and their adjacent dependency relations. We have observed that some adjacent dependency relations of the root node (e.g. &lt;SUBJ&gt;or &lt;OBJ&gt;) can play important roles in predicting the entailment relationship. For instance, the verb "sell" has a direction of the action from the subject to the object. In addition, the verb "sell" and "buy" convey totally different semantics. Therefore, we assign them two extra simple kernels named Verb Consistence (VC) and Verb Relation Consistence (VRC). The former indicates whether two root nodes have a similar meaning, and the latter indicates whether the relations are contradictive (e.g. &lt;SUBJ&gt; and &lt;OBJ&gt; are contradictive).</p><p>Finally, the main approach is assisted by two backup strategies: one is called the Triple Similarity and the other is called the BoW Similarity. Chief requirements for the backup strategy are robustness and simplicity. Accordingly, we construct a similarity function, which operates on two triple (dependency structure represented in the form of &lt;head, relation, modifier&gt;) sets and determines how many triples of H are contained in T. The core assumption here is that the higher the number of matching triple elements, the more similar both sets are, and the more likely it is that T entails H. The function uses an approximate matching function. Different cases (i.e. ignoring either the parent node or the child node, or the relation between nodes) might provide different indications for the similarity of T and H. We then sum them up using different weights and divide the result by the cardinality of H for normalization. The BoW similarity score is calculated by dividing the number of overlapping words between T and H by the total number of words in H after a simple tokenization according to the space between words. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The AVE System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing and Post-processing</head><p>Since the input of the AVE task is a list of questions, their corresponding answers and the documents containing these answers, we need to adapt them into T-H pairs for the RTE system. For instance, the question is, How many "Superside" world championships did Steve Webster win between 1987 and 2004? (id=87)<ref type="foot" coords="3,136.44,415.60,3.00,5.40" target="#foot_1">2</ref> The QA system gives out several candidate answers to this question, as follows, ten (id=87_1) 24 (id=87_2) … Each answer will have one supporting document where the answer comes from, like this, The most successful sidecar racer in Superside has been Steve Webster MBE, who has won ten world championships between 1987 and 2004. (id=87_1) The assumption here is that if the answer is relevant to the question, the document which contains the answer should entail the statement derived by combining the question and the answer. This section will mainly focus on the combination of the question and the answer and in the next sections the RTE system and how to deal with the output of the system will be described.</p><p>In order to combine the question and the answer into a statement, we need some language patterns. Normally, we have different types of questions, such as Who-questions asking about persons, What-questions asking about definitions, etc. Therefore, we manually construct some language patterns for the input questions. For the example given above (id=87), we will apply the following pattern, Steve Webster won &lt;Answer&gt; "Superside" world championships between 1987 and 2004. (id=87) Consequently, we substitute the &lt;Answer&gt; by each candidate answer to form Hs -hypotheses. Since the supporting documents are naturally the Ts -texts, the T-H pairs are built up accordingly, Id: 87_1 Entailment: Unknown Text: The most successful sidecar racer in Superside has been Steve Webster MBE, who has won ten world championships between 1987 and 2004. Hypothesis: Steve Webster won ten "Superside" world championships between 1987 and 2004. These T-H pairs can be the input for any generic RTE systems. In practice, after applying our RTE system, if the T-H pairs are covered by our main approach, we will directly use the answers; if not, we will use a threshold</p><p>to decide the answer based on the two similarity scores. Therefore, every T-H pair has a triple similarity score and a BoW similarity score, and for some of the T-H pairs, we directly know whether the entailment holds. The post-processing is straightforward, the "YES" entailment cases will be validated answers and the "NO" entailment cases will be rejected answers. In addition, the selected answers (i.e. the best answers) will naturally be the pairs covered by our main approach or (if not,) with the highest similarity scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Additional Components</head><p>The RTE system is used as a core component of the AVE system. Based on the error analysis of last year's results, this year we use additional components to out noisy candidates. Therefore, two extra components are added to the architecture, the NE recognizer and the question analyzer. For NE recognition, we use StanfordNER <ref type="bibr" coords="4,128.80,209.76,81.89,9.02" target="#b3">(Finkel et al., 2005)</ref> for English and SPPC <ref type="bibr" coords="4,307.82,209.76,130.01,9.02" target="#b7">(Neumann and Piskorski, 2002)</ref> for German; and for question analysis, we use the SMES system <ref type="bibr" coords="4,248.50,221.22,125.37,9.02" target="#b7">(Neumann and Piskorski, 2002)</ref>. The detailed workflow is as follows,</p><p>1. Annotate NEs in H, store them in an NE list; if the answer is an NE, store the NE type as A'_Type;</p><p>2. Analyze the question and obtain expected answer type, store it as A_Type;</p><p>3. Synthesize all the information, i.e. NE list, A_Type, A'_Type, BoW similarity, Triple similarity, etc. As for the example mentioned above (id=87), the additional information will be, NE list: Steve Webster (person), 1987 (date), 2004 (date); A_Type: Number A'_Type: Number Then, heuristic rules are straightforward to be applied, e.g. checking the consistence between A_Type and A'_Type, checking whether all (or how many of) the NEs also appear in the documents, etc. All these results together with the outputs of the RTE system will be synthesized to make the final decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We have submitted two runs for this year's AVE tasks, one for English and one for German. In the following, we will first show the table of the results and then present an error analysis. In the table, we notice that both for English and German, our validation system outperforms the best QA systems, which suggests the necessity of the validation step. Although there is a gap between the system performance and the perfect selection, the results are quite satisfactory. If we compare this year's results with last year's, the additional information does improve the results significantly.</p><p>Comparing the recall and precision, for both languages, the latter is worse. Therefore, we did some error analysis to see whether there is still some space for improvements. An interesting example in the English data is as follows, Question: What is the name of the best known piece by Jeremiah Clarke? (id=0011) Answer: a rondo (id=0011_7) Document: The most famous piece known by that name, however, is a composition by Jeremiah Clarke, properly a rondo for keyboard named Prince of Denmark's March.</p><p>Our system wrongly validated this answer, because "a rondo" is not the name of that music work. In fact, what we need here is a special proper name recognizer which can differentiate whether the noun is a name for a music work.</p><p>In the German data, other kinds of errors occur. For instance, Question: Wer war Russlands Verteidigungsminister 1994? (id=0020<ref type="foot" coords="5,385.38,164.98,3.00,5.40" target="#foot_2">3</ref> ) Answer: Pawel Gratschow (id=0020_6) Document: Wie der russische Verteidigungsminister Pawel Gratschow am Mittwoch in Tiflis weiter bekanntgab, will Rußland insgesamt fünf Militärstützpunkte in den Kaukasus-Republiken Georgien, Armenien und Aserbaidschan einrichten. 1994-02-02 The key problem here is that the year "1994" in the document might not be the year when the event happened, but the year of the report. This asks us to further synthesize the information we have, i.e. NE annotation and dependency parsing, to make better use of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>To sum up, in this paper, we described our participation of AVE 2008. Based on the experience of last year's participation, apart from the RTE core system, we add two extra components, NE recognizer and question analyzer, to further improve the results. The strategy is quite successful according to the comparison of system performances.</p><p>However, the problem has not been fully solved. Due to the noisy web data, filtering some documents in the preprocessing step could be even more effective than working on the post-processing phase. Another direction considered by us is to take a closer look at the different performances between different languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,70.92,274.41,453.49,8.10;3,70.92,284.73,453.54,8.10;3,70.92,295.11,453.61,8.10;3,70.92,305.43,453.62,8.10;3,70.92,315.81,453.62,8.10;3,70.92,326.13,132.00,8.10;3,71.16,152.64,453.37,107.76"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our AVE system uses the RTE system (Tera -Textual Entailment Recognition for Application) as a core component. The preprocessing module mainly adapts questions, their corresponding answers, and supporting documents into Text (T)-Hypothesis (H) pairs, assisted by some manually designed patterns. The post-processing module (i.e. the Answer Validation in the picture) will validate each answer and select a most proper one based on the output of the RTE system. The new modules added are the NE Recognition and Question Analysis. Thus, we will have extra information like NEs in the answers, Expected Answer Types (EATs), etc.</figDesc><graphic coords="3,71.16,152.64,453.37,107.76" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,70.92,447.69,433.20,207.82"><head>Table 1 .</head><label>1</label><figDesc>Results of our submissions compared with last year's</figDesc><table coords="4,81.24,471.96,422.88,183.56"><row><cell>Submission Runs</cell><cell>Recall</cell><cell>Precision</cell><cell>F-measure</cell><cell>Estimated QA Performance</cell><cell>QA Accuracy</cell></row><row><cell>100% VALIDATED (EN)</cell><cell>1</cell><cell>0.08</cell><cell>0.14</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>50%VALIDATED (EN)</cell><cell>0.5</cell><cell>0.08</cell><cell>0.13</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Perfect Selection (EN)</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>0.56</cell><cell>0.34</cell></row><row><cell>Best QA System (EN)</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>0.21</cell><cell>0.21</cell></row><row><cell>dfki07-run1 (EN)</cell><cell>0.62</cell><cell>0.37</cell><cell>0.46</cell><cell>N/A</cell><cell>0.16</cell></row><row><cell>dfki07-run2 (EN)</cell><cell>0.71</cell><cell>0.44</cell><cell>0.55</cell><cell>N/A</cell><cell>0.21</cell></row><row><cell>dfki08run1 (EN)</cell><cell>0.78</cell><cell>0.54</cell><cell>0.64</cell><cell>0.34</cell><cell>0.24</cell></row><row><cell>100% VALIDATED(DE)</cell><cell>1</cell><cell>0.12</cell><cell>0.21</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>50% VALIDATED (DE)</cell><cell>0.5</cell><cell>0.12</cell><cell>0.19</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Perfect Selection (DE)</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>0.77</cell><cell>0.52</cell></row><row><cell>Best QA System (DE)</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>0.38</cell><cell>0.38</cell></row><row><cell>dfki08run1 (DE)</cell><cell>0.71</cell><cell>0.54</cell><cell>0.61</cell><cell>0.52</cell><cell>0.43</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,78.42,748.59,442.14,8.10"><p>The Root Node of T is not necessary to be a verb, instead, it could be a noun, a preposition, or even a dependency relation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,78.42,748.59,298.40,8.10"><p>The "id" comes from AVE 2008 test data, i.e. "AVE2008-annotated-test-EN.xml".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,78.42,748.59,211.99,8.10"><p>This "id" comes from "AVE2008-annotated-test-DE.xml".</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,74.30,444.99,450.14,8.10;5,82.26,455.31,442.18,8.10;5,82.26,465.69,169.84,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,444.40,444.99,80.04,8.10;5,82.26,455.31,165.43,8.10">The Second PASCAL Recognising Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,271.39,455.31,253.05,8.10;5,82.26,465.69,115.01,8.10">Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the Second PASCAL Challenges Workshop on Recognising Textual Entailment<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.30,476.01,450.18,8.10;5,82.26,486.39,126.95,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,219.07,476.01,166.39,8.10">Subsequence Kernels for Relation Extraction</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,403.80,476.01,120.68,8.10;5,82.26,486.39,71.24,8.10">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.30,496.71,450.15,8.10;5,82.26,507.09,331.05,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,258.56,496.71,207.97,8.10">The PASCAL Recognising Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,165.13,507.09,74.28,8.10">MLCW 2005, LNAI</title>
		<editor>
			<persName><surname>Quiñonero-Candela</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3944</biblScope>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.29,517.41,450.21,8.10;5,82.26,527.79,442.31,8.10;5,82.26,538.11,192.54,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,365.63,517.41,158.88,8.10;5,82.26,527.79,192.36,8.10">Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling</title>
		<author>
			<persName coords=""><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,282.48,527.79,242.10,8.10;5,82.26,538.11,140.67,8.10">Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005)</title>
		<meeting>the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.30,548.49,450.16,8.10;5,82.26,558.81,418.38,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,323.74,548.49,200.72,8.10;5,82.26,558.81,34.86,8.10">The Third PASCAL Recognizing Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,133.07,558.81,252.29,8.10">Proceedings of the Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the Workshop on Textual Entailment and Paraphrasing<address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">2007. June 2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.30,569.19,450.27,8.10;5,82.26,579.51,373.49,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,208.26,569.19,229.29,8.10">The Necessity of Parsing for Predicate Argument Recognition</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,455.52,569.19,69.06,8.10;5,82.26,579.51,272.17,8.10">Proceedings of the 40th Meeting of the Association for Computational Linguistics (ACL 2002)</title>
		<meeting>the 40th Meeting of the Association for Computational Linguistics (ACL 2002)<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="239" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.30,589.89,409.26,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,132.87,589.89,156.75,8.10">Dependency-based Evaluation of MINIPAR</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,306.69,589.89,172.83,8.10">Workshop on the Evaluation of Parsing Systems</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.30,600.21,450.34,8.10;5,82.26,610.59,163.08,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,223.44,600.21,150.63,8.10">A Shallow Text Processing Core Engine</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Piskorski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,381.64,600.21,139.55,8.10">Journal of Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="476" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.30,620.91,450.28,8.10;5,82.26,631.29,77.76,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,287.51,620.91,165.02,8.10">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,488.72,620.91,35.86,8.10;5,82.26,631.29,73.90,8.10">the CLEF 2007 Working Notes</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,78.42,641.61,446.21,8.10;5,82.26,651.99,45.02,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,217.61,641.61,258.89,8.10">Recognizing Textual Entailment Using a Subsequence Kernel Method</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,494.99,641.61,29.65,8.10;5,82.26,651.99,22.49,8.10">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,78.42,662.31,446.02,8.10;5,82.26,672.69,442.30,8.10;5,82.26,683.01,20.28,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="5,219.24,662.31,305.20,8.10;5,82.26,672.69,53.60,8.10">Recognizing Textual Entailment Using Sentence Similarity based on Dependency Tree Skeletons</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,154.25,672.69,263.09,8.10">Proceedings of the Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the Workshop on Textual Entailment and Paraphrasing<address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">2007. June 2007</date>
			<biblScope unit="page" from="36" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,78.42,693.39,446.12,8.10;5,82.26,703.71,442.29,8.10;5,82.26,714.09,33.79,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="5,229.20,693.39,295.34,8.10;5,82.26,703.71,36.61,8.10">DFKI-LT at AVE 2007: Using Recognizing Textual Entailment for Answer Validation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,138.07,703.71,188.81,8.10">online proceedings of CLEF 2007 Working Notes</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">2007. September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
