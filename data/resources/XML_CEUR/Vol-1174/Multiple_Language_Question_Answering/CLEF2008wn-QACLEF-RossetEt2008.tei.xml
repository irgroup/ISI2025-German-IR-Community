<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,129.84,85.12,315.69,15.49">The LIMSI participation to the QAst track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,128.88,118.88,55.11,8.97"><forename type="first">Sophie</forename><surname>Rosset</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Spoken Language Processing Group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>B.P. 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.68,118.88,61.81,8.97"><forename type="first">Olivier</forename><surname>Galibert</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Spoken Language Processing Group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>B.P. 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.72,118.88,74.94,8.97"><forename type="first">Guillaume</forename><surname>Bernard</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Spoken Language Processing Group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>B.P. 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.32,118.88,49.35,8.97"><forename type="first">Eric</forename><surname>Bilinski</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Spoken Language Processing Group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>B.P. 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,396.00,118.88,45.65,8.97"><forename type="first">Gilles</forename><surname>Adda</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Spoken Language Processing Group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>B.P. 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,129.84,85.12,315.69,15.49">The LIMSI participation to the QAst track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5D462610659C0CB833D3968BEF5895AE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation Question answering, speech transcriptions</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present the LIMSI question-answering systems on speech transcripts which participated to the QAst 2008 evaluation. These systems are based on a complete and multilevel analysis of both queries and documents. These systems use an automatically generated research descriptor. A score based on those descriptors is used to select documents and snippets. The extraction and scoring of candidate answers is based on proximity measurements within the research descriptor elements and a number of secondary factors. We participated to all the subtasks and submitted 18 runs (for 16 sub-tasks). The evaluation results for manual transcripts range from 31% to 45% for accuracy depending on the task and from 16 to 41% for automatic transcripts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the QA and Information Retrieval domains progress has been demonstrated via evaluation campaigns for both open domain and limited domains <ref type="bibr" coords="1,248.40,540.32,10.77,8.97" target="#b6">[7,</ref><ref type="bibr" coords="1,261.96,540.32,7.53,8.97" target="#b3">4,</ref><ref type="bibr" coords="1,272.28,540.32,7.26,8.97" target="#b0">1]</ref>. In these evaluations systems are presented with either independent or linked questions and should provide one answer extracted from textual data to each question. Recently, there has been growing interest in extracting information from multimedia data such as meetings, lectures... Spoken data is different from textual data in various ways. The grammatical structure of spontaneous speech is quite different from written discourse and include various types of disfluencies. The lecture and interactive meeting data provided in QAst evaluation are particularly difficult due to run-on sentences and interruptions. Most of the QA systems use a complete and deep syntactic and semantic analysis of both the question and the document, or snippets given by a search engine, and search for the answer in the result. Such an analysis cannot be performed reliably on the data we are interested in.</p><p>The Question Answering on Speech Transcripts track of the QA@CLEF task gives then an opportunity to evaluate approaches able to handle speech transcriptions.</p><p>In this paper, we present the architecture of the QA systems developed at LIMSI for the QAst evaluation. This year 10 general subtasks have been proposed : -T1a : Question Answering in manual transcriptions of lectures (CHIL corpus) -T1b : Question Answering in automatic transcriptions of lectures (CHIL corpus) -T2a : Question Answering in manual transcriptions of meetings (AMI corpus) -T2b : Question Answering in automatic transcriptions of meetings (AMI corpus) -T3a : Question Answering in manual transcriptions of broadcast news for French (ESTER corpus) -T3b : Question Answering in automatic transcriptions of broadcast news for French (ESTER corpus) -T4a : Question Answering in manual transcriptions of European Parliament Plenary sessions in English (EPPS English corpus) -T4b : Question Answering in automatic transcriptions of European Parliament Plenary sessions in English (EPPS English corpus) -T5a : Question Answering in manual transcriptions of European Parliament Plenary sessions in Spanish (EPPS Spanish corpus) -T5b : Question Answering in automatic transcriptions of European Parliament Plenary in Spanish (EPPS Spanish corpus)</p><p>For the tasks T3b, T4b and T5b, 3 different collections (one collection corresponding to one automatic speech recognition output) have been provided with 3 different Word Error Rates (WER) in order to allow studies on the impact of the WER on the Question Answering task. We submitted 2 runs for T3a and T5a tasks and one for each other tasks. In total, we submitted 18 runs. We used the exact same system for each manual and ASR collection in order to be able to evaluate the impact of the WER on the overall system. For the different languages and tasks, we used basically the same system, the only changes were the analysis which is language dependant and the tuning parameters learned on the development data set.</p><p>Figure <ref type="figure" coords="2,138.72,359.00,4.98,8.97">1</ref> shows the general organisation of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FIG. 1 -General overview of the LIMSI QAst systems</head><p>The following sections present the documents and queries pre-processing and the non-contextual analysis with the work carried out this year on the adaptation of our analysis system to Spanish. In section 3, we present the documents and snippets selection and the answer extraction and scoring. Section 4 finally presents the results for these two systems on both development and test data.</p><p>Usually, the syntactic/semantic analysis is different for the document and for the query ; our approach is instead to perform the same complete and multilevel analysis on both queries and documents. There are several reasons for this : First of all, the system has to deal with both transcribed speech (transcriptions of meetings and lectures, user utterances) and text documents, so there should be a common analysis that takes into account the specifics of both data types. Moreover, incorrect analysis due to the lack of context or limitations of hand-coded rules are likely to happen on both data types, so using the same strategy for document and utterance analysis helps to reduce their negative impact. But first, we need to reduce the surface forms variations between the different modalities (text, manual transcripts, automatic transcripts) in order to have a common representation and use of words, sentences, case, etc. This process, a superset of tokenization, is called normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Normalization</head><p>Normalization, in our application, is the process by which raw texts are converted to a text form where words and numbers are unambiguously delimited, capitalization happens on proper nouns only, punctuation is separated from words, and the text is split into sentence-like segments (or as close to sentences as is reasonably possible). Different normalization steps are applied, depending of the kind of input data ; these steps are :</p><p>1. Separating words and numbers from punctuation.</p><p>2. Reconstructing correct case for the words.</p><p>3. Adding punctuation.</p><p>4. Splitting into sentences at period marks.</p><p>Reconstructing the case and adding punctuation is done in the same process based on using a fully-cased, punctuated language model <ref type="bibr" coords="3,189.96,431.60,10.60,8.97" target="#b2">[3]</ref>. A word graph was built covering all the possible variants (all possible punctuations added between words, all possible word cases), and a 4-gram language model was used to select the most probable hypothesis. The language model was estimated on House of Commons Daily Debates, final edition of the European Parliament Proceedings and various newspapers archives. The final result, with uppercase only on proper nouns and words clearly separated by white-spaces, is then passed to the non-contextual analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Analysis module</head><p>The non-contextual analysis aims at extracting, from both user utterances and documents, what is considered to be pertinent information. The analysis covers multiple levels : Named entities detection, Linguistic chunking, Question words classification and Question topic detection. An example of an analysis result appears on figure 2. In that example, New-York is recognized as a named entity, specifically an organization. municipal elections is chunked together as a compound noun, which makes it available as a search key in the QA system. who is detection as a question word related to a person, and its combination with won allows to classify the question as one about someone's victory or achievement.</p><p>The types we need to detect correspond to two levels of analysis : named-entity recognition and chunkbased shallow parsing. Various strategies for named-entity recognition using machine learning techniques</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FIG. 2 -Example of user utterance analysis</head><p>have been proposed <ref type="bibr" coords="4,195.24,140.72,10.77,8.97" target="#b1">[2,</ref><ref type="bibr" coords="4,209.76,140.72,7.41,8.97" target="#b4">5,</ref><ref type="bibr" coords="4,220.92,140.72,7.26,8.97">6]</ref>. In these approaches, a statistically pertinent coverage of all defined types and subtypes induced the need of a large number of occurrences, and therefore rely on the availability of large annotated corpora which are difficult to build. Rule-based approaches to named-entity recognition (e.g. <ref type="bibr" coords="4,130.20,176.60,11.22,8.97" target="#b7">[8]</ref>) rely on morphosyntactic and/or syntactic analysis of the documents. However, in the present work, performing this sort of analysis is not feasible : the speech transcriptions are too noisy to allow for both accurate and robust linguistic analysis based on typical rules. We use a internal tool to write grammars based on regular expressions on words. Our tools allows the use of lists for initial detection, and the definition of local contexts and simple categorizations. This engine matches (and substitutes) regular expressions using words as the base unit instead of characters. This property allows for a more readable syntax than traditional regular expressions and enables the use of classes (lists of words) and macros (sub-expressions in-line in a larger expression).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Adaptation to English and Spanish languages</head><p>This analysis is obviously language dependant. The French analyser detects about 300 types and constitutes the basis for the Spanish and English (T4 task only) analyzers adaptation. This year was our first attempt in working with spanish. The Spanish analyser has been created as a simple adaptation of the French one where only the lexicons were adapted, and only around 50% of them. For the English a deeper adaptation is required, in particular the order in which the blocks of rules are applied is reversed. The English and Spanish analysers detect only about a hundred types.</p><p>We now plan to use some aligned corpus in order to automatically acquire some specific lexicons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Question-Answering system</head><p>The input request takes the form of an analyzed question. From that information a Search Descriptor is built which is the basis of all the following search algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Search Descriptor Generation</head><p>This descriptor is structured in 3 parts : the elements of the input considered pertinent for the search, the expected type or types for the answer, and a number of tuning parameters.</p><p>The types considered pertinent are the named entities (standard, extended and nonspecific) and the linguistic chunks. Each entity also carries a weight, set by rules, and a critical/secondary flag. Critical entities must be present in a document near a candidate answer, secondary entities only give a bonus to the final score. This distinction aims at increasing the system precision. In practice, all named entities and some linguistic chunks are considered critical according to, once again, a set of rules. The expected answer types and their weights are decided using a 2-level rule-based classifier built by examining the development data and generalized by hand. Rhe tuning parameters are set by systematic trials on the development data. Moreover, as shown in Figure3, possible transformations of the elements are described. These possible transformations are obtained from a few rules. This year, we used this concept to allow weighted morphological derivations and synonymic transformations. The lexicon used for morphological derivations have been built on our corpus using the analysis module to extract all values of the considered types (for example all adjectives and nouns) and to apply some derivational rules on these lists in order to built morphological correspondances. We tried various algorithms and that simple method was the one obtaining the best results on the development data set for each language and task.</p><p>Question : when was Hans Krasa killed ? -Critical element -1,0 pers identity(Hans Krasa) -0,2 pers expand(Hans Krasa) -Secondary element -1,0 verb identity(killed) -0,7 verb lemma(killed) -0,5 verb synonym(killed) -0,5 subs verb_subs(killed) -Answer types -1,0 full_date -0,9 month_year, day_month, hour -0,7 year FIG. <ref type="figure" coords="5,94.92,333.56,4.98,8.97">3</ref> -Example of a Search Descriptor : each element contains the list of triplets (type, transformation, value) under which it is expected to appear. Each triplet is (0,5 verb synonym(killed) a synonym of killed is accepted with a weight of 0.5) ; each possible answer type contains also a weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Documents selection and scoring</head><p>Once the Search Descriptor (SD) is built, the next step is to generate a list of the n documents with the highest probability of containing the answer. The method is fundamentally simple : give a score to all the documents that include at least one element of the SD and pick the n with the best scores. The score we've chosen is based on the counts of occurrences of elements, ponderated by the SD weights. The tree structure is taken into account : the scores of elements in the same node are added, the scores for children have their geometric mean taken. The geometric mean has two advantages, it avoids needing to compensate for the differences in global frequency of the elements, since the counts are multiplied together, and it ensures that a zero count on a critical element propagates into a global zero count. Accordingly, 1 is added to the secondary element nodes to avoid the zero-propagation effect. The document score is the score of a virtual root node of all the top nodes.</p><p>The index gives the raw occurrence counts for each of the elements. The analysis producing hierarchical annotations, the same instance of an elements can appear under multiple types. For instance, France is typed as both country and location or organization each time it appears in a document. To compensate for that the counts are recomputed by subtracting the number of occurrences taken into account for the other elements of the same or upper nodes.</p><p>In the specific case of QAst where the document count is very low, n is set high enough that all the documents with as least one element are picked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Snippets selection and scoring</head><p>The snippet selection step aims at selecting in the documents blocks of lines with a high expectation of containing the answer. That action has a dual effect : faster answers by reducing the number of candidates to look at, and better precision of the answers given by reducing the noise introduced by faraway candidates.</p><p>The idea of the method is that elements of the SD has a distance of influence or range which is counted in lines, that is sentences for text documents or utterances for spoken documents. The algorithm starts by extracting all the lines which have elements in range to satisfy all the critical elements of the SD, building that way a series of blocks. Too big blocks, i.e. above a critical size, are split up to try to push them under the critical size by temporarily promoting some of the secondary elements to critical status. Eventually all the blocks are small enough or all the elements have become critical and no more splitting is possible.</p><p>We want these snippets to be self-contained for later candidate evaluation, which means that they must include all the elements found in the SD that made them pertinent. But in some cases two critical elements are too far apart from each other that the line they're in is kept, while some lines in the middle are within range of both and as such form an element-less snippet. To fix these situations the snippets frontiers are extended to cover the neighboring lines where influential elements are present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Answers selection and scoring</head><p>The snippets are sorted by score and examined one by one independently. Every element in a snippet with a type found in the list of expected answer types of the SD is considered an answer candidate. Each candidate is given a score, which is the sum of the the distances between itself and the elements of the SD, each elevated to the power -α, ponderated by the element weights. That score is smoothed with the snippet score through a δ-ponderated geometric mean. This extraction and scoring stops once a number m of candidates has been reached, once again to control the speed of the system. All the scores for the different instances of the same element are added together, and in order to compensate for the differencing natural frequencies of the entities in the documents the final score is divided by the occurence count in all the documents and in all the examined snippets, each elevated to the power β and γ respectively. The entities with the best scores then win. The tuning parameters α, β, γ, δ all come from the third part of the SD and has been selected by systematic trials on the develoment corpus. These parameters are set for each question class.</p><p>Our second approach for answer scoring is built upon the results of that first one. We compute a new ranking of the answers with a tree transformation method. For each candidate answer to a question, we transform the tree of the snippet from where the answer was extracted into the tree of the question. The sequence of operations used for the transformation gives us a transformation cost. The candidate answers are re-ranked using these costs. We applied this method as a second run for T3a and T5a tasks. The results do not yet show the expected improvement. But this work is still in progress and further analysis is needed. One positive aspect of these trials is that they show that this approach is completely language independant (same results are obtained for French and Spanish languages).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training and Development</head><p>The official development data consisted of 50 questions for each task. The development documents were 10 seminars for the T1 task, 50 meetings for the T2 task, 6 shows for the T3 task, 4 for the T4 task and 1 for the T5 task. As we have observed last year, 50 questions are clearly not enough to correctly tune a system. We decided to hand-build and use a corpus of reformulated questions for each task and used them as training corpus. We built corpus of questions/answering/documents for the T3, T4 and T5 tasks and we used the 2007 evaluation data for T1 and T2 tasks as blind development data. The -Poor quality of the answer scoring. Intrinsically, working only with distances and redundancy is not enough (especially with such a small number of documents as in QAst), dependencies in particular would probably be a big help. -For T1 and T2, large differences between the development and test data, in particular related to the definition questions, made for over-specialisation in some of the routing rules and poor tuning. -Some analysis errors, especially in Spanish and English, resulted in making some answers impossible to extract by the system. The analysis in better in French (T3) and it shows. While the first and last point are entirely due to the system, the second one could have been avoided if the development data had been more representative of the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">General results on automatic transcripts</head><p>did not do anything specific in order to handle recognition errors in the documents, the systems have been used as-is. As such our results show the loss due to the ASR on a decent but non-adapted system. The T3b, T4b and T5b tasks provided three different ASR outputs allowing an analysis of the impact of WER on the overall QA results. Table <ref type="table" coords="9,188.88,115.28,4.98,8.97">8</ref> gives the results on the ASR output depending on the task, the word error rate and the accuracy obtained on the respective manual transcriptions. The WERs for the T1b and T2b tasks are unknown. The better quality, including robustness, on the French analysis shows up immediatly again, the loss at equivalent error rate being roughly halved (5% instead of 10% at 11% WER). The loss rate does not seem to be easily predictable from the WER, but there are not enough data points to be sure. It may just be that 100 questions and a small number of documents is not enough to compute reliable statistics. A deeper analysis measuring the word error rate by word category could provide some intersting insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ASR_A</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented the LIMSI question-answering systems on speech transcripts which participated to the QAst 2008 evaluation. These systems are based on a complete and multi-level language dependant analysis of both queries and documents followed by a language independant information retrieval and answer extraction and scoring. These systems obtained state-of-the-art results on the different tasks and languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,73.44,175.76,428.51,161.85"><head>table 1</head><label>1</label><figDesc>Off. Dev. : the official development data ; Ref. q. : the reformulated questions based on the development documents ; Blind Corpus : 2007 test data for T1 and T2 and new questions for T4, new questions and new documents for T3 and T5 ; Between parenthesis is the number of documents</figDesc><table coords="7,378.24,175.76,123.70,8.97"><row><cell>gave a general overview of the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,73.44,371.31,428.53,306.25"><head>4.2 Results 4.2.1 General results on manual transcripts</head><label></label><figDesc>We compared the results obtained on our different corpus (training, on which the tuning is done, and development, blind corpus on which only the synthetic scores are looked at) and on the 2008 evaluation. The following tables give results obtained on the different development sets and on the test. Comparison between Information Retrieval module and answer extraction and scoring module Table7gives the results for information retrieval and answer extraction and scoring allowing a direct comparison between them. A quick analysis of the problems have shown us that 3 main error sources were present :</figDesc><table coords="7,73.44,482.36,428.53,195.21"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">50+</cell><cell>bc</cell><cell>test</cell></row><row><cell></cell><cell></cell><cell cols="2">Accuracy 82%</cell><cell cols="3">79.1% 41.5% 45%</cell></row><row><cell></cell><cell></cell><cell>MRR</cell><cell>0.90</cell><cell cols="2">0.86</cell><cell>0.50</cell><cell>0.49</cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell cols="4">100% 94.9% 61.3% 58%</cell></row><row><cell cols="7">TAB. 4 -T3a task (French, BN data : Results on the 3 different corpus and the test (50 : 50 official</cell></row><row><cell cols="7">development questions ; 50+ : 350 reformulated questions ; bc : Blind Corpus, 248 questions on 3 new</cell></row><row><cell cols="4">documents not in the 2008 test ; test : 2008 test)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>50</cell><cell cols="2">50+</cell><cell>bc</cell><cell>test</cell></row><row><cell></cell><cell></cell><cell cols="5">Accuracy 80% 65.5% 26.9% 33%</cell></row><row><cell></cell><cell></cell><cell>MRR</cell><cell cols="3">0.84 0.68</cell><cell>0.31</cell><cell>0.42</cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell cols="4">90% 71.2% 38.7% 56%</cell></row><row><cell cols="7">TAB. 5 -T4a task (English, EPPS data : Results on the 3 different corpus and the test (50 : 50 official deve-</cell></row><row><cell cols="7">lopment questions ; 50+ : 277 reformulated questions ; bc : Blind Corpus, 186 questions on the development</cell></row><row><cell cols="2">documents ; test : 2008 test)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>50</cell><cell cols="2">50+</cell><cell>bc</cell><cell>test</cell></row><row><cell></cell><cell></cell><cell cols="5">Accuracy 68% 65.4% 36.1% 33%</cell></row><row><cell></cell><cell></cell><cell>MRR</cell><cell cols="3">0.76 0.71</cell><cell>0.45</cell><cell>0.36</cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell cols="4">88% 79.7% 61.1% 42%</cell></row><row><cell></cell><cell></cell><cell cols="4">Information Retrieval</cell><cell>Answer Extraction</cell></row><row><cell></cell><cell cols="6">Task Acc. MRR Recall Acc. MRR Recall</cell></row><row><cell></cell><cell>T1a</cell><cell cols="2">43% 0.50</cell><cell>58%</cell><cell></cell><cell>41% 0.45</cell><cell>52%</cell></row><row><cell></cell><cell>T2a</cell><cell cols="2">46% 0.53</cell><cell>62%</cell><cell></cell><cell>33% 0.40</cell><cell>51%</cell></row><row><cell></cell><cell>T3a</cell><cell cols="2">69% 0.75</cell><cell>84%</cell><cell></cell><cell>45% 0.49</cell><cell>58%</cell></row><row><cell></cell><cell>T4a</cell><cell cols="2">53% 0.62</cell><cell>73%</cell><cell></cell><cell>33% 0.42</cell><cell>56%</cell></row><row><cell></cell><cell>T5a</cell><cell cols="2">50 50% 0.56 50+</cell><cell>65%</cell><cell>bc</cell><cell>test 33% 0.36</cell><cell>42%</cell></row><row><cell></cell><cell cols="2">Accuracy 96%</cell><cell cols="4">83.5% 64.3% 41%</cell></row><row><cell>TAB. 7 -</cell><cell>MRR</cell><cell>0.98</cell><cell>0.85</cell><cell cols="2">0.71</cell><cell>0.45</cell></row><row><cell></cell><cell>Recall</cell><cell cols="2">100% 88%</cell><cell cols="3">80.6% 52%</cell></row><row><cell cols="7">TAB. 2 -T1a task (English, seminar data) : Results on the 3 different corpus and the test (50 : 50 official</cell></row><row><cell cols="7">development questions ; 50+ : 565 reformulated questions ; bc : 2007 test data ; test : 2008 test)</cell></row><row><cell></cell><cell></cell><cell>50</cell><cell>50+</cell><cell></cell><cell>bc</cell><cell>test</cell></row><row><cell></cell><cell cols="6">Accuracy 66% 60.4% 44.8% 33%</cell></row><row><cell></cell><cell>MRR</cell><cell cols="2">0.72 0.66</cell><cell cols="2">0.52</cell><cell>0.40</cell></row><row><cell></cell><cell>Recall</cell><cell cols="5">82% 75.5% 61.5% 51%</cell></row><row><cell cols="7">TAB. 3 -T2a task (English, meeting data) : Results on the 3 different corpus and the test (50 : 50 official</cell></row><row><cell cols="7">development questions ; 50+ : 587 reformulated questions ; : 2007 test data ; test : 2008 test)</cell></row></table><note coords="8,110.40,350.96,428.53,8.97;8,110.16,362.96,428.50,8.97;8,110.16,374.96,205.81,8.97"><p>TAB. 6 -T5a task, Spanish, EPPS data : Results on the 3 different corpus (50 : 50 official development questions ; 50+ : 217 reformulated questions ; Blind Corpus, 36 questions, development document + one other document not in the 2008 test ; test : 2008 test</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,73.44,161.00,428.56,101.73"><head></head><label></label><figDesc>Comparative results for T3b, T4b, T5b and corresponding manual data ; Acc. : % correct answers in first rank ; WER : Word Error Rate</figDesc><table coords="9,73.68,161.00,340.79,89.73"><row><cell></cell><cell>ASR_B</cell><cell>ASR_C</cell><cell>MAN</cell></row><row><cell>Acc. WER</cell><cell>Acc. WER</cell><cell>Acc. WER</cell><cell>Acc.</cell></row><row><cell>T3 41% 11%</cell><cell cols="3">25% 23.9% 21% 35.4% 45%</cell></row><row><cell cols="2">T4 21% 10.6% 20% 14%</cell><cell cols="2">19% 24.1% 33%</cell></row><row><cell cols="4">T5 24% 11.5% 19% 12.7% 23% 13.7% 33%</cell></row><row><cell>TAB. 8 -</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,90.00,521.24,411.94,8.97;9,90.00,533.12,411.90,8.97;9,90.00,545.12,22.65,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,304.92,521.24,197.02,8.97;9,90.00,533.12,188.20,8.97">Evaluation of question-answering systems : The French EQueR-EVALDA Evaluation Campaign</title>
		<author>
			<persName coords=""><forename type="first">Christelle</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brigitte</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Vilnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,296.76,533.12,97.05,8.97">Proceedings of LREC&apos;06</title>
		<meeting>LREC&apos;06<address><addrLine>Genoa -Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,561.92,412.04,8.97;9,90.00,573.80,167.01,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,318.96,561.92,183.08,8.97;9,90.00,573.80,21.68,8.97">Nymble : a high-performance learning namefinder</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,129.60,573.80,97.29,8.97">Proceedings of ANLP&apos;97</title>
		<meeting>ANLP&apos;97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,590.60,412.21,8.97;9,90.00,602.48,137.25,8.97" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,315.00,590.60,187.21,8.97;9,90.00,602.48,28.01,8.97">Improved machine translation of speech-to-text outputs</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Déchelotte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Antwerp. Belgium</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,619.28,411.80,8.97;9,90.00,631.16,411.90,8.97;9,90.00,643.16,332.61,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,200.28,631.16,282.40,8.97">Overview of the CLEF 2007 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cristea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,90.00,643.16,176.61,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,659.96,411.92,8.97;9,90.00,671.84,98.49,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,201.12,659.96,251.19,8.97">Efficient support vector classifiers for named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,470.76,659.96,31.16,8.97;9,90.00,671.84,67.72,8.97">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,126.72,48.20,412.26,8.97;10,126.72,60.20,213.93,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,303.60,48.20,235.38,8.97;10,126.72,60.20,25.48,8.97">Named entity recognition from spontaneous open-domain speech</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Comelles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,180.96,60.20,59.44,8.97">InterSpeech&apos;05</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,126.72,75.68,411.89,8.97;10,126.72,87.68,332.01,8.97" xml:id="b6">
	<analytic>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,284.40,75.68,254.21,8.97;10,126.72,87.68,21.68,8.97">The Sixteenth Text REtrieval Conference Proceedings (TREC 2007)</title>
		<editor>
			<persName><forename type="first">Buckland</forename><surname>Voorhees</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="500" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,126.72,103.16,411.85,8.97;10,126.72,115.16,64.65,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,276.00,103.16,184.95,8.97">Automatic processing of proper names in texts</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wolinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Vichot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dillet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,478.92,103.16,59.65,8.97;10,126.72,115.16,34.73,8.97">Proceedings of EACL&apos;95</title>
		<meeting>EACL&apos;95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
