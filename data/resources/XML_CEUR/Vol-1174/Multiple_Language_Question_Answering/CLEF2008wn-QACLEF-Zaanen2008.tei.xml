<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,132.24,98.63,338.73,15.51;1,248.16,120.59,106.84,15.51">Multi-lingual Question Answering using OpenEphyra</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,260.04,153.97,83.19,9.96"><forename type="first">Menno</forename><surname>Van Zaanen</surname></persName>
							<email>mvzaanen@uvt.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Tilburg University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,132.24,98.63,338.73,15.51;1,248.16,120.59,106.84,15.51">Multi-lingual Question Answering using OpenEphyra</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0F905278D00D4DE0022A607C7460D43B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>I.2.7 [Natural Language Processing]: Text Analysis Measurement, Performance, Experimentation Question answering, Multi-lingual retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this article we describe our submission to the Dutch-English QA@CLEF task. We took the publicly available OpenEphyra question answering system, which is an opensource English question answering system. This was turned into a multi-lingual variant by translating questions from Dutch to English using Systran's online-translation system. The current approach has some known problems, for example, we do not distinguish between factoid, lists, and definition questions (all questions are treated as factoid questions), OpenEphyra does not provide support text for answers (text in the document surrounding the answer is used as support text), temporal restrictions and anaphora are not handled at all. The amount of modifications of OpenEphyra required to run the experiment were such that due to time constraints only one experiment could be submitted. The original idea behind this research was to investigate the impact of the quality of the question analysis. In particular, we are interested in the difference between the analysis on the question in the source language and the question in the target language.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In 2007, we participated in the Dutch-English QA@CLEF task using the AnswerFinder system (van Zaanen and <ref type="bibr" coords="1,166.58,619.69,53.66,9.96" target="#b8">Mollá, 2007;</ref><ref type="bibr" coords="1,223.35,619.69,126.28,9.96" target="#b2">Mollá and van Zaanen, 2005;</ref><ref type="bibr" coords="1,352.74,619.69,103.74,9.96" target="#b9">van Zaanen et al., 2006)</ref>. The aim of that research was to investigate the flexibility, configurability, and scalability of the AnswerFinder system. The experiment showed that AnswerFinder is flexible (in that it allows the necessary modifications), configurable (allowing for dynamic selection of parameters and algorithms in the different phases), and scalable (working on larger document collections) enough to be used for different projects. The experiment also showed that AnswerFinder is usable in a multi-lingual context, although several additional changes were needed to ensure useful results.</p><p>Currently, the main problem of using AnswerFinder is its reliance on the Connexor <ref type="bibr" coords="2,458.53,61.33,54.63,9.96;2,90.00,73.33,86.22,9.96" target="#b4">(Tapanainen and Järvinen, 1997)</ref> dependency parser. Licences for this parser are assigned per computer, which limits the portability of the system. In the meantime, the AnswerFinder project has finished and the licence has expired. This led to a largely reduced functionality of the system. While the framework is still useable, the shallow semantic representations <ref type="bibr" coords="2,384.60,109.21,56.76,9.96" target="#b1">(Mollá, 2006)</ref>, which relies on output of Connexor, cannot be generated anymore.</p><p>With the limited usability of AnswerFinder in mind, there are two possibilities to resolve this problem. Firstly, we could re-implement the shallow representations based on the output of a different, non-commercial, dependency parser. This requires a thorough analysis of the output of the parsers and a mapping from the one representation to the other. Initial attempts at this have been made, but are currently not in a usable stage. Secondly, we could look for another question answering system that has a similar setup to AnswerFinder.</p><p>The original background of modifying AnswerFinder from an English-only question answering system into a multi-lingual one was to investigate the impact of the quality of the output of several phases. The idea is that when analysis is performed on (partially) incorrectly translated texts (in our case questions), the results will be worse than when that same analysis is performed on the original questions.</p><p>The research described in this article describes a first attempt at answering this question. We explain how we modify an existing English question answering system into a multi-lingual version by combining several existing components. Using this modified system, we participated in the Dutch-English QA@CLEF competition. Unfortunately, time constraints did not allow us to compare the results of question answering using analysis of source question against that of the target language. However, the system described here is ready to be used for this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System components</head><p>The results of the Dutch-English QA@CLEF task are generated using a system that is created using several components. All of the components are freely available. Starting from the OpenEphyra open-source question answering system, we appended Systran's online machine translation system, which translated the Dutch questions into English, to allow OpenEphyra to answer English questions. In the future, we would like to perform question classification on the Dutch questions and incorporate these classes into the English question answering system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question answering system</head><p>We start building the multi-lingual question answering system using an existing question answering system. OpenEphyra<ref type="foot" coords="2,185.64,496.34,3.97,4.84" target="#foot_0">1</ref> is an open-source question answering system. This system is based on Ephyra, which is developed by Nico Schlaefer and has participated in the TREC question answering competition <ref type="bibr" coords="2,145.08,521.05,96.48,9.96" target="#b3">(Schlaefer et al., 2006)</ref>.</p><p>The system is written in Java and is highly configurable. In fact, the setup of the framework is very similar to AnswerFinder. It consists of a collection of factory classes (these generate objects with a certain interface) for all the phases in the system, which makes it very easy to select the wanted algorithm and also extending the system by adding a new algorithms is easy. As long as the algorithm provides the functionality required by the interface and the factory knows of it, the algorithm can be selected without rebuilding the system.</p><p>OpenEphyra needs to be modified in several ways before it can be used in this particular context. Firstly, by default, OpenEphyra searches for answers on the Internet. In QA@CLEF, the answers need to be found in a fixed set of documents. In 2008, for the Dutch-English task, the November 2007 dump of Wikipedia<ref type="foot" coords="2,246.60,639.74,3.97,4.84" target="#foot_1">2</ref> (in HTML format), the 1994 collection of the Los Angeles Times (LA94) and the 1995 collection of The Glasgow Herald (GH95) were used.</p><p>The indexing of all the documents is done using Indri<ref type="foot" coords="3,334.56,60.50,3.97,4.84" target="#foot_2">3</ref> , which is part of the open-source Lemur project<ref type="foot" coords="3,120.84,72.50,3.97,4.84" target="#foot_3">4</ref> . Lemur is a toolkit for language modeling and information retrieval. Building the indices using Indri is straightforward. However, the documents need to be converted so Indri can find the correct XML tags to index. The LA94 documents are already in the correct format, but in the GH95 documents we need to inserted an XML "P" tag within the "TEXT" tag. The Wikipedia documents also need to be converted. We built documents in a format similar to those of the LA94 and GH95 documents. The filename of the Wikipedia page serves as the contents of the "DOCNO" and "DOCID" tags, the title of the page is used in the "HEADLINE" tag and the document contents are put within the "TEXT" tag (combined with a "P" tag). The indices generated by Indri are used by OpenEphyra directly.</p><p>Secondly, the input and output formats of the questions and answers in OpenEphyra are aimed towards the TREC competitions. However, the formats in used in CLEF are somewhat different. Starting from the questions in CLEF format, we removed all XML formatting. The plain text questions are then translated (as described in section 2.2). The translated questions are then inserted into an XML file in TREC format. The TREC format question file format contains similar information to the CLEF format with one main difference. Where in CLEF questions are organized by numbered topic, the TREC topics contain text. This text is used in OpenEphyra in the anaphora resolution module. Since in this experiment we do not have topic names, we have to turn anaphora resolution off (or implement a more complex algorithm that takes into account resolution based on words in the previous questions or answers). The file containing the questions in TREC format is fed to OpenEphyra, which results in an answer file in TREC format. This file is then converted into the final CLEF answer file. Essentially, this conversion is straightforward. The only problem is that in the TREC format, no support text (supporting the answer) is provided. We have solved this by retrieving the document in which the answer is found and identify the answer string in the text. The sentence around the answer is used as supporting text. Note that this does not necessarily provide correct support text, especially if the answer string occurs in the document multiple times.</p><p>Finally, we want to experiment with moving the analysis of the question from the target language (English) to the source language (Dutch). Normally, OpenEphyra performs question analysis. The idea is to perform the analysis on the original questions and insert this information into OpenEphyra directly, thus disabling the normal question analysis of the system. This is visualized in Figure <ref type="figure" coords="3,160.93,432.01,3.90,9.96" target="#fig_0">1</ref>. This figure shows the "standard" multi-lingual OpenEphyra layout. Following the dotted arrows as well (to the question analysis phase), the system is extended with the analysis on the source questions. Obviously, a new question analysis algorithm has to be incorporated in OpenEphyra that inserts the question information generated by the question analysis on the source questions. We have implemented an algorithm that reads question information from an external file, which allows us to perform the question analysis on the source questions first (before running OpenEphyra), write the results to file and use the contents of the file in OpenEphyra. This way, OpenEphyra does not explicitly have to know about the questions in the source language.<ref type="foot" coords="3,482.76,514.82,3.97,4.84" target="#foot_4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Machine translation</head><p>To translate the questions from Dutch to English, we selected the Systran online machine translation system just like last year (van Zaanen and <ref type="bibr" coords="3,309.62,573.85,53.37,9.96" target="#b8">Mollá, 2007)</ref>. We compared the quality of the output of several machine translations systems and based on these results, we selected Systran. Using the web interface provided<ref type="foot" coords="3,233.64,596.90,3.97,4.84" target="#foot_5">6</ref> , the Dutch questions are translated to English automatically.</p><p>Glancing over the translated questions, most questions are quite understandable even though native speakers of English might have phrased them otherwise. For example, "With which pianists he has cooperated?", "For which films he has written music?", or "Call two instruments which are played on by Emerson." would probably be phrased differently. Other questions, however, are very unclear, such as "How did he come for living?" (from "Hoe kwam hij om het leven", meaning "How did he die?"), or "Where did he become herbegraven on 30 November 2002?" (from "Waar werd hij herbegraven op 30 november 2002?", meaning "Where was he re-buried on 30 November 2002?").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Alignment-Based Learning question classification</head><p>As question analysis tool, we wanted to use the question classifier that is based on Alignment-Based Learning (ABL) <ref type="bibr" coords="4,196.68,342.49,113.16,9.96" target="#b7">(van Zaanen et al., 2005)</ref>. An ABL classifier takes a set of questions together with question classes for training. The questions are compared against each other, which identifies patterns in the questions. For each of the question classes, patterns are assigned. During testing, all of the patterns are matched against the new question. The question class that has most patterns matching is returned as the correct class.</p><p>Recently, a similar system has been applied to classifying musical pieces to their composer <ref type="bibr" coords="4,90.00,414.25,147.00,9.96" target="#b0">(Geertzen and van Zaanen, 1997)</ref>. The system described here is similar to that of <ref type="bibr" coords="4,462.13,414.25,51.02,9.96;4,90.00,426.13,52.16,9.96" target="#b7">van Zaanen et al. (2005)</ref>, but more specific patterns are generated. In the context of music classification the results are much better.</p><p>At the moment, we have not used this question classification system due to time constraints. The experiment that used the externally provided question classes took somewhat longer than the experiment using the internal question classification. Running and evaluating this approach and comparing the results against the experiments without external question classification is considered future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Applying the system to the 200 questions provided, takes approximately ten hours from start to finish. This results in 200 answers, as all questions are considered of the factoid type. Of these answers 40 are NIL, indicating that the system cannot find an answer for those questions.</p><p>The overall accuracy of the system is 3.5%. Seven out of the 200 questions are answered correctly, two are inexact and nine unsupported. This means that 182 answers are incorrect.</p><p>The overall accuracy measure is very strict, taking into account that the system does not even try to identify and thus answer list or definition questions. Furthermore, the supporting text for an answer is selected in an ad hoc way.</p><p>Looking at the answers while considering the limitations of the system, more encouraging results are found. Firstly, if we do not require the system to provide correct support text (OpenEphyra does not generate support text) 16 answers out of the 200 are correct. This yields an accuracy of 8%. (Including the inexact answers as well, we get an accuracy of 9%.)</p><p>Secondly, if we take into account that the system only aims to generate answers for factoid questions, we see an accuracy of 4.375% (seven out of 160). If we then take include unsupported answers, we find an accuracy of 10% (16 out of 160). (There is one inexact factoid answer. Including that as well results in an accuracy of 10.625%). None of the ten list questions are correctly answered and only one of the 30 definition questions has an inexact answer, the rest is incorrect. Also, all 40 NIL questions are incorrect. Thirdly, the system does not do anything about temporally restricted aspects of questions (or answers). As a results, as could be expected, all temporally restricted questions are answered incorrectly.</p><p>Finally, no anaphora resolution is performed, so it can be expected that all questions that contain anaphora are answered incorrectly. This is true except for one question, which is answered correctly, but does not have correct supporting text. The original question was "Hoe kwam hij om het leven?", which is translated as "How did he come for living?". The word "he" refers to "Jeremiah Clarke", which was mentioned in the previous question, but the system has no way of knowing this. The inexact answer to this question is "shot".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We built a multi-lingual question answering system using a collection of publicly available tools. The mono-lingual English OpenEphyra question answering system was extended using Systran's online machine translation system, translating Dutch questions to English. This simple system already generates encouraging results, given that several aspects were not treated at all. The system does not take into account temporally restricted questions, no anaphora resolution is performed, and only factoid questions are handled.</p><p>We wanted to investigate the impact of performing as much analysis on the original texts. In particular, we aimed at doing question analysis (classification) on the Dutch questions and compare the results against the system described in this paper, where question analysis was performed on the translated English question. Intuitively, one would expect the analysis on the texts in the original language to perform better.</p><p>Unfortunately, time restrictions did not allow us to evaluate our intuition regarding the quality of intermediate results with respect to the language of the source. We would have liked to perform question analysis on the Dutch questions and incorporate these results directly into the OpenEphyra system, disabling the internal question analysis algorithms (that work on the target language).</p><p>There are several possible directions for future work. Firstly, we would like to experiment with performing more analysis on the source language side. Question analysis is a first step in this direction, but query generation (used in the document selection phase) can perhaps also benefit from information extracted from the source questions. Secondly, we would like to improve the machine translation step. For example, finding named entities in the source language, including names and titles of books, songs, etc., and translating these separately may improve the translation quality. Systran consistently makes certain mistakes, for instance, using the word "call" instead of "name" (e.g. in "Call two instruments which are played on by Emerson."). We would like to investigate whether these mistakes can be recognized and corrected automatically. Finally, the current system does not handle anaphora at all. A new anaphora resolution algorithm needs to be implemented, which does not depend on topics having a name as in TREC.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,111.84,204.61,379.59,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Layout of the extension of OpenEphyra for multi-lingual question answering.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,672.63,96.59,7.35"><p>http://www.ephyra.info/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,682.11,105.10,7.35"><p>http://www.wikipedia.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,105.24,653.19,143.13,7.35"><p>http://www.lemurproject.org/indri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,105.24,662.67,117.82,7.35"><p>http://www.lemurproject.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,105.24,671.70,407.62,7.97;3,90.00,681.18,228.19,7.97"><p>In the end, the experiment that uses externally generated question information (based on the questions in the source language) were not submitted due to time restrictions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,105.24,691.11,109.30,7.35"><p>http://www.systranbox.com/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,90.00,649.09,423.03,9.96;5,99.96,661.09,413.07,9.96;5,99.96,672.97,285.37,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,270.51,649.09,225.18,9.96">Composer classification using grammatical inference</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Geertzen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Zaanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,373.08,661.09,139.95,9.96;5,99.96,672.97,220.99,9.96">Proceedings of the International Workshop on Machine Learning and Music (MML)</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Ramirez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Conklin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Anagnostopoulou</surname></persName>
		</editor>
		<meeting>the International Workshop on Machine Learning and Music (MML)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="17" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.00,61.33,422.95,9.96;6,99.96,73.33,413.09,9.96;6,99.96,85.21,81.61,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,166.94,61.33,210.90,9.96">Learning of graph-based question answering rules</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mollá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,396.72,61.33,116.23,9.96;6,99.96,73.33,342.49,9.96">Proceedings of TextGraphs: the Second Workshop on Graph Based Methods for Natural Language Processing</title>
		<meeting>TextGraphs: the Second Workshop on Graph Based Methods for Natural Language Processing<address><addrLine>New York; NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.00,105.25,422.97,9.96;6,99.96,117.13,413.02,9.96;6,99.96,129.13,399.51,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,261.88,105.25,123.19,9.96">Answerfinder at TREC 2005</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Zaanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,407.04,105.25,105.93,9.96;6,99.96,117.13,206.59,9.96">Proceedings of the Fourteenth Text Retrieval Conference (TREC 2005)</title>
		<meeting>the Fourteenth Text Retrieval Conference (TREC 2005)<address><addrLine>Gaithersburg:MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publication. Department of Commerce, National Institute of Standards and Technology</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>cd-rom</note>
</biblStruct>

<biblStruct coords="6,90.00,149.05,423.02,9.96;6,99.96,160.93,62.41,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,327.29,149.05,168.17,9.96">The Ephyra QA system at TREC 2006</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Schlaefer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gieselmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sautter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,99.96,160.93,28.55,9.96">TREC</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.00,180.85,422.87,9.96;6,99.96,192.85,412.86,9.96;6,99.96,204.85,271.35,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,278.93,180.85,159.29,9.96">A non-projective dependency parser</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tapanainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Järvinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,463.32,180.85,49.55,9.96;6,99.96,192.85,331.86,9.96">Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP-97)</title>
		<meeting>the Fifth Conference on Applied Natural Language Processing (ANLP-97)<address><addrLine>Washington:DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.00,224.77,377.58,9.96" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><surname>Trec</surname></persName>
		</author>
		<title level="m" coord="6,159.48,224.77,276.65,9.96">Proceedings of the Fifteenth Text Retrieval Conference (TREC</title>
		<meeting>the Fifteenth Text Retrieval Conference (TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,472.32,224.77,40.53,9.96;6,99.96,236.65,413.10,9.96;6,99.96,248.65,117.64,9.96" xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Gaithersburg</surname></persName>
		</author>
		<imprint>
			<pubPlace>MD, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>NIST Special Publication. Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.00,268.57,423.09,9.96;6,99.96,280.57,412.99,9.96;6,99.96,292.45,146.89,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,370.62,268.57,142.47,9.96;6,99.96,280.57,60.34,9.96">Question classification by structure induction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Zaanen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Augusto Pizzato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mollá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,181.32,280.57,327.60,9.96">Proceedings of the International Joint Conferences on Artificial Intelligence</title>
		<meeting>the International Joint Conferences on Artificial Intelligence<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1638" to="1639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.00,312.37,423.15,9.96;6,99.96,324.37,342.13,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,257.44,312.37,142.86,9.96">Answerfinder at QA@CLEF 2007</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Zaanen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mollá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,151.56,324.37,196.07,9.96">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.00,344.29,423.14,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,310.85,344.29,120.89,9.96">Answerfinder at TREC 2006</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Zaanen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Pizzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,451.33,344.29,28.55,9.96">TREC</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
