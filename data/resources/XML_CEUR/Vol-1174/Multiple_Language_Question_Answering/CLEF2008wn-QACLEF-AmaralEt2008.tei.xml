<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,121.56,86.45,352.16,12.58">Priberam&apos;s question answering system in QA@CLEF 2008</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,171.96,136.11,56.40,8.74"><forename type="first">Carlos</forename><surname>Amaral</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.51,136.11,50.62,8.74"><forename type="first">Adán</forename><surname>Cassan</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.99,136.11,62.71,8.74"><forename type="first">Helena</forename><surname>Figueira</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,362.20,136.11,56.93,8.74"><forename type="first">André</forename><surname>Martins</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,183.48,147.63,61.21,8.74"><forename type="first">Afonso</forename><surname>Mendes</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.10,147.63,55.05,8.74"><forename type="first">Pedro</forename><surname>Mendes</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,314.56,147.63,36.00,8.74"><forename type="first">José</forename><surname>Pina</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.03,147.63,54.76,8.74;1,279.36,170.61,36.71,8.74"><forename type="first">Cláudia</forename><forename type="middle">Pinto</forename><surname>Priberam</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,208.74,182.13,80.35,8.74"><forename type="first">Alameda</forename><forename type="middle">D</forename><surname>Afonso</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,121.56,86.45,352.16,12.58">Priberam&apos;s question answering system in QA@CLEF 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">40F8F3FCE2521036611E0BB274521DAD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ACM Categories and Subject Descriptors H.2 [Database Management]: H.2.3 Languages -Query Languages H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation Question answering, Questions beyond factoids, Query Expansion, Portuguese, Spanish</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the changes implemented in Priberam's question answering (QA) system since our last QA@CLEF participation, followed by the discussion of the results obtained in Portuguese and Spanish monolingual runs at the main task of QA@CLEF 2008. This time, the main goal of Priberam's participation, following the results of last year's evaluation, was to stabilize the system in order to achieve its potential performance. To attain that performance status, we enhanced the syntactic analysis of the question and improved the indexing process by using question categories at the sentence retrieval level and ontology domains of the expected answer in document retrieval. The fine-tuning of the syntactic analysis, by defining and using core nodes of phrases as objects, allowed the system to more precisely match the pivots of the question with their counterparts in the answer, taking into account their syntactic functions. As a result, in QA@CLEF 2008, Priberam's system achieved a considerable overall accuracy increase in the Portuguese run.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The performance of Priberam's system in last year's QA@CLEF displayed internal and external changes. Internally, the system underwent several modifications, both in the Portuguese and in the Spanish modules, the most relevant one being the introduction of syntactic question processing <ref type="bibr" coords="1,368.86,644.37,10.64,8.74" target="#b0">[1]</ref>. Externally, the CLEF organisation introduced topic-related questions (questions clustered around a common topic that might present anaphoric links between them) and added Wikipedia as a target document collection to the already existent newspaper corpora <ref type="bibr" coords="1,105.84,678.88,10.64,8.74" target="#b1">[2]</ref>. As a result, there was a slight increase of the overall accuracy in the Spanish (ES) run and a significant decrease of the overall accuracy in the Portuguese (PT) run. Nevertheless, Priberam's system achieved a more accurate question categorisation, hence decreasing the number of wrong candidate answers, due to the introduction of syntactic parsing during question processing.</p><p>The main goal of Priberam's participation in QA@CLEF 2008 was to stabilize the system in order to surpass the results it obtained in previous QA@CLEF participations <ref type="bibr" coords="1,355.37,736.35,10.84,8.74" target="#b2">[3,</ref><ref type="bibr" coords="1,371.09,736.35,7.22,8.74" target="#b3">4]</ref>. To enhance its performance, we improved the indexing/retrieval process by using question categories (QC) at sentence retrieval level and ontology domains of the expected answer in document retrieval. The fine-tuning of the syntactic analysis, by using the phrases' core nodes as objects, allowed the system to more precisely match the pivots of the question with their counterparts in the answer, taking into account their syntactic functions. As a result, in QA@CLEF 2008, Priberam's system achieved a considerable overall accuracy increase in the Portuguese run.</p><p>The paper is organised as follows: section 2 describes the major adjustments made to the system, such as the work done in improving the syntactic processing of the question and the adaptations to deal with topicrelated questions; section 3 analyses and discusses the results of both monolingual runs; section 4 presents the conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Adaptations and improvements of the system</head><p>Priberam's QA open-domain system has already been described in detail <ref type="bibr" coords="2,378.52,225.27,10.88,8.74" target="#b2">[3,</ref><ref type="bibr" coords="2,393.44,225.27,7.22,8.74" target="#b4">5]</ref>. Briefly, it relies on a set of linguistic resources (such as a wide coverage lexicon, a thesaurus and a multilingual ontology) and software tools (which can be used to write and test grammars, to build contextual rules for performing morphological disambiguation or named entity (NE) recognition, to build patterns for question categorization/answer extraction, etc.). This general domain QA system is based on a five-step architecture: the indexing process, the question analysis, the document retrieval, the sentence retrieval, and the answer extraction. When a question is submitted and matches a given question pattern (QP), a category is assigned to it and a set of question answering patterns (QAPs) becomes active. Then, documents containing sentences with categories in common with the question (earlier determined during indexation via answer patterns (APs)) are analysed; the active QAPs are then applied to each sentence in order to extract the possible answers.</p><p>Since the overall architecture of Priberam's QA system remains unchanged, this year we focused on (i) the improvement of the indexing/retrieval process, (ii) the refinement of the question syntactic analysis, (iii) the finetuning of named entity recognition (NER), and we also revised (iv) the treatment of topic-related questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Improvements of the indexing/retrieval process</head><p>This year we kept the approach used and described on previous CLEF campaigns <ref type="bibr" coords="2,424.25,410.43,10.61,8.74" target="#b2">[3]</ref>, but the system was submitted to a lot of fine-tuning and optimization in order to improve performance. Some of the enhancements allowed us to go further on what we indexed and queried for without major performance penalties. The most important changes were indexing of QCs at sentence level instead of at document level, the complete indexation of ontology domains at document level and the use of different ratings for document titles and document body (both for Wikipedia and newspaper articles).</p><p>In <ref type="bibr" coords="2,103.96,479.43,11.68,8.74" target="#b2">[3]</ref> we described the work done in two different steps, document retrieval and sentence retrieval. Much of the work done on the second step is now also done on the first step because many of the problems the system experienced in the retrieval process were due to the loss of documents in document retrieval.</p><p>The following summarizes the most important changes implemented:</p><p>1) It is now possible to embed in the QAPs rules for querying the ontology of the target answer (see section 2.2); 2) A document indexed with the QC on the same sentence as the pivots has now a much higher rating; 3) Documents where the pivots (especially NEs) appear in the title have priority over the other documents; 4) Documents that are more recent have higher priority (this is relevant for news corpora); 5) It is now possible to write rules to tag some pivots with higher/lower priority or discard them for retrieval.</p><p>Both in the Portuguese and Spanish runs the changes proved to be very rewarding, since the failures due to the retrieval stage dropped from PT-45%, ES-17.6% last year to PT-4.1%, ES-0.9%.</p><p>From the analysis of the four failures during document retrieval stage on the Portuguese run, we can see that two were due to bad handling of date restrictions. In the PT question 75 "Quantos tem hoje em dia?" [How many does it have nowadays?], whose topic is Berlim, the system did not translate the expression "hoje em dia" [nowadays] to the current date. In the PT question 6 "Diga uma escola de samba fundada nos anos 40." [Say one samba school founded during the 40s] the date expression "anos 40" [the 40s], which should be selecting all documents with dates between 1940 and 1950, was wrongly only selecting documents with dates from 1940. The other two failures were due to the presence of very common pivots with very frequent QCs for those pivots. Even though this did not happen very often this year, it is probably the main cause of errors in the retrieval stage.</p><p>On the Spanish run there was only one failure during the retrieval stage. ES question 112 "En qué año la construyeron?" [In which year was it built?] failed because the sentence containing the answer "en lo alto de la fachada está grabada la fecha de su construcción: 1539" was not being indexed with QC &lt;DATE&gt;.</p><p>Even though the results were very good for the retrieval stage, some improvements still need to be addressed in the future: (i) a new schema for indexing and querying date periods and (ii) a new schema for indexing QCs, where we plan to tag each word or phrase in the indexing process with QCs, instead of indexing QCs by sentence. For instance, in the sentence "Manuel II de Portugal, último rei de Portugal de 1908 a 1910" we should index the noun phrases "Manuel II de Portugal" and "último rei de Portugal" with QCs &lt;CHRONOLOGY&gt; and &lt;FUNCTION&gt; and "último rei de Portugal" with QCs &lt;CHRONOLOGY&gt; and &lt;DENOMINATION&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Refinement of the question syntactic analysis</head><p>During the question analysis stage, questions are categorised and syntactically parsed. We maintain the approach presented last year, which introduced the possibility to capture the syntactic structure of the question by using FLiP's linguistic technology<ref type="foot" coords="3,186.78,244.99,3.24,5.65" target="#foot_0">1</ref>  <ref type="bibr" coords="3,194.58,247.17,10.63,8.74" target="#b0">[1]</ref>. The main difference is that now we detect the core nodes of the syntactic phrases and use them as the question's objects (its main constituents).</p><p>Each syntactic phrase may have one or more core nodes, that may coincide with the head phrase or not, and that are assigned to different object types accordingly to their relevance in extracting the expected answer. Object assignment is done after parsing, using the syntactic information that was treated in that stage. Typically, object assignment establishes a hierarchy of objects: it places the core nodes of subjects at the top, followed by those of the verb's complements, the head of the verb phrase and the adjuncts. It also gives priority to NEs: for example, PT question 33 "Que político é conhecido como Iznogoud?" [Which politician is known as Iznogoud?] retains "Iznogoud" as the object, "é conhecido" as the verbal object and "político" as the restraining object.</p><p>This strategy can help solving a few simple instances of syntactic ambiguity, such as those derived from prepositional phrase (PP) attachment, in case of overgeneration or parsing errors <ref type="bibr" coords="3,396.27,362.13,10.61,8.74" target="#b5">[6]</ref>, since the core nodes remain the same. For instance, in PT question 62 "Qual a largura do Canal da Mancha no seu ponto mais estreito?" [What is the width of the English Channel at its narrowest point?], which has three contracted prepositions ("do", "da" and "no"), the parser could wrongly build the PP "do Canal da Mancha no seu ponto mais estreito" [of the English Channel at its narrowest point]. If the parser could not find its core nodes, the whole PP would be used as the object, thus introducing noise in the document retrieval stage. By establishing core nodes, one can assign the detected NE "Canal da Mancha" as the object and "no seu ponto mais estreito" as the modifying object.</p><p>We added a specific object, the interrogative object, which works as a placeholder for the expected answer. We use it along with the QC to narrow the search for target sentences and extract the answer. The use of its ontological domains led to a considerable increase in the accuracy of the retrieval process. For instance, in PT question 1 "Que tipo de animal é o Cocas?" [What kind of animal is Kermit?], the system looks for documents containing words and expressions belonging to the same ontology level of "animal", the question's interrogative object. Thus, sentences that do not contain the word "animal", but contain words like "sapo" [toad] or "rã" (frog), are retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fine-tuning of named entity recognition (NER)</head><p>The NER engine Priberam has been using in its QA system participated this year in the second edition of HAREM, an evaluation contest for Portuguese NER <ref type="foot" coords="3,289.68,568.09,3.24,5.65" target="#foot_1">2</ref> . This participation led to an external evaluation of the engine and, consequently to its improvement and refinement. This had an impact on the precision of the answer extraction, namely in the more specific QCs. Besides the NEs already detected (e.g. people, places and organisations), we had to build new rules to recognize NEs that denote written and not written works, things (objects, substances), events, abstractions and numeric values (currencies, quantities, classifications). The rules that recognise time expressions were also improved, because Priberam's NER engine was a participant in the time track of the second HAREM as well.</p><p>This, as mentioned above, was particularly important for some QCs such as &lt;WRITTEN WORK&gt;, &lt;NOT WRITTEN WORK&gt;, &lt;STAR&gt; or &lt;CLASSIFICATION&gt;. For QCs such as &lt;DENOMINATION&gt;, &lt;FUNCTION&gt; or &lt;LOCATION&gt;, the semantic values of NEs were already being used in the indexing process and answer extraction, allowing the system to perform more accurately in these categories. With the addition of the new semantic tags and the creation of new rules that classify NEs using those tags, we were able to narrow the number of candidate answers in the more specific QCs. Thus, for a question such as topic-related PT question 162 "Diga um desses filmes." [Name one of those films.], whose topic is Jean Vigo, candidate answers that contained NEs classified as not written works were given a higher score.</p><p>Not only does this fine-tuning of the NER improve the answer extraction process, it also improves the syntactic parsing by restricting, for example, the number of PPs, hence preventing overgeneration, which will in turn create a more precise parser (see section 2.2).</p><p>The performance of Priberam's NER engine led to its commercial exploration: it is now being used for search refining in the sites of two major news media, TSF radio station <ref type="foot" coords="4,354.42,186.31,3.24,5.65" target="#foot_2">3</ref> and Jornal de Notícias newspaper<ref type="foot" coords="4,497.28,186.31,3.24,5.65" target="#foot_3">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dealing with topic-related questions</head><p>As mentioned on last year's working notes, the procedure for dealing with topic-related questions could perform poorly because of the excess of pivots. Moreover, since we just merged the question pivots, we loosed the question syntactical analysis. Like last year we only analyse the first question from the set and the current question, which means that we do not keep track of the changes to the topic. This had an impact on the Spanish questions but not on the Portuguese ones. In our opinion, topic-related questions are not very interesting for a commercial system at this stage of QA systems. Having this in mind, we developed the module only for CLEF and did not invest a lot of effort here.</p><p>The strategy we applied this year to topic-related questions was the following (see This procedure still has many flaws and systematically failed in questions like PT questions 37 "E um nãometal." [And a nonmetal.], 65 "E do pão?" [And of bread?] and 144 "E a segunda?" [And the second one?],</p><p>where the arguments of the first question were not replaced but added. In the Spanish run, topic-related questions suffered with this new schema, since question syntactical analysis is still quite poor when compared to Portuguese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Table <ref type="table" coords="5,96.42,167.79,5.01,8.74" target="#tab_2">2</ref> presents the results of Portuguese and Spanish monolingual runs submitted by Priberam to the main task of QA@CLEF 2008. The sets of questions were classified according to three question categories: factoid (FACT), definition (DEF) and list (LIST), with the judgments used for evaluation (R=Right, W=Wrong, X=Inexact, U=Unsupported), as defined in CLEF 2008 guidelines. Regarding the Portuguese run, the improvement of more than 20% in the accuracy of general factoid questions considerably contributed to the increase of the overall accuracy, which surpassed that of last year (50%). Besides that, an analysis of PT question clusters shows that there was an increase in the number of clusters (37) in a total of 88 questions, 51 topic-related, but that the system was able to extract the correct answers 45% of the times, which means a boost of nearly 30%, when comparing to last year's results.</p><formula xml:id="formula_0" coords="5,182.34,250.05,190.19,8.74">Q A R W X U</formula><p>Despite these general positive results, Table <ref type="table" coords="5,275.78,564.27,5.01,8.74" target="#tab_2">2</ref> also shows a decrease of accuracy in DEF and LIST questions. The reasons for failures are assembled in Table <ref type="table" coords="5,305.34,575.79,3.74,8.74" target="#tab_3">3</ref>, which displays the distribution of errors, in both monolingual runs, along the main stages of Priberam's QA system. In the Portuguese run, the main source of error was the extraction of candidate answers, followed by the choice of the final answer. The main reason for errors in extraction of candidate answers is the coverage of QAPs, which are handwritten and therefore limited. In the case of DEF questions, two of the extracted answers considered inexact by the CLEF assessors presented brackets (PT questions 27 and 88), here accounted as a problem of choice of the final answer. We opted to keep the text in brackets (the way the links in the Wikipedia articles were stored) because they normally present useful information to the user. At least one of the answers classified as Other in the PT run could be considered a possible assessor's error: in DEF question 137, whose topic is Prémio Cervantes, "Quem é que ganhou o prémio em 1994?" [Who won the prize in 1994?], the QA system extracted the answer "Mario Vargas Llosa" from the snippet "O escritor peruano Mario Vargas Llosa ganha o Prémio Cervantes 1994, o maior galardão literário espanhol." [Peruvian writer Mario Vargas Llosa wins Cervantes Prize 1994, the biggest Spanish literary prize]. Finally, the system did not extract any answer to DEF question 66 "O que é o jagertee?" [What is the jagertee?] because the snippet containing the correct answer was not indexed for unknown reasons and thus could not be retrieved.</p><p>With regard to the Spanish run, Table <ref type="table" coords="6,245.88,377.25,5.01,8.74" target="#tab_2">2</ref> shows that results within non-topic-related questions are quite similar to those of last year, while topic-related questions had a decrease in its accuracy of almost 20%. At this point, it deserves to be said that the number of both question clusters and topic-related questions doubled in 2008 for the ES test set: from 20 clusters and 30 topic-related questions, it passed to 48 clusters and 62 topic-related questions. This fact had, consequently, a strong impact on the Spanish results, both on the falling of non-topic questions accuracy by itself and, mainly, on the global results. Another remarkable fact about the Spanish set is a significant increase of the number of LIST questions compared to last year's set or to the Portuguese set.</p><p>In Table <ref type="table" coords="6,129.54,457.71,5.01,8.74" target="#tab_3">3</ref> we classified as Other all the unsupported answers in the ES run. All of them are certainly correct answers, but at least three of them do not explicitly contain all the needed supporting information in the snippet, although this information does appear in the document. Those errors could be seen as presentation errors, as a limitation of the system in the way of presenting the information, and not in the way it processes those questions. One of these examples is ES question 10 "¿A qué edad murió Wallace Rowling?" [At which age did Wallace Rowling die?]. The QA system correctly answered "67 años" from the snippet "-Sir Wallace Rowling, ex primer ministro de Nueva Zelanda, 67 años.". Although the actual snippet does not support the answer, the document where it comes from is a list of deceased people in 1995 from EFE, but that is not shown in the snippet. Something similar happens with ES question 170 "Según el plan Belloch, ¿cuántos vehículos policiales habrá en Barcelona?" [Acording to the Belloch plan, how many police vehicles will there be in <ref type="bibr" coords="6,70.91,572.76,48.35,8.74">Barcelona?]</ref> where the given answer is "301" from the snippet "Barcelona contará a partir del día 1 de enero con 301 vehículos policiales patrullando sus diez distritos: 114 por la mañana, 114 por las tardes y 73 por la noche." Again, the document does talk about the matter, the "Plan Belloch", even if the snippet does not.</p><p>Finally, there is an interesting case of extraction problem with the answer to the ES question 75 "¿Cómo se pronuncia eso?" [How is it pronounced?], whose topic is TeX. The correct answer is displayed in the Wikipedia between square brackets, and it happens to be ignored by the QA system because of that.</p><p>From the analysis of the results, we conclude that the retrieval stage and the question analysis stage are performing very well for questions like those posed in CLEF, that QAPs need to broaden their coverage and that the work done for Portuguese this year must be ported to the Spanish rules. improved the capacity to answer multiple questions simultaneously by enhancing the parallelism of the algorithms. These improvements were crucial for the implementation of the search engine in the sites of TSF and Jornal de Notícias. The retrieval stage is performing very well and the changes in the syntactical/semantic analysis now cover all the QCs.</p><p>During last year, we have been working on anaphora resolution and we had a first prototype of the system a few days before CLEF. As we had no feedback on how the system was behaving, we decided not to submit the results with anaphora resolution. After CLEF, we managed to run the tests and found out that the results were almost the same. This is an interesting result and we are convinced that this is due to the CLEF set of questions being extracted directly from what is written in the documents. Anaphora resolution is important only when dealing with Wikipedia articles between the body of the text and the title (this was already implemented last year).</p><p>The work done this year in the Portuguese module must be done in the Spanish module, specifically on the syntactical/semantic analysis of the questions and NER. As we mentioned in section 2.1, we plan to tag each word/phrase with the QC in the indexing stage. We hope that without a big penalty on index size we can achieve better accuracy and speed. Future work will also include, since we now have a big corpus of questions/answers/false answers, working on algorithms to automatically learn new question patterns from corpora.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,73.02,316.11,473.44,346.24"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="4,456.72,316.11,43.85,8.74"><row><cell>examples):</cell></row></table><note coords="4,92.52,373.65,273.35,8.74;4,92.52,385.11,119.49,8.74;4,92.52,396.63,280.56,8.74;4,92.52,408.15,319.79,8.74;4,92.52,419.61,184.27,8.74"><p>4) handle explicit anaphors (those where the pronoun is expressed); 5) use the last expressed QC; 6) use the argument analysis of the question which expresses the QC; 7) import the missing arguments from the first question to the current question; 8) if the QC changes, also import the answer.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,174.18,677.94,282.26,9.02"><head>Table 1 -Examples of question analysis of topic-related questions.</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,89.88,250.05,415.57,231.11"><head>Table 2 -Results by category of question, including detailed results of topic and non topic-related questions.</head><label>2</label><figDesc></figDesc><table coords="5,400.38,250.05,86.88,8.74"><row><cell>Total</cell><cell>Accuracy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,170.46,96.99,254.45,128.15"><head>Table 3 -Reasons for W, X and U answers</head><label>3</label><figDesc></figDesc><table coords="6,170.46,96.99,254.45,114.76"><row><cell>Stage ↓</cell><cell>Question →</cell><cell cols="2">W+X+U</cell><cell cols="2">Failure (%)</cell></row><row><cell></cell><cell></cell><cell cols="3">PT ES PT</cell><cell>ES</cell></row><row><cell cols="2">Document retrieval</cell><cell>4</cell><cell>1</cell><cell>4.1</cell><cell>0.9</cell></row><row><cell cols="2">Extraction of candidate answers</cell><cell>33</cell><cell>75</cell><cell>46.6</cell><cell>66.4</cell></row><row><cell cols="2">Choice of the final answer</cell><cell>20</cell><cell>17</cell><cell>27.3</cell><cell>15.0</cell></row><row><cell>NIL validation</cell><cell></cell><cell>8</cell><cell>9</cell><cell>11.0</cell><cell>8.0</cell></row><row><cell>Topic</cell><cell></cell><cell>4</cell><cell>7</cell><cell>5.5</cell><cell>6.2</cell></row><row><cell>Other</cell><cell></cell><cell>4</cell><cell>4</cell><cell>5.5</cell><cell>3.5</cell></row><row><cell>Total</cell><cell></cell><cell cols="4">73 113 100.0 100.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,77.52,696.75,446.85,8.10;3,70.92,707.37,453.59,7.85;3,70.92,717.70,453.52,7.85;3,70.92,728.07,92.33,7.85"><p>FLiP, or Ferramentas para a Língua Portuguesa, is Priberam's proofing tools package for Portuguese. FLiP includes a grammar and style checker, a spell checker, a thesaurus and a hyphenator that enable different proofing levels -word, sentence, paragraph and text -of European and Brazilian Portuguese. An online version is available at http://www.flip.pt/online.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,76.20,738.40,339.66,7.85"><p>HAREM is organised by Linguateca; more information at http://www.linguateca.pt/HAREM/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,76.20,738.40,63.75,7.85"><p>http://www.tsf.pt.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,76.20,748.78,61.73,7.85"><p>http://www.jn.pt.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4" coords="6,70.92,735.99,453.50,8.74;6,70.92,747.51,453.46,8.74"><p>Priberam's aim for QA@CLEF 2008 was to consolidate the system and improve its performance. Even though this year there was no real time exercise, from our tests we verified that we doubled the speed of the system and</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Priberam would like to thank <rs type="institution">Synapse Développement</rs>, as well as the CLEF organisation and Linguateca.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,85.11,377.07,425.44,8.74;7,70.92,388.32,419.68,9.02;7,70.92,399.84,421.74,9.02;7,70.92,411.57,352.94,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,430.33,377.07,80.22,8.74;7,70.92,388.59,152.73,8.74">Priberam&apos;s question answering system in QA@CLEF 2007</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vidal</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/2007/working_notes/AmaralCLEF2007.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="7,361.44,388.32,129.16,9.02;7,70.92,399.84,259.16,9.02">Cross Language Evaluation Forum:Working Notes for the CLEF 2007 Workshop (CLEF 2007)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007. 19-21 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,85.06,435.75,437.64,8.74;7,70.92,447.21,439.63,8.74;7,70.92,458.46,439.38,9.02;7,70.92,470.26,160.01,8.74;7,70.92,481.71,416.83,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,154.03,447.21,275.94,8.74">Overview of the CLEF 2007 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cristea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Stutcliffe</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/2007/working_notes/giampiccoloCLEF2007_Overview.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="7,125.40,458.46,384.91,9.02">Cross Language Evaluation Forum: Working Notes for the CLEF 2007 Workshop (CLEF 2007)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007. 19-21 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,85.06,505.89,432.44,8.74;7,70.92,517.08,453.00,9.02;7,70.92,528.87,171.93,8.74;7,70.92,540.39,392.99,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,393.69,505.89,123.81,8.74;7,70.92,517.08,330.22,9.02">Priberam&apos;s question answering system for Portuguese, Cross Language Evaluation Forum: Working Notes for the</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pinto</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/2005/working_notes/workingnotes2005/amaral05.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="7,403.67,517.08,120.25,9.02;7,70.92,528.87,23.39,8.74">CLEF 2005 Workshop (CLEF 2005)</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005. 21-23 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,85.16,563.37,426.94,8.74;7,70.92,574.62,451.14,9.02;7,70.92,586.08,412.47,9.02;7,70.92,597.88,48.65,8.74;7,70.92,609.39,426.38,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,431.88,563.37,80.22,8.74;7,70.92,574.89,202.20,8.74">Priberam&apos;s question answering system in a cross-language environment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vidal</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/2006/working_notes/workingnotes2006/cassanCLEF2006.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="7,456.18,574.62,65.88,9.02;7,70.92,586.08,316.63,9.02">Cross Language Evaluation Forum:Working Notes for the CLEF 2006 Workshop (CLEF 2006)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Vicedo</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006. 20-22 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,85.10,633.51,427.81,8.74;7,70.92,644.76,434.50,9.02;7,70.92,656.22,309.14,9.02;7,70.92,668.01,252.18,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,344.24,633.51,168.67,8.74;7,70.92,645.03,117.41,8.74">Design and Implementation of a Semantic Search Engine for Portuguese</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pinto</surname></persName>
		</author>
		<ptr target="http://www.priberam.pt/docs/LREC2004.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="7,195.06,644.76,310.37,9.02;7,70.92,656.22,100.02,9.02">Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004)</title>
		<meeting>the 4th International Conference on Language Resources and Evaluation (LREC 2004)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 26-28 May</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="247" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,85.15,692.13,433.29,8.74;7,70.92,703.38,448.39,9.02;7,70.92,714.90,436.32,9.02;7,70.92,726.63,191.16,8.74;7,70.92,738.15,291.17,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,186.46,692.13,279.68,8.74">A Nearest-Neighbor Method for Resolving PP-Attachment Ambiguity</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.cs.rochester.edu/~zhao/IJCNLP04-ppa.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="7,309.00,703.38,210.31,9.02;7,70.92,714.90,121.99,9.02">Natural Language Processing -IJCNLP 2004, First International Joint Conference</title>
		<editor>
			<persName><forename type="first">Keh-Yih</forename><surname>Su</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jun'ichi</forename><surname>Tsujii</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jong-Hyeok</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Oiyee</forename><surname>Kwong</surname></persName>
		</editor>
		<meeting><address><addrLine>Hainan Island, China; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004">2005. March 22-24, 2004</date>
			<biblScope unit="volume">3248</biblScope>
			<biblScope unit="page" from="545" to="554" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
