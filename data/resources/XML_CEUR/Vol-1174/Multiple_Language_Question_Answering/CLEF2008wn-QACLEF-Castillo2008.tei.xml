<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,93.12,97.81,409.08,12.64;1,272.28,113.89,50.63,12.64">The Contribution of FaMAF at QA@CLEF 2008.Answer Validation Exercise</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,266.64,145.60,61.86,8.96"><forename type="first">Julio</forename><forename type="middle">J</forename><surname>Castillo</surname></persName>
							<email>cj@famaf.unc.edu.ar</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics Astronomy</orgName>
								<orgName type="institution">Physics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National University of Cordoba</orgName>
								<address>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,93.12,97.81,409.08,12.64;1,272.28,113.89,50.63,12.64">The Contribution of FaMAF at QA@CLEF 2008.Answer Validation Exercise</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">405376C78F44D9B00F351DD8672042AD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.3. Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Algorithms, Measurement, Experimentation Question Answering, Answer Validation, Recognizing Textual Entailment, QA@CLEF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The system utilizes a Recognizing Textual Entailment (RTE) approach. The system uses the question string (q_str) as a Text (T) and one answer (t_str) as a Hypothesis (H). We use two different approaches using machine learning, specifically Support Vector Machine as classifier. The results show an increment over the baselines, however enhanced is needed. The features used are unigram, bigram, and trigram overlap of lexemes and stems, Levenshtein distance, tf-idf measure, and semantic similarity using wordnet. Experimental results show that the best run of our initial system achieved a 0.21 of F-measure and 0.17 of QAaccuracy. This shows an increment of 23.53% over the QA accuracy baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The objective of the Answer Validation Exercise (AVE) 2008 is to develop systems able to decide if the answer to a question is correct or not <ref type="bibr" coords="1,190.92,504.28,10.69,8.96" target="#b4">[5]</ref>. This is a three years old track and is part of Cross Language Evaluation Forum (CLEF) 2008.</p><p>AVE challenge is an evaluation framework for Question Answering (QA) systems. The inputs for the AVE systems are a set of triplets (Question, Answer, Supporting Text) and the results are a boolean value indicating whether the answer is supported by the text <ref type="bibr" coords="1,249.00,550.24,10.69,8.96" target="#b5">[6]</ref>. Answer Validation task must select the best answer for the final output. Therefore, AVE task is very similar to RTE (Recognition of Textual Entailments).</p><p>We have developed a system that performs morphological analysis (stemming, and POS tagging), and extract lexical (uni-bi and trigram overlap, Levenshtein, and tf-idf), and semantic measures using Wordnet 1 to build a model using a Support Vector Machine (SVM <ref type="bibr" coords="1,288.24,596.20,10.57,8.96" target="#b3">[4]</ref>), to determinate whether the implication holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System description</head><p>We have developed a system and carried out two runs. Both are based on a supervised learning approach using a SVM. One of this consists of twelve features, and the other takes in account only three features. Figure <ref type="figure" coords="1,491.64,667.48,4.98,8.96">1</ref> shows the general architecture of our system. Similar to <ref type="bibr" coords="1,268.92,678.88,10.69,8.96" target="#b6">[7]</ref>. Two different runs were submitted to AVE 2008 Challenge, and the experiments shows that our second run achieved a 0.21 F-measure, and that outperformed the standard baseline by 0.7 points. The first run is based on 12 feature vectors, and our second run is based on three features.</p><p>In order to extract all of these features, the system employs different tools such as a stemmer, a POS tagger, and Wordnet. The run one, is based on lexical overlaps and is quite simple, and his results are mayor that baselines, however we think that is not very competitive. This system approach is based on determinate if H (Hypothesis) is entailed by T (Text) considering only lexical similarities. The procedure for both runs will be described in sections 2.1 and 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Run 1: Lexical similarity</head><p>The main idea is to extract a set of lexical measures to determinate the similarity between (Hypothesis, Text) pairs. Our lexical approach is similar to <ref type="bibr" coords="2,231.48,568.84,11.71,8.96" target="#b0">[1]</ref> and <ref type="bibr" coords="2,262.68,568.84,10.60,8.96" target="#b1">[2]</ref>.</p><p>We don't preprocess the pairs using English stopwords list, because this process could remove important information of question, such as "the type" of question.</p><p>We have used an implementation of Porter stemmer<ref type="foot" coords="2,300.60,601.09,3.24,5.83" target="#foot_0">2</ref>  <ref type="bibr" coords="2,306.84,603.28,11.71,8.96" target="#b2">[3]</ref> to obtain the stems of tokens. The bag of stems is used for process the last six features.</p><p>Follow the first run features are depicted:</p><p>1. Percentage of word of the hypothesis in the text (treated as bags of words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Percentage of word of the text in the hypothesis (treated as bags of words).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Percentage of bigrams of the hypothesis in the text (treated as bags of words).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Percentage of trigrams of the hypothesis in the text (treated as bags of words).</head><p>5. TF-IDF + Cosine measure between the text and hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Levenshtein 3 distance between T and H (over words).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.</head><p>Percentage of word of the hypothesis in the text (treated as bags of stems).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8.</head><p>Percentage of word of the text in the hypothesis (treated as bags of stems). 9. Percentage of bigrams of the hypothesis in the text (treated as bags of stems).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Percentage of trigrams of the hypothesis in the text (treated as bags of stems).</head><p>11. TF-IDF + Cosine measure between the text and hypothesis (treated as bags of stems).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Levenshtein distance between T and H (over stems).</head><p>TF-IDF measure is calculated over T and H's that are thought as little documents. One of this is q_str and the others are t_str. The value of this feature is between 0 and 1.The similarity of both documents is captured by using the cosine similarity measure. This measure is performed measuring the cosine of the angle for the two documents vectors.</p><p>Levenshtein distance between T and H is the minimum numbers of operations (edit distance) that are needed to transform one string T, into the other string H, using only insertion, deletion, or substitution of a single character. We think that the system should be able of insert, delete or substitute entire word or stems, instead of characters. It will be our next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Run 2: Lexical and Semantic similarity</head><p>The second run that we have submitted uses only three features. The first step to preprocess these pairs is to obtain the stems of tokens. We have used OpenNLP <ref type="foot" coords="3,280.32,450.13,3.24,5.83" target="#foot_2">4</ref> to obtain pos-tagging. Additionally we have used Wordnet <ref type="foot" coords="3,521.16,450.13,3.24,5.83" target="#foot_3">5</ref>as tool for determinate synonym and hypernonym relationship. The features used for the second run are the followings:</p><p>1. Levenshtein distance between T and H (over stems). Identical to feature 6 in Run1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Lexical similarity using Levenshtein distance:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Semantic similarity using Wordnet</head><p>The steps necessary to obtain lexical similarity using Levenshtein distance are:</p><p>-Each string T and H are divided in a list of tokens.</p><p>-The similarity between two different tokens is performed using the "string edit-distance matching"(Levenshtein distance). This is for all tokens. -The string similarity between two list of tokens is reduced to the problem of "bipartite graph matching". Therefore, it is performed using the Hungarian algorithm over this bipartite graph. After, we find the optimal assignment that maximizes the sum of ratings of each token. Note that each graph node is a token of the list.</p><p>Finally the final score is calculated by:</p><formula xml:id="formula_0" coords="4,232.20,73.19,149.87,21.04">)) ( ), ( ( H Lenght T Lenght Max TotalSim finalscore =</formula><p>Where: TotalSim: is the sum of the similarities with the optimum assignment (using Hungarian algorithm). The maximum value of TotalSim is equal to Max (Length (Text), Length (H)). Length (T): is equal to the quantity of tokens that the graphs have in his origin component. Length (H): is equal to the quantity of tokens that the graphs have in his destine component.</p><p>Wordnet is used to calculate the semantic similarity of two strings. Given two tokens, they are used only in synonym and hypernonym relationship (is a type of relation). After, BFS (Breadth First Search) algorithm is used over these tokens, and then if two words are found, his similarity is calculated using two factors: length of the path, and orientation of the path.</p><p>The required steps are the followings: 3.1. Tokenization 3.2. Stemming words 3.3. POS tagging (to compare the word senses using the same POS. Therefore only are compared words in the same taxonomy). 3.4. WSD is performed using the Lesk algorithm, based on Wordnet definitions. 3.5. A semantic similarity matrix is defined. The semantic similarity is computed by: -s,t : are source and target words that we are comparing(s is the Hypothesis and t is the Text).</p><formula xml:id="formula_1" coords="4,261.36,363.92,111.95,23.59">) ( ) ( )) , (<label>( 2 )</label></formula><p>-Depth(s): Is the shortest distance between root node to current node (in the local taxonomy).</p><p>-LCS(s,t): is the least common sub-summer of "s" and "t".</p><p>3.6. A bipartite graph is building and computed using Hungarian Algorithm. 3.7. To obtain the final score, we use "matching average", i.e.:</p><formula xml:id="formula_2" coords="4,203.04,484.44,188.32,22.82">) ( ) ( ) , ( 2 Y Length X Length Y X Match erage MatchingAv + × =</formula><p>Where:</p><p>-X and Y are two sentences, particularly are T and H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training and Test Sets</head><p>The development set available for the AVE 2007 English task consists of 1121 answers, where 11,59% are validated answers and the rest 88.41% are rejected. On the other hand, the AVE 2008 development set consists of 195 pairs, only 21 are positives, it is 10,77% of the total.</p><p>Our system was sensitive to this unbalanced training set, so we built a balanced set, with approximately the same number of TRUE and FALSE pairs. We took all TRUE pairs from the training sets in AVE 2006 and AVE 2007 and then we incorporated a number of FALSE pairs totalling a 40% of the total. We have testing with 10%, 20%, 40%, 50%, and 60% of negatives of the total, and the best development set result was with 40% of FALSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analysis of Results</head><p>The described system has been tested in English using training sets of AVE 2006 and AVE 2007. The tables 1 and 2 show the recall, precision and f-measure over correct answers that were obtained with Run 1 and Run 2. Two baseline systems return VALIDATED for 100% and 50% of answers in the test set. The results were a relative high qa_accuracy and an acceptable f-measure. The results obtained by both systems have been better than the baselines, achieving the Run 1 a high recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language</head><p>The Run 1 represents a very positivist approach because the systems highly classify an (T,H) pair as a positive entailment.</p><p>Regarding the false positives, we can see that the Levenshtein distance is the best of the twelve features, because is the most differentiable among the others, and helps the classifier to classify correctly. The first 6 features (over bag of words) are more descriptive that the last 6 features (over stems). However, his difference is not significant.</p><p>Table <ref type="table" coords="5,114.48,380.92,4.98,8.96" target="#tab_3">2</ref> shows the results obtained for the two runs, compared with the value obtained in a perfect selection, best QA system, and a baseline system that validates all answers and randomly select one of them. This measure is used for compare the AV systems with QA systems presented in QA@CLEF.</p><p>Baselines for comparing AV systems performance with QA systems in English. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>estimated_qa_performance qa_accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and Future Work</head><p>We presented to AVE 2008 our initial RTE system that is based on machine learning approach using SVM as classifier. We have used lexical features such as unigram, bigram, and trigram overlap of lexemes and stems, and also we have used Levenshtein distance, tf-idf measure, and semantic similarity with Wordnet. Experimental results show that the best run of the system achieved a 0.21 of F-measure and 0.17 of QAaccuracy, an increment of 23.53% over the QA accuracy baseline. In spite of the simplicity of the approach, we have obtained a reasonable 0.17 of QA accuracy for the second run. Future work is oriented to probe with different classifiers as Bayesian Binary Regression (BBR), and use different datasets RTE, and RTE+AVE. To enhance the system, we will work with lexical and semantic similarity, adding features and testing his improvement. Additionally an NER module will be incorporated and combined with the rest of the system and his performance will be evaluated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,108.37,153.52,338.88,116.00"><head>Table 1 .</head><label>1</label><figDesc>General evaluation of the Famaf's system</figDesc><table coords="5,108.37,153.52,338.88,95.12"><row><cell>: English</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>F</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>RUN 2</cell><cell>0.21</cell><cell>0.13</cell><cell>0.56</cell></row><row><cell>RUN 1</cell><cell>0.17</cell><cell>0.09</cell><cell>0.94</cell></row><row><cell>100% VALIDATED</cell><cell>0.14</cell><cell>0.08</cell><cell>1</cell></row><row><cell>50% VALIDATED</cell><cell>0.13</cell><cell>0.08</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,156.96,479.80,270.46,94.40"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results obtained by the qa-accuracy measure</figDesc><table coords="5,156.96,479.80,236.34,68.12"><row><cell>Perfect selection</cell><cell>0.56</cell><cell>0.34</cell></row><row><cell>Best QA system</cell><cell>0,21</cell><cell>0,21</cell></row><row><cell>RUN 2</cell><cell>0.16</cell><cell>0.17</cell></row><row><cell>RUN 1</cell><cell>0.16</cell><cell>0.16</cell></row><row><cell>random</cell><cell>0.09</cell><cell>0,09</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,76.68,747.52,170.05,8.96"><p>http://tartarus.org/~martin/PorterStemmer/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,76.68,724.60,208.09,8.96"><p>http://www.let.rug.nl/~kleiweg/lev/levenshtein.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="3,76.68,736.00,123.25,8.96"><p>http://opennlp.sourceforge.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="3,76.80,747.52,92.53,8.96"><p>wordnet.princeton.edu/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,85.20,121.36,397.38,8.96;6,83.52,132.88,402.73,8.96;6,83.52,144.40,143.01,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,342.96,121.36,139.62,8.96;6,83.52,132.88,170.41,8.96">Experiments of UNED at the Third Recognizing Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">Alvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesus</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,258.33,132.88,227.91,8.96;6,83.52,144.40,113.60,8.96">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,85.08,167.32,416.70,8.96;6,86.04,178.84,409.69,8.96;6,83.52,190.36,412.17,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,291.60,167.32,210.18,8.96;6,86.04,178.84,78.55,8.96">Learning Textual Entailment using SVMs and String Similarity Measures</title>
		<author>
			<persName coords=""><forename type="first">Prodromos</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,171.72,178.84,324.01,8.96;6,83.52,190.36,281.47,8.96">ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007)</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,88.08,213.40,407.41,8.96;6,86.04,224.80,100.53,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,163.56,213.40,151.78,8.96">Development of a stemming algorithm</title>
		<author>
			<persName coords=""><forename type="first">Julie</forename><surname>Beth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lovins</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,322.08,213.40,173.41,8.96;6,86.04,224.80,42.76,8.96">Mechanical Translation and Computational Linguistics</title>
		<imprint>
			<date type="published" when="1968-03">March 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.72,247.84,349.89,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,214.44,247.84,100.03,8.96">Support-Vector Networks</title>
		<author>
			<persName coords=""><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,321.48,247.84,71.65,8.96">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.12,270.88,437.34,8.96;6,88.92,282.28,435.45,8.96;6,88.92,293.80,67.89,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,297.12,270.88,187.42,8.96">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,88.92,282.28,353.59,8.96">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2006)</title>
		<meeting><address><addrLine>Alicante, España</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">2006. September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.12,316.84,437.34,8.96;6,88.92,328.36,435.45,8.96;6,88.92,339.88,67.89,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,297.12,316.84,187.42,8.96">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,88.92,328.36,345.79,8.96">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2007)</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">2007. September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,89.64,362.80,434.77,8.96;6,88.92,374.32,435.54,8.96;6,88.92,385.84,252.81,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,485.16,362.80,39.25,8.96;6,88.92,374.32,183.56,8.96">Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>García-Cumbreras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Perea-Ortega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">Martínez</forename><surname>Santiago</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Ureña-López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,290.88,374.32,233.59,8.96;6,88.92,385.84,96.07,8.96">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2007)</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">2007. September 2007</date>
		</imprint>
	</monogr>
	<note>SINAI at QA@CLEF</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
