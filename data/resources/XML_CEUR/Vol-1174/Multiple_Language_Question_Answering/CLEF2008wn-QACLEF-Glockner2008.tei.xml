<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,100.39,148.79,402.22,15.48;1,270.58,170.71,61.84,15.48">University of Hagen at CLEF 2008: Answer Validation Exercise</title>
				<funder ref="#_2RcsmcF">
					<orgName type="full">DFG (Deutsche Forschungsgemeinschaft)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,266.81,203.69,69.39,10.37"><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
							<email>iglockner@fernuni-hagen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Information and Communication Systems (IICS)</orgName>
								<orgName type="institution">FernUniversität in Hagen</orgName>
								<address>
									<postCode>58084</postCode>
									<settlement>Hagen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,100.39,148.79,402.22,15.48;1,270.58,170.71,61.84,15.48">University of Hagen at CLEF 2008: Answer Validation Exercise</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">58924E84EF51093C9A7EF970097D0612</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Information filtering, Selection process</term>
					<term>I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-Predicate Logic, Semantic networks</term>
					<term>I.2.7 [Artificial Intelligence]: Natural Language Processing Experimentation, Measurement, Verification Logical Answer Validation, Answer Selection, Question Answering, Recognizing Textual Entailment (RTE), Information Fusion, Robust Inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RAVE (Real-time Answer Validation Engine</head><p>) is a logic-based answer validator/selector designed for application in real-time question answering. RAVE uses the same toolchain for deep linguistic analysis and the same background knowledge as its predecessor (MAVE), which took part in the AVE 2007. However, a full logical answer check as in MAVE was not considered suitable for real-time answer validation since it requires parsing of all answer candidates. Therefore RAVE uses a simplified validation model where the prover only checks if the support passage contains a correct answer at all. This move from logic-based answer validation to logical validation of supporting snippets permits RAVE to avoid any parsing of answers, i.e. the system only needs a parse of the question and pre-computed snippet analyses. In this way very low validation/selection times can be achieved. Machine learning is used for assigning local validation scores using both logic-based and shallow features. The resulting local validation scores are improved by aggregation. One of the key features of RAVE is its innovative aggregation model, which is robust against duplicated information in the support passages. In this model, the effect of aggregation is controlled by the lexical diversity of the support passages for a given answer. If the support passages have no terms in common, then the aggregation has maximal effect and the passages are treated as providing independent evidence. Repetition of a support passage, by contrast, has no effect on the results of aggregation at all. In order to obtain a richer basis for aggregation, an active validation approach was chosen, i.e. the original pool of support passages in the AVE 2008 test set was enhanced by retrieving additional support passages from the CLEF corpora. This technique already proved effective in the AVE 2007. The development of RAVE is not finished yet, but the system already achieved an F-score of 0.39 and a selection rate of 0.61 compared to optimal selection. Judging from last year's runs of MAVE (with a 0.93 selection rate and F-score of 0.72), this may look disappointing. However, the AVE task for German was much more difficult this year, and the F-score gain of RAVE (over the 100% yes baseline) and qa-accuracy gain (compared to random selection) are better than in last year's runs of MAVE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Answer validation for question answering (QA) systems is often construed as a problem of recognizing textual entailment (RTE). In this approach, the question (e.g. 'When was the Eiffel tower constructed?') and the answer candidate to be validated (e.g. '1889') are turned into a textual hypothesis ('The Eiffel tower was constructed in 1889') which is then checked against a supporting snippet extracted from the document collection -a task which can be implemented as a logical entailment test. The starting point for the current work is MAVE <ref type="bibr" coords="2,218.29,194.38,10.58,8.64" target="#b1">[2]</ref>, an answer validator for German which embarks on this logic-based approach. While MAVE achieved excellent results in the AVE 2007, the system was not yet suitable for answer validation in interactive QA. When used for real-time question answering, the validator must be able to evaluate hundreds of candidate answers in a few seconds. To achieve this, a departure from the RTE approach is inevitable. The reason is that there is no time available to construct a logical representation for the answer candidates, i.e. syntactic-semantic parsing of hundreds of answers candidates (or hundreds of textual hypotheses constructed from question and answers) at query time is not feasible for real-time QA. There is sufficient time for parsing the question, however (since only one question must be analyzed for each query of a user), and there is also sufficient time for parsing all documents and possible support passages (since the corpus is known in advance, so the linguistic analysis of all documents in the corpus can be done at indexing time). Based on these observations, a new approach based on logical passage validation was proposed <ref type="bibr" coords="2,188.44,325.89,10.58,8.64" target="#b2">[3]</ref>. The basic idea is that of limiting the logical validation to a logical verification of the provided snippets: the system does not form a hypothesis from question and answer, but only tries to prove the logical representation of the question from the logical representation of the support passage. In this way the system can determine if the snippet contains a correct answer at all (not necessarily identical to the answer candidate). If the snippet does not contain a correct answer, then the snippet does not provide evidence for any answer candidate extracted from it, and the answer-support pair can be rejected on logical grounds. If the considered snippet does contain a correct answer, however, then additional (extra-logical) criteria are needed for verifying that a given answer candidate is indeed the correct answer contained in the passage. By avoiding the parsing of answers, the current validator, RAVE, can meet the requirements on processing speeds: an exact logical validation for 200 retrieved passages is accomplished in 2.4 seconds on average <ref type="bibr" coords="2,123.25,445.44,10.58,8.64" target="#b2">[3]</ref>. There is also an anytime variant of the method which allows the system to generate validated answers with a user-specified maximum latency <ref type="bibr" coords="2,283.33,457.39,10.79,8.64" target="#b0">[1,</ref><ref type="bibr" coords="2,296.61,457.39,7.19,8.64" target="#b6">7]</ref>.</p><p>Another important improvement of RAVE compared with its predecessor is the treatment of replicated information in aggregation. This is perhaps not so important for the Wikipedia corpus (which contains few redundancy) but it is crucial when working with news corpora (that may contain several duplicates and variants of the same news) and when setting up a multi-stream system since the same answer candidate together with the same supporting snippets can occur in several QA streams. The problem that a simple aggregation model faces here is 'spurious' aggregated evidence, i.e. repetition of a support passage (or of minor variants) could result in an unwanted increase of the confidence score. To eliminate this problem, a replication-tolerant aggregation model was designed for RAVE. In this model, repetition of an answersupport passage pair does not affect aggregation results at all. <ref type="foot" coords="2,333.23,563.32,3.49,6.05" target="#foot_0">2</ref> On the other hand, the effect of aggregation is maximal when the aggregated supporting snippets have no terms in common.</p><p>The paper is organized as follows: Sect. 2 explains the system architecture of RAVE. The focus is placed on the validation core; for details on the actual use of RAVE in QA systems see <ref type="bibr" coords="2,453.96,600.86,10.79,8.64" target="#b0">[1,</ref><ref type="bibr" coords="2,468.20,600.86,7.19,8.64" target="#b6">7]</ref>. Sect. 3 presents the results of RAVE in the AVE 2008, together with ablation studies which reveal the effect of various system components. The paper closes with a discussion of the progress made and also sketches necessary improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System description 2.1 Overview</head><p>The architecture of the RAVE system is shown in Fig. <ref type="figure" coords="2,315.07,713.29,3.74,8.64">1</ref>. The input to the system comprises a question together with answer candidates for the question and the supporting text snippets. In order to introduce Figure <ref type="figure" coords="3,243.49,453.52,3.88,8.64">1</ref>: Architecture of the RAVE system more redundancy for aggregation, a first step of support pool enhancement is used which retrieves additional supporting passages for the candidate answers from the document collections. Notice that this step is not part of the core system since there is usually sufficient redundancy in the QA streams. The question is subjected to a deep linguistic analysis. In the AVE, parsing the supporting snippets during validation was also necessary due to the presence of illegal document ids and corrupted snippets in the test set. In regular operation, however, RAVE never parses any snippets at query time, since the deep parse of the snippet can always be fetched from the pre-analyzed document collections. The question classification serves to identify the descriptive core of the question, and it also determines the expected answer type (EAT) and the question category. Depending on the question classification, the system performs a number of sanity tests on the answer candidates which eliminate trivial answers, for example. The remaining answers are validated based on the results of shallow feature extraction (like lexical overlap) and logic-based feature extraction. A local score is then computed by applying a classifier obtained by machine learning. Aggregation is used to determine a combined score for each answer which captures the joint evidence of all snippets supporting the answer (including the retrieved, auxiliary snippets introduced by the support pool enhancement). After aggregation, these auxiliary support passages are eliminated in the support pool reduction step since they have served their purpose of refining answer evidence. The final step then involves the selection of the best answer based on the aggregated evidence for the answer and the justification strengh of the considered validation item in the test set. The remaining answers are classified as validated or rejected depending on a given threshold.</p><p>In formal terms, the AVE task to be solved by RAVE can be described as follows. The AVE test set consists of validation items i ∈ I given by the question string q i , the answer candidate a i and the supporting snippet s i . For convencience, let Q = {q i : i ∈ I} denote the set of all questions in the test set and let I q = {i ∈ I : q i = q} be the set of all validation items for a given question q ∈ Q.</p><p>The goal of the answer validation and selection task is that of assigning a validation decision v i ∈ {REJECTED, SELECTED, VALIDATED} and a confidence value c i ∈ [0, 1] to each i ∈ I such that at most one answer for each question is selected, i.e. |{i ∈ I q : v i = SELECTED}| ≤ 1. Moreover answers can only be validated if an answer has been selected as the best answer, i.e. if {i ∈ I q : v i = SELECTED} = ∅, then {i ∈ I q : v i = VALIDATED} = ∅ as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Support Pool Enhancement</head><p>As in the last year, the AVE 2008 test set has also been constructed in such a way that every answer candidate occurs only once, i.e. there is virtually no redundancy available in the test set. Typical answer selection techniques rely heavily on redundancy, though, since the existence of many support passages for a given answer often indicates that the answer is correct. In a logic-based answer validation and selection setting, redundancy also helps increase robustness: If several snippets are available which support the same answer, then there is a better chance that a deep linguistic analysis exists for at least one of the support items for an answer. On the other hand, if there is only one support item for the answer (i.e. no redundancy) and if parsing of the support item fails, then the prover cannot be applied for logical validation. Thus, it makes sense to search for additional supporting snippets for each answer in the document collections, and add these snippets to the support pool as auxiliary validation items. <ref type="foot" coords="4,352.18,325.24,3.49,6.05" target="#foot_1">3</ref> Thus, RAVE starts from the pool of 'regular' or 'original' validation items i ∈ I q for a question q. Then A q = {α i : q i = q} is the set of answer candidates q in the test set. An existing QA environment -in this case the IRSAW system <ref type="bibr" coords="4,484.44,350.82,11.62,8.64" target="#b6">[7]</ref> -is used to actively search for additional supporting snippets for each of the answer candidates in A q . These are filtered from the set of answer/support pairs generated by the IRSAW QA streams for the question. This process results in a number of new support items for the answers in A q . In order to improve recall, answers are clustered into groups of minor variants with the same 'answer key' κ(a i ) by applying a simplification function κ on the answers. <ref type="foot" coords="4,198.91,408.93,3.49,6.05" target="#foot_2">4</ref> Not only exact matches of answer candidates pass the filter, but also those which only share the same answer key. This process results in a set of auxiliary validation items i ∈ I q with q i = q and κ(a i ) = κ(a j ) for some original answer a j ∈ A q , and a supporting snippet s i for a i found by the IRSAW QA system for the considered question. The original and auxiliary validation items are joined into the enhanced validation pool I * q = I q ∪ I q for q that contains all original and auxiliary support passages for the answers to be validated. Notice that exact duplicates are not included into I q , i.e. each combination of answer key and supporting snippet is added only once. <ref type="foot" coords="4,393.53,481.89,3.49,6.05" target="#foot_3">5</ref> Though RAVE does not use support pool enhancement when applied in actual QA systems, its discernment of 'regular' support items (which may be shown to the user) and 'auxiliary' support items (which only serve to improve selection, but may not be shown to the user) can be very useful in practice. One obvious application is the use of auxiliary document collections whose licensing conditions do not permit snippets from these collections to be shown to third parties. When treated as a source of 'inofficial' support items that are only internally used for deciding which answer candidates are the correct ones, such a corpus with rigid licensing conditions can still prove useful. Another obvious scenario is multilingual QA. In this case, considering snippets in a language that the user does not understand can still be valuable for improving results by aggregation. However, presenting such snippets to the user would be pointless.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Linguistic Analysis</head><p>RAVE uses WOCADI <ref type="bibr" coords="4,181.19,638.35,10.58,8.64" target="#b5">[6]</ref>, a syntactic-semantic parser for German, for computing deep linguistic analyses of the question and of the documents. As mentioned above, parsing of the snippets at validation time is normally not needed, since all documents are known in advance and can be pre-analyzed. A full parse is found for about 60% of the sentences in the CLEF corpora; in this case a semantic representation in the MultiNet formalism <ref type="bibr" coords="4,171.88,686.17,11.62,8.64" target="#b7">[8]</ref> is constructed which forms the basis for logical validation. If a full parse fails, then WOCADI still produces results of a morpho-lexical analysis (lemmata, possible word senses, numerals, named entity types, results of compound decomposition) which can be utilized for implementing a fallback validation method that works for arbitrary sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Question Classification</head><p>A rule-based approach is used for question classification, with a total of currently 127 classification rules. Consider the query Nenne mir drei Beispiele für Vulkaninseln! ('Name three examples of volcanic islands!'), for example. The classification rules of RAVE then determine the expected answer type (EAT; in this case: island-name), the query category (factual), the desired number of results (three) and the descriptive core of the query (Vulkaninseln, i.e. volcanic islands). The rules also remove parts of the query which are not part of the descriptive question core. In this case, Nenne drei Beispiele ('Name three examples') will be removed from the logical representation of the query and also from the set of word senses and numbers used for the lexical overlap test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Sanity Tests</head><p>A number of sanity tests are applied to the answers and support passages in order to eliminate false positives. The MAVE system used 'soft' sanity checks (which merely reduce the confidence score of an answer candidate); these checks were applied late (after aggregation) in order to avoid the effect of a failed sanity check to be evened out by aggregation. RAVE, by contrast, uses strict sanity checks which are applied at the very moment that an answer or passage enters into the system. In this case, aggregation cannot compromise the effect of a failed sanity check because validation items which fail one of the tests are discarded immediately. The RAVE prototype currently uses the following sanity checks:</p><p>• A test for trivial answers, which checks if the answer merely repeats content of the question. Example: Was ist die Arche Noah? ('What is Noah's Ark?') -Die Arche Noah ('Noah's Ark').</p><p>• A test for non-informative definitions, which eliminates incomplete answers to definition questions involving relational nouns or nominalizations. Examples: Wer war Antonio Gaudi? ('Who was Antonio Gaudi?') -Gegenspieler ('opponent'); Was ist ein Echolot? ('What is Sonar?') -zur Detektion ('for detection').</p><p>• A test for failure of temporal restrictions, currently implemented as a simple year check on support passages. Example: Wer war Russlands Verteidigungsminister 1994? ('Who was Russia's Minister of Defence in 1994?') -rejected snippet: Russlands Verteidigungsminister Gratschow zu Besuch in Bonn ('Russia's Minister of Defence, Gratschow, is visiting Bonn').</p><p>• Two tests for incompatible measurement units, which apply only to MEASURE questions:</p><p>-The question requests a certain class of measurement units (e.g. length units) by naming the measurement dimension ('What length. . . ', 'How long. . . '), but the answer contains an incompatible measurement unit. Example: Bei welcher Temperatur schmilzt Eisen? ('At which temperature does iron glow?') -1,86 m ('1.86 m').</p><p>-The question requests a specific measurement unit, but the measurement unit in the answer differs. Example: Mit wie viel Dollar ist der UNESCO-Friedenspreis dotiert? ('How many Dollars is the UNESCO Prize for Peace endowed with?') -1 Million DM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Shallow Feature Extraction</head><p>RAVE extracts 17 shallow features which only depend on the results of morpho-lexical analysis and on the question classification. Therefore these shallow features can be computed for arbitrary snippets.</p><p>The following passage features only depend on the question and on the support passage (i.e. the need not be recomputed for each answer candidate):</p><p>• failedMatch Number of lexical concepts and numerals in the question which cannot be matched with the candidate document.</p><p>• matchRatio Relative proportion of lexical concepts and numerals in the question which find a match in the candidate document.</p><p>• failedNames Proper names mentioned in the question, but not in the passage.</p><p>• containsBrackets Indicates that the passage contains a pair of parentheses.</p><p>• knownEat Indicates that the expected answer type is known, i.e. question classification has succeeded.</p><p>• testableEat Indicates that the expected answer type is fully supported by the current implementation of the answer type check.</p><p>• eatFound Indicates that an occurrence of the expected answer type has been found in the snippet.</p><p>• isDefQuestion Indicates that the question is a definition question.</p><p>• defLevel Indicates that the snippet contains a defining verb or apposition (defLevel = 2) or a relative clause (defLevel = 1); else defLevel = 0.</p><p>The matching technique used to obtain the values of these shallow features also takes into account synonyms and nominalizations <ref type="bibr" coords="6,203.52,327.53,10.58,8.64" target="#b1">[2]</ref>. The containsBrackets feature was introduced since the queried information is often contained in parentheses, e.g. Mount Everest (4,848 m). Thus the presence of brackets in the passage affects the relevance judgement.</p><p>The above answer-type related passage features are independent of any answer candidate. They describe aspects of the question classification and check for the presence of an expression with the expected answer type in the passage. The implementation of the answer type check in RAVE is not complete yet. While all answer types can be extracted from a successful parse of the snippet, the system is currently only able to extract proper name types (i.e., types of actual named entities) from answers and from snippets with a failed parse. The feature testableEat indicates if the expected answer type can always be checked (i.e. if the EAT is a named entity type), or if the answer type check is only possible for a support passage with known parse.</p><p>When using the validator in actual QA systems, an additional irScore feature is used which contains the original retrieval score of the support passage assigned by the retrieval system. Incorporating a retrieval score based on the tf/idf measure has a positive effect (this is known from tests on the training data), but in the AVE test set, the feature is of course not available.</p><p>In addition to the passage-related features, RAVE also uses several answer-related features which depend on the answer candidate. These features must be computed for each considered answer/passage combination.</p><p>• awFailedMatch Number of lexical concepts and numerals in the answer which cannot be matched with the snippet sentence.</p><p>• awMatchRatio Relative proportion of lexical concepts and numerals in the answer which find a match in the snippet sentence.</p><p>• awFailedNames Proper names mentioned in the answer, but not in the snippet sentence.</p><p>• synthFailedMatch Number of lexical concepts and numerals in the question or in the answer which cannot be matched with the snippet sentence.</p><p>• synthMatchRatio Relative proportion of lexical concepts and numerals in the question or in the answer which find a match in the candidate document.</p><p>• synthFailedNames Proper names mentioned in the question or in the answer, but not in the snippet sentence.</p><p>• awLength Length of the answer, i.e. the number of characters.</p><p>• awEatMatch Indicates that the actual answer type of the answer matches the expected answer type.</p><p>When using the validator in actual QA systems, there is also a producerScore feature which represents the relevance score assigned by the answer source that generated the answer candidate. RAVE then uses separate models for each stream in order to account for individual characteristics of each QA source. This customization is not possible in the AVE task since the QA sources and their characteristics are not known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Logic-Based Feature Extraction</head><p>In order to obtain a more reliable basis for validation, RAVE also computes a set of logic-based features, given that parsing of question and snippet has succeeded. The basic idea of the logical passage validation approach is that proving the question from the supporting snippet can reveal whether the snippet contains a correct answer at all. RAVE thus attempts to prove the logical representation of the question from the logical representation of the supporting snippet and from its background knowledge. <ref type="foot" coords="7,426.51,241.36,3.49,6.05" target="#foot_4">6</ref> The logical query is represented as a conjunction of literals. Consider the query Nennen Sie einige einfache Elementarteilchen.</p><p>('Please name some elementary particles'). The logical query which results from parsing and subsequent question analysis is: prop(F OCU S, einfach. The basic idea behind these features is that the ability of the system to verbalize the answer binding might also have something to say about the relevance of the passage. RAVE extracts useful information even in the case that a proof of the complete query fails. In this case the prover returns the longest proven fragment of the query along with the answer substitution determined from the proof of this fragment. RAVE then extracts the number of proven literals and other features from the results of this partial proof. The system also supports relaxation (i.e. a failed proof can be restarted after simplifying the query by skipping a problematic literal), but this mechanism was switched off in the AVE since it has high computational cost and only a modest effect on validation quality <ref type="bibr" coords="7,419.90,458.22,10.58,8.64" target="#b4">[5]</ref>.</p><p>• skippedLitsLb Number of known failed literals. Since relaxation is switched off, this feature is 0 if a complete proof succeeds and 1 if a complete proof fails.</p><p>• skippedLitsUb Number of known failed literals, plus literals with unknown status. Since no relaxation is used, this feature equals the number of all literals minus the length of the longest proven fragment of the query.</p><p>• litRatioLb Fraction of actually proved literals compared to the total number of query literals, i.e. 1 -skippedLitsUb/allLits.</p><p>• litRatioUb Fraction of potentially provable literals vs. all literals, i.e. 1 -skippedLitsLb/allLits.</p><p>• boundFocus Indicates that a binding for the queried variable was found.</p><p>• extractedFocus Signals that RAVE has managed to extract the substring of the snippet which corresponds to the computed binding of the FOCUS variable.</p><p>• npFocus Indicates that the answer string obtained by the logical answer extraction method is a nominal phrase (NP).</p><p>• focusEatMatch Indicates that the answer type of the answer binding found by the prover matches the expected answer type.</p><p>• focusDefLevel Indicates that the answer binding found by the prover corresponds to an apposition (result: 2) or to an NP with a relative clause (result: 1), else 0.</p><p>It should be pointed out that all logic-based features depend only on the question and on the supporting snippet, i.e. one proof per snippet is sufficient to determine these features. No attempt is made at this time to compare the answer string extracted as a side-effect of logical passage validation with the answer candidates to be checked. There is apparent room for improvements here, and features which relate the answer candidates to the results of logical validation/extraction will be added in a later version of RAVE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">ML-Based Local Scoring</head><p>A machine learning approach is used on order to assign an evidence score η i ∈ [0, 1] to each i ∈ I * q which estimates the probability that answer a i is both correct and properly supported by snippet s i . A basic technique suitable for the task was developed in <ref type="bibr" coords="8,315.06,231.27,10.58,8.64" target="#b2">[3]</ref>, using the Weka toolbench <ref type="bibr" coords="8,442.31,231.27,15.27,8.64" target="#b9">[10]</ref>. Bagging of decision trees is used for learning a model which then serves for obtaining probability estimates. Due to the unbalanced training data (there are typically few positive exemplars and a large number of negative exemplars), cost-sensitive learning by re-weighting of training examples was applied <ref type="bibr" coords="8,430.00,267.13,39.83,8.64">[10, p. 165</ref>]. In order to emphasize the results of interest (i.e. the correct answers), a weight of β = 0.3 for false positives and of 1.0 for false negatives was used throughout. <ref type="foot" coords="8,281.28,289.37,3.49,6.05" target="#foot_5">7</ref> Class probabilities were obtained from the numbers of positive and negative examplars at the leaves of the decision trees in the usual way. However, as a result of the re-weighting of training examples, these numbers refer to the re-weighted training set and do not reflect the true class probabilities. This effect is corrected by applying the function ρ(x) = β x/(1 -x + β x) to the relative frequencies of the YES class at the leaves of the decision trees. In particular, for β = 0.3, one obtains ρ(0.5) ≈ 0.23 as the appropriate threshold for the validation decision. This basic approach was used for learning separate models for factual vs. definition questions and for the full set of features vs. shallow-only features. However, the AVE 2008 development set was considered too small for providing useful training data. Therefore existing training sets for the IRSAW system <ref type="bibr" coords="8,387.87,386.68,11.62,8.64" target="#b6">[7]</ref> were re-used as training sets for RAVE. These training sets cover a total of 10,447 annotated answer candidates and 27,919 passages generated in an IRSAW run on the CLEF 2007 questions. <ref type="foot" coords="8,331.28,408.93,3.49,6.05" target="#foot_6">8</ref> Notice that all these models were trained on single-sentence snippets. For the moment, RAVE handles multi-sentence snippets by iterating over all sentences and choosing the maximum sentence score for η i . RAVE also remembers the sentence that produced the maximum score; this best sentence is later needed for aggregation. Moreover there is a special treatment of full-sentence answers to definition questions. If a full-sentence answer to a definition question is recognized, and if the answer sentence is part of the snippet, then RAVE judges the correctness of the long answer by a classifier trained to recognize sentences which contain answers to definition questions. If the score obtained in this way exceeds the result obtained by the regular method determining the evaluation score, then the result of the full-sentence classifier is chosen for η i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">Aggregation</head><p>So far, we have local scores η i that express the direct evidence for an answer a i judging from a particular supporting snippet s i . Intuitively, the plausibility of an answer candidate increases when there are multiple support passages for the answer. In order to capture the joint evidence γ(a) for an answer judging from all snippets supporting it, an aggregation technique is used. The aggregation model of RAVE was designed to be robust against replicated information. That is, multiple copies of the same supporting snippet should not result in an increase of the aggregated score since such copies do not provide independent evidence. Assuming that more diversity of the snippets usually means better independence of the sources, strongly overlapping snippets should also have few effect on the aggregated score, while snippets with no common terms at all should achieve the strongest gain of the aggregation result. These requirements can be met by the following aggregation model.</p><p>We assume a simplification mapping κ from answers to simplified answer keys (see Sect. 2.2; in the simplest case, κ can also be the identity). The set of answer keys for a given question is K q = {κ(a i ) : i ∈ I q }. For k ∈ K q , let I q,k = {i ∈ I q : κ(a i ) = k}, i.e. I q,k is the set of all support items for the considered answer key. For each i ∈ I q , let Ω i be the set of term occurrences<ref type="foot" coords="9,408.75,146.53,3.49,6.05" target="#foot_7">9</ref> in the supporting snippet s i . For a term occurrence ω ∈ Ω i , let t(ω) be the corresponding term. Further let T i = {t(ω) : ω ∈ Ω i } be the set of all terms in the support passage and T k = {T i : i ∈ I q,k } the set of all terms which occur in a passage for answer key k ∈ K q . We abbreviate</p><formula xml:id="formula_0" coords="9,179.00,203.38,334.00,14.38">µ(k, t) = min {(1 -η i ) ν : i ∈ I q,k , t ∈ T i } , ν = occ(t,i) |Ωi| ,<label>(1)</label></formula><p>where occ(t, i) = |{ω ∈ Ω i : t(ω) = t}| is the occurrence count of term t in snippet i, and η i is the correctness probability of the passage estimated by the ML classifier. The aggregated support for an answer key k ∈ K q is then given by</p><formula xml:id="formula_1" coords="9,251.40,275.07,261.60,20.73">γ(k) = 1 - t∈T k µ(k, t) .<label>(2)</label></formula><p>We extend this to extracted answers by stipulating that γ(a) = γ(κ(a)).</p><p>In general, the effect of aggregation will be strongest when two aggregated passages have no terms in common (i.e. the passages represent independent evidence), and there will be no effect of aggregation at all when the same passage is encountered repeatedly. The following properties of the method are obvious: a. Suppose that i, i ∈ I * q with κ(a i ) = κ(a i ), s i = s i (i.e. the two snippets are identical and support equivalent answers). Further suppose without loss of generality that η i ≥ η i (otherwise i and i can be swapped). <ref type="foot" coords="9,167.65,383.20,6.97,6.05" target="#foot_8">10</ref> Let γ(a i ) be the aggregation result obtained from I * q and γ (a i ) be the aggregation result obtained from I * q \ {i }. Then γ (a i ) = γ(a i ), i.e. the aggregation result is not affected by the duplicate support item i. Moreover, when there are repeated snippets, then the maximum score of these support items will enter into the aggregation. b. It generally holds that γ max (k) ≤ γ(k) ≤ γ indep (k), where</p><formula xml:id="formula_2" coords="9,252.91,458.16,260.09,11.72">γ max (k) = max {η i : i ∈ I q,k }<label>(3)</label></formula><formula xml:id="formula_3" coords="9,248.62,476.20,264.38,22.81">γ indep (k) = 1 - i∈I q,k (1 -η i ) .<label>(4)</label></formula><p>c. The equality γ(k) = γ indep (k) holds exactly if either η i = 1 for some i ∈ I q,k , or if T i ∩ T j = ∅ for all i, j ∈ I q,k with η i &gt; 0 and η j &gt; 0. In particular, when there is no overlap between the snippets at all, then the evidence provided by the snippets is treated as independent.</p><p>The actual implementation of the method in RAVE is slighly more complicated because RAVE uses word senses and numerals as the terms (rather than word forms or word stems). The point is that the occurrence of a word only corresponds to a unique word sense if the WOCADI parser can determine a complete parse of the snippet. When a parse fails, however, then the possible word senses for a word cannot be disambiguated. The original approach is therefore changed in such a way that multiple word-sense alternatives for a given word occurrence can be handled as well. A word occurrence ω now corresponds to a finite set of terms T (ω) = {t 1 , . . . , t r } with r &gt; 0, as compared to the single term t(ω) in the original approach. If T (ω) contains more than one alternative, then all elements in T (ω) are considered as occurring with weight 1/r. Now abbreviating T i = {T (ω) : ω ∈ Ω i }, we adapt the definition of the term occurrence count for t ∈ T i as follows,</p><formula xml:id="formula_4" coords="9,227.21,686.60,281.92,20.53">occ(t, i) = {ω∈Ωi:t∈T (ω)} 1/ |T (ω)| . (<label>5</label></formula><formula xml:id="formula_5" coords="9,509.13,686.92,3.87,8.64">)</formula><p>Another modification of the basic method is concerned with the considered segment of the snippet. Rather than extracting word senses and numerals from the whole snippet, RAVE only considers a single, 'best' sentence, viz the sentence which attained the best score when determining η i . This modification is motivated as follows: if the supporting snippets are longer than necessary, then the excess parts of the snippets can wrongly suggest diversity (because the text surrounding the relevant information is different) though the relevant sentence is in fact only duplicated. In order to avoid such negative effects of long snippets that also contain irrelevant sentences, a single most relevant sentence is picked from each snippet. Notice that the actual implementation does not use equations ( <ref type="formula" coords="10,352.57,196.02,3.87,8.64" target="#formula_0">1</ref>) and ( <ref type="formula" coords="10,383.31,196.02,3.87,8.64" target="#formula_1">2</ref>) directly, but rather switches to logarithms in order to improve numerical stability. Thus, the following equations are used:</p><formula xml:id="formula_6" coords="10,169.56,228.05,343.44,14.38">µ (k, t) = min {ν • ln(1 -η i ) : i ∈ I q,k , t ∈ T i } , ν = occ(t,i) |Ωi| ,<label>(6)</label></formula><formula xml:id="formula_7" coords="10,180.67,247.01,332.33,12.78">γ(k) = 1 -e P t∈T k µ (k,t)<label>(7)</label></formula><p>If η i for some i ∈ I q,k , we stipulate that γ(k) = 1, as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10">Support Pool Reduction</head><p>After aggregation, the auxiliary supporting snippets are no longer needed. They are dropped to prevent them from showing up in the final output of RAVE. This is achieved by reducing the support pool from I * q to the original cases in I q again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.11">Answer Selection and Validation Decision</head><p>The global answer score γ(a) obtained by aggregation abstracts from individual support items. However, the validation/selection decisions apply to answer/snippet pairs (a i , s i ). Therefore the final selection score σ i should not only depend on the aggregated score γ(a i ) but also take the justification strength η i of the given snippet s i into account. The submitted runs use the following formula for computing the σ i score:</p><formula xml:id="formula_8" coords="10,220.19,447.51,292.81,24.19">σ i = η i γ(a i ) max {η j : j ∈ I * q , κ(a i ) = κ(a j )}<label>(8)</label></formula><p>This means that the 'best' support item in I * q which supports a given answer a, i.e. i * ∈ I * q with a i * = a and η i * = max {η j : j ∈ I * q , κ(a i ) = κ(a j )}, is boosted to the aggregated score σ i * = γ(a i * ). The formula shows some undesirable effects, though. For example, if the best support item for the considered answer a is an auxiliary support item i * ∈ I q with a direct score η i * = 1, then the aggregation result γ(a) has no effect at all, i.e. σ i = η i for all i ∈ I * q with κ(a i ) = κ(a). In order to avoid this effect, the current version of RAVE (after the AVE) now uses the following improved formula:</p><formula xml:id="formula_9" coords="10,215.01,563.09,297.99,23.23">σ new i = η i γ(a i ) max {η j : j ∈ I q , κ(a i ) = κ(a j )}<label>(9)</label></formula><p>where the index j ranges only over support items in the original set I q . The modified formula will boost the best original support item for a given answer, i.e. i.e. i * ∈ I q with a i * = a and η i * = max {η j : j ∈ I q , κ(a i ) = κ(a j )}, yielding the aggregated value σ i * = γ(a). Instead of <ref type="bibr" coords="10,147.00,633.47,10.58,8.64" target="#b8">(9)</ref>, it is also possible to use a simple weighted average of aggregated and direct score, viz</p><formula xml:id="formula_10" coords="10,197.67,653.54,311.18,14.07">σ (λ) i = λγ(a i ) + (1 -λ)η i for some γ ∈ [0, 1]. (<label>10</label></formula><formula xml:id="formula_11" coords="10,508.85,656.99,4.15,8.64">)</formula><p>Based on the assignment of final selection scores σ i , the system determines a choice of i opt ∈ I q which maximizes σ i , i.e. σ i opt = max {σ i : i ∈ I q }. The chosen i opt is marked as</p><formula xml:id="formula_12" coords="10,90.00,690.55,423.00,21.74">v i opt = SELECTED if σ i opt ≥ θ sel , where θ sel ∈ [0, 1] is the selection threshold; otherwise i opt is marked as v i opt = REJECTED.</formula><p>In particular, θ sel = 0 can be used in order to force selection (i.e. the best validation item for a question will always be selected); this is usually a good choice for maximizing the number of correct selections. In experiments aiming at a good F-score, a threshold of θ sel = 0.23 ≈ ρ(0.5) was used. The non-best items i ∈ I q \ {i opt } are classified as follows: if v i opt = REJECTED, i.e. no selection has been made, then v i = REJECTED for all i ∈ I q \ {i opt } as well. On the other hand, if a selection has been made, i.e. v i opt = SELECTED, then we set v i = VALIDATED if σ i ≥ θ val and v i = REJECTED otherwise, where θ val ∈ [0, 1] is the decision threshold for validating the non-best items. In the experiments, θ val = 0.23 ≈ ρ(0.5) was used throughout.</p><p>The confidence into this validation decision is given by c i = σ i in the positive case that v i = SELECTED or v i = VALIDATED, and by</p><formula xml:id="formula_13" coords="11,211.22,277.76,130.77,9.65">c i = 1 -σ i if v i = REJECTED.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Preliminaries</head><p>The AVE 2008 test set for German contains 1027 validation items for 119 questions. 111 of these answer/support items are classified as correct, 854 as wrong and 66 as undecided. A 100% YES baseline (all items accepted) thus achieves a precision of 0.12 and F-score of 0.21. A correct answer exists for 62 of the questions, i.e. the qa-accuracy (correct selections divided by number of questions) is bounded by 0.52. Random selection of an answer yields a qa-accuracy of 0.11 and selection rate (compared to optimal selection) of 0.21. These numbers are very different from the situation in 2007 where the test set for German had a precision of 0.25 and F-score of 0.40 for the 100% YES baseline, and a qa-accuracy of 0.28 and selection rate as high as 0.52 for random selection.</p><p>The support pool enhancement by mining for additional supporting snippets in the QA@CLEF corpora for German resulted in 579 auxiliary answer-snippet pairs and an enhanced pool with 1,606 items total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of AVE 2008 Runs</head><p>The results of RAVE in the AVE 2008 are shown in Table <ref type="table" coords="11,319.94,509.43,3.74,8.64" target="#tab_1">1</ref>. The following column labels are used: f-score (F-score, i.e. harmonic mean of precision and recall), f-gain (actual F-score divided by F-score for the 100% YES baseline), prec (precision for the YES class), p-gain (actual precision divided by precision of the 100% YES baseline), recall for the YES class, qa-acc (qa-accuracy, i.e. correct selections divided by number of questions), qa-perf (estimated qa-performance as defined in the AVE 2008 task description), sel-rate (selection rate, i.e. successful selections divided by optimal selections), s-gain (selection gain, i.e. actual selection rate divided by the selection rate for random selection). Both submitted runs used equation ( <ref type="formula" coords="11,130.39,593.12,3.87,8.64" target="#formula_8">8</ref>) for combining the local snippet score and the aggregated score. Run1 also included the special treatment of full-sentence answers to definition questions. Since it was not clear from the QA@CLEF 2008 guidelines if such answers would be accepted or not, the second run was configured such that all fullsentence answers were rejected. However, the low scores for Run2 demonstrate that defining sentences were generally accepted by the annotators. Therefore only Run1, whose treatment of defining sentences agrees with the official annotation, will be considered further in the following ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies and Results for System Variants</head><p>Table <ref type="table" coords="11,113.83,700.09,4.98,8.64" target="#tab_2">2</ref> lists the reference results for the current version of RAVE after correcting a few bugs that surfaced in the meantime. <ref type="foot" coords="11,146.45,710.38,6.97,6.05" target="#foot_9">11</ref> The letter 'R' refers to the standard method of RAVE for combining local and aggregated  <ref type="formula" coords="12,175.10,392.06,3.53,8.64" target="#formula_9">9</ref>), 'F' means the use of θ sel = 0.23 for F-score oriented runs, and 'Q' signals the use of θ sel = 0 for runs aiming at qa-accuracy. The RQ method corresponds to that used for Run1.</p><p>We will first consider effects related to aggregation. Table <ref type="table" coords="12,353.15,415.97,4.98,8.64">3</ref> lists the results obtained when using equation <ref type="bibr" coords="12,126.37,427.92,16.60,8.64" target="#b9">(10)</ref> instead of (9) for a weighted average of the local score and the aggregated score with different values λ ∈ {0, 0.25, 0.5, 0.75, 1} for the weight of the aggregated score. (This is symbolized by the letter 'W' and the value of λ shown as the suffix of the runs.) Choosing γ = 0 in the WF0 and WQ0 runs means that no aggregation is used at all. Comparing WF0 with RF, and WQ0 with WF, we find a strong effect of aggregation (plus 8 percent points of F-score and a similar increase in the selection rate). The best F-score of 0.47 is reached for λ = 0.75 and the best selection rate (of 0.65) for λ = 1. In this case, the selection and validation decision depends only on the aggregated evidence for the answer a i , and the evidence η i from the considered snippet s i itself has zero weight. Surprisingly, this method still has relatively good F-score. The selection results for RF/WF1 and RQ/WQ1 are identical.</p><p>Table <ref type="table" coords="12,129.31,535.52,4.98,8.64" target="#tab_3">4</ref> shows the results obtained when replacing the replication-tolerant aggregation scheme of Sect. 2.9 with either 'best evidence' aggregation score γ max given by (3), letter 'B', or with the independent evidence aggregation score γ indep given by (4), symbolized by 'I'. It turns out that γ max generally performs worse than the standard method of RAVE. The results of γ indep are closer to those of the standard method of RAVE. Since γ indep is not stable against duplication of snippets, these results profit from the fact that I * q does not contain exact duplicates (but there still can be overlapping snippets). Table <ref type="table" coords="12,130.85,607.25,4.98,8.64">5</ref> shows the results of RAVE without active validation, i.e. when working with the original answer-support items in I q only. Since RAVE aggregates results for answer keys rather than exact answer strings, it can exploit some minimal redundancy still present in the AVE08 test set. Therefore results are better than in the WF0 and WQ0 runs that use no aggregation at all, but considerably worse than those for support pool enhancement; see F-scores for RF (0.45), PRF (0.40) and WF0 (0.37).</p><p>Table <ref type="table" coords="12,129.92,667.03,4.98,8.64" target="#tab_4">6</ref> shows the results of RAVE when run with the prover switched off. There is a drastic loss in selection rate (e.g. a decrease by 15 percent points for SRQ compared to RQ), but surprisingly, a consistent improvement in the F-score. The SWF1 run even achieved the best F-score of all system variants of RAVE that were tested. These findings run counter to experience from many experiments on annotated CLEF07 data all of which show a better F-score when adding logic-based features <ref type="bibr" coords="12,436.93,714.85,10.79,8.64" target="#b2">[3,</ref><ref type="bibr" coords="12,451.52,714.85,7.19,8.64" target="#b4">5]</ref>. A possible work. Synonym normalization of word senses was switched on for all questions, but should not be used for definition questions. The implementation of the important awEatMatch feature had to be corrected. Finally equation ( <ref type="formula" coords="12,384.09,743.36,3.10,6.91" target="#formula_8">8</ref>) was replaced by the improved <ref type="bibr" coords="12,489.57,743.36,8.46,6.91" target="#b8">(9)</ref>. explanation is that eight of the eleven runs submitted for German were produced by QA systems which use variants of RAVE as the validator -and repeating the same, logic-based validation criterion already used when generating the runs is hardly effective in detecting the remaining false positives. The shallow-only approach, by contrast, is implemented by a different classifier trained on a larger data set.<ref type="foot" coords="13,448.37,495.97,6.97,6.05" target="#foot_10">12</ref> It is probably only this independence benefit which reflects in the improved F-scores of the shallow technique. Finally, Table <ref type="table" coords="13,162.65,521.55,4.98,8.64">7</ref> shows the results of the MAVE system <ref type="bibr" coords="13,329.22,521.55,11.62,8.64" target="#b1">[2]</ref> on the AVE 2008 test set, using clustering of answers ('C') and optimizing either F-score or qa-accuracy ('F' vs. 'Q'). Despite its use of a full logical answer-validation (i.e., hypothesis-snippet proofs), MAVE does not outperform RAVE with its simple logical passage test based on question-snippet proofs. The results may look disappointing compared to the 0.72 F-score and 0.93 selection rate of MAVE in the AVE 2007. However, the former F gain of 0.79 and selection gain of 1.8 in the AVE 2007 are considerably lower than current results in the AVE 2008.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suitability for Real-Time Validation</head><p>Since RAVE is designed for real-time answer validation in interactive QA systems, the actual processing time needed for validation and aggregation is also of interest. For the standard method (RQ run), validation and aggregation/selection took an average 126 ms per question (or 9.35 ms per validation item). <ref type="foot" coords="13,476.91,650.76,6.97,6.05" target="#foot_11">13</ref> When switching off the support pool enhancement (PRQ run), validation time drops to an average 76 ms per question (or 8.8 ms per validation item). For comparison, the validation time for shallow-only validation without using the prover is 77 ms per question (or 5.7 ms per validation item) in the SRQ run, and 50 ms </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The main objective for the current work was developing a logic-based answer validator suitable for use in real-time QA. The design of RAVE results from the premise that optimizing the prover is not sufficient to achieve this goal. Therefore the RTE paradigm for answer validation was replaced by a simplified model which only uses logic for passage validation. The actual processing times of RAVE on the AVE 2008 data confirm that the method can be applied in real-time QA even for large numbers of answer candidates.</p><p>Another key feature of RAVE is the replication-tolerant aggregation model. Redundancy is usually helpful for answer validation. However, redundancy only means stronger evidence when the sources are sufficiently independent. In particular, multiple copies or variants of the same document (e.g. news report or press release) should not provide more weight than a single occurrence of the original document. The aggregation model of RAVE leverages redundancy without being misled by replicated content. Experiments confirm the superiority of the model over several alternative approaches.</p><p>An interesting aspect of RAVE is the discernment of regular and auxiliary support passages; the latter are only used for aggregation and never actually shown to the user. For example, snippets from corpora with restrictive licensing can be valuable sources of information but may not be presented to the user for legal reasons. The same holds for supporting passages in a language unknown to the user; in this case, presenting the snippet in the final result would be pointless.</p><p>RAVE achieved an F-score of 0.39 and qa-accuracy of 0.32 in the AVE 2008. These scores are lower than those of MAVE in the last year. However, the test sets for German had very different characteristics in these two years, and the F-score and qa-accuracy are not commensurable for different test sets. The F-score gain compared to the 100% YES baseline <ref type="bibr" coords="14,263.29,661.13,10.58,8.64" target="#b8">[9]</ref>, and the selection gain compared to random selection, are better suited for a comparison across test sets since they relate the validation and selection results to the obvious baselines. From this perspective, the RAVE results for 2008 even look better than those of MAVE for 2007. Moreover RAVE outperforms MAVE on the 2008 test set. It must be admitted, though, that the best individual QA system in the test set performed better than RAVE: It achieved a qa-accuracy of 0.38 (or selection rate of 0.73), compared to the qa-accuracy of 0.34 and selection rate of 0.65 of RAVE in the RQ run. This clearly indicates that improvements of the validator are necessary. Generally speaking, RAVE has good features for recognizing passages with a correct answer but (currently) poor means for verifying that a given candidate equals this correct answer. Therefore the most important change to RAVE will be the addition of features which judge the compatibility of the answer candidate with the result of the question-passage proof. The answer-type check must also be improved since the current implementation is very limited.</p><p>On the other hand, the AVE imposed some adverse conditions on the validator so that RAVE will likely perform better in actual applicatons. Firstly, the features used by RAVE normally include the retrieval score of the passage retrieval system and a producer score assigned by the QA source; these features are not available in the AVE test set. Cross-validation on the training set shows that support for these features might improve the F score by up to 8 percent points. Second, the AVE development set for German was too small for training the classifiers. Therefore an existing training set of sufficient size had to be chosen whose characteristics are not necessarily similar to those of the AVE 2008 test set. In practice, RAVE is normally adjusted to each QA source by learning a separate model for each QA system. Such customization was not possible in the AVE since the QA sources and their characteristics were not disclosed.</p><p>RAVE is already part of two question answering systems. It serves as the answer validator in a multistream system (IRSAW) <ref type="bibr" coords="15,190.30,279.71,10.58,8.64" target="#b6">[7]</ref>, where it provides incremental, any-time answer validation capability. Moreover RAVE is the core of the evolving LogAnswer system <ref type="bibr" coords="15,327.12,291.67,10.79,8.64" target="#b0">[1,</ref><ref type="bibr" coords="15,340.74,291.67,7.19,8.64" target="#b4">5]</ref>, which uses the question-passage proofs of RAVE for simultaneously extracting answer bindings and validating the corresponding answers. It is too early at this time to decide which of the two approaches is superior to the other. However, the existing integration of RAVE in both kinds of systems will make it possible to address this issue in the near future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,145.59,108.86,311.82,319.26"><head></head><label></label><figDesc></figDesc><graphic coords="3,145.59,108.86,311.82,319.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,90.00,278.57,423.00,92.64"><head></head><label></label><figDesc>1.1) ∧ sub(F OCU S, elementarpartikel.1.1). The example demonstrates how synonyms are used for normalization of word senses: elementarteilchen.1.1 (elementary particle) is replaced by the canonical elementarpartikel.1.1. The FOCUS variable represents the queried information. If a proof of the query succeeds, then the FOCUS variable gets bound to an entity in the logical representation of the snippet. The WOCADI parser provides word alignment information which can be used to recover the substring of the snippet which corresponds to the binding of the FOCUS variable.</figDesc><table /><note coords="7,90.00,350.62,423.00,8.64;7,90.00,362.58,257.47,8.64"><p>Due to the availability of alignment information, this logic-based answer extraction can be performed very quickly. It is used to define several extraction-related features.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,122.88,119.13,357.24,52.41"><head>Table 1 :</head><label>1</label><figDesc>Results of RAVE in the AVE 2008model f-score f-gain prec p-gain recall qa-acc qa-perf sel-rate s-gain</figDesc><table coords="11,122.88,150.95,357.24,20.59"><row><cell>Run1</cell><cell>0.39</cell><cell>0.89 0.33</cell><cell>2.83</cell><cell>0.49</cell><cell>0.32</cell><cell>0.32</cell><cell>0.61</cell><cell>2.91</cell></row><row><cell>Run2</cell><cell>0.29</cell><cell>0.40 0.25</cell><cell>2.16</cell><cell>0.34</cell><cell>0.23</cell><cell>0.23</cell><cell>0.44</cell><cell>2.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,90.00,119.13,392.19,281.56"><head>Table 2 :</head><label>2</label><figDesc>Reference runs for the ablation experiments: Boosting method</figDesc><table coords="12,90.00,140.62,392.19,260.08"><row><cell cols="9">model f-score f-gain prec p-gain recall qa-acc qa-perf sel-rate s-gain</cell></row><row><cell>RF</cell><cell>0.45</cell><cell>1.20 0.43</cell><cell>3.75</cell><cell>0.48</cell><cell>0.26</cell><cell>0.34</cell><cell>0.50</cell><cell>2.37</cell></row><row><cell>RQ</cell><cell>0.44</cell><cell>1.11 0.36</cell><cell>3.10</cell><cell>0.56</cell><cell>0.34</cell><cell>0.34</cell><cell>0.65</cell><cell>3.06</cell></row><row><cell></cell><cell cols="7">Table 3: Results for weighted averages instead of the boosting method</cell><cell></cell></row><row><cell>model</cell><cell cols="8">f-score f-gain prec p-gain recall qa-acc qa-perf sel-rate s-gain</cell></row><row><cell>WF0</cell><cell>0.37</cell><cell>0.78 0.40</cell><cell>3.44</cell><cell>0.34</cell><cell>0.21</cell><cell>0.28</cell><cell>0.40</cell><cell>1.91</cell></row><row><cell>WF.25</cell><cell>0.43</cell><cell>1.09 0.44</cell><cell>3.82</cell><cell>0.42</cell><cell>0.22</cell><cell>0.29</cell><cell>0.42</cell><cell>1.99</cell></row><row><cell>WF.5</cell><cell>0.43</cell><cell>1.10 0.43</cell><cell>3.76</cell><cell>0.43</cell><cell>0.23</cell><cell>0.30</cell><cell>0.44</cell><cell>2.07</cell></row><row><cell>WF.75</cell><cell>0.47</cell><cell>1.29 0.45</cell><cell>3.92</cell><cell>0.50</cell><cell>0.25</cell><cell>0.33</cell><cell>0.48</cell><cell>2.29</cell></row><row><cell>WF1</cell><cell>0.47</cell><cell>1.26 0.44</cell><cell>3.83</cell><cell>0.50</cell><cell>0.26</cell><cell>0.34</cell><cell>0.50</cell><cell>2.37</cell></row><row><cell>WQ0</cell><cell>0.36</cell><cell>0.75 0.31</cell><cell>2.69</cell><cell>0.43</cell><cell>0.29</cell><cell>0.29</cell><cell>0.56</cell><cell>2.68</cell></row><row><cell>WQ.25</cell><cell>0.40</cell><cell>0.95 0.34</cell><cell>2.93</cell><cell>0.50</cell><cell>0.29</cell><cell>0.29</cell><cell>0.55</cell><cell>2.60</cell></row><row><cell>WQ.5</cell><cell>0.41</cell><cell>0.99 0.34</cell><cell>2.99</cell><cell>0.51</cell><cell>0.30</cell><cell>0.30</cell><cell>0.58</cell><cell>2.75</cell></row><row><cell>WQ.75</cell><cell>0.45</cell><cell>1.18 0.37</cell><cell>3.20</cell><cell>0.58</cell><cell>0.33</cell><cell>0.33</cell><cell>0.63</cell><cell>2.98</cell></row><row><cell>WQ1</cell><cell>0.45</cell><cell>1.16 0.36</cell><cell>3.16</cell><cell>0.58</cell><cell>0.34</cell><cell>0.34</cell><cell>0.65</cell><cell>3.06</cell></row><row><cell cols="2">scores, see equation (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="13,117.49,119.13,368.03,307.84"><head>Table 4 :</head><label>4</label><figDesc>Results for alternative aggregation methods</figDesc><table coords="13,117.49,140.62,368.03,286.36"><row><cell>model</cell><cell cols="8">f-score f-gain prec p-gain recall qa-acc qa-perf sel-rate s-gain</cell></row><row><cell>BRF</cell><cell>0.41</cell><cell>0.97 0.42</cell><cell>3.61</cell><cell>0.40</cell><cell>0.24</cell><cell>0.33</cell><cell>0.47</cell><cell>2.22</cell></row><row><cell>BWF.75</cell><cell>0.41</cell><cell>0.97 0.42</cell><cell>3.61</cell><cell>0.40</cell><cell>0.23</cell><cell>0.30</cell><cell>0.44</cell><cell>2.07</cell></row><row><cell>BWF1</cell><cell>0.41</cell><cell>0.97 0.42</cell><cell>3.61</cell><cell>0.40</cell><cell>0.24</cell><cell>0.33</cell><cell>0.47</cell><cell>2.22</cell></row><row><cell>IRF</cell><cell>0.45</cell><cell>1.16 0.42</cell><cell>3.63</cell><cell>0.48</cell><cell>0.26</cell><cell>0.34</cell><cell>0.50</cell><cell>2.37</cell></row><row><cell>IWF.75</cell><cell>0.47</cell><cell>1.26 0.44</cell><cell>3.83</cell><cell>0.50</cell><cell>0.26</cell><cell>0.34</cell><cell>0.50</cell><cell>2.37</cell></row><row><cell>IWF1</cell><cell>0.46</cell><cell>1.22 0.43</cell><cell>3.71</cell><cell>0.50</cell><cell>0.26</cell><cell>0.34</cell><cell>0.50</cell><cell>2.37</cell></row><row><cell>BRQ</cell><cell>0.39</cell><cell>0.89 0.33</cell><cell>2.86</cell><cell>0.48</cell><cell>0.32</cell><cell>0.32</cell><cell>0.61</cell><cell>2.91</cell></row><row><cell>BWQ.75</cell><cell>0.39</cell><cell>0.88 0.33</cell><cell>2.84</cell><cell>0.48</cell><cell>0.30</cell><cell>0.30</cell><cell>0.58</cell><cell>2.75</cell></row><row><cell>BWQ1</cell><cell>0.39</cell><cell>0.89 0.33</cell><cell>2.86</cell><cell>0.48</cell><cell>0.32</cell><cell>0.32</cell><cell>0.61</cell><cell>2.91</cell></row><row><cell>IRQ</cell><cell>0.43</cell><cell>1.06 0.35</cell><cell>3.01</cell><cell>0.55</cell><cell>0.33</cell><cell>0.33</cell><cell>0.63</cell><cell>2.98</cell></row><row><cell>IWQ.75</cell><cell>0.44</cell><cell>1.14 0.36</cell><cell>3.13</cell><cell>0.57</cell><cell>0.33</cell><cell>0.33</cell><cell>0.63</cell><cell>2.98</cell></row><row><cell>IWQ1</cell><cell>0.44</cell><cell>1.11 0.35</cell><cell>3.08</cell><cell>0.57</cell><cell>0.33</cell><cell>0.33</cell><cell>0.63</cell><cell>2.98</cell></row><row><cell></cell><cell></cell><cell cols="4">Table 5: Results without active validation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>model</cell><cell cols="8">f-score f-gain prec p-gain recall qa-acc qa-perf sel-rate s-gain</cell></row><row><cell>PRF</cell><cell>0.40</cell><cell>0.94 0.42</cell><cell>3.69</cell><cell>0.38</cell><cell>0.22</cell><cell>0.29</cell><cell>0.42</cell><cell>1.99</cell></row><row><cell>PWF.75</cell><cell>0.41</cell><cell>0.98 0.43</cell><cell>3.74</cell><cell>0.39</cell><cell>0.22</cell><cell>0.29</cell><cell>0.42</cell><cell>1.99</cell></row><row><cell>PWF1</cell><cell>0.41</cell><cell>0.98 0.43</cell><cell>3.74</cell><cell>0.39</cell><cell>0.22</cell><cell>0.29</cell><cell>0.42</cell><cell>1.99</cell></row><row><cell>PRQ</cell><cell>0.38</cell><cell>0.84 0.32</cell><cell>2.81</cell><cell>0.46</cell><cell>0.29</cell><cell>0.29</cell><cell>0.56</cell><cell>2.68</cell></row><row><cell>PWQ.75</cell><cell>0.39</cell><cell>0.87 0.33</cell><cell>2.84</cell><cell>0.47</cell><cell>0.29</cell><cell>0.29</cell><cell>0.56</cell><cell>2.68</cell></row><row><cell>PWQ1</cell><cell>0.39</cell><cell>0.87 0.33</cell><cell>2.84</cell><cell>0.47</cell><cell>0.29</cell><cell>0.29</cell><cell>0.56</cell><cell>2.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,90.00,119.13,423.00,267.59"><head>Table 6 :</head><label>6</label><figDesc>Shallow-only results</figDesc><table coords="14,118.04,140.62,366.92,190.71"><row><cell>model</cell><cell cols="8">f-score f-gain prec p-gain recall qa-acc qa-perf sel-rate s-gain</cell></row><row><cell>SRF</cell><cell>0.50</cell><cell>1.42 0.42</cell><cell>3.67</cell><cell>0.61</cell><cell>0.23</cell><cell>0.30</cell><cell>0.44</cell><cell>2.07</cell></row><row><cell>SWF0</cell><cell>0.45</cell><cell>1.18 0.40</cell><cell>3.47</cell><cell>0.51</cell><cell>0.18</cell><cell>0.24</cell><cell>0.35</cell><cell>1.68</cell></row><row><cell>SWF.75</cell><cell>0.51</cell><cell>1.49 0.43</cell><cell>3.78</cell><cell>0.63</cell><cell>0.21</cell><cell>0.27</cell><cell>0.40</cell><cell>1.91</cell></row><row><cell>SWF1</cell><cell>0.52</cell><cell>1.50 0.43</cell><cell>3.76</cell><cell>0.64</cell><cell>0.23</cell><cell>0.30</cell><cell>0.44</cell><cell>2.07</cell></row><row><cell>SRQ</cell><cell>0.46</cell><cell>1.21 0.35</cell><cell>3.05</cell><cell>0.65</cell><cell>0.26</cell><cell>0.26</cell><cell>0.50</cell><cell>2.37</cell></row><row><cell>SWQ0</cell><cell>0.41</cell><cell>0.99 0.32</cell><cell>2.82</cell><cell>0.56</cell><cell>0.23</cell><cell>0.23</cell><cell>0.44</cell><cell>2.07</cell></row><row><cell>SWQ.75</cell><cell>0.47</cell><cell>1.27 0.36</cell><cell>3.14</cell><cell>0.67</cell><cell>0.24</cell><cell>0.24</cell><cell>0.47</cell><cell>2.22</cell></row><row><cell>SWQ1</cell><cell>0.47</cell><cell>1.28 0.36</cell><cell>3.13</cell><cell>0.68</cell><cell>0.26</cell><cell>0.26</cell><cell>0.50</cell><cell>2.37</cell></row><row><cell></cell><cell></cell><cell cols="5">Table 7: Results of MAVE on the AVE08 test set</cell><cell></cell><cell></cell></row><row><cell cols="9">model f-score f-gain prec p-gain recall qa-acc qa-perf sel-rate s-gain</cell></row><row><cell>CF</cell><cell>0.42</cell><cell>1.05 0.32</cell><cell>2.80</cell><cell>0.61</cell><cell>0.27</cell><cell>0.30</cell><cell>0.52</cell><cell>2.45</cell></row><row><cell>CQ</cell><cell>0.41</cell><cell>1.00 0.31</cell><cell>2.67</cell><cell>0.63</cell><cell>0.29</cell><cell>0.29</cell><cell>0.55</cell><cell>2.60</cell></row></table><note coords="14,90.00,366.13,423.00,8.64;14,90.00,378.09,387.21,8.64"><p>per question (or 5.8 ms per validation item) without support pool enhancement. The extra effort required for applying the prover to a validation item suitable for logical processing was an average 5.4 ms.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,104.35,744.55,386.51,6.91"><p>To be precise, the contribution of repeated answer-snippet pairs is the maximum score of any of these items; see Sect. 2.9</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,104.35,717.44,279.97,6.91"><p>Such an 'active validation approach' already proved useful for MAVE in the AVE 2007.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="4,104.35,727.17,408.65,6.91;4,90.00,736.38,165.82,7.17"><p>The chosen simplification function<ref type="bibr" coords="4,218.10,727.17,9.29,6.91" target="#b3">[4]</ref> drops accents, removes whitespace and eliminates insignificant words from the answers. For example, κ(im Jahr 2001) = 2001 = κ(2001).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="4,104.35,744.65,69.09,8.62;4,169.07,746.10,147.43,8.60"><p>Thus for all i, j ∈ I * q , if κ(a i ) = κ(a j ) and s i = s j , then i = j.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="7,104.35,747.26,300.23,6.91"><p>RAVE uses the same sources of background knowledge as the MAVE system described in<ref type="bibr" coords="7,393.29,747.26,8.46,6.91" target="#b1">[2]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="8,104.35,680.26,408.65,6.91;8,90.00,689.72,423.00,6.91;8,90.00,699.19,423.00,6.91;8,90.00,709.40,416.04,5.61;8,90.00,718.86,416.04,5.61;8,90.00,728.33,143.46,5.61"><p>The Weka Bagging learner was chosen for learning the classifier, using default settings (averaging over results of 10 decision trees computed by the Weka REPTree decision tree learner). It was wrapped in a Weka CostSensitiveClassifier configured for reweighting of training examples. The following Weka command line was used to generate the classifiers: weka.classifiers.meta.CostSensitiveClassifier -cost-matrix "[0.0 0.3; 1.0 0.0]" -S 1 -W weka.classifiers.meta.Bagging ---P 100 -S 1 -I 10 -W weka.classifiers.trees.REPTree ---M 2 -V 0.0010 -N 3 -S 1 -L -1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="8,100.86,735.79,412.14,8.43;8,90.00,746.77,327.24,6.91"><p><ref type="bibr" coords="8,100.86,735.79,2.99,5.18" target="#b7">8</ref> An answer was annotated YES if the answer is correct and supported, and NO otherwise. All answer candidates were produced by the MIRA answer extractor of IRSAW which appears representative of mainstream QA technology.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="9,104.35,718.35,408.65,7.17;9,90.00,728.07,223.17,6.91"><p>A term occurrence is a pair (t, i) where t is a term and i is the position of the occurrence in the passage. In RAVE, word senses and numerals (rather than words or word stems) are used as the terms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8" coords="9,104.35,737.80,408.65,6.91;9,90.00,747.00,155.81,8.34"><p>If different QA streams with different ML-based models are used, the same answer/snippet pair can be assigned different scores η i , η i since the streams may differ in reliability.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9" coords="11,104.35,731.36,408.65,6.91;11,90.00,740.82,423.00,6.91"><p>The following bugs have been fixed: An error in the test for non-informative answers to definition questions resulted in many correct answers to be dropped. The lexical overlap test takes compound decompositions into account, but the implementation did not</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10" coords="13,104.35,707.61,408.65,6.91"><p>The data set used for training the shallow-only classifier also includes all answer candidates supported by non-parseable snippets.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11" coords="13,104.35,717.34,408.65,6.91;13,90.00,726.80,423.00,6.91;13,90.00,736.27,345.64,6.91"><p>These validation times do not include the time needed for parsing, since parsing reduces to the analysis of the question in normal operation of RAVE. For the same reason, the time needed for support pool enhancement was also excluded. All processing times were measured by running RAVE in a single thread on an Athlon64X2 4800+ CPU with 2.4 GHz clock rate.</p></note>
		</body>
		<back>

			<div type="funding">
<div> <ref type="bibr" coords="1,100.86,745.74,2.99,5.18" target="#b0">1</ref> <p>Funding by the <rs type="funder">DFG (Deutsche Forschungsgemeinschaft)</rs> under contract <rs type="grantNumber">HE 2847/10-1</rs> (LogAnswer) is gratefully acknowledged.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2RcsmcF">
					<idno type="grant-number">HE 2847/10-1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="15,111.58,383.42,401.42,8.64;15,111.58,395.20,290.80,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,403.24,383.42,109.76,8.64;15,111.58,395.38,68.34,8.64">The LogAnswer project at QA@CLEF 2008</title>
		<author>
			<persName coords=""><forename type="first">Ulrich</forename><surname>Furbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Helbig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Pelzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,198.78,395.20,174.11,8.59">Working notes for the CLEF 2008 workshop</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,111.58,415.12,401.42,8.82;15,111.58,427.08,212.17,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,177.48,415.30,281.32,8.64">University of Hagen at QA@CLEF 2007: Answer validation exercise</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,479.61,415.12,33.39,8.59;15,111.58,427.08,140.54,8.59">Working Notes for the CLEF 2007 Workshop</title>
		<meeting><address><addrLine>Budapest</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,111.58,447.00,401.42,8.82;15,111.58,458.96,366.33,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,174.85,447.18,251.40,8.64">Towards logic-based question answering under time constraints</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,444.34,447.00,68.65,8.59;15,111.58,458.96,285.68,8.59">Proc. of the 2008 IAENG Int. Conf. on Artificial Intelligence and Applications (ICAIA-08)</title>
		<meeting>of the 2008 IAENG Int. Conf. on Artificial Intelligence and Applications (ICAIA-08)<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,111.58,479.06,401.42,8.64;15,111.58,490.84,396.97,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,350.43,479.06,162.57,8.64;15,111.58,491.02,245.82,8.64">Logical validation, answer merging and witness selection: A study in multi-stream question answering</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,376.11,490.84,57.20,8.59">Proc. RIAO-07</title>
		<meeting>RIAO-07<address><addrLine>Pittsburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,111.58,510.94,401.42,8.64;15,111.58,522.72,345.29,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,240.81,510.94,268.78,8.64">Exploring robustness enhancements for logic-based passage filtering</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Pelzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,122.37,522.72,72.76,8.59">Proc. of KES-2008</title>
		<title level="s" coord="15,202.45,522.90,140.52,8.64">Lecture Notes in Computer Science</title>
		<meeting>of KES-2008</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="15,111.58,542.65,401.42,8.82;15,111.58,554.78,100.74,8.64" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="15,185.58,542.65,219.66,8.59">Hybrid Disambiguation in Natural Language Analysis</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Der Andere Verlag</publisher>
			<pubPlace>Osnabrück, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,111.58,574.70,401.42,8.64;15,111.58,586.48,401.42,8.82;15,111.58,598.44,182.93,8.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,344.56,574.70,168.44,8.64;15,111.58,586.66,359.25,8.64">University of Hagen at QA@CLEF 2008: Efficient Question Answering with Question Decomposition and Multiple Answer Streams</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,489.02,586.48,23.98,8.59;15,111.58,598.44,153.44,8.59">Working notes for the CLEF 2008 workshop</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,111.58,618.36,401.42,8.82" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="15,182.74,618.36,262.17,8.59">Knowledge Representation and the Semantics of Natural Language</title>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Helbig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,111.58,636.29,401.42,10.81;15,111.58,650.24,401.42,8.82;15,111.58,662.20,168.33,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,389.52,638.47,123.48,8.64;15,111.58,650.42,101.23,8.64">Testing the reasoning for question answering validation</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,222.67,650.24,290.33,8.59;15,111.58,662.20,122.94,8.59">Journal of Logic and Computation, Special Issue on Natural Language and Knowledge Representation</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="15,111.58,682.12,401.42,8.82;15,111.58,694.26,103.25,8.64" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<title level="m" coord="15,244.97,682.12,263.78,8.59">Data Mining. Practical Machine Learning Tools and Techniques</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
