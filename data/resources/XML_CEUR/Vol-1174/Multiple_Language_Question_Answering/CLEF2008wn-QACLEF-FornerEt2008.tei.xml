<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,186.30,145.89,237.37,14.40;1,138.12,164.31,321.87,14.40;1,270.18,182.73,57.77,14.40">OVERVIEW OF THE CLEF 2008 MULTILINGUAL QUESTION ANSWERING TRACK</title>
				<funder ref="#_KUAhJKm">
					<orgName type="full">European Union (FEDER and FSE)</orgName>
				</funder>
				<funder ref="#_tSKRWwz">
					<orgName type="full">Portuguese Government</orgName>
				</funder>
				<funder ref="#_T3g5J47">
					<orgName type="full">Spanish Ministry of Education and Science</orgName>
				</funder>
				<funder ref="#_seJ9uqA">
					<orgName type="full">Spanish Ministry of Science and Technology</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,133.26,218.11,64.13,9.01"><forename type="first">Pamela</forename><surname>Forner</surname></persName>
							<email>forner@celct.it</email>
							<affiliation key="aff0">
								<orgName type="institution">CELCT</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.67,218.11,64.76,9.01"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
							<affiliation key="aff1">
								<orgName type="department">Departamento de Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.68,218.11,58.05,9.01"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
							<email>e.agirre@ehu.es</email>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Basque Country</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.98,218.11,56.84,9.01"><forename type="first">Iñaki</forename><surname>Alegria</surname></persName>
							<email>i.alegria@ehu.es</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Basque Country</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,410.10,218.11,29.98,9.01;1,133.26,229.63,34.49,9.01"><forename type="first">Corina</forename><surname>Forăscu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">UAIC and RACAI</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,175.08,229.63,67.57,9.01"><forename type="first">Nicolas</forename><surname>Moreau</surname></persName>
							<email>moreau@elda.org</email>
							<affiliation key="aff5">
								<orgName type="institution">ELDA/ELRA</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,250.82,229.63,62.98,9.01"><forename type="first">Petya</forename><surname>Osenova</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">BTB</orgName>
								<address>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.14,229.63,87.47,9.01"><forename type="first">Prokopis</forename><surname>Prokopidis</surname></persName>
							<email>prokopis@ilsp.gr</email>
							<affiliation key="aff7">
								<orgName type="department">Athena Research Center</orgName>
								<orgName type="institution">ILSP Greece</orgName>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="department">Linguateca</orgName>
								<orgName type="institution">DEI UC</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,417.83,229.63,42.57,9.01;1,133.26,241.15,15.03,9.01"><forename type="first">Paulo</forename><surname>Ro- Cha</surname></persName>
							<email>paulo.rocha@di.uminho.pt</email>
						</author>
						<author>
							<persName coords="1,156.54,241.15,78.66,9.01"><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
							<email>bogdan@dfki.de</email>
							<affiliation key="aff9">
								<orgName type="institution">DFKI</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.66,241.15,73.12,9.01"><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
							<email>richard.sutcliffe@ul.ie</email>
							<affiliation key="aff10">
								<orgName type="department">DLTG</orgName>
								<orgName type="institution">University of Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.86,241.15,47.48,9.01"><forename type="first">Erik</forename><surname>Tjong</surname></persName>
						</author>
						<author>
							<persName coords="1,399.88,241.15,42.46,9.01"><forename type="first">Kim</forename><surname>Sang</surname></persName>
							<affiliation key="aff11">
								<orgName type="institution">University of Groningen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,186.30,145.89,237.37,14.40;1,138.12,164.31,321.87,14.40;1,270.18,182.73,57.77,14.40">OVERVIEW OF THE CLEF 2008 MULTILINGUAL QUESTION ANSWERING TRACK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5024B0A53437A9B6A95DF079D98DED99</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The QA campaign at CLEF [1], was manly the same as that proposed last year. The results and the analyses reported by last year's participants suggested that the changes introduced in the previous campaign had led to a drop in systems' performance. So for this year's competition it has been decided to practically replicate last year's exercise. Following last year's experience some QA pairs were grouped in clusters. Every cluster was characterized by a topic (not given to participants). The questions from a cluster contained co-references between one of them and the others. Moreover, as last year, the systems were given the possibility to search for answers in Wikipedia 1 as document corpus beside the usual newswire collection. In addition to the main task, three additional exercises were offered, namely the Answer Validation Exercise (AVE), the Question Answering on Speech Transcriptions (QAST), which continued last year's successful pilot, and Word Sense Disambiguation for Question Answering (QA-WSD). As general remark, it must be said that the task still proved to be very challenging for participating systems. In comparison with last year's results the Best Overall Accuracy dropped significantly from 41,75% to 19% in the multi-lingual subtasks, while instead it increased a little in the monolingual sub-tasks, going from 54% to 63,5%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>QA@CLEF 2008 was carried out according to the spirit of the campaign, consolidated in previous years. Beside the classical main task, three additional exercises were proposed:</p><p>• the main task: several monolingual and cross-language sub-tasks, were offered: Bulgarian, English, French, German, Italian, Portuguese, Romanian, Greek, Basque and Spanish were proposed as both query and target languages.</p><p>• the Answer Validation Exercise (AVE) <ref type="bibr" coords="2,301.89,307.23,10.82,8.87">[2]</ref>: in its third round was aimed at evaluating answer validation systems based on textual entailment recognition. In this task, systems were required to emulate human assessment of QA responses and decide whether an Answer to a Question is correct or not according to a given Text. Results were evaluated against the QA human assessments.</p><p>• the Question Answering on Speech Transcripts (QAST) <ref type="bibr" coords="2,375.22,367.41,11.56,8.87">[3,</ref><ref type="bibr" coords="2,386.78,367.41,11.56,8.87" target="#b10">14]</ref>: which continued last year's successful pilot task, aimed at providing a framework in which QA systems could be evaluated when the answers to factual and definition questions must be extracted from spontaneous speech transcriptions.</p><p>• the Word Sense Disambiguation for Question Answering (QA-WSD) [4], a pilot task which provided the questions and collections with already disambiguated Word Senses in order to study their contribution to QA performance.</p><p>As far as the main task is concerned, following last year experience, the exercise consisted of topic-related questions, i.e. clusters of questions which were related to the same topic and contained co-references between one question and the others. The requirement for questions related to a topic necessarily implies that the questions refer to common concepts and entities within the domain in question. This is accomplished either by co-reference or by anaphoric reference to the topic, implicit or explicitly expressed in the first question or in its answer.</p><p>Moreover, besides the usual news collections provided by ELRA/ELDA, articles from Wikipedia were considered as an answer source. Some questions could have answers only in one collection, i.e. either only in the news corpus or in Wikipedia.</p><p>As a general remark, this year we had the same number of participants as in 2007 campaign, but the number of submissions went up. Due to the complexity of the innovation introduced in 2007 -the introduction of topics and anaphora, list questions, Wikipedia corpus -the questions tended to get a lot more difficult and the performance of systems dropped dramatically, so, people were disinclined to continue the following year (i.e. 2008), inverting the positive trend in participation registered in the previous campaigns.</p><p>As reflected in the results, the task proved to be even more difficult than expected. Results improved in the monolingual subtasks but are still very low in the cross-lingual subtasks.</p><p>This paper describes the preparation process and presents the results of the QA track at CLEF 2008. In section 2, the tasks of the track are described in detail. The results are reported in section 3. In section 4, some final analysis about this campaign is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>As far as the main task is concerned, the consolidated procedure was followed, capitalizing on the experience of the task proposed in 2007.</p><p>The exercise consisted of topic-related questions, i.e. clusters of questions which were related to the same topic and contained co-references between one question and the others. Neither the question types (F, D, L) nor the topics were given to the participants.</p><p>The systems were fed with a set of 200 questions -which could concern facts or events (F-actoid questions), definitions of people, things or organisations (Definition questions), or lists of people, objects or data (L-ist questions)-and were asked to return up to three exact answers per question, where exact meant that neither more nor less than the information required was given.</p><p>The answer needed to be supported by the docid of the document in which the exact answer was found, and by portion(s) of text, which provided enough context to support the correctness of the exact answer. Supporting texts could be taken from different sections of the relevant documents, and could sum up to a maximum of 700 bytes. There were no particular restrictions on the length of an answer-string, but unnecessary pieces of information were penalized, since the answer was marked as ineXact. As in previous years, the exact answer could be exactly copied and pasted from the document, even if it was grammatically incorrect (e.g.: inflectional case did not match the one required by the question). Anyway, systems were also allowed to use natural language generation in order to correct morpho-syntactical inconsistencies (e.g., in German, changing dem Presidenten into der President if the question implies that the answer is in nominative case), and to introduce grammatical and lexical changes (e.g., QUESTION: What nationality is X? TEXT: X is from the Netherlands EXACT ANSWER: Dutch).</p><p>The subtasks were both:</p><p>• monolingual, where the language of the question (Source language) and the language of the news collection (Target language) were the same;</p><p>• cross-lingual, where the questions were formulated in a language different from that of the news collection.</p><p>Two new languages have been added, i.e. Basque and Greek both as source and target languages. In total eleven source languages were considered, namely, Basque, Bulgarian, Dutch, English, French, German, Greek, Italian, Portuguese, Romanian and Spanish. All these languages were also considered as target languages. As shown in Table <ref type="table" coords="4,220.08,594.63,3.74,8.87" target="#tab_0">1</ref>, 43 tasks were proposed:</p><p>• 10 Monolingual -i.e. Bulgarian (BG), German (DE), Greek (EL), Spanish (ES), Basque (EU), French (FR), Italian (IT), Dutch (NL), Portuguese (PT) and Romanian (RO); • 33 Cross-lingual (as customary in recent campaigns, in order to prepare the cross-language subtasks, for which at least one participant had regis-tered, some target language question sets were translated into the combined source languages).</p><p>Anyway, as Table <ref type="table" coords="5,221.40,173.97,5.01,8.87" target="#tab_1">2</ref> shows, not all the proposed tasks were then carried out by the participants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLEF-2008 8 12</head><p>As long-established, the monolingual English (EN) task was not available as it seems to have been already thoroughly investigated in TREC campaigns. English was still both source and target language in the cross-language tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Questions Grouped by Topic</head><p>The procedure followed to prepare the test set was the same as that used in the 2007 campaign. First of all, each organizing group, responsible for a target language, freely chose a number of topics. For each topic, one to four questions were generated. Topics could be not only named entities or events, but also other categories such as objects, natural phenomena, etc. (e.g. George W. Bush; Olympic Games; notebooks; hurricanes; etc.). The set of ordered questions were related to the topic as follows:</p><p>• the topic was named either in the first question or in the first answer • the following questions could contain co-references to the topic expressed in the first question/answer pair.</p><p>Topics were not given in the test set, but could be inferred from the first question/answer pair. For example, if the topic was George W. Bush, the cluster of questions related to it could have been:</p><p>Q1: Who is George W. Bush?; Q2: When was he born?; Q3: Who is his wife?</p><p>The requirement for questions related to a same topic necessarily implies that the questions refer to common concepts and entities within the domain. The most common form is pronominal anaphoric reference to the topic declared in the first question, e.g.: Q4: What is a polygraph?; Q5: When was it invented? However, other forms of co-reference occurred in the questions. Here is an example: Q6: Who wrote the song "Dancing Queen"?; Q7: How many people were in the group?</p><p>Here the group refers to an entity expressed not in the question but only in the answer. However the QA system does not know this and has to infer it, a task which can be very complex, especially if the topic is not provided in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document collections</head><p>Beside the data collections composed of news articles provided by ELRA/ELDA (see Table <ref type="table" coords="6,232.38,324.75,3.62,8.87" target="#tab_3">3</ref>), also Wikipedia was considered.</p><p>The Wikipedia pages in the target languages, as found in the version of November 2006, could be used. Romanian had Wikipedia<ref type="foot" coords="6,351.72,346.59,3.24,5.73" target="#foot_1">2</ref> as the only document collection, because there was no newswire Romanian corpus. The "snapshots" of Wikipedia were made available for download both in XML and HTML versions. The answers to the questions had to be taken from actual entries or articles of Wikipedia pages. Other types of data such as images, discussions, categories, templates, revision histories, as well as any files with user information and metainformation pages, had to be excluded.</p><p>One of the major reasons for using Wikipedia was to make a first step towards web formatted corpora where to search for answers. In fact, as nowadays so large information sources are available on the web, this may be considered a desirable next level in the evolution of QA systems. An important advantage of Wikipedia is that it is freely available for all languages so far considered. Anyway the variation in size of Wikipedia, depending on the language, is still problematic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Types of Questions</head><p>As far as the question types are concerned, as in previous campaigns, the three following categories were considered:</p><p>1. Factoid questions, fact-based questions, asking for the name of a person, a location, the extent of something, the day on which something happened, etc. We consider the following 8 answer types for factoids:</p><p>-PERSON, e.g.: Q8: Who was called the "Iron-Chancellor"? A8: Otto von Bismarck.</p><p>-TIME, e.g.  As only one answer was allowed, all the items had to be present in sequence in the document and copied, one next to the other, in the answer slot.</p><p>Besides, all types of questions could contain a temporal restriction, i.e. a temporal specification that provided important information for the retrieval of the correct answer, for example: Q21: Who was the Chancellor of Germany from 1974 to 1982? A21: Helmut Schmidt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q22: Which book was published by George Orwell in 1945?</head><p>A22: Animal Farm.</p><p>Q23: Which organization did Shimon Perez chair after Isaac Rabin's death? A23: Labour Party Central Committee. Some questions could have no answer in the document collection, and in that case the exact answer was "NIL" and the answer and support docid fields were left empty. A question was assumed to have no right answer when neither human assessors nor participating systems could find one. The distribution of the questions among these categories is described in Table <ref type="table" coords="8,454.63,598.17,3.74,8.87" target="#tab_4">4</ref>. Each question set was then translated into English, which worked as interlanguage during the translation of the datasets into the other tongues for the activated cross-lingual subtasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Formats</head><p>As the format is concerned, also this year both input and output files were formatted as an XML file. For example, the first four questions in the EN-FR test set, i.e. English questions that hit a French document collection -were represented as follows: &lt;input&gt; &lt;q target_lang="FR" source_lang="EN" q_id="0001" q_group_id="1600"&gt;Which is the largest bird in Africa?&lt;/q&gt; &lt;q target_lang="FR" source_lang="EN" q_id="0002" q_group_id="1600"&gt;How many species of ostriches are there?&lt;/q&gt; &lt;q target_lang="FR" source_lang="EN" q_id="0003" q_group_id="1601"&gt;Who served as a UNICEF goodwill ambassador between 1988 and 1992?&lt;/q&gt; &lt;q target_lang="FR" source_lang="EN" q_id="0004" q_group_id="1601"&gt;What languages did she speak?&lt;/q&gt; ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;/input&gt;</head><p>An example of system output which answered the above questions was the following: </p><formula xml:id="formula_0" coords="9,133.26,645.25,23.98,6.85">&lt;?xml</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Evaluation</head><p>As far the evaluation process is concerned, no changes were made with respect to the previous campaigns. Human judges assessed the exact answer (i.e. the shortest string of words which is supposed to provide the exact amount of information to answer the question) as:</p><p>• R (Right) if correct;</p><p>• W (Wrong) if incorrect;</p><p>• X (ineXact) if contained less or more information than that required by the query; • U (Unsupported) if either the docid was missing or wrong, or the supporting snippet did not contain the exact answer.</p><p>Most assessor-groups managed to guarantee a second judgement of all the runs. As regards the evaluation measures, the main one was accuracy, defined as the average of SCORE(q) over all 200 questions q, where SCORE(q) is 1 in the first answer to q in the submission file is assessed as R, and 0 otherwise. In addition most assessor groups computed the following measures:</p><p>• Confident Weighted Score (CWS). Answers are in a decreasing order of confidence and CWS rewards systems that give correct answers at the top of the ranking <ref type="bibr" coords="11,227.91,335.55,16.70,8.87" target="#b12">[16]</ref> • the Mean Reciprocal Rank (MRR) over N assessed answers per question (to consider the three answers). That is, the mean of the reciprocal of the rank of the first correct label over all questions. If the first correct label is ranked as the 3rd label, then the reciprocal rank (RR) is 1/3. If none of the first N responses contains a correct label, RR is 0. RR is 1 if the highest ranked label matches the correct label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>As far as accuracy is concerned, scores were generally far lower than usual, as Figure <ref type="figure" coords="11,163.27,489.15,5.01,8.87">1</ref> shows. Although comparison between different languages and years is not possible, in Figure <ref type="figure" coords="11,232.02,501.15,5.01,8.87">1</ref> we can observe some trends which characterized this year's competition: best accuracy in the monolingual task increased with respect to last year, going up again to the values recorded in 2006. But systems -even those that participated in all previous campaigns -did not achieve a brilliant overall performance. Apparently systems could not manage suitably the new challenges, although they improved their performances when tackling issues already treated in previous campaigns.</p><p>More in detail, best accuracy in the monolingual task scored 63,5 almost ten points up with respect to last year, meanwhile the overall performance of the systems was quite low, as average accuracy was 23,63, practically the same as last year. On the contrary, the performances in the cross-language tasks recorded a drastic drop: best accuracy reached only 19% compared to 41,75% in the previous year, which means more than 20 points lower, meanwhile average accuracy was more or less the same as in 2007 -13,24 compared to 10,9.  On the contrary, Best accuracy over the bilingual tasks, decreased considerably. This is also true for average performances. This year a small increase was recorded in the bilingual tasks but it seems that the high level of difficulty of the question sets particularly impacted the bilingual tasks and the task proved to be still difficult also for veterans. The number of participants has remained almost the same as in 2007 (see Table <ref type="table" coords="13,133.26,156.99,3.61,8.87" target="#tab_7">5</ref>). As noticed, this is probably the consequence of the new challenges introduced last year in the exercise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Participation</head><p>Also the geographical distribution remained almost unchanged, even though there was no participation from Australia and Asia. No runs were submitted neither for Italian or Greek tasks. Anyway, the number of submitted runs, increased from a total of 37 registered last year to 51 (see Table <ref type="table" coords="13,226.69,228.99,3.61,8.87" target="#tab_8">6</ref>). The breakdown of participants and runs, according to language, is shown in Table <ref type="table" coords="13,249.80,240.99,5.01,8.87" target="#tab_4">4</ref> (Section 2.3). As in previous campaigns, more participants chose the monolingual tasks, which once again demonstrated to be more approachable. In the following subsections a more detailed analysis of the results in each language follows, giving specific information on the performances of the participating systems in the single sub-tasks and on the different types of questions, providing the relevant statistics and comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Basque as target</head><p>In the first year working with Basque as target only a research groups submitted runs for evaluation in the track having Basque as target language, the Ixa group from the University of the Basque Country. They sent four runs: one monolingual, one English-Basque and two Spanish-Basque.</p><p>The Basque question set consisted of 145 factoid questions, 39 definition questions and 16 list questions. 39 questions contained a temporal restriction, and 10 had no answer in the Gold Standard. 40 answers were retrieved from Wikipedia, the remains from the news collections. Half of the questions were linked to a topic, so the second (and sometimes the 3rd) question was more difficult to answer.</p><p>The news were from the Egunkaria newspaper during 2000, 2001 and 2002 years and the information from Wikipedia was the exportation corresponding to the 2006 year.</p><p>Table <ref type="table" coords="14,167.95,180.99,5.01,8.87" target="#tab_9">7</ref> shows the evaluation results for the four submitted runs (one monolingual and three cross-lingual). The table shows the number of Right, Wrong, in-eXact and Unsupported answers, as well as the percentage of correctly answered Factoids, Temporally restricted questions, Definition and List questions. The monolingual run (ixag081eueu.xml) achieved accuracy of 13%, lower than the most systems for other target languages during the evaluation of 2007 but better than some of them. It is necessary to underline that Basque is a highly flexional language, doing matching of term and entities more complex, and that ir is the first participation. The system achieved better accuracy in factoids questions (15.9%). No correct answers was retrieved for list questions. It is necessary to remark that 57 answers were NIL (only four of them were corrects), perhaps participants can improve this aspect.</p><p>Looking to the cross-lingual runs the loss of accuracy respect to the monolingual system is a bit more than 50% for the two best runs. This percentage is quite similar with runs for other target languages in 2007. The overall accuracy is the same for both (English and Spanish to Basque) but only they agree in five correct answers (each system gives other six correct answers). The second system for Spanish-Basque get poorer results and only is slightly better in inexact answers. These runs get also a lot of NIL answers. This year, contrary to our optimistic expectations, only one run by one group (BTB) was performed for Bulgarian. As the table above shows, the result is far from satisfying. Again, the definitions were detected better in comparison to other question types. Also, the difference between the detection of factoids and of temporally restricted questions is negligible. The results from the previous years decreased in both directions -as participating groups and as system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bulgarian as Target</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dutch as Target</head><p>The questions for the Dutch subtask of CLEF-QA 2008 were written by four native speakers. They selected random articles from either Wikipedia or the news collection and composed questions based on the topics of the articles. The quartet produced a total of 222 question-answer pairs from which they selected a set of 200 that satisfied the type distribution requirements of the task organizers. An overview of the question types and answer types can be found in Table <ref type="table" coords="16,145.15,180.99,3.74,8.87" target="#tab_11">9</ref>.</p><p>This year, only one team took part in the question answering task with Dutch as target language: the University of Groningen. The team submitted two monolingual runs and two cross-lingual runs (English to Dutch). All runs were assessed twice by a single assessor. This resulted in a total of eight conflicts (1%). These were corrected. The results of the assessment can be found in Table <ref type="table" coords="16,403.30,240.99,8.35,8.87" target="#tab_12">10</ref>. The two cross-lingual runs gron081ennl andron082ennl produced exactly the same answers.</p><p>The best monolingual run (gron082nlnl) achieved exactly the same score as the best run of 2007 (25.5%). The same is true for the best monolingual run (13.5%). The fact that the two scores are in the same range as last year is no big surprise since the task has not changed considerably this year and all scores have been achieved by the same system.</p><p>Like in 2007, the system performed better for definition questions than for other question types. The definition questions could be divided in two subtypes: those that asked for a definition (26) and those that contained a definition and asked for the name of the defined object <ref type="bibr" coords="16,255.00,540.33,15.34,8.87" target="#b8">(12)</ref>. The monolingual runs performed similarly for both subtypes but the cross-lingual runs did not contain a correct answer to any question of the second subtype.</p><p>None of the runs obtained any points for the list questions. The answers contained some parts that were correct but none of them were completely correct. We were unable to award points for partially correct answers in the current assessment scheme.</p><p>All the runs were produced by the same system and the differences between the runs are small. The cross-lingual runs contained seven correct answers that were not present in any of the monolingual runs (for questions 20, 25, 120, 131, 142, 150 and 200). Eight questions were only answered correctly in a single monolingual run <ref type="bibr" coords="17,171.65,156.99,155.71,8.87">(1, 28, 54, 72, 83, 143, 193 and 199)</ref>. Thirty-five questions were answered correctly in two runs, three in three runs and seventeen in all four runs. 137 questions failed to receive any correct answer. Creation of Questions. The task this year was exactly the same as in 2007 and moreover the three collections were the same: Glasgow Herald, LA Times and Wikipedia. However, given the considerable interest in the Wikipedia which has been shown by Question Answering groups generally, it was decided to increase the number of questions drawn from it to 75% overall, with just 25% coming from the two newspaper collections. This means that 40 of the 160 Factoids came from the newspapers, together with seven of the 30 Definitions and two of the ten Lists. These questions were divided equally between the Glasgow Herald and LA Times. All the remainder we drawn from the Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">English as Target</head><p>Considerable care was taken in the selection of the questions. The distribution by answer type was controlled exactly as in previous years. As requested by the organisers there were exactly twenty each of Factoid target type PERSON, TIME, LOCATION, MEASURE, COUNT, ORGANIZATION, OBJECT and OTHER. Similarly for Definitions there were eight PERSON, seven ORGANIZATION, seven OBJECT and eight OTHER. For Lists there were four OTHER, two each of PERSON and ORGANIZATION, and one each of LOCATION and OBJECT.</p><p>In addition to the above distribution, we also controlled the distribution of topics for the question groups, something which was made practicable by the use of the Wikipedia. Questions were drawn from a number of predefined subject fields: countries towns, roads and bridges, shops, politicians and politics, sports and sports people, foods and vegetables, cars, classical music including instruments, popular music, literature poetry and drama, philosophy, films, architecture, languages, science, consumer goods, and finally organisations. Questions were distributed among these topics. The maximum in any topic was twenty (sports) and the minimum was two (shops). For the majority there were between four and six question groups. For each such topic, one or more questions were set depending on what information the texts contained. As a change from last year, the organisers asked us to include 100 singleton topics. This effectively meant that half the questions in the overall set of 200 were simple "one-off" queries as were set in CLEF prior to 2007 and for the earlier TREC campaigns.</p><p>Questions were entered via a web interface developed by the organisers last year. However, this year they improved it considerably, for example allowing modifications to be made to existing entries. This was a great help and a commendable effort on their part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary Statistics.</head><p>Five cross-lingual runs with English as target were submitted this year, as compared with eight in 2007 and thirteen in 2006. Four groups participated in three languages, Dutch, German and Romanian. Each group worked with only one source language, and only DCUN submitted two runs. The rest submitted only one run.</p><p>Assessment Procedure. Last year we used the excellent Web-based assessment system developed originally for the QiQA task by University of Amsterdam. However, we were asked not to use this in 2008 because it only allows one answer per question per system to be assessed and it was required to assess multiple answers per question per system. For this reason we used a Web-based tool developed by UNED in Madrid.</p><p>All answers were double-judged. Where the assessors differed, the case was reviewed and a decision taken. There were 63 judgement differences in total. Three of the runs contained multiple answers to individual questions in certain cases, and these were all assessed, as per the requirement of the organisers. If we assume that the number of judgements was in fact 200 questions * five runs, i.e. 1,000, we can compute a lower bound for the agreement level. This gives a figure of (1,000-63)/1,000, i.e. 93.7%. The equivalent figure for 2007 (called Agreement Level 2 in the Working Notes for last year) was 97.6%. Given that we have computed a lower bound this year (and not therefore the exact figure) this seems acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results Analysis.</head><p>Of the five runs with English as target, wlvs081roen was the best with an accuracy of 19.00% overall. They also did very will on the definitions, scoring 66.67%. The only source language for which there was more than one run was German, for which there were three submissions from two groups. dfki081 scored the best with 14.00% and this was followed by dcun081deen with 8.00% and dcun082deen with 0.50%. dfki also did very well on definitions with an accuracy of 60.00. Interestingly, none of the systems answered any of the list questions correctly. Only dcun082deen answered one list question inexactly.</p><p>If we compare the results this year with those of last year when the task was very similar, performance has improved here. The best score in 2007 was wolv071roen with 14.00% (the best score) which has now improved to 19.00%. Similarly, dfki071deen scored 7.00% in 2007 but increased this to 14.00% this year in dfki081deen. An attempt was made to set easier questions this year, which might have affected performance. In addition, many more questions came from the Wikipedia in 2008 with only a minority being drawn from the newspaper corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">QA-WSD subtask</head><p>The QA-WSD task brings semantic and retrieval evaluation together. The participants were offered the same queries and document collections as for the main QA exercise, but with the addition of word sense tags as provided by two automatic word sense disambiguation (WSD) systems. Contrary to the main QA task, Wikipedia articles are not included, and thus systems need to reply to the questions that have an answer in the news document collection. The goal of the task is to test whether WSD can be used beneficially for Question Answering, and is closely related to the Robust-WSD subtask of the ad-hoc track in CLEF 2008.</p><p>The exercise scenario is event-targeted QA on a news document collection. In the QA-WSD track only English monolingual and Spanish to English bilingual tasks are offered, i.e. English is the only target language, and queries are available on both English and Spanish. The queries were the same as for the main QA exercise, and the participation followed the same process, except for the use of the senseannotated data.</p><p>The goal of this task is to evaluate whether word sense information can help in certain queries. For this reason, participants were required to send two runs for each of the monolingual/bilingual tasks where they participate: one which does not use sense annotations and another one which does use sense annotations. Whenever possible, the only difference between the two runs should be solely the use or not of the sense information. Participants which send a single run would be discarded from the evaluation.</p><p>The WSD data is based on WordNet version 1.6 and was supplemented with freely available data from the English and Spanish WordNets in order to test different expansion strategies. Two leading WSD experts run their systems <ref type="bibr" coords="19,410.88,660.75,15.90,8.87" target="#b13">[17]</ref>[18], and provided those WSD results for the participants to use.</p><p>The task website [4] provides additional information on data formats and resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>From the 200 questions provided to participants, only 49 queries had a correct answer in the news collection. The table below provides the results for the participant on those 49 questions. The first run does not use WSD, while the second uses the sense tags returned by the NUS WSD system. The WSD tags where used in the passage retrieval module. The use of WSD does not provide any improvement, and causes one more error. For the sake of completeness we also include below the results on all 200 queries. Surprisingly the participant managed to find two (one in the WSD run) correct answer for the Wikipedia questions in the news collection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">French as Target</head><p>This year only one group took part in the evaluation tasks using French as a target language: the French group Synapse Développement. Last year's second participant, the Language Computer Corporation (LCC, USA) didn't send any submission this time.</p><p>Synapse submitted three runs in total:</p><p>• one monolingual run: French to French (FR-to-FR),</p><p>• two bilingual runs: English-to-French (EN-to-FR) and Portuguese-to-French (PT-to-FR).</p><p>In the following, these will be referred to as:</p><p>• syn08frfr (for FR-to-FR), • syn08enfr (for EN-to-FR),</p><p>• syn08ptfr (for PT-to-FR).</p><p>As last year, three types of questions were proposed: factual, definition and closed list questions. Participants could return one exact answer per question and up to two runs. Some questions (10%) had no answer in the document collection, and in this case the exact answer is "NIL".</p><p>The French test set consists of 200 questions:</p><p>• 135 Factual (F),</p><p>• 30 Definition (D),</p><p>• 35 closed List questions (L).</p><p>Among these 200 questions, 66 were temporally restricted questions (T) and 12 were NIL questions (i.e. a "NIL" answer was expected, meaning that there is no valid answer for this question in the document collection). Table <ref type="table" coords="22,168.46,144.99,10.01,8.87" target="#tab_16">14</ref> shows the final results of the assessment of the 3 runs submitted by Synapse. For each run, the following statistics are provided:</p><p>• The number of correct (R), wrong (W), inexact (X) and unsupported answers (U), • The accuracy calculated within each of the categories of questions: F, D, T and L questions, • The number of NIL answers and the proportion of correct ones (i.e.</p><p>corresponding to a NIL questions), • The Confidence Weighted Score (CWS) measure.</p><p>• The accuracy calculated over all answers.</p><p>Figure <ref type="figure" coords="22,172.11,276.63,5.01,8.87" target="#fig_1">2</ref> shows the best scores for systems using French as target in the last five CLEF QA campaigns. For the monolingual task, the Synapse system returned 113 correct answers (accuracy of 56.5%), slightly more than last year (accuracy of 54.0%). The bilingual runs performance is quite low, with an accuracy of 18.0% for EN-to-FR and 16.5% for PT-to-FR. It cannot be fairly compared to the results of CLEF2007, because Synapse didn't submit bilingual runs last year. Last year, LCC obtained an accuracy of 41.7% for EN-to-FR, but did not submit anything this year.</p><p>It appears that the level of performance strongly depends on the type of questions. The monolingual run scores very high on the definition questions (86.7%). The lowest performance is obtained with closed list questions (37.1%).</p><p>It is even more obvious when looking at the bilingual runs. If the systems performed pretty well on the definition questions (50.0% and 43.3% for EN-to-FR and PT-to-FR respectively), they could not cope with the closed list questions. The PT-to-FR system could only give one close list correct answer. The EN-to-FR system could not even answer to any of these questions. The bilingual runs did not reach high accuracy with factoid and temporally restricted questions (50.0% and 43.3% for EN-to-FR and PT-to-FR respectively). This year, the complexity of the task, in particular regarding closed list questions, seems to have been hard to cope with for the bilingual systems.</p><p>The complexity of the task is also reflected by the number of NIL answers. The monolingual system returned 20 NIL answers (to be compared with the 12 expected). The bilingual systems returned 60 (EN-to-FR) and 67 (EN-to-FR) NIL answers, i.e. at least 5 times more as expected.</p><p>It is also interesting to look at the results when categorizing questions by the size of the topic they belong to. This year, topics could contain from 1 single question to 4 questions. The CLEF 2008 set consists of:</p><p>• 52 single question topics,</p><p>• 33 topics with 2 questions (66 questions in total),</p><p>• 18 topics with 3 questions (54 questions in total),</p><p>• 7 topics with 4 questions (28 questions in total).</p><p>Table <ref type="table" coords="23,171.29,385.89,8.35,8.87" target="#tab_17">15</ref>, Table <ref type="table" coords="23,213.13,385.89,10.03,8.87" target="#tab_18">16</ref> and Table <ref type="table" coords="23,270.25,385.89,10.03,8.87" target="#tab_19">17</ref> give the results of each run according to the size of the topics. The monolingual system (Table <ref type="table" coords="24,279.43,273.69,8.88,8.87" target="#tab_17">15</ref>) is not sensitive to the size of the topic question set. On the opposite, the performances of the bilingual systems (Table <ref type="table" coords="24,452.07,285.69,9.99,8.87" target="#tab_18">16</ref> and Table <ref type="table" coords="24,172.60,297.69,8.90,8.87" target="#tab_19">17</ref>) decrease by a half, when comparing the 1-and 2-question sets to the 3-and 4-question sets. A possible explanation is that the bilingual systems perform poorly with questions containing anaphoric references (which are more likely to occur in the 3-and 4-question sets).</p><p>In conclusion, there was unfortunately only one participant this year. In particular; it would have been interesting to see how the LCC group, which submitted a bilingual run last year, would have performed this year. This decrease in participation can be explained by the discouragement of some participants. Some have complained that the task is each year harder (e.g. this year, there were more closed list questions and anaphoric references than last year) that can result in a decrease in the systems performances. This year, the number and complexity of closed list questions was clearly higher than the previous year. In the same way, there were more temporally restricted questions, more topics (comprising from 2 to 4 questions) and more anaphoric references. It seems that this higher level of difficulty particularly impacted the bilingual tasks. In spite of this, the monolingual Synapse system performed slightly better than last year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">German as Target</head><p>Three research groups submitted runs for evaluation in the track having German as target language: The German Research Center for Artificial Intelligence (DFKI), the Fern Universität Hagen (FUHA) and the Universität Koblenz-Landau (LOGA). All groups provided system runs for the monolingual scenario, DFKI and FUHA submitted runs for the cross-language English-German scenario and FUHA had also runs for the Spanish-German scenario. Compared to the previous editions of the evaluation forum, this year an increase in the accuracy of the best performing system and of an aggregated virtual system for monolingual and a decrease in the accuracy of the best performing system and of an aggregated virtual system for cross-language tasks was registered. The number of topics covered by the test set questions was of 120 distributed as it follows: 74 topics consisting of 1 question, 24 topics of 2 related questions, 10 topics of 3 related questions, and 12 topics of 4 related questions. The distribution of the topics over the document collections (CLEF vs. Wikipedia) is presented in Table <ref type="table" coords="26,155.68,190.41,8.34,8.87" target="#tab_20">18</ref>. The details of systems' results can be seen in Table <ref type="table" coords="26,338.05,376.35,8.34,8.87" target="#tab_23">21</ref>. According to Table <ref type="table" coords="27,214.64,144.45,10.04,8.87" target="#tab_21">19</ref> the most frequent topic types were OTHER (32), OBJECT (29) and ORGANIZATION (24), with first two types more present for the Wikipedia collection of documents (WIKI).</p><p>As regards the source of the answers, 97 questions from 57 topics asked for information out of the CLEF document collection and the rest of 103 from 63 topics for information from Wikipedia. Table <ref type="table" coords="27,310.76,201.93,10.03,8.87" target="#tab_22">20</ref> shows a breakdown of the test set questions by the expected answer type (EAType) for each collection of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Portuguese as Target</head><p>The Portuguese track had six different participants: beside the veteran groups of Priberam, Linguateca, Universidade de Évora, INESC and FEUP, we had a new participants this year, Universidade Aberta. No bilingual task occurred this year.</p><p>In this fourth year of Portuguese participation, Priberam repeated the top place of its previous years, with University of Évora behind. Again we added the classification the classification X-, meaning incomplete, keeping the classification X+ for answers with extra text or other kinds of inexactness. In Table <ref type="table" coords="27,407.83,357.27,10.02,8.87" target="#tab_24">22</ref> we present the overall results (all tables in these notes refer exclusively to the first answer by each system). To provide a more direct comparison with pre-2006 results, in Table <ref type="table" coords="27,439.18,610.89,9.99,8.87" target="#tab_26">23</ref> we present the results both for first question of each topic (which we believe is more readily comparable to such results) and for the linked questions.</p><p>On the whole, compared to last year, Priberam and Senso (UE) improved their results, which were already the best. INESC system and Esfinge (Linguateca) also showed some improvement, at a lower level Raposa (FEUP) showed similar re-  sults. The system of Universidade Aberta appeared with good results compared to some veteran systems. We leave it to the participants to comment on whether it might have been caused by harder questions or changes (or lack thereof) in the systems. Unlike last year , the results over linked questions are significatively different (and below) from those over not-linked. Question 180 was wrongly redacted, referring to Aida's opera Verdi instead of the other way around, which also affected two linked questions. Therefore, we accepted both NIL answers to those questions, as well as correct ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Table <ref type="table" coords="28,168.24,651.69,10.02,8.87" target="#tab_27">24</ref> shows the results for each answer type of definition questions, while Table <ref type="table" coords="28,155.71,663.69,10.02,8.87" target="#tab_28">25</ref> shows the results for each answer type of factoid questions (including list questions). As it can be seen, four out of six systems perform clearly better when it comes to definitions than to factoids. Particularly Senso has a high accuracy regarding definitions. We included in both Table <ref type="table" coords="29,254.46,377.49,10.04,8.87" target="#tab_27">24</ref> and Table <ref type="table" coords="29,309.29,377.49,9.99,8.87" target="#tab_28">25</ref> a virtual run, called combination, in which one question is considered correct if at least one participating system found a valid answer. The objective of this combination run is to show the potential achievement when combining the capacities of all the participants. The combination run can be considered, somehow, state-of-the-art in monolingual Portuguese question answering. All definition questions were answered by at least one system. The system with best results, Priberam, answered correctly 64.8% the questions with at least one correct answer. In all, 130 questions were answered by more than one system.</p><p>In Table <ref type="table" coords="30,166.52,144.99,8.35,8.87" target="#tab_29">26</ref>, we present some values concerning answer and snippet size.</p><p>Temporally restricted questions: Table <ref type="table" coords="30,311.33,168.99,10.01,8.87" target="#tab_31">27</ref> presents the results of the 17 temporally restricted questions. As in previous years, the effectiveness of the systems to answer those questions is visibly lower than for non-TRQ questions. List questions: ten questions were defined as list questions all closed list factoids with two to five each<ref type="foot" coords="30,229.80,395.87,3.00,5.31" target="#foot_2">3</ref> . The results haven't improved with UE getting two correct answers. Priberam three and all other system zero. There were however seven cases of incomplete answers (i.e.. answering some elements of the list only) although only two of them with than one element of the answer. Answer source: Table <ref type="table" coords="31,240.88,144.99,9.99,8.87" target="#tab_32">28</ref> presents the distribution of questions by source during their selection. The distribution of sources used by the different runs and their correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10">Romanian as Target</head><p>In the third year of Romanian participation in QA@CLEF, and the second one with Romanian addressed as a target language, the question generation was based on the collection of Wikipedia Romanian pages frozen in November 2006<ref type="foot" coords="31,442.26,262.59,3.24,5.73" target="#foot_3">4</ref> -the same corpus as in the previous edition <ref type="foot" coords="31,285.96,276.41,3.00,5.31" target="#foot_4">5</ref> .</p><p>Creation of Questions. The questions were generated starting from the corpus and based on the Guidelines for Question Generation<ref type="foot" coords="31,349.98,310.59,3.24,5.73" target="#foot_5">6</ref> , the Guidelines for Participants <ref type="foot" coords="31,154.38,322.59,3.24,5.73" target="#foot_6">7</ref> and the final decisions taken after email discussions between the organizers. The 200 questions are distributed according to Table <ref type="table" coords="31,354.87,336.75,8.34,8.87" target="#tab_33">29</ref>, where for each type of question and expected answer we indicate also the temporally restricted questions out of the total number of questions. Without counting the NIL questions, 100% of the questions has the answer in Wikipedia collection. As the Guidelines for Question Generation did not change since the previous edition, there were no major difficulties in creating the Romanian gold standard for the 2008 QA@CLEF. The working version of the GS was uploaded on the question generation interface developed at CELCT (Italy), by filling all the required fields.</p><p>For the topic-related questions (clusters of up to four questions, related to one same topic) we kept about the same number as in the previous edition: in 2007 we had 122 topics and now there are 119 topics. The percentage of topic-linked questions is illustrated in Table <ref type="table" coords="32,240.94,240.99,8.33,8.87" target="#tab_34">30</ref>, showing that 127 questions were grouped under 46 topics, hence 63.5% out of the total 200 questions were linked in topics with more than one question. In fact the questions contain not 127, but only 51 anaphoric elements of various types, so that 25.5% of the questions are linked through coreferential relations.</p><p>The personal, possessive or demonstrative pronouns were used in most of the cases to create anaphoric relations. The antecedents are mainly the focus of the previous question, or the previous answer. Few such questions require inference in order to be correctly answered. For example in order to correctly answer the F-Time question When was the first Esperanto dictionary for Romanian published? and then the L-Other Name all the grammatical cases of this artificial language., one needs to correctly link the anaphor "artificial language" to its antecedent which is "Esperanto" and not "Romanian" (also a language but not artificial); this is possible by establishing, based on a text snippet, that Esperanto is an artificial language. The 8 NIL questions, even though they seem somehow unnatural, were created by including questions about facts impossible from a human perception; for example the question In which year did Paul Kline publish his work about the natural phenomena called hail? has no answer in any of the articles about the psychologist. Another type of NIL questions are those based on inference -the question How many bicameral Parliaments are there in Cuba? is a NIL question because in all wiki articles one can find that Cuba has a unicameral parliament. Another type of NIL questions (with answer in English, but not in Romanian) we have created cannot be good items neither in a cross-lingual evaluation where the answers are to be find in any language, nor in an evaluation based on an open text collection such as the web. The question What is a micron? has no answer in the Romanian wiki articles from 2006, but it can have an answer in other Romanian webpages, and, moreover, in the English wiki articles it has more than a correct answer depending on the domain where the term is used (in the metric system or in vacuum engineering).</p><p>For the LIST type we created only questions whose answers are to be found in one same text section. The 2007 evaluation for Romanian showed that "open list" questions (with answers in various sections of an article or even in various articles) are difficult to handle, therefore we made the LIST questions easier.</p><p>Systems' analysis and evaluation. Like in the 2007 edition, this year two Romanian groups took part in the monolingual task with Romanian as a target language: the Faculty of Computer Science from the Al. I. Cuza University of Iasi (UAIC), and the Research Institute for Artificial Intelligence from the Romanian Academy (ICIA), Bucharest. Each group submitted two runs, the four systems having an average of 2.4 answers per question for ICIA, and 1.92 for UAIC. The 2008 general results are presented in Tables 31 below.</p><p>The statistics includes a system, named combined, obtained through the combination of the 4 participating RO-RO systems. Because at the evaluation time we observed that there are correct answers not only in the first position, but also on the second or the third, the combined system considers that an answer is R if there exists at least one R answer among all the answers returned by the four systems. If there is no R answer, the same strategy is applied to X, U and finally W answers. This "ideal" system permits to calculate the percentage of the questions (and their type), answered by at least one of the four systems in any of the maximum 3 answers returned for a question. All three systems crashed on the LIST questions. The best results were obtained by ICIA for DEFINITION questions, whereas UAIC performed best with the FACTOID questions. The combined system suggests that a joint system, developed by both groups, would improve substantially the general results for Romanian.</p><p>Using in a first stage the web interface for assessing the QA runs, developed at UNED in Spain, the assessment took into consideration one question with all its answers at the time, assuring that the same evaluation criteria are applied to all answers. The judgment of the answers was based on the same Guidelines as in 2007, therefore we kept the same criteria as in 2007, in order to assure consistency inside the Romanian language, which gives also the possibility to evaluate the systems in their evolution from one year to another. For example, one could easily see that the UAIC systems had most of the answers for the DEFINITION questions evaluated as ineXact, because the answers were judged as being "longer than the minimum amount of information required" and hence "unnecessary pieces of information were penalized". Since all the 2007 and 2008 answers were evaluated this way, we considered it is more important to have uniformly applied rules in-side one language than to change the evaluation in order to be consistent across languages. On the other hand the ICIA answers judged as ineXact are due to answers that are too long, snippets shortened as such as they do not contain the answer, or because the answer and the snippet has no connections. The evaluation was made more difficult because two of the submitted runs contain the answers in a totally arbitrary order, with topic-related questions having their answers in various parts of the submitted file. If in the first stage the UNED interface was of a great help, after the xml file was generated with all the evaluations, the corrections needed a thorough manual inspection. Anyway it was nice to find out that the answer to the question Which terrorist organization does Osama bin Laden belong to? is Pentagon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.11">Spanish as Target</head><p>The participation at the Spanish as Target subtask has decreased from 5 groups in 2007 to 4 groups this year. 6 runs were monolingual and 3 runs were crosslingual. Table <ref type="table" coords="35,160.07,202.86,9.99,9.02" target="#tab_36">32</ref> shows the summary of systems results with the number of Right (R), Wrong (W), Inexact (X) and Unsupported (U) answers. The table shows also the accuracy (in percentage) of factoids (F), factoids with temporal restriction (T), definitions (D) and list questions (L). Best values are marked in bold face. Table <ref type="table" coords="36,171.96,168.99,10.00,8.87" target="#tab_37">33</ref> shows that the first question of the topic group is answered much more easily than the rest of the questions which need to solve some references to previous questions and answers.</p><p>Regarding NIL questions, Table <ref type="table" coords="36,276.20,216.99,10.03,8.87" target="#tab_39">34</ref> shows the harmonic mean (F) of precision and recall for self-contained questions, linked questions and all questions, taking into account only the first answer. In most of the systems, NIL is not given as second or third candidate answer. The correlation coefficient r between the self-score and the correctness of the answers (shown in Table <ref type="table" coords="36,234.94,474.03,8.92,8.87" target="#tab_39">34</ref>) has been similar to the obtained last year, being not good enough yet, and explaining the low results in CWS and K1 <ref type="bibr" coords="36,390.86,486.03,11.64,8.87" target="#b1">[6]</ref> measures. Since a supporting snippet is requested in order to assess the correctness of the answer, we have evaluated the systems capability to extract the answer when the snippet contains it. The first column of Table <ref type="table" coords="37,326.20,168.99,10.03,8.87" target="#tab_40">35</ref> shows the percentage of cases where the correct answer was present in the snippet and correctly extracted. This information is very useful to diagnose if the lack of performance is due to the passage retrieval or to the answer extraction process. As shown in the table, the best systems are also better in the task of answer extraction. In general, all systems have improved their performance in Answer Extraction compared with previous editions.</p><p>With respect to the source of the answers, Table <ref type="table" coords="37,347.99,264.99,10.02,8.87" target="#tab_41">36</ref> shows that in this second year of using Wikipedia, this collection is now the main source of correct answers for most of the systems (with the exception of U. of Alicante). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This year we proposed the same evaluation setting as in 2007 campaign. In fact, last year the task was changed considerably and this affected the general level of results and also the level of participation in the QA task. This year participation increased slightly but the task proved to be still very difficult. Wikipedia increased its presence as a source of questions and answers. Following last year's conclusions Wikipedia seemed to be a good source for finding answers to simple factoid questions.</p><p>Moreover, the overall decrease in accuracy was probably due to linked questions. This fact confirms that topic resolution is a weak point for QA systems.</p><p>Only 5 out of 11 target languages had more than one different participating group. Thus from the evaluation methodology perspective, a comparison between systems working under similar circumstances cannot be accomplished and this impedes one of the major goals of campaigns such the QA@CLEF, i.e. the systems comparison which could determine an improvement in approaching QA problematic issues.</p><p>In six years of QA experimentation, a lot of resources and know-how have been accumulated, nevertheless systems do not show a brilliant overall performance, even those that have participated to most QA campaigns, and still seem not to manage suitably the different challenges proposed.</p><p>In conclusion, it is clear that a redefinition of the task should be thought in the next campaign. This new definition of the task should permit the evaluation and comparison of systems even working in different languages. The new setting should also take as reference a real user scenario, perhaps in a new document collection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="22,158.16,510.72,276.16,7.66;22,145.44,309.42,313.32,187.02"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Best scores for systems using French as target in CLEF QA campaigns</figDesc><graphic coords="22,145.44,309.42,313.32,187.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="25,252.06,323.16,94.10,7.66"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Results evolution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,146.64,251.34,299.33,322.84"><head>Table 1 .</head><label>1</label><figDesc>Tasks activated in 2008 (coloured cells)</figDesc><table coords="4,146.64,284.58,299.33,289.60"><row><cell></cell><cell cols="6">TARGET LANGUAGES (corpus and answers)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>BG</cell><cell>DE</cell><cell>EL</cell><cell>EN</cell><cell>ES</cell><cell>EU FR</cell><cell>IT</cell><cell>NL</cell><cell>PT</cell><cell>RO</cell></row><row><cell></cell><cell>BG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>DE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(questions)</cell><cell>EL EN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SOURCE LANGUAGES</cell><cell>ES EU FR IT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>NL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>RO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,133.26,209.10,311.85,80.92"><head>Table 2 .</head><label>2</label><figDesc>Tasks chosen by at least 1 participant in QA@CLEF campaigns</figDesc><table coords="5,133.26,229.56,311.85,60.46"><row><cell></cell><cell>MONOLINGUAL</cell><cell>CROSS-LINGUAL</cell></row><row><cell>CLEF-2004</cell><cell>6</cell><cell>13</cell></row><row><cell>CLEF-2005</cell><cell>8</cell><cell>15</cell></row><row><cell>CLEF-2006</cell><cell>7</cell><cell>17</cell></row><row><cell>CLEF-2007</cell><cell>7</cell><cell>11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,145.20,306.12,292.93,366.16"><head>Table 3 .</head><label>3</label><figDesc>Document collections used in QA@CLEF 2008</figDesc><table coords="7,145.20,326.58,292.93,345.70"><row><cell cols="2">TARGET LANG. COLLECTION</cell><cell>PERIOD</cell><cell>SIZE</cell></row><row><cell>[BG] Bulgarian</cell><cell>Sega</cell><cell>2002</cell><cell>120 MB (33,356 docs)</cell></row><row><cell></cell><cell>Standart</cell><cell>2002</cell><cell>93 MB (35,839 docs)</cell></row><row><cell></cell><cell>Novinar</cell><cell>2002</cell><cell></cell></row><row><cell>[DE] German</cell><cell>Frankfurter Rundschau</cell><cell>1994</cell><cell>320 MB (139,715 docs)</cell></row><row><cell></cell><cell>Der Spiegel</cell><cell>1994/1995</cell><cell>63 MB (13,979 docs)</cell></row><row><cell></cell><cell>German SDA</cell><cell>1994</cell><cell>144 MB (71,677 docs)</cell></row><row><cell></cell><cell>German SDA</cell><cell>1995</cell><cell>141 MB (69,438 docs)</cell></row><row><cell>[EL] Greek</cell><cell>The Southeast European Times</cell><cell>2002</cell><cell></cell></row><row><cell>[EN] English</cell><cell>Los Angeles Times</cell><cell>1994</cell><cell>425 MB (113,005 docs)</cell></row><row><cell></cell><cell>Glasgow Herald</cell><cell>1995</cell><cell>154 MB (56,472 docs)</cell></row><row><cell>[ES] Spanish</cell><cell>EFE</cell><cell>1994</cell><cell>509 MB (215,738 docs)</cell></row><row><cell></cell><cell>EFE</cell><cell>1995</cell><cell>577 MB (238,307 docs)</cell></row><row><cell>[EU] Basque</cell><cell>Egunkaria</cell><cell>2001/2003</cell><cell></cell></row><row><cell>[FR] French</cell><cell>Le Monde</cell><cell>1994</cell><cell>157 MB (44,013 docs)</cell></row><row><cell></cell><cell>Le Monde</cell><cell>1995</cell><cell>156 MB (47,646 docs)</cell></row><row><cell></cell><cell>French SDA</cell><cell>1994</cell><cell>86 MB (43,178 docs)</cell></row><row><cell></cell><cell>French SDA</cell><cell>1995</cell><cell>88 MB (42,615 docs)</cell></row><row><cell>[IT] Italian</cell><cell>La Stampa</cell><cell>1994</cell><cell>193 MB (58,051 docs)</cell></row><row><cell></cell><cell>Italian SDA</cell><cell>1994</cell><cell>85 MB (50,527 docs)</cell></row><row><cell></cell><cell>Italian SDA</cell><cell>1995</cell><cell>85 MB (50,527 docs)</cell></row><row><cell>[NL] Dutch</cell><cell>NRC Handelsblad</cell><cell>1994/1995</cell><cell>299 MB (84,121 docs)</cell></row><row><cell></cell><cell>Algemeen Dagblad</cell><cell>1994/1995</cell><cell>241 MB (106,483 docs)</cell></row><row><cell>[PT] Portuguese</cell><cell>Público</cell><cell>1994</cell><cell>164 MB (51,751 docs)</cell></row><row><cell></cell><cell>Público</cell><cell>1995</cell><cell>176 MB (55,070 docs)</cell></row><row><cell></cell><cell>Folha de São Paulo</cell><cell>1994</cell><cell>108 MB (51,875 docs)</cell></row><row><cell></cell><cell>Folha de São Paulo</cell><cell>1995</cell><cell>116 MB (52,038 docs)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,175.92,144.12,240.77,181.90"><head>Table 4 .</head><label>4</label><figDesc>Test</figDesc><table coords="9,175.92,144.24,240.77,181.78"><row><cell></cell><cell></cell><cell></cell><cell cols="4">set breakdown according to question type,</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">number of participants and number of runs</cell><cell></cell></row><row><cell></cell><cell>F</cell><cell>D</cell><cell>L</cell><cell>T</cell><cell>NIL</cell><cell cols="2"># Participants # Runs</cell></row><row><cell>BG</cell><cell>159</cell><cell>24</cell><cell>17</cell><cell>28</cell><cell>9</cell><cell>1</cell><cell>1</cell></row><row><cell>DE</cell><cell>160</cell><cell>30</cell><cell>10</cell><cell>9</cell><cell>13</cell><cell>3</cell><cell>12</cell></row><row><cell>EL</cell><cell>163</cell><cell>29</cell><cell>8</cell><cell>31</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>EN</cell><cell>160</cell><cell>30</cell><cell>10</cell><cell>12</cell><cell>0</cell><cell>4</cell><cell>5</cell></row><row><cell>ES</cell><cell>161</cell><cell>19</cell><cell>20</cell><cell>42</cell><cell>10</cell><cell>4</cell><cell>10</cell></row><row><cell>EU</cell><cell>145</cell><cell>39</cell><cell>16</cell><cell>23</cell><cell>17</cell><cell>1</cell><cell>4</cell></row><row><cell>FR</cell><cell>135</cell><cell>30</cell><cell>35</cell><cell>66</cell><cell>10</cell><cell>1</cell><cell>3</cell></row><row><cell>IT</cell><cell>157</cell><cell>31</cell><cell>12</cell><cell>13</cell><cell>10</cell><cell>0</cell><cell>0</cell></row><row><cell>NL</cell><cell>151</cell><cell>39</cell><cell>10</cell><cell>13</cell><cell>10</cell><cell>1</cell><cell>4</cell></row><row><cell>PT</cell><cell>162</cell><cell>28</cell><cell>10</cell><cell>16</cell><cell>11</cell><cell>6</cell><cell>9</cell></row><row><cell>RO</cell><cell>162</cell><cell>28</cell><cell>10</cell><cell>47</cell><cell>11</cell><cell>2</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="12,199.20,548.70,191.62,106.90"><head>Table 5 .</head><label>5</label><figDesc>Number of participants in QA@CLEF</figDesc><table coords="12,199.20,569.16,191.62,86.44"><row><cell></cell><cell cols="5">America Europe Asia Australia TOTAL</cell></row><row><cell>CLEF 2003</cell><cell>3</cell><cell>5</cell><cell>0</cell><cell>0</cell><cell>8</cell></row><row><cell>CLEF 2004</cell><cell>1</cell><cell>17</cell><cell>0</cell><cell>0</cell><cell>18</cell></row><row><cell>CLEF 2005</cell><cell>1</cell><cell>22</cell><cell>1</cell><cell>0</cell><cell>24</cell></row><row><cell>CLEF 2006</cell><cell>4</cell><cell>24</cell><cell>2</cell><cell>0</cell><cell>30</cell></row><row><cell>CLEF 2007</cell><cell>3</cell><cell>16</cell><cell>1</cell><cell>1</cell><cell>21</cell></row><row><cell>CLEF 2008</cell><cell>1</cell><cell>20</cell><cell>0</cell><cell>0</cell><cell>21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="13,198.78,288.12,199.89,144.04"><head>Table 6 .</head><label>6</label><figDesc>Number of submitted runs</figDesc><table coords="13,198.78,310.80,199.89,121.36"><row><cell></cell><cell cols="3">Submitted runs Monolingual Cross-lingual</cell></row><row><cell>CLEF 2003</cell><cell>17</cell><cell>6</cell><cell>11</cell></row><row><cell>CLEF 2004</cell><cell>48</cell><cell>20</cell><cell>28</cell></row><row><cell>CLEF 2005</cell><cell>67</cell><cell>43</cell><cell>24</cell></row><row><cell>CLEF 2006</cell><cell>77</cell><cell>42</cell><cell>35</cell></row><row><cell>CLEF 2007</cell><cell>37</cell><cell>23</cell><cell>14</cell></row><row><cell>CLEF 2008</cell><cell>51</cell><cell>31</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="14,135.78,240.12,322.83,136.50"><head>Table 7 .</head><label>7</label><figDesc>Evaluation results for the four submitted runs.</figDesc><table coords="14,135.78,260.13,322.83,116.49"><row><cell>Run</cell><cell>R</cell><cell>W</cell><cell>X</cell><cell>U</cell><cell>%F</cell><cell>%T</cell><cell>%D</cell><cell>L%</cell><cell>NIL</cell><cell></cell><cell>CWS</cell><cell>Over-</cell></row><row><cell></cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>[145]</cell><cell>[23]</cell><cell>[39]</cell><cell>[16]</cell><cell>#</cell><cell>%</cell><cell></cell><cell>all</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[*]</cell><cell></cell><cell>accu-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>racy</cell></row><row><cell>ixag08</cell><cell>26</cell><cell>163</cell><cell>11</cell><cell>0</cell><cell>15.9</cell><cell>8.7</cell><cell>7.7</cell><cell>0</cell><cell>4</cell><cell>7.0</cell><cell>0.023</cell><cell>13</cell></row><row><cell>1eueu</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ixag08 1eneu</cell><cell>11</cell><cell>182</cell><cell>7</cell><cell>0</cell><cell>5.5</cell><cell>4.3</cell><cell>7.7</cell><cell>0</cell><cell>6</cell><cell>6.2</cell><cell>0.004</cell><cell>5.5</cell></row><row><cell>ixag08 1eseu</cell><cell>11</cell><cell>182</cell><cell>7</cell><cell>0</cell><cell>6.9</cell><cell>4.3</cell><cell>2.6</cell><cell>0</cell><cell>4</cell><cell>4.8</cell><cell>0.004</cell><cell>5.5</cell></row><row><cell>ixag08 2eseu</cell><cell>7</cell><cell>185</cell><cell>8</cell><cell>0</cell><cell>4.8</cell><cell>4.3</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>3.5</cell><cell>0.003</cell><cell>3.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="15,136.74,185.88,322.71,80.10"><head>Table 8 .</head><label>8</label><figDesc>Results for the submitted run for Bulgarian</figDesc><table coords="15,136.74,206.16,322.71,59.83"><row><cell>Run</cell><cell>R</cell><cell>W</cell><cell>X</cell><cell>U</cell><cell cols="2">% F % T</cell><cell>% D</cell><cell cols="2">% L NIL</cell><cell>CWS</cell><cell>MRR</cell><cell>accuracy</cell><cell>Overall</cell></row><row><cell></cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>[*]</cell><cell>[*]</cell><cell>[*]</cell><cell>[*]</cell><cell>#</cell><cell>% [*]</cell><cell></cell></row><row><cell>btb1</cell><cell>20</cell><cell>173</cell><cell>7</cell><cell>0</cell><cell cols="2">8.80 7.14</cell><cell cols="3">25.00 0.00 -</cell><cell cols="2">0.00 0.01 -</cell><cell>10 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="15,133.26,461.04,329.57,197.71"><head>Table 9 .</head><label>9</label><figDesc>Properties of the 200 Dutch questions (134 topics) in the test set</figDesc><table coords="15,133.26,478.27,329.57,180.49"><row><cell>Question types</cell><cell></cell><cell cols="2">Factoid answer types</cell><cell cols="2">Temporal restric-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>tion</cell><cell></cell></row><row><cell>Definition</cell><cell>39</cell><cell>Count</cell><cell>20</cell><cell>No</cell><cell>187</cell></row><row><cell>Factoid</cell><cell>151</cell><cell>Location</cell><cell>18</cell><cell>Yes</cell><cell>13</cell></row><row><cell>List</cell><cell></cell><cell>Measure</cell><cell>20</cell><cell cols="2">Question per topic</cell></row><row><cell>Answer source</cell><cell></cell><cell>Object</cell><cell>19</cell><cell>1 question</cell><cell>100</cell></row><row><cell>News</cell><cell>20</cell><cell>Organization</cell><cell>18</cell><cell>2 questions</cell><cell>15</cell></row><row><cell>None (NIL answer)</cell><cell>5</cell><cell>Other</cell><cell>17</cell><cell>3 questions</cell><cell>6</cell></row><row><cell>Wikipedia</cell><cell>175</cell><cell>Person</cell><cell>19</cell><cell>4 questions</cell><cell>13</cell></row><row><cell cols="2">Definition answer types</cell><cell>Time</cell><cell>20</cell><cell>Topic types</cell><cell></cell></row><row><cell>Location</cell><cell>3</cell><cell>List answer types</cell><cell></cell><cell>Location</cell><cell>15</cell></row><row><cell>Object</cell><cell>6</cell><cell>Location</cell><cell>6</cell><cell>Object</cell><cell>23</cell></row><row><cell>Organization</cell><cell>8</cell><cell>Other</cell><cell>1</cell><cell>Organization</cell><cell>14</cell></row><row><cell>Other</cell><cell>12</cell><cell>Person</cell><cell>2</cell><cell>Other</cell><cell>50</cell></row><row><cell>Person</cell><cell>10</cell><cell>Time</cell><cell>1</cell><cell>Person</cell><cell>32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="16,135.78,264.12,322.83,128.58"><head>Table 10 .</head><label>10</label><figDesc>Assessment results for the four submitted runs for Dutch.</figDesc><table coords="16,135.78,284.13,322.83,108.57"><row><cell>Run</cell><cell>R</cell><cell>W</cell><cell>X</cell><cell>U</cell><cell>%F</cell><cell>%T</cell><cell>%D</cell><cell>L%</cell><cell>NIL</cell><cell></cell><cell>CWS</cell><cell>Over-</cell></row><row><cell></cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>[151]</cell><cell>[13]</cell><cell>[39]</cell><cell>[10]</cell><cell>#</cell><cell>%</cell><cell></cell><cell>all</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[*]</cell><cell></cell><cell>accu-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>racy</cell></row><row><cell>gron0 81nlnl</cell><cell>50</cell><cell>138</cell><cell>11</cell><cell>1</cell><cell>24.5</cell><cell>15.4</cell><cell>33.3</cell><cell>0.0</cell><cell>19</cell><cell>5.3</cell><cell>0.342</cell><cell>25.0</cell></row><row><cell>gron0 82nlnl</cell><cell>51</cell><cell>136</cell><cell>10</cell><cell>3</cell><cell>24.5</cell><cell>15.4</cell><cell>35.9</cell><cell>0.0</cell><cell>15</cell><cell>6.7</cell><cell>0.331</cell><cell>25.5</cell></row><row><cell>gron0 81ennl</cell><cell>27</cell><cell>157</cell><cell>10</cell><cell>6</cell><cell>13.2</cell><cell>7.7</cell><cell>17.9</cell><cell>0.0</cell><cell>30</cell><cell>3.3</cell><cell>0.235</cell><cell>13.5</cell></row><row><cell>gron0 82ennl</cell><cell>27</cell><cell>157</cell><cell>10</cell><cell>6</cell><cell>13.2</cell><cell>7.7</cell><cell>17.9</cell><cell>0.0</cell><cell>30</cell><cell>3.3</cell><cell>0.235</cell><cell>13.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="17,136.74,263.88,322.77,147.79"><head>Table 11 .</head><label>11</label><figDesc>Evaluation results for the English submitted runs.</figDesc><table coords="17,136.74,278.52,322.77,112.09"><row><cell>Run</cell><cell>R</cell><cell>W</cell><cell cols="3">X U % F</cell><cell>% T % D</cell><cell>% L NIL</cell><cell></cell><cell>CWS</cell><cell>K1</cell><cell>accuracy</cell><cell>Overall</cell></row><row><cell></cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>[160]</cell><cell>[12] [30]</cell><cell>[10] #</cell><cell>%[0]</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">dcun081deen 16</cell><cell>168</cell><cell>7</cell><cell>9</cell><cell>5.00</cell><cell>8.33 26.67</cell><cell>0.00 0</cell><cell>0.00</cell><cell>0.00516</cell><cell>0.10</cell><cell cols="2">8.00</cell></row><row><cell cols="2">dcun082deen 1</cell><cell>195</cell><cell>3</cell><cell>1</cell><cell>0.63</cell><cell>0.00 0.00</cell><cell>0.00 0</cell><cell>0.00</cell><cell>0.00013</cell><cell>0.03</cell><cell cols="2">0.50</cell></row><row><cell>dfki081deen</cell><cell>28</cell><cell>164</cell><cell>5</cell><cell>3</cell><cell>6.25</cell><cell>8.33 60.00</cell><cell>0.00 0</cell><cell>0.00</cell><cell>0.01760</cell><cell>N/A</cell><cell cols="2">14.00</cell></row><row><cell cols="2">ilkm081nlen 7</cell><cell>182</cell><cell>2</cell><cell>9</cell><cell>4.38</cell><cell>0.00 0.00</cell><cell>0.00 0</cell><cell>0.00</cell><cell>0.00175</cell><cell>N/A</cell><cell cols="2">3.50</cell></row><row><cell cols="2">wlvs081roen 38</cell><cell>155</cell><cell>2</cell><cell>5</cell><cell>11.25</cell><cell>0.00 66.67</cell><cell>0.00 0</cell><cell>0.00</cell><cell>0.05436</cell><cell>0.13</cell><cell cols="2">19.00</cell></row></table><note coords="17,145.14,402.81,119.24,8.87"><p>* Total number in the test set.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="20,132.30,264.12,327.90,105.36"><head>Table 12 .</head><label>12</label><figDesc>Results of the EN2EN QA-WSD runs on the 49 queries which had replies in the news collections</figDesc><table coords="20,135.78,294.09,323.13,75.39"><row><cell>Run</cell><cell>R</cell><cell>W</cell><cell>X</cell><cell>U</cell><cell>%F</cell><cell>%T</cell><cell>%D</cell><cell>L%</cell><cell>NIL</cell><cell></cell><cell>CWS</cell><cell>Over-</cell></row><row><cell></cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>[40]</cell><cell>[5]</cell><cell>[7]</cell><cell>[2]</cell><cell>0</cell><cell>%</cell><cell></cell><cell>all</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[0]</cell><cell></cell><cell>accu-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>racy</cell></row><row><cell>nlel08 1enen</cell><cell>8</cell><cell>41</cell><cell>0</cell><cell>0</cell><cell>17.5</cell><cell>0</cell><cell>14.2</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.03</cell><cell>16.32</cell></row><row><cell>nlel08 2enen</cell><cell>7</cell><cell>42</cell><cell>0</cell><cell>0</cell><cell>15.0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.02</cell><cell>14.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="20,132.48,480.30,327.59,105.42"><head>Table 13 .</head><label>13</label><figDesc>Results of the EN2EN QA-WSD runs on all 200 queries, just for the sake of comparison</figDesc><table coords="20,135.78,510.27,322.83,75.45"><row><cell>Run</cell><cell>R</cell><cell>W</cell><cell>X</cell><cell>U</cell><cell>%F</cell><cell>%T</cell><cell>%D</cell><cell>L%</cell><cell>NIL</cell><cell></cell><cell>CWS</cell><cell>Over-</cell></row><row><cell></cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>[160]</cell><cell>[5]</cell><cell>[7]</cell><cell>[10]</cell><cell>0</cell><cell>%</cell><cell></cell><cell>all</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[0]</cell><cell></cell><cell>accu-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>racy</cell></row><row><cell>nlel08 1enen</cell><cell>10</cell><cell>188</cell><cell>0</cell><cell>2</cell><cell>5.6</cell><cell>0</cell><cell>3.3</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.00</cell><cell>5.00</cell></row><row><cell>nlel08 2enen</cell><cell>8</cell><cell>189</cell><cell>0</cell><cell>3</cell><cell>4.4</cell><cell>0</cell><cell>3.3</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.00</cell><cell>4.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="21,137.13,469.48,326.82,189.68"><head>Table 14 .</head><label>14</label><figDesc>Results of the monolingual and bilingual French runs.</figDesc><table coords="21,137.13,489.81,326.82,169.35"><row><cell>Run</cell><cell>Assessed Answers</cell><cell>(#)</cell><cell>R #</cell><cell>W #</cell><cell>X #</cell><cell>U #</cell><cell>%F [135]</cell><cell>%T [66]</cell><cell>%D [30]</cell><cell>L% [35]</cell><cell cols="2">NIL Answers % # [12]</cell><cell>CWS</cell><cell>Overall accuracy</cell></row><row><cell>syn08frfr</cell><cell>200</cell><cell></cell><cell>131</cell><cell>77</cell><cell>9</cell><cell>1</cell><cell>54.8</cell><cell>51.5</cell><cell>86.7</cell><cell>37.1</cell><cell>20</cell><cell>50.0</cell><cell>0.30937</cell><cell>56.5</cell></row><row><cell>syn08enfr</cell><cell>200</cell><cell></cell><cell>36</cell><cell>157</cell><cell>6</cell><cell>1</cell><cell>15.6</cell><cell>15.1</cell><cell>50.0</cell><cell>0.0</cell><cell>60</cell><cell>8.3</cell><cell>0.02646</cell><cell>18.0</cell></row><row><cell>syn08ptfr</cell><cell>200</cell><cell></cell><cell>33</cell><cell>163</cell><cell>4</cell><cell>0</cell><cell>14.1</cell><cell>13.6</cell><cell>43.3</cell><cell>2.9</cell><cell>67</cell><cell>11.9</cell><cell>0.02387</cell><cell>16.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="23,191.16,411.06,225.12,106.72"><head>Table 15 .</head><label>15</label><figDesc>Results per topic size (FR-to-FR)</figDesc><table coords="23,191.16,424.57,225.12,93.21"><row><cell>Run</cell><cell>Size of topic</cell><cell>Assessed Answers #</cell><cell>Overall accuracy (%)</cell></row><row><cell>syn08frfr</cell><cell>1</cell><cell>52</cell><cell>55.8</cell></row><row><cell>syn08frfr</cell><cell>2</cell><cell>66</cell><cell>50.0</cell></row><row><cell>syn08frfr</cell><cell>3</cell><cell>24</cell><cell>66.7</cell></row><row><cell>syn08frfr</cell><cell>4</cell><cell>28</cell><cell>53.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="23,206.34,546.30,193.02,112.78"><head>Table 16 .</head><label>16</label><figDesc>Results per topic size (EN-to-FR)</figDesc><table coords="23,206.34,565.87,193.02,93.21"><row><cell>Run</cell><cell>Size of topic</cell><cell>Assessed An-swers #</cell><cell>Overall ac-curacy (%)</cell></row><row><cell>syn08enfr</cell><cell>1</cell><cell>52</cell><cell>21.2</cell></row><row><cell>syn08enfr</cell><cell>2</cell><cell>66</cell><cell>22.7</cell></row><row><cell>syn08enfr</cell><cell>3</cell><cell>24</cell><cell>13.0</cell></row><row><cell>syn08enfr</cell><cell>4</cell><cell>28</cell><cell>10.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" coords="24,199.44,144.12,205.87,112.24"><head>Table 17 .</head><label>17</label><figDesc>Results per topic size (PT-to-FR)</figDesc><table coords="24,199.44,163.63,205.87,92.73"><row><cell>Run</cell><cell>Size of top-ic</cell><cell>Assessed An-swers #</cell><cell>Overall accu-racy (%)</cell></row><row><cell>syn08ptfr</cell><cell>1</cell><cell>52</cell><cell>25.0</cell></row><row><cell>syn08ptfr</cell><cell>2</cell><cell>66</cell><cell>18.2</cell></row><row><cell>syn08ptfr</cell><cell>3</cell><cell>24</cell><cell>9.3</cell></row><row><cell>syn08ptfr</cell><cell>4</cell><cell>28</cell><cell>10.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20" coords="25,185.94,396.42,206.46,128.62"><head>Table 18 .</head><label>18</label><figDesc>Topic distribution over data collections</figDesc><table coords="25,185.94,415.39,206.46,109.65"><row><cell>Topic Size</cell><cell># Topics / CLEF</cell><cell># Topics / WIKI</cell><cell># Topics</cell></row><row><cell>1</cell><cell>39</cell><cell>35</cell><cell>74</cell></row><row><cell>2</cell><cell>10</cell><cell>14</cell><cell>24</cell></row><row><cell>3</cell><cell>5</cell><cell>5</cell><cell>10</cell></row><row><cell>4</cell><cell>3</cell><cell>9</cell><cell>12</cell></row><row><cell>Total</cell><cell>57</cell><cell>63</cell><cell>120</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21" coords="25,151.14,537.00,303.88,143.08"><head>Table 19 .</head><label>19</label><figDesc>Topic type breakdown over data collections</figDesc><table coords="25,151.14,556.03,303.88,124.05"><row><cell></cell><cell></cell><cell></cell><cell cols="2">CLEF</cell><cell></cell><cell></cell><cell></cell><cell cols="2">WIKI</cell><cell></cell></row><row><cell>Topic Type</cell><cell>1</cell><cell cols="2">Topic Size 2 3</cell><cell>4</cell><cell>Total</cell><cell>1</cell><cell cols="2">Topic Size 2 3</cell><cell>4</cell><cell>Total</cell></row><row><cell>PERSON</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>9</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>2</cell><cell>3</cell></row><row><cell>OBJECT</cell><cell>7</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>8</cell><cell>16</cell><cell>3</cell><cell>0</cell><cell>2</cell><cell>21</cell></row><row><cell>ORGANIZATION</cell><cell>9</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>13</cell><cell>7</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>11</cell></row><row><cell>LOCATION</cell><cell>8</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>13</cell><cell>1</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>8</cell></row><row><cell>EVENT</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>2</cell></row><row><cell>OTHER</cell><cell>9</cell><cell>4</cell><cell>0</cell><cell>1</cell><cell>14</cell><cell>11</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>18</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22" coords="26,137.82,207.42,293.97,153.46"><head>Table 20 .</head><label>20</label><figDesc>Question EAType breakdown over data collections</figDesc><table coords="26,137.82,232.39,293.97,128.49"><row><cell>EAType</cell><cell>CLEF</cell><cell>WIKI</cell><cell>Total</cell></row><row><cell>PERSON</cell><cell>15</cell><cell>15</cell><cell>30</cell></row><row><cell>LOCATION</cell><cell>13</cell><cell>12</cell><cell>25</cell></row><row><cell>TIME</cell><cell>13</cell><cell>8</cell><cell>21</cell></row><row><cell>COUNT</cell><cell>13</cell><cell>7</cell><cell>20</cell></row><row><cell>OBJECT</cell><cell>7</cell><cell>18</cell><cell>25</cell></row><row><cell>MEASURE</cell><cell>12</cell><cell>8</cell><cell>20</cell></row><row><cell>ORGANIZATION</cell><cell>15</cell><cell>13</cell><cell>28</cell></row><row><cell>OTHER</cell><cell>9</cell><cell>22</cell><cell>31</cell></row><row><cell>Total</cell><cell>97</cell><cell>103</cell><cell>200</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23" coords="26,130.38,393.36,326.81,243.81"><head>Table 21 .</head><label>21</label><figDesc>System Performance -Details</figDesc><table coords="26,130.38,425.89,326.81,211.28"><row><cell></cell><cell>R</cell><cell cols="2">W X</cell><cell>U % F</cell><cell cols="4">% T % D % L NIL</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell># [160]</cell><cell>[9]</cell><cell cols="3">[30] [10] #</cell><cell>% [10]</cell><cell>CWS</cell><cell>MRR</cell><cell>accuracy</cell><cell>Overall</cell></row><row><cell cols="4">dfki081dede M 73 119 2</cell><cell cols="3">6 30.62 44.44 80</cell><cell>0</cell><cell cols="2">0 0</cell><cell cols="2">0.16 0</cell><cell>36.5</cell></row><row><cell cols="4">dfki082dede M 74 120 2</cell><cell cols="3">4 31.25 33.33 80</cell><cell>0</cell><cell cols="2">0 0</cell><cell cols="2">0.16 0</cell><cell>37</cell></row><row><cell cols="4">fuha081dede M 45 141 8</cell><cell cols="3">6 24.37 44.44 20</cell><cell>0</cell><cell cols="5">1 4.76 0.05 0.29 22.5</cell></row><row><cell cols="8">fuha082dede M 46 139 11 4 25.62 33.33 16.66 0</cell><cell cols="5">21 4.76 0.048 0.29 23</cell></row><row><cell cols="6">loga081dede M 29 159 11 1 13.75 0</cell><cell>20</cell><cell>10</cell><cell cols="5">55 5.45 0.031 0.19 14.5</cell></row><row><cell cols="4">loga082dede M 27 163 9</cell><cell cols="2">1 13.12 0</cell><cell cols="2">16.66 10</cell><cell cols="5">48 4.16 0.029 0.17 13.5</cell></row><row><cell cols="4">dfki081ende C 29 164 2</cell><cell>5 10</cell><cell>0</cell><cell cols="2">43.33 0</cell><cell cols="2">0 0</cell><cell cols="2">0.038 0</cell><cell>14.5</cell></row><row><cell cols="4">fuha081ende C 28 163 6</cell><cell>3 15</cell><cell cols="3">11.11 13.33 0</cell><cell cols="5">81 7.4 0.023 0.24 14</cell></row><row><cell cols="4">fuha082ende C 28 160 6</cell><cell>6 15</cell><cell cols="3">11.11 13.33 0</cell><cell cols="5">81 7.4 0.019 0.22 14</cell></row><row><cell cols="4">fuha081esde C 19 169 9</cell><cell>2 9.43</cell><cell>0</cell><cell cols="2">13.33 0</cell><cell cols="2">9 0</cell><cell cols="3">0.015 0.15 9.54</cell></row><row><cell cols="4">fuha082esde C 17 173 5</cell><cell>5 8.12</cell><cell>0</cell><cell cols="2">13.33 0</cell><cell cols="5">61 3.27 0.007 0.13 8.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24" coords="27,140.10,404.40,318.17,7.66"><head>Table 22 :</head><label>22</label><figDesc>Results of the runs with Portuguese as target: all 200 questions (first answers only)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26" coords="28,130.44,204.12,331.65,180.40"><head>Table 23 .</head><label>23</label><figDesc>Results of the runs with Portuguese as target: answers to linked and unlinked questions</figDesc><table coords="28,139.56,224.17,313.35,160.35"><row><cell></cell><cell></cell><cell></cell><cell cols="2">First questions</cell><cell></cell><cell></cell><cell cols="2">Linked questions</cell></row><row><cell>Run</cell><cell></cell><cell></cell><cell cols="2">(# 151)</cell><cell></cell><cell></cell><cell cols="2">(# 49)</cell></row><row><cell>Name</cell><cell>R</cell><cell>W</cell><cell>X+</cell><cell>X-</cell><cell>U</cell><cell>Accuracy</cell><cell>R</cell><cell>Accuracy</cell></row><row><cell></cell><cell>(#)</cell><cell>(#)</cell><cell>(#)</cell><cell>(#)</cell><cell>(#)</cell><cell>(%)</cell><cell>(#)</cell><cell>(%)</cell></row><row><cell>diue081</cell><cell>82</cell><cell>59</cell><cell>6</cell><cell>3</cell><cell>1</cell><cell>54.3</cell><cell>11</cell><cell>22.4</cell></row><row><cell>esfi081</cell><cell>42</cell><cell>92</cell><cell>5</cell><cell>7</cell><cell>5</cell><cell>27.3</cell><cell>7</cell><cell>14.3</cell></row><row><cell>esfi082</cell><cell>33</cell><cell>97</cell><cell>6</cell><cell>9</cell><cell>6</cell><cell>21.9</cell><cell>8</cell><cell>16.3</cell></row><row><cell>feup081</cell><cell>29</cell><cell>116</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>19.2</cell><cell>3</cell><cell>6.1</cell></row><row><cell>feup082</cell><cell>25</cell><cell>120</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>16.6</cell><cell>3</cell><cell>6.1</cell></row><row><cell>idsa081</cell><cell>54</cell><cell>85</cell><cell>6</cell><cell></cell><cell>6</cell><cell>35.8</cell><cell>11</cell><cell>22.4</cell></row><row><cell>ines081</cell><cell>35</cell><cell>106</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>23.2</cell><cell>8</cell><cell>16.3</cell></row><row><cell>ines082</cell><cell>35</cell><cell>106</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>23.2</cell><cell>8</cell><cell>16.3</cell></row><row><cell>prib081</cell><cell>105</cell><cell>32</cell><cell>9</cell><cell>4</cell><cell>1</cell><cell>69.5</cell><cell>22</cell><cell>44.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27" coords="28,151.80,399.30,288.67,163.72"><head>Table 24 .</head><label>24</label><figDesc>Results of the assessment of the monolingual Portuguese runs: definitions</figDesc><table coords="28,175.44,418.32,241.55,144.69"><row><cell>Run</cell><cell>loc 1</cell><cell>obj</cell><cell>6</cell><cell>org</cell><cell>6</cell><cell>oth</cell><cell>8</cell><cell>per 6</cell><cell>TOT 27</cell><cell>%</cell></row><row><cell>diue081</cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell>6</cell><cell></cell><cell>8</cell><cell>5</cell><cell>24</cell><cell>89%</cell></row><row><cell>esfi081</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>2</cell><cell></cell><cell>4</cell><cell>2</cell><cell>9</cell><cell>33%</cell></row><row><cell>esfi082</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>7%</cell></row><row><cell>feup081</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>1</cell><cell></cell><cell>1</cell><cell>1</cell><cell>4</cell><cell>15%</cell></row><row><cell>feup082</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>1</cell><cell></cell><cell>1</cell><cell>1</cell><cell>4</cell><cell>15%</cell></row><row><cell>idsa081</cell><cell>1</cell><cell></cell><cell>5</cell><cell></cell><cell>1</cell><cell></cell><cell>5</cell><cell>5</cell><cell>17</cell><cell>63%</cell></row><row><cell>ines081</cell><cell>1</cell><cell></cell><cell>5</cell><cell></cell><cell>1</cell><cell></cell><cell>7</cell><cell>3</cell><cell>17</cell><cell>63%</cell></row><row><cell>ines082</cell><cell>1</cell><cell></cell><cell>5</cell><cell></cell><cell>1</cell><cell></cell><cell>7</cell><cell>3</cell><cell>17</cell><cell>63%</cell></row><row><cell>prib081</cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell>5</cell><cell></cell><cell>6</cell><cell>2</cell><cell>18</cell><cell>67%</cell></row><row><cell>combination</cell><cell>1</cell><cell></cell><cell>6</cell><cell></cell><cell>6</cell><cell></cell><cell>8</cell><cell>6</cell><cell cols="2">27 100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28" coords="29,143.94,192.12,313.84,169.42"><head>Table 25 .</head><label>25</label><figDesc>Results of the assessment of the Portuguese runs: factoids, including lists.</figDesc><table coords="29,143.94,211.63,313.84,149.91"><row><cell>Run</cell><cell>cou 17</cell><cell>loc 38</cell><cell>mea 16</cell><cell>obj</cell><cell>2</cell><cell>org 10</cell><cell>oth 33</cell><cell>per 33</cell><cell>tim 24</cell><cell>TOT 173</cell><cell>%</cell></row><row><cell>diue081</cell><cell>6</cell><cell>17</cell><cell>8</cell><cell>1</cell><cell></cell><cell>5</cell><cell>13</cell><cell>8</cell><cell>11</cell><cell>69</cell><cell>35%</cell></row><row><cell>esfi081</cell><cell>8</cell><cell>8</cell><cell>2</cell><cell></cell><cell></cell><cell>2</cell><cell>2</cell><cell>14</cell><cell>4</cell><cell>40</cell><cell>20%</cell></row><row><cell>esfi082</cell><cell>8</cell><cell>8</cell><cell>2</cell><cell></cell><cell></cell><cell>2</cell><cell>2</cell><cell>13</cell><cell>4</cell><cell>39</cell><cell>20%</cell></row><row><cell>feup081</cell><cell>5</cell><cell>4</cell><cell>4</cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>8</cell><cell>4</cell><cell>28</cell><cell>14%</cell></row><row><cell>feup082</cell><cell>5</cell><cell>3</cell><cell>4</cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>6</cell><cell>3</cell><cell>24</cell><cell>12%</cell></row><row><cell>idsa081</cell><cell>9</cell><cell>9</cell><cell>9</cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell>8</cell><cell>7</cell><cell>48</cell><cell>24%</cell></row><row><cell>ines081</cell><cell>4</cell><cell>9</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>4</cell><cell>6</cell><cell>26</cell><cell>13%</cell></row><row><cell>ines082</cell><cell>4</cell><cell>9</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>4</cell><cell>6</cell><cell>26</cell><cell>13%</cell></row><row><cell>prib081</cell><cell>11</cell><cell>21</cell><cell>13</cell><cell>1</cell><cell></cell><cell>7</cell><cell>18</cell><cell>22</cell><cell>16</cell><cell>109</cell><cell>55%</cell></row><row><cell>combination</cell><cell>16</cell><cell>31</cell><cell>15</cell><cell>1</cell><cell></cell><cell>7</cell><cell>23</cell><cell>27</cell><cell>21</cell><cell>141</cell><cell>82%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29" coords="29,136.08,472.62,272.53,32.20"><head>Table 26 .</head><label>26</label><figDesc>Average size of answers (values in number of words)</figDesc><table coords="29,136.08,497.65,14.65,7.17"><row><cell>Run</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30" coords="29,140.64,491.64,321.50,131.67"><head>name Non-NIL Answers (#) Average an- swer size Average answer size (R only) Average snip- pet size Average snippet size (R only)</head><label></label><figDesc></figDesc><table coords="29,140.64,516.24,300.90,107.08"><row><cell>diue081</cell><cell>179</cell><cell>2.8</cell><cell>3.6</cell><cell>25.9</cell><cell>26.1</cell></row><row><cell>esfi081</cell><cell>180</cell><cell>2.6</cell><cell>3.0</cell><cell>78.4</cell><cell>62.5</cell></row><row><cell>esfi082</cell><cell>180</cell><cell>1.8</cell><cell>1.7</cell><cell>78.2</cell><cell>62.4</cell></row><row><cell>feup081</cell><cell>58</cell><cell>1.8</cell><cell>3.4</cell><cell>64.2</cell><cell>51.6</cell></row><row><cell>feup081</cell><cell>51</cell><cell>1.8</cell><cell>3.7</cell><cell>63.3</cell><cell>51.4</cell></row><row><cell>idsa081</cell><cell>188</cell><cell>5.0</cell><cell>10.0</cell><cell>28.6</cell><cell>34.4</cell></row><row><cell>ines081</cell><cell>77</cell><cell>3.0</cell><cell>7.4</cell><cell>79.6</cell><cell>36.6</cell></row><row><cell>ines082</cell><cell>77</cell><cell>3.0</cell><cell>7.4</cell><cell>79.6</cell><cell>36.6</cell></row><row><cell>prib081</cell><cell>192</cell><cell>3.2</cell><cell>3.4</cell><cell>27.6</cell><cell>25.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31" coords="30,153.72,216.12,304.62,150.70"><head>Table 27 .</head><label>27</label><figDesc>Accuracy of temporally restricted questions.</figDesc><table coords="30,153.72,235.15,304.62,131.67"><row><cell>Run name</cell><cell>Correct answers (#)</cell><cell>T.R.Q correctness (%)</cell><cell>Non-T.R.Q correctness (%)</cell><cell>Total correctness (%)</cell></row><row><cell>diue081</cell><cell>4</cell><cell>23.5</cell><cell>48..6</cell><cell>46.5</cell></row><row><cell>esfi081</cell><cell>3</cell><cell>17.6</cell><cell>24.0</cell><cell>23.5</cell></row><row><cell>esfi082</cell><cell>3</cell><cell>17.6</cell><cell>19.7</cell><cell>19.5</cell></row><row><cell>feup081</cell><cell>1</cell><cell>5.9</cell><cell>15.3</cell><cell>14.5</cell></row><row><cell>feup082</cell><cell>1</cell><cell>5.9</cell><cell>13.1</cell><cell>12.5</cell></row><row><cell>Idsa081</cell><cell>2</cell><cell>11.8</cell><cell>34.4</cell><cell>32.5</cell></row><row><cell>ines081</cell><cell>1</cell><cell>5.9</cell><cell>21.3</cell><cell>20.0</cell></row><row><cell>ines082</cell><cell>1</cell><cell>5.9</cell><cell>21.3</cell><cell>20.0</cell></row><row><cell>prib081</cell><cell>8</cell><cell>47.1</cell><cell>65.0</cell><cell>63.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32" coords="30,132.42,455.34,319.94,161.20"><head>Table 28 .</head><label>28</label><figDesc>Answers by source and their correctness</figDesc><table coords="30,132.42,475.15,319.94,141.39"><row><cell>Run</cell><cell>#</cell><cell>News % correct</cell><cell>#</cell><cell>Wikipedia % correct</cell><cell>#</cell><cell>NIL % correct</cell></row><row><cell>Selection</cell><cell>34</cell><cell>-</cell><cell>144</cell><cell>-</cell><cell>10</cell><cell>-</cell></row><row><cell>diue081</cell><cell>35</cell><cell>40%</cell><cell>144</cell><cell>53%</cell><cell>21</cell><cell>10%</cell></row><row><cell>esfi081</cell><cell>85</cell><cell>21%</cell><cell>95</cell><cell>28%</cell><cell>20</cell><cell>10%</cell></row><row><cell>esfi082</cell><cell>81</cell><cell>17%</cell><cell>99</cell><cell>24%</cell><cell>20</cell><cell>5%</cell></row><row><cell>feup081</cell><cell>10</cell><cell>40%</cell><cell>48</cell><cell>33%</cell><cell>142</cell><cell>6%</cell></row><row><cell>feup082</cell><cell>9</cell><cell>44%</cell><cell>42</cell><cell>29%</cell><cell>149</cell><cell>6%</cell></row><row><cell>idsa081</cell><cell>50</cell><cell>28%</cell><cell>138</cell><cell>36%</cell><cell>12</cell><cell>17%</cell></row><row><cell>ines081</cell><cell>31</cell><cell>23%</cell><cell>46</cell><cell>52%</cell><cell>123</cell><cell>7%</cell></row><row><cell>ines082</cell><cell>31</cell><cell>23%</cell><cell>46</cell><cell>52%</cell><cell>123</cell><cell>7%</cell></row><row><cell>prib081</cell><cell>46</cell><cell>63%</cell><cell>146</cell><cell>66%</cell><cell>8</cell><cell>13%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33" coords="31,133.26,395.88,330.40,146.23"><head>Table 29 .</head><label>29</label><figDesc>Question &amp; Answer types distribution in Romanian (in brackets the number of temporally restricted questions)</figDesc><table coords="31,133.26,425.81,325.40,116.29"><row><cell>Q /expected A type type</cell><cell>PER SON</cell><cell>TIM E</cell><cell>LOC.</cell><cell>ORG.</cell><cell>MEAS URE</cell><cell>COU NT</cell><cell>OBJE CT</cell><cell>OTH ER</cell><cell>TOTAL</cell></row><row><cell>FACTOID</cell><cell>20 (9)</cell><cell>23 (5)</cell><cell cols="2">26 (4) 20 (10)</cell><cell>17 (3)</cell><cell>22 (5)</cell><cell>18 (4)</cell><cell>16 (4)</cell><cell>162 (44)</cell></row><row><cell>DEF.</cell><cell>8</cell><cell></cell><cell>1</cell><cell>6 (2)</cell><cell></cell><cell></cell><cell>6</cell><cell>7</cell><cell>28 (2)</cell></row><row><cell>LIST</cell><cell>3</cell><cell></cell><cell>1 (1)</cell><cell>1</cell><cell></cell><cell></cell><cell>2 (1)</cell><cell>3</cell><cell>10 (2)</cell></row><row><cell>NIL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34" coords="32,130.38,278.16,321.70,136.69"><head>Table 30 .</head><label>30</label><figDesc>Topic-related questions</figDesc><table coords="32,130.38,291.13,321.70,123.73"><row><cell># of questions / Topic type</cell><cell>PERSO N</cell><cell>LOC.</cell><cell>ORG.</cell><cell>EVEN T</cell><cell>OBJE CT</cell><cell>OTHE R</cell><cell>Total topics</cell><cell>Total ques-tions</cell></row><row><cell>4 Qs</cell><cell>5</cell><cell>1</cell><cell>1</cell><cell></cell><cell></cell><cell>5</cell><cell>12</cell><cell>48</cell></row><row><cell>3 Qs</cell><cell>5</cell><cell>1</cell><cell></cell><cell>1</cell><cell>1</cell><cell>3</cell><cell>11</cell><cell>33</cell></row><row><cell>2 Qs</cell><cell>5</cell><cell>3</cell><cell>4</cell><cell></cell><cell>2</cell><cell>9</cell><cell>23</cell><cell>46</cell></row><row><cell>1 Q</cell><cell>13</cell><cell>6</cell><cell>19</cell><cell></cell><cell>17</cell><cell>18</cell><cell>73</cell><cell>73</cell></row><row><cell>TOTAL</cell><cell>28</cell><cell>11</cell><cell>24</cell><cell>1</cell><cell>20</cell><cell>35</cell><cell>119</cell><cell>200</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35" coords="34,118.32,206.16,355.82,340.00"><head></head><label></label><figDesc>Tables 31. Results in the monolingual task, Romanian as target language</figDesc><table coords="34,118.32,214.74,355.82,331.42"><row><cell>Run</cell><cell>R</cell><cell>W</cell><cell></cell><cell cols="2">U F</cell><cell>T</cell><cell>D</cell><cell></cell><cell>L</cell><cell></cell><cell>NIL</cell><cell>CWS</cell><cell>MRR</cell><cell>accuracy</cell><cell>Overall</cell></row><row><cell></cell><cell>#</cell><cell>#</cell><cell></cell><cell cols="5"># [162] [47] [28]</cell><cell cols="2">[10] #</cell><cell>% [8]</cell></row><row><cell>icia08 1roro</cell><cell cols="2">10 179</cell><cell>1</cell><cell cols="2">0 4.938</cell><cell>8.51 1</cell><cell cols="5">7.143 0.0 15 6.667</cell><cell>0.0081 2</cell><cell>0.0821 7</cell><cell>5.0</cell></row><row><cell>icia08 2roro</cell><cell cols="2">21 168</cell><cell>1</cell><cell cols="2">0 6.173</cell><cell>8.51 1</cell><cell cols="5">39.286 0.0 15 6.667</cell><cell>0.0219 1</cell><cell>0.1431 9</cell><cell>10.5</cell></row><row><cell>uaic08 1roro</cell><cell cols="2">41 128</cell><cell>7</cell><cell>3</cell><cell>24.69 1</cell><cell>25.5 32</cell><cell cols="5">3.571 0.0 65 7.692</cell><cell>0.0367 9</cell><cell>0.3432 4</cell><cell>20.5</cell></row><row><cell>uaic08 2roro</cell><cell cols="2">45 125</cell><cell>6</cell><cell>4</cell><cell>26.54 3</cell><cell>27.6 60</cell><cell cols="5">3.571 10.0 64 9.375</cell><cell>0.0489 2</cell><cell>0.3679 9</cell><cell>22.5</cell></row><row><cell>Run</cell><cell cols="5">FACTOID QUESTIONS</cell><cell></cell><cell cols="4">LIST QUESTIONS</cell><cell></cell><cell>DEFINITION QUESTION</cell></row><row><cell></cell><cell>R</cell><cell>W</cell><cell>X</cell><cell cols="3">U ACC</cell><cell cols="5">R W X U ACC</cell><cell>R</cell><cell>W X</cell><cell>U ACC</cell></row><row><cell>Combined</cell><cell cols="2">72 75</cell><cell cols="2">12 3</cell><cell cols="3">44.444 1</cell><cell>9</cell><cell>0</cell><cell>0</cell><cell cols="2">10.000 14 5</cell><cell>10 0</cell><cell>50.000</cell></row><row><cell>icia081roro</cell><cell>8</cell><cell cols="3">144 10 0</cell><cell cols="2">4.938</cell><cell>0</cell><cell cols="2">10 0</cell><cell>0</cell><cell>0.000</cell><cell>2</cell><cell>25 1</cell><cell>0</cell><cell>7.143</cell></row><row><cell>icia082roro</cell><cell cols="3">10 143 9</cell><cell>0</cell><cell cols="2">6.173</cell><cell>0</cell><cell cols="2">10 0</cell><cell>0</cell><cell>0.000</cell><cell>11 15 2</cell><cell>0</cell><cell>39.286</cell></row><row><cell cols="4">uaic081roro 40 113 6</cell><cell>3</cell><cell cols="3">24.691 0</cell><cell>9</cell><cell>1</cell><cell>0</cell><cell>0.000</cell><cell>1</cell><cell>6</cell><cell>21 0</cell><cell>3.571</cell></row><row><cell cols="4">uaic082roro 43 110 5</cell><cell>4</cell><cell cols="3">26.543 1</cell><cell>9</cell><cell>0</cell><cell>0</cell><cell cols="2">10.000 1</cell><cell>6</cell><cell>21 0</cell><cell>3.571</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_36" coords="35,127.86,260.70,345.87,177.05"><head>Table 32 .</head><label>32</label><figDesc>Results for Spanish as target</figDesc><table coords="35,127.86,279.06,345.87,158.69"><row><cell>Run</cell><cell>R #</cell><cell>W #</cell><cell>X #</cell><cell>U #</cell><cell>% F [124]</cell><cell>% T [36]</cell><cell>% D [20]</cell><cell>% L [20]</cell><cell>NIL #</cell><cell>F [10]</cell><cell>CWS</cell><cell>MRR</cell><cell>accuracy</cell><cell>Overall</cell></row><row><cell cols="9">prib081eses 86 105 5 4 41,13 41,67 75 20</cell><cell>3</cell><cell cols="5">0,17 0,178 0,4483 42,5</cell></row><row><cell cols="7">inao082eses 44 152 3 1 19,35 8,33</cell><cell>80</cell><cell>5</cell><cell>4</cell><cell cols="3">0,10 0,068 0,2342</cell><cell cols="2">22</cell></row><row><cell cols="7">inao081eses 42 156 1 1 15,32 8,33</cell><cell>95</cell><cell>5</cell><cell>3</cell><cell cols="3">0,13 0,053 0,2375</cell><cell cols="2">21</cell></row><row><cell cols="8">qaua082eses 39 156 4 1 22,58 13,89 30</cell><cell>-</cell><cell>6</cell><cell cols="5">0,15 0,041 0,2217 19,5</cell></row><row><cell cols="7">mira081eses 32 156 3 9 12,90 2,78</cell><cell>75</cell><cell>-</cell><cell>3</cell><cell cols="3">0,21 0,032 0,1766</cell><cell cols="2">16</cell></row><row><cell cols="7">mira082eses 29 159 3 9 11,29 2,78</cell><cell>70</cell><cell>-</cell><cell>3</cell><cell cols="5">0,23 0,026 0,1591 14,50</cell></row><row><cell cols="8">qaua081enes 25 173 -2 11,29 16,67 20</cell><cell>5</cell><cell>6</cell><cell cols="5">0,19 0,011 0,1450 12,50</cell></row><row><cell cols="6">qaua082enes 18 176 3 3 9,68</cell><cell>8,33</cell><cell>15</cell><cell>-</cell><cell>8</cell><cell cols="3">0,15 0,006 0,1108</cell><cell></cell><cell>9</cell></row><row><cell cols="6">mira081fres 10 185 2 3 5,65</cell><cell>-</cell><cell>15</cell><cell>-</cell><cell>3</cell><cell cols="3">0,12 0,008 0,0533</cell><cell></cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37" coords="35,145.02,469.74,308.15,7.66"><head>Table 33 .</head><label>33</label><figDesc>Results for self-contained and linked questions, compared with overall accuracy</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_38" coords="35,168.36,490.20,245.19,175.19"><head>Run % Accuracy over Self-contained questions [139] % Accuracy over Linked questions [61] % Overall Accuracy [200]</head><label></label><figDesc></figDesc><table coords="35,168.36,551.37,243.34,114.02"><row><cell>prib081eses</cell><cell>53,24</cell><cell>18,03</cell><cell>42,50</cell></row><row><cell>inao082eses</cell><cell>25,18</cell><cell>13,11</cell><cell>22,00</cell></row><row><cell>inao081eses</cell><cell>25,18</cell><cell>9,84</cell><cell>21,00</cell></row><row><cell>qaua082eses</cell><cell>22,30</cell><cell>13,11</cell><cell>19,50</cell></row><row><cell>mira081eses</cell><cell>21,58</cell><cell>3,28</cell><cell>16,00</cell></row><row><cell>mira082eses</cell><cell>21,58</cell><cell>3,28</cell><cell>14,50</cell></row><row><cell>qaua081enes</cell><cell>17,27</cell><cell>-</cell><cell>12,50</cell></row><row><cell>qaua082enes</cell><cell>12,23</cell><cell>1,64</cell><cell>9,00</cell></row><row><cell>mira081fres</cell><cell>6,47</cell><cell>1,64</cell><cell>5,00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_39" coords="36,178.26,275.94,236.76,169.36"><head>Table 34 .</head><label>34</label><figDesc>Results for Spanish as target for NIL questions</figDesc><table coords="36,178.26,296.89,236.76,148.41"><row><cell></cell><cell>F-measure</cell><cell>F-measure</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell></cell><cell>(Self-contained@1)</cell><cell>(@1)</cell><cell>(@1)</cell><cell>(@1)</cell></row><row><cell>prib081eses</cell><cell>0,26</cell><cell>0,17</cell><cell>0.12</cell><cell>0.30</cell></row><row><cell>inao082eses</cell><cell>0,14</cell><cell>0.10</cell><cell>0.06</cell><cell>0.40</cell></row><row><cell>inao081eses</cell><cell>0,19</cell><cell>0.13</cell><cell>0.08</cell><cell>0.30</cell></row><row><cell>qaua082eses</cell><cell>0,27</cell><cell>0.15</cell><cell>0.09</cell><cell>0.60</cell></row><row><cell>mira081eses</cell><cell>0,27</cell><cell>0.21</cell><cell>0.17</cell><cell>0.30</cell></row><row><cell>mira082eses</cell><cell>0,29</cell><cell>0.23</cell><cell>0.19</cell><cell>0.30</cell></row><row><cell>qaua081enes</cell><cell>0,26</cell><cell>0.19</cell><cell>0.11</cell><cell>0.80</cell></row><row><cell>qaua082enes</cell><cell>0,20</cell><cell>0.15</cell><cell>0.09</cell><cell>0.60</cell></row><row><cell>mira081fres</cell><cell>0,15</cell><cell>0.12</cell><cell>0.07</cell><cell>0.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_40" coords="36,159.96,508.98,272.50,156.88"><head>Table 35 .</head><label>35</label><figDesc>Answer extraction and correlation coefficient (r) for Spanish as target</figDesc><table coords="36,238.32,529.92,130.11,135.94"><row><cell>Run</cell><cell>%Answer Ex-traction</cell><cell>r</cell></row><row><cell>prib081eses</cell><cell>90,53</cell><cell>0,4006</cell></row><row><cell>mira082eses</cell><cell>80,56</cell><cell>0,0771</cell></row><row><cell>inao082eses</cell><cell>80,00</cell><cell>0,1593</cell></row><row><cell>mira081eses</cell><cell>80,00</cell><cell>0,0713</cell></row><row><cell>qaua082eses</cell><cell>73,58</cell><cell>0,2466</cell></row><row><cell>inao081eses</cell><cell>67,74</cell><cell>0,1625</cell></row><row><cell>qaua081enes</cell><cell>75,76</cell><cell>0,0944</cell></row><row><cell>qaua082enes</cell><cell>58,06</cell><cell>0,0061</cell></row><row><cell>mira081fres</cell><cell>55,56</cell><cell>0,0552</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_41" coords="37,168.36,305.94,258.39,185.63"><head>Table 36 .</head><label>36</label><figDesc>Results  for questions with answer in Wikipedia and EFE</figDesc><table coords="37,168.36,326.40,258.39,165.17"><row><cell>Run</cell><cell></cell><cell>% Of Correct</cell><cell></cell></row><row><cell></cell><cell>% Of correct answers</cell><cell>Answers found</cell><cell>% Of Correct an-</cell></row><row><cell></cell><cell>found in EFE</cell><cell>in Wikipedia</cell><cell>swers found NIL</cell></row><row><cell>prib081eses</cell><cell>36,97</cell><cell>60,50</cell><cell>2,52</cell></row><row><cell>inao082eses</cell><cell>24,14</cell><cell>68,97</cell><cell>6,90</cell></row><row><cell>inao081eses</cell><cell>25</cell><cell>70</cell><cell>5</cell></row><row><cell>qaua082eses</cell><cell>48,53</cell><cell>42,65</cell><cell>8,82</cell></row><row><cell>mira081eses</cell><cell>23,26</cell><cell>69,77</cell><cell>6,98</cell></row><row><cell>mira082eses</cell><cell>21,62</cell><cell>70,27</cell><cell>8,11</cell></row><row><cell>qaua081enes</cell><cell>52,27</cell><cell>29,55</cell><cell>18,18</cell></row><row><cell>qaua082enes</cell><cell>48,57</cell><cell>34,29</cell><cell>17,14</cell></row><row><cell>mira081fres</cell><cell>33,33</cell><cell>41,67</cell><cell>25</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,138.18,673.94,95.97,6.27"><p>http://wikipedia.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,135.90,675.98,263.94,6.27"><p>http://static.wikipedia.org/downloads/November_2006/ro/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="30,148.32,663.98,313.62,6.27;30,130.38,675.98,199.30,6.27"><p>There were some open list questions as well, but they were classified and evaluated as ordinary factoids.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="31,150.66,591.98,263.94,6.27"><p>http://static.wikipedia.org/downloads/November_2006/ro/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="31,153.72,603.98,311.03,6.27;31,133.26,615.98,331.50,6.27;31,133.26,627.98,81.58,6.27"><p>At http://static.wikipedia.org/downloads/ the frozen versions of Wikipedia exist for April 2007 and June 2008, for all languages involved in QA@CLEF</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="31,214.84,627.98,4.80,6.27;31,145.14,638.03,314.94,8.23;31,133.26,651.98,38.39,6.27"><p>.<ref type="bibr" coords="31,145.14,638.03,3.00,5.31" target="#b1">6</ref> http://celct.isti.cnr.it/ClefQA/QA@CLEF08_Question_Generation_Gui delines.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="31,171.64,651.98,14.39,6.27;31,145.14,662.03,252.54,8.23;31,133.26,675.98,76.78,6.27"><p>pdf 7 http://nlp.uned.es/clef-qa/QA@CLEF08_Guidelines-for-Participants.pdf</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements.</head><p>A special thank to <rs type="person">Danilo Giampiccolo</rs> (<rs type="affiliation">CELCT, Trento, Italy</rs>), who has given his precious advise and valuable support at many levels for the preparation and realization of the QA track at CLEF 2008.</p><p><rs type="person">Jesús Herrera</rs> has been partially supported by the <rs type="funder">Spanish Ministry of Education and Science</rs> (<rs type="grantNumber">TIN2006-14433-C02-01</rs> project).</p><p><rs type="person">Anselmo Peñas</rs> has been partially supported by the <rs type="funder">Spanish Ministry of Science and Technology</rs> within the <rs type="projectName">Text-Mess-INES</rs> project (<rs type="grantNumber">TIN2006-15265-C06-02</rs>).</p><p><rs type="person">Paulo Rocha</rs> was supported by the <rs type="projectName">Linguateca</rs> project, jointly funded by the <rs type="funder">Portuguese Government</rs> and the <rs type="funder">European Union (FEDER and FSE)</rs>, under contract ref. <rs type="grantNumber">POSC/339/1.3/C/NAC</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_T3g5J47">
					<idno type="grant-number">TIN2006-14433-C02-01</idno>
				</org>
				<org type="funded-project" xml:id="_seJ9uqA">
					<idno type="grant-number">TIN2006-15265-C06-02</idno>
					<orgName type="project" subtype="full">Text-Mess-INES</orgName>
				</org>
				<org type="funded-project" xml:id="_tSKRWwz">
					<orgName type="project" subtype="full">Linguateca</orgName>
				</org>
				<org type="funding" xml:id="_KUAhJKm">
					<idno type="grant-number">POSC/339/1.3/C/NAC</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="39,136.64,220.06,229.17,7.97;39,151.26,230.38,295.10,7.97" xml:id="b0">
	<monogr>
		<ptr target="http://clef-qa.itc.it/2007/download/QA@CLEF07_Guidelines-for-Participants.pdf" />
		<title level="m" coord="39,151.26,220.06,146.63,7.97">QA@CLEF 2007 Organizing Committee</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="39,136.64,240.76,328.23,7.97;39,151.26,251.08,266.93,7.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="39,308.02,240.76,132.85,7.97;39,151.26,251.08,213.07,7.97">Coreference Resolution for Questions and Answer Merging</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leveling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>University of Hagen at QA@CLEF. This volume</note>
</biblStruct>

<biblStruct coords="39,136.64,261.46,328.22,7.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="39,279.45,261.46,149.37,7.96">Question Answering Pilot Task at CLEF</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="39,151.26,271.78,313.67,7.96;39,151.26,282.16,313.64,7.96;39,151.26,292.48,313.61,7.96;39,151.26,302.86,13.50,7.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="39,151.26,282.16,223.53,7.96">Multilingual Information Access for Text, Speech and Images</title>
	</analytic>
	<monogr>
		<title level="s" coord="39,381.35,282.16,83.55,7.96;39,151.26,292.48,47.98,7.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="581" to="590" />
			<date type="published" when="2005">2005</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin Hidelberg New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="39,136.64,313.18,328.27,7.96;39,151.26,323.56,167.79,7.96" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ion</surname></persName>
		</author>
		<title level="m" coord="39,181.05,313.18,261.07,7.96">Word Sense Disambiguation Methods Applied to English and Romanian</title>
		<meeting><address><addrLine>Bucharest</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Romanian Academy</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="39,136.64,333.88,328.31,7.96;39,151.26,344.26,313.55,7.96;39,151.26,354.58,157.81,7.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="39,236.39,333.88,139.22,7.96">Constrained Lexical Attraction Models</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">B</forename><surname>Mititelu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="39,394.56,333.88,70.38,7.96;39,151.26,344.26,236.32,7.96">Nineteenth International Florida Artificial Intelligence Research Society Conference</title>
		<meeting><address><addrLine>Menlo Park, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="297" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="39,140.76,364.96,324.26,7.96;39,151.26,375.28,147.47,7.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="39,266.37,364.96,198.64,7.96;39,151.26,375.28,26.57,7.96">The measurements of observer agreement for categorical data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="39,183.56,375.28,37.91,7.97">Biometrics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="159" to="174" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="39,140.76,385.66,324.14,7.96;39,151.26,395.98,147.26,7.96" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="39,300.84,385.66,164.06,7.96;39,151.26,395.98,75.01,7.96">Cross Lingual Question Answering using QRISTAL for CLEF</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Séguéla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nêgre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>This volume</note>
</biblStruct>

<biblStruct coords="39,140.76,406.36,324.16,7.96;39,151.26,416.68,313.76,7.96;39,151.26,427.06,313.64,7.96;39,151.26,437.38,313.62,7.96;39,151.26,447.76,167.80,7.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="39,325.76,416.68,139.26,7.96;39,151.26,427.06,116.63,7.96">Overview of the CLEF 2006 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="39,288.23,427.06,176.67,7.96;39,151.26,437.38,69.95,7.96">Evaluation of Multilingual and Multi-modal Information Retrieval</title>
		<title level="s" coord="39,227.44,437.38,128.82,7.96">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4730</biblScope>
			<biblScope unit="page" from="223" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="39,140.76,458.08,324.17,7.96;39,151.26,468.46,70.08,7.96" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="39,293.67,458.08,171.26,7.96">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>This volume</note>
</biblStruct>

<biblStruct coords="39,140.76,478.78,324.20,7.96;39,151.26,489.16,46.01,7.96" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="39,418.31,478.78,46.65,7.96;39,151.26,489.16,23.49,7.97">Overview of QAST</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Comas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mostefa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="39,140.76,499.48,324.24,7.96;39,151.26,509.86,313.76,7.97;39,151.26,520.18,313.77,7.97;39,151.26,530.56,313.70,7.97;39,151.26,540.88,142.99,7.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="39,381.38,509.86,83.64,7.97;39,151.26,520.18,164.67,7.97">Overview of the CLEF 2005 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Aunimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="39,334.90,520.18,130.13,7.97;39,151.26,530.56,43.61,7.97">Accessing Multilingual Information Repositories</title>
		<title level="s" coord="39,201.33,530.56,129.32,7.97">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="307" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="39,140.76,551.26,324.22,7.97;39,151.26,561.58,313.69,7.97;39,151.26,571.90,211.18,7.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="39,203.71,551.26,205.26,7.97">Overview of the TREC 2002 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<idno>500-251</idno>
	</analytic>
	<monogr>
		<title level="m" coord="39,425.75,551.26,39.23,7.97;39,151.26,561.58,56.63,7.97;39,245.52,561.58,199.52,7.97">The Eleventh Text REtrieval Conference (TREC 2002)</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>National Institute of Standards and Technology</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>NIST Special Publication</note>
</biblStruct>

<biblStruct coords="39,140.76,582.28,324.10,7.97;39,151.26,592.60,313.75,7.97;39,151.26,602.98,220.03,7.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="39,334.36,582.28,130.51,7.97;39,151.26,592.60,51.59,7.97">UBC-ALM: Combining k-NN with SVD for WSD</title>
		<author>
			<persName coords=""><forename type="first">Eneko</forename><forename type="middle">&amp;</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Oier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="39,211.66,592.60,253.34,7.97;39,151.26,602.98,76.94,7.97">Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval 2007)</title>
		<meeting>the 4th International Workshop on Semantic Evaluations (SemEval 2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="39,140.76,613.30,324.24,7.97;39,151.26,623.68,313.79,7.97;39,151.26,634.00,313.59,7.97;39,151.26,644.38,123.59,7.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="39,366.09,613.30,98.91,7.97;39,151.26,623.68,274.58,7.97">NUS-PT: Exploiting Parallel Texts for Word Sense Disambiguation in the English All-Words Tasks</title>
		<author>
			<persName coords=""><forename type="first">Yee</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Seng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hwee</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhi</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="39,432.98,623.68,32.07,7.97;39,151.26,634.00,295.77,7.97">Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval 2007)</title>
		<meeting>the 4th International Workshop on Semantic Evaluations (SemEval 2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
