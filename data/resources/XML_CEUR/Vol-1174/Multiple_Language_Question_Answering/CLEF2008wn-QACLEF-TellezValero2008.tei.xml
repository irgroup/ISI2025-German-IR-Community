<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,182.06,146.21,238.85,18.08;1,163.69,168.13,275.61,18.08;1,180.22,190.05,242.56,18.08">INAOE at QA@CLEF 2008: Evaluating Answer Validation in Spanish Question Answering</title>
				<funder ref="#_bs3VPqx #_xs9D4kG">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_MV23Erz #_eTBFp9R">
					<orgName type="full">CONACYT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,95.98,225.11,92.09,10.46"><forename type="first">Alberto</forename><surname>Téllez-Valero</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Óptica y Electrónica Luis Enrrique Erro no</orgName>
								<orgName type="laboratory">Laboratorio de Tecnologías del Lenguaje</orgName>
								<orgName type="institution">Instituto Nacional de Astrofísica</orgName>
								<address>
									<addrLine>1, Sta. María Tonantzintla, Pue</addrLine>
									<postCode>72840</postCode>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,195.66,225.11,107.43,10.46"><forename type="first">Antonio</forename><surname>Juárez-González</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Óptica y Electrónica Luis Enrrique Erro no</orgName>
								<orgName type="laboratory">Laboratorio de Tecnologías del Lenguaje</orgName>
								<orgName type="institution">Instituto Nacional de Astrofísica</orgName>
								<address>
									<addrLine>1, Sta. María Tonantzintla, Pue</addrLine>
									<postCode>72840</postCode>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.02,225.11,106.57,10.46"><forename type="first">Manuel</forename><surname>Montes-Y-Gómez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Óptica y Electrónica Luis Enrrique Erro no</orgName>
								<orgName type="laboratory">Laboratorio de Tecnologías del Lenguaje</orgName>
								<orgName type="institution">Instituto Nacional de Astrofísica</orgName>
								<address>
									<addrLine>1, Sta. María Tonantzintla, Pue</addrLine>
									<postCode>72840</postCode>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,425.99,225.11,98.67,10.46"><forename type="first">Luis</forename><surname>Villaseñor-Pineda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Óptica y Electrónica Luis Enrrique Erro no</orgName>
								<orgName type="laboratory">Laboratorio de Tecnologías del Lenguaje</orgName>
								<orgName type="institution">Instituto Nacional de Astrofísica</orgName>
								<address>
									<addrLine>1, Sta. María Tonantzintla, Pue</addrLine>
									<postCode>72840</postCode>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,182.06,146.21,238.85,18.08;1,163.69,168.13,275.61,18.08;1,180.22,190.05,242.56,18.08">INAOE at QA@CLEF 2008: Evaluating Answer Validation in Spanish Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CF3C4FF42CD23801DE2CB4F525935FAB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Question Answering, Answer Validation, Textual Entailment, Machine Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces the new INAOE's answer validation method. This method is based on supervised learning approach that uses a set of attributes that capture some lexical-syntactic relations among the question, the answer and the given support text. In addition, the paper describes the evaluation of the proposed method at both the Spanish Answer validation Exercise (AVE 2008)  and the Spanish Question Answering Main Task (QA 2008). The evaluation objectives were twofold. One the one hand, evaluate the ability of our answer validation method to discriminate correct from incorrect answers, and on the other hand, measure the impact of including an answer validation module in our QA system. The evaluation results were encouraging; the proposed method achieved a 0.39 F-measure in the detection of correct answers, outperforming the baseline result of the AVE 2008 task by more than 100%. It also enhanced the performance of our QA system, showing a gain in accuracy of 22% for answering factoid questions. Furthermore, when there were evaluated three candidate answers per question, the answer validation method increased the MRR of our QA system by 40%, reaching a MRR of 0.28.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Latest evaluations of question answering (QA) systems evidenced two important facts about the state of the art of this field. First, they indicated that it already does not exist any system capable of answering all types of questions with similar precision rates, and second, they revealed that most current QA systems are complementary (see for instance the Spanish QA evaluation overview at CLEF 2005 <ref type="bibr" coords="2,195.35,122.49,10.30,10.46" target="#b0">[1]</ref>). These two facts have triggered the development of answer validation (AV) methods, which allow determining if a specified answer is correct and supported <ref type="bibr" coords="2,467.61,134.45,9.96,10.46" target="#b1">[2]</ref>.</p><p>In line with these recent efforts, in this paper we describe a new AV method. This method, similar to our previous year work <ref type="bibr" coords="2,233.40,158.36,9.96,10.46" target="#b2">[3]</ref>, is based on a supervised learning approach for recognizing the textual entailment. It mainly uses a set of attributes that capture some simple relations among the question, the answer and the given supported text. In particular, it considers some novel attributes that characterize: (i) the compatibility between question and answer types; (ii) the redundancy of answers across streams; and (iii) the overlap (as well as the non-overlap) between the question-answer pair and the core fragment of the support text.</p><p>In order to evaluate the proposed method we considered two different scenarios: the Answer Validation Exercise <ref type="bibr" coords="2,175.88,242.04,46.10,10.46">(AVE 2008</ref>) and the Question Answering Main Task <ref type="bibr" coords="2,401.84,242.04,43.81,10.46">(QA 2008)</ref>. The objective of the first scenario was to evaluate the ability of our AV method to discriminate correct from incorrect answers as well as its capacity to combine the answers from several QA systems. In contrast, the goal of the second evaluation scenario was to measure the impact of including an answer validation module in our QA system <ref type="bibr" coords="2,285.33,289.86,9.96,10.46" target="#b3">[4]</ref>.</p><p>The evaluation results were encouraging; the proposed method achieved a 0.39 F-measure in the detection of correct answers, outperforming the baseline result of the AVE 2008 task by more than 100%. It also enhanced the performance of our QA system, producing a gain in accuracy of 22% for answering factoid questions. Furthermore, when there were evaluated three candidate answers per question, the answer validation method increased the MRR of our QA system by 40%, reaching a MRR of 0.28.</p><p>The rest of the paper is organized as follows. Section 2 describes our answer validation method. Section 3 presents the evaluation results of the proposed method in both the Answer Validation Exercise and Spanish QA Main Task. Finally, Section 4 exposes our conclusions and outlines some future work directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Answer Validation Method</head><p>Given a question (Q), a candidate answer (A) and a support text (S ), this method returns a confidence value (β) that allows deciding whether to accept or reject a candidate answer. In other words, it helps to determine if the specified answer is correct and if it can be deduced from the given support text.</p><p>Like other previous answer validation methods evaluated in the last AVE <ref type="bibr" coords="2,427.36,512.01,10.52,10.46" target="#b4">[5,</ref><ref type="bibr" coords="2,441.14,512.01,7.01,10.46" target="#b5">6]</ref>, and following the original idea proposed in the first RTE challenge at PASCAL <ref type="bibr" coords="2,375.95,523.96,9.96,10.46" target="#b6">[7]</ref>, our method is mainly based on recognizing the textual entailment (RTE) between the support text (T ) and an affirmative sentence (H ) called hypothesis, created from the combination of the question and the answer <ref type="foot" coords="2,496.52,546.80,3.97,7.32" target="#foot_0">1</ref> .</p><p>The returned confidence value β is generated by means of a supervised learning approach that considers three main processes: preprocessing, attribute extraction and answer classification. The following sections describe each of these processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>The objective of this process is to extract the main content elements from the question, answer and support text, which will be subsequently used for deciding about the correctness of the answer. This process considers two basic tasks: on the one hand, the identification of the main constituents from the question-answer pair, and on the other hand, the detection of the core fragment of the support text as well as the consequent elimination of the unnecessary information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Constituent Identification</head><p>We detect three basic constituents from the questions: its main action, the action actors, and if exist, the action restriction. As an example, consider the question in Table <ref type="table" coords="3,427.85,140.88,3.87,10.46" target="#tab_0">1</ref>. In this case, the action is represented by the verb invade, its actors are the syntagms Which country and Iraq, and the action restriction is described by the propositional syntagma in 1990. In order to detect the question constituents we firstly apply a shallow parsing to the given question<ref type="foot" coords="3,126.31,368.90,3.97,7.32" target="#foot_1">2</ref> . Then, from the resulting syntactic tree (Qparsed), we construct a new representation of the question (called Q' ) by detecting and tagging the following elements:</p><p>1. The action constituent. It corresponds to the syntagm in Qparsed that includes the main verb.</p><p>2. The restriction constituent. It is represented by the prepositional syntagm in Qparsed having at least one explicit time expression (e.g., in 1990), or including a preposition such as after or before.</p><p>3. The actors constituents. These constituents are formed by the rest of the elements in Qparsed.</p><p>It is commonly divided in two parts. The first one, henceforth called hidden actor constituent, corresponds to the syntagm that includes the interrogative word and it is generally located at the left of the action constituent. The second part, which we call the visible actor constituent, is formed by the rest of the syntagms, generally located at the right of the action constituent.</p><p>Finally, we also consider an answer constituent, which is simply the lemmatized candidate answer (denoted by A' ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Support Text's Core Fragment Detection</head><p>Commonly, the support text is a short paragraph -of maximum 700 bytes according to CLEF evaluations-which provides the context necessary to support the correctness of a given answer. However, in many cases, it contains more information than required, damaging the performance of RTE methods based on lexical-syntactic overlaps. For instance, the example of Table <ref type="table" coords="3,479.23,626.71,4.98,10.46" target="#tab_0">1</ref> shows that only a part of the last sentence in the support text (i.e., Iraqi invasion of Kuwait) is useful for validating the given answer, whereas the rest of the text only contribute to produce an irrelevant overlap (e.g., Kuwait was a close ally of Iraq).</p><p>In order to reduce the support text to the minimum useful text fragment according to the candidate answer validation, we proceed as follows:</p><p>• First, we apply a shallow parsing to the support text, obtaining the syntactic tree (Sparsed).</p><p>• Second, we match the content terms (nouns, verbs, adjectives and adverbs) from the question constituents against the terms from Sparsed. In order to avoid some minimal writing differences of the same concept not solved by the morphological analysis (e.g., Iraq against Irak or Iraqi), we compare the terms using the Levenshtein edition distance <ref type="foot" coords="4,415.79,145.33,3.97,7.32" target="#foot_2">3</ref> . Mainly, we consider that two different words are equal if their distance value is less than 0.4.</p><p>• Third, based on the number of matched terms, we align the question constituents with the syntagms from the support text.</p><p>• Forth, we match the answer constituent against the syntactic tree (Sparsed). The idea is to find all occurrences of the answer in the given support text.</p><p>• Fifth, we determine the minimum context of the answer in the support text that contains all matched syntagms. This minimum context (represented by a sequence of words around the answer) is what we call the core fragment (denoted by T' ). In the case that the support text includes several occurrences of the answer, we select the one with the smallest context.</p><p>Applying the procedure described above we determine that the core fragment of the support text showed at Table <ref type="table" coords="4,184.69,311.78,4.98,10.46" target="#tab_0">1</ref> is in an Iraqi invasion of Kuwait.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attribute Extraction</head><p>This stage gathers a set of processes that allow extracting several attributes from the question, the answer and the support text. These attributes can be categorized in two different groups: the attributes that indicate the relation between the question and the answer, and the attributes that measure the entailment relation between the question-answer pair and the support text.</p><p>The following sections describe both kinds of attributes and explain the way they are calculated from Q', A' and T'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Attributes about the Question-Answer Relation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Characteristics</head><p>We consider four different attributes from the question: the question word (what, how, where, etc.), the question category (factoid or definition), the expected answer type (date, quantity, name or other), and the type of question restriction (date, period, event, or none).</p><p>The question word, question category, and the expected answer type are determined using a set of simple lexical patterns. Some of these patterns are showed below. It can be observed that each of them includes information about the question category and the expected answer type.</p><formula xml:id="formula_0" coords="4,163.94,561.55,275.13,34.37">(WHAT OR WHO) is [whatever ] → DEFINITION -OTHER HOW many [whatever ] → FACTOID -QUANTITY WHEN [whatever ] → FACTOID -DATE</formula><p>On the other hand, the value of the question restriction (date, period, event or none) depends on the form of the restriction constituent. If this constituent contains only one time expression, then this value is set to "date". In the case the restriction constituent includes two time expressions, it is set to "period". If the restriction constituent does not include any time expression, then the question restriction is defined as "event". Finally, when the question does not have any restriction constituent, the value of the question restriction is set to "none".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question-Answer Compatibility</head><p>This attribute indicates if the question and answer types are compatible. The idea of this attribute is to capture the situation where the semantic class of the evaluated answer does not correspond to the expected answer type. For instance, having the answer yesterday for the question How many inhabitants are there in Longyearbyen?.</p><p>This is a binary attribute: it is equal to 1 when the answer corresponds to the expected answer type, and it is equal to 0 if this correspondence does not exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Redundancy</head><p>Taking into account the idea of "considering candidates as allies rather than competitors" <ref type="bibr" coords="5,499.72,232.99,9.96,10.46" target="#b8">[9]</ref>, we decided to include an attribute related to the occurrence of the answers across the pool of candidate answers.</p><p>Different from other redundancy methods (like the one present in <ref type="bibr" coords="5,396.47,268.85,15.50,10.46" target="#b9">[10]</ref>) that directly uses the frequency of occurrence of the answers, the proposed attribute indicates the sum of the edition distances between the actual evaluated answer to each one of the rest of the candidate answers.</p><p>The edition distance strategy allows dealing with the great language variability and also with the presence of some typing errors. In this way, an answer X contributes to the redundancy rate of another answer Y and vice versa, even though X and Y are not exactly the same (e.g. spacial telescope and Hubble telescope).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Attributes related to the Textual Entailment Recognition</head><p>The attributes of this category are of two main types: (i) attributes that measure the overlap between the support text and the hypothesis (an affirmative sentence formed by combining the question and the answer); and (ii) attributes that denote the differences (non-overlap) between these two components.</p><p>It is important to explain that, different from other RTE methods, we do not use the complete support text, instead we only use its core fragment T'. In addition, we neither need to construct an hypothesis text, instead we use as hypothesis the set of question-answer constituents (the union of Q' and A', which we call H' ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overlap Characteristics</head><p>These attributes express the degree of overlap -in number of words-between T' and H'. In particular, we compute an overlap attribute for each one of the fourth types of content terms (nouns, verbs, adjectives and adverbs) as well as for each one of the six types of named entities (names of persons, places, organizations, and other things, as well as dates and quantities). We generate these ten different overlap attributes for each one of the five constituents in H' (the action constituent, the restriction constituent, the hidden actor constituent, the visible actor constituent, and the answer constituent). In this way, we get a total of fifty attributes that represent the overlap characteristics.</p><p>Similar to the calculation of the answer redundancy attribute, in this case we also apply the edition distance to evaluate the overlap between the terms of H' and T'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Overlap Characteristics</head><p>These attributes indicate the number of non-overlapped terms from the core fragment of the support text, that is, they indicate the number of terms from T' that are not present in any of the detected constituents. Mainly, we measure the non-overlap between the answer constituent and each one of the other constituents, and compute this non-overlap for each type of content term as well as for each type of named entity. In total we generate forty different non-overlap attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Classification</head><p>This final process generates the answer validation decision by means of a supervised learning approach. In particular, it applies a boosting ensemble formed by ten decision tree classifiers <ref type="foot" coords="6,495.15,139.80,3.97,7.32" target="#foot_3">4</ref> .</p><p>The constructed classifier decides whether to accept or reject the candidate answer based on the ninety-six attributes described in the previous section. In addition, it also generates a validation confidence (β) that indicates how reliable is the given answer in accordance to the support text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Evaluation</head><p>As we previously mentioned, we evaluated the proposed AV method in two different scenarios: the Answer Validation Exercise (AVE 2008) and the Question Answering Main Task <ref type="bibr" coords="6,442.70,243.45,44.35,10.46">(QA 2008)</ref>. The objective of the first scenario was to evaluate the ability of the AV method to discriminate correct from incorrect answers as well as its capacity to combine the answers from several QA systems. In contrast, the goal of the second evaluation scenario was to measure the impact of including an answer validation module in our QA system <ref type="bibr" coords="6,288.15,291.28,9.96,10.46" target="#b3">[4]</ref>. The following sections present the results from both scenarios. On the other hand, we consider two different test sets, one for the Spanish Answer Validation Exercise (AVE) and other for the Spanish Question Answering Main Task (QA). The AVE test set consists of 1528 answers; these answers correspond to 136 different questions and were generated by all participating systems at the 2008 Spanish QA task. Whereas, the QA test set includes 1152 answers returned by our QA system. These answers correspond to 164 questions that our system catalogue as not NIL from the entire 2008 test set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training and Test Sets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Spanish Answer Validation Exercise</head><p>This evaluation exercise focuses on analyzing two different aspects of the AV methods: on the one hand, their ability to discriminate correct from incorrect answers (answer validation evaluation), and on the other hand, their capacity to select the correct answer from a pool of candidate answers returned by diverse QA systems (stream fusion evaluation).</p><p>From previous experiments <ref type="bibr" coords="6,227.71,668.78,14.61,10.46" target="#b10">[11]</ref>, we noticed that the best method for AV (for discriminating correct from incorrect answers) is not necessarily the best option for a QA stream fusion, it taking into account that the actual AV methods are far away of the perfect validation. Based on this evidence, we decided to evaluate two different runs obtained by applying two different acceptance thresholds over the confidence value (β). The first run (RUN 1) aimed to increase the recall by reducing in 10% the default acceptance threshold, whereas the second run (RUN 2) maintained the default threshold (β = 0.5).</p><p>Table <ref type="table" coords="7,132.48,134.45,4.98,10.46" target="#tab_3">3</ref> shows the answer validation results corresponding to our two submitted runs. It also shows (in the last row) the results for a 100% VALIDATED baseline (i.e., an answer validation system that accepted all given answers). The results indicate that reducing the acceptance threshold (RUN 1) our method achieved a high recall but a low precision, which means that it correctly accepts must correct answers (there are a few false negatives), but it also incorrectly accepts many wrong responses (there are several false positives). In contrast, the second run (RUN 2) got a worst recall, but achieved a major precision and F-measure, outperforming the baseline result in more than 100%. Complementary to the previous data, Table <ref type="table" coords="7,299.19,329.86,4.98,10.46" target="#tab_4">4</ref> shows the evaluation results for the QA stream fusion. These results indicate that the QA-accuracy of RUN 1 is 19% better than the accuracy of RUN 2. Given that RUN 2 clearly outperformed the answer validation result of RUN 1 (see Table <ref type="table" coords="7,105.61,365.73,3.87,10.46" target="#tab_3">3</ref>), these results confirm our observation that the best answer validation method not necessary produces the best QA stream fusion performance.</p><p>Besides the traditional QA-accuracy measure, this year the AVE organizers included a new evaluation measure called QA-performance. This measure allows evaluating the influence into the question answering task of not only correctly accept those right answers but also to correctly reject those wrong ones. The results in Table <ref type="table" coords="7,264.88,425.50,4.98,10.46" target="#tab_4">4</ref> indicate that, because of the better capacity of RUN 2 to reject wrong answers, the QA-performance of both runs were very similar. Finally, it is important to comment that, in order to understand the behavior of the proposed method, we carried out a deep analysis of the usefulness of each one of the used characteristics. Table <ref type="table" coords="7,116.78,575.02,4.98,10.46">5</ref> resumes the information gain values of the main kinds of attributes used by our supervised AV method. Surprisingly, these values show that the proposed non-overlap characteristics are more discriminative than the traditional overlap features.</p><p>Table <ref type="table" coords="7,117.09,629.71,3.87,10.46">5</ref>: Information gain of the used characteristics (For the overlap and non-overlap characteristics, the showed value indicates the average of the complete subset of attributes)</p><p>Answer redundancy Overlap characteristics Non-overlap characteristics 0.023 0.002 0.012</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Spanish QA Main Task</head><p>In order to evaluate the effect of including an AV module in a QA system, this year we submitted two different runs at the Main QA task. The first run (inao081eses) was the original output of our QA system (refer to <ref type="bibr" coords="8,178.56,110.53,10.52,10.46" target="#b3">[4]</ref> for details), whereas the second run (inao082eses) was the result of applied the AV method over the set of candidate answers generated by the first run. Table <ref type="table" coords="8,461.47,122.49,4.98,10.46" target="#tab_5">6</ref> shows the evaluation results of both runs as well as a baseline result corresponding to a perfect validation of the output of our QA system.  <ref type="table" coords="8,195.51,281.54,4.98,10.46" target="#tab_5">6</ref> indicate that the AV module helped increasing the number of right answers for factoid questions, improving the accuracy of our QA system by 22%. In contrast, the AV module damaged the treatment of definition question since it incorrectly rejected three right answers. In this case, taking into account that our QA system is very accurate for answering definition questions, our conclusion is that it is better not to include the AV module.</p><p>Given that this year was allowed to deliver three candidate answers per question, we not only evaluated the effect of the AV module in the answer accuracy but also in rank of the correct answers. For achieving this objective, we included more that one answer for some questions; the first run (inaoe081eses) consisted of 422 answers, whereas the second run (inaoe082eses) included a total of 343 answers. The difference in the number of answers between both runs was caused because many candidate answers were rejected in the second run during the validation process. It is important to mention that the AV module not only eliminated some tentative incorrect answers, but also modified their final order. In particular, the rank of the answers in the second run was determined by means of their confidence values. The evaluation results indicated that the second run outperformed by 40% (with a Mean Reciprocal Rank of 0.28) the output of the first run (with a Mean Reciprocal Rank of 0.20).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper presented a new AV method based on a supervised textual entailment approach. This method mainly differs from previous ones in the kind of used attributes. In particular, it considers some novel attributes that characterize: (i) the compatibility between question and answer types; (ii) the redundancy of answers across streams; and (iii) the overlap as well as the non-overlap between the question-answer pair and the core fragment of the support text. Regarding these attributes, it is important to mention that an analysis about their usefulness showed that the proposed non-overlap characteristics are more discriminative than the traditional overlap features.</p><p>The proposed method was evaluated in two different scenarios: the Spanish Answer Validation Exercise (AVE 2008) and the Spanish QA Main Task (QA 2008). The objective of the first scenario was to evaluate the ability of our AV method to discriminate correct from incorrect answers as well as its capacity to combine the answers from several QA systems. In contrast, the goal of the second evaluation scenario was to measure the impact of including an answer validation module in a QA system. The evaluation results were encouraging; the proposed method achieved a 0.39 F-measure in the detection of correct answers, outperforming the baseline result of the AVE 2008 task by more than 100%. It also enhanced the performance of our Spanish QA system, producing a gain in accuracy of 22% for factoid questions. Furthermore, when there were evaluated three candidate answers per question, the AV method allowed increasing the MRR of our QA system by 40%, reaching a MRR of 0.28.</p><p>Finally, it is important to comment that this year our best results in the AVE (a F-measure of 0.39 and a qa-accuracy of 0.32) were very distant from those corresponding to a perfect validation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,137.58,195.20,323.01,144.70"><head>Table 1 :</head><label>1</label><figDesc>Example of excessive support text to accept or reject an answer</figDesc><table coords="3,137.58,209.88,322.71,130.01"><row><cell>Question: Which country did Iraq invade in 1990?</cell></row><row><cell>Candidate answer: Kuwait</cell></row><row><cell>Support text: Kuwait was a close ally of Iraq during</cell></row><row><cell>the Iraq-Iran war and functioned as the</cell></row><row><cell>country's major port once Basra was shut</cell></row><row><cell>down by the fighting. However, after the</cell></row><row><cell>war ended, the friendly relations between</cell></row><row><cell>the two neighboring Arab countries turned</cell></row><row><cell>sour due to several economic and diplomatic</cell></row><row><cell>reasons which finally culminated in an Iraqi</cell></row><row><cell>invasion of Kuwait.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,90.00,349.50,423.01,34.37"><head>Table 2</head><label>2</label><figDesc>resumes the used training and test sets. The training set combines the instances from the training set of the AVE 2006 and the instances from the test sets of the AVE 2006 and 2007. In total, it contains 574 questions with 2905 answers (the first row of the table details this set).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,90.00,475.89,423.04,76.91"><head>Table 2 :</head><label>2</label><figDesc>Training and Test Sets (VALIDATED are the answers judged as right, REJECTED are the answers judged as wrong or unsupported, and UNKNOWN are the answers judged as inexact)</figDesc><table coords="6,156.14,500.09,290.73,52.70"><row><cell></cell><cell>VALIDATED</cell><cell>REJECTED</cell><cell>UNKNOWN</cell></row><row><cell>Training set</cell><cell>1436 (25%)</cell><cell>4306 (75%)</cell><cell>0</cell></row><row><cell>AVE test set</cell><cell>153 (10%)</cell><cell>1354 (89%)</cell><cell>21 (1%)</cell></row><row><cell>QA test set</cell><cell>74 (6%)</cell><cell>1066 (93%)</cell><cell>12 (1%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,169.95,247.52,263.10,65.46"><head>Table 3 :</head><label>3</label><figDesc>Results for the answer validation evaluation</figDesc><table coords="7,169.95,260.28,263.10,52.70"><row><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell>RUN 1</cell><cell>0.13</cell><cell>0.86</cell><cell>0.23</cell></row><row><cell>RUN 2</cell><cell>0.30</cell><cell>0.59</cell><cell>0.39</cell></row><row><cell>100% VALIDATED</cell><cell>0.10</cell><cell>1.0</cell><cell>0.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,170.09,468.22,262.82,67.40"><head>Table 4 :</head><label>4</label><figDesc>Results for the QA stream fusion evaluation</figDesc><table coords="7,170.09,482.92,262.82,52.70"><row><cell></cell><cell>QA-accuracy</cell><cell>QA-performance</cell></row><row><cell>RUN 1</cell><cell>0.32</cell><cell>0.34</cell></row><row><cell>RUN 2</cell><cell>0.27</cell><cell>0.33</cell></row><row><cell>PERFECT FUSION</cell><cell>0.62</cell><cell>0.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,90.00,177.17,429.65,114.83"><head>Table 6 :</head><label>6</label><figDesc>Results of the QA main task (Also the Accuracy (Ac) the table presents, by question type (factoid</figDesc><table coords="8,90.00,188.33,429.65,103.67"><row><cell cols="8">and definition), the number of questions answered right (R), wrong (W), inexact (X), and unsupported (U))</cell></row><row><cell></cell><cell cols="7">Factoid questions Definition questions</cell></row><row><cell></cell><cell>R</cell><cell>W</cell><cell cols="4">X U R W X</cell><cell>U</cell><cell>Ac</cell></row><row><cell>inaoe081eses (original QA system)</cell><cell cols="3">23 156 1</cell><cell>1 19</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.21</cell></row><row><cell cols="4">inaoe082eses (QA system with an AV module) 28 149 3</cell><cell>1 16</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>0.22</cell></row><row><cell>PERFECT VALIDATION</cell><cell cols="3">30 147 3</cell><cell>1 19</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.25</cell></row><row><cell>Results from Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,697.09,407.77,8.37"><p>The entailment between the pair (T, H ) occurs when the meaning of H can be inferred from the meaning of T.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.24,726.93,407.87,8.37;3,90.00,736.39,423.12,8.37;3,90.01,745.85,144.81,8.37"><p>In all the text processing used in our method (i.e., lemmatization, part of speech tag, named entities recognition and classification, and shallow parsing), we employed the open source tool called Freeling (http://garraf.epsevg.upc.es/freeling/).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,105.24,686.52,407.87,8.37;4,90.00,695.98,108.57,8.37"><p>The Levenshtein edition distance has been previously used in other works related to answer validation in Spanish language, see for instance<ref type="bibr" coords="4,187.28,695.98,8.47,8.37" target="#b7">[8]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,105.24,736.39,407.87,8.37;6,90.00,745.85,155.12,8.37"><p>We used the Weka implementations for the boosting (AdaBoostM1) and C4.5 (J48) algorithms (http://www.cs.waikato.ac.nz/ml/weka/).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We presume that this situation was caused by the decreasing number of right answers together with the increasing number of relevant support passages related to the wrong answers. In order to tackle these problems, and based on the fact that non-overlap attributes were the most discriminative, we plan to include more elements (such as prepositions, conjunctions, and some punctuation marks) for their computation.</p></div>
<div><head>Acknowledgments</head><p>This work was done under partial support of <rs type="funder">CONACYT</rs> (project grants <rs type="grantNumber">43990</rs> and <rs type="grantNumber">61335</rs>, and scholarships <rs type="grantNumber">171610</rs> and <rs type="grantNumber">165499</rs>). We also want to thank CLEF organizers for the provided resources.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MV23Erz">
					<idno type="grant-number">43990</idno>
				</org>
				<org type="funding" xml:id="_eTBFp9R">
					<idno type="grant-number">61335</idno>
				</org>
				<org type="funding" xml:id="_bs3VPqx">
					<idno type="grant-number">171610</idno>
				</org>
				<org type="funding" xml:id="_xs9D4kG">
					<idno type="grant-number">165499</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.48,283.31,402.52,10.46;9,110.48,295.27,402.54,10.46;9,110.48,307.22,402.52,10.46;9,110.48,319.17,402.52,10.46;9,110.48,331.14,193.25,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,370.79,295.27,142.23,10.46;9,110.48,307.22,141.41,10.46">Overview of the clef 2005 multilingual question answering track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Aunimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,336.56,319.17,29.47,10.46">CLEF</title>
		<title level="s" coord="9,440.34,319.17,72.66,10.46;9,110.48,331.14,75.91,10.46">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="307" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,348.54,402.53,12.98;9,110.48,363.02,294.63,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,318.38,351.06,194.63,10.46;9,110.48,363.02,41.52,10.46">Testing the reasoning for question answering validation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,160.55,363.02,151.13,10.46">Journal of Logic and Computation</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007-12">December 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,382.94,402.52,10.46;9,110.48,394.89,402.53,10.46;9,110.48,406.85,165.18,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,394.82,382.94,118.18,10.46;9,110.48,394.89,170.33,10.46">INAOE at AVE 2007: Experiments in spanish answer validation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Téllez-Valero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,308.95,394.89,198.92,10.46">Working Notes for the CLEF 2007 Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,426.77,402.53,10.46;9,110.48,438.73,402.53,10.46;9,110.48,450.69,117.94,10.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,110.48,438.73,182.48,10.46">INAOE&apos;s participation at QA@CLEF 2007</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Téllez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Juárez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Denicia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Villatoro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villaseñor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,316.69,438.73,145.11,10.46">Working Notes for the CLEF 2007</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,468.09,402.52,12.97;9,110.48,482.57,80.79,10.46" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,325.02,470.61,187.98,10.46">Overview of the answer validation exercise</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,499.97,402.52,12.98;9,110.48,514.45,368.65,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,279.12,502.49,188.80,10.46">Overview of the answer validation exercise</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,110.48,514.45,195.01,10.46">Working Notes for the CLEF 2007 Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">2007. September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,534.37,402.54,10.46;9,110.48,546.33,402.54,10.46;9,110.48,558.28,155.01,10.46" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,289.65,534.37,223.37,10.46;9,110.48,546.33,20.76,10.46">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,159.81,546.33,348.56,10.46">Proceedings of Pascal Challenge Workshop on Recognizing Textual Entailment</title>
		<meeting>Pascal Challenge Workshop on Recognizing Textual Entailment<address><addrLine>Southampton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-04">April 2005</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,575.68,402.53,12.98;9,110.48,590.16,103.76,10.46" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,329.36,578.21,183.65,10.46;9,110.48,590.16,41.52,10.46">The effect of entity recognition on answer validation</title>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="483" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,610.08,402.52,10.46;9,110.48,622.04,114.34,10.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,230.78,610.08,228.82,10.46">Answer comparison in automated question answering</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dalmas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">L</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,468.08,610.08,44.93,10.46;9,110.48,622.04,23.38,10.46">J. Applied Logic</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="104" to="120" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,641.96,402.54,10.46;9,110.48,653.93,402.53,10.46;9,110.48,665.88,217.57,10.46" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,369.84,641.96,143.18,10.46;9,110.48,653.93,220.56,10.46">Building on redundancy: Factoid question answering, robust retrieval and the &quot;other</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roussinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Filatova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Robles-Flores</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,360.14,653.93,152.88,10.46;9,110.48,665.88,153.92,10.46">Proceedings of the Thirteenth Text REtreival Conference (TREC 2005)</title>
		<meeting>the Thirteenth Text REtreival Conference (TREC 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="15" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,685.81,402.53,10.46;9,110.48,697.76,402.53,10.46;9,110.48,709.71,383.18,10.46" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,442.11,685.81,70.90,10.46;9,110.48,697.76,296.51,10.46">Improving question answering by combining multiple systems via answer validation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Téllez-Valero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,110.48,709.71,40.27,10.46">CICLing</title>
		<title level="s" coord="9,224.63,709.71,151.70,10.46">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Gelbukh</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">4919</biblScope>
			<biblScope unit="page" from="544" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,110.53,402.53,10.46;10,110.48,122.49,402.55,10.46;10,110.48,134.45,402.53,10.46;10,110.48,146.40,402.53,10.46;10,110.48,158.36,402.53,10.46;10,110.48,170.31,242.18,10.46" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,218.18,122.49,294.84,10.46;10,110.48,134.45,256.73,10.46">Evaluation of Multilingual and Multi-modal Information Retrieval, 7th Workshop of the Cross-Language Evaluation Forum</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,378.90,134.45,50.72,10.46;10,425.01,158.36,23.58,10.46">CLEF 2006</title>
		<title level="s" coord="10,121.82,170.31,151.69,10.46">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Stempfhuber</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">September 20-22, 2006. 2007</date>
			<biblScope unit="volume">4730</biblScope>
		</imprint>
	</monogr>
	<note>CLEF</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
