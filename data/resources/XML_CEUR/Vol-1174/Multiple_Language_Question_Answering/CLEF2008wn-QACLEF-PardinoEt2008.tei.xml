<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,92.59,148.62,418.00,15.51;1,205.59,170.53,191.90,15.51;1,397.41,167.11,5.98,12.22">Adapting IBQAS to work with text transciptions in QAst Task: IBQAst *</title>
				<funder ref="#_pdGMZyT">
					<orgName type="full">Spanish government</orgName>
				</funder>
				<funder ref="#_9pFuVar">
					<orgName type="full">Generalitat Valenciana</orgName>
				</funder>
				<funder ref="#_HSM5KKg">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_MNGXwFP">
					<orgName type="full">European Union</orgName>
					<orgName type="abbreviated">EU</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-08-15">August 15, 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,107.91,204.00,47.59,9.96"><forename type="first">M</forename><surname>Pardiño</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Grupo de Investigación en Procesamiento del Lenguaje Natural y Sistemas de Información Natural Language Processing and Information Systems Group Department of Software and Computing Systems</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,163.46,204.00,50.38,9.96"><forename type="first">J</forename><forename type="middle">M</forename><surname>Gómez</surname></persName>
							<email>jmgomez@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Grupo de Investigación en Procesamiento del Lenguaje Natural y Sistemas de Información Natural Language Processing and Information Systems Group Department of Software and Computing Systems</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.61,204.00,43.79,9.96"><forename type="first">H</forename><surname>Llorens</surname></persName>
							<email>hllorens@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Grupo de Investigación en Procesamiento del Lenguaje Natural y Sistemas de Información Natural Language Processing and Information Systems Group Department of Software and Computing Systems</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.03,204.00,66.72,9.96"><forename type="first">R</forename><surname>Muñoz-Terol</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Grupo de Investigación en Procesamiento del Lenguaje Natural y Sistemas de Información Natural Language Processing and Information Systems Group Department of Software and Computing Systems</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,348.93,204.00,88.44,9.96"><forename type="first">B</forename><surname>Navarro-Colorado</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Grupo de Investigación en Procesamiento del Lenguaje Natural y Sistemas de Información Natural Language Processing and Information Systems Group Department of Software and Computing Systems</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,445.42,204.00,45.08,9.96"><forename type="first">E</forename><surname>Saquete</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Grupo de Investigación en Procesamiento del Lenguaje Natural y Sistemas de Información Natural Language Processing and Information Systems Group Department of Software and Computing Systems</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,198.91,217.95,77.83,9.96"><forename type="first">P</forename><surname>Martínez-Barco</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Grupo de Investigación en Procesamiento del Lenguaje Natural y Sistemas de Información Natural Language Processing and Information Systems Group Department of Software and Computing Systems</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,284.77,217.95,45.01,9.96"><forename type="first">P</forename><surname>Moreda</surname></persName>
							<email>moreda@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Grupo de Investigación en Procesamiento del Lenguaje Natural y Sistemas de Información Natural Language Processing and Information Systems Group Department of Software and Computing Systems</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.48,217.95,51.62,9.96"><forename type="first">M</forename><surname>Palomar</surname></persName>
							<email>mpalomar@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Grupo de Investigación en Procesamiento del Lenguaje Natural y Sistemas de Información Natural Language Processing and Information Systems Group Department of Software and Computing Systems</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,92.59,148.62,418.00,15.51;1,205.59,170.53,191.90,15.51;1,397.41,167.11,5.98,12.22">Adapting IBQAS to work with text transciptions in QAst Task: IBQAst *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-08-15">August 15, 2008</date>
						</imprint>
					</monogr>
					<idno type="MD5">E8BE1ED58C265000301C3F66D0121D41</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper shows the results of adapting a modular domain English QA system (called IBQAS, whose initials correspond to Interchangeable Blocks Question Answering System) to work with both manual and automatic text transcriptions. This system provides a generic and modular framework using an approach based on the recognition of named entities as a method of extracting answers.</p><p>The system architecture follows the general methodology of QA systems incorporating the modules detailed below: analysis of the question, information retrieval and extraction of the answer. In the analysis phase of the system, we extracted the type of question or type of answer expected, keywords and focus. Next, we used JIRS, a traditional Passage Retrieval system which is able to find structures in questions using n-gram models, for the information retrieval process. Finally, we selected the potential answers and those with higher scores were given as result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we will explain the adaptation to the CLEF 2008 QAST (Question Answering on Speech Transcription) track of the Question Answering (QA) system IBQAS previously developed by the University of Alicante <ref type="bibr" coords="1,224.42,568.01,15.51,9.96" target="#b13">[14]</ref>  <ref type="bibr" coords="1,244.20,568.01,14.62,9.96" target="#b12">[13]</ref>, and we will report our official evaluation results in the frame of this CLEF 2008 QAST track.</p><p>In order to perform the first participation of the University of Alicante in the CLEF 2008 QAST track, only the Question Answering process over manual and automatic transcriptions of European Parliament Plenary sessions in English (EPPS English corpus) has been carried out. So, the goal of the QAST process is to extract the correct answer to factual and definition questions over these recordings from the European Parliament. Nevertheless, we will only deal here with factual questions.</p><p>With the aim of performing the QAST process, the applied QA system is functionally structured in three QA tasks: question analysis, retrieval of relevant passages from automatic and manual speech transcripts and answer extraction.</p><p>The application of this QA system to the QAST process is explained in the following sections of the paper. So, next section presents the state-of-the-art of the systems that also perform QAST process. Section number three details the core of the QAST-based system and its application to the QAST process. Section number four shows the results obtained by the system according to the CLEF 2008 QAST evaluation track. Finally, the last section details the conclusions and further works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">QAST background</head><p>More concretely, in the state of the art for QAST task in CLEF 2007 there are five main systems:</p><p>• University of Catalonia <ref type="bibr" coords="2,219.72,271.31,14.61,9.96" target="#b15">[16]</ref>: This research group participated with two systems in the four sub-tasks. Its main feature was that the systems made minimal use of syntactic analysis and used a data-driven query relaxation algorithm to extract the best answer context from the input question. The difference between the two systems was that one was tailored for manual transcripts, while the other was tailored for automatic transcripts. In all four sub-tasks they obtained the best performance with the system that was initially designed for manual transcripts. Although the system designed for automatic transcripts performed worse than expected, this approach is a good long-term research direction because it is the only one of the two systems developed that can truly address the specific phenomena of automatic transcripts. Their best performing runs have TOP1 scores that range from 0.21 (on automatic transcripts with WER of 38%) to 0.51 (on manual transcripts).</p><p>• LIMSI participation <ref type="bibr" coords="2,203.49,410.65,14.61,9.96" target="#b17">[18]</ref>: This group presented two different QA systems based on a complete and multilevel analysis of both queries and documents. The main changes between both systems were the replacement of the large set of hand-made rules by the automatic generation of a research descriptor, and the addition of an efficient scoring of the candidate answers. The evaluation of the systems showed that, on transcribed lectures, the best Accuracy result was 39% for manual and 21.3% for automatic, and, on transcribed meetings was 24% for manual and 18.3% for automatic.</p><p>• AnswerFinder <ref type="bibr" coords="2,178.49,502.17,9.96,9.96" target="#b1">[2]</ref>: This contribution was centered on a study of Named Entity (NE) recognition on speech transcripts, and how such NE recognition impacts on the accuracy of the final question answering system. The NE recognizer (AFNER) of the AnswerFinder questionanswering project was ported to the types of answer expected in the QAst track. They participated in all QAst sub-tasks with two runs per task. Their conclusions were that the small training corpus and the presence of annotation errors in the AMI corpus made the machine learning component of AFNER ineffective. Nevertheless, the system was second (out of three participants) in one of the QAst subtasks (Task 3) with 19.77% accuracy for the second run.</p><p>• Tokyo Institute of Technology <ref type="bibr" coords="2,250.39,617.60,10.52,9.96" target="#b2">[3]</ref> presented a QAST system based on non-linguistic, datadriven approach with a noisy channel model. This system had two modules: the first one is a IR system with a sentence-based retrieval approach. The corpus was pre-processed with simple text processing: fillers and pauses removing, etc. The second module was the answer extractor: the best answer was extracted with the maximum probability based on Bayes' rules. The system was fourth in the QAst subtasks 1 with 0.20% of MRR.</p><p>• Finally, as previous papers, Neumann and Wang <ref type="bibr" coords="2,332.04,697.16,15.51,9.96" target="#b14">[15]</ref> adapted a previous open-domain QA system to the specific task of QA in speech transcription: QAst-v1. This system was developed for factual questions, and it was based on a NER system. They pre-process the speech transcription corpus with automatic annotation of sentence boundaries, chunk structures (based on dependency analysis) and Named Entities. Furthermore, they analyzed questions with shallow dependency structures, NE recognition and expected answer type. For the location of candidates answers they used redundancy, filtered by the correspondence between the named entities of possible answers and the expected answer type. Ill-formed candidates answers were deleted by manually specified rules. They run the system in two subtasks: T1 and T2. In the first one achieved 0.15 accuracy, and in the second one 0.09.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Description of the System</head><p>This work shows the results of adapting a modular domain English QA system IBQAS based on the proposal of Pizzato <ref type="bibr" coords="3,210.39,225.90,15.51,9.96" target="#b16">[17]</ref> to work with text transcripts both manual and automatic. This system provides a generic and modular framework using an approach based on the recognition of named entities as a method of extracting answers.</p><p>The system architecture follows the general methodology of QA systems incorporating the modules detailed below: analysis of the question, information retrieval and extraction of the answer.</p><p>In the analysis phase of the system, we extracted the type of question or type of answer expected (by means of patterns like a question type taxonomy previously defined), keywords (verb main phrases without nominal stopwords, denials) and focus (it describes the type of answer expected when it is not possible to infer it from the interrogative particle, but it rarely appears in the sentence where the answer is, so it should be removed from the list of keywords).</p><p>For the information retrieval process, we did not use the IR module incorporated in the original IBQAS but we adapted JIRS. JIRS is an IR system able to find structures in question using ngram models. It uses a traditional Passage Retrieval system and searches each n-gram of the question in the retrieved passages. Afterwards, it rates them depending on quantity and weight of the n-grams of these passages.</p><p>Finally, relevant documents are filtered and potential answers are extracted from them (using Lingpipe to recognize location, person and organization entities; TERSEO for temporal expressions and patterns to recognize other types of entities such as numeric entities, languages ...). The last step consists on scoring and sorting the responses obtained to select several of them according to the distance between each response and the keywords, as well as the mutual information of the bigrams and trigrams of the passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Analysis</head><p>The importance of this module relies on the fact that the success of all the other parts of the system depends on it. Its goal is to extract any relevant information from the question. More specifically, the presented system extracts the question type, the focus and the keywords (see figure <ref type="figure" coords="3,90.00,571.05,3.87,9.96">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Question Type</head><p>The question type indicates the expected answer type. For instance, the question ¿Where is the Eiffel Tower? is expecting a location as answer. In this way, the goal of this section is to classify the questions among the different types, given a taxonomy. Several taxonomies have been developed with this purpose. Normally, these taxonomies are hierarchically organized offering different granularities (coarse or fine) <ref type="bibr" coords="3,262.91,663.15,15.51,9.96" target="#b10">[11,</ref><ref type="bibr" coords="3,283.23,663.15,7.01,9.96" target="#b7">8]</ref>. Regarding the classification method, two main types can be considered. On the one hand, methods based on Regular Expressions (REs) or patterns <ref type="bibr" coords="3,124.38,687.07,17.19,9.96" target="#b10">[11,</ref><ref type="bibr" coords="3,145.66,687.07,7.76,9.96" target="#b6">7]</ref> are faster but have a lower recall and a higher development cost. On the other hand, methods based on machine learning <ref type="bibr" coords="3,276.29,699.02,10.52,9.96" target="#b7">[8]</ref> are slower but offer, normally, a higher recall and a lower development cost.</p><p>The presented system uses a set of REs in order to determine the question type. The implemented patterns represent an extension of the ones used by Molla in AnswerFinder <ref type="bibr" coords="3,446.95,734.89,18.62,9.96" target="#b11">[12]</ref>. The REs Figure <ref type="figure" coords="4,250.17,374.27,3.87,9.96">1</ref>: Configuration of our system are ordered by relevance. Therefore, if one question matches with more than one RE the system returns the first one, that is, the most relevant one. Table <ref type="table" coords="4,356.97,418.05,4.98,9.96" target="#tab_0">1</ref> shows the taxonomy used that is based on Li and Roth taxonomy <ref type="bibr" coords="4,225.77,430.01,14.05,9.96" target="#b7">[8]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Question Keywords</head><p>The relevant words of a question are considered keywords. The presented system considers as keywords:</p><p>• Main verb of the question</p><p>• Nominal phrases, once stopwords are removed (the, of, etc.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Negations</head><p>As implementation, a syntactical analysis of dependencies is carried out using MINIPAR 1 [9] in order to select keywords:</p><p>1. Using MINIPAR information, the system extracts: MINIPAR detects multiwords and the presented system takes advantage of this feature treating them as single words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Question Focus</head><p>Question focus describes the expected answer type when this type can not be deduced from the question word, that is, in What/Which type questions. Furthermore, it is an expression that normally does not appear in sentences containing the answer. The method to detect the question focus consists in extracting the expression that follows or is related to the question word <ref type="bibr" coords="5,473.81,378.03,12.99,9.96" target="#b5">[6]</ref>. The presented system uses this method to detect question focus and it omits it from the question keywords list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Description of the JAVA Information Retrieval System</head><p>JAVA Information Retrieval System (JIRS) is an IR system specially adapted to retrieve passages. Our Passage Retrieval (PR) system is based on searching the question structures rather than just the keywords, and it makes a comparison between them. JIRS uses a traditional search engine as the first step and then searches all possible n-grams of the question in the retrieved passages and rates them depending on the number and the weight of the n-grams that appeared in these passages. The system architecture is shown in Fig. <ref type="figure" coords="5,315.83,507.81,3.87,9.96" target="#fig_1">2</ref>.</p><p>JIRS is based on searching the heaviest n-grams (i.e., those with the greatest weight) instead of the longest one using the Distance Density n-gram model <ref type="bibr" coords="5,340.06,531.73,9.96,9.96" target="#b4">[5]</ref>. With this model, the final similarity is obtained by multiplying the n-gram weight by a distance factor that takes into account the distance with respect to the heaviest n-gram. Therefore, the similarity value depends on the density of question terms in the passage, and it is calculated as the sum of all n-gram weights, multiplied by the distance factor and divided by the sum of all term weights of the question. The equation we have used is the following:</p><formula xml:id="formula_0" coords="5,212.66,610.87,300.34,36.77">Sim(p, q) = 1 n i=1 w i • ∀x∈ P h(x) 1 d(x, x max )<label>(1)</label></formula><p>Let p be the set of n-grams composed by passage terms and Q be the set of n-grams of p composed only by question terms. Therefore, we define P = {x 1 , x 2 , ..., x M } as a sorted subset of Q that fulfills the following conditions: where T (x) is the set of terms of the n-gram x, and h(x) is the function which measure the n-gram weight and it is defined by the Equation (2):</p><formula xml:id="formula_1" coords="5,102.18,697.80,218.89,58.69">1. ∀x i ∈ P : h(x i ) ≥ h(x i+1 ) i ∈ {1, 2, ..., M -1} 2. ∀x, y ∈ P : x = y ⇒ T (x) T (y) = ∅ 3. min x∈ P h(x) ≥ max y∈ Q- P h(y)</formula><formula xml:id="formula_2" coords="6,271.17,411.68,241.83,31.24">h(x) = j k=1 w k<label>(2)</label></formula><p>where w 1 , w 2 , ..., w j are the term weights of the j-gram x = t 1 t 2 ...t j . These weights should penalize the terms that appear frequently in the document collection (e.g. stopwords) and promote the relevant words (i.e. the question terms that are of crucial importance to retrieve a relevant passage). The following function was introduced to assign the weight to a term:</p><formula xml:id="formula_3" coords="6,255.64,505.19,257.36,23.54">w k = 1 - log(n k ) 1 + log(N )<label>(3)</label></formula><p>where n k is the number of passages in which the term t k appears, and N is the number of system passages. We assume that stopwords occur in every passage (i.e., n k takes the value of N ). For instance, if the term t k occurs only once in the passage collection, its weight will be equal to 1 (the greatest weight). However, if it is a stopword, its weight will be the lowest one.</p><p>The importance of the term proximity weighting has already been addressed in IR <ref type="bibr" coords="6,468.98,582.97,15.46,9.96" target="#b19">[20]</ref> which describes how lexical cohesion between query terms in documents might be used in document ranking. From our perspective, the simplest measure of distance between two n-grams could be defined as the number of terms between them. Nevertheless, this function has the disadvantage that it grows linearly and, therefore, the weight of the n-gram decreases too fast with respect to its distance from the heaviest n-gram. In order to address this issue, we use a logarithmic distance instead of the linear one. The distance function we have used is the following:</p><formula xml:id="formula_4" coords="6,237.36,678.40,271.41,10.61">d(x, x max ) = 1 + k • ln(1 + L) (<label>4</label></formula><formula xml:id="formula_5" coords="6,508.76,678.62,4.24,9.96">)</formula><p>where L is the number of terms between the n-gram x max (x max is the n-gram with the maximum weight calculated in the Equation ( <ref type="formula" coords="6,298.33,708.51,4.15,9.96" target="#formula_2">2</ref>)) and the n-gram x of the passage. We have introduced the k constant to adjust the importance of the distance in the similarity equation. In previous experiments, we have determined that the best score for this value is 0.1. The other added constants are used to avoid the infinities when L is equal to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer Extraction</head><p>The aim of this module is to determinate which parts of the selected information are potential answers for the question formulated by the user. The main difference between this module and the previous one is that here we are looking at concrete pieces of information, exact answers. Once extracted the feasible answers for the question, they are scored and reranked with the aim of selecting one of them as the final answer.</p><p>Figure <ref type="figure" coords="7,137.16,189.52,4.98,9.96">1</ref> shows the architecture used by the system in this module. The implementation is divided into three main processes:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Candidate Answer Detection</head><p>The presented system uses Named Entity Recognizers (NERs) in order to detect candidate answers. To do this, the system uses different NERs to detect and classify different Named Entity (NE) types according to different question types.</p><p>1. Named Entity Recognition: Used NERs.</p><p>• Location, Person and Organization (Lingpipe). To be able to detect answers for Person, Organization or Location type questions, the presented system uses Lingpipe<ref type="foot" coords="7,472.07,320.62,3.97,5.52" target="#foot_0">2</ref> by Carpenter y Baldwin. Lingpipe was evaluated with ConLL 2002 data obtaining a 77.29% F1.</p><p>• Temporal Expressions (TERSEO). Due to the spareness of temporal expression recognition feature in most NERs, a specialized NER to recognize and normalize temporal expressions was used. The presented system uses TERSEO<ref type="foot" coords="7,395.21,383.87,3.97,5.52" target="#foot_1">3</ref>  <ref type="bibr" coords="7,403.01,384.71,15.50,9.96" target="#b18">[19]</ref> to do this.</p><p>• Other entities. In order to detect other types of entities like Number, the system uses specific patterns of intern and extern evidences following the steps of Mikheev, Moens y Grover <ref type="bibr" coords="7,178.47,424.05,14.61,9.96" target="#b9">[10]</ref>.</p><p>2. Candidate Answer Selection Process: Knowing the question type, and therefore the entity type of the answer the system follows the following steps.</p><p>(a) It executes the corresponding NER over the information obtained by the IR module.</p><p>(b) Once entities are tagged, it removes every entity of non searched types.</p><p>(c) It omits entities corresponding to keywords.</p><p>(d) It applies filters in order to remove punctuation symbols, blanks or stopwords<ref type="foot" coords="7,476.39,520.29,3.97,5.52" target="#foot_2">4</ref> tagged as candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Candidate Answer Scoring and Reranking</head><p>The presented system uses different techniques to score and rerank obtained answers. The scoring techniques used are:</p><p>1. Answer-Keywords Distance This method assumes that the closer the keywords are from candidate answers the better the method is. Formula 5 shows the way this distance is calculated.</p><formula xml:id="formula_6" coords="7,261.32,656.91,251.68,30.53">D(A) = n i=1 δ(A, f i ) -1 n<label>(5)</label></formula><p>Defining δ(a, b) as the number of words between a and b, and F = f 1, f 2, ..., f n as the list of keywords, distance function D for a concrete candidate answer A, is defined as shown in formula 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Mutual Information and WordNet relationships</head><p>As an additional scoring method, the presented system uses Mutual Information (MI) <ref type="bibr" coords="8,499.74,159.17,13.27,9.96" target="#b3">[4]</ref> of bigrams and trigrams increasing the score of the answers that has a high MI in direct relationship with the keywords or in indirect relationship through WordNet (WN) with the keywords. M I(a, b) of two words, a bigram, a and b is calculated as shown in formula 6.</p><formula xml:id="formula_7" coords="8,265.11,219.44,247.88,23.54">I(a, b) = log P (a, b) P (a)P (b)<label>(6)</label></formula><p>The probability of an n-gram is calculated as described in formula 7.</p><formula xml:id="formula_8" coords="8,260.25,277.80,252.74,23.54">P (x) = f req(x) total n -grams<label>(7)</label></formula><p>The probability P of a concrete n-gram x is calculated by dividing its frequency in the text f req(x) by the total of n-grams in the text total ngrams.</p><p>The presented system calculates the MI of the bigrams and trigrams once stopwords 4 are removed from the text. Only n-grams repeated more than 5 times are taken into account in order to smooth the imprecision caused by the spareness of data in MI <ref type="bibr" coords="8,426.98,362.99,9.96,9.96" target="#b0">[1]</ref>.</p><p>The relevant n-grams are considered using the following steps:</p><p>(a) Remove n-grams composed only of keywords.</p><p>(b) Remove n-grams not containing any of the candidate answers.</p><p>(c) The rest are scored as follows:</p><p>• Number of keywords they contain.</p><p>• Number of words in the n-gram that have any relationship with keywords. To perform this the system uses WN information.</p><p>(d) Additional Score: AS = (M I * (numkws + numrels + 1))/20</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Answer Clustering</head><p>Before determining which are the best answers, the presented system carries out an Answer Clustering process. Equal answers or overlapped answers are grouped adding its scores to the most scored one. Once an ordered list of grouped and rescored answers is obtained, the final answers are those with the highest score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>Finally, we present the results obtained with our system in Qast task. We sent one manual run (for task T4a) and three automatic runs, one for each existing automatic transcriptions (for task T4b) all working with EPPS English corpus. The results are shown in Tables <ref type="table" coords="8,431.44,638.93,4.98,9.96">2</ref> and<ref type="table" coords="8,459.07,638.93,3.87,9.96" target="#tab_3">3</ref>. Specifically, Tables <ref type="table" coords="8,174.97,650.89,4.98,9.96">2</ref> shows the number of wrong answers (W), unsupported answers (U), inexact answers (X) and right answers(R) in the first answer returned for each question given to our system.</p><p>Such us we expected, the best results have been obtained with the manual transcription. This is due to the fact that this transcription has fewer errors than automatic transcriptions because most of the problems have been checked manually. Table <ref type="table" coords="9,221.17,170.97,3.87,9.96">2</ref>: Results obtained by our system at QAst Table <ref type="table" coords="9,131.93,212.76,4.98,9.96" target="#tab_3">3</ref> shows the comparative of the values obtained for MRR and Accuracy with the manual run and with the three automatic runs. MRR is the Mean Reciprocal Rank measures, that is to say how well ranked is the right answer in the list of 5 possible answers in average, while Accuracy is the fraction of correct answers ranked in the first position in the list of 5 possible answers. With regard to the automatic transcripts, the best results were obtained with the transcript C versus transcription B (although these are very close to those of B) and transcription C (these are slightly smaller).  In addition, to explain the results obtained, we must not forget the problems arisen in the development of this work. On the one hand, the small size of the corpus, and hence, the consequent low redundancy in them, made difficult to adapt our system. On the other hand, the existence of broad types of questions made not possible to cover them in our system (we only dealt with factual questions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In our first participation in QAst, we have adapted a generic and modular QA system to work with text transcriptions. We want to highlight that its results are above expectation because we did not use any specific resource to deal with automatic transcriptions. Despite using a generalist system, the results are not discouraging. Nevertheless, we want to compare our results with those obtained by the rest of the participants to be able to give an opinion. In the future, we hope to obtain a better system capable of answering questions from the task in a more precise way and we wish to measure the improvements we introduce in our system compared to the state-of-art at the moment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,101.09,735.66,3.65,4.16;4,105.25,736.53,181.13,7.97;5,126.86,111.14,80.31,10.18;5,126.86,126.66,76.99,10.18;5,126.86,142.19,120.99,10.18;5,126.86,157.71,91.96,10.18;5,126.86,173.23,52.89,10.18;5,102.18,192.96,219.52,9.96;5,126.86,212.25,234.43,10.18;5,126.86,227.78,238.32,10.18;5,102.18,247.51,222.34,9.96;5,102.18,267.01,164.19,9.96"><head>1</head><label></label><figDesc>http://www.cs.ualberta.ca/∼lindek/minipar.htm • Subject Nucleus • Object Nucleus • Nominal Phrases Nucleus • Main verb Nucleus • Negations 2. Each item of the previous list is extended with: • Expressions related through prepositional modifiers • Expressions related through subordination modifiers 3. Duplicate entries are removed from keyword list 4. Focus is omitted from keyword list</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,147.90,336.81,307.20,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The main architecture of the JIRS n-gram based PR system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,200.13,461.33,202.73,74.91"><head>Table 1 :</head><label>1</label><figDesc>Coarse grain question type taxonomy</figDesc><table coords="4,236.03,483.49,130.94,52.74"><row><cell cols="2">Question Type Taxonomy</cell></row><row><cell>Person (Human)</cell><cell>Temporal</cell></row><row><cell>Number</cell><cell>Organization</cell></row><row><cell>Location</cell><cell>Language</cell></row><row><cell>Definition</cell><cell>Unknown</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,193.76,388.45,215.48,9.96"><head>Table 3 :</head><label>3</label><figDesc>Results obtained by our system at QAst</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="7,105.25,727.46,124.16,7.97"><p>http://www.alias-i.com/lingpipe/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="7,105.25,736.96,150.79,7.97"><p>http://gplsi.dlsi.ua.es/∼stela/TERSEO/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="7,105.25,746.47,245.68,7.97"><p>http://dev.mysql.com/doc/refman/5.0/en/fulltext-stopwords.html</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>* This paper has been partially supported by the <rs type="funder">Spanish government</rs>, project <rs type="grantNumber">TIN-2006-15265-C06-01</rs> and project <rs type="grantNumber">GV06028</rs>, by the framework of the project <rs type="projectName">QALL-ME</rs>, which is a <rs type="programName">6th Framework Research Programme of</rs> the <rs type="funder">European Union (EU)</rs>, contract number: <rs type="grantNumber">FP6-IST-033860</rs>, and by the <rs type="funder">Generalitat Valenciana</rs> throught the research grant <rs type="grantNumber">BFPI/2008/093</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pdGMZyT">
					<idno type="grant-number">TIN-2006-15265-C06-01</idno>
				</org>
				<org type="funded-project" xml:id="_MNGXwFP">
					<idno type="grant-number">GV06028</idno>
					<orgName type="project" subtype="full">QALL-ME</orgName>
					<orgName type="program" subtype="full">6th Framework Research Programme of</orgName>
				</org>
				<org type="funding" xml:id="_9pFuVar">
					<idno type="grant-number">FP6-IST-033860</idno>
				</org>
				<org type="funding" xml:id="_HSM5KKg">
					<idno type="grant-number">BFPI/2008/093</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.48,662.80,402.54,9.96;9,110.47,674.76,260.58,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,293.55,662.80,219.46,9.96;9,110.47,674.76,52.13,9.96">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName coords=""><forename type="first">Kenneth</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,171.38,674.76,113.07,9.96">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,694.69,402.52,9.96;9,110.48,706.64,233.91,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,285.65,694.69,227.35,9.96;9,110.48,706.64,119.57,9.96">Answerfinder at qast 2007: Named entity recognition for qa on speech transcripts</title>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cassidy</forename><forename type="middle">D</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Zaanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,251.03,706.64,61.48,9.96">QAst at CLEF</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,726.57,402.51,9.96;9,110.48,738.52,249.50,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,334.38,726.57,178.61,9.96;9,110.48,738.52,134.86,9.96">Clef 2007 question answering experiments at tokyo institute of technology</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">W D</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,266.61,738.52,61.48,9.96">QAst at CLEF</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,111.36,390.91,9.96" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,172.49,111.36,123.48,9.96">Transmission of Information</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Fano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961">1961</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,130.82,402.55,9.96;10,110.48,142.77,402.54,9.96;10,110.48,154.73,402.53,9.96;10,110.48,166.69,368.93,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,175.99,142.77,281.16,9.96">Language independent passage retrieval for question answering</title>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Manuel Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Montes-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emilio</forename><surname>Sanchis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,484.22,142.77,28.79,9.96;10,110.48,154.73,319.48,9.96">Fourth Mexican International Conference on Artificial Intelligence MICAI 2005</title>
		<title level="s" coord="10,438.92,154.73,74.08,9.96;10,110.48,166.69,77.10,9.96">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Monterrey, Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag GmbH</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="816" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,186.15,402.52,9.96;10,110.48,198.11,402.51,9.96;10,110.48,210.07,165.21,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,391.71,198.11,121.27,9.96;10,110.48,210.07,78.34,9.96">Falcon: Boosting knowledge for answer engines</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Razvan</forename><forename type="middle">C</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Vasile Rus</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Morarescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,209.99,210.07,34.03,9.96">TREC-9</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,229.53,402.52,9.96;10,110.48,241.49,196.13,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,474.21,229.53,38.79,9.96;10,110.48,241.49,108.91,9.96">Question answering in webclopedia</title>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurie</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Junk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,240.90,241.49,34.03,9.96">TREC-9</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,260.95,402.52,9.96;10,110.48,272.91,402.51,9.96;10,110.48,284.87,279.00,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,220.21,260.95,123.52,9.96">Learning question classifiers</title>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,369.06,260.95,143.93,9.96;10,110.48,272.91,280.26,9.96;10,171.15,284.87,213.14,9.96">Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002)</title>
		<meeting>the 19th International Conference on Computational Linguistics (COLING 2002)<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-08">August 2002</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics (ACL)</note>
</biblStruct>

<biblStruct coords="10,110.48,304.33,402.52,9.96;10,110.48,316.29,100.32,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,171.25,304.33,177.90,9.96">Dependency-based evaluation of minipar</title>
		<author>
			<persName coords=""><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,374.04,304.33,138.96,9.96;10,110.48,316.29,69.65,9.96">Workshop on the Evaluation of Parsing Systems</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,335.75,402.52,9.96;10,110.48,347.71,167.15,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,357.29,335.75,155.71,9.96;10,110.48,347.71,41.82,9.96">Named entity recognition without gazetteers</title>
		<author>
			<persName coords=""><forename type="first">Andrei</forename><surname>Mikheev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Moens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Claire</forename><surname>Grover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,173.37,347.71,24.05,9.96">EACL</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,367.18,402.52,9.96;10,110.48,379.13,402.53,9.96;10,110.48,391.08,402.55,9.96;10,110.48,403.05,205.24,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,227.44,379.13,168.99,9.96">Lasso: A tool for surfing the answer net</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Goodrum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,415.85,379.13,97.16,9.96;10,110.48,391.08,175.39,9.96">Proceedings of The 8th Text REtrieval Conference (TREC 1999)</title>
		<meeting>The 8th Text REtrieval Conference (TREC 1999)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-11">November 1999</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,422.51,402.52,9.96;10,110.48,434.46,140.69,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,187.04,422.51,106.26,9.96">AnswerFinder in TREC</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mollá-Aliod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,341.03,422.51,171.97,9.96;10,110.48,434.46,110.17,9.96">Proceedings of the 12th Text REtrieval Conference (TREC 2003)</title>
		<meeting>the 12th Text REtrieval Conference (TREC 2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,453.93,402.53,9.96;10,110.48,465.89,402.52,9.96;10,110.48,477.84,330.78,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,339.54,453.93,173.47,9.96;10,110.48,465.89,186.91,9.96">Automatic generalization of a qa answer extraction module based on semantic roles</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Moreda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Llorens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Saquete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,320.10,465.89,192.89,9.96;10,110.48,477.84,201.60,9.96">Proceedings of the 11th edition of the Ibero-American Conference on Artificial Intelligence</title>
		<meeting>the 11th edition of the Ibero-American Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>To be published</note>
</biblStruct>

<biblStruct coords="10,110.48,497.31,402.52,9.96;10,110.48,509.26,402.54,9.96;10,110.48,521.22,22.68,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,348.36,497.31,164.64,9.96;10,110.48,509.26,100.87,9.96">The influence of semantic roles in qa: A comparative analysis</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Moreda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Llorens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Saquete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,233.87,509.26,172.53,9.96">Actas del XXIV Congreso de la SEPLN</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>To be published</note>
</biblStruct>

<biblStruct coords="10,110.48,540.69,402.51,9.96;10,110.48,552.64,198.96,9.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,235.82,540.69,277.16,9.96;10,110.48,552.64,84.59,9.96">Dfki-lt at qast 2007: Adapting qa components to mine answers in speech trancripts</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,216.07,552.64,61.48,9.96">QAst at CLEF</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,572.11,402.52,9.96;10,110.48,584.06,228.01,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,276.70,572.11,236.30,9.96;10,110.48,584.06,113.67,9.96">Robust question answering for speech transcripts using minimal syntactic analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Comas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,245.13,584.06,61.48,9.96">QAst at CLEF</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,603.54,402.52,9.96;10,110.48,615.49,402.52,9.96;10,110.48,627.44,204.08,9.96" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,296.25,603.54,216.76,9.96;10,110.48,615.49,75.86,9.96">Extracting Exact Answers using a Meta Question answering System</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Sangoi Pizzato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mollá-Aliod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,209.43,615.49,303.57,9.96;10,110.48,627.44,45.41,9.96">Proceedings of the Australasian Language Technology Workshop 2005 (ALTW05)</title>
		<meeting>the Australasian Language Technology Workshop 2005 (ALTW05)<address><addrLine>Sidney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-12">December 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,646.91,402.53,9.96;10,110.48,658.87,93.37,9.96" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,321.61,646.91,173.97,9.96">The limsi participation to the qast track</title>
		<author>
			<persName coords=""><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adda</forename><forename type="middle">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Galibert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bilinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.48,658.87,61.49,9.96">QAst at CLEF</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,678.34,402.53,9.96;10,110.48,690.29,402.51,9.96;10,110.48,702.24,46.62,9.96" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="10,165.43,678.34,347.58,9.96;10,110.48,690.29,31.05,9.96">Temporal information Resolution and its application to Temporal Question Answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Saquete</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
		<respStmt>
			<orgName>Departamento de Lenguages y Sistemas Informáticos. Universidad de Alicante</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Phd</note>
</biblStruct>

<biblStruct coords="10,110.48,721.71,402.53,9.96;10,110.48,733.67,265.00,9.96" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,313.83,721.71,199.18,9.96;10,110.48,733.67,56.46,9.96">Lexical cohesion and term proximity in document ranking</title>
		<author>
			<persName coords=""><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Murat</forename><surname>Karamuftuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,175.81,733.67,92.32,9.96">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1485" to="1502" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
