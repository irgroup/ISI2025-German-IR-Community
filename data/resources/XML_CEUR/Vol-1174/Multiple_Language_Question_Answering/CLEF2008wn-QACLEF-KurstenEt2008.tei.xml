<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,78.29,110.86,455.42,15.15;1,284.43,132.78,43.15,15.15">QA Extension for Xtrieval: Contribution to the QAst track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.46,166.68,55.33,8.74"><forename type="first">Jens</forename><surname>Kürsten</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. Computer Science and Media</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>09107</postCode>
									<settlement>Chemnitz</settlement>
									<country>Germany [</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.83,166.68,72.58,8.74"><forename type="first">Holger</forename><surname>Kundisch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. Computer Science and Media</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>09107</postCode>
									<settlement>Chemnitz</settlement>
									<country>Germany [</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.12,166.68,70.43,8.74"><forename type="first">Maximilian</forename><surname>Eibl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. Computer Science and Media</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>09107</postCode>
									<settlement>Chemnitz</settlement>
									<country>Germany [</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,78.29,110.86,455.42,15.15;1,284.43,132.78,43.15,15.15">QA Extension for Xtrieval: Contribution to the QAst track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4D7821400937780B42FF0F761A468F49</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval Measurement</term>
					<term>Performance</term>
					<term>Experimentation Question answering</term>
					<term>Manual Speech Transcripts</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes our first participation at the QAst task of the CLEF campaign 2008. We submitted 4 experiments in total, two for each subtask t1 and t4. These subtasks employed manual speech transcription collections. We used the Stanford Named Entity Recognizer for tagging named entities and the CRFTagger -Conditional Random Fields Part-of-Speech (POS) Tagger for English. The passage retrieval was done with the Xtrieval framework and its Apache Lucene implementation. For the classification of the question hand-crafted patterns were implemented. Our experiments achieved an accuracy of about 20%. The rate of returned NIL answers was too high for all of our experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This article describes the design, architecture and evaluation of a prototype extension of the Xtrieval framework <ref type="bibr" coords="1,94.38,574.85,9.96,8.74" target="#b1">[2]</ref>. It is used for the participation at the QAst task of the CLEF 2008 campaign. The task concerns with two main types of questions: factual questions (ca. 70%) and definition questions (ca. 20%). The remaining proportion of 10% contains questions that could not be answered by querying the provided collections. The organizers of the task defined 10 types of named entities and they divided the definitional questions into 4 subsets (see QAst task guideline<ref type="foot" coords="1,211.52,621.10,3.97,6.12" target="#foot_0">1</ref> ).</p><p>We developed our prototype with respect to the restrictions and assumptions of the task. The prototype includes all classical components of a QA system: (a) question classification, (b) passage retrieval, (c) answer extraction and (d) natural language processing (NLP) components for named entity recognition (NER) and a part-of-speech (POS) tagger. Since we are newcomers to the topic of QA, we decided to start with two simpler tasks (t1 and t4) on manual speech transcriptions. These collections allow a rather simpler strategy for the identification of the right answer to a factual or definitional question, because no phonetic analysis is necessary.</p><p>The remainder of the article is organized as follows. In section 2 we describe our system and its architecture. Section 3 shows the results of our submitted experiments. A summary of the result analysis is given in section 4. The final section concludes our experiments with respect to our expectations and gives and outlook to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Architecture</head><p>The main architecture of our prototype is illustrated in figure <ref type="figure" coords="2,337.85,188.57,3.87,8.74">1</ref>. We use the Xtrieval framework with Apache Lucene<ref type="foot" coords="2,101.45,198.95,3.97,6.12" target="#foot_1">2</ref> as core implementation to retrieve answer candidates from our passage index. Both questions and collections are preprocessed with two kinds of taggers. For the selection of the final answer(s) we use: (a) the RSV's of the returned passages, (b) the class the current question belongs to and (c) the NE and POS tags from question and collection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Collection and Question Preprocessing</head><p>A NLP component could be beneficial for running experiments for the QAst task with factual and definitional questions, because with the help of this tagger some answers to the question could be identified directly by searching the collection for specific tags. Since many NLP systems exist in the community and also because we are not experts in the NLP topic, we decided to compare different systems. We also investigated whether the systems were adaptable to our special needs for the task. For a more robust QA system we thought it might be useful to integrate both a NER and a POS tagger. The prerequisite for both systems was Java compatibility, because our prototype and the Xtrieval framework are implemented in Java. We did not bother when the support for Java was provided by a Java API. In the two following subsections we go into the details of the tagging systems we applied in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">NER Tagger</head><p>In our system we used the Stanford Named Entity Recognizer<ref type="foot" coords="3,342.79,188.47,3.97,6.12" target="#foot_2">3</ref> (SNER) for named entity recognition. It is used for fast extraction of answers to questions, that imply a certain type of named entity <ref type="bibr" coords="3,466.28,202.00,9.96,8.74" target="#b0">[1]</ref>. We chose the SNER tagger, because it contains predefined classifiers that are very similar to the entities that are defined in the task description. Unfortunately, it does not cover all the defined entities, but it supports three types: (a) person (PER), (b) organization (ORG) and (c) location (LOC). Additionally, the SNER tagger has another classifier called MISC for entities that could not be assigned to a specific class. In some preliminary test the SNER tagger achieved quite formidable recognition rates. Table <ref type="table" coords="3,353.47,261.77,4.98,8.74" target="#tab_1">1</ref> illustrates the statistics of named entities that were assigned with the SNER tagger. It shows the distribution of the recognized entities and the rate (NE2TR) of all recognized tags and the number of unique terms (UT) for the two collections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">POS Tagger</head><p>Besides the NER tagger, a POS tagger is also a vital component in our system. This is due to the fact that almost every step depends on the extraction of particular lexical categories. During the query formulation procedure nouns are extracted or during search for measures we look for adjectives just to give a few examples. We use the CRFTagger -Conditional Random Fields Part-of-Speech (POS) Tagger for English <ref type="bibr" coords="3,491.56,454.60,10.52,8.74" target="#b2">[3]</ref> for POS tagging. It is free, implemented in Java and achieves a high recognition rate <ref type="foot" coords="3,401.02,464.98,3.97,6.12" target="#foot_3">4</ref> . A comparison to the Stanford Log-linear Part-Of-Speech Tagger<ref type="foot" coords="3,217.14,476.94,3.97,6.12" target="#foot_4">5</ref>  <ref type="bibr" coords="3,225.18,478.51,10.20,8.74" target="#b4">[5]</ref>, <ref type="bibr" coords="3,238.78,478.51,10.20,8.74" target="#b3">[4]</ref> on the test collections showed marginal better performance of the CRFTagger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question Classification</head><p>In this step the question is analyzed to decide whether the question is a factual or a definitional one. Therefore we simply apply a hand-crafted pattern to the interrogative. If the question starts with what/who is/are we regard the question as definitional question. Otherwise we assume the current question is a factual question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Question Analysis</head><p>The types of the named entities are also very important for the analysis of the question itself. The question could be classified by using the 10 types of entities defined in the task guideline. Due to that fact, we try to assign at least one type of entities to the question by using hand-crafted patterns that contain special words. Again to give an easy example one might imagine that a question starting with how many/much/long/old is most likely to be answered with a measure. We defined a number of these patterns to help estimating what might be a right answer to the question. There are a number of easy types of entities in the group of the 10 entities proposed in the guideline, like numbers or locations. But there are also a number of hard entities like color and shape. When our system is not able to assign a single class to a question, it assigns a number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Passage Extraction</head><p>We separated the test collection into phrase for the retrieval stage. For the separation of the collection we simply use punctuation marks and all sentences were indexed with the Xtrieval framework. We assume this is a good solution because if the passages are larger than it might be much harder to extract the correct answer. Contrary, when we had used smaller passages it may happen that a correct answer is split into two different passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Answer Finding Procedure</head><p>The answer finding procedure mainly consists of four important step. At first a query is formulated and feed into the retrieval system. The retrieval system ranks the passages based on their RSV and returns the top passages. Thereafter, the best matching passage is selected and the corresponding answer will be extracted.</p><p>In the next subsections we go into the details of these four steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Query Formulation Procedure</head><p>At first, we need to formulate a query to the IR system. The query has to be created depending on the type and the content of the question. Therefore, the system extracts nouns with the help of the POS tagger and assigns weights to the different types of part of speech, e.g. proper names/nouns (NNP/NNPS), which have to occur in a corresponding passage or a standard noun (NN/NNS) that could occur in the relevant passage.</p><p>The system automatically forms phrase queries for neighboring nouns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Passage Ranking</head><p>The ranking of the passages is defined by the RSV for the passage returned from the IR system. There is only one exception: when the second method for the answer extraction is used (see corresponding section below) the score will be the inverse of the calculated distance value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Passage Selection</head><p>The formulated query is fed into the IR system, which queries the passage index. If this step does not return any relevant passages, a fall-back algorithm will be applied. This method simplifies the query by removing possible phrases or changing a mandatory term into an optional term. As a matter of principle we do only consider the first passage returned by the IR system, because we observed that in the test set the best matching passage contained the correct answer in many cases. This could help us to achieve a high precision, which is very important in a QA scenario from our point of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.4">Answer Extraction</head><p>The module for the answer extraction consists of two parts. At first, we try to find corresponding answers for special classes of questions by using the POS or NER tags. For example, if the question asks for values or measures, the system will search for adjectives and tries to combine groups of words to build a complete answer like more than five hundred. If a posed question contains a person, organization or location, the system will use the terms that were tagged by the SNER tagger. Unfortunately, we discovered that quite a large number of entities are tagged wrong, i.e. they are tagged with the wrong class. For example persons can be recognized as organizations and vice versa. Therefore we think we should use an NER tagger that was adapted or trained with the collections we use for the experiments.</p><p>If the first part of the answer extraction did not return any answers or the question was not assigned to a single type a fall-back strategy will be applied. We analyzed some experimental answers on the development data and we observed that the correct answers to some questions are near the terms that occurred in the question. Therefore, we implemented an algorithm to take advantage of this observation. It calculates the distances between all nouns in a sentence that do also occur in the query. We use this measure for the fall-back and return the noun with the lowest overall distance to all query terms. This calculation was only used when the best matching passage returned by the IR system had a RSV that was higher than a certain threshold. This threshold is necessary because otherwise the system would return an answer for every question even for those that are not supported by the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>The general setup of the system was discussed in the preceding sections. We submitted 4 experiments in total. For both collections (CHIL and EPPS) we tested a configuration (cut1*) that returned only one answer per question. We also submitted another configuration (cut2*), where up to 3 answers per questions were returned. Tables <ref type="table" coords="5,146.52,236.39,4.98,8.74" target="#tab_2">2</ref> and<ref type="table" coords="5,174.20,236.39,4.98,8.74" target="#tab_3">3</ref> present the results of the evaluation. In table <ref type="table" coords="5,107.32,357.53,4.98,8.74" target="#tab_2">2</ref> we have the following values: (Q) the total number of question that were processed, (CA) the total count of correct answers, (MRR) the mean reciprocal rank and (ACC) the overall accuracy of the system configuration. The results show that the proposed prototype did only answer a fifth of all questions correctly. Interestingly, the accuracy of the first answer is very high. Another observation can be made by looking at the results for the two different collections. We found a higher number of correct answers for the considerably smaller EPPS collection, which might be due to the larger ratio of recognized named entities. But another reason could also be the distribution of the question types over both collections. ) the total number of answers per question and the total number of all assessments: correct (R), inexact (X), unsupported (U), wrong (W), NIL answers (NIL) and correct NIL answers (NIL=R). The analysis of the result shows, that the accuracy of the system is acceptable, but the total number of correct answers is much too low. Generally the total number of NIL answers is too large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Result Analysis -Summary</head><p>The following list provides a summary of the analysis of our retrieval experiments for the QAst task at CLEF 2008:</p><p>• Task 1a (CHIL-manual): Both experimental configurations of the system achieved an accuracy slightly below 20%. Interestingly, this accuracy was achieved by generating only one answer per question (experiments cut1*). The total number of NIL answers is above 50%, which is an almost inacceptable value.</p><p>• Task 4a (EPPS-manual): Both experimental configurations of the system achieved an accuracy slightly above 20%. The total number of NIL answers is above 40%, which is also an almost inacceptable value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>The experiments for the CLEF 2008 QAst task allow us to draw some conclusions on how to improve the quality of the used system. At first, we should improve the ratio of correctly recognized named entities either by training the tagger for the special collection or by using a more general tagger. With this improvement we could probably assign more correct classes to the questions posed, which will boost the total number of correct answers without deteriorating the accuracy of the system. Additionally, we could improve or vary the passage extraction procedure to improve the passage retrieval itself. Last but not least one could try to implement an answer extraction strategy that is adapted to find more answers to raise the total number of answered question. This could easily be achieved by tuning the RSV based parameter in the answer extraction fall-back strategy to lower values.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,128.53,319.44,354.56,47.81"><head>Table 1 :</head><label>1</label><figDesc>Distribution of detected named entities</figDesc><table coords="3,128.53,331.81,354.56,35.44"><row><cell>corpus</cell><cell cols="6"># PER # ORG # LOC # MISC # UT NE2TR</cell></row><row><cell cols="2">CHIL-manual (t1) 52</cell><cell>71</cell><cell>40</cell><cell>47</cell><cell>2264</cell><cell>09.28%</cell></row><row><cell cols="2">EPPS-manual (t4) 47</cell><cell>68</cell><cell>69</cell><cell>35</cell><cell>1418</cell><cell>15.28%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,173.01,268.59,265.98,71.72"><head>Table 2 :</head><label>2</label><figDesc>QAst evaluation results</figDesc><table coords="5,173.01,280.96,265.98,59.35"><row><cell>run ID</cell><cell>corpus</cell><cell cols="3"># Q # CA # MRR ACC</cell></row><row><cell cols="3">cut1 t1a CHIL-manual 100</cell><cell>16</cell><cell>0.16</cell><cell>16.0%</cell></row><row><cell cols="3">cut2 t1a CHIL-Manual 100</cell><cell>24</cell><cell>0.20</cell><cell>17.0%</cell></row><row><cell cols="3">cut1 t4a EPPS-manual 100</cell><cell>21</cell><cell>0.21</cell><cell>21.0%</cell></row><row><cell cols="3">cut2 t4a EPPS-Manual 100</cell><cell>23</cell><cell>0.22</cell><cell>21.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,102.28,461.47,407.05,71.72"><head>Table 3 :</head><label>3</label><figDesc>QAst evaluation results in detail</figDesc><table coords="5,102.28,473.84,407.05,59.35"><row><cell>run ID</cell><cell cols="10"># 1Q # 2Q # 3Q # MQ # R # X # U # W # NIL NIL=R</cell></row><row><cell cols="2">cut1 t1a 100</cell><cell>0</cell><cell>0</cell><cell>1.0</cell><cell>16</cell><cell>1</cell><cell>0</cell><cell>83</cell><cell>53</cell><cell>3</cell></row><row><cell cols="2">cut2 t1a 68</cell><cell>17</cell><cell>15</cell><cell>1.5</cell><cell>17</cell><cell>1</cell><cell>0</cell><cell>82</cell><cell>60</cell><cell>5</cell></row><row><cell cols="2">cut1 t4a 100</cell><cell>0</cell><cell>0</cell><cell>1.0</cell><cell>21</cell><cell>1</cell><cell>0</cell><cell>78</cell><cell>41</cell><cell>6</cell></row><row><cell cols="2">cut2 t4a 64</cell><cell>20</cell><cell>16</cell><cell>1.6</cell><cell>21</cell><cell>1</cell><cell>0</cell><cell>78</cell><cell>44</cell><cell>6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,70.87,550.41,388.49,8.74"><head>Table 3</head><label>3</label><figDesc>goes into the details of the evaluation. It shows the following values: ([1|2|3|M]Q</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,86.11,701.52,131.59,6.99"><p>http://www.lsi.upc.edu/˜qast/2008</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,86.11,684.19,91.28,6.99;2,81.96,692.24,95.43,8.44"><p>http://lucene.apache.org 2 http://lucene.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,86.11,673.68,187.38,6.99"><p>http://nlp.stanford.edu/software/CRF-NER.shtml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,86.11,683.19,121.22,6.99"><p>http://crftagger.sourceforge.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,86.11,692.69,171.41,6.99"><p>http://nlp.stanford.edu/software/tagger.shtml</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,86.36,323.03,454.77,8.74;6,86.36,334.99,454.76,8.74;6,86.36,346.95,360.87,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,382.24,323.03,158.89,8.74;6,86.36,334.99,238.15,8.74">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName coords=""><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,334.93,334.99,206.20,8.74;6,86.36,346.95,238.26,8.74">Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005)</title>
		<meeting>the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005)</meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,86.36,366.87,454.77,8.74;6,86.36,378.83,454.77,8.74;6,86.36,390.78,109.09,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,334.78,366.87,206.36,8.74;6,86.36,378.83,33.48,8.74">Extensible retrieval and evaluation framework: Xtrieval</title>
		<author>
			<persName coords=""><forename type="first">Jens</forename><surname>Kürsten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maximilian</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,132.26,378.83,182.25,8.74;6,440.79,378.83,95.98,8.74">LWA 2008: Lernen -Wissen -Adaption</title>
		<meeting><address><addrLine>Würzburg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10">October 2008. October 2008</date>
		</imprint>
	</monogr>
	<note>Workshop Proceedings. to appear</note>
</biblStruct>

<biblStruct coords="6,86.36,410.71,254.66,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,166.74,410.71,142.91,8.74">Crftagger: Crf english pos tagger</title>
		<author>
			<persName coords=""><forename type="first">Xuan-Hieu</forename><surname>Phan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,86.36,430.63,454.77,8.74;6,86.36,442.59,434.56,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,423.21,430.63,117.93,8.74;6,86.36,442.59,180.77,8.74">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,276.22,442.59,145.93,8.74">Proceedings of HLT-NAACL 2003</title>
		<meeting>HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="252" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,86.36,462.51,454.77,8.74;6,86.36,474.47,454.77,8.74;6,86.36,486.42,418.75,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,302.17,462.51,238.96,8.74;6,86.36,474.47,128.18,8.74">Enriching the knowledge sources used in a maximum entropy part-of-speech tagger</title>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,225.76,474.47,315.38,8.74;6,86.36,486.42,328.87,8.74">Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000)</title>
		<meeting>the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
