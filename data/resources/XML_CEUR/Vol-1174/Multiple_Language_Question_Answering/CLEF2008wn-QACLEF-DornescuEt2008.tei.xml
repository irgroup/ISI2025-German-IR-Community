<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_x3Sze5c">
					<orgName type="full">EU</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,170.09,181.27,66.87,10.46"><forename type="first">Iustin</forename><surname>Dornescu</surname></persName>
							<email>i.dornescu2@wlv.ac.uk</email>
						</author>
						<author>
							<persName coords="1,245.08,181.27,82.82,10.46"><forename type="first">Georgiana</forename><surname>Puşcaşu</surname></persName>
						</author>
						<author>
							<persName coords="1,350.59,181.27,82.34,10.46"><forename type="first">Constantin</forename><surname>Orȃsan</surname></persName>
							<email>c.orasan@wlv.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">CLEF</orgName>
								<orgName type="institution">University of Wolverhampton</orgName>
								<address>
									<postCode>2008</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">CLEF</orgName>
								<orgName type="institution">University of Wolverhampton</orgName>
								<address>
									<postCode>2007</postCode>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">73AD5E3B866DC25C81E4C16ED382E82C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Question answering, Questions beyond factoids</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article presents the participation of University of Wolverhampton in the Romanian to English Question Answering task at CLEF-2008. This year we employed a modular framework which allows different modules to be easily plugged in and customised. The main components of our system deal with the three standard stages used in question answering: question processing, paragraph retrieval and answer extraction, and the system's cross-linguality is ensured by a term translator. The question processor analyses Romanian questions and produces a detailed representation of each question including the terms it contains. English translations are then generated for all question terms by exploiting information included in the Romanian and English WordNets, as well as aligned Wikipedia pages. They form the query that Lucene uses to extract English paragraphs which constitute the input for an answer extractor largely based on the one distributed with the OpenEphyra framework. The results indicate a small improvement in comparison with last year's performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the second participation of the University of Wolverhampton in the Romanian-English cross-language QA@CLEF competition. The objective for this year was to develop a question answering (QA) framework in which different modules are plugged in dynamically so that different processing components can be included as needed. This framework also allows individual module tuning and performance assessment, and can easily be developed further. Our architecture relies on OpenEphyra, a modular and extensible framework for question answering <ref type="bibr" coords="1,135.87,706.75,9.96,10.46" target="#b4">[5]</ref>, which was the basis for some TREC submissions <ref type="bibr" coords="1,364.49,706.75,9.96,10.46" target="#b7">[8]</ref>. Whilst our objective of having a modular system was achieved, due to time restrictions, some of the system components are in the early stages of development, which has led to a relatively small performance increase compared to last year's results <ref type="bibr" coords="1,181.05,742.62,9.96,10.46" target="#b6">[7]</ref>.</p><p>Our system adheres to the established pipeline architecture for QA that consists of a question processor, a passage retrieval module and an answer extractor <ref type="bibr" coords="2,364.57,122.49,9.96,10.46" target="#b1">[2]</ref>. Similar to the approach taken in previous years, our system relies on an intermediate stage that translates questions processed in the source language and feeds its output into the answer extractor that retrieves passages and locates exact answers in the target language. In this way it is possible to have questions asked in Romanian and answers extracted from an English collection. The rest of the paper describes the system components and the evaluation results. Sections 2 to 5 describe in turn each of the four stages involved in the cross-lingual QA process: question processing (section 2), term translation (section 3), passage retrieval (section 4) and answer extraction (section 5). Section 6 captures the results achieved and an error analysis. Conclusions and directions of future work tackling the problems encountered during this year's participation are presented in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Question Processing</head><p>At this first stage of the system, each Romanian question is analysed with the aim of identifying relevant information necessary in the subsequent stages of the system's answering process. The question processing module produces a custom representation of the question that includes: the question focus, the expected answer type (EAT), the question type, the relevant terms identified in the question and the question topic. The way each of these is determined is explained below.</p><p>The question processor employed this year is an improved version of the one embedded in last year's system. As before, this module involves several preprocessing stages such as part-of-speech, noun and verb phrase identification, numerical and temporal expression annotation, all improved to address problems identified during last year's participation in QA@CLEF. This year's system uses a revamped phrase and named entity recogniser based on dictionaries and entity mappings extracted from the aligned Romanian and English Wikipedias as described in Section 3.</p><p>The linguistic information obtained during preprocessing is used to identify the question focus, which is important in the search for an answer, as it normally reveals what the question is asking for or what the question is about.</p><p>The semantic type of the expected answer is then determined by matching question focus and the first question verb phrase against syntactic constraints and semantic hierarchies extracted from WordNet (e.g. the noun arhitect ro /architect en , as hyponym of the synset person, individual, someone, somebody, mortal, soul in WordNet, is automatically mapped to the EAT category person). As we did last year, predefined mappings between categories of expected answers and WordNet synset hierarchies are employed. A more detailed EAT categorisation is covered by this year's question processing module and includes the following classes: person, location, organization, name, type, nationality, language, occupation, definition, numeric (with the sub-classes quantity, measure, economic, percentage) and temporal (with the sub-classes century, year, month, week, date, time, duration). The number of categories employed by our system was increased after noticing that a number of questions from last year's test set could not be answered due to the fact that we had too general categories.</p><p>As in previous editions, this year's questions are concerned with facts or events (factoid questions), definitions of people, organisations or things (definition questions), or lists of people, objects or dates (list questions). These question types are recognised using the same approach as last year <ref type="bibr" coords="2,142.71,631.56,9.96,10.46" target="#b6">[7]</ref>.</p><p>The question processor also produces a list of keywords in decreasing order of their relevance. The list contains noun and verb phrases, named entities, temporal and numeric expressions that appear in the question to be answered, and is used as the input of the term translation module. The topic corresponding to a cluster of questions, whenever it can be identified following the procedure described below, is also added to the keyword list of each question in the cluster.</p><p>The fact that questions are grouped in clusters related to the same topic is exploited by the question processor to improve the results of the system. This topic is usually present in the first question or represents the answer to the first question. Due to the fact that the current architecture does not allow us to feed answers back into the system, we consider as topic the first named entity of type person that appears in the first question of each cluster. If no such entity can be found, the first entity of the question is selected as the topic. If the question contains no named entities, the topic is considered to be the question focus. This approach was designed after empirical analysis of last year's questions. The topic is used to boost the retrieval scores of the Wikipedia articles describing it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Term Translation</head><p>The cross-linguality of the system is ensured by a term translation module that takes as input the keywords identified during question processing and generates a ranked list of translation equivalents. Firstly, a list of translation equivalents mined from the mappings between the Romanian and English Wikipedias is used in order to obtain high quality translations of entities. This method provides a high precision, but low recall term translation.</p><p>The Inter-Lingual Index (ILI) between the Romanian <ref type="bibr" coords="3,339.17,272.77,10.52,10.46" target="#b8">[9]</ref> and the English <ref type="bibr" coords="3,423.85,272.77,10.52,10.46" target="#b0">[1]</ref> WordNets is used to obtain all translation equivalents for the remaining keywords. If a word does not appear in the Romanian WordNet, alternative dictionaries are consulted. The drawback to this method is that it yields too many translation candidates. For this reason, a ranking method that relies on co-occurrence of words in Wikipedia is used to filter out infrequent candidates. This method proved particularly useful for translating noun phrases, where all the combinations of word by word translations were generated and sought in Wikipedia. The infrequent ones were removed, as they indicated incorrect translations.</p><p>For example, given the Romanian term plan general, the translation equivalents generated using the Romanian and English Wordnets are the following: general plan, general plane, general mind, general idea, general program, general sheet, general design, general cadre, general canvas, general programme, general inclined plane, general architectural plan</p><p>The ranking method then identifies the words that most frequently appear together in Wikipedia and eliminates those with infrequent use, and the resulted translations are: general design, general plan, general program, general idea, general programme</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Passage Retrieval</head><p>The purpose of this module is to provide the answer extractor with sentences from the document collection that are relevant to the question. This is achieved by using the identified topic and the translated terms. A two stage retrieval approach is employed: first, the most relevant documents are selected, and then all their sentences are extracted and re-ranked according to their relevance to the query. In order to do this the corpus was preprocessed and indexed using the Lucene retrieval engine <ref type="bibr" coords="3,160.07,602.37,9.96,10.46" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Corpus preprocessing and indexing</head><p>As last year, the answers had to be extracted from a heterogeneous collection consisting of two news corpora: Los Angeles Times from 1994 and Glasgow Herald from 1995, as well as English Wikipedia pages from November 2006. Wikipedia can be processed either as a static HTML dump, or in its native wikisource format. The former has a lot of content that is not part of the article itself and could have a negative impact on the answer extraction process, whereas the later is easier to process as it contains less information irrelevant to our purposes. After analysing the advantages and disadvantages of each format, we decided to convert the wikisource dump to plain text, preserving the information included in infoboxes, lists and tables, together with their markers. The inter-language links between the Romanian and English Wikipedias were also preserved in order to create the Romanian to English bilingual entity dictionary that was employed by the term translation module (see <ref type="bibr" coords="4,250.40,122.49,42.62,10.46">Section 3)</ref>.</p><p>The text of the articles was indexed using a standard approach. No stopword filtering was employed, since stopwords are important for question answering. The index was enriched with a stemmed version of the text obtained using Porter Stemmer <ref type="bibr" coords="4,356.22,158.36,9.96,10.46" target="#b5">[6]</ref>. This was done because retrieval using stemming offers higher recall, whereas retrieval using full words offers greater precision. Our retrieval engine used a combination of the two available in Lucene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">First stage -document retrieval</head><p>The first retrieval stage uses the translated query to extract documents which might contain the answer to a question. As we previously mentioned, our system relies heavily on information extracted from Wikipedia. To this end, we first try to identify documents related to the topic of the question by retrieving documents which contain the question topic in their title. In cases where the topic is not reliably identified or no documents that contain the topic in their title can be found, documents which contain the query terms are retrieved.</p><p>In order to create complex queries that would rank higher the documents describing the target entities, we use a combination of phrase, fuzzy and proximity queries, as well as term boosting techniques offered by Lucene. A maximum of 5 documents from the query result set are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Second stage -sentence selection</head><p>The second stage of paragraph retrieval tries to identify sentences which are relevant to the question. In order to do this, the documents retrieved at the previous stage are segmented into sentences using LingPipe <ref type="bibr" coords="4,202.43,394.37,10.52,10.46" target="#b2">[3]</ref> and used to create an in-memory index. This index is queried using the keywords produced by the term translation module in order to extract up to 50 sentences which are passed to the answer extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Answer extractor</head><p>The answer extractor employed by our system relies heavily on the answer extraction modules provided by OpenEphyra. These modules implement several answer extraction strategies depending on the type of question, source of answer, and above all, expected answer type. In order to employ these components, our EAT hierarchy had to be mapped to the one used by OpenEphyra. In this way, most of the answers retrieved by our system are named entities of the type given by the expected answer type. This approach yielded good results for the categories identified by usual named entity recognisers (i.e. location, organization, person, number, date), but proved unsuitable for questions requiring generic answers such as Numiti doua instrumente la care canta Emerson./Name two instruments played by Emerson. Due to time restrictions, no attempt was made to change the ranking algorithm implemented by OpenEphyra. For this reason, in a large number of cases, the correct answer was among the first three answers retrieved by the system, but not the top one.</p><p>Definition questions are answered using a different answer extraction module from the one provided by OpenEphyra. Our module relies on a cascade of high precision filters designed after analysis of the data sources and experience gained in previous CLEF participations.</p><p>Wikipedia is a great source for definitions. For this reason, whenever we try to find a definition for a given term, Wikipedia is the first place to look for one. We start by locating Wikipedia pages which contain the term to be defined in their title. In some cases this approach fails because the title of the page has a different surface form even though it refers to the same concept (e.g. the page about CORGI has the title Council for Registered Gas Installers). In these cases, we check whether the term to be defined is used as the name of a file (i.e. considering the previous example we check whether there is a Wikipedia file called CORGI ). Once candidate pages are located, a set of patterns is applied in order to extract the definition of the term. The patterns are designed in such a way that they can cater for situations where alternative forms are used to refer to the same concept (e.g. Steve Redgrave is referred to in an Wikipedia article as Sir Stephen Geoffrey Redgrave). The definition of a term is considered to be the whole sentence to which a pattern can be applied.</p><p>If no definition can be located in a Wikipedia page using the method described above, we then search the whole collection for sentences that contain not only the term to be defined, but also other terms that might appear in a definition question (sometimes the question provides disambiguation clues for the term to be defined, e.g. Ce este "bungo" in japoneza? /What is "bungo" in Japanese? ). A different set of manually created patterns is applied to these sentences in order to extract the definition of the term. In contrast to the definitions extracted by the previous method, at this stage we extract only the noun phrase which is considered to define a term (e.g. Richard D. Farman is defined as CEO of Southern California Gas Co.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation results</head><p>The evaluation results reveal an improvement in comparison with last year's accuracy. They are presented in Table <ref type="table" coords="5,173.66,308.77,3.87,10.46" target="#tab_0">1</ref>. As expected, the best results are obtained for definition questions which can easily be answered thanks to the structure of Wikipedia pages. The main source of errors in the case of definition questions is the incorrect translation of the term to be defined and errors introduced by preprocessing tools. For example, one definition was marked as inexact due to the fact that the NP extractor wrongly identified a bigger chunk of text which included the NP that constituted the answer. Translation errors also contributed to wrong answers or no answers being extracted for factoid questions. However, the main source of errors was the fact that no mapping between the generic expected answer type and the named entity classes covered by OpenEphyra was used. As a result, all the questions expecting generic answers were answered with NIL.</p><p>A large number of questions with numeric EAT were wrongly answered due to errors in the named entity recogniser employed by OpenEphyra. Numeric parts of date expressions were wrongly labelled as numbers and returned as answers (e.g. the number 12,1853 is wrongly extracted as answer from the date March 12, 1853 ). In many cases the correct answer was extracted as well, but received a lower confidence.</p><p>Currently our system does not have a way to deal with list questions and for this reason it returned NIL for all these questions.</p><p>The modular structure of our system also enables us to assess the performance of each individual component. The question analysis module identifies the question type with an accuracy of 98% and the expected answer type with 94%.</p><p>Empirical observation of the translation output indicates that there are still issues to be addressed in the future such as the ranking of the translation equivalents and translation of named entities especially when the questions contain words from several languages (e.g. La ce data a scris Mathieu Orfila al sau "Tráite des poisons"? /When did Mathieu Orfila write his "Tráite des poisons"? ).</p><p>In this article we presented our participation in the Romanian to English Question Answering task at CLEF-2008. We employed a modular system consisting of the three standard stages of a QA system: question processing, paragraph retrieval and answer extraction. The question processor analyses Romanian questions and produces a detailed representation of each question including the terms it contains. These terms are then translated to English using several techniques based on the Romanian and English WordNets and aligned Wikipedia pages. Lucene is used to extract English paragraphs which constitute the input for the answer extractor employed. Due to time restrictions, some of the modules are in early stages of development or have been adapted from other projects. For example, the answer extractor is largely based on the one distributed with the OpenEphyra framework.</p><p>The existing components of OpenEphyra were designed to process English questions and retrieve answers from English document collections. For the future, we plan to use existing translation services such as Google Translate to obtain full translations of the Romanian questions as an additional source of information for our system. These translations can also be used by a monolingual English QA system such as OpenEphyra. Comparison between the two approaches can give us further insights into the best approach for cross-lingual question answering, and how and whether they can be combined.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,148.30,339.54,306.41,70.84"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="5,148.30,339.54,306.41,70.84"><row><cell cols="6">Official results Question Type Right Inexact Unsupported Wrong Accuracy</cell></row><row><cell>Definition</cell><cell>20</cell><cell>1</cell><cell>0</cell><cell>9</cell><cell>66.67%</cell></row><row><cell>Factoid</cell><cell>18</cell><cell>1</cell><cell>5</cell><cell>136</cell><cell>11.25%</cell></row><row><cell>Lists</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>10</cell><cell>0.00%</cell></row><row><cell>Overall</cell><cell>38</cell><cell>2</cell><cell>5</cell><cell>154</cell><cell>18.00%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p>This work has been supported by the <rs type="funder">EU</rs> funded project <rs type="projectName">QALL-ME</rs> (<rs type="grantNumber">FP6 IST-033860</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_x3Sze5c">
					<idno type="grant-number">FP6 IST-033860</idno>
					<orgName type="project" subtype="full">QALL-ME</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,105.50,433.18,407.50,10.46" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,232.44,433.18,176.09,10.46">WordNet: An Eletronic Lexical Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,453.10,407.51,10.46;6,105.50,465.05,407.51,10.46;6,105.50,477.01,22.69,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,274.46,453.10,85.59,10.46">Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,483.82,453.10,29.19,10.46;6,105.50,465.05,168.93,10.46">Oxford Handbook of Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Mitkov</surname></persName>
		</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="560" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,496.93,171.06,10.46" xml:id="b2">
	<monogr>
		<ptr target="http://alias-i.com/lingpipe/" />
		<title level="m" coord="6,105.50,496.93,37.39,10.46">LingPipe</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,516.86,210.81,10.46" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Lucene</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org/java/docs/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,536.78,261.35,10.46" xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Openephyra</surname></persName>
		</author>
		<ptr target="http://sourceforge.net/projects/openephyra/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,556.71,368.60,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,186.38,556.71,140.77,10.46">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,335.72,556.71,34.40,10.46">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,576.64,407.51,10.46;6,105.50,588.59,407.50,10.46;6,105.50,600.55,65.61,10.46" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,301.54,576.64,171.32,10.46">University of Wolverhampton at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Georgiana</forename><surname>Puşcaşu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Constantin</forename><surname>Orȃsan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,105.50,588.59,355.40,10.46">Working Notes for the Cross Language Evaluation Forum (CLEF) 2007 Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,620.47,407.51,10.46;6,105.50,632.43,407.50,10.46;6,105.50,644.38,226.97,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,143.88,632.43,250.51,10.46">Semantic Extensions of the Ephyra QA System for TREC</title>
		<author>
			<persName coords=""><forename type="first">Nico</forename><surname>Schlaefer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeongwoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guido</forename><surname>Sautter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manas</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,435.82,632.43,77.17,10.46;6,105.50,644.38,195.36,10.46">Proceedings of the Sixteenth Text REtrieval Conference (TREC)</title>
		<meeting>the Sixteenth Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,664.31,407.51,10.46;6,105.50,676.26,407.50,10.46;6,105.50,688.22,343.33,10.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,325.54,664.31,187.46,10.46;6,105.50,676.26,149.73,10.46">BalkaNet: Aims, Methods, Results and Perspectives. A General Overview</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Tufis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Cristea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sofia</forename><surname>Stamou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,358.66,676.26,154.34,10.46;6,105.50,688.22,100.83,10.46">Romanian Journal on Information Science and Technology</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Tufis</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Romanian Academy</publisher>
		</imprint>
	</monogr>
	<note>Special Issue on BalkaNet</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
