<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,121.73,148.86,359.53,15.15;1,107.15,170.78,388.70,15.15">Experiments with Query Expansion in the RAPOSA (FOX) Question Answering System</title>
				<funder>
					<orgName type="full">POSI</orgName>
				</funder>
				<funder ref="#_qQfsjjh">
					<orgName type="full">Fundação para a Ciência e Tecnologia (Portugal)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,188.44,210.65,60.92,8.74"><forename type="first">Luís</forename><surname>Sarmento</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Engenharia</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.57,210.65,61.63,8.74"><forename type="first">Jorge</forename><surname>Teixeira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Engenharia</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,341.89,210.65,72.67,8.74"><forename type="first">Eugénio</forename><surname>Oliveira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Engenharia</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,121.73,148.86,359.53,15.15;1,107.15,170.78,388.70,15.15">Experiments with Query Expansion in the RAPOSA (FOX) Question Answering System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FF8AE260104FA9AF614540F4166A649E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Question answering, Questions beyond factoids</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present the results of applying a statistical query expansion method on the retrieval stage of a QA system for Portuguese (RAPOSA). Our approach involves expanding queries for event-related or action-related factoid questions using a verb thesaurus automatically generated using information extracted from large corpora. We show that our expansion approach improves QA recall when compared with applying expansion based on a simple form of stemming, while simultaneously requiring the analysis of only 30% as many text snippets. However, we were not able to outperform the recall obtained using an even simpler expansion method, which nevertheless achieves lower precision and requires analyzing many more text snippets. We conclude by observing that a more thorough analysis of the usefulness of our approach on QA performance requires improving other stages of the QA pipeline which currently impose significant limitations on the overall performance of the system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head><p>One of the most obvious limitation in the performance of many automatic question answering (QA) systems is their relatively low recall: for a very large proportion of questions many QA systems are unable to produce any answer at all. There are many possible causes for this low recall. To name just a few of the most common:</p><p>1. inability to parse the question. The QA system does not have rules to decompose the input question and the QA process is thus immediately ended.</p><p>2. inability to find text passages where candidate answers can be found. After parsing the question and choosing the relevant terms / keywords, the system is not successful in its information retrieval (IR) stage, and is unable to retrieve text passages for any subsequent information extraction procedure.</p><p>3. inability to extract answer candidates from retrieved text passages. Since no candidate is found no answer is produced.</p><p>Problem 1) can be more or less relevant depending on the specific application scenario. For example, when performing QA under restricted application scenarios, systems can usually be prepared to parse a relatively small number of types of questions that they are expected to be asked. In these cases, problems at this stage can usually be solved by adding more parsing rules. On the other hand, for unrestricted question answering scenarios, this problems becomes much more severe, since users can pose all sorts of question. Interactive question answering scenarios involve even more difficulties as they require the need to resolve possible co-references among question and answers. Additionally, robustness to typos and malformed questions is required in practice and may be quite difficult to achieve.</p><p>Solving problem 3) involves developing more sophisticated information extraction strategies, which sometimes require using additional knowledge resource such as lexical databases, ontologies, or pre-built factoid databases. Research in this field focus on applying techniques such as named-entity disambiguation, factoid extraction and semantic relation discovery improve question answering.</p><p>We will mainly focus on problem 2), the inability of a QA system to retrieve the appropriate text passages for extracting answer candidates. In the current paper, we will describe how we tried to increased the overall recall of our QA system RAPOSA (http://pattie.fe.up.pt/RAPOSA/) by applying query expansion techniques to improve recall in the IR stage. Following RAPOSA participation in 2006 <ref type="bibr" coords="2,188.47,390.97,15.50,8.74" target="#b17">[18]</ref> and 2007 <ref type="bibr" coords="2,254.27,390.97,15.50,8.74" target="#b18">[19]</ref> editions of QA-CLEF, we believed that tackling this problem should be the next step.</p><p>First of all, from a strategic point of view there is much interest in solving problems related to IR stage because they transversally affect overall performance for all types of questions. Second, the goal we should seek is very well defined. As mentioned in <ref type="bibr" coords="2,370.02,438.79,9.97,8.74" target="#b1">[2]</ref>, in the standard pipeline QA architecture (used by RAPOSA), during the IR stage recall in retrieval is more important than precision: subsequent processing stages may filter out uninteresting text passages obtained, but they will unable to extract the right answer candidates if the passage that contains the answer is not retrieved. Therefore, the main goal should be to increase recall in the IR stage. Finally, the problem scope is well localized and constrained inside a single stage of the QA system. This allows easier testing because it involves changing only a very specific stage of the pipeline without the need to change any of the others (either before or after the IR stage).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Efforts to increase the recall in traditional IR system have focused on trying to circumvent morphological, lexical and semantic differences between the query terms and the terms in the documents. One would expect that such general IR techniques would be suitable for improving QA performance. However, there are a few important difference between general IR and QA-centric IR. While in general IR the retrieval unit is the document, in QA-centric IR the unit of retrieval is usually much smaller, such as for example, a paragraph, a sentence or even a smaller text fragment. Also, in QA-centric IR very fine-tuned ranking is not as crucial and in general IR, because further filtering will be performed along the QA pipeline. These are two important differences which motivate additional and specific efforts in QA-centric IR.</p><p>From a purely morphological point of view, there have been two main approaches. The first is to apply a stemming procedure at indexing time that will conflate morphological variations to the same index entry. At retrieval time, query terms are also stemmed and matched against the stems stored in the index. The second alternative involves indexing document terms directly (no changes are made to terms), and performing morphological expansion of the query terms at retrieval time, so they can be matched to more (unstemmed) index entries.</p><p>The benefits for QA of using applying such morphological-based techniques are not clear. In <ref type="bibr" coords="3,90.00,123.98,9.97,8.74" target="#b4">[5]</ref>, a component evaluation of the Esfinge QA system for portuguese showed that turning off the stemming component improved the results, although only slightly, when attempting to answer the CLEF 2005 question set. Such slight improvement was observed for about half the types of factoid questions. The only exception was the performance for date question ("When... ?" ) which dropped significantly when stemming was turned off. In <ref type="bibr" coords="3,369.40,171.80,10.52,8.74" target="#b1">[2]</ref> the authors studied the effect of stemming and morphological expansion on document retrieval for the purpose of answering factoid questions. They concluded that indexing stemmed word forms actually lead to a decrease document retrieval recall, when compared to baseline (no stemming nor expansion). On the other hand retrieval-time query expansion tends to increase document retrieval recall at the cost of bringing more irrelevant documents and placing relevant documents in lower ranks.</p><p>Another technique aiming at improving recall in QA systems involves expanding terms in query to lexically or semantically related ones. For example, each term in the query can be expanded to the set of all know synonyms, by terms that generalize or specialize the concept, or by other related terms. These type of semantic expansion techniques require specific language or knowledge resources such as lexical databases (e.g. Wordnet <ref type="bibr" coords="3,307.93,291.35,10.80,8.74" target="#b6">[7]</ref>) and ontologies (e.g.: Cyc <ref type="bibr" coords="3,437.50,291.35,14.77,8.74" target="#b10">[11]</ref>).</p><p>In <ref type="bibr" coords="3,116.94,303.30,9.96,8.74" target="#b8">[9]</ref>, Wordnet is used to expand the query terms found in the question by all terms contained in its synsets. A Boolean search expression is made by combining all expanded terms in a logical OR. The authors observe that such a direct approach may bring problems when synonyms are highly polysemous words. For example "high" can be a possible synonym of "high school" but since it is much more frequent (and polysemous) it will make the original "high school" term relatively less significant in the search expression. To account for this problem, document ranking is made by pondering the original terms twice as much as the synonyms. However, problematic situations arise when the original word is itself polysemous, leading to totally inappropriate expansions.</p><p>An approach that tries to solve some of the problem generated by ambiguity is presented in in <ref type="bibr" coords="3,102.87,410.90,14.62,8.74" target="#b12">[13]</ref>. The proposed technique uses a combination of Blind Relevance Feedback (BRF) and Word-Sense Disambiguation (WSD) named Sense-based Blind Relevance Feedback (S-BRF). In a first step, sets of paragraphs are retrieved using several combinations of the original terms found in questions. In a second step, the retrieved paragraphs are subject linguistic analysis (POStagging, multi-word recognition, named-entity recognition) and to word-sense disambiguation over WordNet senses. For each of the original question terms, the most frequent sense found on the retrieved paragraphs is chosen. Query expansion is then made by expanding only the previously found sense, using WordNet hierarchy (synonyms, hypernyms, holonyms, etc.). S-BRF leads to an increase of 7% in the precision of retrieval of answer-bearing documents, in relation to results obtained using "standard" morphological query expansion.</p><p>The work described in <ref type="bibr" coords="3,203.09,530.45,10.52,8.74" target="#b5">[6]</ref> show an example of how Cyc can be used in query expansion in a QA system, the MySentient system. MySentient uses Cyc to expand terms to its synonyms (including acronym expansion), to its specializations or generalizations, to possible instances or classes (e.g. "MasterCard" is an instance-of "credit card"), and to concepts related by meronomy/holonomy (is-part-of or is-composed-by). The authors claim that such expansion procedures improve system performance, although no performance figures are given.</p><p>When resources like Wordnet of Cyc are not available, systems may use smaller hand-crafted ontologies ( <ref type="bibr" coords="3,139.96,614.14,14.86,8.74" target="#b15">[16]</ref>), or follow alternative approaches supported by statistical techniques. In <ref type="bibr" coords="3,478.72,614.14,15.49,8.74" target="#b14">[15]</ref> two query expansion methods based on statistical machine translation models are proposed, although focusing on a different yet related problem: answer retrieval. In the first method, a "translation model" from question words to answers words was learned using a large corpus of question-answer pairs. Using such translation model, each word questions can be expanded to a set of words that are expected to occur in the answer. A second method a english-chinese parallel corpus was used to learn english paraphrases. Query expansion was then achieved by adding in the query the n-best paraphrases of the original terms. Authors report significant improvement over both methods over two alternative methods, <ref type="bibr" coords="3,201.61,709.78,15.50,8.74" target="#b9">[10]</ref> and <ref type="bibr" coords="3,239.80,709.78,14.61,8.74" target="#b19">[20]</ref>.</p><p>We follow a different alternative to deal with the lack of standard Wordnet-like resources for doing query expansion in Portuguese. Our approach consists in automatically building thesaurus using statistical processing of a large corpus. Such thesaurus should contain information about synonyms or strongly related words, which can then be used to expand queries. Several different approaches have been tried for automatic building thesaurus from corpora. These usually involve either finding distributional similarities between words and clustering (e.g. <ref type="bibr" coords="4,413.47,135.93,15.49,8.74" target="#b11">[12]</ref> [3]), or mining text with patterns (manually defined or automatically learned) that allow identifying specific semantic relations (e.g. <ref type="bibr" coords="4,152.55,159.84,28.22,8.74">[14] [8]</ref>). Since the goal using the thesaurus is query expansion within a QA-pipeline, we believe that the problem of thesaurus generation can in fact be relaxed: having very precise semantic (namely synonymy) might not be an absolute requirement. We have thus used a rather simple method, to be described in Section 4, for building such "relaxed thesaurus".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RAPOSA</head><p>The architecture of RAPOSA has been described in <ref type="bibr" coords="4,329.42,250.47,14.62,8.74" target="#b18">[19]</ref>. Briefly, RAPOSA is a pipeline QA systems composed by six main modules:</p><p>1. Question Parser: identifies the type of question, the expected semantic type(s) of the answer, its arguments, possible restrictions and other relevant keywords. Morphological analysis is made using JSpell ([1]).</p><p>2. Query Generator: the Query Generator selects which term from the question must necessarily occur in target text snippets and which terms are optional.</p><p>3. Snippet Searcher: takes the queries and searches several available text bases to retrieve text snippets where candidate answer may eventually be found. The retrieval unit is a text snippet, which, depending on the text base queried, can be a sentence or a paragraphs (but not usually a complete document).</p><p>4. Answer Extractor: tries to identify candidate answers in text snippets using two possible strategies. The first one is based on a set of context evaluation rules that search for given answer patterns. The second is called simple type checking strategy and extracts the most frequently found candidates whose type is compatible with the expected semantic type of the answer.</p><p>5. Answer Fusion: the role of the Answer Fusion module is to cluster lexically different but possible semantically equivalent (or overlapping) answers in to a single "answer group". At this moment, this module is not yet developed and it simply outputs previously chosen candidates.</p><p>6. Answer Selector: selects one of the candidate answers produced by the Answer Fusion module and choses the supporting text / answer justification among previously extracted text snippets.</p><p>The focus of our work this year has been the Query Generator. Up to CLEF 2007 this module had a very simple role. Using the information given by the Question Parser, the Query Generator selected terms that are required to be found in text snippets where answer might eventually be found, and which terms are optional. For example, all named-entities found in questions are required to occur in the text snippets.</p><p>A very rudimentary query expansion technique was also applied in this module. For terms that were not identified as named-entities, suffixes were stripped to obtain a simple form query expansion by pseudo-stemming. Suffixes were considered to be the last 2-4 characters of terms with more the 5 characters long. These were substituted by wild-cards in order to generalize the query and obtain more text snippets to be further analyzed down the pipeline.</p><p>Obviously, being a purely lexical transformation process, this method has several limitations and problems. In fact, pseudo-stemming should have all the possible problems of standard stemming, worsened by the fact the our current retrieval system is not using any form of relevance ranking because it is a simple text database system. If too many snippets are returned, which can happen when pseudo-stemming generates a very frequent stem, only the first text snippets are kept and used for answer extraction. The problem is that keeping only the first N max snippets increases the chances of not forwarding any relevant information to the Answer Extraction module. We believe this to be the one of the major sources of nil (and also incorrect) answers in RAPOSA. In the next section we will explain how we attempted to solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Expansion using an Automatically Generated Thesaurus</head><p>In this work, we focused on expanding queries for answering factoid questions related to actions or events, such as for example "Who killed J.F.K?" or "When did Brazil last won the World Cup?". In this type of questions, in which an action or event that is central to the answer is directly or indirectly mentioned, verbs have the key role in retrieving the relevant text snippets for finding answer candidates. Therefore, expanding the verb to semantically equivalent verbs, and ideally also to verbal and nominal paraphrases, should help increasing retrieval recall without adding too many irrelevant or noisy text snippets. For instance, for the question "Who killed J.F.K?", answers could be found in texts containing both "J.F.K." and forms of the verb "to kill" but also in texts where semantically equivalent or related verbs occur, such as "to murder", "to assassinate", "to shoot", etc. Such expansion requires a large coverage verb thesaurus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Building a Verb Thesaurus</head><p>For building a verb thesaurus for Portuguese we followed a simplified approach of that described in <ref type="bibr" coords="5,101.65,380.44,14.62,8.74" target="#b11">[12]</ref>. The basic principle is that "similar" word should have "similar" distributional properties under a given context. Such context can be defined, for example, by the set of grammatical relations that such word establishes with other words, or even simpler, by the set of words with which the word co-occurs within a predefined lexical window (e.g. two words to each side).</p><p>For the case of verbs in Portuguese, one can intuitively see that much of the information capable of describing the semantic properties of a verb can be found in the two following words. Within this context we can observe many of the more relevant verb-object relations as well as the most typical adverbial constructions. We used n-gram information compiled from a large web-corpus of about 1000 million words <ref type="bibr" coords="5,220.43,476.08,15.50,8.74" target="#b16">[17]</ref> to obtain a distributional description of verbs in portuguese. N-gram information is not POS-tagged but we used the following regular expression pattern over the 4-gram list:</p><formula xml:id="formula_0" coords="5,100.46,520.63,381.81,8.30">token1 = "para" &amp; token2 ends_with(ar|er|ir|or) &amp; token3 = * &amp; token4 = *</formula><p>to constrain 4-grams so that token at position 2 almost surely referes to a verb in the infinitive form (equivalent in english "[to] [verb in infinitive] [*] [*]"). Thus, for the list of mined verbs we obtained the distributional profile of the two following words which can be represented by tuples of the form (verb, token3, token4, frequency). We had 293,130,369 distinct 4-grams available from which 435,702 (approx. 0.15%) matched the previous pattern. These 4-grams correspond to 6862 distinct words at position token2, corresponding mostly to verbs, as expected.</p><p>Using such information we can now describe each verb v i using a feature vector [v i ] containing information about the previously found co-occurring words. [v i ] belongs to a space with 174,764 dimensions, each dimension being defined by a distinct co-occurring bigram (found at positions token3 token4 ). Features were weighted using Mutual Information <ref type="bibr" coords="5,389.41,647.44,9.97,8.74" target="#b3">[4]</ref>. This weighting function allows to consider global information about the features in each vector, demoting the importance of features that occur in many vectors, and that should be considered less relevant. Therefore, Mutual Information helps to reduce the influence of noisy features.</p><p>The next step for building a thesaurus is to find the top-k closest vectors for each vector (i.e. for each verb). Vectors were compared using the cosine metric. Although we are dealing with a moderate size the vector set (less than 7000 vectors), performing an "all-against-all" comparison between the vectors can still take too long. Alternatively, we built a feature index and computed the contribution of each feature to the similarity of each pair of vectors that shares such feature.</p><p>All features that occurred in more than 5% of the items were considered noisy and were left out. These very frequent features represent a significant part of the computational effort because they are common to many vectors. However, they tend to have a very low weight when pondered by Mutual Information, so in practice they should only contribute marginally to the values of similarity. Globally, this feature filtering procedure leads to substantial computational savings. and allowed us to quickly compute a very close approximation of the results that would be obtained if an "all-against-all" comparison was followed. The current version of our automatically generated thesaurus can be queried and visualized via: http://pattie.fe.up.pt/cgi-bin/tep/word map. pl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Query Expansion Procedure</head><p>We only apply query expansion to factoid questions that explicitly refer to an action or to an event. For these question the verb has the central role in finding the answer, so providing alternatives for it should lead to improvement in retrieval recall. For example, we try to apply query expansion to questions such as "Em que ano houve um terramoto no Irão", "Quando começou o Neolítico?" "Quantas vezes ganhou Portugal a Taça Davis?", but not "Qual a capital de estado de Nova York?" (examples taken from QA-CLEF 2008 question set).</p><p>After the question being processed by the Question Parsed module, the type of the question and all its main components have been identified. Then for factoid questions referring to an action or event, the following procedure is executed to expand the verb:</p><p>1. take the verb and find its radical; 2. using the statistical thesaurus, find n top = 5 related verbs; 3. apply pseudo-lematization to source and related verbs by substituting last character by a wild card (to match most possible verb inflections);</p><p>For example, for the question "Quantas vezes ganhou Portugal a Taça Davis", the previous procedure would be instantiated to:</p><p>1. "ganhou": "ganhar";</p><p>2. "ganhar": "poupar", "vencer", "conquistar", "perder", "ter" ("angariar", "dar", "disputar") 3. 'ganh*", "poup*", "venc*", "conquist*", "perd*", "ter"</p><p>All the options that result from expansion are then combined in a boolean OR to make the actual query. One can see from the results of Step 2 that there are some possibly problematic situation in the set of expanded verbs but two of the 5 expansion options -"vencer" and "conquistar" -seem to be clearly correct. Problematic situations that can occur include those related to the ambiguity of the source verb ("poupar") or the difficulty in identifying opposite senses / antonyms ("perder").</p><p>For the sake of comparison, if we performed expansion using the OpenOffice thesaurus for Portuguese -a manually created thesaurus -the verb "ganhar" would have only one expansion, "lucrar", which almost surely would not provide any benefit to retrieval, because it refers to a different sense of the original verb. This example clearly shows the type of problems that may arise from using manually created dictionaries for query expansion, and illustrates how statistical thesaurus, despite obvious errors, may actually lead to higher recall and a much less biased semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results at QA@CLEF 2008</head><p>In order to test the impact of our statistical query expansion approach in the performance of RAPOSA, we submitted two runs for evaluation at the 2008 CLEF QA track. RAPOSA was configured to execute one of two types of query expansion for factoid action / event factoid questions only (as explained before). The two distinct runs this year are:</p><p>• Run R 0 : in this run, query expansion is made through pseudo-stemming, i.e. by substituting the last characters of the verb by a wild-card. This was the method used in QA@CLEF 2007 version of RAPOSA, and in the current evaluation should be considered the baseline run.</p><p>Up to a maximum of 150 snippets could be retrieved and analyzed.</p><p>• Run R + : in this run, query expansion is made using the procedure described in Section 4.3: the verb is expanded to 5 statistically related verb options using the thesaurus and then pseudo-lematization is applied to each of the resulting options. A maximum of 25 snippets could be retrieved per expanded query, leading to a maximum of 150 snippets (for the original verb + up to 5 expanded verb options).</p><p>The  <ref type="table" coords="7,267.15,339.85,4.98,8.74" target="#tab_0">1</ref> it is important to take into account that RAPOSA is not trying to answer all questions in the test set. RAPOSA is not considering list questions (10 in 2008 test set) nor questions that require anaphoric resolution. For each "cluster" of anaphorically related questions, RAPOSA only tries to answer the first question in the cluster, because is the only one that is not dependent on anaphoric resolution. Basically, this means that RAPOSA did not even try to answer 51 dependent questions from the 2008 test set. In the following sections we will focus our analysis taking into account these factors. Still, we will make a brief comparison between the performance of RAPOSA in 2007 and 2008.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">2008 vs. 2007</head><p>The results in 2008 were somehow disappointing because RAPOSA was able to correctly answer only 25 questions in run R 0 , whereas in 2007 RAPOSA answered 38 questions, with the exact same working configuration. When comparing the 2007 and 2008 test sets we noticed that there is not significant difference between the number of factoid and the number of definition questions in both test sets. Also, the number of anaphorically related questions is approximately the same: 50 in 2007 vs. 51 in 2008.</p><p>A closer look at the 2007 and 2008 results shows that RAPOSA performance improved slightly this year for factoid questions. However performance for definitions questions dropped abruptly: from 16 correct answers in 2007 to just 4 in 2008. This suggests that this year's test set has harder definition questions than in 2007. The most important difference seems to be that the pattern of definition questions changed significantly in 2008. While most definition question in 2007 where person related definitions (e.g.: "Quem é George Vassiliou?" / "Who is George Vassiliou?"), the 2008 definition questions addressed canonical definitions (e.g. "O que é uma cítara?" / "What is a zither?"). In our opinion this is a more realistic scenario for QA evaluation, and is very appropriate for the Wikipedia collection. However, our work this year did not focus on these issues and, thus, RAPOSA was not prepared for answering canonical definitions. That will be subject of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluating Query Expansion</head><p>Since our query expansion method is to be applied only to a specific type of factoid questions, for the purposes of evaluating query expansion it only makes sense to observe results over that smaller subset of factoid question in test set. Nevertheless, Table <ref type="table" coords="7,376.03,731.28,4.98,8.74" target="#tab_1">2</ref>  questions RAPOSA was only able to parse 90 questions, i.e. only 56%. This relatively low parsing performance is due to the lack of a more complete base of parsing rules and to some unexpected encoding problems. For the correctly parsed questions, our query expansion method could be applied in 41 questions. Table <ref type="table" coords="8,278.13,236.24,4.98,8.74" target="#tab_2">3</ref> contains some statistics regarding performance on those 41 questions, differentiating results of using each of the two individual text collectionsthe XML dump of portuguese Wikipedia<ref type="foot" coords="8,273.28,258.57,3.97,6.12" target="#foot_0">1</ref> and CHAVE news collection -for extracting answers candidates. Columns indicate then number of questions for which no text snippet was found, "0 snip.", the total number of snippets found for all questions, "# snip.", the average number of snippets anaylized for each question (i.e. when at least one snippet was found), "avg. snip.", the number of nil answers, "nil answers", and the number of correct non-nil answers.  <ref type="table" coords="8,158.53,435.78,4.98,8.74" target="#tab_2">3</ref> one can make the following observations:</p><p>• query expansion allowed RAPOSA to find up to 4 additional answers, despite the fact that much less snippets were retrieved (and analyzed) for both collections;</p><p>• query expansion reduced significantly the number of questions for which no single text snippet was found;</p><p>• query expansion reduced significantly the number of nil answers, although this only allowed to improve the number of correct non-nil answers when the Wikipedia collection was used for extracting results;</p><p>• query expansion seems to have more success when used in the Wikipedia collection: all relevant parameters improved relatively more for Wikipedia than for CHAVE collection, when query expansion was used, but we are not sure if additional answers could in fact be found in CHAVE.</p><p>For better understanding why query expansion helped we looked in more detail to 4 questions that were correctly answered when using query expansion and Wikipedia. Table <ref type="table" coords="8,459.74,631.05,4.98,8.74" target="#tab_3">4</ref> shows the questions, the set of expanded verbs and the number of snippets retrieved. Verbs that helped retrieving snippets from which the correct answer was extracted are written in bold.</p><p>For question 0015 and 0091, query expansion allowed retrieving one relevant snippet for each of the indicated expanded verbs. In the case of question 0015, the connection between the expanded verb and the original verb is very strong (they are quasi-synonym in this context) and the positive effects of query expansion on the result are easy to understand. For question 0091 the verb at stake, ficar, is highly polysemous so the expansion provided by the automatically generated thesaurus looks less accurate, yet still quite reasonable. Nevertheless, it was enough for retrieving two snippets where the correct answer could be found.</p><p>For questions 0007 and 0063, the snippets that provided to the right answers were apparently retrieved when using the verb "ter". However, because our current retrieval system actually ignores any search term with less that 4 character, expansion with "ter" is virtually equivalent to excluding the verb from the query. This lead to the generation of less restrictive queries containing only the arguments of the question ("terramoto and Irão" and "Descobridores de Catan"), allowing to retrieve more snippets, in which the correct answers ended up being found.</p><p>Thus, we decided to experiment what happens if we totally remove the verb from the query, for each of the 41 question at test. Results are presented in Table <ref type="table" coords="9,361.16,380.32,3.88,8.74" target="#tab_4">5</ref>. For the Wikipedia collection the correct answers found are exactly the same as the ones found with query expansion. For CHAVE collection we actually found one additional correct answer. In both cases, when comparing with results obtained with query expansions, there are also much less nil answers but RAPOSA ended up analyzing about 3.5 times more snippets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>Results confirm that retrieving and analyzing more snippets does helps RAPOSA finding more corrects answers (higher recall). This seems to be more the case when the number of existing snippets available for extracting answers to a given question is very low. In those situations, if the answer is in fact included in one of the few retrieved snippets, RAPOSA seems to be able to find the it, specially because there are also less chances of choosing a wrong answer among the few possible candidates found. Questions 0007, 0015, 0063 and 0091 illustrate such type of situations. The big question is: is our query expansion approach useful? We first need to focus on the different reasons why query expansion helped. There were two cases where query expansion was clearly successful: question 0015 and 0091. For questions 0009 and 0063 our expansion method indirectly caused the removal of the verb from the query (due to the minimum 4 character threshold on our retrieval index) and this lead to retrieving more snippets and the correct answer. We shall consider these two later cases as resulting from luck, so we will not consider them as successful examples of our expansion method. We believe, however, that a different indexing mechanism (combined with a better strategy for building the statistical thesaurus) could eventually help to solve this problem and, thus increase the number of cases where our expansion approach can be considered valid.</p><p>Thus, if we consider only the cases where our expansion method clearly worked as intended, we conclude that query expansion is marginally beneficial in comparison with performing no expansion. When comparing with simple verb removal, query expansion does not help to achieve as many correct answers, but it also does not require analyzing so many text snippets for extracting candidates answers. However, because RAPOSA is producing many nil and incorrect answers it becomes very hard to access if expansion is really beneficial: do the much fewer snippets retrieved really contain the correct answers, even when RAPOSA is not able to find them (i.e. the problems occur later in the pipeline)? What would be the effect of query expansion if RAPOSA was able to correctly parse more questions? And, if another thesaurus was built, from a larger corpus or using linguistically informed methods, would that improve the results significantly?</p><p>At this point we can only conclude that there are too many important and basic limitations in RAPOSA at the level of question parsing, candidate extraction and candidate selection, to allow a thorough evaluation of the query expansion method we propose. Nevertheless, we believe that our expansion method can help improve RAPOSA performance when more of these problems are solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper we presented a query expansion system intended to improve recall in answering a event-related or action-related factoid questions. Expansion is achieved by using a verb thesaurus, automatically generated from corpora. We have showed that our expansion method provides a marginal increase in the recall of our question answering system when compared with not using any form of expansion. Query expansion seems to help RAPOSA in finding additional correct answers, while reducing the number of text snippets retrieved and analyzed. However, because of many other limitations in RAPOSA, it is not yet clear how much contribution is actually provided by our method, when compared to the extremely simple strategy of increasing recall by removing the verb from the query.</p><p>Future work will necessarily focus on improving specific stages of RAPOSA, namely implementing better Question Parsing and Snippets Searcher modules. We are currently implementing from scratch a generic wide-spectrum semantic analyzer system for portuguese, which will replace our current named-entity recognition system. The new analyzer will help to improve recall of the question parsing module so that we can increase the number of questions RAPOSA tries to answer. Also, a better analysis will help to achieve more efficient text preprocessing and indexing. We are also experimenting using native XML databases for storing pre-processed source collections, and for retrieving text snippets based on new semantic annotations. After these improvements in RA-POSA we will repeat these experiments with query expansion, possibly with thesaurus generated using information gathered by our new semantic parser.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,104.94,235.56,368.64,113.03"><head>Table 1 :</head><label>1</label><figDesc>global results obtained in CLEF 2008 for these runs were: Global results obtained on each run, R 0 and R + , for the 2008 test set When looking at results from Table</figDesc><table coords="7,182.56,258.22,237.88,33.44"><row><cell cols="6">Run Right Wrong ineXact Unsup. Accuracy</cell></row><row><cell>R 0</cell><cell>25</cell><cell>169</cell><cell>4</cell><cell>2</cell><cell>12.5 %</cell></row><row><cell>R +</cell><cell>29</cell><cell>165</cell><cell>4</cell><cell>2</cell><cell>14.5 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,90.00,731.28,423.01,20.69"><head>Table 2 :</head><label>2</label><figDesc>shows the performance of run R 0 and R + over all 162 factoid questions contained in the 2008 test set. From all 162 factoid Results obtained for factoid questions, R 0 and R + , for the 2008 test set</figDesc><table coords="8,173.43,110.82,256.16,33.44"><row><cell cols="6">Run Right Wrong ineXact Unsup. Accuracy(all)</cell></row><row><cell>R 0</cell><cell>21</cell><cell>138</cell><cell>1</cell><cell>2</cell><cell>12.96 %</cell></row><row><cell>R +</cell><cell>25</cell><cell>134</cell><cell>1</cell><cell>2</cell><cell>15.43 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,104.01,330.63,394.97,113.89"><head>Table 3 :</head><label>3</label><figDesc>Statistics for the 41 factoid questions when verb is removed from the query From Table</figDesc><table coords="8,104.01,330.63,394.97,58.15"><row><cell>Collection</cell><cell>mode</cell><cell cols="5">0 snip. # snip. avg. snip. nil ans. correct non-nil ans.</cell></row><row><cell cols="2">Wikipedia no expansion</cell><cell>28</cell><cell>296</cell><cell>22.77</cell><cell>31</cell><cell>1</cell></row><row><cell>Wikipedia</cell><cell>expansion</cell><cell>21</cell><cell>154</cell><cell>7.7</cell><cell>24</cell><cell>5</cell></row><row><cell>CHAVE</cell><cell>no expansion</cell><cell>29</cell><cell>303</cell><cell>25.25</cell><cell>32</cell><cell>1</cell></row><row><cell>CHAVE</cell><cell>expansion</cell><cell>24</cell><cell>173</cell><cell>10.18</cell><cell>27</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,115.02,110.82,371.11,128.85"><head>Table 4 :</head><label>4</label><figDesc>The 4 additional questions that were correctly answered in R + .</figDesc><table coords="9,115.02,110.82,371.11,105.97"><row><cell>#</cell><cell>Question</cell><cell>Verb Expansion</cell><cell># snip.</cell></row><row><cell cols="3">0007 Em que ano houve um terramoto no Irão? ter, garantir, obter,</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>permitir, estabelecer</cell><cell></cell></row><row><cell cols="2">0015 Quem escreveu Fernão Capelo Gaivota?</cell><cell>ler, criar, ver, pub-</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>licar, seleccionar</cell><cell></cell></row><row><cell cols="2">0063 Quem criou Descobridores de Catan?</cell><cell>construir, desenvolver,</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell>obter, produzir, ter</cell><cell></cell></row><row><cell cols="2">0091 Em que ilha fica Sapporo?</cell><cell>estar, viver, andar,</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell>trabalhar, entrar</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,90.00,450.80,423.00,77.23"><head>Table 5 :</head><label>5</label><figDesc>Statistics for the 41 factoid questions in which our query expansion method could be applied, when applying verb removal instead of query expansion</figDesc><table coords="9,104.43,450.80,394.14,33.44"><row><cell>Collection</cell><cell>mode</cell><cell cols="5">0 snip. # snip. avg. snip. nil ans. correct non-nil ans.</cell></row><row><cell cols="2">Wikipedia verb removal</cell><cell>15</cell><cell>689</cell><cell>26.5</cell><cell>19</cell><cell>5</cell></row><row><cell>CHAVE</cell><cell>verb removal</cell><cell>16</cell><cell>901</cell><cell>36.0</cell><cell>21</cell><cell>2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="8,105.24,733.66,315.30,7.21"><p>Available from the University of Amsterdam: http://ilps.science.uva.nl/WikiXML</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgments</head><p>This work was partially supported by grant <rs type="grantNumber">SFRH/BD/ 23590/2005</rs> from <rs type="funder">Fundação para a Ciência e Tecnologia (Portugal)</rs>, co-financed by <rs type="funder">POSI</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qQfsjjh">
					<idno type="grant-number">SFRH/BD/ 23590/2005</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,110.48,694.75,402.51,8.74;10,110.48,704.19,402.52,11.26;10,110.48,718.66,48.71,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,248.96,694.75,264.03,8.74;10,110.48,706.71,30.05,8.74">Jspell -um módulo para análise léxica genérica de linguagem natural</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulisses</forename><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,160.39,706.71,267.58,8.74">Actas do X Encontro da Associação Portuguesa de Linguística</title>
		<meeting><address><addrLine>Évora</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994. 1995</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,112.02,402.53,8.74;11,110.48,123.98,402.53,8.74;11,110.48,135.93,387.76,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,327.91,112.02,185.09,8.74;11,110.48,123.98,201.72,8.74">What works better for question answering: Stemming or morphological query expansion?</title>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Bilotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,334.50,123.98,178.51,8.74;11,110.48,135.93,250.29,8.74">Proceedings of the Information Retrieval for Question Answering (IR4QA) Workshop. SIGIR 2004</title>
		<meeting>the Information Retrieval for Question Answering (IR4QA) Workshop. SIGIR 2004<address><addrLine>Sheffield, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,155.06,402.51,8.74;11,110.48,167.01,402.53,8.74;11,110.48,178.97,259.20,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,290.31,155.06,222.68,8.74;11,110.48,167.01,98.33,8.74">VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations</title>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><surname>Chklovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,247.22,167.01,265.78,8.74;11,110.48,178.97,148.07,8.74">Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-04)</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing (EMNLP-04)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,198.10,402.52,8.74;11,110.48,210.05,245.10,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,273.96,198.10,239.03,8.74;11,110.48,210.05,36.68,8.74">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName coords=""><forename type="first">Kenneth</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,156.16,210.05,113.08,8.74">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,229.18,402.52,8.74;11,110.48,241.14,402.52,8.74;11,110.48,253.09,169.74,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,262.48,229.18,245.88,8.74">Component evaluation in a question answering system</title>
		<author>
			<persName coords=""><forename type="first">Luís</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luís</forename><surname>Sarmento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,123.85,241.14,389.14,8.74;11,110.48,253.09,56.60,8.74">Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC&apos;2006)</title>
		<meeting>the 5th International Conference on Language Resources and Evaluation (LREC&apos;2006)<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,272.22,402.53,8.74;11,110.48,284.17,402.53,8.74;11,110.48,296.13,175.53,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,332.99,272.22,180.01,8.74;11,110.48,284.17,73.27,8.74">On the effective use of cyc in a question answering system</title>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gavin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,202.96,284.17,310.05,8.74;11,110.48,296.13,48.48,8.74">IJCAI Workshop on Knowledge and Reasoning for Answering Questions (KRAQ&apos;05)</title>
		<meeting><address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,315.26,402.52,8.74;11,110.48,327.21,98.82,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,243.04,315.26,186.12,8.74">WordNet: An Electronic Lexical Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,346.34,402.52,8.74;11,110.48,358.30,90.83,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,184.64,346.34,266.22,8.74">Automatic Acquisition of Hyponyms from Large Text Corpora</title>
		<author>
			<persName coords=""><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,470.26,346.34,36.63,8.74">COLING</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,377.42,402.52,8.74;11,110.48,389.38,402.52,8.74;11,110.48,401.33,203.47,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,474.20,377.42,38.81,8.74;11,110.48,389.38,109.64,8.74">Question answering in webclopedia</title>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurie</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Junk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,243.06,389.38,214.09,8.74">Proceedings of the 9th Text REtrieval Conference</title>
		<meeting>the 9th Text REtrieval Conference<address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-11">November 2000</date>
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,420.46,402.51,8.74;11,110.48,432.42,402.52,8.74;11,110.48,444.37,402.53,8.74;11,110.48,456.33,25.73,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,288.83,420.46,224.17,8.74;11,110.48,432.42,73.41,8.74">Retrieving answers from frequently asked questions pages on the web</title>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,206.32,432.42,306.67,8.74;11,110.48,444.37,180.65,8.74">CIKM &apos;05: Proceedings of the 14th ACM International Conference on Information and Knowledge Management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="76" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,475.46,402.53,8.74;11,110.48,487.41,166.39,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,193.89,475.46,255.32,8.74">CYC: A large-scale investment in knowledge infrastructure</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lenat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,457.75,475.46,55.25,8.74;11,110.48,487.41,72.44,8.74">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="33" to="38" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,506.54,402.52,8.74;11,110.48,518.50,275.71,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,175.86,506.54,243.70,8.74">Automatic Retrieval and Clustering of Similar Words</title>
		<author>
			<persName coords=""><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,449.76,506.54,63.24,8.74;11,110.48,518.50,86.14,8.74">Proceedings of COLING-ACL 1998</title>
		<meeting>COLING-ACL 1998<address><addrLine>Montreal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="768" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,537.62,402.52,8.74;11,110.48,549.58,402.52,8.74;11,110.48,561.53,108.63,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,175.16,537.62,258.55,8.74">Sense-based blind relevance feedback for question answering</title>
		<author>
			<persName coords=""><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,146.02,549.58,361.71,8.74">SIGIR-2004 Workshop on Information Retrieval For Question Answering (IR4QA)</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,580.66,402.53,8.74;11,110.48,592.62,155.84,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,308.28,580.66,182.83,8.74">Automatically Labeling Semantic Classes</title>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,110.48,592.62,55.52,8.74">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,611.75,402.52,8.74;11,110.48,623.70,402.53,8.74;11,110.48,635.66,402.52,8.74;11,110.48,647.61,117.61,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,110.48,623.70,314.61,8.74">Statistical machine translation for query expansion in answer retrieval</title>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Vasserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vibhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,450.54,623.70,62.47,8.74;11,110.48,635.66,330.17,8.74">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">June 23-30 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,666.74,402.52,8.74;11,110.48,678.69,402.52,8.74;11,110.48,690.65,114.90,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,267.06,666.74,245.94,8.74;11,110.48,678.69,54.16,8.74">The Senso question answering approach to portuguese qa@clef-2007</title>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Saias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Quaresma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,185.85,678.69,275.42,8.74">Proceedings of Cross Language Evaluation Forum (CLEF 2007)</title>
		<meeting>Cross Language Evaluation Forum (CLEF 2007)<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,709.78,402.52,8.74;11,110.48,721.73,402.52,8.74;11,110.48,733.69,402.52,8.74;11,110.48,745.64,324.44,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,180.32,709.78,227.53,8.74">BACO -A large database of text and co-occurrences</title>
		<author>
			<persName coords=""><forename type="first">Luís</forename><surname>Sarmento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,180.79,733.69,332.21,8.74;11,110.48,745.64,106.64,8.74">Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC&apos;2006)</title>
		<editor>
			<persName><forename type="first">Nicoletta</forename><surname>Calzolari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aldo</forename><surname>Gangemi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bente</forename><surname>Maegaard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joseph</forename><surname>Mariani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan</forename><surname>Odjik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Tapias</surname></persName>
		</editor>
		<meeting>the 5th International Conference on Language Resources and Evaluation (LREC&apos;2006)<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="page" from="22" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,112.02,402.52,8.74;12,110.48,123.98,402.52,8.74;12,110.48,135.93,402.52,8.74;12,110.48,147.89,402.52,8.74;12,110.48,159.84,316.53,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,179.31,112.02,290.21,8.74">A first step to address biography generation as an iterative QA task</title>
		<author>
			<persName coords=""><forename type="first">Luís</forename><surname>Sarmento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,353.24,135.93,159.76,8.74;12,110.48,147.89,137.38,8.74">7th Workshop of the Cross-Language Evaluation Forum, CLEF 2006</title>
		<title level="s" coord="12,110.48,159.84,152.88,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maximilian</forename><surname>Stempfhuber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain; Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006-09">September 2006. 2007</date>
		</imprint>
	</monogr>
	<note>Revised Selected papers</note>
</biblStruct>

<biblStruct coords="12,110.48,179.77,402.52,8.74;12,110.48,191.72,402.52,8.74;12,110.48,203.68,235.34,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,283.81,179.77,126.86,8.74">Making raposa (fox) smarter</title>
		<author>
			<persName coords=""><forename type="first">Luís</forename><surname>Sarmento</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugénio</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,226.36,191.72,286.64,8.74;12,110.48,203.68,65.25,8.74">Working Notes of the Cross-Language Evaluation Forum (CLEF) Workshop 2007</title>
		<editor>
			<persName><forename type="first">Alessandro</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,223.60,402.53,8.74;12,110.48,235.56,222.24,8.74" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,244.26,223.60,251.55,8.74">Query expansion using local and global document analysis</title>
		<author>
			<persName coords=""><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,110.48,235.56,100.21,8.74">Proceeding of SIGIR&apos;96</title>
		<meeting>eeding of SIGIR&apos;96<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
