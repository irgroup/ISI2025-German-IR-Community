<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,122.71,148.79,357.58,15.48;1,179.07,170.71,244.86,15.48">The LogAnswer Project at CLEF 2008: Towards Logic-Based Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,220.40,203.69,69.39,10.37"><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
							<email>ingo.gloeckner@fernuni-hagen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Information and Communication Systems Group (IICS)</orgName>
								<orgName type="institution">University of Hagen</orgName>
								<address>
									<postCode>59084</postCode>
									<settlement>Hagen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.77,203.69,60.10,10.37"><forename type="first">Björn</forename><surname>Pelzer</surname></persName>
							<email>bpelzer@uni-koblenz.de</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Artificial Intelligence Research Group</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Koblenz-Landau</orgName>
								<address>
									<addrLine>Universitätsstr. 1</addrLine>
									<postCode>56070</postCode>
									<settlement>Koblenz</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,122.71,148.79,357.58,15.48;1,179.07,170.71,244.86,15.48">The LogAnswer Project at CLEF 2008: Towards Logic-Based Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">28E0E826A600CB99F4C126CED21E2E70</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Search Process, Selection process</term>
					<term>I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-Predicate Logic, Semantic networks</term>
					<term>I.2.7 [Artificial Intelligence]: Natural Language Processing Experimentation, Measurement, Verification Logical Question Answering, Real-Time Question Answering, Answer Selection, Robust Inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>LogAnswer is a logic-oriented question answering system jointly developed by the AI research group at the University of Koblenz-Landau and by the IICS at the University of Hagen. The system was designed to address two notorious problems of the logic-based approach: Achieving robustness and acceptable response times. The main innovation of LogAnswer is its use of logic for simultaneously extracting answer bindings and validating the corresponding answers. In this way the inefficiency of the classical answer extraction/answer validation pipeline is avoided. The prototype of the system, which can also be tested on the web, demonstrates response times suitable for real-time querying. Emphasis was also placed on developing techniques for making the logic-based approach more robust against gaps in the background knowledge and against errors of linguistic analysis. To this end, the optimized deductive subsystem is combined with shallow techniques by machine learning. The same background knowledge as in the MAVE validator of the IICS presented at CLEF 2007 was used: 10,000 lexical-semantic relations (e.g. describing nominalizations), 109 logical rules, and a list of synonyms covering more than 111,000 lexical constants which is also utilized for determining the shallow features. Two monolingual runs of LogAnswer for German were submitted to QA@CLEF 2008. The results of 29 correct answers in the best run (accuracy: 0.145) indicate that further development of the current prototype is necessary. An error analysis shows that the linguistic processing and also the coreference resolution generally performed quite well. The rudimentary implementation of answer extraction based on the answer substitution determined by the prover must be improved, though, since extracted answers for appositions and constructions involving a defining verb are not reliable yet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The system architecture of the LogAnswer QA system is shown in Fig. <ref type="figure" coords="2,374.51,451.34,3.74,8.64" target="#fig_0">1</ref>. In the following we describe the processing stages of the system.</p><p>Question Input In normal operation of the system, the natural language question of the user is entered into the LogAnswer web search box. <ref type="foot" coords="2,236.64,500.10,3.49,6.05" target="#foot_1">4</ref> For QA@CLEF, a batch querying option was added.</p><p>Deep Linguistic Processing of the Question The question is analyzed by the WOCADI parser <ref type="bibr" coords="2,498.89,528.30,10.58,8.64" target="#b6">[7]</ref>, which generates a semantic representation of the question in the MultiNet formalism <ref type="bibr" coords="2,440.49,540.25,10.58,8.64" target="#b7">[8]</ref>. The standard coreference resolution module of WOCADI is used for treating follow-up questions involving pronouns and nominal anaphora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Classification</head><p>The question classification of LogAnswer, based on 127 recognition rules, identifies the category (factual vs. definition) and the expected answer type (e.g. PERSON) of the question. The descriptive core of the question is also identified. Consider Nennen Sie einige einfache Elementarteilchen! ('Name some elementary particles!'). Then nenne ('name') is not treated as part of the query content since it only specifies what the system should do but does not describe the correct answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support Passage Retrieval</head><p>The document collection of LogAnswer comprises the November 2006 snapshot of the German Wikipedia; for QA@CLEF, the news collection of CLEF was also added. In order to avoid parsing of documents at query time, all documents are pre-analyzed by the WOCADI parser. The resulting MultiNet representations are segmented into passages and stored in a Lucene-based retrieval module. <ref type="foot" coords="2,122.38,711.19,3.49,6.05" target="#foot_2">5</ref> An interesting aspect of LogAnswer is the kind of information stored in the index:</p><p>• The system uses lexical concepts (word senses from the lexicon) rather than word forms or word stems for indexing. At the moment, all possible word senses for each word are indexed.</p><p>• Synonymy relationships are utilized for replacing all possible synonym variants by a canonical representation. <ref type="foot" coords="3,173.83,154.50,3.49,6.05" target="#foot_3">6</ref> For example, the lexical concept attacke.1.1 (attack) is replaced by the canonical angriff.1.1 during indexing. A similar normalization at query time ensures that all synonym variants can be used for retrieval.</p><p>• Nominalizations are utilized for indexing: if the text contains erfindung.1.1 (invention), for example, then erfinden.1.1 (word sense of 'to invent') is also added to the index, and vice versa.</p><p>• Compound decompositions are added to the index. For example, if the text contains an occurrence of verteidigungsminister.1.1 (minister of defence), then minister.1.1 is also indexed.</p><p>• Adjective-attribute relationships are expanded. Thus, an occurrence of hoch.1.1 (high) results in höhe.1.1 (height) to be indexed as well.</p><p>Moreover all answer types contained in a sentence are indexed. In order to improve retrieval results for definition question, information about the containment of appositions, relative clauses, copula constructions, and defining verbs like stehen für ('stand for'), is also stored in the index. Notice that only sentences with a successful parse were indexed since the subsequent logic-based answer extraction requires the semantic representation constructed by the parser. For generating the submitted runs, the system was configured to retrieve 100 supporting snippets per question.</p><p>Shallow Feature Extraction and Reranking In order to avoid logical processing of all retrieved passages, LogAnswer tries to identify the most promising cases by reranking passages using shallow features which can be computed very quickly without the help of the prover. These features comprise: failedMatch (number of lexical concepts and numerals in the question which cannot be matched with the candidate document); matchRatio (relative proportion of lexical concepts and numerals in the question which find a match in the candidate document); failedNames (proper names mentioned in the question, but not in the passage); containsBrackets (the passage contains a pair of parentheses); knownEat (the expected answer type is known); testableEat (the expected answer type is fully supported by the current implementation of the answer type check); eatFound (an occurrence of the expected answer type has been found in the snippet); isDefQuestion (the question is a definition question). The defLevel feature is useful for definition questions. A value of defLevel = 2 indicates that the snippet contains a defining verb or apposition, and defLevel = 1 indicates a relative clause. Finally, there is an irScore feature which provides the retrieval score determined by Lucene. The machine learning approach used for reranking the retrieved snippets based on the shallow features is the same as in <ref type="bibr" coords="3,281.54,537.37,10.79,8.64" target="#b4">[5,</ref><ref type="bibr" coords="3,295.24,537.37,7.19,8.64" target="#b5">6]</ref>. The Weka toolbench <ref type="bibr" coords="3,396.32,537.37,16.60,8.64" target="#b11">[12]</ref> was used for implementation. The training data consisted of 17,350 annotated snippets retrieved in a run of LogAnswer on the QA@CLEF 2007 questions.</p><p>Logical Query Construction The semantic network for the question is turned into a conjunctive list of query literals. For example, Wie hoch ist der chilenische Berg La Silla? ('How high is the Chilean mountain La Silla?') translates into the following logical query (with the F OCU S variable representing the queried information): During query construction, concept identifiers of synonyms are normalized by replacing the original concept identifiers with canonical synset representatives (however, no replacement occurs in the example).</p><p>Robust Logic-Based Processing LogAnswer uses logic for simultaneously extracting and validating answers. To this end, the system tries to prove the logical representation of the question from the representation of the passage and the background knowledge. <ref type="foot" coords="4,292.94,134.58,3.49,6.05" target="#foot_4">7</ref> Robustness is gained by using relaxation: if a proof is not found within a time limit, then query literals are skipped until a proof of the remaining query succeeds, and the skip count indicates (non-)entailment <ref type="bibr" coords="4,276.47,160.16,10.79,8.64" target="#b2">[3,</ref><ref type="bibr" coords="4,290.39,160.16,7.19,8.64" target="#b5">6]</ref>. For efficiency reasons, relaxation is stopped before all literals are proved or skipped; a maximum of 3 relaxation cycles was chosen for the QA@CLEF runs. Notice that relaxation does not necessarily find the largest provable query fragment, since it only inspects a single sequence of simplification steps. Moreover the choice of skipped literals usually depends on factors like internal literal order of the prover which are arbitrary to some degree. It therefore makes sense to abstract from such idiosyncratic aspects, by combining relaxation results of different provers. LogAnswer has interfaces to two provers in order to permit such a combination:</p><p>• The system includes a native prover for MultiNet representations, which is part of the MWR+ toolbench. <ref type="foot" coords="4,141.19,261.68,3.49,6.05" target="#foot_5">8</ref> The MultiNet prover is very limited in expressive power (it only supports inferences over range-restricted horn formulas), but its specialization to the task ensures high efficiency <ref type="bibr" coords="4,465.98,275.30,10.58,8.64" target="#b3">[4]</ref>.</p><p>• E-KRHyper <ref type="bibr" coords="4,165.69,295.02,16.60,8.64" target="#b9">[10]</ref> is the latest version in the KRHyper-series of theorem provers and model generation systems for first-order logic with equality developed at the University Koblenz-Landau. It is an implementation of the E-hyper tableau calculus <ref type="bibr" coords="4,306.40,318.93,10.58,8.64" target="#b1">[2]</ref>, which integrates a superposition-based handling of equality into the hyper tableau calculus <ref type="bibr" coords="4,293.47,330.88,10.58,8.64" target="#b0">[1]</ref>. E-KRHyper is capable of handling large sets of uniformly structured input facts, and it can rapidly switch and retract input clause sets for an efficient usage as a reasoning server. Embedded in the LogAnswer system, E-KRHyper is supplied with the MultiNet axioms transformed into first-order TPTP syntax <ref type="bibr" coords="4,348.97,366.75,15.27,8.64" target="#b10">[11]</ref>. The inference process then operates on the axioms and the negated query literals, with a refutation result indicating a successful answer and providing the binding for the queried variable. If the reasoning is interrupted due to exceeding the time limit, then partial results can be retrieved that can guide in the relaxation process <ref type="bibr" coords="4,473.38,402.61,10.58,8.64" target="#b8">[9]</ref>.</p><p>Answer Extraction If a proof of the question from a passage succeeds, then LogAnswer obtains an answer binding which represents the queried information. In order to find more answers, LogAnswer also tries to determine a substitution when a strict proof of the query fails. The system then resorts to the intermediate substitution of the prover for the largest proven fragment of the query. LogAnswer uses word alignment information provided by WOCADI for extracting the corresponding answer string from the supporting text passage.</p><p>Logic-Based Feature Extraction In the following it will be assumed that the answer extraction was successful, i.e. a binding to the queried variable was found and the system has managed to determine the corresponding answer string. Based on the results of the relaxation proof and on the extracted answer, Log-Answer then determines the following logic-oriented features: skippedLitsLb (number of literals skipped in the relaxation proof); skippedLitsUb (number of skipped literals, plus literals with unknown status); litRatioLb (relative proportion of actually proved literals compared to the total number of query literals, i.e. 1 -skippedLitsUb/allLits); litRatioUb (relative proportion of potentially provable literals vs. all query literals, i.e. 1 -skippedLitsLb/allLits); npFocus (the queried variable was bound to a constant which corresponds to a nominal phrase in the text); focusEatMatch (the answer type of the answer binding found by the prover matches the expected answer type). The focusDefLevel feature is relevant for definition questions. A value of focusDefLevel = 2 indicates that the answer binding found by the prover corresponds to an apposition, and focusDefLevel = 1 occurs if the answer binding corresponds to a noun phrase involving a relative clause.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logic-Based Scoring</head><p>The logic-based answer scores are computed by the same ML approach also used for the shallow reranking. However, the shallow and logic-based features are now combined for better precision. In regular operation of LogAnswer, passages are considered in the order determined by the shallow feature-based ranking, and the logical processing is stopped after a pre-defined time limit. For QA@CLEF, this mechanism was switched off, i.e. all passages were considered for deep processing and answer extraction.</p><p>Support Passage Selection Depending on user preferences, the system answers the question either by presenting supporting text passages only, or alternatively, by presenting exact answers together with the supporting passage. For QA@CLEF, only the precise answer mode was relevant.</p><p>Sanity Checks Two sanity checks are applied in order to eliminate false positives: A triviality check eliminates answers which only repeat contents of the question. For the question 'Who is Virginia Kelley?', this test rejects trivial answers like 'Virginia' or 'Virginia Kelley'. A special sanity check also rejects incomplete answers to definition questions. For example, 'the mother of Bill Clinton' is a correct answer to the above question, but 'the mother' must be rejected as incomplete. Notice that the compatibility of expected and found answer type is not treated as a strict sanity check but rather encoded by answer-type related features passed to the machine learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregation and Answer Selection</head><p>The answer integration module computes a global score for each answer, based on the local score for each passage from which the answer was extracted. The aggregation method is detailed in <ref type="bibr" coords="5,178.86,431.21,10.58,8.64" target="#b4">[5]</ref>. The k = 3 distinct answers with the highest aggregated scores were selected for the submitted QA@CLEF runs. For each answer, the supporting passage with the highest score was selected as the justification for the considered answer.</p><p>2 Results on the QA@CLEF Test Set for German</p><p>The results of LogAnswer in the QA@CLEF 2008 task are shown in Table <ref type="table" coords="5,393.47,510.88,3.74,8.64" target="#tab_0">1</ref>. The first run, loga081dede, used only the native prover of the MultiNet toolkit for logical processing. The second run, loga082dede, used a combination of the MultiNet prover and of the E-KRHyper prover based on the 'OPT' method described in <ref type="bibr" coords="5,139.95,546.74,10.58,8.64" target="#b5">[6]</ref>. The motivation for using more than one prover is that following several relaxation paths by applying several provers might increase the chance of discovering a good provable query fragment. While the combination of both provers worked well in earlier experiments based on cross-validation reported in <ref type="bibr" coords="5,90.00,582.61,10.58,8.64" target="#b5">[6]</ref>, results in the QA@CLEF 2008 task were slightly worse for the combined method compared to the first run which used only one prover.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Error Analysis</head><p>An error analysis was made for the loga081dede run in order to identify the main deficits of the subsystems of LogAnswer. Concerning the linguistic processing stage, it was found that parsing of the question failed for 4 out of the 200 questions. Moreover the coreference resolution produced useless results (like unresolved pronouns) for 5 questions. Thus, the linguistic processing of the question was successful for 191 out of 200 questions in the QA@CLEF test set for German. Turning to the passage retrieval stage, the 19,064 retrieved supporting sentences (95.32 per question) were assessed for containment of a correct answer. The annotation revealed that for 119 of the questions, at least one passage which provides a correct answer was retrieved (see Fig. <ref type="figure" coords="5,211.36,734.01,4.98,8.64" target="#fig_3">2</ref> for more details on the performance of the passage retrieval module). This means that, assuming perfect answer extraction and validation, the system can theoretically answer 119  In order to improve this number, the retrieval stage should be optimized. The following improvements are likely the most urgent:</p><p>• The retrieval module of LogAnswer is configured to return only 100 support sentences per question.</p><p>Increasing this number will improve recall at the cost of more processing effort. Experiments should be made in order to find a good trade-off for these factors.</p><p>• The restriction to single-sentence snippets must be dropped. The system should use coreference resolution for elaborating the content analysis of the text. Moreover the document date should be taken into account for resolving deictic temporal expressions (like 'yesterday').</p><p>• At present, the system only indexes sentences with a perfect parse. This means that only about 60% of all sentences in the corpus are visible to LogAnswer. In order to improve recall, non-parseable answers should be indexed as well and a fallback method for answer extraction from answers without a parse must be added.</p><p>Another significant source of errors is answer extraction. LogAnswer found 26 correct non-NIL answers. However, 46 of the supporting snippets for the top-1 answers actually contain a correct answer. Assuming perfect selection, the system would therefore be able to correctly answer 46 non-NIL questions. This means that the success rate of answer extraction for sentences at top-1 position is 56.5%. Considering all top-3 results, we find that the achieved MRR for 120 questions with multiple answers was 0.1944, while for perfect extraction, an MRR of 0.3222 would have been possible in the loga081dede run. In practice, the ranking is not fixed though, but rather depends on the results of answer extraction. Therefore the overall success rate of the answer extraction stage for arbitrary snippets is even lower. The reason of these problems are two phenomena not adequately treated in LogAnswer yet:</p><p>• The answer is often expressed by an apposition, as in Albert Einstein, der Begründer der Relativitätstheorie ('Albert Einstein, the founder of the theory of relativity'). In this case, the answer extractor must not return the full noun phrase which corresponds to the answer binding of the queried variable. By contrast, it is necessary to split the extracted noun phrase and identify the relevant part.</p><p>• Copula constructions and constructions involving defining verbs also pose problems for the logicbased extraction method. If the sentence has a form such as 'X is Y ' or 'X means Y ', then the logic-based answer extraction will often extract X even though the question targets at Y . These problems result in wrong or inexact extractions, as witnessed by the relatively large number of 11 inexact answers of LogAnswer in the loga081dede run.</p><p>With LogAnswer, we have developed a logic-based QA system suitable for real-time question answering. Earlier experiments on the QA@CLEF 2007 questions for German confirm that the system works well when used for finding supporting sentences that contain an answer to the question <ref type="bibr" coords="7,428.97,158.52,10.79,8.64" target="#b3">[4,</ref><ref type="bibr" coords="7,443.05,158.52,7.19,8.64" target="#b5">6]</ref>. However, the naive solution for extracting exact answers that was added for generating runs for QA@CLEF 2008 is not yet reliable enough. Especially appositions, copula constructions and defining verbs pose problems to the current implementation of logic-based answer extraction. Nevertheless, the simultaneous extraction of answer bindings and validation features from a relaxation proof of the question from the supporting snippet should be investigated further, since it avoids the extraction of a large number of answer candidatates from which the few correct ones must then be selected by extensive validation. An intrinsic problem of logicbased answer extraction is that the method only works for snippets with a full parse. For the remaining sentences, there is no meaning representation of the snippet on the logical level and the prover cannot be applied. In order to overcome this limitation, the logic-based extraction should be complemented with a shallow QA technique which can be used for finding answers in snippets with a failed parse.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,183.33,397.02,233.85,8.64;2,111.15,108.86,380.68,262.77"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System architecture of the LogAnswer prototype</figDesc><graphic coords="2,111.15,108.86,380.68,262.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,157.04,645.54,290.45,8.06;3,157.04,659.77,233.50,7.77"><head></head><label></label><figDesc>modp(X1, F OCU S, hoch.1.1), sub(X2, berg.1.1), prop(X2, X1), attr(X2, X3), prop(X2, chilenisch.1.1), val(X3, la silla.0), sub(X3, name.1.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,105.56,312.84,391.88,8.64"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Number of questions with a given number of retrieved passages that answer the question</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,142.60,119.13,317.80,54.83"><head>Table 1 :</head><label>1</label><figDesc>Results of LogAnswer in QA@CLEF 2008</figDesc><table coords="5,142.60,141.02,317.80,32.95"><row><cell>Run</cell><cell cols="5">#Right #Unsupported #Inexact #Wrong CWS MRR</cell></row><row><cell cols="2">loga081dede 29</cell><cell>1</cell><cell>11</cell><cell>159</cell><cell>0.032 0.194</cell></row><row><cell cols="2">loga082dede 27</cell><cell>1</cell><cell>9</cell><cell>163</cell><cell>0.029 0.179</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="1,104.35,729.12,408.65,6.91;1,90.00,738.59,107.81,6.91"><p>Funding of this work by the DFG (Deutsche Forschungsgemeinschaft) under contracts FU 263/12-1 and HE 2847/10-1 (LogAnswer) is gratefully acknowledged.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,104.35,732.16,184.40,6.91"><p>The system is available online at www.loganswer.de.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="2,104.35,741.89,408.63,6.91"><p>Notice that at present, only single-sentence snippets are considered, but an extension to larger passages is planned for the future.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="3,104.35,710.81,247.48,6.91"><p>The system uses 48,991 synsets (synonym sets) for 111,436 lexical constants.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="4,104.35,728.07,408.65,6.91;4,90.00,737.53,390.40,6.91"><p>The background knowledge of LogAnswer comprises 10,000 lexical-semantic facts (e.g. for nominalizations) and 109 logical rules, which define main characteristics of MultiNet relations and also handle meta verbs like 'stattfinden' (take place)<ref type="bibr" coords="4,469.11,737.53,8.46,6.91" target="#b2">[3]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="4,104.35,747.26,224.40,6.91"><p>See http://pi7.fernuni-hagen.de/research/mwrplus</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,111.58,333.78,401.42,8.82;7,111.58,345.91,72.50,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,336.42,333.96,62.56,8.64">Hyper Tableaux</title>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulrich</forename><surname>Furbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilkka</forename><surname>Niemelä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,417.80,333.78,90.88,8.59">JELIA&apos;96, Proceedings</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.58,365.66,401.42,8.82;7,111.58,377.61,170.83,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,331.30,365.84,119.47,8.64">Hyper Tableaux with Equality</title>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulrich</forename><surname>Furbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Pelzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,470.03,365.66,42.97,8.59;7,111.58,377.61,141.60,8.59">Automated Deduction -CADE-21, Proceedings</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.58,397.54,401.42,8.82;7,111.58,409.49,212.17,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,177.48,397.72,281.32,8.64">University of Hagen at QA@CLEF 2007: Answer validation exercise</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,479.61,397.54,33.39,8.59;7,111.58,409.49,140.54,8.59">Working Notes for the CLEF 2007 Workshop</title>
		<meeting><address><addrLine>Budapest</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.58,429.42,401.42,8.82;7,111.58,441.37,366.33,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,174.85,429.60,251.40,8.64">Towards logic-based question answering under time constraints</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,444.34,429.42,68.65,8.59;7,111.58,441.37,285.68,8.59">Proc. of the 2008 IAENG Int. Conf. on Artificial Intelligence and Applications (ICAIA-08)</title>
		<meeting>of the 2008 IAENG Int. Conf. on Artificial Intelligence and Applications (ICAIA-08)<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.58,461.30,401.42,8.82;7,111.58,471.89,239.09,10.19" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,177.48,461.48,281.32,8.64">University of Hagen at QA@CLEF 2008: Answer validation exercise</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,479.61,461.30,33.39,8.59;7,111.58,473.26,138.22,8.59">Working notes for the CLEF 2008 workshop</title>
		<meeting><address><addrLine>Århus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.58,493.36,401.42,8.64;7,111.58,505.14,345.29,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,240.81,493.36,268.78,8.64">Exploring robustness enhancements for logic-based passage filtering</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Pelzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,122.37,505.14,72.76,8.59">Proc. of KES-2008</title>
		<title level="s" coord="7,202.45,505.32,140.52,8.64">Lecture Notes in Computer Science</title>
		<meeting>of KES-2008</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="7,111.58,525.06,401.42,8.82;7,111.58,537.20,100.74,8.64" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,185.58,525.06,219.66,8.59">Hybrid Disambiguation in Natural Language Analysis</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Der Andere Verlag</publisher>
			<pubPlace>Osnabrück, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.58,556.94,401.42,8.82" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,182.74,556.94,262.17,8.59">Knowledge Representation and the Semantics of Natural Language</title>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Helbig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.58,577.05,401.42,8.64;7,111.58,588.82,401.42,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,247.46,577.05,248.19,8.64">Combining theorem proving with natural language processing</title>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Pelzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,111.58,588.82,266.13,8.59">Workshop on Practical Aspects of Automated Reasoning (PAAR-08)</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-08">August 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.58,608.75,401.42,8.82;7,111.58,620.70,186.05,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,269.30,608.93,130.32,8.64">System Description: E-KRHyper</title>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Pelzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><surname>Wernhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,419.85,608.75,93.15,8.59;7,111.58,620.70,91.80,8.59">Automated Deduction -CADE-21, Proceedings</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="508" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.58,640.63,401.42,8.82;7,111.58,652.58,179.12,8.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,266.23,640.81,196.88,8.64">The TPTP Problem Library: CNF Release v1.2.1</title>
		<author>
			<persName coords=""><forename type="first">Geoff</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Suttner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,471.68,640.63,41.32,8.59;7,111.58,652.58,85.56,8.59">Journal of Automated Reasoning</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="203" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.58,672.51,401.42,8.82;7,111.58,684.64,103.25,8.64" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<title level="m" coord="7,244.97,672.51,263.78,8.59">Data Mining. Practical Machine Learning Tools and Techniques</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
