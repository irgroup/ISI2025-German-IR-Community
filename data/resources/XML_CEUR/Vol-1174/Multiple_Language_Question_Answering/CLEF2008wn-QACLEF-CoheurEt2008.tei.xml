<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.51,148.86,55.86,15.15;1,199.38,146.25,5.86,10.48;1,205.73,148.86,253.76,15.15">QA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,204.08,182.75,56.20,8.74"><forename type="first">Luísa</forename><surname>Coheur</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INESC-ID Lisboa</orgName>
								<address>
									<addrLine>Rua Alves Redol, 9</addrLine>
									<postCode>1000-029</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.51,182.75,51.96,8.74"><forename type="first">Ana</forename><surname>Mendes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INESC-ID Lisboa</orgName>
								<address>
									<addrLine>Rua Alves Redol, 9</addrLine>
									<postCode>1000-029</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.89,182.75,70.03,8.74"><forename type="first">João</forename><surname>Guimarães</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INESC-ID Lisboa</orgName>
								<address>
									<addrLine>Rua Alves Redol, 9</addrLine>
									<postCode>1000-029</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.41,196.70,71.97,8.74"><forename type="first">Nuno</forename><forename type="middle">J</forename><surname>Mamede</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INESC-ID Lisboa</orgName>
								<address>
									<addrLine>Rua Alves Redol, 9</addrLine>
									<postCode>1000-029</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.35,196.70,69.24,8.74"><forename type="first">Ricardo</forename><surname>Ribeiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INESC-ID Lisboa</orgName>
								<address>
									<addrLine>Rua Alves Redol, 9</addrLine>
									<postCode>1000-029</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.51,148.86,55.86,15.15;1,199.38,146.25,5.86,10.48;1,205.73,148.86,253.76,15.15">QA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">33FBF1B348C95F273AA3D339C178BB80</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Question answering, Question Interpretation, Anaphora Resolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of QA@L 2 F, the question-answering system from L 2 F/INESC-ID, at the QA track of CLEF in 2008.</p><p>Making intensive use of a Natural Language Processing chain (which includes, among others, a morphological analyzer, a disambiguation module, a multi-word recognizer, a chunker and a named entities recognizer), QA@L 2 F is based on a three module approach to answer questions: corpora pre-processing (where the information sources are processed and potentially relevant information is extracted), question interpretation (where the question is converted into a frame) and answer extraction (where different strategies are used to retrieve the final answer to the input question).</p><p>QA@L 2 F system was created in 2007 and had its first participation at CLEF07, with results we considered auspicious. Nevertheless, with the objectives of correcting some detected failures, increasing the percentage of questions the system deals with and correctly answers, and also experiment new techniques using the same processing tools, the system suffered modifications during this year: the question interpretation step was improved to better profit from the results of the Natural Language Processing chain; an anaphora solver module was introduced, which allowed us to answer some questions containing backwards references; finally, some other small improvements were done on the system, especially in the answer extraction module.</p><p>QA@L 2 F had 20% of precision at the competition this year, which represents an increase in the number of correct answers returned by the system of 6%, as compared to the last year results. The system highest accuracy values are on definition questions, in which it achieved 60.714% of precision. However, much work is still to be done in order to improve the system's results, like, for instance, the introduction of an answer validation module, in order to minimize the number of answers given with different type from the expected type, which was the case this year with 10 of our wrong answers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we present QA@L 2 F, the question-answering (QA) system developed at L 2 F/INESC-ID, as well as the results it obtained at CLEF 2008.</p><p>QA systems aim at returning the exact answer for a question formulated in natural language from a (usually) very large amount of text collections. While some QA systems are said to be domain-specific, as they are focused on particular information that concerns a specific topic (like WEBCOOP <ref type="bibr" coords="2,171.71,193.62,10.52,8.74" target="#b2">[3]</ref> for the tourism domain), open-domain QA systems, like Priberam's QA system <ref type="bibr" coords="2,123.68,205.57,10.52,8.74" target="#b1">[2]</ref> and Senso <ref type="bibr" coords="2,186.55,205.57,14.62,8.74" target="#b9">[10]</ref>, both for Portuguese, deal with general questions. These two heavily relly in documents processing. However, some systems employ other resources, like Esfinge <ref type="bibr" coords="2,499.71,217.53,9.97,8.74" target="#b3">[4]</ref>, again for Portuguese, which uses data available on the Web. Moreover, Esfinge makes use of a named entities recognizer, like RAPOSA <ref type="bibr" coords="2,267.30,241.44,15.50,8.74" target="#b10">[11]</ref> does in both question analysis and snippet searching, taking in consideration the benefits of using named entities in QA, like it has been proven by several experiments on different languages <ref type="bibr" coords="2,244.31,265.35,15.50,8.74" target="#b11">[12,</ref><ref type="bibr" coords="2,263.13,265.35,7.01,8.74" target="#b6">7]</ref>.</p><p>QA@L 2 F is an open-domain QA system for the Portuguese language, created in the year of 2007, that also uses a named entities recognizer in its processing. This paper focus the main differences in the system since last year <ref type="bibr" coords="2,261.79,301.21,9.96,8.74" target="#b5">[6]</ref>. The rest of the paper is organized as follows: section 2 describes the most recent version of the QA@L 2 F system, presenting the modifications introduced in it since 2007; section 3 shows, discusses and compares the evaluation results; finally, section 4 concludes and points to future work.</p><p>2 QA@L 2 F QA@L 2 F system makes use of a three step approach and deeply rellies in L 2 F's Natural Language Processing (NLP) chain. A short description of QA@L 2 F's steps are depicted as follows:</p><p>• Corpus Pre-processing: the available information sources were partially processed in order to extract potentially relevant information (specifically, named entities and relations between concepts). The resulting database -created last year -is used by QA@L 2 F; however, since it is based on last year's named entities recognizer, poorer than the one we possess nowadays, an information extraction step is also executed online, using the current named entities recognizer and up to date linguistic patterns;</p><p>• Question Interpretation: the question is interpreted and transformed into a frame, which can be mapped into an SQL query or used to search relevant snippets;</p><p>• Answer Extraction: according with the question type, different strategies are used in order to find the answer.</p><p>This section continues by describing the modifications in the system since CLEF 2007, including a different approach for the question interpretation step and a new anaphora solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Interpretation</head><p>The current question interpretation step differs from last year's process. It is now partly independent from the named entities recognition, namely the identification of the question type and target, as well as the main verb, adjectives and some adverbs. This decision was due to the fact that the named entities recognizer we use is still under development and based on several layers of rules, applied in a pipeline.</p><p>In last year's system, in order to profit from the most recent version of the named entities recognizer (XIP <ref type="bibr" coords="2,162.08,694.53,10.79,8.74" target="#b0">[1]</ref>) <ref type="foot" coords="2,179.75,692.96,3.97,6.12" target="#foot_0">1</ref> , we used several rules in its last layer to extract other relevant information from the question, like its type and target. However, the triggers to these rules are highly dependent on (the continuous) modifications in XIP's previous layers. Being so, we decided to perform this step independently: the named entities recognizer is used only to retrieve the named entities in the question, while the other important features (e.g., type and target) are collected using other tools in the NLP chain.</p><p>As a consequence of this change, in this new version of QA@L 2 F we can get the most from the newer version of the named entities recognizer and put our efforts on a stable extractor for the rest of the relevant information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Question Interpretation process</head><p>The question interpretation process involves several tools, described in the following:</p><p>• Palavroso <ref type="bibr" coords="3,160.24,236.01,9.97,8.74" target="#b4">[5]</ref>, which performs a morphological analysis;</p><p>• MARv <ref type="bibr" coords="3,147.42,255.93,9.96,8.74" target="#b8">[9]</ref>, which disambiguates the result of the morphological analyser;</p><p>• RuRriCo (an improved version of PAsMo <ref type="bibr" coords="3,300.79,275.86,10.29,8.74" target="#b7">[8]</ref>), which is a ruled based tool, that recognizes multi-word terms, collapses them into single tokens and can also split tokens;</p><p>• XIP, which returns the input organized into chunks, connected by dependency relations, and also identifies and classifies the named entities in the input.</p><p>Figure <ref type="figure" coords="3,136.39,339.62,4.98,8.74">1</ref> illustrates the entire question interpretation process used in QA@L 2 F.</p><p>Figure <ref type="figure" coords="3,231.08,619.31,3.88,8.74">1</ref>: Question interpretation in QA@L 2 F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Rules</head><p>From Figure <ref type="figure" coords="3,147.89,685.51,3.88,8.74">1</ref>, it can be noticed that RuDriCo plays a central role in QA@L 2 F's question interpretation. In fact, this tool is responsible for identifying the question type and target, as well as other relevant elements, like the question subtype and important verbs. Rules are patterns that match against labeled text. The previous RuDriCo rule means that:</p><p>• if there is a sequence in the question having:</p><p>a word that has "onde" (where) as lemma, captured by S1; a conjugation of the verb "ser" (to be) and a "que" (that), both optional -marked with the question mark "?";<ref type="foot" coords="4,240.66,299.28,3.97,6.12" target="#foot_1">2</ref> a verb "nascer" (to be born), captured by S4; a noun ("noun"), captured by L6, and then a sequence that ends at the question mark, captured by S7;</p><p>• then it is created a sequence with:</p><p>the same first word (S1), lemma "onde", same category and type "onde verb"; the same main verb (S4); -L6 plus S7, which is tagged as "target".</p><p>For instance, considering the question "Onde nasceu a Florbela Espanca?" (Where was Florbela Espanca born? ), and after being processed by Palavroso, Marv and RuDriCo, by using the previous described rule, RuDriCo returns the following output: &lt;sentence&gt; &lt;word name="Onde"&gt; &lt;class root="onde"&gt; &lt;id atrib="CAT" value="adv"/&gt; &lt;id atrib="type" value="onde_verb"/&gt; &lt;/class&gt; &lt;/word&gt; &lt;word name="nasceu"&gt; &lt;class root="nascer"&gt; &lt;id atrib="CAT" value="verb"/&gt; &lt;/class&gt; &lt;/word&gt; &lt;word name="Florbela Espanca"&gt; &lt;class root="Florbela espancar"&gt; &lt;id atrib="type" value="target"/&gt; &lt;/class&gt; &lt;/word&gt; &lt;/sentence&gt; In parallel, XIP, the responsible tool for named entities recognition, identifies the named entities in the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Frames</head><p>During the question interpretation a frame is created. This frame is then mapped into SQL or used to extract relevant snippets from the database. Each frame consists in the following elements:</p><p>• the name of the script that should be called, regarding the question type;</p><p>• the question target;</p><p>• a set of entities identified by the named entities recognizer;</p><p>• a set of auxiliar (and optional) elements from the question such as:</p><p>the target-type; main verbs; adjectives; adverbs.</p><p>After the processing done by RuDriCo, the frame creator builds a pre-frame. Using the tags added by RuDriCo, the frame creator outputs the script to be called, the question target and so on. For instance, considering the previous example, the script script-wiki-target.pl is identified by the label onde verb and the identified target is Florbela Espanca.</p><p>As the named entities recognizer identifies "Florbela Espanca" as a PERSON, the merge of the two inputs results in: &lt;frame&gt; &lt;script name="script-wiki-target.pl"/&gt; &lt;target value="Florbela Espanca"/&gt; &lt;entidades&gt; &lt;entidade type="PEOPLE" value="Florbela Espanca"/&gt; &lt;/entidades&gt; &lt;auxiliares&gt; &lt;auxiliar type="verb" value="nasceu"/&gt; &lt;/auxiliares&gt; &lt;/frame&gt; After a XML transformation the final frame is created for the input question "Onde nasceu a Florbela Espanca?":</p><p>where/script-wiki-target.pl target="florbela espanca" entities people="florbela espanca" auxiliares verb="nasceu"</p><p>The obtained script is then called and uses its arguments either to build the SQL query or to obtain the snippets that may contain the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Anaphora resolution</head><p>Current QA@L 2 F system integrates an anaphora resolution module that addresses: 1. ellipsis, in which the question starts by a conjunction and is followed by a noun; 2. pronouns; 3. ellipsis, in which the question consists either by a single interrogative pronoun/adverb or by an interrogative pronoun/adverb followed by verbs like to be or stay;</p><p>4. ellipsis, in which the question consists either by an interrogative pronoun/adverb followed either by a noun or by a noun and a verb or only by a verb like die;</p><p>5. all the situations envolving more than one noun or verbs not belonging to the previous mentioned sets of verbs.</p><p>Consider again the (reference) question "Onde nasceu a Florbela Espanca?" (Where was Florbela Espanca born? ). Next examples illustrate the presented situations, respectively:</p><p>1. E Saramago? (And Saramago? ) 2. Quem era ela? (Who was she? ) 3. Quando? (When? ) 4. Onde morreu? (Where did (she) die? ) <ref type="foot" coords="6,286.60,267.86,3.97,6.12" target="#foot_2">3</ref>5. Quantos poemas escreveu? (How many poems did (she) write? ) 3   In order to implement this, we made the choice to manipulate the obtained frames and not the surface question itself. It should also be mentioned that all the anaphoras were solved having the first question of a group of questions as the reference, which is not always the case.</p><p>The following example illustrates the anaphora solving process. Let us start by considering the previous sentences as the unique input of QA@L 2 F. The following frames are obtained:</p><p>1. E Saramago? (And Saramago? )</p><p>target="saramago" entities people="saramago"</p><p>2. Quem era ela? (Who was she? ) who/script-who-people.pl 3. Quando? (When? ) when/script-when.pl 4. Onde morreu? (Where did (she) die? ) where/script-where.pl auxiliars verb="morreu" 5. Quantos poemas escreveu? (How many poems did (she) write? ) howM/script-wiki-target.pl auxiliars target-type="poemas" verb="escreveu"</p><p>The folowing frame results from the reference question Where was Florbela Espanca born? :</p><p>Onde nasceu a Florbela Espanca? where/script-wiki-target.pl target="florbela espanca" entities people="florbela espanca" auxiliares verb="nasceu"</p><p>Being so, the following replacements are made (respectively to each identified situation):</p><p>1. the script called and the auxiliars from the first sentence frame are added to the anaphoric frame;</p><p>2. the target from the first sentence frame is added to the anaphoric frame;</p><p>3. the target, entities and auxiliars from the first sentence frame are added to the anaphoric frame;</p><p>4. the target from the first sentence frame as well as its entities are added to the frame. Auxiliars are also added as long as they don't have the same type of an auxiliar from the obtained frame (for instance verb);</p><p>5. the target from the first sentence frame is added to the anaphoric frame.</p><p>As a result of the mentioned process, we obtain the following as final frames:</p><p>1. E Saramago? (And Saramago? ) where/script-wiki-target.pl target="saramago" entities people="saramago" verb="nasceu"</p><p>2. Quem era ela? (Who was she? ) who/script-who-people.pl target="florbela espanca"</p><p>3. Quando? (When? ) when/script-wiki-target.pl target="florbela espanca" entities people="florbela espanca" auxiliares verb="nasceu" 4. Onde morreu? (Where did (she) die? ) where/script-wiki-target.pl target="florbela espanca" auxiliars verb="morreu"</p><p>5. Quantos poemas escreveu? (How many poems did (she) write? ) howM/script-wiki-target.pl target="florbela espanca" entities people="florbela espanca" auxiliars target-type="poemas" verb="escreveu"</p><p>Using this anaphora solver, that still needs strong improvements, we were able to successfully generate the correct frame for 13 of the 52 anaphoric situations. In fact, 4 of these 13 frames were incorrect due to errors occurred in the generation of the reference frame; however, since these errors were not directly due to the anaphora solver, we considered those results as being correct. This means that we were able to generate the correct frames for 25% of the anaphora's situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Other improvements</head><p>In the answer extraction step, we introduced a method for retrieving answers based on the words proximity in the text, that works similarly in both Wikipedia and newspaper corpora. For every relevant passage in the corpus, snippets are searched that contain the auxiliar concepts which are also in the question, such as verbs or adjectives. All the named entities that match the expected answer type are extracted by using the NLP chain. Afterwards, the distances between the auxiliar concepts and the extracted named entities are measured <ref type="foot" coords="7,338.68,676.77,3.97,6.12" target="#foot_3">4</ref> , and the named entity of the expected type with smallest distance to an auxiliar concept is retrieved as the final answer.</p><p>Consider, for instance, the question "Quantos jogadores tem uma equipe de Basquete?" (How many players has a basketball team? ). It has as target "Basquete" (basketball ) and as auxiliar word "jogadores" (players). The implemented method allows the answer extraction step to return "5" as final answer, based on the sentence " É jogado por dois times de 5 jogadores, que têm como objectivo..." (It's played by two teams of 5 players, that have as goal...). In this case, the distance between the named entity "5" and the auxiliar word "jogadores" is the smallest possible.</p><p>Also, we developed several linguistic patterns in order to detect and create more complex dependencies between concepts. These were introduced in the NLP chain that supports our system. As an example, see the sentence "Nascido em Coimbra, em 10 de Novembro de 1913, Álvaro Cunhal..." (Born in Coimbra, on the 10th of November 1913, Álvaro Cunhal...). Patterns were created to gather the non-ambiguous information that Álvaro Cunhal was born in Coimbra (represented internally by the dependency LOCATION OK( Álvaro Cunhal, Coimbra, Nascido)) and on the 10th of November 1913 (represented internally by the dependency DATE OK( Álvaro Cunhal, 10 de Novembro de 1913, Nascido)).</p><p>Finally, based on the fact that Wikipedia's text is presented in a semi-structured way, we created a module to extract information directly from Wikipedia's tables. This can be used, for instance, to faster retrieve answers to questions like "What is the capital of ... ?" or "What is the language spoken in ...?" as they can be easily found in Wikipedia and there is no need to perform any kind of time-consuming processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>QA@L 2 F was evaluated at CLEF, using Portuguese as source and target language. Table <ref type="table" coords="8,479.74,358.07,4.98,8.74" target="#tab_1">1</ref> shows the obtained results. The system had better overall results this year: 20% of correct answers, versus 14% last year. However, the number of wrong answers continues high ( <ref type="formula" coords="8,432.10,381.98,12.73,8.74">150</ref> Table <ref type="table" coords="8,133.38,485.87,4.98,8.74" target="#tab_2">2</ref> shows the detailed results for each question type. Just like what happened at the competition in 2007, the system obtained this year the best results in the definition questions. Also, the accuracy in factoids questions improved: we had 22 factoid questions answered correctly (corresponding to 13.580% of precision), versus 8 (5.03%) last year. Moreover, the system answered right to one list question: last year no correct answers were given to any question of this type. One thing to be mentioned is that we did not profit from the fact that the system could return 3 answers. In fact we only presented 230 answers: 184 single answers, 2 double answers and 14 triple answers.</p><p>Finally, we would like to mention that several answers were extracted from Wikipedia's tables and, although the page from where they were extracted was correctly identified, they were considered unsupported. Although the entire system needs strong improvements, we believe that there are many small things to be done in QA@L 2 F that can make it achieve better results, such as:</p><p>• the answer type should be validated. This year, 10 out of the 150 wrong questions do not have the expected type from the question. Being so, if we have a tool that is able to say that that something is a PERSON or a LOCATION (for instance), it will not be difficult, if one is expecting a PERSON or a LOCATION, to validate it. This will certainly give better results, when articulated with redundancy, than using redundancy by itself. We are aware that this will certainly lead to a hierarchy of named entities types;</p><p>• taking advantage of the possibility of returning 3 answers to each question;</p><p>• improve the anaphora solver. For instance, the system only solves anaphoras based on the frame constructed for the first question of a group of related questions (the reference question, in bold in the example).</p><p>Onde nasceu Florbela Espanca? Where was Florbela Espanca born?</p><p>Onde é que ela morreu? Where did she died?</p><p>Quando? When?</p><p>In such cases, the anaphora solver presents an undiserable behaviour: on the third question, instead of searching "When did Florbela Espanca die?", the system will try to find the date when Florbela Espanca was born. This and other improvements should be done in our anaphora solver.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,132.30,360.31,338.43,243.88"><head></head><label></label><figDesc></figDesc><graphic coords="3,132.30,360.31,338.43,243.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,90.00,381.98,423.00,86.79"><head>Table 1 :</head><label>1</label><figDesc>), even if it has decreased from 166 since 2007. QA@L 2 F results at CLEF 2008.</figDesc><table coords="8,99.76,414.66,403.48,21.09"><row><cell>Right</cell><cell>Wrong</cell><cell>ineXact</cell><cell>Unsupported</cell><cell>Accuracy over the FIRST answer (%)</cell></row><row><cell>40</cell><cell>150</cell><cell>5</cell><cell>5</cell><cell>40/200 = 20%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,95.98,557.35,415.38,91.17"><head>Table 2 :</head><label>2</label><figDesc>QA@L 2 F results for each question type.</figDesc><table coords="8,95.98,557.35,415.38,58.15"><row><cell>Question Type</cell><cell cols="5">Total Right Wrong ineXact Unsupported</cell><cell>Accuracy (%)</cell></row><row><cell>Factoids</cell><cell>162</cell><cell>22</cell><cell>132</cell><cell>3</cell><cell>5</cell><cell>22/162 = 13.580%</cell></row><row><cell>Lists</cell><cell>10</cell><cell>1</cell><cell>8</cell><cell>1</cell><cell>0</cell><cell>1/10 = 10.0%</cell></row><row><cell>Definition</cell><cell>28</cell><cell>17</cell><cell>10</cell><cell>1</cell><cell>0</cell><cell>17/28 = 60.714%</cell></row><row><cell>Temporally Restricted</cell><cell>16</cell><cell>1</cell><cell>14</cell><cell>0</cell><cell>1</cell><cell>1/16 = 6.250%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,737.54,407.76,6.99;2,90.00,747.00,314.48,6.99"><p>As it will be explained later on, XIP is not only responsible for the recognition of named entities; however, for the sake of simplicity, we decided to introduce it here as a named entities recognizer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,105.24,718.61,407.75,6.99;4,90.00,728.08,423.00,6.99;4,90.00,737.54,422.99,6.99;4,90.00,747.00,354.12,6.99"><p>In Portuguese, some questions can start either by an interrogative pronoun, like "Onde nasceu a Florbela Espanca?" (literally Where was born Florbela Espanca? ), or by an interrogative pronoun followed by the inflected verb to be and the pronoun that, like "Onde é que nasceu a Florbela Espanca?" (literally Where is that was born Florbela Espanca? ). In such cases, both constructions are possible and have the same meaning.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,105.24,742.66,145.58,6.99"><p>In Portuguese, the pronoun is optional.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="7,105.24,733.70,407.75,6.99;7,90.00,743.16,161.74,6.99"><p>If two words appear together in a text, the distance between them is "1"; if two words have a one word in-between, the distance is "2"; and so on...</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.48,452.82,402.52,8.74;9,110.48,464.77,402.52,8.74;9,110.48,476.73,174.24,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,391.04,452.82,121.96,8.74;9,110.48,464.77,26.07,8.74">A Multi-Input Dependency Parser</title>
		<author>
			<persName coords=""><forename type="first">Salah</forename><surname>Aït-Mokhtar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Pierre</forename><surname>Chanod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Claude</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,164.07,464.77,348.93,8.74;9,110.48,476.73,36.86,8.74">Proceedings of the Seventh IWPT (International Workshop on Parsing Technologies)</title>
		<meeting>the Seventh IWPT (International Workshop on Parsing Technologies)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-10">October 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,496.36,402.52,8.74;9,110.48,508.31,402.52,8.74;9,110.48,520.27,227.21,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,264.21,508.31,244.25,8.74">Priberam&apos;s question answering system in qa@clef 2007</title>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adán</forename><surname>Cassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Helena</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andr</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Afonso</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pedro</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cludia</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,110.48,520.27,196.10,8.74">Working Notes for the CLEF 2007 Workshop</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,539.90,402.52,8.74;9,110.48,551.85,358.32,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,171.41,539.90,341.59,8.74;9,110.48,551.85,22.83,8.74">Cooperative question answering in restricted domains: the WEBCOOP experiment</title>
		<author>
			<persName coords=""><surname>Benamara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,142.29,551.85,295.33,8.74">ACL 2004 Workshop on Question Answering in Restricted Domains</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,571.48,402.53,8.74;9,110.48,583.44,244.95,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,213.15,571.48,278.48,8.74">Esfinge a question answering system in the web using the web</title>
		<author>
			<persName coords=""><forename type="first">Luís</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Costa</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,110.48,583.44,214.80,8.74">EACL. The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,603.07,402.52,8.74;9,110.48,615.02,332.36,8.74" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jos</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Medeiros</forename></persName>
		</author>
		<title level="m" coord="9,207.85,603.07,226.44,8.74">Anlise Morfolgica e Correco Ortogrfica do Portugus</title>
		<meeting><address><addrLine>Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Instituto Superior Técnico, Universidade Técnica de Lisboa</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="9,110.48,634.65,402.53,8.74;9,110.48,646.61,402.52,8.74;9,110.48,658.56,264.15,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,222.79,646.61,148.94,8.74">QA@L2F, first steps at QA@CLEF</title>
		<author>
			<persName coords=""><forename type="first">Ana</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lusa</forename><surname>Coheur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nuno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ricardo</forename><surname>Mamede</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Martins De Matos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,394.18,646.61,103.11,8.74">CLEF 2007 Proceedings</title>
		<title level="s" coord="9,148.77,658.56,152.87,8.74">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="9,110.48,678.19,402.53,8.74;9,110.48,690.15,256.69,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,351.61,678.19,161.39,8.74;9,110.48,690.15,41.17,8.74">Named entity recognition for question answering</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Menno</forename><surname>Van Zaanen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Luiz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pizzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,173.09,690.15,105.27,8.74">Proceedings ALTW 2006</title>
		<meeting>ALTW 2006</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,709.78,402.52,8.74;9,110.48,721.73,402.53,8.74;9,110.48,733.69,402.52,8.74;9,110.48,745.64,228.53,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,304.96,709.78,204.16,8.74">Terms Spotting with Linguistics and Statistics</title>
		<author>
			<persName coords=""><forename type="first">Joana</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Pardal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nuno</forename><forename type="middle">J</forename><surname>Mamede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,124.17,721.73,388.83,8.74;9,110.48,733.69,402.52,8.74;9,110.48,745.64,82.78,8.74">Proceedings of the international workshop &quot;Taller de Herramientas y Recursos Lingusticos para el Espanl y el Portugus</title>
		<meeting>the international workshop &quot;Taller de Herramientas y Recursos Lingusticos para el Espanl y el Portugus</meeting>
		<imprint>
			<date type="published" when="2004-11">November 2004</date>
			<biblScope unit="page" from="298" to="304" />
		</imprint>
	</monogr>
	<note>IX Iberoamerican Conference on Artificial Intelligence (IBERAMIA 2004)</note>
</biblStruct>

<biblStruct coords="10,110.48,112.02,402.53,8.74;10,110.48,123.98,402.52,8.74;10,110.48,135.93,402.52,8.74;10,110.48,147.89,402.52,8.74;10,110.48,159.84,22.69,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,354.91,112.02,158.10,8.74;10,110.48,123.98,271.58,8.74">Using Morphossyntactic Information in TTS Systems: comparing strategies for European Portuguese</title>
		<author>
			<persName coords=""><forename type="first">Ricardo</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nuno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isabel</forename><surname>Mamede</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,401.26,123.98,111.74,8.74;10,110.48,135.93,326.68,8.74">Computational Processing of the Portuguese Language: 6th International Workshop, PROPOR 2003</title>
		<title level="s" coord="10,315.20,147.89,151.06,8.74">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Faro, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">June 26-27, 2003. 2003</date>
			<biblScope unit="volume">2721</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,179.77,402.52,8.74;10,110.48,191.72,307.32,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,260.62,179.77,252.38,8.74;10,110.48,191.72,69.86,8.74">The Senso Question Answering Approach to Portuguese QA@CLEF-2007</title>
		<author>
			<persName coords=""><forename type="first">Jos</forename><surname>Saias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Quaresma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,190.58,191.72,196.10,8.74">Working Notes for the CLEF 2007 Workshop</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,211.65,402.52,8.74;10,110.48,223.60,142.64,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,273.00,211.65,147.22,8.74">Making RAPOSA (FOX) smarter</title>
		<author>
			<persName coords=""><forename type="first">Lus</forename><surname>Sarmento</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugnio</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,430.91,211.65,82.09,8.74;10,110.48,223.60,111.53,8.74">Working Notes for the CLEF 2007 Workshop</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,243.53,402.53,8.74;10,110.48,255.48,402.51,8.74;10,110.48,267.44,291.30,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,427.15,243.53,85.86,8.74;10,110.48,255.48,176.90,8.74">Improving question answering using named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Toral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elisa</forename><surname>Noguera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rafael</forename><surname>Muñoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,306.68,255.48,166.45,8.74">Proceedings of the 10th NLDB congress</title>
		<title level="s" coord="10,480.17,255.48,32.82,8.74;10,110.48,267.44,114.79,8.74">Lecture notes in Computer Science</title>
		<meeting>the 10th NLDB congress<address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
