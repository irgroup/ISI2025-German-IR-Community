<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.45,148.79,302.10,15.48;1,175.01,170.71,252.98,15.48;1,99.62,192.62,403.77,15.48">University of Hagen at QA@CLEF 2008: Efficient Question Answering with Question Decomposition and Multiple Answer Streams</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,198.63,226.91,62.35,8.64"><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Information and Communication Systems (IICS</orgName>
								<orgName type="institution">University of Hagen (FernUniversität in Hagen</orgName>
								<address>
									<postCode>58084</postCode>
									<settlement>Hagen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.93,226.91,55.56,8.64"><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Information and Communication Systems (IICS</orgName>
								<orgName type="institution">University of Hagen (FernUniversität in Hagen</orgName>
								<address>
									<postCode>58084</postCode>
									<settlement>Hagen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.34,226.91,74.03,8.64"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Information and Communication Systems (IICS</orgName>
								<orgName type="institution">University of Hagen (FernUniversität in Hagen</orgName>
								<address>
									<postCode>58084</postCode>
									<settlement>Hagen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.45,148.79,302.10,15.48;1,175.01,170.71,252.98,15.48;1,99.62,192.62,403.77,15.48">University of Hagen at QA@CLEF 2008: Efficient Question Answering with Question Decomposition and Multiple Answer Streams</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9BD3B94FCA842FF6CF38554FC6DEA75B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing-Linguistic processing H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Search process H.3.4 [Information Storage and Retrieval]: Systems and Software-Performance evaluation I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-Semantic networks I.2.7 [Artificial Intelligence]: Natural Language Processing-Language parsing and understanding Experimentation</term>
					<term>Measurement</term>
					<term>Performance Question answering</term>
					<term>Deep semantic processing of questions and documents</term>
					<term>Follow-up questions</term>
					<term>Coreference resolution</term>
					<term>Question decomposition</term>
					<term>Answer merging</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The German question answering (QA) system IRSAW (formerly: InSicht) participated in QA@CLEF for the fifth time. IRSAW was introduced in 2007, by integrating the deep answer producer InSicht, several shallow answer producers, and a logical validator.</p><p>InSicht realizes a deep QA approach: it transforms documents to semantic representations using a parser, draws inferences on semantic representations with rules, and matches semantic representations derived from questions and documents. InSicht was improved for QA@CLEF 2008 mainly in the following areas. The coreference resolver was trained on question series instead of newspaper texts in order to be better applicable for follow-up questions in question series. Questions are decomposed by several methods on the level of semantic representations. On the shallow processing side, the number of answer producers was increased from 2 to 4, by adding FACT and SHASE.</p><p>The answer validator introduced in the previous year was replaced with the faster RAVE validator designed for logic-based answer validation under time constraints. Using RAVE for merging the results of the answer producers, monolingual German runs and bilingual runs with source language English and Spanish were produced by applying a machine translation web service. An error analysis showed the main problems for the precision-oriented deep answer producer InSicht and the potential offered by the recall-oriented shallow answer producers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The German question answering (QA) system IRSAW (Intelligent Information Retrieval on the Basis of a Semantically Annotated Web) employs deep and shallow methods. The deep answer producer is In-Sicht, which transforms documents to semantic representations using a syntactico-semantic parser, draws inferences on semantic representations with rules, matches semantic representations derived from questions and documents, and generates natural language answers from the semantic representations of relevant documents. Specialized modules refine the semantic representations in several directions: resolving coreferences in documents (and questions) and resolving temporal deixis in documents. To provide a robust strategy for difficult text passages or passages mixing text and other elements, four shallow answer producers are employed. The resulting five streams of answer candidates, which are produced in parallel, are logically validated and merged by RAVE. Based on the results of validation, RAVE scores the answer candidates and selects the final results.</p><p>2 Changes of InSicht for QA@CLEF 2008</p><p>The deep answer producer InSicht was changed in three main aspects that are described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Improved Dialog Treatment</head><p>In contrast to last year, we retrained the coreference resolver CORUDIS <ref type="bibr" coords="2,389.06,369.20,74.49,8.64" target="#b2">(Hartrumpf, 2001)</ref> on a dialog corpus with anaphors in questions, namely the test questions from QA@CLEF 2007. The training set was derived as follows. First, all coreferences (pronoun to NP, less specific NP to more specific NP) were annotated yielding 29 questions from 20 question series with a coreference. Second, as 20 training texts will not deliver good results, additional question series were created by taking every continuous subsequence of 1 to 4 questions from the QA@CLEF 2007 questions. Information about discourse boundaries (topic starts) was ignored because this kind of information will not be available in many real-world applications. A subsequence is discarded for training if an anaphora leads outside the selected subsequence. Third, the resulting 462 question series were fed into the usual training process of CORUDIS. Note that also the answer to a question could be integrated as a possible antecedent, but as only two QA@CLEF 2007 questions show a coreference to the preceding answer, it was not done. After the 2008 campaign, it turned out that the number of such cases increased to 4 in QA@CLEF 2008 (qa08 048, qa08 050, qa08 053, and qa08 106) so that this option seems to become more relevant. When CORUDIS is applied to corpus documents, the statistical model trained on newspaper articles is chosen instead of the model from question series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question Decomposition</head><p>Question decomposition was systematically added to InSicht for QA@CLEF 2008. A decomposition method tries to simplify answering complex questions by first asking a subquestion whose answer is used to form a revised question which is often more easy to answer than the original question.<ref type="foot" coords="2,487.59,605.75,3.69,6.39" target="#foot_0">1</ref> For example, question decomposition for Welches Metall wird zur Goldwäsche benutzt?/'Which metal is used for washing gold?' (qa08 192) leads to the subquestion Nenne Metalle/'Name metals' with answers like Eisen/'iron' and Quecksilber/'quicksilver' and the revised question Wird Quecksilber zur Goldwäsche benutzt?/'Is quicksilver used for washing gold?' Note that answers to original questions found by question decomposition often require support for the answered subquestions and the revised question, i.e. the answer to the original question is supported by sentences from different documents.</p><p>To evaluate question decomposition after QA@CLEF 2008, we annotated all German QA@CLEF questions since 2003 with decomposition classes (see <ref type="bibr" coords="2,307.97,703.32,71.44,8.64" target="#b4">Hartrumpf (2008)</ref> for details on the annotation, the decomposition classes, and the decomposition methods). For 2008, 21 questions (10.5%) were annotated as decomposable. This percentage is lower than in previous years; for example from 2004 till 2007, the percentage was 17.1%. Examples from QA@CLEF 2008 are qa08 044 (Wieviele Bundesländer hat Österreich?)<ref type="foot" coords="3,140.39,146.27,3.69,6.39" target="#foot_1">2</ref> and question qa08 192 as discussed above. But as expected, some answers (e.g. for question qa08 192) were not found when decomposition was turned off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Performance Improvement</head><p>Adding features to the deep producer InSicht yields better results, but often with a longer runtime. Therefore, several performance improvements were tried. As query expansion by logical rules (applied in backward chaining) expands the search space dramatically, the search space should be reduced by some efficient heuristics that do not eliminate good answers. To this end, statistics on successful rule applications (i.e. combinations of logical rules that led to at least one correct answer) were collected from the test collections of QA@CLEF from 2003 to 2007 and some separate question collections. When restricting query expansion to successful rule combinations, results for the QA@CLEF 2008 questions stayed stable while runtime decreased by 56%. This simple technique turned out to be very effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Shallow QA Subsystems</head><p>In addition to the deep producer, IRSAW now employs four shallow producers of answer candidates: QAP <ref type="bibr" coords="3,112.97,358.89,65.31,8.64" target="#b7">(Leveling, 2006)</ref>, MIRA <ref type="bibr" coords="3,214.81,358.89,65.32,8.64" target="#b8">(Leveling, 2007)</ref>, FACT, and SHASE. The latter two have been added for QA@CLEF 2008. FACT makes use of a fact database in which relational triples have been indexed, e.g. name2date of death("Galileo Galilei", "8. Januar 1642"). <ref type="foot" coords="3,341.85,380.87,3.69,6.39" target="#foot_2">3</ref> Relational triples take the same form as triples used in the MIRA producer. The relational triples have been extracted automatically from various sources, including the PND <ref type="bibr" coords="3,206.10,406.71,107.85,8.64" target="#b6">(Hengel and Pfeifer, 2005)</ref>, the acronym database VERA, monetary names from ISO-34217, and appositions from the semantic network representation of the Wikipedia and CLEF-News corpora. To answer a question, the relational triple is determined for a question using a machine learning (ML) approach and keywords from the question are used to fill in one argument position of the triple. Answers are extracted from the other argument position of matching triples. Document sentences containing keywords from the question as well as the exact answer string are returned as support for the answer candidate.</p><p>SHASE uses the semantic network representation of both question and document sentences to produce answer candidates. The core node representing an answer node is identified in the question semantic network (i.e. the question focus node determined by the syntactico-semantic parser). To find answer candidates, the semantic relations for the core node, its semantic sort, and its semantic entity are calculated; see <ref type="bibr" coords="3,105.17,538.22,56.13,8.64" target="#b5">Helbig (2006)</ref> for more details on the semantic hierarchies. These features are matched with the corresponding features of nodes in the document semantic networks. Matching nodes represent answer candidates: the answer string is extracted from the semantic network representation and the document sentence is returned as answer support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Merging Answer Streams by Validation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview of the RAVE Validator</head><p>The answer candidates in the InSicht stream and the shallow QA streams are validated and merged by RAVE (Real-time Answer Validation Engine), a logic-based answer validator designed for real-time QA. It is crucial for the efficiency of RAVE that no answer must ever be parsed at query time -computing a deep linguistic analysis for hundreds of extracted answer candidates during validation is not realistic in the real-time QA setting. This problem is solved by using logic only for validating support passages, i.e. for deciding if the given passage contains a correct answer at all. Establishing the relationship between the considered answer candidate and the information obtained from the question-passage proof is not part of the logic-based processing. Therefore a logical analysis of the answer candidate is no longer needed for validation. Moreover, one question-passage proof is sufficient per passage. The deep features determined by proving the question from the support passage can be re-used if several answer candidates were extracted from the same passage. The parses of the sentences needed for the question-passage proofs are computed before indexing and fetched along with retrieving the support passages.</p><p>Local validation scores are determined by an ML method applied to shallow and (if available) also logic-based features. Separate models were trained for each producer in order to tailor the validation criterion to the characteristics of each answer stream. Notice that both a combined model (using the full set of deep and shallow features) and a shallow-only model was generated for each stream. Training data was obtained from a run of the system on the QA@CLEF 2007 test set for German. A total of 21,447 answer candidates extracted from 27,919 retrieved passages were annotated as the basis for machine learning. Preparatory experiments based on cross-validation on the training set have shown that bagging of decision trees with reweighting of training examples is particularly suited for the task. The local ML-based scores, which estimate the probability that an answer is correct judging from a specific supporting snippet, are then aggregated in order to determine the total evidence for each answer candidate. RAVE uses a novel aggregation model for that purpose, which aims at robustness against duplicated information; see <ref type="bibr" coords="4,475.93,315.58,37.07,8.64;4,90.00,327.53,26.56,8.64" target="#b0">Glöckner (2008)</ref> for a detailed description of the validation approach and the aggregation model.<ref type="foot" coords="4,435.68,325.60,3.69,6.39" target="#foot_3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Real-Time Validation Approach</head><p>RAVE uses an anytime validation technique based on incremental processing of the available answer streams. In order to implement incremental validation, RAVE maintains a priority queue for retrieved passages and a second priority queue for pending answer candidates. The processing loop of RAVE works as follows: If more than t = 100 ms have elapsed since the last attempt of reading new passages or new answer candidates, then RAVE reads all passages and answer candidates that have arrived in the meantime. By applying a model learned from passage annotations to the shallow passage features, each passage is assigned a quality score which estimates the probability that the passage contains a correct answer, and the shallow features are cached. Passages with a deep parse are added to the priority queue for later logic-based processing.</p><p>The newly arriving answer candidates are first checked for violation of sanity checks. If an answer passes the test, a score based on shallow features is computed. If the supporting passage from which the answer candidate was extracted has a deep parse, then the candidate is added to the priority queue of answer candidates awaiting deep validation. If the supporting passage has no deep parse, however, then the score based on shallow features is directly aggregated, provided that it exceeds a given quality threshold. Having integrated all newly arriving data, the system fetches the answer candidate with the best shallow score from the queue of answer candidates. If no logic-based features are already cached for the corresponding support passage yet, the passage is subjected to logical processing by trying to prove the question literals from the logical representation of the passage and storing the resulting logic-based features. Then a 'deep' answer score based on the combination of deep and shallow features is computed and the result is aggregated if it exceeds the quality threshold. In the event that the priority queue of answer candidates is empty, the available time is utilized by pre-computing logic-based features for the best item in the priority queue of support passages. This will speed up the later evaluation of answer candidates extracted from this passage since the logic-based features can then be fetched from the cache.</p><p>This incremental reading/processing loop is repeated until all streams are exhausted or until the specified time limit is exceeded. The system then iterates over all remaining items in the priority queue of answer candidates. If logic-based features are already cached for such a remaining answer candidate, the candidate is assigned a deep score; otherwise only shallow features are used. In any case, the validation score is aggregated provided that it exceeds the quality threshold. Finally, RAVE determines the three distinct answers with the highest aggregated score. These answers are returned together with the best supporting snippet found for each answer. If there is no aggregated evidence for any answer at all, then a NIL answer with zero confidence is generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Validation of InSicht Results</head><p>Answer candidates generated by InSicht are always directly aggregated -these answers result from a precision-oriented QA technique and do not require logical validation. Since InSicht works independently of the passage retrieval step, RAVE lacks the morpho-lexical needed for computing shallow passage features and assigning a validation score in the usual way. Two methods for assigning validation scores to the InSicht results were tried. The first was training an ML-based classifier using a special set of features for InSicht, which consists only of the producerScore assigned by InSicht itself and an occurrences count for the number of alternative justifications that InSicht has found for the answer. The second method was directly using the self-assessment of InSicht, i.e. the producerScore, as the local validation score for answer candidates contributed by InSicht.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Description of Runs</head><p>All runs with prefix fuha081 were generated using the ML-based validation scores for InSicht, whereas the runs with prefix fuha082 used the self-assessment of InSicht. For bilingual QA experiments, the Promt Online Translator<ref type="foot" coords="5,161.22,531.18,3.69,6.39" target="#foot_4">5</ref> was employed to translate the questions from English or Spanish to German. From experience in previous CLEF campaigns, it was expected that this web service would return translations containing fewer errors than other comparable web services for machine translation, which becomes important when deep NLP is applied, i.e. when the translated questions are parsed. However, we found that, in this year, Promt offers a new machine translation service (in beta status) and experiments using translations from other web services had a higher performance, see <ref type="bibr" coords="5,310.76,592.88,123.83,8.64" target="#b9">Leveling and Hartrumpf (2008)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation and Discussion</head><p>We submitted two runs for the German monolingual task in QA@CLEF 2008 and four bilingual runs with English and Spanish as source language and German as target language (see Table <ref type="table" coords="5,433.37,660.73,3.60,8.64" target="#tab_0">1</ref>). The syntacticosemantic parser employed in InSicht was used to provide an approximate complexity measure for the German questions by counting the semantic relations in parse results (after coreference resolution). This showed a decrease compared to previous years: 9.05 relations per question on average <ref type="bibr" coords="5,433.70,696.59,79.30,8.64;5,90.00,708.55,122.41,8.64">(2007: 11.41; 2006: 11.34; 2005: 11.33; 2004: 9.84</ref>). q.error error on question side q.parse error question parse is not complete and correct q.no parse parse fails 4.1 q.chunk parse only chunk parse result 0.0 q.incorrect coreference a coreference is resolved incorrectly 5.4 q.incorrect parse parser generates full parse result, but it contains errors 6.8 q.ungrammatical question is ungrammatical 0.0 d.error error on document side d.parse error document sentence parse is not complete and correct d.no parse parse fails 12.2 d.chunk parse only chunk parse result 14.9 d.incorrect parse parser generates full parse result, but it contains errors 13.5 d.ungrammatical document sentence is ungrammatical 1.4 q-d.error error in connecting question and document q-d.failed generation no answer string can be generated for a found answer 1.4 q-d.matching error match between semantic networks is incorrect 1.4 q-d.missing cotext answer is spread across several sentences 5.4 q-d.missing inferences inferential knowledge is missing 33.8</p><p>In the bilingual experiments with English and Spanish about 60% and 40%, respectively, of the performance (measured in right answers) for monolingual German were achieved. Results may have been better with another machine translation service for QA@CLEF 2008.</p><p>The evaluation of dialog treatment for the 2008 questions showed that the coreference resolver performed correctly, with one exception: The anaphors in the four questions that referred to the answer of the preceding question were incorrectly resolved because this case was not allowed in the trained coreference model (see Sect. 2.1).</p><p>Table <ref type="table" coords="6,130.63,484.99,4.98,8.64" target="#tab_1">2</ref> contains an error analysis for the deep answer producer InSicht with a predefined classification. The classes are problem classes that lead to not finding the correct answer; the same classes were used for our first participation, QA@CLEF 2004 <ref type="bibr" coords="6,324.08,508.90,73.67,8.64" target="#b3">(Hartrumpf, 2005)</ref>, except that the new class q.incorrect coreference (coreference resolution errors for questions) is needed for the question series introduced in QA@CLEF 2007. A random sample of 74 questions that InSicht answered incorrectly were investigated. If several problem classes were visible for a question, only the one that occurred in the earlier component of processing was annotated in order to avoid speculation about subsequent errors. Similar to our analysis for QA@CLEF 2004, missing inferences (between document and question representations) and parser errors on the document side are the two main problems for InSicht.</p><p>The performance of the shallow QA subsystem has also been assessed. For the 200 questions, a total number of 36,757 distinct supporting passages was retrieved (183.8 per question). 1,264 of these passages contain a correct answer, i.e. the precision of passage retrieval is 3.44%. For 165 of the questions, there is at least one passage that contains an answer to the question. Since these passages form the basis for answer extraction by the shallow producers MIRA, QAP, FACT and SHASE, this means that for perfect answer extraction and validation, it would theoretically be possible for the shallow subsystem to answer 165 non-NIL questions correctly (or 175 questions including the NIL case). More details on the number of available correct passages for each question are shown in Figure <ref type="figure" coords="6,348.26,676.27,3.74,8.64" target="#fig_0">1</ref>.</p><p>The actual extraction performance achieved by the answer producers of the shallow subsystem of IR-SAW has also been investigated, see Table <ref type="table" coords="6,268.18,700.18,3.74,8.64" target="#tab_2">3</ref>. The following labels are used in the table: #candidates (average number of extracted answer candidates per question), #answers (average number of right answers per question), precision (correctness rate of answer extraction, i.e. #answers/#candidates), pass-rate (fraction of the 1,264 correct passages from which a correct answer is extracted), pass-prec (precision of answer ), and answer-rate (answered questions divided by total number of questions with a correct supporting passage, i.e. #answered/165 in this case). As witnessed by the answer-rate of 0.8 for all shallow producers in combination, the answer candidates extracted by the shallow producers cover most of the correct answers contained in the retrieved passages. However, the precision of answer extraction is very low (only 3% when considering all producers), and even for those passages that contain an answer, the precision of extraction (i.e., pass-prec), is only 29%. While the strong recall-orientedness of the shallow subsystem provides a good basis for answer selection, the very low precision also means that the burden of spotting the correct answers is shifted to the validation component.</p><p>Assuming perfect validation, it would be possible to answer 132 non-NIL questions correctly based on the results of the shallow subsystem (or 142 if one includes the NIL questions). There are even more correct answers if InSicht is also taken into account. However, subsequent processing by RAVE only resulted in 46 correct answers in the best submitted run (fuha082dede). This clearly demonstrates that improvements of the validator are necessary. While some minor bugs of the validator have already been fixed (see description of recent changes in <ref type="bibr" coords="7,172.44,512.26,63.84,8.64" target="#b0">Glöckner (2008)</ref>), three other problems must be addressed in the near future:</p><p>• The first problem is the lack of features which relate the answer candidate to the result of the question-passage proof. As a consequence, RAVE is good at identifying passages which contain an answer, but it often cannot discern right answer candidates (extracted from such a passage) from wrong answer candidates.</p><p>• Another problem is the incomplete implementation of the answer-type test which checks the compatibility of the expected answer type of the question and the found answer type. This test is limited  to a few special answer types at the moment, and it is even more restricted for support passages with a failed parse.</p><p>• The third problem is concerned with the training set of RAVE. Due to unstable operation of IRSAW when the training set was generated, the annotations cover only 151 questions of the QA@CLEF07 test set and less than 30 definition questions. In order to provide a suitable basis for machine learning, a considerable increase in the number of annotated questions is necessary. Moreover the training data for SHASE is not representative of the current version of the producer since SHASE was apparently broken when generating the training set and hardly produced any correct answers. Therefore the machine learning approach did not result in a useful model for SHASE.</p><p>Despite this need for additional validation features and a larger, up-to-date training set, the observed processing times for RAVE confirm that the validator is suitable for application in real-time QA. The processing times for a complete logical validation, i.e. without using a time limit, are shown in Figure <ref type="figure" coords="8,505.53,492.33,3.74,8.64" target="#fig_1">2</ref>. The average time needed for answer validation and selection is 1.48 seconds per question. <ref type="foot" coords="8,453.91,502.36,3.69,6.39" target="#foot_5">6</ref> This process involved an average of 79 question-passage proofs per question, which cover all parseable snippets retrieved by IRSAW. Notice that the complete logical validation takes less than 0.95 seconds for half of the questions, and less than 2.45 seconds for 90% of the questions. By specifying a time limit, these processing times can be constrained even further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The QA system IRSAW was successfully improved in several ways for QA@CLEF 2008. Coreference resolution for questions was strengthened by generating suitable training data. Question decomposition in the deep answer producer InSicht opens interesting ways to a fusion of information from different documents or corpora. With increasing system complexity, runtime performance becomes critical, but specialized optimization techniques allow to provide useful answers in near real-time. The latter aspect is still open for improvements in the future, especially with the advent of computers with more and more CPU cores. Adding two more shallow answer producers turned out beneficial for robustness, although the integration in the validator must be improved further. The first prototype of the RAVE answer validator already demonstrates that logic-based processing and real-time answer validation can be reconciled, but additional validation features and an improved training set must be provided in the next development phase.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,105.56,312.84,391.88,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Number of questions with a given number of retrieved passages that answer the question</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,162.81,312.84,277.38,8.64"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Processing times of RAVE for a complete logical validation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,90.00,119.13,423.00,128.12"><head>Table 1 :</head><label>1</label><figDesc>Results for the German question set from QA@CLEF 2008 (CWS: confidence-weighted score; MRR: mean reciprocal rank). Note that only 199 questions were assessed for fuha081esde.</figDesc><table coords="5,95.98,145.12,411.05,102.13"><row><cell>Run</cell><cell></cell><cell></cell><cell>Results</cell><cell></cell></row><row><cell></cell><cell cols="4">#Right #Unsupported #Inexact #Wrong</cell><cell>CWS</cell><cell>MRR</cell></row><row><cell>fuha081dede</cell><cell>45</cell><cell>6</cell><cell>8</cell><cell cols="2">141 0.05210 0.29706</cell></row><row><cell>fuha082dede</cell><cell>46</cell><cell>4</cell><cell>11</cell><cell cols="2">139 0.04868 0.29608</cell></row><row><cell>fuha081ende</cell><cell>28</cell><cell>3</cell><cell>6</cell><cell cols="2">163 0.02369 0.24041</cell></row><row><cell>fuha082ende</cell><cell>28</cell><cell>6</cell><cell>6</cell><cell cols="2">160 0.01987 0.22619</cell></row><row><cell>fuha081esde</cell><cell>19</cell><cell>2</cell><cell>9</cell><cell cols="2">169 0.01541 0.15672</cell></row><row><cell>fuha082esde</cell><cell>17</cell><cell>5</cell><cell>5</cell><cell cols="2">173 0.04868 0.29608</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,90.00,119.13,423.00,34.63"><head>Table 2 :</head><label>2</label><figDesc>Problem classes and problem class frequencies for QA@CLEF 2008 (percentages sum to 100.3</figDesc><table coords="6,90.00,131.09,177.26,22.67"><row><cell>due to rounding)</cell><cell></cell></row><row><cell>Name</cell><cell>Description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,95.98,638.77,414.08,109.21"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="7,95.98,638.77,414.08,109.21"><row><cell></cell><cell></cell><cell cols="4">: Extraction performance of shallow answer producers</cell><cell></cell><cell></cell></row><row><cell>Producer</cell><cell></cell><cell></cell><cell></cell><cell>Results</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">#Candidates #Answers Precision Pass-rate Pass-prec #Answered Answer-rate</cell></row><row><cell>MIRA</cell><cell>80.09</cell><cell>2.15</cell><cell>0.03</cell><cell>0.31</cell><cell>0.32</cell><cell>107</cell><cell>0.65</cell></row><row><cell>QAP</cell><cell>1.43</cell><cell>0.02</cell><cell>0.01</cell><cell>0.00</cell><cell>0.43</cell><cell>2</cell><cell>0.01</cell></row><row><cell>SHASE</cell><cell>80.89</cell><cell>1.15</cell><cell>0.01</cell><cell>0.16</cell><cell>0.16</cell><cell>81</cell><cell>0.49</cell></row><row><cell>FACT</cell><cell>14.38</cell><cell>1.43</cell><cell>0.10</cell><cell>0.19</cell><cell>0.57</cell><cell>34</cell><cell>0.21</cell></row><row><cell>all</cell><cell>176.79</cell><cell>4.74</cell><cell>0.03</cell><cell>0.50</cell><cell>0.29</cell><cell>132</cell><cell>0.80</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,104.35,722.57,408.65,7.05;2,90.00,732.18,256.80,6.91"><p>Note that the term decomposition is sometimes used in a different sense when a biographical question like Who was Bernini? is broken down into a set of standard questions, see for example<ref type="bibr" coords="2,288.38,732.18,55.11,6.91" target="#b1">Harabagiu (2006)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,104.35,717.86,229.32,6.91"><p>The correct answer could also be found directly without decomposition.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,104.35,727.66,408.65,6.91;3,90.00,737.13,270.51,6.91"><p>The relation type name2date of death is viewed as the first component of the triple. Other common date formats are explicitly generated and indexed as well because no normalization takes place at this level, yet.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,104.35,704.95,408.65,6.91;4,90.00,714.42,423.00,6.91;4,90.00,723.88,423.00,6.91;4,90.00,733.35,423.00,6.91;4,90.00,742.81,69.27,6.91"><p>The version of RAVE used for the submitted runs was still lacking the test for compatibility of measurement units of question and answer and the test for fulfillment of temporal restrictions described in the reference. Two additional features were incorporated for the QA@CLEF runs that were not available in the Answer Validation Exercise, viz irScore (retrieval score for the supporting passage determined by the IRSAW retrieval system) and producerScore (a quality score assigned by each answer producer when generating an answer candidate).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,104.35,727.55,74.76,6.91"><p>http://www.promt.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="8,104.35,734.98,401.43,6.91"><p>Processing times were measured by running RAVE in a single thread on an Athlon64X2 4800+ CPU with 2.4 GHz clock rate.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,90.00,134.43,423.00,8.82;9,99.96,146.38,413.04,8.59;9,99.96,158.34,117.78,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,186.53,134.61,278.30,8.64">University of Hagen at QA@CLEF 2008: Answer validation exercise</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,484.22,134.43,28.78,8.59;9,99.96,146.38,413.04,8.59;9,99.96,158.34,37.31,8.59">Results of the CLEF 2008 Cross-Language System Evaluation Campaign, Working Notes for the CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,178.26,423.00,8.82;9,99.96,190.22,413.04,8.82;9,99.96,202.17,182.61,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,202.71,178.44,99.68,8.64">Questions and intentions</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,325.77,178.26,187.24,8.59;9,99.96,190.22,12.73,8.59">Advances in Open Domain Question Answering</title>
		<title level="s" coord="9,423.86,190.22,89.14,8.59;9,99.96,202.17,43.46,8.59">Speech and Language Technology</title>
		<editor>
			<persName><forename type="first">Tomek</forename><surname>Strzalkowski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="99" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,222.28,423.00,8.64;9,99.96,234.05,413.03,8.82;9,99.96,246.19,275.90,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,195.16,222.28,300.72,8.64">Coreference resolution with syntactico-semantic rules and corpus statistics</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W01-0717" />
	</analytic>
	<monogr>
		<title level="m" coord="9,99.96,234.05,371.42,8.59">Proceedings of the Fifth Computational Natural Language Learning Workshop (CoNLL-2001)</title>
		<meeting>the Fifth Computational Natural Language Learning Workshop (CoNLL-2001)<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,266.11,423.00,8.64;9,99.96,277.89,413.04,8.59;9,99.96,289.84,413.04,8.82;9,99.96,301.80,413.03,8.82;9,99.96,313.93,232.89,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,193.64,266.11,302.08,8.64">Question answering using sentence parsing and semantic network matching</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<idno type="DOI">10.1007/11519645_50</idno>
		<ptr target="http://dx.doi.org/10.1007/11519645" />
	</analytic>
	<monogr>
		<title level="m" coord="9,99.96,277.89,413.04,8.59;9,99.96,289.84,123.75,8.59">Multilingual Information Access for Text, Speech and Images: 5th Workshop of the Cross-Language Evaluation Forum, CLEF 2004</title>
		<title level="s" coord="9,330.81,301.80,140.14,8.59">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><forename type="middle">;</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">;</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">;</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,344.74,313.93,4.15,8.64;9,90.00,333.68,423.00,8.82;9,99.96,345.63,413.03,8.82;9,99.96,357.77,307.28,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,197.73,333.86,194.44,8.64">Semantic decomposition for question answering</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<idno type="DOI">10.1007/11519645_50</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,415.35,333.68,97.65,8.59;9,99.96,345.63,219.29,8.59">Proceedings of the 18th European Conference on Artificial Intelligence (ECAI)</title>
		<editor>
			<persName><forename type="first">Malik</forename><forename type="middle">;</forename><surname>Ghallab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Constantine</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nikos</forename><surname>Spyropoulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">;</forename><surname>Fakotakis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nikos</forename><surname>Avouris</surname></persName>
		</editor>
		<meeting>the 18th European Conference on Artificial Intelligence (ECAI)<address><addrLine>Patras, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="313" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,377.52,423.00,8.82;9,99.96,389.65,323.17,8.64" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,200.24,377.52,273.33,8.59">Knowledge Representation and the Semantics of Natural Language</title>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Helbig</surname></persName>
		</author>
		<ptr target="http://www.springer.com/computer/artificial/book/978-3-540-24461-5" />
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,409.57,423.00,8.64;9,99.96,421.35,152.28,8.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,271.65,409.57,236.95,8.64">Kooperation der Personennamendatei (PND) mit Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Christel</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Pfeifer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,99.96,421.35,93.98,8.59">Dialog mit Bibliotheken</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="18" to="24" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,441.45,423.00,8.64;9,99.96,453.23,413.04,8.82;9,99.96,465.19,413.03,8.82;9,99.96,477.32,113.06,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,201.98,441.45,305.56,8.64">On the role of information retrieval in the question answering system IRSAW</title>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<ptr target="http://web1.bib.uni-hildesheim.de/2006/fgir2006/Leveling.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,111.27,453.23,401.73,8.59;9,99.96,465.19,24.81,8.59">Proceedings of the LWA 2006 (Learning, Knowledge, and Adaptability), Workshop Information Retrieval</title>
		<meeting>the LWA 2006 (Learning, Knowledge, and Adaptability), Workshop Information Retrieval<address><addrLine>Hildesheim, Germany; Universität Hildesheim</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="119" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,497.25,423.00,8.64;9,99.96,509.02,413.04,8.82;9,99.96,521.16,313.94,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,204.80,497.25,308.20,8.64;9,99.96,509.20,74.77,8.64">A modified information retrieval approach to produce answer candidates for question answering</title>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,191.61,509.02,291.82,8.59">Proceedings of the LWA 2007 (Lernen-Wissen-Adaption), Workshop FGIR</title>
		<editor>
			<persName><forename type="first">Alexander</forename><surname>Hinneburg</surname></persName>
		</editor>
		<meeting>the LWA 2007 (Lernen-Wissen-Adaption), Workshop FGIR<address><addrLine>Halle/Saale, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Gesellschaft für Informatik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,541.08,423.00,8.64;9,99.96,552.86,413.04,8.82;9,99.96,564.81,349.61,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,289.45,541.08,223.55,8.64;9,99.96,553.04,181.73,8.64">University of Hagen at GeoCLEF 2008: Combining IR and QA for geographic information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,304.54,552.86,208.46,8.59;9,99.96,564.81,269.14,8.59">Results of the CLEF 2008 Cross-Language System Evaluation Campaign, Working Notes for the CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
