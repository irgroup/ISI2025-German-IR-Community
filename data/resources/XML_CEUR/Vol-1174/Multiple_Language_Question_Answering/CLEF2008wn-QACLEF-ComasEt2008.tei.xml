<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.05,148.86,332.90,15.15;1,116.53,170.78,369.93,15.15">Robust Question Answering for Speech Transcripts: UPC Experience in QAst 2008</title>
				<funder ref="#_nTne9VW">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder>
					<orgName type="full">Spanish Ministry of Science (TEXTMESS project)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.10,204.67,65.39,8.74"><forename type="first">Pere</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
							<email>pcomas@lsi.upc.edu</email>
						</author>
						<author>
							<persName coords="1,318.19,204.67,54.70,8.74"><forename type="first">Jordi</forename><surname>Turmo</surname></persName>
							<email>turmo@lsi.upc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">TALP Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Catalonia (UPC)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.05,148.86,332.90,15.15;1,116.53,170.78,369.93,15.15">Robust Question Answering for Speech Transcripts: UPC Experience in QAst 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3FA02A7CB30161C2F62711EE6A308676</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Question Answering, Spoken Document Retrieval, Phonetic Distance Question Answers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the Technical University of Catalonia in the CLEF 2008 Question Answering on Speech Transcripts track. We have participated in the English and Spanish scenarios of QAst. For the processing of manual transcripts we have deployed a robust factual Question Answering that uses minimal syntactic information. For the handling of automatic transcripts we combine the QA system with a Passage Retrieval and Answer Extraction engine based on a sequence alignment algorithm that searches for "sounds like" sequences. We perform a detailed analysis of our results and draw conclusions relating QA performance to word error rate (WER) in transcripts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The CLEF 2008 Question Answering on Speech Transcripts (QAst) track <ref type="bibr" coords="1,415.90,592.67,10.52,8.74" target="#b8">[9]</ref> consists of five scenarios with several tasks: Question Answering (QA) in manual transcripts of recorded lectures (T1A) and their corresponding automatic transcripts (T1B), QA in manual transcripts of recorded meetings (T2A) and their corresponding automatic transcripts (T2B), QA in manual transcripts of french European Parliament Sessions (T3A) and three different automatic transcripts (T3B-A, T3B-B, T3B-C), QA in manual transcripts of English European Parliament Sessions (T4A) and three different automatic transcripts (T4B-A, T4B-B, T4B-C), QA in manual transcripts of Spanish European Parliament Sessions (T5A) and three different automatic transcripts (T5B-A, T5B-B, T5B-C). The automatic transcripts for tasks T3, T4 and T5 have different levels of word error rate (WER). WERs for T4 are 10.6%, 14%, and 24.1%. For T5 WERs are 11.5%, 12.7% and 13.7%. This paper summarizes our methods and results in QAst. We have participated in all the scenarios except the french language one (T3). Our QA system is based on our previous work in <ref type="bibr" coords="2,325.62,309.87,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="2,339.81,309.87,7.75,8.74" target="#b6">7]</ref> and <ref type="bibr" coords="2,370.98,309.87,9.96,8.74" target="#b7">[8]</ref>. We have used the same system architecture for all the tasks, having interchangeable language-dependant parts and different passage retrieval algorithms for automatic transcripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the System Architecture</head><p>The architecture of our QA system follows a commonly-used schema which splits the process into three phases performed sequentially: Question Processing (QP), Passage Retrieval (PR), and Answer Extraction (AE), as shows Figure <ref type="figure" coords="2,279.71,412.46,3.87,8.74" target="#fig_0">1</ref>. These three phases are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Processing and Classification</head><p>The main goal of this component is to detect the type of the expected answer. We currently recognize the 53 open-domain answer types from <ref type="bibr" coords="2,310.03,482.65,10.52,8.74" target="#b4">[5]</ref> plus 3 types specific to QAst corpora (i.e., system/method, shape, and material). The answer types are extracted using a multi-class Perceptron classifier and a rich set of lexical, semantic and syntactic features. This classifier obtains an accuracy of 88% on the corpus of <ref type="bibr" coords="2,259.84,518.51,9.96,8.74" target="#b4">[5]</ref>. Additionally, the QP component extracts and ranks relevant keywords from the question For scenario T5, he have developed an Spanish question classifier using human translated questions from the corpus of <ref type="bibr" coords="2,222.00,554.38,10.52,8.74" target="#b4">[5]</ref> following the same machine learning approach. This classifier obtains an accuracy of 74%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage Retrieval</head><p>This component retrieves a set of relevant passages from the document collection, given the previously extracted question keywords. The PR algorithm uses a query relaxation procedure that iteratively adjusts the number of keywords used for retrieval and their proximity until the quality of the recovered information is satisfactory (see <ref type="bibr" coords="2,317.68,648.48,10.30,8.74" target="#b6">[7]</ref>). In each iteration a Document Retrieval application (Lucene IR engine) fetches the documents relevant for the current query and a subsequent passage construction module builds passages as segments where two consecutive keyword occurrences are separated by at most t words.</p><p>When dealing with automatic transcripts, you have to bear in mind that the state of the art in ASR technology is far from perfect. For example, the word error rate (WER) of the meetings automatic transcripts (T1B) is around 38% and the WER of the lectures (T2B) is over 20%, and from 10.6% to 24.1% for the T4B transcripts. Figure <ref type="figure" coords="2,317.77,732.16,4.98,8.74" target="#fig_1">2</ref> shows three real examples of common errors when generating automatic transcripts. From the point of view of passage retrieval, imperfect 1M: "The pattern frequency relevance rate indicates the ratio of relevant documents. . . " 1A: "the putt and frequency illustrating the case the ratio of relevant documents. . . " 2M: "The host system it is a UNIX Sun workstation" 2A: "that of system it is a unique set some workstation" 3M: "Documents must be separated into relevant documents and irrelevant documents by a manual process, which is very time consuming." 3A: "documents must be separated into relevant documents and in relevant document by a manual process witches' of very time consuming" To overcome such drawbacks, we have used an IR engine relying on phonetic similarity for the automatic transcripts. This tool is called PHAST (after PHonetic Alignment Search Tool) and uses pattern matching algorithms to search for small sequences of phones (the keywords) into a larger sequence (the documents) using a measure of sound similarity. A detailed description of PHAST can be found in <ref type="bibr" coords="3,199.17,308.03,9.96,8.74" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Extraction</head><p>Identifies the exact answer to the given question within the retrieved passages. First, answer candidates are identified as the set of NEs that occur in these passages and have the same type as the answer type detected by QP. Then, these candidates are ranked using a scoring function based on a set of heuristics that measure keyword distance and density <ref type="bibr" coords="3,367.26,390.18,12.09,8.74" target="#b5">[6]</ref>. These heuristic measures use approximated matching for AE in automatic transcripts as shown in the passage retrieval module from the previous section.</p><p>The same measure is used for English and Spanish scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Named Entity Recognition and Classification</head><p>As described before, we extract candidate answers from the NEs that occur in the passages retrieved by the PR component. We detail below the strategies used for NERC in both manual and automatic transcripts. NERC for English Manual Transcripts.</p><p>We have used a modified version of the NERC presented in <ref type="bibr" coords="3,365.12,531.47,9.96,8.74" target="#b7">[8]</ref>. One change from the previous system is that it uses multi-class Perceptron instead of the existing SVM classifiers. As training data we annotated the NEs that occur in the QAst development corpus with their types (i.e., person, organization, location, language, measure, system/method and time) and used an 80-20% corpus split for training and testing for both lectures and meetings corpora. This experiment indicated that the development data is sufficient for good generalization for meetings (a F 1 score of +75 points in the development test partition) but it is insufficient in lectures: 33 points. This is most likely caused by the small size of the development corpus and the large number of topics addressed. To compensate for the insufficient training data we perform a combination of several NERC models for this task. We merged the outputs of: (a) a rule-based NERC developed previously <ref type="bibr" coords="3,137.18,651.02,9.96,8.74" target="#b6">[7]</ref>, (b) the NERC trained on the existing development data, and (c) the NERC trained on the CoNLL English corpus. <ref type="foot" coords="3,227.08,661.40,3.97,6.12" target="#foot_0">1</ref> We used the above priority ordering for conflict resolution in case of overlapping assignments (e.g., lectures model has higher priority than the CoNLL model). After model combination the NERC F 1 score in the development test partition did not improve but the recall did increase, so we decided to use this combination strategy in the testing since recall is paramount for QA NERC for English Automatic Transcripts. We have used a similar framework for the processing of automatic transcripts: we annotated the development corpora and trained specific NERC models for lectures and meetings. The significant difference is that here we expand the classifiers' feature sets with phonetic attributes. These features are motivated by the fact that even when the ASR incorrectly transcribes NEs the phonetic structure is by and large maintained in the transcript (e.g. in Figure <ref type="figure" coords="4,203.93,171.80,4.98,8.74" target="#fig_1">2</ref> the name "Sun" is recognized as "some"). We used an unsupervised hierarchical clustering algorithm that groups tokens based on the similarity of their phonetic sequences. The stop condition of the algorithm is set to reach a local maximum of the Calinski criterion <ref type="bibr" coords="4,129.23,207.66,9.96,8.74" target="#b0">[1]</ref>. Then the cluster of each token is added as a feature (e.g. "Sun" and "some" share the same cluster), which helps the NERC model generalize from the correct to the incorrect transcript. We also added phonetic features that model prefix and suffix similarity.</p><p>NERC for Spanish. For the Spanish track T5 we have used a previously developed NERC. It uses a machine learning approach and it has been trained with the CoNLL Spanish corpus. See details in <ref type="bibr" coords="4,132.85,270.27,9.96,8.74" target="#b1">[2]</ref>. Unfortunately, this NERC can recognize only person, location and organization NE types. Thus only this types can be used as answer candidates. It supposes a serious shortcoming for QA performance as the results show in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>UPC participated in 4 of the 5 scenarios, all but the French one (T3). We submitted two runs for the tasks on automatic transcripts, one using run using the standard QA system for written text (QA m ) and another run using the system tailored for automatic transcripts (QA a ). See section 2 for the differences between both. Each scenario included 100 test questions, from which 10 does not have an answer in the corpora (these are nil questions). Around 75% of the questions are of factual types and around 25% are definitional. Our QA system is designed to answer only factual questions, therefore the our experimental analysis will refer only to factual questions.</p><p>We report two measures: (a) TOPk, which assigns to a question a score of 1 only if the system provided a correct answer in the top k returned; and (b) Mean Reciprocal Rank (MRR), which assigns to a question a score of 1/k, where k is the position of the correct answer, or 0 if no correct answer is found. The official evaluation of QAst 2008 uses TOP1 and TOP5 measures []. An answer is considered correct by the human evaluators if it contains the complete answer and nothing more, and it is supported by the corresponding document. If an answer was incomplete or it included more information than necessary or the document did not provide the justification for the answer, the answer was considered incorrect.</p><p>Table <ref type="table" coords="4,133.41,528.28,4.98,8.74" target="#tab_0">1</ref> summarizes our overall results for factual question only. The cost of moving from manual transcripts to automatic transcripts (i.e., the difference between TXA and TXB) is a loss in TOP1 score of at last 10% for T1, 43% for T2, 50% for T4 and 42% for T5. The performance of QA a is very similar to QA m . As shown in QAst 2008 Overview paper <ref type="bibr" coords="4,422.00,564.14,9.96,8.74" target="#b8">[9]</ref>, UPC has ranked among the top teams in tasks T1, T2 and T4. Our team got the best TOP1 score in T1B, T2B and TA4 tracks, although the differences were not significant. For task T5 our results were far beyond other participants.</p><p>Table <ref type="table" coords="4,132.83,611.96,4.98,8.74" target="#tab_1">2</ref> shows the distribution of correct answers for all tasks according to the answer type. In scenario T4, a design error prevented our NERC from recognizing entity types Sha, Mat and Col. Therefor there are 20 unanswerable questions from the 78 factual ones. Our system for the Spanish scenario (T5) is limited to answer types Org, Per, and Loc, so the real upper bound for factual questions is 36 instead of 75.</p><p>Finally, Table <ref type="table" coords="4,167.60,671.74,4.98,8.74">3</ref> summarizes the error analysis of QP, PR, and AE parts. The meaning of each column is the following. Q: number of factual question. QC: number of questions with answer type correctly detected by QP. PR: number of question where at least on passage with the correct answer war retrieved. C.NE: number of questions where the retrieved passages contain the correct answer tagged as a NE of the right type. U.NE: number of questions where the retrieved passages contain the correct answer but it remains undetected by the NERC. Er.NE: number of questions where the retrieved passages contain the correct answer tagged as a NE with an incorrect type.  QC&amp;PR: number of questions with correct answer type and correct passage retrieval. QC&amp;NE: number of questions with correct answer type and correctly tagged answer in the passages. TOP5 non-nil: number of question with non-nil answer correctly answered by our system in the TOP5 candidates. Due to technical reasons this analysis has not been performed on task T2B. We can draw several important observations from this error analysis: Question classification performs better for T1 question set than T2 and T4 question sets. This suggests that in this evaluation T1 questions were more domain specific than the others. In T5, results are really disappointing and this suggests that our Spanish classifier may be too domain dependant since it achieves 74% accuracy in our test data. "PR" is specially degraded in task T4B-C, where we processed automatic transcripts with the highest WER (24.1%). This proves that passage retrieval is indeed affected by a high WER but is robust enough to be used with a good ASR. Passage retrieval using PHAST performed better than the passage retrieval with classical retrieval for tasks in T5 and worse for tasks in T4. Since both scenarios have similar domain, we think this difference is due to the nature of Spanish and English phonology. Further experiments in <ref type="bibr" coords="5,478.11,704.53,10.52,8.74" target="#b2">[3]</ref> show consistently that passage retrieval in Spanish is improved by using PHAST. As the table shows, the bad performance of NERC is the critical problem of our QA system. The difference between "C.NE" and "PR" values is much bigger than between "PR" and "Q", thus the theoretical upper </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper describes UPC's participation in the CLEF 2008 Question Answering on Speech Transcripts track. We submitted runs for all English and Spanish scenarios, obtaining the best results in some tasks. In this evaluation we analyzed the behavior of two systems differing in that one is tailored for manual transcripts while the other is tailored for automatic transcripts (uses approximate keyword search based on phonetic distances and a NERC enhanced with phonetic features).</p><p>Our approximated keyword search algorithm used for passage retrieval obtains mixed results. It can improve standard search for Spanish but makes little difference for English. We think this because in some document collections it may generated too many false-positive, introducing noise in sets of candidate passages and answers. Nevertheless, we believe that this approach is a good long-term research direction because it can truly address the phenomena specific to automatic transcripts.</p><p>Finally, our results show that automatic speech recognition has critical impact on the performance of NERC but its affect on passage retrieval is much less severe.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,216.83,278.04,169.34,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of QA architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,156.50,203.93,290.00,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of manual (M) and automatic (A) transcripts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,232.65,383.37,137.71,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Impact of ASR errors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,114.81,110.44,373.37,359.66"><head>Table 1 :</head><label>1</label><figDesc>Overall results for our twenty QAst runs.</figDesc><table coords="5,114.81,110.44,373.37,359.66"><row><cell>Task, System</cell><cell>#Q</cell><cell>MRR</cell><cell>TOP1</cell><cell>TOP5</cell><cell cols="3">Task, System</cell><cell>#Q</cell><cell cols="2">MRR</cell><cell>TOP1</cell><cell>TOP5</cell></row><row><cell>T1A, QAm</cell><cell>78</cell><cell>0.44</cell><cell>30</cell><cell>39</cell><cell cols="2">T2A, QAm</cell><cell></cell><cell>74</cell><cell cols="2">0.35</cell><cell>23</cell><cell>29</cell></row><row><cell>T1B, QAm</cell><cell>78</cell><cell>0.39</cell><cell>27</cell><cell>35</cell><cell cols="2">T2B, QAm</cell><cell></cell><cell>74</cell><cell cols="2">0.20</cell><cell>13</cell><cell>19</cell></row><row><cell>T1B, QAa</cell><cell>78</cell><cell>0.37</cell><cell>26</cell><cell>35</cell><cell cols="2">T2B, QAa</cell><cell></cell><cell>74</cell><cell cols="2">0.16</cell><cell>8</cell><cell>16</cell></row><row><cell>T4A, QAm</cell><cell>75</cell><cell>0.44</cell><cell>30</cell><cell>38</cell><cell cols="2">T5A, QAm</cell><cell></cell><cell>75</cell><cell cols="2">0.11</cell><cell>7</cell><cell>9</cell></row><row><cell>T4B A, QAm</cell><cell>75</cell><cell>0.22</cell><cell>15</cell><cell>18</cell><cell cols="3">T5B A, QAm</cell><cell>75</cell><cell cols="2">0.05</cell><cell>3</cell><cell>5</cell></row><row><cell>T4B B, QAm</cell><cell>75</cell><cell>0.18</cell><cell>12</cell><cell>15</cell><cell cols="3">T5B B, QAm</cell><cell>75</cell><cell cols="2">0.06</cell><cell>4</cell><cell>5</cell></row><row><cell>T4B C, QAm</cell><cell>75</cell><cell>0.11</cell><cell>7</cell><cell>11</cell><cell cols="3">T5B C, QAm</cell><cell>75</cell><cell cols="2">0.03</cell><cell>2</cell><cell>2</cell></row><row><cell>T4B A, QAa</cell><cell>75</cell><cell>0.16</cell><cell>10</cell><cell>16</cell><cell cols="2">T5B A, QAa</cell><cell></cell><cell>75</cell><cell cols="2">0.06</cell><cell>4</cell><cell>5</cell></row><row><cell>T4B B, QAa</cell><cell>75</cell><cell>0.16</cell><cell>10</cell><cell>14</cell><cell cols="2">T5B B, QAa</cell><cell></cell><cell>75</cell><cell cols="2">0.06</cell><cell>4</cell><cell>5</cell></row><row><cell>T4B C, QAa</cell><cell>75</cell><cell>0.11</cell><cell>6</cell><cell>11</cell><cell cols="2">T5B C, QAa</cell><cell></cell><cell>75</cell><cell cols="2">0.03</cell><cell>2</cell><cell>3</cell></row><row><cell>Task, System</cell><cell>Org</cell><cell>Per</cell><cell>Loc</cell><cell>Tim</cell><cell>Mea</cell><cell>Sys</cell><cell cols="2">Lan</cell><cell>Sha</cell><cell>Mat</cell><cell>Col</cell><cell>Def</cell></row><row><cell>T1A, QAm</cell><cell>4/8</cell><cell>8/9</cell><cell>1/2</cell><cell>3/5</cell><cell>13/19</cell><cell>4/5</cell><cell cols="2">6/10</cell><cell>0/8</cell><cell>0/3</cell><cell>0/9</cell><cell>4/22</cell></row><row><cell>T1B, QAm</cell><cell>3/8</cell><cell>5/9</cell><cell>1/2</cell><cell>3/5</cell><cell>13/19</cell><cell>4/5</cell><cell cols="2">6/10</cell><cell>0/8</cell><cell>0/3</cell><cell>0/9</cell><cell>4/22</cell></row><row><cell>T1B, QAa</cell><cell>4/8</cell><cell>4/9</cell><cell>2/2</cell><cell>2/5</cell><cell>14/19</cell><cell>3/5</cell><cell cols="2">6/10</cell><cell>0/8</cell><cell>0/3</cell><cell>0/9</cell><cell>4/22</cell></row><row><cell>T2A, QAm</cell><cell>1/8</cell><cell>2/8</cell><cell>7/10</cell><cell>1/8</cell><cell>4/10</cell><cell>3/6</cell><cell cols="2">2/8</cell><cell>1/4</cell><cell>4/6</cell><cell>4/6</cell><cell>3/26</cell></row><row><cell>T2B, QAm</cell><cell>3/8</cell><cell>2/8</cell><cell>1/10</cell><cell>0/8</cell><cell>3/10</cell><cell>1/8</cell><cell cols="2">1/8</cell><cell>1/4</cell><cell>4/6</cell><cell>3/6</cell><cell>5/26</cell></row><row><cell>T2B, QAa</cell><cell>1/8</cell><cell>3/8</cell><cell>2/10</cell><cell>0/8</cell><cell>1/10</cell><cell>1/8</cell><cell cols="2">1/8</cell><cell>1/4</cell><cell>4/6</cell><cell>2/6</cell><cell>6/26</cell></row><row><cell>T4A, QAm</cell><cell>7/14</cell><cell>9/14</cell><cell>6/15</cell><cell>9/15</cell><cell>7/15</cell><cell>0/2</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4/25</cell></row><row><cell>T4B-A, QAm</cell><cell>1/14</cell><cell>0/14</cell><cell>3/15</cell><cell>8/15</cell><cell>6/15</cell><cell>0/2</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4/25</cell></row><row><cell>T4B-B, QAm</cell><cell>1/14</cell><cell>0/14</cell><cell>2/15</cell><cell>8/15</cell><cell>4/15</cell><cell>0/2</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5/25</cell></row><row><cell>T4B-C, QAm</cell><cell>0/14</cell><cell>1/14</cell><cell>2/15</cell><cell>1/15</cell><cell>6/15</cell><cell>1/2</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4/25</cell></row><row><cell>T4B-A, QAa</cell><cell>1/14</cell><cell>0/14</cell><cell>2/15</cell><cell>7/15</cell><cell>6/15</cell><cell>0/2</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4/25</cell></row><row><cell>T4B-B, QAa</cell><cell>1/14</cell><cell>0/14</cell><cell>1/15</cell><cell>8/15</cell><cell>4/15</cell><cell>0/2</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5/25</cell></row><row><cell>T4B-C, QAa</cell><cell>0/14</cell><cell>1/14</cell><cell>2/15</cell><cell>1/15</cell><cell>6/15</cell><cell>1/2</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4/25</cell></row><row><cell>T5A, QAm</cell><cell>1/10</cell><cell>8/21</cell><cell>0/5</cell><cell>0/25</cell><cell>0/14</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3/25</cell></row><row><cell>T5B-A, QAm</cell><cell>1/10</cell><cell>3/21</cell><cell>1/5</cell><cell>0/25</cell><cell>0/14</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0/25</cell></row><row><cell>T5B-B, QAm</cell><cell>2/10</cell><cell>2/21</cell><cell>0/5</cell><cell>0/25</cell><cell>0/14</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0/25</cell></row><row><cell>T5B-C, QAm</cell><cell>0/10</cell><cell>3/21</cell><cell>0/5</cell><cell>0/25</cell><cell>0/14</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2/25</cell></row><row><cell>T5B-A, QAa</cell><cell>1/10</cell><cell>3/21</cell><cell>1/5</cell><cell>0/25</cell><cell>0/14</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0/25</cell></row><row><cell>T5B-B, QAa</cell><cell>2/10</cell><cell>3/21</cell><cell>0/5</cell><cell>0/25</cell><cell>0/14</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2/25</cell></row><row><cell>T5B-C, QAa</cell><cell>0/10</cell><cell>2/21</cell><cell>0/5</cell><cell>0/25</cell><cell>0/14</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1/25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,90.00,493.38,423.00,32.65"><head>Table 2 :</head><label>2</label><figDesc>Distribution of correct answers (TOP5) according to answer type. Org = organization, Per = person, Tim = time, Mea = measure, Met/Sys = method/system, Mat = material, Col = color, Def = definitional.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,730.30,143.96,6.64"><p>http://cnts.ua.ac.be/conll2002/ner</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,105.24,705.37,407.75,6.99;6,90.00,714.84,372.39,6.99"><p>In questions such as "Who helped solving the packet loss problem? " is impossible to know if the correct answer is a person name or an organization name. For this question, the answer is the name of a university.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been partially funded by the <rs type="funder">European Commission</rs> (<rs type="projectName">CHIL</rs>, <rs type="grantNumber">IST-2004-506909</rs>) and the <rs type="funder">Spanish Ministry of Science (TEXTMESS project)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_nTne9VW">
					<idno type="grant-number">IST-2004-506909</idno>
					<orgName type="project" subtype="full">CHIL</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table" coords="6,205.91,323.97,3.87,8.74">3</ref>: Error analysis of the QA system components.</p><p>bound for answer extraction is limited specially by NERC performance. The average number of factual questions in all runs is 75.3, the average value for PR is 56.61 and the average for "C.NE" is 22.44, so in less than 40% of the passages the answer is correctly tagged allowing its correct extraction in the answer extraction step. "QC&amp;NE" is a theoretical upper bound of the total score of each task. We can see that the performance of our answer extraction process is very good since "TOP5" score is very near this upper bound in all tasks. As a remark, all of the scores in T5 are above the upper bound. This is due to the combination of two factors: first, a fall-back mechanism in our answer extraction process to help overcome the PER/ORG ambiguity 2 in question classification, this mechanism allows to answer misclassified questions. Second, a double-error situation when the question is misclassified and the answer is erroneously tagged but matches the question type. The impact of transcription errors in QA can be analyzed in detail thanks to the three different automatic transcripts for task T4B (WERs of T5B have very close values and our overall performance is far too poor for this analysis). Figure <ref type="figure" coords="6,300.89,511.21,4.98,8.74">3</ref> shows graphically the values in table 3 for T4, QA m . The yellow bars show the WER percentage for each transcript (0% for manual reference) and the lines show the evolution of variables "PR", "C.NE", "U.NE", "QC&amp;NE" and "TOP5". The performance of passage retrieval decreases linearly with WER increase. The linear regression PR = 59.78 -0.33 • WER fits the data with a Pearson coefficient r = 0.99. Other measures such as "C.NE", "QC&amp;NE" and "TOP5" are also strongly related to WER and its diminishment is more pronounced. All this measures decrease the same amount when going from 0% WER to 10.6% WER than from 10.6%% to 24.1%. In fact "C.NE" values fit the non-linear regression curve C.NE = 43.9 • 0.94 WER with a coefficient r = 0.97. Therefore we can conclude that the passage retrieval performance decreases linearly with WER while NERC performance decreases exponentially with WER.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.50,711.93,407.51,8.74;7,105.50,723.89,79.32,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,242.99,711.93,172.15,8.74">A dendrite method for cluster analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Calinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Harabasz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,426.95,711.93,86.06,8.74;7,105.50,723.89,38.39,8.74">Communications in Statistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,112.02,407.50,8.74;8,105.50,123.98,318.02,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,287.75,112.02,172.85,8.74">Named entity extraction using adaboost</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ll</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ll</forename><surname>Padró</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,469.47,112.02,43.53,8.74;8,105.50,123.98,287.84,8.74">COLING-02: proceedings of the 6th conference on Natural language learning</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,143.90,407.50,8.74;8,105.50,155.86,375.90,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,225.35,143.90,287.64,8.74;8,105.50,155.86,19.70,8.74">Spoken document retrieval based on approximated sequence alignment</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,134.54,155.86,316.31,8.74">11th International Conference on Text, Speech and Dialogue (TSD 2008)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,175.78,407.50,8.74;8,105.50,187.74,407.50,8.74;8,105.50,199.69,192.53,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,296.61,175.78,216.39,8.74;8,105.50,187.74,138.22,8.74">Robust question answering for speech transcripts using minimal syntactic analysis</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,251.41,187.74,261.59,8.74;8,105.50,199.69,162.04,8.74">Proceedings of the CLEF 2007 Workshop on Cross-Language Information Retrieval and Evaluation</title>
		<meeting>the CLEF 2007 Workshop on Cross-Language Information Retrieval and Evaluation</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,219.62,407.50,8.74;8,105.50,231.57,172.42,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,196.10,219.62,273.75,8.74">Learning question classifiers: The role of semantic information</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,479.89,219.62,33.11,8.74;8,105.50,231.57,141.87,8.74">Journal of Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,251.50,407.51,8.74;8,105.50,263.45,247.33,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,150.93,251.50,334.89,8.74">High-performance, open-domain question answering from large text collections</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Paşca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Dallas, TX</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Southern Methodist University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="8,105.50,283.38,407.50,8.74;8,105.50,295.33,407.50,8.74;8,105.50,307.29,386.40,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,343.41,283.38,169.59,8.74;8,105.50,295.33,318.08,8.74">Design and performance analysis of a factoid question answering system for spontaneous speech transcriptions</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dominguez-Sal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,433.97,295.33,79.02,8.74;8,105.50,307.29,355.86,8.74">Proceedings of the International Conference on Spoken Language Processing (INTERSPEECH 2006)</title>
		<meeting>the International Conference on Spoken Language Processing (INTERSPEECH 2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,327.21,407.50,8.74;8,105.50,339.17,407.50,8.74;8,105.50,351.12,133.67,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,293.85,327.21,219.15,8.74;8,105.50,339.17,62.56,8.74">Named entity recognition from spontaneous opendomain speech</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Comelles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,178.48,339.17,334.52,8.74;8,105.50,351.12,103.12,8.74">Proceedings of the International Conference on Spoken Language Processing (INTERSPEECH 2005)</title>
		<meeting>the International Conference on Spoken Language Processing (INTERSPEECH 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,371.05,407.50,8.74;8,105.50,383.00,407.50,8.74;8,105.50,394.96,136.88,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,472.29,371.05,40.71,8.74;8,105.50,383.00,40.70,8.74">Overview of QAST</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Moureau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mostefa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,182.19,383.00,330.81,8.74;8,105.50,394.96,106.39,8.74">Proceedings of the CLEF 2008 Workshop on Cross-Language Information Retrieval and Evaluation</title>
		<meeting>the CLEF 2008 Workshop on Cross-Language Information Retrieval and Evaluation</meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
