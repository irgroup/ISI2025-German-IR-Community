<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,212.31,182.75,77.35,8.74"><forename type="first">Eamonn</forename><surname>Newman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.36,182.75,78.33,8.74"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">74CC4C69D57C7EE577E424B5D42705CC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ASR</term>
					<term>IR</term>
					<term>Classification Classification</term>
					<term>Information Retrieval</term>
					<term>Automatic Speech Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe a baseline system for the VideoCLEF Vid2RSS task. The system uses an unaltered off-the-shelf Information Retrieval system. ASR content is indexed using default stemming and stopping methods. The subject categories are populated by using the category label as a query on the collection, and assigning the retrieved items to that particular category. We describe the results of the system and provide some high-level analysis of its performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We implemented a system for the generation of topic-based RSS feeds of dual language audiovisual content for the Vid2RSS task <ref type="bibr" coords="1,249.91,482.63,9.96,8.74" target="#b1">[2]</ref>. Our system provides a baseline based on an Information Retrieval approach. We built a standard free text index using videos' ASR transcripts (and metadata) as the content. To populate a particular feed, the feed label (i.e., the category name) was used as a query to the search engine. The retrieved results were used as the component items of the feed. Some of the runs allowed an item to appear in one feed only, while others allowed items to appear in multiple feeds. These are described in full detail in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>We used the open source Lucene Search Engine technology <ref type="bibr" coords="1,361.09,597.17,10.52,8.74" target="#b0">[1]</ref> as the base technology for our system. Dutch-language content was stopped, stemmed and tokenised using Lucene's built-in Dutch analyser, DutchAnalyzer<ref type="foot" coords="1,232.00,619.51,3.97,6.12" target="#foot_0">1</ref> . English-language content was stopped and tokenised by the Lucene default tokeniser, StandardAnalyzer<ref type="foot" coords="1,290.17,631.46,3.97,6.12" target="#foot_1">2</ref> . The StandardAnalyzer does not perform any stemming of tokens.</p><p>For indexing the ASR transcripts, we parsed the documents and processed all FreeTextAnnotation elements. We did not make any use of the timestamp information available. For indexing the metadata documents (for Run 5), we indexed all description elements. All other metadata fields were discarded.</p><p>The feed categories were populated by using the label of each feed as a query. Feeds were populated in the order given in Table <ref type="table" coords="1,261.87,716.72,3.88,8.74" target="#tab_0">1</ref>. The labels were arranged in order of most specific to least specific. This meant that when an item was retrieved, it was placed into the most specific category possible. In Runs 1,3, and 5, the item was categorised only once. In Runs 2 and 4, it was placed in all categories for which it was retrieved. Items were permitted to appear in multiple feeds in two of the runs (Runs 3 and 4), but were restricted to single appearances in the other runs. The restriction was imposed to improve the precision of the classification task, since labels such as "film" were very general and tended to capture most, if not all, of the items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dutch</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Run Configurations</head><p>In this section, we describe each of the separate runs which were submitted to the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Dutch ASR transcripts:</head><p>In this run, we indexed the entire set of Dutch ASR transcripts (the FreeTextAnnotation elements). The index was queried with the labels in the order given above and the feeds were populated.</p><p>2. English ASR transcripts: This is identical to Run 1, using English ASR transcripts and translations of the category labels as queries.</p><p>3. Dutch ASR with blind relevance feedback: We ran the same queries as Run 1, but added an additional step of blind relevance feedback, to expand the query terms. Since this action reduces the precision of any query, we also relaxed the restriction on items appearing in only one feed. Queries were expanded by performing an initial query which consisted of just the category label. We take the first 10 retrieved documents and extract the 5 most frequently occuring terms in each. We process this set of 50 terms to remove any duplicates.</p><p>The remaining terms are combined with the original query to form the expanded query.</p><p>4. English ASR with blind relevance feedback: This is identical in method to Run 3, but is the data now consists of the English ASR transcripts, rather than the Dutch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Dutch metadata:</head><p>We indexed the catalogue metadata from B&amp;G which were supplied in the data sets. Specifically, we used the description elements from the metadata documents. Once again, the Dutch category term labels were used as queries, and the items were restricted to appear in one feed only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In Table <ref type="table" coords="2,132.43,721.73,4.98,8.74" target="#tab_1">2</ref> we present the retrieval scores attained by our system runs. A direct comparison of Runs 1 and 2 suggest that the Dutch transcripts were more useful in identifying the subject categories than the English ones. Indeed, the English transcripts had the poorest f-scores at both micro and macro level. This is likely attributable to the fact that the majority of the dialogue was in Dutch and so contained less "noise" than the English counterparts. Processing of the ASR transcripts to identify the points at which the language changed would allow for the combination of transcripts (or the removal of erroneous segments) which would improve classification performance. Runs 3 and 4 used relevance feedback to expand the queries and allowed items to be placed in multiple categories. As we can see, this relaxation resulted in a large drop in the micro-average precision scores of these systems; conversely, the micro-average recall is much higher in these runs. As items were placed in multiple categories, so the chances of an item being correctly classified was much greater, however the number of false positives also increased.</p><p>As can be seen from the results, Run 5 performed particularly well in terms of precision, and relatively well (when compared to our other runs) in terms of Recall. However, since this was on the metadata, and not on the ASR transcripts it cannot be directly compared to the others. The higher precision scores do suggest that there may be merit in combining the different data sets available.</p><p>One immediately obvious drawback with this system is that it is not possible to guarantee that all items will be classified. If an item is not retrieved for any of the queries, then it will not be placed in any of the category feeds. As it happens, this was not the case for any of the runs with this particular data set (probably due to the presence of highly generic labels such as "film" and "music")</p><p>Additionally, the number of terms added in the query expansion phase could be reduced. The maximum for this was 50, but elimination of duplicates meant that the size of the set was generally much smaller. Nevertheless, it seems that too many terms were added to the queries, and this is supported by the difference in micro-average precision between Runs 1 and 3 and Runs 2 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>These notes discussed the baseline system implemented in DCU to address the VidCLEF challenge. We outlined the system architecture and how data was processed. The parameters for each run were given and the results of these were examined and analysed.</p><p>The results suggest that there is room for improvement in our system. The precision scores could be improved by finer-grained query expansion, which will be examined in the next set of experiments we carry out. Additionally, the performance on the English-language content could be improved by use of a stemming algorithm, such as Porter <ref type="bibr" coords="3,356.97,623.82,9.97,8.74" target="#b2">[3]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,197.29,157.74,208.41,150.15"><head>Table 1 :</head><label>1</label><figDesc>Category Label Order</figDesc><table coords="2,197.29,157.74,208.41,128.69"><row><cell></cell><cell>English</cell></row><row><cell>archeologie</cell><cell>archeology</cell></row><row><cell>architectuur</cell><cell>architecture</cell></row><row><cell>chemie</cell><cell>chemistry</cell></row><row><cell>dansen</cell><cell>dance</cell></row><row><cell>schilderijen</cell><cell>paintings</cell></row><row><cell cols="2">wetenschappelijk onderzoek scientific research</cell></row><row><cell>beeldende kunst</cell><cell>visual arts</cell></row><row><cell>geschiedenis</cell><cell>history</cell></row><row><cell>film</cell><cell>film</cell></row><row><cell>muziek</cell><cell>music</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,152.89,110.43,297.22,102.73"><head>Table 2 :</head><label>2</label><figDesc>Vid2RSS Scores for Runs 1 to 5</figDesc><table coords="3,152.89,110.43,297.22,81.27"><row><cell cols="6">metric Run 1 Run 2 Run 3 Run 4 Run 5</cell></row><row><cell>micro-average precision</cell><cell>0.50</cell><cell>0.32</cell><cell>0.16</cell><cell>0.17</cell><cell>0.83</cell></row><row><cell>micro-average recall</cell><cell>0.35</cell><cell>0.21</cell><cell>0.91</cell><cell>0.72</cell><cell>0.18</cell></row><row><cell>f-score micro-average</cell><cell>0.41</cell><cell>0.25</cell><cell>0.28</cell><cell>0.28</cell><cell>0.29</cell></row><row><cell>macro-average precision</cell><cell>0.54</cell><cell>0.62</cell><cell>0.42</cell><cell>0.50</cell><cell>0.93</cell></row><row><cell>macro-average recall</cell><cell>0.55</cell><cell>0.38</cell><cell>0.90</cell><cell>0.70</cell><cell>0.28</cell></row><row><cell>f-score macro-average</cell><cell>0.54</cell><cell>0.47</cell><cell>0.58</cell><cell>0.59</cell><cell>0.43</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,735.65,163.47,6.99"><p>org.apache.lucene.analysis.nl.DutchAnalyzer</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,105.24,745.16,200.10,6.99"><p>org.apache.lucene.analysis.standard.StandardAnalyzer</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,105.50,678.58,407.50,8.74;3,105.50,690.54,115.16,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="3,252.98,678.58,255.62,8.74">Lucene: Java-based Indexing and Searching technology</title>
		<ptr target="http://lucene.apache.org/" />
		<imprint>
			<publisher>Apache Software Foundation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,105.50,710.46,407.49,8.74;3,105.50,722.42,407.50,8.74;3,105.50,734.37,280.40,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,348.25,710.46,164.74,8.74;3,105.50,722.42,333.75,8.74">Overview of VideoCLEF 2008: Automatic generation of topic-based feeds for dual language audio-visual content</title>
		<author>
			<persName coords=""><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eamonn</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gareth</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,462.06,722.42,50.94,8.74;3,105.50,734.37,64.84,8.74">CLEF 2008 workshop notes</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Borri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.50,112.02,301.26,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,173.79,112.02,140.78,8.74">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,323.13,112.02,30.53,8.74">Progam</title>
		<imprint>
			<date type="published" when="1980-07">July 1980</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
