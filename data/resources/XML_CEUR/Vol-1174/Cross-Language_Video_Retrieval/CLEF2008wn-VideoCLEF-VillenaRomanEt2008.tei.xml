<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,200.28,80.29,194.99,12.64;1,150.96,96.37,293.34,12.64">MIRACLE at VideoCLEF 2008: Classification of Multilingual Speech Transcripts</title>
				<funder ref="#_E2CpRMp">
					<orgName type="full">Spanish R+D National Plan</orgName>
				</funder>
				<funder ref="#_rrauSce">
					<orgName type="full">Madrid R+D Regional Plan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,207.96,123.28,83.58,8.96"><forename type="first">Julio</forename><surname>Villena-Román</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Decisions and Language</orgName>
								<orgName type="institution">DAEDALUS -Data</orgName>
								<address>
									<country>S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.68,123.28,74.58,8.96"><forename type="first">Sara</forename><surname>Lana-Serrano</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Decisions and Language</orgName>
								<orgName type="institution">DAEDALUS -Data</orgName>
								<address>
									<country>S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,200.28,80.29,194.99,12.64;1,150.96,96.37,293.34,12.64">MIRACLE at VideoCLEF 2008: Classification of Multilingual Speech Transcripts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F59A742B24C484A48F7A786B3B4E5108</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.2 Information Storage</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital libraries. H.2 [Database Management]: H.2.5 Heterogeneous Databases</term>
					<term>E.2 [Data Storage Representations] Video retrieval, domain-specific vocabulary, thesaurus, linguistic engineering, information retrieval, indexing, relevance feedback, multilingual speech transcripts. VideoCLEF, VID2RSS, CLEF, 2008</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of MIRACLE research consortium at the VideoCLEF track at CLEF 2008. We took part in both the main mandatory Classification task that consists in classifying videos of television episodes using speech transcripts and metadata, and the Keyframe Extraction task, whose objective is to select keyframes that represent individual episodes from a set of supplied keyframes (one from each shot of the video source). For the first task, our system is composed of two main blocks, the first in charge of building the core system knowledge base, and then the set of operational elements that are needed to classify the speech transcripts of the topic episodes and generate the output in RSS format. For the second task, our approach is based on the assumption that the most representative fragment (shot) of each episode is the one whose distance to the whole episode is the lowest, considering a vector space model. 4 runs were submitted in all. Regarding the classification task, we ranked 3 rd (out of 6 participants) in terms of precision and 2 nd in terms of recall.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>MIRACLE team is a research consortium formed by research groups of three different universities in Madrid (Universidad Politécnica de Madrid, Universidad Autónoma de Madrid and Universidad Carlos III de Madrid) along with DAEDALUS, a small/medium size enterprise (SME) founded in 1998 as a spin-off of two of these groups and a leading company in the field of linguistic technologies in Spain. MIRACLE has taken part in CLEF since 2003 in many different tracks and tasks, including the main bilingual, monolingual and cross lingual tasks as well as in ImageCLEF <ref type="bibr" coords="1,199.44,599.44,11.71,8.96" target="#b3">[4]</ref>  <ref type="bibr" coords="1,213.72,599.44,10.60,8.96" target="#b8">[8]</ref>, Question Answering, WebCLEF, GeoCLEF and VideoCLEF tracks. This paper describes our participation in the VideoCLEF task, a new track for CLEF 2008. The goal of this track is to develop and evaluate tasks in processing video content in a multilingual environment. This track for 2008 is dedicated to Vid2RSS task which comprises a number of subtasks including topic classification performed on dual language videos. The main objective involves assigning topic class labels to videos of television episodes. Speech recognition transcripts, metadata records (containing title and description) and video keyframes (and shot boundaries) for each episode are supplied. The video data are Dutch television documentaries and contain Dutch as a dominant language, but also contain a high proportion of spoken English (i.e., interview guests often speak in English). The output format is a set of RSS-feeds, one for each topic class, created by concatenating the metadata records for the episodes assigned to a given topic class.</p><p>We have participated in the main mandatory Classification task that consists in classifying videos of television episodes using speech transcripts and metadata, and in the Keyframe Extraction task, whose objective is to select keyframes that represent individual episodes from a set of supplied keyframes (one from each shot of the video source).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Classification task</head><p>The objective of the mandatory Classification task is to perform the speech recognition transcript-based topic classification (i.e., classify the videos of the television documentary episodes using the speech recognition output only). Videos include dual language (Dutch + English) episodes. Output is 10 topic-based feeds, each containing the episodes that have been classified in that topic category. The defined topic classes are Archeology, Architecture, Chemistry, Dance, Film, History, Music, Paintings, Scientific research and Visual arts. While classification task I is based on episode speech transcripts only, classification task II allows to use episode metadata for classification.</p><p>Figure <ref type="figure" coords="2,100.56,220.36,4.98,8.96">1</ref> shows the logical architecture of our system. The system is composed of two main blocks. The first block is in charge of building a corpus that can be used as the core system knowledge base. The second block includes the set of operational elements that are needed to classify the speech transcripts of the topic episodes and generate the output in RSS format. Those operational elements cover an information retrieval system and a classifier, as well as auxiliary modules for text extraction, filtering and RSS generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. Overview of the classification system</head><p>The first step is to obtain the necessary training data for the classifier, as part of the task is, given the description of the subject class, to gather the necessary data to train the classifier. Our knowledge base for training the classifier was generated from Wikipedia articles. In order to do so, we first established a matching between the topic classes provided for the task and the classification topics that Wikipedia uses for articles, encoded in metadata. The next step was to obtain a list of Wikipedia articles belonging to each of the 10 topic class, for each task language, i.e. English and Dutch. Table <ref type="table" coords="2,250.08,577.36,4.98,8.96" target="#tab_0">1</ref> shows the number of articles for each topic class and language. Next, each document is processed through the following sequence of operations:</p><p>1. Text extraction: Ad-hoc scripts are run to obtain the actual text content of articles, filtering out Wikipedia tags.</p><p>2. Diacritics removal and conversion to lowercase: all terms are normalized by removing diacritics (in the case of Dutch) and changing all letters to lowercase.</p><p>3. Filtering: All words recognized as stopwords are filtered out. Stopwords in the two target languages were initially obtained from <ref type="bibr" coords="3,220.68,160.12,11.83,8.96" target="#b6">[6]</ref> and afterwards extended using several other sources <ref type="bibr" coords="3,452.28,160.12,11.71,8.96" target="#b2">[3]</ref> as well as our own knowledge and resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Stemming: This process is applied to each one of the terms to be indexed or used for retrieval. Standard stemmers from Porter <ref type="bibr" coords="3,192.84,200.56,11.71,8.96" target="#b5">[5]</ref> have been used.</p><p>The processed corpus is indexed with Lucene retrieval engine <ref type="bibr" coords="3,342.96,218.08,11.71,8.96" target="#b0">[1]</ref> to allow a fast and efficient access to information needed for the classification. Two different indexes are built, one for each language.</p><p>The classifier is based on the k-Nearest Neighbour algorithm <ref type="bibr" coords="3,331.20,247.12,10.69,8.96" target="#b7">[7]</ref>. To find the class for a given episode, the content is first processed as explained before. Then the whole set of resulting terms is used to build a query that is given to the Lucene search engine to obtain the list of the top k most relevant (i.e., most similar) articles in the Wikipedia-based corpus. Finally, the class of the given episode is the most frequent class in the top k results. After some preliminary experiments, a value of k set to 10 was chosen for our runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Keyframe extraction task</head><p>Our system is based on the assumption that, in the context of a vector space model representation <ref type="bibr" coords="3,472.44,336.40,10.69,8.96" target="#b1">[2]</ref>, the most representative fragment (shot) of each episode (represented by a vector) is the one whose distance to the whole episode (also a vector) is the lowest. The contents of both each shot and the whole episode are first processed as explained before, after extracting the text from the speech transcription. Based on the vector space model, a weighted vector is built for each episode and set of shots, representing the term frequency of the main most significant terms in the given episode. Finally the keyframe extraction module selects the keyframe belonging to the most representative shot in the episode, which is the shot whose vector has the lowest distance from (i.e., is nearest) the vector of the whole episode. The metric used here was the cosine distance <ref type="bibr" coords="3,428.88,416.92,10.69,8.96" target="#b1">[2]</ref>, although Euclidean distance could also be valid. An overview of the system architecture is shown in Figure <ref type="figure" coords="3,423.00,428.32,3.76,8.96" target="#fig_0">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and results</head><p>We have submitted different runs for each proposed subtask: three for the classification task and one for the keyframe extraction. Table <ref type="table" coords="3,181.56,655.00,4.98,8.96" target="#tab_1">2</ref> shows the list of submitted runs. In short, "CNL" run only uses the index for the Dutch corpus, "CNLEN" uses both indexes and gathers together results from any of them, and "CNLMeta" uses the Dutch index but also includes the episode metadata to build the query for the retrieval engine.</p><p>Table <ref type="table" coords="4,96.60,113.56,4.98,8.96" target="#tab_2">3</ref> shows the values of precision and recall obtained by the different runs in the classification task. Classes marked in italics are those whose number of relevant documents is equal to 0, i.e., no episode belongs to this class, according to the task organizers.</p><p>It can be observed that the best precision is achieved with the "CNL" run in which only the Dutch transcription is used. When the knowledge base and the transcription in English are involved, results are noticeably and significantly worse. This could be directly motivated by the fact that the dominant language of the episodes is Dutch. However, the best modelled class is "Music", corresponding to one of the classes that own a higher number of Wikipedia articles in the training set. This may suggest other explanations, such as the fact that training set (the knowledge base) for English is much smaller than the one available for Dutch. Another possible explanation could be that the voice recognition system for English is not as good as for Dutch. Obviously, these issues have to be further studied. Although results for the classification task II, i.e. including metadata, seem to be worse than results for classification task I, this is misleading. For all classes except Scientific research and Visual arts, precision values are higher if metadata is used, but the average value is worse specially due to the very low value for Visual arts. This issue still has to be analyzed.</p><p>Comparing to other groups, we successfully ranked 3 rd out of 6 participants in terms of precision, 2 nd in terms of recall and also f-score (not shown in the table).</p><p>Regarding the keyframe extraction task, MIRACLE was the only participant who submitted results. Thus, the evaluation has been manually made. Five native Dutch speakers (in the age range 20-50) were presented with the title and the description of each video episode along with two keyframes, one manually extracted and one automatically extracted provided by us. They were asked to choose which keyframe they preferred. Of the 40 videos, 1 did not have a keyframe. In two cases, the keyframe chosen manually and that chosen by the system was the same. Thus, each subject was asked about 37 different pairs of keyframes.</p><p>On average, the subjects chose the automatic over the manually selected keyframe in 15.2 cases (41.08%) and the manually over the automatic in 21.8 cases (58.92%). Table <ref type="table" coords="5,324.96,240.64,4.98,8.96" target="#tab_3">4</ref> shows the results of the evaluation process. The "Automatic keyframe" and "Manual keyframe" column represent the number of episodes that the evaluator has selected as more adequate between the automatically or manually extracted keyframe. The last column tries to show the number of correctly extracted keyframes, assuming that the evaluator has chosen correctly. These promising figures indicate that the automatically extracted keyframes may be strong competitors with the manual ones in the short-or middle-term future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>After a preliminary analysis of results obtained in the classification task, we can conclude that there seems to be a direct relationship between the knowledge base associated to a given class and results achieved in it. This probably indicates that the architecture of the system and provided algorithms are useful, but more effort must be invested to improve the knowledge base, both in its volume (coverage) and the pre-processing activities.</p><p>Despite the subjectivity of the keyframe extraction task and lack of any reference experiment to which compare our own system, we can say that these results are promising and encourage us to keep on this line of research for future participations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,188.28,594.16,218.82,8.96"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of the keyframe extraction system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,227.28,600.76,140.82,154.04"><head>Table 1 .</head><label>1</label><figDesc>Articles in the training set</figDesc><table coords="2,227.28,619.60,140.82,135.20"><row><cell>Topic</cell><cell>EN</cell><cell>NL</cell></row><row><cell>Archaeology</cell><cell>666</cell><cell>1280</cell></row><row><cell>Architecture</cell><cell cols="2">1409 2002</cell></row><row><cell>Chemistry</cell><cell cols="2">2207 3130</cell></row><row><cell>Dance</cell><cell>497</cell><cell>3248</cell></row><row><cell>Film</cell><cell>338</cell><cell>4934</cell></row><row><cell>History</cell><cell cols="2">1655 7822</cell></row><row><cell>Music</cell><cell>506</cell><cell>8691</cell></row><row><cell>Paintings</cell><cell>612</cell><cell>213</cell></row><row><cell cols="3">Scientific research 2845 15307</cell></row><row><cell>Visual arts</cell><cell>754</cell><cell>1280</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,136.20,678.52,327.68,67.76"><head>Table 2 .</head><label>2</label><figDesc>List of runs</figDesc><table coords="3,136.20,698.20,327.68,48.08"><row><cell cols="3">Run Identifier Language Task</cell></row><row><cell>MIRACLE-CNL</cell><cell>NL</cell><cell>Classification I + Keyframe_Extraction I</cell></row><row><cell>MIRACLE-CNLEN</cell><cell>NL+EN</cell><cell>Classification I</cell></row><row><cell>MIRACLE-CNLMeta</cell><cell>NL</cell><cell>Classification II</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,110.04,258.04,375.54,471.32"><head>Table 3 .</head><label>3</label><figDesc>Classification task results</figDesc><table coords="4,110.04,277.12,375.54,178.28"><row><cell></cell><cell></cell><cell>Precision</cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell></row><row><cell></cell><cell cols="3">CNL CNLEN CNLMeta</cell><cell cols="3">CNL CNLEN CNLMeta</cell></row><row><cell>Archaeology</cell><cell>0.25</cell><cell>0.25</cell><cell>0.40</cell><cell>0.14</cell><cell>0.14</cell><cell>0.29</cell></row><row><cell>Architecture</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>Chemistry</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>Dance</cell><cell>0.00</cell><cell>0.00</cell><cell>0.13</cell><cell>0.00</cell><cell>0.00</cell><cell>0.67</cell></row><row><cell>Film</cell><cell>1.00</cell><cell>0.25</cell><cell>1.00</cell><cell>0.00</cell><cell>0.33</cell><cell>0.00</cell></row><row><cell>History</cell><cell>0.25</cell><cell>0.26</cell><cell>0.38</cell><cell>0.30</cell><cell>0.50</cell><cell>0.60</cell></row><row><cell>Music</cell><cell>0.64</cell><cell>0.65</cell><cell>0.65</cell><cell>0.95</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>Paintings</cell><cell>1.00</cell><cell>0.00</cell><cell>1.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>Scientific Research</cell><cell>0.29</cell><cell>0.21</cell><cell>0.21</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>Visual Arts</cell><cell>1.00</cell><cell>0.20</cell><cell>0.14</cell><cell>0.00</cell><cell>0.40</cell><cell>0.20</cell></row><row><cell>ALL (microaveraged 1 )</cell><cell>0.43</cell><cell>0.29</cell><cell>0.37</cell><cell>0.51</cell><cell>0.61</cell><cell>0.65</cell></row><row><cell>ALL (macroaveraged 1 )</cell><cell>0.44</cell><cell>0.18</cell><cell>0.39</cell><cell>0.44</cell><cell>0.54</cell><cell>0.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,193.32,304.60,268.74,119.24"><head>Table 4 .</head><label>4</label><figDesc>Keyframe extraction task results</figDesc><table coords="5,193.32,324.40,268.74,99.44"><row><cell></cell><cell>Automatic keyframe</cell><cell>Manual Keyframe</cell><cell>Well selected?</cell></row><row><cell>Subject 1</cell><cell>16</cell><cell>21</cell><cell>18</cell></row><row><cell>Subject 2</cell><cell>14</cell><cell>23</cell><cell>16</cell></row><row><cell>Subject 3</cell><cell>16</cell><cell>21</cell><cell>18</cell></row><row><cell>Subject 4</cell><cell>17</cell><cell>20</cell><cell>19</cell></row><row><cell>Subject 5</cell><cell>13</cell><cell>24</cell><cell>15</cell></row><row><cell>Average</cell><cell>41.08%</cell><cell>58.92%</cell><cell>44.10%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,121.80,466.35,363.06,7.24;4,116.04,476.07,368.70,7.24;4,116.04,485.31,137.85,7.24"><p>By definition, macroaverage values are computed as the mean of values of all classes that are first individually calculated. In contrast, the microaverage measures first obtain the aggregate counts for all classes and then calculate the values of precision and recall.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been partially supported by the <rs type="funder">Spanish R+D National Plan</rs>, by means of the project <rs type="projectName">BRAVO (Multilingual and Multimodal Answers Advanced Search -Information Retrieval</rs>), <rs type="grantNumber">TIN2007-67407-C03-03</rs> and by <rs type="funder">Madrid R+D Regional Plan</rs>, by means of the project <rs type="projectName">MAVIR (Enhancing the Access and the Visibility of Networked Multilingual Information for the Community of Madrid</rs>), <rs type="grantNumber">S-0505/TIC/000267</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_E2CpRMp">
					<idno type="grant-number">TIN2007-67407-C03-03</idno>
					<orgName type="project" subtype="full">BRAVO (Multilingual and Multimodal Answers Advanced Search -Information Retrieval</orgName>
				</org>
				<org type="funded-project" xml:id="_rrauSce">
					<idno type="grant-number">S-0505/TIC/000267</idno>
					<orgName type="project" subtype="full">MAVIR (Enhancing the Access and the Visibility of Networked Multilingual Information for the Community of Madrid</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,97.92,714.40,314.73,8.96" xml:id="b0">
	<monogr>
		<ptr target="http://lucene.apache.org[Visited10/08/2008" />
		<title level="m" coord="5,97.92,714.40,91.56,8.96">Apache Lucene project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,97.92,731.92,368.01,8.96" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,241.92,731.92,119.42,8.96">Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribeiro-Prieto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,97.92,73.12,426.49,8.96;6,97.92,84.64,203.61,8.96" xml:id="b2">
	<monogr>
		<ptr target="http://www.computing.dcu.ie/~gjones/CLEF2005/Multi-8/[Visited10/08/2008" />
		<title level="m" coord="6,97.92,73.12,261.02,8.96">CLEF 2005 Multilingual Information Retrieval resources page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,97.92,102.04,426.58,8.96;6,97.92,113.56,426.54,8.96;6,97.92,125.08,426.45,8.96;6,97.92,136.60,101.37,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,129.48,113.56,242.91,8.96">Combining Textual and Visual Features for Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Martínez-Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><forename type="middle">;</forename><surname>Villena-Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><forename type="middle">M</forename><surname>García-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>González-Cristóbal</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Carlos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,379.56,113.56,144.90,8.96;6,97.92,125.08,349.56,8.96">Accessing Multilingual Information Repositories: 6th Workshop of the Cross-Language Evaluation Forum, CLEF 2005</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="6,202.32,136.60,322.29,8.96;6,97.92,148.12,72.93,8.96" xml:id="b4">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="s" coord="6,305.40,136.60,143.13,8.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<idno type="ISSN">0302-9743</idno>
		<imprint>
			<biblScope unit="volume">4022</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,97.92,165.64,426.42,8.96;6,97.92,177.04,51.45,8.96" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://www.snowball.tartarus.org" />
		<title level="m" coord="6,160.32,165.64,156.50,8.96">Snowball stemmers and resources page</title>
		<imprint>
			<date type="published" when="2008-08-10">10/08/2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,97.92,194.56,426.46,8.96;6,97.92,206.08,207.57,8.96" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,201.48,194.56,269.91,8.96">Page of resources for CLEF (Stopwords, transliteration, stemmers</title>
		<ptr target="http://www.unine.ch/info/clef[Visited10/08/2008" />
		<imprint/>
		<respStmt>
			<orgName>University of Neuchatel</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,97.92,223.60,426.45,8.96;6,97.92,235.12,165.33,8.96" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,216.00,223.60,304.37,8.96">Data Mining: Practical machine learning tools and techniques, 2nd Edition</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,97.92,252.64,426.58,8.96;6,97.92,264.04,426.42,8.96;6,97.92,275.56,426.45,8.96;6,97.92,287.08,67.89,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,130.92,264.04,393.42,8.96;6,97.92,275.56,138.26,8.96">MIRACLE at ImageCLEFphoto 2007: Evaluation of Merging Strategies for Multilingual and Multimedia Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Julio</forename><forename type="middle">;</forename><surname>Villena-Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sara</forename><forename type="middle">;</forename><surname>Lana-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Martínez-Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Luis; González-Cristóbal</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Carlos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,244.44,275.56,189.21,8.96">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
