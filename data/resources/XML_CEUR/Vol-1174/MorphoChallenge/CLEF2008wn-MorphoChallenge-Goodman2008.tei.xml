<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.17,111.99,290.66,8.77">Morphological Induction Through Linguistic Productivity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,252.99,123.95,97.02,8.77"><forename type="first">Sarah</forename><forename type="middle">A</forename><surname>Goodman</surname></persName>
							<email>sagoodm@umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.17,111.99,290.66,8.77">Morphological Induction Through Linguistic Productivity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E07F94B4F4CBAE1D3809B1B3AD139280</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>productivity</term>
					<term>feature sets</term>
					<term>clustering</term>
					<term>infixation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The induction program we have crafted relies primarily on the linguistic notion of productivity to find affixes in unmarked text and without the aid of prior grammatical knowledge. In doing so, the algorithm unfolds in two stages. It first finds seed affixes, to include infixes and circumfixes, by assaying the character of all possible internal partitions of all words in a small corpus no larger than 3,000 tokens. It then selects a small subset of these seed affixes by examining the distribution patterns of roots they fashion to, as demonstrated in a possibly larger second training file. Specifically, it hypothesizes that valid roots take a partially overlapping affix-set, and develops this conjecture into agendas for both feature-set generation and binary clustering. It collects feature sets for each candidate by what we term affix-chaining, delineating (and storing) a path of affixes joined, with thresholding caveats, via the roots they share. After clustering these resultant sets, the program yields two affix groups, an ostensibly valid collection and a putatively spurious one. It refines the membership of the former by again examining the quality of shared root distributions across affixes. This second half of the program, furthermore, is iterative. This fact is again based in productivity, as we ration that, should a root take one affix, it most likely takes more. The code therefore seeds a subsequent iteration of training with affixes that associate with roots learned during the current pass. If, for example, it recognizes view on the first pass, and viewership occurs in the second training file, the program will evaluate -ership, along with its mate -er, via clustering and root connectivity on the second pass. The results of this method are thus far mixed according to training file size. Time constraints imposed by shortcomings in the algorithm's code have thus far prevented us from fully training on a large file. For Morpho Challenge 2008, not only did we only train on just 1-30% of the offered text, thereby saddling the stemmer with a number of Out Of Voculary items, but, we also divided that text into smaller parts, thereby, as the results show, omitting valuable information about the true range of affix distributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Here we offer an algorithm that can detect not only standard prefixation and suffixation, but also multiple infixes per word, covering templatic morphologies; reduplication, to include a combination of reduplication and infixation; and point-of-affix gemination. Furthermore, the program can elicit these linguistic features from a relatively small file. As it largely eschews calculations based on root or affix frequency in a training file, it can therefore avoid sparse data pitfalls. It may therefore be a particular boon to processing of Less Commonly Taught Languages, given that text-based resources are often hard to find in certain regional languages.</p><p>To shepherd this algorithm to such success, we will gradually and successively winnow affix candidates through two general principles, one founded in a breed of common sense, the other in descriptive linguistic doctrines. First, we repeatedly employ the use of what we term co-occurrence, a form of boot-strapping. In this, we assume that a less reliable unit, be it a character or an affix, in the context of a more trustworthy counterpart should be retained in the candidate pool. Here, this context can refer to a single word or to a root taken in common by multiple affixes. Second, we leverage and deploy co-occurrence by placing great faith in the linguistic notion of productivity. The term ultimately refers to the prevalence of a linguistic unit, generally a morpheme, in a language. In so denoting, it specifically invokes the degree to which a unit may combine or directly co-occur with a separate linguistic unit. We interpret the prospect of modified linguistic ubiquity to increase recall, especially among derivational affixes, by specifically investigating root connectivity among affix candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>Dasgupta and Ng (2007) develop a knowledge-free analyzer to parse Bengali by relying on perceived general principles of morphemic concatenation and root distribution, and they assess them by means of heuristics that concern frequency but mostly stop short of being probabilities.</p><p>Cavar et. al (2004) attempt an induction schema based on a combination of Dasgupta and Ng's seeding techniques to derive a weighted implementation of information theory metrics. The authors generate a seed list of morphemes substring matching and refine the resultant candidate list by incremental construction of a grammatical framework meant to dictate, or at least clarify, emergent patterns in the input. <ref type="bibr" coords="2,104.94,229.48,81.66,8.74" target="#b1">Brent et. al (1995)</ref> and <ref type="bibr" coords="2,209.00,229.48,76.35,8.74" target="#b5">Goldsmith (2001)</ref> both practice Minimal Description Length and take the condensation of space quite seriously. Their programs find affixes based on how well they can shorten the training corpus by counting repeated endings and beginnings of words just one time. To ensure that each doesn't over-estimate the presence of affixes by this process, the method imposes a weight or penalty for each affix guessed, partially based upon its length. <ref type="bibr" coords="2,104.94,289.26,94.12,8.74" target="#b7">Monson, et. al (2007)</ref> structure the backbone of their induction program to be what they term schemes, which are similar, if not identical to, Goldsmith's signatures. These consist of classes of affixes united by a set of roots that exhaustively take them. Each such scheme they arrange into a lattice network based upon parent-child relations. The authors then traverse this constructed network and target possible spurious classes based largely upon their lack of a parent-child stem ratio above a certain, variable threshold. They then engage in group agglomerative hierarchical clustering to further target valid members of the remaining schemes. <ref type="bibr" coords="2,104.94,372.94,72.09,8.74">Bernhard (2005)</ref> and Dejean (1998) develop a means of segmentation based on word-internal character transition probabilities. Their concept is, in fact, the basis of <ref type="bibr" coords="2,408.23,384.90,56.73,8.74">Harris (1955)</ref>, and both modern instantiations of the idea, perhaps as expected, vary slightly from each other. Dejean, for his part, uses a bootstrapping measure to boost the discovery or morphemes based upon patterns already heavily favored as correct. In Bernhard's conceptualization of the solution, transition probabilities are employed based upon substring incidence ratios. Shortcomings of the above approaches include a requirement for large amounts of training data (Dasgupta and Ng; Ćavar et. al), increased memory accommodations (Monson et al), low recall (MDL approaches; Dasgupta and Ng) and general over-reliance on mathematical technique at the expense of linguistic primacy <ref type="bibr" coords="2,219.65,480.54,98.50,8.74">(Brent and Goldsmith)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generating Seed Affixes</head><p>The algorithm's first section is guided by two goals: first, we wish to produce infixes among prefixes and suffixes, and, second, we wish to yield multi-morphemic combinations of concatenative languages amid the sparse data given by smaller corpora. Here, the code generates what we'll call seed affixes. These are character sequences assigned a position relative to either the beginning or ending of a word and are selected based upon their frequency of occurrence in a relatively small file of approximately 3000 tokens.</p><p>The first step in this process is to take all possible internal partitions of every word in the seed file, which themselves sum to 2 word.length-1 . We call each resultant partition analysis a partition schema. Some of these schemas we then toss from consideration, both to manage memory allocation and, primarily, to downplay the significance and effect of such unigrams. As a rule, we omit any partition schema, such as any of the 16 listed in table 1, if the number of internal candidate morpheme breaks for a given word is greater than half the letters composing that word.</p><p>Potentially, each break in each schema represents a morpheme. We reduce this multitude in two stages. The first method examines the frequency of each positioned segment across partition schemas and yields two sets of 100 candidates, one set for potential prefixes and infixes and a second for suffixes and additional infixes. This task modifies the assessed prevalence of each segment according to a weight ratio it applies to each instance, dividing the longest word length in the seed corpus by the length of the word containing the segment being evaluated. measure avoids disproportionately overstating the effect of longer words in the seed file, which, by their nature, propagate a greater number of partition schemas. The second stage uses each list of 100 positioned segments to deliver all possible segmentations of the seed corpus. It then examines each (possibly competing) lexical analysis to derive a smaller set of candidates: those that that are the 15 most frequent segments and the less frequent segments that occur along side them in anyone analysis. This generally yields between 40-50 seed affix candidates.</p><p>The final step in seed affix generation involves taking all possible sequential or quasi-sequential combinations of the the top 15 affix candidates and their bootstrapped associates. This process covers, or, at least supplements, affix generation more languages that one might think, as the seed segments themselves are usually no longer than two characters. We can therefore derive the English affix ous from seed affixes s|0 and ou|1. This combinatory process is even more important for polysynthetic languages. In these cases, individual morphemes can occur in multiple contexts of others, and, theoretically, the previous work done on partition schemas should discover these individual units separately. Taking their combinations, then, allows us to generate the range of concatenative manifestations that the particular language shows, even if only a fraction of them were present in the seed file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Using a Trie</head><p>In order to assess the validity of the seed affix candidates and their possible combinations, we store contents of a second, usually larger, file into both a forwards and backwards trie. We chose a trie to winnow good from bad sequences for simple reasons of efficiency. Such a storage mechanism reduces memory load because it conflates stored values for words composed of identical sub-sequences. A trie also allows for relatively quick root recovery without reliance on regular expressions.</p><p>In addition to simply weeding out invalid or unattested candidate affixes, through simple book-keeping of sorts, the trie can also be used to learn affix candidates based on, or grown from, currently attested segments. This process is similar to the deduction structures of <ref type="bibr" coords="3,452.18,608.51,56.47,8.74">Harris (1955)</ref>, which itself inspired Dejean (1998). Their system, like ours, functions on character succession options following a given segment that partially enumerates a word or words in a language. If the range of valid alphabetic options extending the sequence is above a certain threshold, it may signal a morpheme boundary. In our version, we admit extensions based both upon whether a ratio-metric is passed and upon whether the sequence has initiated a new branch in the trie. The latter method, in particular, allows supersets of unsuccessful subsequences to be retained. For example, the sequence me grown from the base affix nt may fail to meet the ratio specifications, but its follow-on sequence eme might not. The algorithm now separates good from bad affix candidates by utilizing two parallel vistas: the perspective of individual roots and the affix sets each takes, and the converse examination of individual affixes and the root sets each assumes. In this, it determines patterns of diverse reliability though co-occurrence. Specifically, the program assumes that valid roots take a partially overlapping set of affixes in common, and it creates two sets based upon this choice. It then utilizes set membership in conjunction with thresholding caveats to further extend validity assessments, such that, if there's evidence that affix α is a true affix in a language, and affix β occurs on a subset of roots that α does, then it takes steps to consider β a true affix as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Clean-Up Procedures</head><p>Before examining distribution patterns across the present set of roots and affixes, we potentially manipulate each slightly, either expanding them or over-writing them. First, many seed affix candidates resisted combination with other early induced units and are therefore somewhat short. While many of these may be correct, the pool is still largely incomplete. We expand the affix collection, then, to increase the overall membership and enhance the prospect of greater overlap in clustering, by taking a Maximum Likelihood Estimate (MLE) of root termination (for suffixes) or initiation (for prefixes) patterns for each root. Here, we consider up to the first or last three character and retain those extensions scoring above a certain threshold. Secondly, our code looks for sequences of contiguous or non-contiguous reduplication between root and affix. This type of linguistic phenomenon must be addressed in our case as it evokes the same trepidation involving the hypothetical training pair stars and started. In this case, the training pair may be shops and shopping, where the root taking a both a valid and an invalid affix supports both guesses. In these cases, the algorithm changes the affix's repeated sequences to the cover symbol RR, for reduplication, or R, point of change gemination, if it can find other instances to apply the same revisions. If both planning and occurring exist in the training file, for instance, then the program creates an affix class of Ring. For the Tagalog pairs binabasa and sinasabi, the program creates the affix class RinR and assigns it the roots basa and sabi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Feature Set Generation in Preparation for Binary Clusterization</head><p>We now develop feature sets based upon patterns of root co-occurrence. The backbone of this process is therefore root productivity, and it was selected for simple predictability. Valid roots in a language take a wider set of affixes in common than invalid roots, and, because of this, the expected outcome, largely borne out in test files, is that feature sets of valid affixes will show greater membership and a higher degree of content overlap. To produce feature sets that mimic this theory, we co-opt the notion that roots can take multiple affixes and enact that fact into a chain-based agenda. Specifically, for each base affix, we examine affixes with which it shares a common root and use these to potentially populate its feature set. We call these degree-1 affixes. Not content to leave a feature set at that, seeking instead to broaden internal similarity across sets, we further examine the root distribution patterns of the degree-1 affix. We now potentially add to the same set any affix that shares a root in common with a degree-1 candidate. Call these affixes degree-2 affixes.</p><p>For this desired result to be reliably wrought, thresholds must be in place to block incidental feature chaining. For instance, if started and stars both exist in the training file, then ted|0 is a potential degree-1 affix for base affix s|0, despite the later being valid and the former being less so. We therefore resort to measuring the incidence strength between a base and a degree-1 affix by tallying their root connectivity. We call this a primary threshold. In measuring the relatability between a base affix and a degree-2 affix, on the other hand, we examine the cardinality of the degree-1 population that lead us to the association. We must see a certain number of affixes that link s|0 and ting|0 beyond simply ted|0. We call this threshold a secondary threshold.</p><p>The exact values of both the primary and secondary thresholds depend on both the number of types in the test file and which application of feature set generation we find ourselves on. The feature chaining content is in fact run three times with three separate combinations of primary and secondary thresholds, with goal of each iteration to increase the size of a given feature set. The first run includes primary and secondary thresholds that are based on type count; if there are less than 1000 types in the corpus, both thresholds are 2, otherwise, they're .0015 times the number of types. The second application includes the same primary threshold, but a no secondary threshold. The second condition was voided under the assumption that primary thresholds serve as a gateway, and, if they're good enough, they should allow enough trusted content through to negate the use of further safeguards. Unfortunately, for many affixes, both valid and spurious, the primary threshold may be too high to allow most content to pass and many feature sets are left with only the base affix itself and potentially one other member. The third iteration targets only those affixes and instead dictates a primary threshold reduced by half, to be a minimum of 2, and no secondary threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Binary Clusterization</head><p>We next engage in binary clusterization, using affix-populated feature-sets to define a group of valid affixes against a group of, potentially, spurious choices. Grouping as such punctuates each of the three applications of feature set generation. We define therefore sets, then seek to define groups based upon those sets. Furthermore, we carry over results from previous groupings to subsequent applications; the first clustering iteration yields two clusters, while the second two rounds, making use of increasingly expanded feature sets, grows the content of each. During this process, we also avoid repetition of competent categorization. Affixes admitted to a group during one round of categorization are invisible to subsequent feature-set generation and grouping exercises.</p><p>Seeding both groups of affixes amounts to an O(n 2 ) activity. We probe all possible pairs of feature sets for non-normalized, incomplete content overlap, ignoring feature sets of two items or less, which generally belong to spurious bases. <ref type="foot" coords="5,305.54,717.07,3.97,6.12" target="#foot_0">1</ref> . The pair of sets showing the highest overlap Based Affix Feature Set ed|0 ed|0 ers|0 ive|0 Re|1 al|0 ory|0 es|0 ion|0 eR|0 able|0 ions|0 ing|0 e|0 s|0 ation|0 ing|0 er|0 ed|0 ive|0 ers|0 Re|1 ory|0 ion|0 es|0 eR|0 able|0 ing|0 e|0 s|0 e|0 ed|0 Re|1 al|0 es|0 eR|0 ion|0 ely|0 e|0 ing|0 s|0 ation|0 t|0 ts|0 ce|0 ting|0 ted|0 tR|0 tion|0 t|0 s|0 ed|0 ers|0 ing|0 Ring|0 s|0 d|0 tion|0 ting|0 ted|0 te|0 t|0 tion|0 tions|0 ers|0 ed|0 er|0 ers|0 ing|0 s|0 ting|0 ts|0 ted|0 ting|0 tion|0 t|0 ation|0 ated|0 ed|0 ations|0 e|0 ation|0 ted|0 ting|0 ted|0 tion|0 t|0 tions|0 d|0 ding|0 rs|0 s|0 ds|0 d|0 ion|0 ed|0 ion|0 ions|0 ing|0 e|0 eR|0 ed|0 eR|0 ing|0 e|0 Re|1 nt|0 nt|0 nce|0 nts|0 ntly|0 on|0 ve|0 ons|0 on|0 ng|0 st|0 st|0 sts|0 With these two groups in hand, we then begin the process of expanding each through a comparison with each of the remaining unclassified feature sets. We again hunt for content overlap, here, however, normalizing it by the category's total membership. If a feature set's similarity score is greater than .8, it is immediately absorbed; if not, it is set aside until all feature sets have been assessed for classification candidacy. At the end of each such evaluation cycle, we absorb only the highest scoring unclassified feature set as measured against each group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Assessment of Membership of Cluster</head><p>Next, we massage the membership of the now-filled valid group of affixes, growing it or shrinking it based on degrees of co-occurrence relationships. The idea here is to correct threshold errors pertaining to feature set generation.</p><p>To gather such evidence, we examine each affix candidate's productivity, assessing it in terms of both select roots and related affixes. That is, for every affix still in consideration, we tally both the number of co-occurrence associates numbering among the members of the valid affix group and their associated roots. In this, we see how patterns of affixes recommend each other and how fervently they do so. There are two caveats to such an aim. First, we wish to impair invalid affixes from iteratively supporting further invalid affixes. Therefore, in course of tally, we verify that at least one associate belongs to a specially reduced subset of the valid affix group. This portion of members are those classified during the first iteration of the clustering module. The second caveat is one in which we discount any associate that's a sequential subset of a longer group member that could also apply to the shared root. For example, if d|0 is up for consideration, and its co-occurrence associate is s|0 via the shared root stage, we refrain from increasing d|0's tally of supporting evidential affixes because es|0 could also characterize a truncated version of the same root candidate.</p><p>With this number in hand, establish two separate sets of criteria, one for members of the valid affix group whose current membership we're scrutinzing, another out-of-group membership seeking admittance.</p><p>1. For member of the valid affix group, retain the affix if:</p><p>(a) The number of roots it shares with co-occurrence associates from the valid group exceeds 8% of all the roots it takes (b) The affix splits at least 1 root between itself and co-occurrence associates from the valid group AND any of the following (c) The number of roots it shares with affix associates from the valid group are are at least of a certain threshold, set to be 2 for files numbering less than 4000 types and .0005% of the types for files larger OR (d) Its co-occurrence associates from the reliable subset of the valid group number greater than 33% of its co-occurrence associates from the entire valid group OR (e) The number of co-occurrence associates from the reliable subset of the valid group are at least 75% of the total number of affixes composing the reliable subset of the valid group OR (f) Its number of shared roots are 75% of the total number of roots it takes 2. For a member outside the valid affix group, admit the affix if:</p><p>(a) The number of its roots demonstrating co-occurrence associates among the valid group is 20% of all the roots it takes AND (b) Its co-occurrence associates from the reliable subset of the valid group number greater than 33% of its co-occurrence associates from the entire valid group OR (c) Its number of shared roots are 75% of the total number of roots it takes</p><p>To strike an appropriate and respectable F-score, we employ one last measure involving previously described modules. We harvest the affixes having survived filtering and pass them as arguments to the affix-chaining subroutine. In doing so, we seek to assess connections or recommendations yielded by what we assume to be a reliable set of affix candidates. Such a strategy marks a return to the hypothesis informing this algorithmic production: valid roots are more productive than invalid roots and the affixes they take cross paths in their distribution. Based upon such conjecture, we therefore expect to see certain repetitions across these new feature-sets/affix chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Stemming</head><p>Once we have a list of presumed affixes in a language, or, at least, a sample thereof, we use it to truncate presumably inflected text. The stemming procedure we employ to this end is quite simple, and is one in which we strip the longest lexical sequence matching an affix. Therefore, for example, if our inducer surmised both s|0 and es|0 were inflections in English, and the stemmer were presented with churches, it would then produce church rather than churche.</p><p>Furthermore, for languages that have the possibility of taking circumfixes, we first stem applicable suffixes, take the resulting foreshortened lexical items, and pass them to the same procedures to evaluate for the presence of prefixes. If a lexical item is found to not have an applicable suffix, the word is transfered as-is to the prefix-stemming component.</p><p>Additionally, as a matter of typological practicality, we furthermore place an order of importance on infixation versus traditional word-boundary affixation. As fewer languages demonstrate infixation, we keep a stemmed analysis showing the phenomenon only if the same word does not allow for a match to a traditional affix. Therefore, for example, if both in|0 and ing|0 appeared on the list of induced affixes to be stemmed in English, then we would retain talk + ing|0 over talkg + in|1. The simplicity of the procedure does yield problems, however. The stemmer has no way of knowing when not to stem candidates that are in fact false matches. For example, if s|0 is on the final suffix list for English, and discuss is an incoming word, then the former will incorrectly truncate the latter to discus, even though the word as passed was itself a complete root. To guard against some mistakes of this ilk, we have instituted as rule that no resulting root can be less than 3 characters. While this does not ameliorate the case of discuss, it does amend possibly otherwise frequent missteps. In English, for example, the means guarantees that is not reduced to the single character i.</p><p>Lastly, our stemmer does not yet handle reduplication in the way that our inducer does. In this case, the former program cannot precisely identify purely reduplicated material without the aid of the latter's detection module. Solving this problem as indicated would increase both precision and recall but could severely restrict run-time lexical truncation. This does, however, remain a point for future investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Some Options</head><p>For each of the three methods, A, B and C, we also introduced three further parameters. The first was removing infixing from consideration; the second was eliminating the MLE-based extension of affix candidates; and the third was allowing suffix training runs to influence prefix selection, which we have termed co-information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>File Name</head><p>Type </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">The Downside of Productivity</head><p>One may notice the clear disparity in results on our home-grown, small files of English newswire and those provided by Morpho Challenge. We attribute this divergence in file size as coupled with poor program execution. The program uses both nested loops to generate feature sets as well as string comparison functions to evaluate the content overlap of those sets. This construction caused no concern on the small files that we were experimenting with, and training could be accomplished in minutes. However, these such alogrithmic features don't lend themselves to linear training time, and we therefore encountered problems when attempting to punctually absorb patterns from even files as large as 200,000 words.</p><p>To amend the circumstances, we decided not only to train on a fraction of the data, but to also subdivide that fraction and treat each resulting sub-set of data as an entirely distinct training pass.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,482.94,738.55,30.06,8.74"><head>Table 1 :</head><label>1</label><figDesc>All Possible Internal Partitions of tiger</figDesc><table coords="2,482.94,738.55,30.06,8.74"><row><cell>Such a</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,194.86,258.90,213.29,8.74"><head>Table 2 :</head><label>2</label><figDesc>All Retained Internal Partitions of tiger</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,90.00,110.82,394.38,307.96"><head>Table 3 :</head><label>3</label><figDesc>Final List of English Positioned Suffix Segments and Their Relative Scores</figDesc><table coords="4,126.90,110.82,349.19,243.46"><row><cell cols="9">Rank Segment Score Rank Segment Score Rank Segment Score</cell></row><row><cell>0.</cell><cell>e|0</cell><cell>0.35</cell><cell>1.</cell><cell>s|0</cell><cell>0.31</cell><cell>2.</cell><cell>e|1</cell><cell>0.23</cell></row><row><cell>3.</cell><cell>d|0</cell><cell>0.22</cell><cell>4.</cell><cell>i|1</cell><cell>0.20</cell><cell>5.</cell><cell>a|1</cell><cell>0.20</cell></row><row><cell>6.</cell><cell>n|1</cell><cell>0.20</cell><cell>7.</cell><cell>y|0</cell><cell>0.20</cell><cell>8.</cell><cell>t|0</cell><cell>0.19</cell></row><row><cell>9.</cell><cell>n|0</cell><cell>0.19</cell><cell>10.</cell><cell>g|0</cell><cell>0.18</cell><cell>0.</cell><cell>ed|0</cell><cell>1</cell></row><row><cell>1.</cell><cell>ng|0</cell><cell>0.96</cell><cell>2.</cell><cell>ing|0</cell><cell>0.85</cell><cell>3.</cell><cell>er|0</cell><cell>.52</cell></row><row><cell>4.</cell><cell>es|0</cell><cell>0.50</cell><cell>5.</cell><cell>on|0</cell><cell>0.49</cell><cell>6.</cell><cell>in|1</cell><cell>0.49</cell></row><row><cell>7.</cell><cell>ion|0</cell><cell>0.36</cell><cell>8.</cell><cell>nt|0</cell><cell>0.36</cell><cell>9.</cell><cell>al|0</cell><cell>0.35</cell></row><row><cell>10.</cell><cell>ti|1</cell><cell>0.26</cell><cell>11.</cell><cell>re|0</cell><cell>0.25</cell><cell>12.</cell><cell>re|2</cell><cell>0.25</cell></row><row><cell>13.</cell><cell>re|1</cell><cell>0.25</cell><cell>14.</cell><cell>le|0</cell><cell>0.24</cell><cell>15.</cell><cell>ry|0</cell><cell>0.24</cell></row><row><cell>16.</cell><cell>en|0</cell><cell>0.24</cell><cell>17.</cell><cell>ly|0</cell><cell>0.23</cell><cell>18.</cell><cell>ce|0</cell><cell>0.23</cell></row><row><cell>19.</cell><cell>te|0</cell><cell>0.23</cell><cell>20.</cell><cell>ent|0</cell><cell>0.23</cell><cell>21.</cell><cell>er|1</cell><cell>0.22</cell></row><row><cell>22.</cell><cell>an|1</cell><cell>0.21</cell><cell>23.</cell><cell>ar|1</cell><cell>0.20</cell><cell>24.</cell><cell>te|1</cell><cell>0.19</cell></row><row><cell>25.</cell><cell>ca|1</cell><cell>0.19</cell><cell>26.</cell><cell>ll|0</cell><cell>0.19</cell><cell>27.</cell><cell>en|1</cell><cell>0.19</cell></row><row><cell>28.</cell><cell>ca|2</cell><cell>0.19</cell><cell>29.</cell><cell>on|1</cell><cell>0.19</cell><cell>30.</cell><cell>ve|0</cell><cell>0.18</cell></row><row><cell>31.</cell><cell>ty|0</cell><cell>0.18</cell><cell>32.</cell><cell>th|1</cell><cell>0.16</cell><cell>33.</cell><cell>ti|2</cell><cell>0.16</cell></row><row><cell>34.</cell><cell>it|1</cell><cell>0.16</cell><cell>35.</cell><cell>ts|0</cell><cell>0.16</cell><cell>36.</cell><cell>st|0</cell><cell>0.15</cell></row><row><cell>37.</cell><cell>st|2</cell><cell>0.15</cell><cell>38.</cell><cell>co|2</cell><cell>0.15</cell><cell>39.</cell><cell>se|1</cell><cell>0.14</cell></row><row><cell>40.</cell><cell>li|1</cell><cell>0.14</cell><cell>41.</cell><cell>ou|1</cell><cell>0.13</cell><cell>42.</cell><cell>he|1</cell><cell>0.13</cell></row><row><cell>43.</cell><cell>in|2</cell><cell>0.13</cell><cell>44.</cell><cell>al|1</cell><cell>0.13</cell><cell></cell><cell></cell><cell></cell></row></table><note coords="4,90.00,406.16,8.06,12.62;4,114.20,406.16,249.32,12.62"><p>5 Using Root and Affix Distributions</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,90.00,340.31,423.00,88.38"><head>Table 4 :</head><label>4</label><figDesc>Sample of First Round of Affix Chaining During the Last Training Iteration of an 39,000 Word English File initiates the valid group of affixes. The group of invalid affixes initiates as the feature set pair showing the highest internal similarity without demonstrating any coordination with what has become the valid feature set group. That is, at such a point in time, the valid and invalid affix groups are completely distinct in content.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,186.02,502.79,230.96,79.62"><head>Table 5 :</head><label>5</label><figDesc>Small English File Names and Type Counts</figDesc><table coords="8,208.50,502.79,186.00,47.79"><row><cell></cell><cell cols="2">Count Token Count</cell></row><row><cell>English2.txt</cell><cell>1,359</cell><cell>4,222</cell></row><row><cell>English4.txt</cell><cell>4,783</cell><cell>39,406</cell></row><row><cell>English5.txt</cell><cell>3,872</cell><cell>17,067</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,105.24,737.58,293.02,7.21"><p>By incomplete, we mean we do not consider two feature sets that are identical</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Doing so, we believed, would prevent a burdensome amount of roots and affixes from accruing and therefore limit the cycles of both loops and string-compare functions. Such an assumption was likely born out, as this step managed run-time quite well. It did, however, damage recall more than we expected. The drop in scores, we believe was the exclusive result of data segmentation more so than incomplete examination, as recall remained relatively constant between runs examining 5% of the data to those that examined 6 times as much. Data segmentation is a reliable culprit, in fact, because it ultimately undercut the linguistic productivity represented across the whole corpus. A critical amount of affix connectivity crucial for the development of robust feature set was simply wiped out, in that each run was not informed with the root results of previously processed sub-files. Therefore, a fair amount of valid affixes were not included in the final set of induced sequences given that their distribution was small in scope relative to the training file they found themselves in. We are currently experimenting with methods to better and more efficiently train on larger files. For one, we are attempting to develop affix feature sets contemporaneously during the triebased affix and root recovery. This measure involves dual use of both forwards and backwards tries, as, once a root sequence is recovered, it is placed in a reverse trie to guide the recovery of its associated (degree-one) affixes. This process therefore supplants reliance on for-loops nested up to between 3-5 iterative depths. Second, we are weening the code off a preponderance of stringcompare functions. We are drafting a new categorization module that stores both the valid and invalid affix groups in separate tries; it therefore tabulates content overlap between each cluster and a candidate feature set by using depth-first search to result in an end-of-affix marker. We are alternately drafting a technique to encode each affix as a number, as comparing integers is quicker than comparing strings. Finally, we are reducing the value of n when attempting to assay the depth feature-set overlap across all pairs. Figuring that sets closer in total membership cardinality show more non-normalized overlap than those sets which are diverse in size, we only compare sets that are the most populous. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,105.50,372.72,407.50,8.74;10,105.50,384.68,407.50,8.74;10,105.50,396.63,142.38,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,193.21,372.72,319.79,8.74;10,105.50,384.68,121.88,8.74">Unsupervised morphological segmentation based on segment predictability and word segment alignment</title>
		<author>
			<persName coords=""><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,247.26,384.68,265.74,8.74;10,105.50,396.63,111.11,8.74">PASCAL Challenge Workshop on Unsupervised Segmentation of Words into Morphemes</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,416.56,407.49,8.74;10,105.50,428.51,370.65,8.74;10,476.15,426.94,7.68,6.12;10,487.90,428.51,25.11,8.74;10,105.50,440.47,202.36,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,391.73,416.56,121.26,8.74;10,105.50,428.51,262.76,8.74">Discovering morphemic suffixes: A case study in minimum description length induction</title>
		<author>
			<persName coords=""><forename type="first">Micheal</forename><forename type="middle">R</forename><surname>Brent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sreerama</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Lundberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,389.55,428.51,86.60,8.74;10,476.15,426.94,7.68,6.12;10,487.90,428.51,25.11,8.74;10,105.50,440.47,172.51,8.74">Proceedings of the 5 th International Workshop on AI and Statistics</title>
		<meeting>the 5 th International Workshop on AI and Statistics</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,457.88,407.50,11.26;10,105.50,472.35,407.50,8.74;10,105.50,484.30,407.51,8.74;10,105.50,496.26,126.95,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,105.50,472.35,362.12,8.74">Aligment based induction of morphology grammar and its role for bootstrapping</title>
		<author>
			<persName coords=""><forename type="first">Damir</forename><surname>Ćavar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><surname>Herring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Toshikazu</forename><surname>Ikuta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giancarlo</forename><surname>Schrementi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,418.63,484.30,94.38,8.74;10,105.50,496.26,42.08,8.74">Proceedings of Formal Grammar</title>
		<editor>
			<persName><forename type="first">Gerhard</forename><surname>Ja Ger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Poala</forename><surname>Monachesi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shuly</forename><surname>Wintner</surname></persName>
		</editor>
		<meeting>Formal Grammar</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="47" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,516.18,407.50,8.74;10,105.50,528.14,53.79,8.74;10,159.29,526.57,7.68,6.12;10,170.87,528.14,342.13,8.74;10,105.50,540.10,125.23,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,258.69,516.18,193.26,8.74">Unsupervised word segmentation for bangla</title>
		<author>
			<persName coords=""><forename type="first">Sajib</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,477.09,516.18,35.92,8.74;10,105.50,528.14,53.79,8.74;10,159.29,526.57,7.68,6.12;10,170.87,528.14,342.13,8.74;10,105.50,540.10,89.25,8.74">Proceedings of the 5 th International Conference on Language Processing. International Conference on Language Processing</title>
		<meeting>the 5 th International Conference on Language Processing. International Conference on Language Processing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,560.02,407.50,8.74;10,105.50,571.98,407.50,8.74;10,105.50,583.93,90.83,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,168.74,560.02,339.79,8.74">Morphemes as necessary concept for structures discovery from untagged corpora</title>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Dejean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,117.03,571.98,391.45,8.74">Proceedings from the Workshop on Paradigms and Grounding in Natural Language Learning</title>
		<meeting>from the Workshop on Paradigms and Grounding in Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="295" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,603.86,407.50,8.74;10,105.50,615.81,166.12,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,195.19,603.86,274.37,8.74">Unsupervised learning of the morphology of a natural language</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">L</forename><surname>Goldsmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,478.63,603.86,34.37,8.74;10,105.50,615.81,82.27,8.74">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="153" to="198" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,635.74,314.60,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,163.99,635.74,123.24,8.74">From phoneme to morpheme</title>
		<author>
			<persName coords=""><forename type="first">Zelig</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,297.01,635.74,38.57,8.74">Language</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="190" to="222" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,655.66,407.49,8.74;10,105.50,667.62,407.50,8.74;10,105.50,679.57,5.09,8.74;10,110.59,678.00,7.68,6.12;10,122.40,679.57,390.60,8.74;10,105.50,691.53,90.83,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,404.05,655.66,108.94,8.74;10,105.50,667.62,304.82,8.74">ParaMor: Minimally supervised induction of Paradigm structure and Morphological analysis</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Monson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,433.88,667.62,79.11,8.74;10,105.50,679.57,5.09,8.74;10,110.59,678.00,7.68,6.12;10,122.40,679.57,385.91,8.74">Proceedings of the 9 th Meeting of the ACL Special Interest Group in Computational Morphology and Phonology</title>
		<meeting>the 9 th Meeting of the ACL Special Interest Group in Computational Morphology and Phonology</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="117" to="125" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
