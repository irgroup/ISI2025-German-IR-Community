<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,90.61,148.86,421.77,15.15">Retrieval Experiments at Morpho Challenge 2008</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,268.36,182.75,66.28,8.74"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
							<email>paul.mcnamee@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center of Excellence</orgName>
								<orgName type="institution">JHU Human Language Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,90.61,148.86,421.77,15.15">Retrieval Experiments at Morpho Challenge 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">15FF3382DD858CCEA2BB356DC14681A4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval Multilingual text retrieval</term>
					<term>Character n-grams</term>
					<term>Morphological normalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Morpho Challenge 2008 hosted an extrinsic evaluation of morphological analysis that explored whether unsupervised morphology induction could benefit information retrieval. This paper presents results in alternative methods for word normalization using test sets from the Cross-Language Evaluation Forum (CLEF) ad-hoc collections. Preliminary results for the Morpho Challenge 2008 evaluation are consistent with these data. We found that: (1) rule-based stemming is effective in less morphologically complicated languages; (2) alternative methods for stemming such as unsupervised learning of morphemes and least common n-gram stemming are helpful; and, (3) full character n-gram indexing is the most effective form of tokenization in more morphologically complex languages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past year we conducted experiments using alternative methods of lexical normalization. Using test sets in 13 languages used in Cross-Language Evaluation Forum (CLEF) evaluations between 2002 and 2007, we compared the use of a rule-based stemmer (Snowball ), an unsupervised morphological segmenter (Morfessor ), a synthetic form of stemming based on selecting a single character n-gram from each word, and the use of all n-grams for a given word. Character n-grams of length n = 5 were the most effective technique, performing 18% better than unnormalized words, averaged across the set of languages.</p><p>N-grams possess many advantages. They work in every language, require no training, and are more effective than plain words. Their main shortcoming is increased disk space requirements and slower query response times. To address the performance penalty we introduced a method of n-gram stemming that selects for each word its least common n-gram. By selecting only one n-gram per word, the inverted file becomes no larger than when words or stemmed words are used. And query responses times also improve since the number of query terms becomes equal to the number of words, not a function of the number of letters in the query string.</p><p>While our n-gram stems perform better than unlemmatized words, it was clear from the Morpho Challenge 2007 evaluation <ref type="bibr" coords="2,205.42,123.98,10.52,8.74" target="#b1">[2]</ref> that other techniques were even more effective. This year we wanted to compare full (i.e., traditional) n-gram indexing against the more linguistically sophisticated approaches of other participants. For the Information Retrieval subtask at Morpho Challenge 2008 we provided official submissions using three techniques in each of English, Finnish, and German. The methods were:</p><p>• Character n-grams with length n = 5</p><p>• Character n-grams with length n = 4</p><p>• The rarest 5-gram from each word (measured using document frequency statistics)</p><p>In Section 2 we describe our tokenization experiments on the CLEF data sets. In Section 3 we analyze our Morpho Challenge 2008 results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Analysis on Past CLEF Collections</head><p>We compared plain words, stems, induced morphemes, n-gram stems, and character n-grams using test sets from the CLEF ad hoc tasks between 2002 and 2007. In each of the 13 languages we used two years worth of data except for Czech where only one year was available. The number of test queries per language varied from 50 (Czech) to 107 (Spanish). The document collections for each language were indexed using the various methods of tokenization. Common to each method was conversion to lower case letters, removal of punctuation, and truncation of long numbers to 6 digits. The HAIRCUT retrieval engine was used with a statistical language model retrieval metric. The similarity calculation combines document term frequencies and corpus frequencies (for smoothing) using linear interpolation with a smoothing constant of 0.5 <ref type="bibr" coords="2,425.03,413.86,9.96,8.74" target="#b5">[6]</ref>. The methods of tokenization examined were:</p><p>• words: space-delimited tokens.</p><p>• snow: output of the Snowball stemmer<ref type="foot" coords="2,287.19,464.09,3.97,6.12" target="#foot_0">1</ref> , if available in the given language.</p><p>• morf : the set of morphemes produced by Morfessor<ref type="foot" coords="2,350.33,484.02,3.97,6.12" target="#foot_1">2</ref> for each input word. A model was trained using the document collection's lexicon with digit-containing tokens omitted. The default parameters for the Morfessor algorithm were used <ref type="bibr" coords="2,369.09,509.50,9.96,8.74" target="#b0">[1]</ref>.</p><p>• lcn4: the least common word-internal character 4-gram for each input word <ref type="bibr" coords="2,450.46,529.43,9.96,8.74" target="#b2">[3]</ref>.</p><p>• lcn5: the least common word-internal character 5-gram for each input word.</p><p>• 4-gram: overlapping, word-spanning character 4-grams produced from the stream of words encountered in the document or query.</p><p>• 5-gram: length n = 5 n-grams created in the same fashion as the character 4-grams.</p><p>In Table <ref type="table" coords="2,129.37,621.08,4.98,8.74" target="#tab_0">1</ref> results are presented using mean average precision to compare performance. The score for the highest performing technique in each language is emboldened.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Unnormalized words</head><p>Not attempting to control for morphological processes can have harmful effects. In Bulgarian, Czech, Finnish, and Hungarian, more than a 30% loss is observed compared to the use of 4-grams as indexing terms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Snowball stemming</head><p>Snowball does not support Bulgarian, Czech, or Russian and due to character encoding issues with the software we were not able to use it for Portuguese and Hungarian. In Table <ref type="table" coords="3,450.39,557.70,4.98,8.74" target="#tab_0">1</ref> performance for each technique is given averaged over eight remaining languages. Stemming, when available, is quite effective, and just slightly below the top-ranked approach of character n-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Morfessor Segments</head><p>As it may be difficult to find a rule-based stemmer for every language, a language-independent approach can be quite attractive. The Morfessor algorithm only requires a lexicon (i.e., wordlist) for a language to learn to identify morpheme boundaries, even for previously unseen words. Such automatically detected segments can be an effective form of tokenization <ref type="bibr" coords="3,420.07,663.75,9.96,8.74" target="#b4">[5]</ref>. Examples of the algorithm's output are presented in Table <ref type="table" coords="3,282.28,675.70,3.87,8.74">2</ref>, along with results for Snowball and character 5grams.</p><p>Compared to plain words the induced morphemes produced by Morfessor led to gains in 9 of 13 languages; 8 of these were significant improvements with p &lt; 0.05 (Wilcoxon test). The languages where words outperformed segments were English (dramatically), French, Italian, and Spanisheach is relatively low in morphological complexity. The differences in French and Spanish were </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Least Common N-gram Stems</head><p>Another language-neutral approach to stemming is to select for each word, its least common ngram. This requires advance knowledge of n-gram frequencies, but this is easily obtainable by constructing a regular n-gram index, or even by scanning a corpus and counting. Lengths of n = 4 and n = 5 appear about equally effective with a slight advantage for lcn4, but this is influenced primarily by the languages with greater morphological complexity, which see larger changes. An 8% relative improvement in mean average precision over words is obtained. As can be seen from Table <ref type="table" coords="4,178.50,370.59,3.87,8.74" target="#tab_0">1</ref>, in languages where rule-based stemming is available its use is preferable. N-gram stemming achieves comparable performance with Morfessor segments..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Overlapping Character N-grams</head><p>N-grams achieve morphological regularization indirectly due to the fact that subsequences that touch on word roots will match. For example, "juggling" and "juggler" will share the 5-grams jugg and juggl. While n-gram's redundancy enables useful matches, other matches are less valuable, for example, every word ending in 'tion' will share 5-gram tion with all of the others. In practice these morphological false alarms are almost completely discounted because term weighting deemphasizes them. In fact, such affixes can be so common, that ignoring them entirely by treating them as "stop n-grams" is a reasonable thing to do.</p><p>Character n-grams are the most effective technique studied here, giving a relative improvement of 18%. Consistent with earlier work <ref type="bibr" coords="4,252.77,524.46,10.52,8.74" target="#b3">[4]</ref> lengths of n = 4 and n = 5 are equally effective averaged across the 13 languages; however there are some noticeable differences in particular languages. The data is suggestive of a trend that the most morphologically variable languages (i.e., Bulgarian, Czech, Hungarian, and Russian) gain more from 4-grams than 5-grams, while 5-grams have a slight advantage in medium complexity languages.</p><p>Snowball stems are roughly as effective as n-grams, on average, but only available in certain languages (i.e., 8 of 13 in this study). The other "alternative" stemming approaches, segments and least common n-grams, appear to gain about half of the benefit that full n-gram indexing sees compared to unnormalized word forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Morpho Challenge 2008 Submissions</head><p>This year's evaluation was conducted in English, Finnish, and German, as was the case in Morpho Challenge 2007. The contest was conducted by having participants submit an analysis for each word form, which would be used to create replacement indexing terms in an IR system. According to the contest website the LEMUR IR engine was used with Okapi BM 25 term weighting. Preliminary results reported by the organizers are presented in Table <ref type="table" coords="4,418.88,722.69,4.98,8.74" target="#tab_1">3</ref> using mean average precision.</p><p>An examination of the preliminary results shows the same basic trend identified in Section 2. Rule-based stemming is most effective in English, and in a morphologically rich language such as Finnish, n-grams have a strong advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We examined a variety of methods for lexical normalization, finding that the most effective technique was character n-gram indexing which achieved a relative gain of 18% in mean average precision over unlemmatized words. In Czech, Bulgarian, Finnish, and Hungarian gains of over 40% were observed. While rule-based stemming can be quite effective, such tools are not available in every language and even when present, require additional work to integrate with an IR system. When language-neutral methods are able to achieve the same, or better performance, their use should be seriously considered.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,95.98,119.49,417.93,382.72"><head>Table 1 :</head><label>1</label><figDesc>Comparison of 7 Tokenization Alternatives (Mean Average Precision)</figDesc><table coords="3,95.98,131.75,417.93,370.47"><row><cell>Language</cell><cell cols="3">Data Queries Words</cell><cell>Snow</cell><cell>Morf</cell><cell>LCN4 LCN5 4-gram 5-gram</cell></row><row><cell cols="2">Bulgarian 06-07</cell><cell>100</cell><cell>0.2195</cell><cell></cell><cell>0.2786 0.2937 0.2547 0.3163 0.2916</cell></row><row><cell>Czech</cell><cell>07</cell><cell>50</cell><cell>0.2270</cell><cell></cell><cell>0.3215 0.2567 0.2477 0.3294 0.3223</cell></row><row><cell>Dutch</cell><cell>02-03</cell><cell>106</cell><cell cols="3">0.4162 0.4273 0.4274 0.4021 0.4073 0.4378 0.4443</cell></row><row><cell>English</cell><cell>02-03</cell><cell>96</cell><cell cols="3">0.4829 0.5008 0.4265 0.4759 0.4861 0.4411</cell><cell>0.4612</cell></row><row><cell>Finnish</cell><cell>02-03</cell><cell>75</cell><cell cols="3">0.3191 0.4173 0.3846 0.3970 0.3900 0.4827 0.4960</cell></row><row><cell>French</cell><cell>02-03</cell><cell>102</cell><cell cols="3">0.4267 0.4558 0.4231 0.4392 0.4355 0.4442</cell><cell>0.4399</cell></row><row><cell>German</cell><cell>02-03</cell><cell>106</cell><cell cols="3">0.3489 0.3842 0.4122 0.3613 0.3656 0.4281 0.4321</cell></row><row><cell cols="2">Hungarian 06-07</cell><cell>98</cell><cell>0.1979</cell><cell></cell><cell>0.2932 0.2784 0.2704 0.3549 0.3438</cell></row><row><cell>Italy</cell><cell>02-03</cell><cell>100</cell><cell cols="3">0.3950 0.4350 0.3770 0.4127 0.4054 0.3925</cell><cell>0.4220</cell></row><row><cell cols="2">Portuguese 05-06</cell><cell>100</cell><cell>0.3232</cell><cell></cell><cell>0.3403 0.3442 0.3381 0.3316 0.3515</cell></row><row><cell>Russian</cell><cell>03-04</cell><cell>62</cell><cell>0.2671</cell><cell></cell><cell>0.3307 0.2875 0.3053 0.3406 0.3330</cell></row><row><cell>Spanish</cell><cell>02-03</cell><cell>107</cell><cell cols="3">0.4265 0.4671 0.4230 0.4260 0.4323 0.4465</cell><cell>0.4376</cell></row><row><cell>Swedish</cell><cell>02-03</cell><cell>102</cell><cell cols="3">0.3387 0.3756 0.3738 0.3638 0.3467 0.4236 0.4271</cell></row><row><cell>Average</cell><cell></cell><cell></cell><cell>0.3375</cell><cell></cell><cell>0.3698 0.3645 0.3604 0.3955 0.3979</cell></row><row><cell cols="3">Average (8 Snowball langs)</cell><cell cols="3">0.3504 0.3848 0.3608 0.3642 0.3632 0.3885 0.3956</cell></row><row><cell></cell><cell></cell><cell cols="4">Table 2: Word Normalization Examples</cell></row><row><cell>Word</cell><cell>Snowball</cell><cell></cell><cell>Morfessor</cell><cell>5-grams</cell></row><row><cell>authored</cell><cell>author</cell><cell></cell><cell>author+ed</cell><cell cols="2">auth, autho, uthor, thore, hored, ored</cell></row><row><cell>authorized</cell><cell>author</cell><cell cols="2">author+ized</cell><cell cols="2">auth, autho, uthor, thori, horiz, orize, rized, ized</cell></row><row><cell cols="4">authorship authorship author+ship</cell><cell cols="2">auth, autho, uthor, thors, horsh, orshi, rship, ship</cell></row><row><cell>afoot</cell><cell>afoot</cell><cell></cell><cell>a+foot</cell><cell cols="2">afoo, afoot, foot</cell></row><row><cell>footballs</cell><cell>footbal</cell><cell></cell><cell>football+s</cell><cell cols="2">foot, footb, ootba, otbal, tbaall, balls, alls</cell></row><row><cell>footloose</cell><cell>footloos</cell><cell></cell><cell>foot+loose</cell><cell cols="2">foot, footl, ootlo, otloo, tloos, loose, oose</cell></row><row><cell>footprint</cell><cell>footprint</cell><cell></cell><cell>foot+print</cell><cell cols="2">foot, footp, ootpr, otpri, tprin, print, rint</cell></row><row><cell>feet</cell><cell>feet</cell><cell></cell><cell>feet</cell><cell cols="2">feet, feet</cell></row><row><cell>juggle</cell><cell>juggl</cell><cell></cell><cell>juggle</cell><cell cols="2">jugg, juggl, uggle, ggle</cell></row><row><cell>juggled</cell><cell>juggl</cell><cell></cell><cell>juggle+d</cell><cell cols="2">jugg, juggl, uggle, ggled, gled</cell></row><row><cell>jugglers</cell><cell>juggler</cell><cell></cell><cell>juggle+r+s</cell><cell cols="2">jugg, juggl, uggle, ggler, glers, lers</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,118.94,423.00,142.38"><head>Table 3 :</head><label>3</label><figDesc>Preliminary Results for Morpho Challenge 2008</figDesc><table coords="4,90.00,130.64,423.00,130.68"><row><cell></cell><cell cols="3">English Finnish German</cell></row><row><cell cols="2">Plain words 0.3293</cell><cell>0.3519</cell><cell>0.3509</cell></row><row><cell>Snowball</cell><cell cols="2">0.4081 0.4275</cell><cell>0.3865</cell></row><row><cell>Morfessor</cell><cell>0.3861</cell><cell>0.4425</cell><cell>0.4656</cell></row><row><cell>LCN 5</cell><cell>0.3563</cell><cell>0.3688</cell><cell>0.3276</cell></row><row><cell>4-grams</cell><cell cols="2">0.3566 0.4918</cell><cell>0.4388</cell></row><row><cell>5-grams</cell><cell>0.3630</cell><cell>0.4515</cell><cell>0.4331</cell></row><row><cell cols="4">less than 0.004 in absolute terms. Segments achieved more than a 20% relative improvement in</cell></row><row><cell cols="4">Bulgarian, Finnish, and Russian, and over 40% in Czech and Hungarian.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,722.16,165.95,6.99"><p>Available from http://snowball.tartarus.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,731.66,206.17,6.99"><p>Available from http://www.cis.hut.fi/projects/morpho/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,105.50,317.19,407.50,8.74;5,105.50,329.15,407.50,8.74;5,105.50,341.11,76.93,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="5,266.88,317.19,246.11,8.74;5,105.50,329.15,209.27,8.74">Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1</title>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Helsinki University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="5,105.50,361.03,407.50,8.74;5,105.50,372.99,407.50,8.74;5,105.50,384.94,48.13,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,336.04,361.03,176.95,8.74">Overview of Morpho Challenge in CLEF</title>
		<author>
			<persName coords=""><forename type="first">Mikko</forename><surname>Kurimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ville</forename><surname>Turunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,336.34,372.99,176.66,8.74;5,105.50,384.94,17.69,8.74">Working Notes of the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">Alessandro</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,404.87,407.50,8.74;5,105.50,416.82,22.69,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,264.72,404.87,102.03,8.74">Single n-gram stemming</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,386.56,404.87,25.85,8.74">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="415" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,436.75,407.51,8.74;5,105.50,448.70,246.62,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,273.91,436.75,239.10,8.74;5,105.50,448.70,55.43,8.74">Character N-gram tokenization for european language text retrieval</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,169.19,448.70,92.98,8.74">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="73" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,468.63,407.50,8.74;5,105.50,480.58,357.87,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,378.41,468.63,134.59,8.74;5,105.50,480.58,67.35,8.74">Don&apos;t have a stemmer?: Be un+concern+ed</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,194.90,480.58,43.83,8.74">SIGIR &apos;08</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="813" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,500.51,407.50,8.74;5,105.50,512.46,154.79,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,259.34,500.51,236.84,8.74">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jay</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,105.50,512.46,25.85,8.74">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
