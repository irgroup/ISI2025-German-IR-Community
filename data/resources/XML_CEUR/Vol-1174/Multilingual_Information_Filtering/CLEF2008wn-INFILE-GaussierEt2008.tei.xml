<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.52,115.82,292.45,12.93;1,152.88,133.82,309.48,12.93;1,275.16,151.70,65.05,12.93">Working Notes for the InFile Campaign : Online Document Filtering Using 1 Nearest Neighbor</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,183.00,189.19,56.87,9.96"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
							<email>eric.gaussier@imag.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire d&apos;Informatique de Grenoble (LIG) Bâtiment IMAG B - Bibliothèque</orgName>
								<orgName type="institution">d&apos;Hères</orgName>
								<address>
									<addrLine>385 avenue</addrLine>
									<postCode>38400</postCode>
									<settlement>Saint Martin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.57,189.19,82.50,9.96"><forename type="first">Ali</forename><forename type="middle">Mustafa</forename><surname>Qamar</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire d&apos;Informatique de Grenoble (LIG) Bâtiment IMAG B - Bibliothèque</orgName>
								<orgName type="institution">d&apos;Hères</orgName>
								<address>
									<addrLine>385 avenue</addrLine>
									<postCode>38400</postCode>
									<settlement>Saint Martin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.11,189.19,74.36,9.96"><forename type="first">Vincent</forename><surname>Bodinier</surname></persName>
							<email>vincent.bodinier@imag.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire d&apos;Informatique de Grenoble (LIG) Bâtiment IMAG B - Bibliothèque</orgName>
								<orgName type="institution">d&apos;Hères</orgName>
								<address>
									<addrLine>385 avenue</addrLine>
									<postCode>38400</postCode>
									<settlement>Saint Martin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.52,115.82,292.45,12.93;1,152.88,133.82,309.48,12.93;1,275.16,151.70,65.05,12.93">Working Notes for the InFile Campaign : Online Document Filtering Using 1 Nearest Neighbor</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">197BC32374D220B4C3C68B3AEAEF97B5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>filtering</term>
					<term>classification</term>
					<term>InFile</term>
					<term>CLEF 2008</term>
					<term>k Nearest Neighbor</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper has been written as a part of the InFile (IN-Formation, FILtering, Evaluation) campaign. This project is a crosslanguage adaptive filtering evaluation campaign, sponsored by the French national research agency, and it is a pilot track of the CLEF (Cross Language Evaluation Forum) 2008 campaigns. We propose in this paper an online algorithm to learn category specific thresholds in a multiclass environment where a document can belong to more than one class. Our method uses 1 Nearest Neighbor (1NN) algorithm for classification. It uses simulated user feedback to fine tune the threshold and in turn the classification performance over time. The experiments were run on English language corpus containing 100,000 documents. The best results have a precision of 0.366 and the recall is 0.260.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Infile campaign is aimed at testing crosslingual adaptive filtering systems. The task is to classify documents into different topics in an online fashion. In order to improve classification accuracy, a client can request for feedback. The number of feedbacks is limited to 50.</p><p>The k Nearest Neighbor (kNN) algorithm is a widely used supervised learning method and has been applied in a variety of different tasks like text classification, web-page classification etc <ref type="bibr" coords="1,278.28,571.87,7.80,9.96" target="#b0">[1]</ref><ref type="bibr" coords="1,286.08,571.87,3.90,9.96" target="#b1">[2]</ref><ref type="bibr" coords="1,286.08,571.87,3.90,9.96" target="#b2">[3]</ref><ref type="bibr" coords="1,289.98,571.87,7.80,9.96" target="#b3">[4]</ref>. It classifies a new instance based on its k closest examples in the feature space where the closeness is found using distance or similarity measures. Similarity has been preferred over distances while dealing with text. In such a case, cosine measure is used instead of Eucledian or Mahalanobis distance. The kNN rule is also refered to as a lazy method since it defers all computations to the run time. Yang et al. <ref type="bibr" coords="1,359.91,631.63,10.57,9.96" target="#b0">[1]</ref> have described a method where a category-specific threshold is learned on a validation set of examples. An example is said to be belonging to a category only if surpasses the threshold.</p><p>The rest of the paper is organized as follows. The problem is described in Section 2. Section 3 contains the proposed online algorithms followed by the experiments and results in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem formulation</head><p>Our goal for the InFile campaign evaluation is to filter 100,000 English documents provided one by one. The filtering has to be done on 50 topics (numbered from 101 to 150). 30 of them are related with general news and events (national and international affairs, politics, sports etc.) while 20 concern scientific and technical subjects.</p><p>A document can belong to zero, one or more topics. So, the system must be able to process similarity with each topic in order to determine whether a document belongs to it or not. In this project the context of competitive intelligence was considered where information filtering is a very specific subtask of the information management process <ref type="bibr" coords="2,314.53,304.51,10.00,9.96" target="#b4">[5]</ref>. In this approach, the information filtering task is very similar to Selective Dissemination of Information (SDI), one of the original and usual function assumed by documentalists and more recently, by other information intermediaries such as technological watchers or business intelligence professionals.</p><p>3 Algorithms' description</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General explanation</head><p>In this section, we present algorithms based on two different types of similarities. In the first type, a similarity between a topic file and a document is calculated (where a topic file is the profile of the topic) whereas in the other one, we compute a similarity between two documents.</p><p>It is necessary to use two similarities because topic files and documents do not have the same kind of content and hence the significance and interpretation of these two types of similarity are not the same. For instance, the similarity value between a topic file and a relevant document for this topic can be around 0.40 whereas the similarity value between two documents relevant to the same topic can be much higher. These two algorithms are based on a 1 nearest neighbor (1NN) algorithm and cosine similarity.</p><p>Each time a new document is retrieved, similarities with each of the topics are calculated. The comparison of this similarity value with a threshold determines whether the document is relevant to the topic or not.</p><p>The similarity f i (d) between a document d and a topic i is calculated as follows :</p><formula xml:id="formula_0" coords="2,169.80,629.35,225.62,23.04">f i (d) = α * cos(t i ,d) + (1 -α) max (d ′ ∈ti) cos(d,d'), where α ∈ [0,1]</formula><p>Here f i (d) is composed of two terms. The first one is related to the similarity between a topic file and a document weighted by α. The second term represents the similarity of a document d with the nearest neighbor in the topic weighted by 1α (the value of the nearest neighbor similarity is equal to 1 if no document has already been assigned to the topic).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">detailed algorithms</head><p>We will describe here the two algorithms developed during the course of InFile campaign. They are written in pseudo-code. notations : t i : topic file i (i ∈ {101, 102, ..., 150}) d : the current document processed.</p><p>Algorithm 1 This algorithm does not use any feedback. Its principle is rather simple. A threshold called S enables us to determine whether a document d is relevant to the topic or not. Its value is calculated as follows :</p><formula xml:id="formula_1" coords="3,169.80,350.83,133.36,23.16">S = α * β max + ( 1 -α ) * x s where β max , x s ∈ [0,1]</formula><p>The threshold S is composed of two terms. The first one is β max threshold weighted by α while the second one is x s threshold weighted by 1α. The threshold β max is the value above which we consider that the cos(t i ,d) is high enough to say that the document is relevant to the topic i. The threshold x s is a value above which we consider that the max (d ′ ∈ti) cos(d, d ′ ) is high enough and it can be said that the document d is relevant to the topic i.</p><p>This first algorithm requires fixing of three parameters : α, x s and</p><formula xml:id="formula_2" coords="3,134.76,473.23,345.89,167.87">β max ∈ [0,1]. for each new document d for each topic i (i ∈ {101,102,...,150}) if (f i (d) ≥ S) {Assign d to topic i} else {Do not assign d to topic i} where f i (d) = α * cos(t i ,d) + (1 -α) max (d ′ ∈ti) cos(d,d') , max (d ′ ∈i) cos(d, d ′ ) = 1 if no document is already assigned to topic i, S = α * β max + ( 1 -α ) * x s Algorithm 2</formula><p>The basics of the algorithm 2 are the same as that of the first one.</p><p>The main difference is that two different ways are used to judge the relevance.</p><p>Another difference is the threshold used for f i (d).</p><p>The idea is to build a base of 10 documents for each topic by only using cosine with the topic file. And once this base is built, the similarity f i (d) is used in the same way as used in the first algorithm. In this algorithm, feedbacks are used in order to limit the number of mistakes while building a base of 10 documents. γ is the threshold used to judge cos(t i , d) in the first part of the algorithm (γ ∈ [0,1]).</p><p>s i is the one used to judge the f i (d) similarity in the second part of the algorithm. Its formula is the following : </p><formula xml:id="formula_3" coords="4,169.80,216.19,83.10,11.01">s i = min (d∈i) (f i (d)</formula><formula xml:id="formula_4" coords="4,134.76,414.40,250.69,23.16">where f i (d) = α * cos(t i , d) + (1 -α)max (d ′ ∈ti) cos(d, d ′ ) , s i = min (d∈i) (f i (d))</formula><p>These two algorithms' behaviours depend strongly on the relevance values of thresholds that are fixed before launching the tests (in particular, β max and x s for the first one and γ for the second one).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We have used InFile English data for the experimental validation. For each new document retrieved, first of all stemming is performed using Porter's algorithm. This is followed by the removal of stop-words, XML tags skipping and the building of a document vector (which associates each term with its frequency) using rainbow <ref type="bibr" coords="4,172.69,600.07,10.00,9.96" target="#b5">[6]</ref>.</p><p>During the InFile campaign evaluation, three runs have been submitted. The table below described run's features. During the first run, our system retrieved 546 documents. Among these, 152 were relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 2 :</head><p>Relevant Not relevant Retrieved 411 900 Not retrieved 1186 97503</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3. Run 2 -results</head><p>For the second run, 411 documents that were retrieved were relevant. Overall, 1311 documents have been retrieved during this run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 3 :</head><p>Relevant Not relevant Retrieved 601 7037 Not retrieved 996 91366 Table <ref type="table" coords="6,286.44,205.98,4.12,8.08">4</ref>. Run 3 -results</p><p>7638 documents have been retrieved and 601 have been considered as correctly assigned to a topic.</p><p>Fig. <ref type="figure" coords="6,155.53,284.35,4.98,9.96" target="#fig_0">1</ref> shows the number of relevant documents retrieved for each set of 10000 documents. Fig. <ref type="figure" coords="6,207.73,296.23,4.98,9.96" target="#fig_1">2</ref> depicts the different measures evolution during the run 3. Abbreviations : num ret for 'number of documents retrieved', num rel ret for 'number of relevant documents retrieved' and num rel for 'number of relevant documents'. Abbreviations : prec for 'precision', F 0.5 for 'F-measure', util 1 0.5 -0.5 for 'linear utility' and cdet 10 0.1 for 'detection cost'.</p><p>Average scores : Here is a table which contains the whole average scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>Run 1 scores are rather low. In particular, recall's value is very low whereas precision is around 0.36 which shows that this run is precision-oriented because it retrieves a tiny part of the whole documents. Run 2 precision value is close to the first run. The recall and the F-measure are slightly better. This run is also precision-oriented with a precision's value clearly better than the recall one.</p><p>If we consider the overall scores, run 3 is better than the two others. Although the precision is slightly lower, the recall's score attains 0.26 while the F-measure reaches 0.2. The overall detection cost is very low during the runs (less than 0.01). This a strong point for our algorithms. We can also notice that the linear utility progressively increases between 0.2 and 0.3.</p><p>The run 1 is a more conservative method compared to the run 2 because of the differences in β max and x s values which affect the value of S. Hence the run 2 is expected to assign more documents to the topics than the run 1.</p><p>Regarding measures evolution, for run 1, precision, recall and F-measure tend to decrease slightly. For the run 2, they randomly vary but remain the same at the end, whereas they increase slightly during the last run.</p><p>Basically, these runs are clearly precision-oriented. Indeed, for the three runs respectively 0.5, 1.3 and 7.6 percent of the number of documents are retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our participation at this project was a good experience and it enables us to take awareness of the specificity of the information filtering in the frame of competitive intelligence. The major difficulty is to design a system with very quick adaptivity because of the few feedbacks available. Indeed, the system must learn metrics on few data.</p><p>Since we were the only participants who have completed the task during the InFile project, it is difficult to appreciate the results. Comparison with other systems would enable us to have a better analysis of our results.</p><p>As a consequence, we cannot say that our system is good but it clearly seems that the scores obtained are not sufficient to fulfill this task. Indeed, we cannot say that the better F-score we have obtained (around 0.2) is a sufficient one for this specific task which requires much higher scores.</p><p>The interest in these experiments actually remains in the way we computed similarity and judged its relevance. Algorithms presented in this paper are characterized by their obvious simplicity and effectiveness. We could imagine more complex algorithms based on k Nearest Neighbor with k greater than one and an attempt to learn metric using feedbacks. However, it is not expected to give better results. Indeed, we do not think that the use of a higher k would give different results since the topics are relatively distant from each other. In general, a document is considered at the most close to one topic, so there is no conflict at this level. Moreover, learning a metric with only 50 feedbacks is difficult.</p><p>Actually, the difficulty lies in calculating the similarities and finding the decision bounds. In order to refine the results, a solution could be to attach more importance to the content documents processing (for instance, by working on n-grams rather than on stems) in order to be more precise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,212.40,584.98,190.49,8.96;6,153.29,339.66,308.24,230.99"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. run5G -Relevant Documents Retrieved</figDesc><graphic coords="6,153.29,339.66,308.24,230.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,239.88,351.10,135.54,8.96;7,157.37,116.27,300.75,220.50"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. run5G -Scores' Evolution</figDesc><graphic coords="7,157.37,116.27,300.75,220.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.76,216.19,345.98,180.96"><head></head><label></label><figDesc>)In this algorithm two constants must be parametrized namely, α and γ ∈ [0,1].</figDesc><table coords="4,134.76,255.19,340.35,75.12"><row><cell>for each new document d</cell></row><row><cell>for each topic i</cell></row><row><cell>if (number of documents already assigned to topic i &lt; 10)</cell></row><row><cell>if (cos(t i , d) &gt; γ)</cell></row><row><cell>feedback = 1</cell></row><row><cell>if (number of remaining feedbacks != 0) feedback = AskFeedback()</cell></row></table><note coords="4,181.08,333.79,219.06,9.96;4,181.08,347.11,138.90,9.96;4,169.80,360.43,138.90,9.96;4,158.40,373.87,36.80,9.96;4,195.24,377.94,2.82,4.91;4,198.48,373.87,35.22,9.96;4,234.12,373.87,95.70,9.96;4,178.20,387.19,138.90,9.96"><p>if (feedback == 1) {assign document d to topic i} else {do not assign d to topic i} else {do not assign d to topic i} else if (f i (d) &gt; s i ) {assign d to topic i} else {do not assign d to topic i}</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.76,117.34,345.98,230.12"><head>Table 1 .</head><label>1</label><figDesc>Run's features4.1 Results1597 is the total number of relevant documents to find during a run. The different scores have been computed by averaging scores' values on the whole profiles. The complete reports of each runs are presented in the appendix 1, 2 and 3.</figDesc><table coords="5,134.76,117.34,289.84,230.12"><row><cell cols="2">Name algorithm</cell><cell></cell><cell>parameters</cell></row><row><cell>Run 1 runname</cell><cell>1</cell><cell cols="2">α = 0.7; xs = 0.8; βmax = 0.45</cell></row><row><cell>Run 2 run2G</cell><cell>1</cell><cell cols="2">α = 0.7; xs = 0.7; βmax = 0.4</cell></row><row><cell>Run 3 run5G</cell><cell>2</cell><cell></cell><cell>α = 0.7; γ = 0.42</cell></row><row><cell>Run1 :</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Relevant Not relevant</cell></row><row><cell cols="2">Retrieved</cell><cell>152</cell><cell>394</cell></row><row><cell cols="3">Not retrieved 1445</cell><cell>98009</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,257.52,359.38,100.34,8.96"><head>Table 2 .</head><label>2</label><figDesc> </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,179.04,472.42,257.14,63.92"><head>Table 5 .</head><label>5</label><figDesc>runs scores</figDesc><table coords="7,179.04,472.42,257.14,43.04"><row><cell cols="4">Precision Recall F-measure Linear Utility Detection Cost</cell></row><row><cell>Run 1 0.366 0.068</cell><cell>0.086</cell><cell>0.311</cell><cell>0.009</cell></row><row><cell>Run 2 0.357 0.165</cell><cell>0.165</cell><cell>0.335</cell><cell>0.008</cell></row><row><cell>Run 3 0.306 0.260</cell><cell>0.209</cell><cell>0.351</cell><cell>0.007</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.32,142.06,342.35,8.96;9,146.88,153.10,333.51,8.96;9,146.88,164.02,333.75,8.96;9,146.88,174.94,97.11,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,263.50,142.06,197.91,8.96">A re-examination of text categorization methods</title>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,146.88,153.10,333.51,8.96;9,146.88,164.02,213.35,8.96">SIGIR &apos;99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.32,185.98,341.89,8.96;9,146.88,196.90,333.64,8.96;9,146.88,207.82,77.69,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,279.44,185.98,200.77,8.96;9,146.88,196.90,30.54,8.96">An adaptive k-nearest neighbor text categorization strategy</title>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Baoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lu</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Shiwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,185.52,196.90,290.21,8.96">ACM Transactions on Asian Language Information Processing (TALIP)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="215" to="226" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.32,218.86,342.26,8.96;9,146.88,229.78,333.77,8.96;9,146.88,240.70,333.87,8.96;9,146.88,251.74,72.62,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,306.71,218.86,173.87,8.96;9,146.88,229.78,72.49,8.96">Web page classification based on k-nearest neighbor approach</title>
		<author>
			<persName coords=""><forename type="first">Oh-Woog</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jong-Hyeok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,238.56,229.78,242.09,8.96;9,146.88,240.70,186.89,8.96">IRAL &apos;00: Proceedings of the fifth international workshop on on Information retrieval with Asian languages</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.32,262.66,342.25,8.96;9,146.88,273.58,132.76,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,236.02,262.66,210.35,8.96">Machine learning in automated text categorization</title>
		<author>
			<persName coords=""><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,459.12,262.66,21.45,8.96;9,146.88,273.58,57.31,8.96">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.32,284.49,342.20,8.96;9,146.88,295.54,333.65,8.96;9,146.88,306.46,333.67,8.96;9,146.88,317.37,333.63,8.96;9,146.88,328.41,80.80,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,215.98,295.54,264.55,8.96;9,146.88,306.46,20.43,8.96">The infile project: a crosslingual filtering systems evaluation campaign</title>
		<author>
			<persName coords=""><forename type="first">Romaric</forename><surname>Besancon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Chaudiron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Djamel</forename><surname>Mostefa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismail</forename><surname>Timimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,185.28,306.46,203.26,8.96;9,424.56,306.46,55.99,8.96;9,146.88,317.37,281.24,8.96">Proceedings of the Sixth International Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the Sixth International Language Resources and Evaluation (LREC&apos;08)<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA)</note>
</biblStruct>

<biblStruct coords="9,138.32,339.34,342.17,8.96;9,146.88,350.25,137.18,8.96" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,222.69,339.34,257.80,8.96;9,146.88,350.25,109.41,8.96">Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
