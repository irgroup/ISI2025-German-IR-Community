<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,125.98,148.86,351.04,15.15">JHU Ad Hoc Experiments at CLEF 2008</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,268.36,182.75,66.28,8.74"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
							<email>paul.mcnamee@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center of Excellence</orgName>
								<orgName type="institution">JHU Human Language Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,125.98,148.86,351.04,15.15">JHU Ad Hoc Experiments at CLEF 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A9A0203AFFACF81A6B3933BAC08CAA3C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval Multilingual text retrieval</term>
					<term>Character n-grams</term>
					<term>Farsi language retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For CLEF 2008 JHU conducted monolingual and bilingual experiments in the ad hoc TEL and Persian tasks.</p><p>The TEL task involved focused on searching electronic card catalog records in English, French, and German using data from the British Library, the Bibliotheque Nationale de France, and the Österreichische Nationalbibliothek (Austrian National Library). The approach we adopted for TEL was to strip out non-content sections of records and to treat the task as ordinary full-text search using character n-grams and stemmed words.</p><p>For the Persian task, which is based on the Hamshahri corpus, several different forms of textual normalization were compared. Using the provided training topics we compared character n-grams, n-gram stems, ordinary words, words automatically segmented into morphemes, and a novel form of n-gram indexing based on n-grams with character skips. On the training topics we found that character 5-grams and skipgrams performed the best and this was borne out in our official submissions.</p><p>We also did some post hoc experiments using previous CLEF ad hoc tests sets in 13 languages.</p><p>In all three tasks we explored alternative methods of tokenizing documents including plain words, stemmed words, automatically induced segments, a single selected ngrams for each words, and all n-grams from words (i.e., traditional character n-grams). Character n-grams demonstrated consistent gains over ordinary words in each of these three diverse sets of experiments. Using mean average precision, relative gains of of 50-200% on the TEL task, 5% on the Persian task, and 18% averaged over 13 languages from past CLEF evaluations, were observed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As a tokenization scheme character n-grams possess many advantages. They work in every language, require no training, and are more effective than plain words. It also appears that n-grams are beneficial for normalizing morphological variation, particularly in languages where words have many related surface forms.</p><p>Using test sets in the 13 languages used in the ad hoc tracks at previous CLEF evaluations, we compared n-grams to several tokenization alternatives, including a rule-based stemmer (Snowball ), an unsupervised morphological segmenter (Morfessor ), and a synthetic form of stemming based on selecting a single character n-gram from each word. Character n-grams of length n = 5 were the most effective technique, performing 18% better than unnormalized words, averaged across the set of languages.</p><p>Accordingly n-grams were used in official submissions to this year's ad hoc tasks. For this year's participation at CLEF we used the JHU HAIRCUT retrieval system, employing a statistical language model similarity metric with a smoothing constant of 0.5. The similarity calculation combines document term frequencies and corpus frequencies (for smoothing) using linear interpolation with a smoothing constant of 0.5 <ref type="bibr" coords="2,268.43,255.48,9.96,8.74" target="#b8">[9]</ref>. For retrieval of Farsi text, we explored a variant of n-gram indexing, skipgrams, which are n-gram sequences that omit some letters. Farsi has root and template morphology and it was thought that skipgrams might prove effective.</p><p>In Section 2 we describe our experiments for the TEL subtask. In Section 3 we analyze our training experiments and official results for the Persian subtask. In Section 4 some recent experiments on previous CLEF collections are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TEL task</head><p>Our approach to TEL was to treat the collection as unstructured documents. Fields that did not appear to contain good indexable content were removed, including: publisher, rights, format, description, indentifier, contributor, type, language, coverage, issued, available, extent, spatial, and created. Text from the following fields was retained: ispartof, edition, alternative, tableofcontents, abstract, bibliographiccitation, subject, title, abstract, date, creator, source, and relation. All SGML tags were removed. Some of these choices were probably harmful. For example, queries that specified a particular language or document type (i.e., maps) might have benefitted from some of the deleted metadata. The aim of removing these fields was to increase the coherence of each document's indexable terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexing Schemes</head><p>The tokenization methods explored were:</p><p>• words: space-delimited tokens.</p><p>• snow: output of the Snowball stemmer.</p><p>• morf : the set of morphemes for each word identified by the Morfessor algorithm. Morfessor is available online at http://www.cis.hut.fi/projects/morpho/. A model was trained using the document collection's lexicon with digit-containing tokens omitted. The default parameters for the Morfessor algorithm were used <ref type="bibr" coords="2,335.61,607.59,9.96,8.74" target="#b0">[1]</ref>.</p><p>• lcn4/5: least common n-gram stem (i.e., rarest word-internal character n-gram) of length n = 4 or n = 5 <ref type="bibr" coords="2,182.24,639.47,9.96,8.74" target="#b2">[3]</ref>.</p><p>• 4-grams: overlapping, word-spanning character 4-grams produced from the stream of words encountered in the document or query.</p><p>• 5-grams: length n = 5 n-grams created in the same fashion as the character 4-grams.</p><p>Common to each tokenization method was conversion to lower case letters, removal of punctuation, and truncation of long numbers to 6 digits. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Monolingual Results</head><p>Our official submissions were based on 4-grams, both with and without automated relevance feedback, 5-grams (no RF), and stemmed words. Table <ref type="table" coords="3,339.55,294.88,4.98,8.74" target="#tab_0">1</ref> lists mean average precision for these runs, as well as for several unsubmitted runs. In the official run names xx indicates one of de (German), en (English), or fr (French). While performance did not vary dramatically in English, except for the unnormalized word run which performed the worst, 4-grams were dominant with the French and German collections. Large gains were observed with 4-grams compared to plain words -more than a 50% relative gain in French and over 200% in German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bilingual Results</head><p>We considered the following bilingual pairs:</p><formula xml:id="formula_0" coords="3,104.94,434.81,92.73,168.14">• Dutch to English • French to English • German to English • Spanish to English • Dutch to French • English to French • German to French • English to German • French to German</formula><p>For each language pair the source side query was tokenized using only character 5-grams and those n-grams were 'translated' to the target language using a large aligned parallel corpus (content from the Official Journal of the European Journal). The methodology in query term translation was like that in <ref type="bibr" coords="3,161.13,651.99,9.96,8.74" target="#b4">[5]</ref>; however, here no pre-translation query expansion was performed. In Table <ref type="table" coords="3,508.02,651.99,4.98,8.74" target="#tab_1">2</ref> results are presented using mean average precision to compare performance.</p><p>Source language did not make a large difference in performance across the three collections. Bilingual performance was approximately 60% of the highest performing monolingual run, which is a bit lower than we have customarily observed in bilingual retrieval against news corpora at CLEF. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Persian language task</head><p>We made submissions for both the monolingual and bilingual subtasks. The bilingual submissions were based on online machine translation software<ref type="foot" coords="4,316.86,260.87,3.97,6.12" target="#foot_0">1</ref> applied to the queries, so only one set of indexes was required. In addition to the methods in Section X.Y we used skipgrams, 4-or 5grams with and without one internal skip (denoted by sk41 &amp; sk51 ). Snowball does not support Farsi so no stemming runs were attempted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Skipgrams</head><p>Consider the present tense conjugation of the Spanish verb contar (to count): cuento, cuentas, cuenta, contamos, contáis, and cuentan. Such inflectional variation can cause lexical mismatches that would impair retrieval, and character n-grams are unlikely to be a total solution to this problem since the 1st and 2nd person plural forms do not share longer n-grams with the other forms. Similar problems can happen with nouns, for example, in Welsh plentyn (child ) and its plural, plant (children). These two examples contain patterns that could enable matching, such as regular expressions c nt and pl nt, which would match all the related forms. Pirkola et al. <ref type="bibr" coords="4,166.74,428.28,10.52,8.74" target="#b7">[8]</ref> have proposed n-grams with skips 2 to match terminology for cross-language information retrieval in languages sharing a common alphabet. For example, the English word calcitonin can be matched to its Finnish translation kalsitoniini, supported in part by matches like l t and n n. Mustafa <ref type="bibr" coords="4,186.36,464.14,10.52,8.74" target="#b6">[7]</ref> proposed a similar method for monolingual Arabic language processing, where infix morphological changes are common. He identified relevant dictionary terms using bigrams with and without a single skip character and a Dice coefficient to compare sets of bigrams. Järvelin et al. <ref type="bibr" coords="4,152.23,500.01,10.52,8.74" target="#b1">[2]</ref> formalized the notion of skipgrams and investigated methods of comparing lexical terms; however, they focused on the case where a single skip is formed by deleting contiguous letters. This makes sense when only bigrams are considered -then the only place to skip characters is between the first and last letters of the (skip) bigram.</p><p>But with longer n-grams there are multiple places where skips can occur, and character skipgram methods can be generalized even further by including the possibility of multiple non-adjacent skips within a single word (though no such experiments are reported here). In these experiments skipgrams are considered as an alternative method for tokenization that might support matches across morphologically related words. When a letter is skipped we replace that letter in the n-gram subsequence with a special symbol (i.e., a dot character (•). This is done in an attempt to avoid unintended conflations with n-gram strings produced by unrelated words. Skipgram tokenization of length four for the word cream would include the regular n-grams crea and ream in addition to c•eam, cr•am, and cre•m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Data</head><p>The various methods of tokenization were compared on the 50 training topics. In Table <ref type="table" coords="4,485.19,689.75,4.98,8.74" target="#tab_2">3</ref> runs without relevance feedback are presented along with runs that made use of automated feedback using various numbers of expansion terms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Monolingual and Bilingual Results</head><p>In Table <ref type="table" coords="5,129.27,428.58,4.98,8.74">4</ref> mean average precision is reported for eight tokenization methods. The n-grams methods are the highest performing approach and the skipgrams perform slightly worse than traditional character n-grams. The highest performing run was character 4-grams using 200 expansion terms which got a MAP score of 0.4564; however the results on the training topics suggested 5-grams would outperform and we selected them instead. N-grams appear to need more query expansion terms than words to maximize performance, and skipgrams, being even more conflationary require more than regular 4-or 5-grams.</p><p>The results for our official monolingual and bilingual runs are given in Table <ref type="table" coords="5,444.66,512.27,3.87,8.74" target="#tab_3">5</ref>. Tokenization method did not appear to drastically affect the outcome monolingually; however, words and the Morfessor-based runs did markedly worse on the bilingual task compared to the n-gram based methods. We compared plain words, stems, induced morphemes, n-gram stems, and character n-grams using test sets from the CLEF ad hoc tasks between 2002 and 2007. <ref type="foot" coords="6,353.18,380.34,3.97,6.12" target="#foot_1">3</ref> In each of the 13 languages we used two years worth of data except for Czech where only one year was available. The number of test queries per language varied from 50 (Czech) to 107 (Spanish). In Table <ref type="table" coords="6,413.26,405.82,4.98,8.74" target="#tab_4">6</ref> results are presented using mean average precision to compare performance. The score for the highest performing technique in each language is emboldened.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Unnormalized words</head><p>Not attempting to control for morphological processes can have harmful effects. In Bulgarian, Czech, Finnish, and Hungarian, more than a 30% loss is observed compared to the use of 4-grams as indexing terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Snowball stemming</head><p>Snowball does not support Bulgarian, Czech, or Russian and due to character encoding issues with the software we were not able to use it for Portuguese and Hungarian. In Table <ref type="table" coords="6,450.39,558.15,4.98,8.74" target="#tab_0">1</ref> performance for each technique is given averaged over eight remaining languages. Stemming, when available, is quite effective, and just slightly below the top-ranked approach of character n-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Morfessor Segments</head><p>As it may be difficult to find a rule-based stemmer for every language, a language-independent approach can be quite attractive. The Morfessor algorithm only requires a lexicon (i.e., wordlist) for a language to learn to identify morpheme boundaries, even for previously unseen words. Such automatically detected segments can be an effective form of tokenization <ref type="bibr" coords="6,420.07,664.21,9.96,8.74" target="#b5">[6]</ref>. Examples of the algorithm's output are presented in Table <ref type="table" coords="6,282.28,676.16,3.87,8.74" target="#tab_1">2</ref>, along with results for Snowball and character 5grams.</p><p>Compared to plain words the induced morphemes produced by Morfessor led to gains in 9 of 13 languages; 8 of these were significant improvements with p &lt; 0.05 (Wilcoxon test). The languages where words outperformed segments were English (dramatically), French, Italian, and Spanish -each is relatively low in morphological complexity. The differences in French and Spanish were less than 0.004 in absolute terms. Segments achieved more than a 20% relative improvement in Bulgarian, Finnish, and Russian, and over 40% in Czech and Hungarian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Least Common N-gram Stems</head><p>Another language-neutral approach to stemming is to select for each word, its least common ngram. This requires advance knowledge of n-gram frequencies, but this is easily obtainable by constructing a regular n-gram index, or even by scanning a corpus and counting. Lengths of n = 4 and n = 5 appear about equally effective with a slight advantage for lcn4, but this is influenced primarily by the languages with greater morphological complexity, which see larger changes. An 8% relative improvement in mean average precision over words is obtained. As can be seen from Table <ref type="table" coords="7,178.50,253.94,3.87,8.74" target="#tab_0">1</ref>, in languages where rule-based stemming is available its use is preferable. N-gram stemming achieves comparable performance with Morfessor segments..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Overlapping Character N-grams</head><p>N-grams achieve morphological regularization indirectly due to the fact that subsequences that touch on word roots will match. For example, "juggling" and "juggler" will share the 5-grams jugg and juggl. While n-gram's redundancy enables useful matches, other matches are less valuable, for example, every word ending in 'tion' will share 5-gram tion with all of the others. In practice these morphological false alarms are almost completely discounted because term weighting deemphasizes them. In fact, such affixes can be so common, that ignoring them entirely by treating them as "stop n-grams" is a reasonable thing to do.</p><p>Character n-grams are the most effective technique studied here, giving a relative improvement of 18%. Consistent with earlier work <ref type="bibr" coords="7,252.77,407.81,10.52,8.74" target="#b3">[4]</ref> lengths of n = 4 and n = 5 are equally effective averaged across the 13 languages; however there are some noticeable differences in particular languages. The data is suggestive of a trend that the most morphologically variable languages (i.e., Bulgarian, Czech, Hungarian, and Russian) gain more from 4-grams than 5-grams, while 5-grams have a slight advantage in medium complexity languages.</p><p>Snowball stems are roughly as effective as n-grams, on average, but only available in certain languages (i.e., 8 of 13 in this study). The other "alternative" stemming approaches, segments and least common n-grams, appear to gain about half of the benefit that full n-gram indexing sees compared to unnormalized word forms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,161.21,118.94,280.57,118.47"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="3,161.21,118.94,280.57,118.47"><row><cell></cell><cell></cell><cell cols="2">: Monolingual Results</cell><cell></cell></row><row><cell></cell><cell cols="4">English French German Run designation</cell></row><row><cell>words</cell><cell>0.2719</cell><cell>0.2019</cell><cell>0.1073</cell><cell>not submitted</cell></row><row><cell>snow</cell><cell>0.3480</cell><cell>0.2290</cell><cell>0.1757</cell><cell>aplmoxxs</cell></row><row><cell>morf</cell><cell>0.3171</cell><cell>0.2332</cell><cell>0.1989</cell><cell>not submitted</cell></row><row><cell>lcn4</cell><cell>0.3086</cell><cell>0.2223</cell><cell>0.1565</cell><cell>not submitted</cell></row><row><cell>lcn5</cell><cell>0.2993</cell><cell>0.2270</cell><cell>0.1810</cell><cell>not submitted</cell></row><row><cell>4-grams</cell><cell cols="3">0.3382 0.2950 0.3377</cell><cell>aplmoxx4</cell></row><row><cell>5-grams</cell><cell>0.3190</cell><cell>0.2800</cell><cell>0.3102</cell><cell>aplmoxx5</cell></row><row><cell cols="3">4-grams + RF 0.3531 0.2861</cell><cell>0.3176</cell><cell>aplmoxx4rf</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,217.84,118.94,167.32,82.60"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table coords="4,217.84,118.94,167.32,82.60"><row><cell></cell><cell cols="3">Official Bilingual Runs</cell></row><row><cell>Source</cell><cell cols="3">English French German</cell></row><row><cell>Dutch</cell><cell>0.2024</cell><cell>0.1746</cell><cell>x</cell></row><row><cell>English</cell><cell>x</cell><cell>0.1669</cell><cell>0.1899</cell></row><row><cell>French</cell><cell>0.2087</cell><cell>x</cell><cell>0.1829</cell></row><row><cell cols="2">German 0.2111</cell><cell>0.1608</cell><cell>x</cell></row><row><cell cols="2">Spanish 0.1856</cell><cell>x</cell><cell>x</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,160.94,119.49,281.11,263.57"><head>Table 3 :</head><label>3</label><figDesc>Training results for Persian (mean average precision)</figDesc><table coords="5,160.94,131.75,281.11,251.32"><row><cell></cell><cell>No RF</cell><cell>50</cell><cell>100</cell><cell>200</cell><cell>400</cell><cell>800</cell></row><row><cell>4-grams</cell><cell cols="4">0.3883 0.4199 0.4231 0.4172</cell><cell>-</cell><cell>-</cell></row><row><cell>5-grams</cell><cell cols="4">0.3810 0.4225 0.4305 0.4280</cell><cell>-</cell><cell>-</cell></row><row><cell>words</cell><cell cols="4">0.4091 0.4175 0.3999 0.3905</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">morfessor 0.3784 0.3951 0.3801 0.3637</cell><cell>-</cell><cell>-</cell></row><row><cell>lcn4</cell><cell cols="4">0.3914 0.3975 0.3840 0.3730</cell><cell>-</cell><cell>-</cell></row><row><cell>lcn5</cell><cell cols="4">0.3978 0.3960 0.3779 0.3723</cell><cell>-</cell><cell>-</cell></row><row><cell>sk41</cell><cell cols="6">0.3886 0.4000 0.4156 0.4332 0.4372 0.4290</cell></row><row><cell>sk51</cell><cell cols="6">0.3613 0.3607 0.3817 0.4012 0.4216 0.4280</cell></row><row><cell></cell><cell cols="4">Table 4: Monolingual runs</cell><cell></cell></row><row><cell></cell><cell>No RF</cell><cell>50</cell><cell>100</cell><cell>200</cell><cell>400</cell></row><row><cell>words</cell><cell cols="4">0.3617 0.4332 0.4299 0.4211</cell><cell>-</cell></row><row><cell>morf</cell><cell cols="4">0.3559 0.4250 0.4223 0.4156</cell><cell>-</cell></row><row><cell>lcn4</cell><cell cols="4">0.3629 0.4252 0.4256 0.4180</cell><cell>-</cell></row><row><cell>lcn5</cell><cell cols="4">0.3506 0.4225 0.4188 0.4085</cell><cell>-</cell></row><row><cell cols="5">4-grams 0.3986 0.4383 0.4530 0.4564</cell><cell>-</cell></row><row><cell cols="5">5-grams 0.3821 0.4288 0.4493 0.4558</cell><cell>-</cell></row><row><cell>sk41</cell><cell cols="5">0.3906 0.3732 0.4053 0.4384 0.4519</cell></row><row><cell>sk51</cell><cell cols="5">0.3512 0.3238 0.3595 0.4008 0.4250</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,175.63,584.94,251.75,116.93"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table coords="5,175.63,584.94,251.75,116.93"><row><cell></cell><cell></cell><cell cols="2">Official runs</cell><cell></cell></row><row><cell></cell><cell>Task</cell><cell>Index</cell><cell>RF Terms</cell><cell>MAP</cell></row><row><cell>jhufa5r100</cell><cell cols="2">mono 5-grams</cell><cell>100</cell><cell>0.4493</cell></row><row><cell>jhufask41r400</cell><cell>mono</cell><cell>sk41</cell><cell>400</cell><cell>0.4519</cell></row><row><cell>jhufawr50</cell><cell>mono</cell><cell>words</cell><cell>50</cell><cell>0.4332</cell></row><row><cell>jhufamr50</cell><cell>mono</cell><cell>morf</cell><cell>50</cell><cell>0.4250</cell></row><row><cell>jhuenfa5r100</cell><cell>bi</cell><cell>5-grams</cell><cell>100</cell><cell>0.1660</cell></row><row><cell>jhuenfask41r400</cell><cell>bi</cell><cell>sk41</cell><cell>400</cell><cell>0.1892</cell></row><row><cell>jhuenfawr50</cell><cell>bi</cell><cell>words</cell><cell>50</cell><cell>0.0946</cell></row><row><cell>jhuenfamr50</cell><cell>bi</cell><cell>morf</cell><cell>50</cell><cell>0.1112</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,90.00,119.49,423.91,238.23"><head>Table 6 :</head><label>6</label><figDesc>Comparison of 7 Tokenization Alternatives (Mean Average Precision)</figDesc><table coords="6,90.00,131.75,423.91,225.98"><row><cell>Language</cell><cell cols="3">Data Queries Words</cell><cell>Snow</cell><cell>Morf</cell><cell>LCN4 LCN5 4-gram 5-gram</cell></row><row><cell cols="2">Bulgarian 06-07</cell><cell>100</cell><cell>0.2195</cell><cell></cell><cell>0.2786 0.2937 0.2547 0.3163 0.2916</cell></row><row><cell>Czech</cell><cell>07</cell><cell>50</cell><cell>0.2270</cell><cell></cell><cell>0.3215 0.2567 0.2477 0.3294 0.3223</cell></row><row><cell>Dutch</cell><cell>02-03</cell><cell>106</cell><cell cols="3">0.4162 0.4273 0.4274 0.4021 0.4073 0.4378 0.4443</cell></row><row><cell>English</cell><cell>02-03</cell><cell>96</cell><cell cols="3">0.4829 0.5008 0.4265 0.4759 0.4861 0.4411</cell><cell>0.4612</cell></row><row><cell>Finnish</cell><cell>02-03</cell><cell>75</cell><cell cols="3">0.3191 0.4173 0.3846 0.3970 0.3900 0.4827 0.4960</cell></row><row><cell>French</cell><cell>02-03</cell><cell>102</cell><cell cols="3">0.4267 0.4558 0.4231 0.4392 0.4355 0.4442</cell><cell>0.4399</cell></row><row><cell>German</cell><cell>02-03</cell><cell>106</cell><cell cols="3">0.3489 0.3842 0.4122 0.3613 0.3656 0.4281 0.4321</cell></row><row><cell cols="2">Hungarian 06-07</cell><cell>98</cell><cell>0.1979</cell><cell></cell><cell>0.2932 0.2784 0.2704 0.3549 0.3438</cell></row><row><cell>Italy</cell><cell>02-03</cell><cell>100</cell><cell cols="3">0.3950 0.4350 0.3770 0.4127 0.4054 0.3925</cell><cell>0.4220</cell></row><row><cell cols="2">Portuguese 05-06</cell><cell>100</cell><cell>0.3232</cell><cell></cell><cell>0.3403 0.3442 0.3381 0.3316 0.3515</cell></row><row><cell>Russian</cell><cell>03-04</cell><cell>62</cell><cell>0.2671</cell><cell></cell><cell>0.3307 0.2875 0.3053 0.3406 0.3330</cell></row><row><cell>Spanish</cell><cell>02-03</cell><cell>107</cell><cell cols="3">0.4265 0.4671 0.4230 0.4260 0.4323 0.4465</cell><cell>0.4376</cell></row><row><cell>Swedish</cell><cell>02-03</cell><cell>102</cell><cell cols="3">0.3387 0.3756 0.3738 0.3638 0.3467 0.4236 0.4271</cell></row><row><cell>Average</cell><cell></cell><cell></cell><cell>0.3375</cell><cell></cell><cell>0.3698 0.3645 0.3604 0.3955 0.3979</cell></row><row><cell cols="3">Average (8 Snowball langs)</cell><cell cols="3">0.3504 0.3848 0.3608 0.3642 0.3632 0.3885 0.3956</cell></row><row><cell cols="6">4 Analysis from Past CLEF Collections</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,105.24,732.59,184.12,6.99"><p>http://www.parstranslator.net/eng/translate.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="6,105.24,743.46,341.18,6.99"><p>These results are also reported in our Morpho Challenge 2008 paper in these working notes.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Conclusions</head><p>We examined a variety of methods for lexical normalization, finding that the most effective technique was character n-gram indexing. N-grams achieved consistent gains in mean average precision over unlemmatized words. Relative gains of of 50-200% on the TEL task, 5% on the Persian task, and 18% averaged over thirteen languages from past CLEF evaluations, were observed. In languages such as Czech, Bulgarian, Finnish, and Hungarian gains of over 40% were observed. While rule-based stemming can be quite effective, such tools are not available in every language and even when present, require additional work to integrate with an IR system. When languageneutral methods are able to achieve the same, or better performance, their use should be seriously considered.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,105.50,708.63,407.50,8.74;7,105.50,720.58,407.50,8.74;7,105.50,732.54,76.93,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,266.88,708.63,246.11,8.74;7,105.50,720.58,209.27,8.74">Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1</title>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Helsinki University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="8,105.50,112.02,407.50,8.74;8,105.50,123.98,396.51,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,342.10,112.02,170.89,8.74;8,105.50,123.98,103.91,8.74">S-grams: Defining generalized n-grams for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Anni</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antti</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,217.68,123.98,176.63,8.74">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1005" to="1019" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,143.90,407.50,8.74;8,105.50,155.86,22.69,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,264.72,143.90,102.03,8.74">Single n-gram stemming</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,386.56,143.90,25.85,8.74">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="415" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,175.78,407.51,8.74;8,105.50,187.74,246.62,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,273.91,175.78,239.10,8.74;8,105.50,187.74,55.43,8.74">Character N-gram tokenization for european language text retrieval</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,169.19,187.74,92.98,8.74">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="73" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,207.66,407.51,8.74;8,105.50,219.62,407.50,8.74;8,105.50,231.57,387.82,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,267.49,207.66,115.63,8.74">Translating pieces of words</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,403.91,207.66,109.09,8.74;8,105.50,219.62,407.50,8.74;8,105.50,231.57,93.25,8.74">SIGIR 2005: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Salvador, Brazil</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">August 15-19, 2005. 2005</date>
			<biblScope unit="page" from="643" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,251.50,407.50,8.74;8,105.50,263.45,357.87,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,378.41,251.50,134.59,8.74;8,105.50,263.45,67.35,8.74">Don&apos;t have a stemmer?: Be un+concern+ed</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,194.90,263.45,43.83,8.74">SIGIR &apos;08</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="813" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,283.38,407.50,8.74;8,105.50,295.33,329.79,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,201.86,283.38,311.14,8.74;8,105.50,295.33,59.44,8.74">Character contiguity in n-gram based word matching: the case for arabic text searching</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suleiman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mustafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,173.65,295.33,176.63,8.74">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="819" to="827" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,315.26,407.50,8.74;8,105.50,327.21,407.50,8.74;8,105.50,339.17,176.91,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,105.50,327.21,407.50,8.74;8,105.50,339.17,81.24,8.74">Targeted s-gram matching: a novel n-gram matching technique for cross-and mono-lingual word form variants</title>
		<author>
			<persName coords=""><forename type="first">Ari</forename><surname>Pirkola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heikki</forename><surname>Keskustalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erkka</forename><surname>Leppänen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antti-Pekka</forename><surname>Känsälä</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,195.29,339.17,32.75,8.74">Inf. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,359.09,407.50,8.74;8,105.50,371.05,154.79,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,259.34,359.09,236.84,8.74">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jay</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,105.50,371.05,25.85,8.74">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
