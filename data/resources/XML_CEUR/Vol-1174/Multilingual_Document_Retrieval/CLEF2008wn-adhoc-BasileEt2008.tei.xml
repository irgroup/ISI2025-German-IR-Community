<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,124.26,148.86,354.49,15.15;1,204.43,170.78,194.14,15.15">UNIBA-SENSE at CLEF 2008: SEmantic N-levels Search Engine</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,173.13,204.67,69.04,8.74"><forename type="first">Pierpaolo</forename><surname>Basile</surname></persName>
							<email>basilepp@di.uniba.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Univerisity of Bari (ITALY)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.59,204.67,75.00,8.74"><forename type="first">Annalina</forename><surname>Caputo</surname></persName>
							<email>acaputo@di.uniba.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Univerisity of Bari (ITALY)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.28,204.67,82.59,8.74"><forename type="first">Giovanni</forename><surname>Semeraro</surname></persName>
							<email>semeraro@di.uniba.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Univerisity of Bari (ITALY)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,124.26,148.86,354.49,15.15;1,204.43,170.78,194.14,15.15">UNIBA-SENSE at CLEF 2008: SEmantic N-levels Search Engine</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5C476E3BFFCDB2B4C44AAC7438208340</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Indexing methods</term>
					<term>H.3.3 Retrieval models</term>
					<term>H.3.4 Performance evaluation (efficiency and effectiveness) Information Retrieval, Performance, Experimentation Information Retrieval, Evaluation, Cross-language Information Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents evaluation experiments conducted at the University of Bari for the Ad-Hoc Robust WSD task of the Cross-Language Evaluation Forum (CLEF) 2008. The evaluation was performed using SENSE (SEmantic N-levels Search Engine) <ref type="bibr" coords="1,438.58,309.40,9.96,8.74" target="#b1">[2]</ref>. SENSE tries to overcome the limitations of the ranked keyword approach by introducing semantic levels, which integrate (and not simply replace) the lexical level represented by keywords.</p><p>We show how SENSE is able to manage documents indexed at two separate levels, keywords and word meanings, as well as to combine keyword search with semantic information provided by the other indexing levels, in an attempt of improving the retrieval performance.</p><p>Two types of experiments have been performed, by exploiting only one indexing level and exploiting all indexing levels at the same time. The experiments performed combining keywords and word meanings, extracted from the WordNet lexical database, show the promise of the idea and point out the value of our intuition.</p><p>In particular the results confirm our hypothesis that the combination of two indexing levels outperforms a single level. Indeed, an improvement of 35% in precision whose obtained by adopting the N-levels model with respect to the results obtained by exploiting the indexing level based only on keywords. Moreover, the Precision-Recall curve shows that the N-levels model outperforms the keywords level at all values of recall.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information Retrieval (IR) systems are generally concerned with the selection of documents, from a fixed collection, which satisfy a user's one-off information need (query). The traditional search strategy performed by IR systems is ranked keyword search: For a given query, a list of documents, ordered by relevance, is returned. Relevance computation is primarily driven by a string-matching operation: If any query word is found in a document belonging to the collection, a match is made and the document is considered as relevant.</p><p>Ranked keyword search has been quite successful in the past, in spite of its obvious limits basically due to polysemy, the presence of multiple meanings for one word, and synonymy, different words having the same meaning. The result is that, due to synonymy, relevant documents can be missed if they do not contain the exact query keywords, while, due to polysemy, wrong documents could be deemed as relevant. These problems call for alternative methods that work not only at the lexical level of the documents, but also at the meaning level.</p><p>Any attempt to work at the meaning level must solve the problem that, while words occur in a document, meanings do not, since they are often hidden behind words. For example, for the query "apple", some users may be interested in documents dealing with "apple" as a "fruit", while some other users may want documents related to "Apple computers". Some linguistic processing is needed in order to provide a more powerful "interpretation" both of the user needs behind the query and of the words in the document collection. This linguistic processing may result in the production of semantic information that provide machine readable insights into the meaning of the content.</p><p>As shown by the previous example, named entities (people, organizations, locations, etc.) mentioned in the documents constitute important part of their semantics. Therefore, in our interpretation, semantic information could be captured from a text by looking at word meanings, as they are described in a reference dictionary (e.g. WordNet <ref type="bibr" coords="2,373.20,351.12,10.30,8.74" target="#b4">[5]</ref>), as well as named entities.</p><p>We propose an IR system which manages documents indexed at multiple separate levels: keywords, senses (word meanings), and entities. The system is able to combine keyword search with semantic information provided by the two other indexing levels. In particular, for each level:</p><p>1. a local scoring function is used in order to weigh elements belonging to that level according to their informative power;</p><p>2. a local similarity function is used in order to compute document relevance by exploiting the above-mentioned scores.</p><p>Finally, a global ranking function is defined in order to combine document relevance computed at each level. The rest of the paper is structured as follows. The N-levels model used in SENSE is described in Section 2, while Section 3 presents an overview of the meaning level. A brief description of the global ranking function is given in Section 4, followed by the details of the system setup for the CLEF competition in Section 5. Finally the experiments are described in Section 6. Conclusions and future work close the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">N-levels model</head><p>According to <ref type="bibr" coords="2,149.53,594.56,9.96,8.74" target="#b0">[1]</ref>, an IR model is a quadruple:</p><formula xml:id="formula_0" coords="2,268.41,613.52,66.18,8.74">D, Q, F, R(q, d)</formula><p>where:</p><p>• D is a set composed of logical views (or representations) for the documents in the collection;</p><p>• Q is a set composed of logical views (or representations) for user information needs. Such representations are called queries;</p><p>• F is a framework for modeling document representations, queries, and their relationships;</p><p>• R(q, d) is a ranking function which associates a real number with a query q ∈ Q and a document representation d ∈ D. Such a ranking defines an ordering among the documents with respect to the query q.</p><p>For the classic vector model, the framework F is composed of a t-dimensional vectorial space and standard linear algebra operations on vectors. In this model, tf-idf schemes are used to weigh index terms in documents D and queries Q. Term weights are used to compute the degree of similarity between each document d in the collection and the user query q, according to a ranking function R(q, d).</p><p>We propose an extension of the classical Vector Space Model <ref type="bibr" coords="3,368.97,171.80,10.52,8.74" target="#b5">[6]</ref> called N-levels model in which documents are represented at different levels. Each level corresponds to a logical view that aims at describing one of the possible spaces in which documents are represented.</p><p>The lexical space of the vector model is retained and semantic spaces are added, each one providing different information about the meaning of the content.</p><p>Each level is described by means of a specific type of features, where each feature is defined as a prominent attribute or aspect of the document. The model is currently implemented in SENSE, SEmantic N-levels Search Engine, an IR system in which three different levels are considered, corresponding to as many different types of features: keywords, word meanings and named entities. Each document at each level is represented by a Bag-of-Features (BOF), a vector of weights assigned to homogeneous features.</p><p>More formally, given D = {d 1 , . . . , d |D| } the document collection and N the number of levels, the document d k is represented by N vectors:</p><formula xml:id="formula_1" coords="3,213.69,331.44,299.31,18.60">---→ BoF i k = (w i 1,k , . . . , w i |Vi|,k ) i = 1, . . . , N<label>(1)</label></formula><p>where V i is the vocabulary of the features at the i-th level, and w i m,k is the weight of the m-th feature at the i-th level for document d k .</p><p>Analogously, N query vectors (one for each level) are used for representing queries. The N query vectors are not necessarily extracted simultaneously from the original keyword query issued by the user: A query vector can be obtained when needed. For example, the ranked list of documents for the query "Apple growth" might contain documents related to both the growing of computer sales by Apple Inc. and the growth stages of apple trees. Then, when the system collects the user feedback (for instance, a click on a document in which "Apple" has been recognized as a named entity), the query vector for the named entities level will be produced.</p><p>Given these representations, we need to define a strategy to compute both w i m,k and R(q, d) (the degree of similarity between query and document). The weighting scheme for computing w i m,k must be different for each type of feature. The adoption of a simple adjustment of tf-idf for semantic levels would result in a loss of the semantics that we want to capture by our model. More advanced strategies should be adopted in order to take into account the inherent informative power of each specific kind of feature. We call these strategies local scoring functions. Section 3.1 describes the local scoring function defined for weighting features (represented as WordNet synsets) at the word meaning level (hereafter meaning level, for brevity). As regards queries, binary weights are adopted.</p><p>We have also to extend the notion of relevance R(q, d) in order to enhance keyword search with semantic information. Therefore, the degree of similarity between q and d must be evaluated at each level by defining a proper local similarity function that computes document relevance according to the weights defined by the corresponding local scoring function. Section 3.2 describes the local similarity function defined for the word meaning level. Since the final goal is to obtain a single list of documents ranked in decreasing order of relevance, a global ranking function is needed to merge all the result lists that come from each level. This function is independent of both the number of levels and the specific local scoring and similarity functions because it takes as input N ranked lists of documents and produces a unique merged list of the most relevant documents. Section 4 describes the adopted global ranking function.</p><p>To meet all the requirements of the proposed model, we implemented an extension of the Lucene API <ref type="foot" coords="3,153.08,703.95,3.97,6.12" target="#foot_0">1</ref> . Lucene is a full-featured text search engine library that implements the vector space model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Meaning Level</head><p>In SENSE, features at the meaning level are synsets obtained from WordNet, a semantic lexicon for the English language. It groups English words into sets of synonyms called synsets, provides short general definitions (glosses), and records various semantic relations between these synonym sets. WordNet distinguishes between nouns, verbs, adjectives and adverbs because they follow different grammatical rules. Each synset is assigned with a unique identifier and contains a set of synonymous words or collocations; different senses of a word are in different synsets. The meaning of a synset is further clarified by short defining glosses. A typical example of a synset with gloss is:</p><p>01722233 good, right, ripe -(most suitable or right for a particular purpose; "a good time to plant tomatoes"; "the right time to act"; "the time is ripe for great sociological changes")</p><p>In order to assign synsets to words, the original system adopted a Word Sense Disambiguation (WSD) strategy. In the case of CLEF, the system used the synsets provided by the organizers of the Ad-Hoc Robust WSD task. The provided documents contain for each word a list of the possible synsets with a confidence factor. We use this factor to weigh the synset in the meaning index structure.</p><p>The idea behind the adoption of WSD is that each document is represented, at the meaning level, by the senses conveyed by the words in its content, together with their respective occurrences. Documents are represented by using a synset-based vector space. Consequently, the BOF at the meaning level is indeed a bag-of-synsets. The vocabulary at this level is the set of distinct synsets recognized by the WSD procedure in the collection, while w i m,k in ( <ref type="formula" coords="4,389.96,360.99,4.24,8.74" target="#formula_1">1</ref>) is the weight of the m-th synset for document d k , computed according to the local scoring function defined in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Synset Scoring Function</head><p>Given a document d i and its synset representation computed by the WSD procedure, X = [s 1 , s 2 , . . . , s k ], the basic idea is to compute a partial weight for each s j ∈ X, and then to improve this weight by finding out some relations among synsets belonging to X.</p><p>The partial weight, called sfidf (synset frequency, inverse document frequency), is computed according to a strategy resembling the tf-idf score for words:</p><formula xml:id="formula_2" coords="4,217.14,499.32,295.86,38.18">sfidf(s j , d i ) = tf(s j , d i ) synset frequency • log | C | n j IDF<label>(2)</label></formula><p>where | C | is the total number of documents in the collection and n j is the number of documents containing the synset s j . tf(s j , d i ) computes the frequency of s j in document d i .</p><p>Finally, the synset confidence factor (α) is used to weigh the sfidf value. Thus, the final local score for synset s j in d i is:</p><formula xml:id="formula_3" coords="4,255.24,592.26,257.76,9.65">sfidf(s j , d i ) • (1 + α)<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Synset Similarity Function</head><p>The local similarity functions for both the meaning and the keyword levels are computed using a modified version of the LUCENE default document score. For the meaning level, both query and document vectors contain synsets instead of keywords. Given a query q and a document d i , the synset similarity is computed as:</p><formula xml:id="formula_4" coords="4,174.31,698.22,338.70,19.61">synsim(q, d i ) = C(q, d i ) • sj ∈q (sfidf(s j , d i )(1 + α) • N (d i ))<label>(4)</label></formula><p>where:</p><p>• sfidf(s j , d i ) and α are computed as described in the previous section;</p><p>• C(q, d i ) is the number of query terms in d i .</p><p>• N (d i ) is a factor that takes into account document length normalization;</p><p>4 Global Ranking Given a query q, each local similarity function produces a local ranked list of relevant documents. All the local lists must be merged in order to give a single ranked list to the user. The global ranking function is devoted to this task. Algorithms for merging ranked lists are widely used by meta-search engines, which send user requests to several search engines and aggregate results into a single list <ref type="bibr" coords="5,419.36,232.34,9.96,8.74" target="#b3">[4]</ref>. Our strategy for defining the global ranking function is thus inspired by prior work on meta-search engines.</p><p>Formally, we define:</p><p>• U : the universe, that is the set containing all the distinct documents in the local lists;</p><p>• τ j ={ x 1 ≥ x 2 ≥ . . . ≥ x n }: the j-th local list, j = 1, . . . , N , defined as an ordered set S of documents, S ⊆ U , ≥ is the ranking criterion defined by the j-th local similarity function;</p><p>• τ j (x i ): a function that returns the position of x i in the list τ j ;</p><p>• s τj (x i ): a function that returns the score of x i in τ j ;</p><p>• w τj (x i ): a function that returns the weight of x i in τ j .</p><p>Two different strategies can be adopted to obtain w τj (x i ), based on the score or the position of x i in the list τ j . Since local similarity functions may produce scores varying in different ranges, and the cardinality of lists can be different, a normalization process (of scores and positions) is necessary in order to produce weights that are comparable.</p><p>The aggregation of lists in a single one requires two steps: The first one produces the N normalized lists and the second one merges the N lists in a single one denoted by τ . The two steps are thoroughly described in <ref type="bibr" coords="5,246.23,447.21,9.96,8.74" target="#b1">[2]</ref>. After tuning experiments we choose to adopt Z-Score normalization and ComSUM respectively as score normalization and rank aggregation function. In particular the Z-Score normalization is computed using the following formula:</p><formula xml:id="formula_5" coords="5,253.74,485.72,92.54,19.92">w τj (x i ) = s τ j (xi)-µ s τ j σ s τ j</formula><p>Regarding ComSUM list aggregation method, the score of document x i in the global list is computed by summing all the normalized scores for x i :</p><formula xml:id="formula_6" coords="5,251.29,540.38,100.42,12.72">ψ(x i ) = τj ∈R w τj (x i )</formula><p>where R is the set of all local lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">System Setup</head><p>We adopted the SENSE model to build our IR system for CLEF evaluation. We used two different levels: keyword level using word stems and word meaning level using WordNet synsets. All the SENSE components involved in the experiments are implemented in JAVA using the last available version of Lucene API (2.3.2). Experiments were run on an Intel Core 2 Quad processor at 2.4 GHz, operating in 32 bit mode, running Linux (UBUNTU 7.10), with 2 GB of main memory.</p><p>In according to CLEF guidelines we performed two different tracks of experiments: Ad-Hoc Mono-language and Cross-language. Each track required two different evaluations: with and without synsets. We exploited several combination of levels and queries expansion methods, especially for the meaning level. All query expansion methods are automatic and do not require manual operations. Moreover we used different boosting factors for each field contained into the topic. In this way we give more importance to the terms in the fields TITLE and DESCRIPTION.</p><p>In particular for the Ad-Hoc Mono-language track we performed the following runs:</p><p>1. MONO1TDnus2f : the query is built using word stems in the fields TITLE and DESCRIP-TION of the topics. All query terms are joined adopting the OR boolean operator. The terms in the TITLE field are boosted using a factor 2;</p><p>2. MONO11nus2f : similar to the previous run but in this case we add the NARRATIVE field and adopt different term boosting values: 4 for TITLE, 2 for DESCRIPTION and 1 for NARRATIVE. These boost factors are used for all the following runs;</p><p>3. MONO12nus2f : for this instance we adopt the Lucene Phrase Query in addition to the query expansion described in MONO11nus2f. This kind of queries are able to exploit terms proximity in the computation of relevance score. We build proximity query using the terms contained into the TITLE and DESCRIPTION fields. In detail: for TITLE we build a proximity query using all the terms into the field, while for DESCRIPTION we build a proximity query for each sentence;</p><p>4. MONO13nus2f : as the previous run but we adopt a different strategy to build Phrase Query. We exploit PoS-tag in order to build proximity queries. We produce a proximity query for each sequence of PoS-tags that matches the following patterns: adjective-nounverb, verb-adjective-noun, verb-noun, noun-verb and adjective-noun. For example into the sentence: 'The wrapping artist Christo took two weeks...' we build a proximity query using the following terms: "artist Christo took" ; 5. MONO14nus2f : this experiment adopts a combination of all the previous methods;</p><p>6. MONOwsd1nus2f : the query is built expanding the synsets in the TITLE and DESCRIP-TION fields of the topics. This run exploits the hypernyms and hyponyms. In particular, we include only the direct hyponyms and the hypernyms that have a path less or equal to two. For synsets we adopt a different boost factor taking into account both the field and synsets distance;</p><p>7. MONOwsd11nus2f : in this instance each word is expanded using the whole set of synsets in WordNet and we compute a boosting factor using the ZIPF distribution that approximates properly the natural distribution of meanings. The ZIPF formula is:</p><formula xml:id="formula_7" coords="6,264.17,492.96,97.88,21.72">f (k; N ; s) = 1/k s N n=1 1/n s</formula><p>where:</p><p>• N is the number of synsets;</p><p>• k is the synset rank. The synsets in WordNet are ranked according to the their frequency in a reference corpus;</p><p>• s is the value of exponent characterizing the distribution: after tuning experiments we set s equal to 2.</p><p>8. MONOwsd12nus2f : in this experiment we exploit the N-levels architecture of SENSE.</p><p>For keyword level we adopt the query expansion described in MONO14nus2f and for word meaning level the MONOwsd1nus2f; 9. MONOwsd13nus2f : as the previous run but, for the word meaning level we adopt the method described in MONOwsd11nus2f.</p><p>For the Ad-Hoc Cross-language track we performed the following runs:</p><p>1. CROSS1TDnus2f : the query is built using word stems in TITLE and DESCRIPTION fields of the topics. In Cross-language track the topics are in Spanish, thus a translation of terms in English is required. The SENSE system was not developed for Cross-language retrieval and in this instance we adopted a very trivial method in order to translate the query in English. We exploited WordNet dictionary to translate a word. In detail, we query Spanish WordNet using the Spanish word w s and retrieve the whole set of synsets S related to the word w s ; then we use the set S to query English WordNet and retrieve, for each synset in S, the set of the English synonyms W e . Finally, we build the query using the words in W e . The boost factors have the same values used in the Mono-language track; 2. CROSS1nus2f : as described in the previous run adding the NARRATIVE field;</p><p>3. CROSSwsd1nus2f : in this case we adopt the same method presented in MONOwsd1nus2f but we use directly the synsets in Spanish Topic. It is important to notice that terms in a Spanish query are disambiguated using the first sense in Spanish WordNet;</p><p>4. CROSSwsd11nus2f : in this instance we exploit the N-levels architecture. For the keyword level we adopt the method described in CROSS1nus2f and for word meaning level the method proposed in MONOwsd1nus2f;</p><p>5. CROSSwsd12nus2f : this run differs from the CROSSwsd11nus2f for the use of a different Spanish-English translation method. We use directly the Spanish WordNet synset instead of the Spanish word. We query English WordNet using the synsets into the topic and retrieve, for each synset, the set of synonymous English words.</p><p>For all the runs we remove the stop words from both the index and the topics. In particular, we built a different stop words list for topics in order to remove non-informative words such as find, reports, describe that occur with high frequency in topics and are poorly discriminating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Session</head><p>The experiments were carried out on the CLEF Ad-Hoc WSD Robust dataset derived from the English CLEF data, which comprises corpora from "Los Angeles Times" and "Glasgow Herald", amounting to 166, 726 documents and 160 topics in English and Spanish. The relevance judgments were taken from CLEF.</p><p>The goal of our evaluation is to prove that the combination of two levels outperforms a single level. In particular, the combination of keyword and meaning levels is more effectiveness than the keyword level alone.</p><p>To measure retrieval performance, we adopted Mean-Average-Precision (MAP) calculated by the CLEF organizers using the DIRECT system on the basis of 1,000 retrieved items per request. Table <ref type="table" coords="7,117.40,543.37,4.98,8.74">1</ref> shows the results for each run with an overview on the exploited features. We have described and tested SENSE (SEmantic N-levels Search Engine), a semantic N -levels IR system which manages documents indexed at multiple separate levels: keywords and meanings. The system is able to combine keyword search with semantic information provided by the other indexing level.</p><p>The distinctive feature of the system is that, differently from the previous approaches, an adaptation of the vector space model is proposed to integrate, rather than simply replace, the lexical space with semantic spaces. We provided a detailed description of the SENSE model, by defining a local scoring function, a local similarity function for synsets and a global ranking function in order to merge rankings produced by different levels.</p><p>We performed an intensive evaluation using the CLEF Ad-Hoc Robust WSD dataset. This dataset supplies both words and synsets for each document and it is the ideal framework to evaluate the N-levels architecture. The experiments show that the N-levels model is effective when the word meaning level is involved.</p><p>As future research we plan to improve the performance of the system. We can achieve this goal adopting two different strategies: The former involves the change of the relevance function implemented in Lucene; the latter exploits the possibility to replace vector space model with a more effective IR model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,222.60,742.48,157.80,8.74;7,188.11,564.07,226.78,163.29"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Precision vs. Recall graph</figDesc><graphic coords="7,188.11,564.07,226.78,163.29" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,736.41,95.51,6.99"><p>http://lucene.apache.org/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run MONO CROSS N-levels WSD MAP</head><p>The results confirm our hypothesis: The combination of two levels outperforms a single level. In particular, the combination of keyword and meaning levels (MONOwsd12nus2f and MONOwsd13nus2f) is more effectiveness than the single keywords level (MONO1TDnus2f and MONO11nus2f). If we consider MONO1TDnus2f as baseline, we obtain an improvement of 35% in precision using the N-levels model (MONOwsd13nus2f). The behavior of the two systems is shown in Figure <ref type="figure" coords="8,163.28,382.30,7.75,8.74">1:</ref> The N-levels model outperforms the keyword level at all values of recall.</p><p>It is interesting to notice that just the use of the word meaning level alone is able to outperform the keyword level. This result has a motivation: We chose to index all the synsets for each word (not only the synset with the highest confidence factor). This intuition makes the retrieval process easier.</p><p>Regarding the Cross-language track, our system achieves a low precision. This was an expected result because our system is not designed specifically for this kind of task. Moreover, the method adopted for topic translation is based only on the use of WordNet as dictionary. In particular, performance of the Cross-language without WSD (experiments: CROSS1TDnus2f and CROSS1nus2f) are not satisfying because the system exploits only keywords and the translation process introduces a lot of wrong terms into the query, producing a noise effect. Conversely, the word meaning level is able to help the retrieval process, as shown in CROSSwsd1nus2f where we used only the word meaning level (without keywords). In the second attempt (CROSSwsd11nus2f) we combined the keyword level with the word meaning level obtaining worse results due to the keyword translation method (as in CROSS1TDnus2f). Finally, we tried to translate the Spanish words using directly the synsets obtaining a good result with respect to the previous one.</p><p>We noticed that our system has a low precision with respect to the other participants to the CLEF competition. This is due to the standard relevance function implemented in Lucene and this result was expected. In particular, Lucene performance decreases when the number of terms in the query grows. In fact, the experiment MONO14nus2f produces large queries and results point out that the system achieves a low precision in this experiment with respect to the others that rely exclusively on keywords. This problem also affects the Cross-language experiments because we translate a Spanish word using all the possible English translations (CROSS1TDnus2f) producing a query with a lot of terms. Details concerning this well known behavior of Lucene can be found in <ref type="bibr" coords="8,101.44,669.22,9.96,8.74" target="#b2">[3]</ref>. Nonetheless, the goal of our evaluation was to prove the effectiveness of the N-levels model and the experiments confirm our hypothesis.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,105.50,379.89,400.91,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,271.06,379.89,129.54,8.74">Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,399.82,407.50,8.74;9,105.50,411.77,407.50,8.74;9,105.50,423.73,407.50,8.74;9,105.50,435.68,349.99,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,198.70,411.77,293.88,8.74">Enhancing semantic search using n-levels document representation</title>
		<author>
			<persName coords=""><forename type="first">Pierpaolo</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Annalina</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><forename type="middle">Lisa</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Degemmis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pasquale</forename><surname>Lops</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giovanni</forename><surname>Semeraro</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,169.43,435.68,126.92,8.74">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">Stephan</forename><surname>Bloehdorn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marko</forename><surname>Grobelnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Mika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Duc</forename><surname>Thanh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tran</forename></persName>
		</editor>
		<meeting><address><addrLine>SemSearch</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">334</biblScope>
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,455.61,407.51,8.74;9,105.50,467.56,371.26,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,277.26,455.61,231.59,8.74">Lucene and juru at trec 2007: 1-million queries track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amitay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,117.95,467.56,281.19,8.74">Proceedings of the 16th Text REtrieval Conference (TREC 2007)</title>
		<meeting>the 16th Text REtrieval Conference (TREC 2007)</meeting>
		<imprint>
			<date type="published" when="2007-11">November 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,487.49,407.51,8.74;9,105.50,499.44,407.50,8.74;9,105.50,511.40,276.94,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,301.04,487.49,211.96,8.74;9,105.50,499.44,88.25,8.74">An outranking approach for rank aggregation in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Mohamed</forename><surname>Farah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Vanderpooten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,227.65,511.40,25.85,8.74">SIGIR</title>
		<editor>
			<persName><surname>Wessel Kraaij</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Arjen</surname></persName>
		</editor>
		<editor>
			<persName><surname>De Vries</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Norbert</forename><surname>Clarke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noriko</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><surname>Kando</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="591" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,531.32,397.75,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,165.91,531.32,169.31,8.74">Wordnet: a lexical database for english</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,343.74,531.32,65.57,8.74">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,551.25,407.50,8.74;9,105.50,563.20,183.98,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,155.13,551.25,353.52,8.74">The SMART Retrieval System -Experiments in Automatic Document Processing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
