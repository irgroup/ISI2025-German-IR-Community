<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.01,146.21,310.96,18.08;1,241.63,168.13,119.75,18.08">XRCE&apos;s Participation to CLEF 2008 Ad-Hoc Track</title>
				<funder ref="#_2cvXxJf">
					<orgName type="full">European Community</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,202.25,203.19,85.24,10.46"><forename type="first">Stephane</forename><surname>Clinchant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 ch. de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.18,203.19,90.58,10.46"><forename type="first">Jean-Michel</forename><surname>Renders</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 ch. de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.01,146.21,310.96,18.08;1,241.63,168.13,119.75,18.08">XRCE&apos;s Participation to CLEF 2008 Ad-Hoc Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5FD58DAE81BB66DA9228A4CDD6E0073A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Cross Lingual Information Retrieval, Lexicon Extraction, Query Translation and Disambiguation, Dictionary Adaptation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our participation to CLEF2008 (Ad-Hoc Track, TEL Subtask) was an opportunity to develop and assess methods that tackle multilinguilality in a principled -while rather simple -way. It was also an opportunity to demonstrate the effectiveness of the dictionary adaptation method we designed last year in the case of the domainspecific track. Unfortunately, it turned out that several mistakes we accumulated in our implementation impacted significantly and negatively the performance of our submitted runs. We nevertheless decided to experiment extra runs, that we designed to (partially) compensate for the errors made in the official runs and whose performance are reported in this working note. These results are quite satisfying, as they reach (or exceed) the level of the other best participants for the bilingual tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This article describes our participation to the Ad-Hoc Track (TEL Subtask). Our very first motivation was to try to tackle multilinguality in a principled way: this is the object of the next section. Then, we explain the general methodological steps that we followed in our runs. A specific section is devoted to the analysis of the performances and the mistakes of our official runs. Indeed, it appeared after the publication of the results that we accumulated several "bugs" (or errors) that significantly impacted the performance of our methods, so that these are not directly comparable with the other ones. Still, in order to be constructive, we took some actions in order to compensate for these errors after the submission and we present in the last section of this note some new results achieved by runs taking inspiration from the dictionary adaptation algorithm that we proposed last year <ref type="bibr" coords="1,209.16,677.94,9.96,10.46" target="#b0">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dealing with Multilingual Documents</head><p>The framework of our retrieval experiments is the Language Model approach to Information Retrieval <ref type="bibr" coords="1,120.80,744.16,9.96,10.46" target="#b2">[4]</ref>. The TEL collections are clearly multilingual: a document can be described by French words in a field and in German in an other field. Following the language modelling approach, we decide not to split a document into parts according to the language: a document is a sequence of tokens, which may be of any language; accordingly, a single language model is associated to the document, which is a probability distribution over the words (actually lemma's) of three concatenated vocabularies (English, French and German). In the following, this concatenation of vocabularies will be called the "meta-language". Thus, the feature space of different languages is aggregated into a single description space. This way, we do not build different indexes for a collection (according to the identified languages) but a single index is built containing all the languages.</p><p>However, building a single index to cope with multilinguality is just halfway to the solution, as the query is in general expressed only in one language. Indeed, since collections are multilingual, a query word need to be translated into the "meta-language", including its original language. This is done by building probabilistic meta-dictionaries (from a single source language to the metalanguage). To be more concrete, here is a simplified excerpt of a probabilistic meta-dictionary we used: roman(English) Latein(German) 0.02 roman(English) roman(English) 0.8 roman(English) antiqua(German) 0.01 roman(English) lateinisch(German) 0.02 roman(English) roemisch(German) 0.05 roman(English) romain(French) 0.1 Gauguin(English) Gauguin(English) 0.8 Gauguin(English) Gauguin(German) 0.1 Gauguin(English) Gauguin(French) 0.1 This probabilistic dictionary is built as a combination of a monolingual resource (thesaurus) and bilingual lexicons extracted from parallel corpora (in our case, the JRC-AC corpus<ref type="foot" coords="2,484.14,424.28,3.97,7.32" target="#foot_0">1</ref> ) and completed by approximate string matching equivalences (for lemmas not covered by the JRC-AC corpus). An important issue is how to weight the different translation probabilities when we merge the monolingual thesauri and the pair-wise bilingual dictionaries. We have chosen to merge them linearly. We believe that those linear weights should depend on the target collection and the task given. A natural choice, that we propose, is to give more weight to the official language of the target collection (French for BNF, German for ONB and English for BL). Formally, suppose that we are targeting the BL collection (whose official language is English), then the value P (E j |E i ) that represents the fact that English word E j will be used as substitute (synonym) for E i , will be weighted by α (typically, α=0.8); the value P (F j |E i ) that represents the fact that French word F j will be used as substitute (translation) for E i , will be weighted by 1 -α/2 and similarly for the entry P (G j |E i ). Note that, as P (E j |E i ), P (F j |E i ) and P (G j |E i ) individually sum up to 1 (over j) for a given E i , the new probabilities also sum up to 1.</p><p>Once the meta-dictionary is built from these standard monolingual and bilingual resources, we propose to adapt it for a specific (query, target collection) pair, following the method we presented last year <ref type="bibr" coords="2,129.19,604.68,9.96,10.46" target="#b0">[2]</ref>. This amounts to filter out irrelevant, spurious meta-translations, as well as increasing the probabilities of more coherent word translations or synonyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pre-processing and global approach</head><p>We have participated to all 'monolingual' and 'bilingual' tasks. None of the tasks were truly monolingual or bilingual, which motivated our method to cope with multilinguality.</p><p>For the 3 main languages (English, German, French), we used our home-made lemmatiser and word-segmenter (decompounder) for German. From the fields available for a document record, we only kept the title as well as the subject fields. Classical stopword removal was performed. As All our runs consisted in the following methodological steps:</p><p>• meta-translating the query with the multilingual meta-dictionary,</p><p>• adapting the meta-dictionary during a first pseudo-feedback step (details of this are given later),</p><p>• and finally applying another classical (monolingual) pseudo-feedback step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Mistakes in the submitted runs</head><p>In this section, we present the analysis of the mistakes we did in our official runs. The first one stemmed from a misunderstanding of what is considered as "bilingual" in the TEL task. When we preprocessed documents, we made the wrong hypothesis that only documents whose language is either French, English or German should be kept. As a consequence, we did not index documents whose title and content are indicated to belong to another language (Italian, Spanish, . . . ), even if they had a subject field in one of the three main languages. Te post analysis shows that we lost a significant number of relevant documents at indexing time, with respect to the given queries. Table <ref type="table" coords="3,201.44,458.40,4.98,10.46" target="#tab_0">1</ref> shows for each collection the count of relevant documents we lost at indexing time with respect to the total number of relevant documents.</p><p>The second error we made was to weight more the source language instead of the target language through the α parameter when building the meta-dictionary, i.e. we built one meta-dictionary per possible query (source) language giving more weight to this source language, instead of building one meta-dictionary per collection giving more weight to the official language of the collection.</p><p>Last, but not least, the third mistake we did, happened when we meta-translated the queries. Recall that we need to translate a query even in the 'monolingual' setting to address the fact that the collections are multilingual. We used a mixture model to achieve this effect: P (w|q) = βP 0 (w|q) + (1 -β) q j ∈q P (w|q j )P (q j |q)</p><p>(1)</p><p>where P (w|q j ) is given by our meta-dictionary and P 0 (w|q) is the initial language model of the query (obtained by maximum-likelihood estimation, with non-null values only for words of the source language). The β parameter controls the "weight of meta-translation" given to other languages and to a thesaurus (if any). In the scenario of 'monolingual' runs, we kept the β values high (between 0.8 and 0.9). The mistake we did in our 'bilingual' runs was to forget to change this β value to smaller values (between 0 and 0.2) in order to have a real effect of translation. All these factors explain why our runs performed relatively poorly. In the last section (before conclusion), we briefly present some new runs and their results, that partially compensate for these errors. Before this, for the sake of completeness, we describe our dictionary adaptation method, that was already used last year (in the domain-specific track).</p><p>We briefly recall the model underlying our dictionary adaptation method <ref type="bibr" coords="4,420.05,132.36,9.96,10.46" target="#b0">[2]</ref>. As already mentioned, the Language Modelling approach to information retrieval was adopted for our experiments. Crosslingual retrieval models translate the query into a query language model in the target language <ref type="bibr" coords="4,160.00,168.22,9.96,10.46" target="#b1">[3]</ref>. Then a monolingual search is performed, using a ranking criterion such as the Cross-Entropy:</p><formula xml:id="formula_0" coords="4,185.44,202.31,327.56,21.33">CE(q s |d t ) = wt,ws P (w t |w s )P (w s |q s ) log P (w t |d t )<label>(2)</label></formula><p>The main idea of dictionary adaptation is to be able to adapt the entries of a dictionary to a query and a target corpus. Formally, let q s = (w s1 , . . . , w sl ) be the query in source language. Our input data are an initial source query language model p(w s |q s ) and a first dictionary p(w t |w s ). First of all, the source query is translated with all dictionaries entries. Then, we select the top n documents (pseudo-relevance feedback) and we model the set of feedback documents F with a generative model from which we learn a new dictionary θ st : we see each document as the outcome of a multinomial random variable. First, the likelihood of the pseudo-feedback set can be written:</p><formula xml:id="formula_1" coords="4,167.80,323.96,345.20,25.31">P (F|θ) = k wt λ( ws θ st p(w s |q s ) + (1 -λ)P (w t |C) c(wt,d k ) (3)</formula><p>As described in <ref type="bibr" coords="4,158.22,358.07,9.96,10.46" target="#b0">[2]</ref>, the new dictionary θ st can be learned by EM and a new query can be generated by using all entries in the adapted dictionary.</p><p>In all experiments reported in this note, the value of n was chosen as 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Unofficial Runs</head><p>We performed a set of extra runs, with the aim to be comparable with the results of other participants and to compensate for the effects of the mistakes and bugs we identified. In order to get rid of the issue of weighting more one language with respect to the other ones (selection of the α and β parameters) -things that we did in a completely erroneous way in our official runs -, we decided to make a simplifying assumption, namely that 'bilingual runs' are considered as really bilingual, with known source and target languages. In other words, we considered only the French part of BNF, the English part of BL and the German part of ONB and used purely bilingual dictionaries (which were subsequently adapted). A post-analysis on relevant documents shows that this assumption is not unreasonable: In order to compensate for the forgetting of documents in the index (documents whose title/content is not in French, German nor English), we simply removed non-indexed documents from the relevance assessment lists.</p><formula xml:id="formula_2" coords="4,90.01,552.88,15.69,10.46">For</formula><p>Table <ref type="table" coords="5,131.69,329.92,4.98,10.46" target="#tab_1">2</ref> shows the corrected runs using the dictionary adaptation using total translation (β = 0 in equation 1). The second column of the table shows the source and target languages we used for the runs. Our runs could achieve better results if we took into account the other languages and if we performed an additional step of classical pseudo-feedback, but this is left for further experiments. Results are given without and after adaptation. For completeness, we also give the results on the unrestricted relevance list (columns 3 and 4), while the MAP corresponding to the restricted collection (documents whose title/content is not in French, German nor English are removed from the relevance assessment lists) are given in columns 5 and 6.</p><p>Assuming that the documents we removed from the collection are completely random with respect to the queries and that there are no performance bias due to the nature of the removed documents, we can expect from the results given in columns 5 and 6 to be comparable with the performance of other participants. These results are very encouraging, as they first show clearly the beneficial effect of dictionary adaptation and by the fact that we achieve results more or less equivalent to the best results of the other participants (to be more precise, we are just behind the best one for the BL as target collection, and better than the first one for the ONB and BNF collections).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Our work was concerned about dealing with multilinguality in a principled way. Our goal was to get a single retrieval model and index for all the languages of one specific collection. However, this approach required to give weights to each language to merge dictionaries at retrieval time. While assigning such weights requires prior knowledge about the collections, the dictionary adaptation mechanism provides a partial solution to this problem, adapting weights to each query. This year, the accumulation of some mistakes rendered our official runs relatively inefficient. We presented the reasons of these mistakes and corrected partly some of them in a set of extra unofficial runs whose performances are among the best ones; they demonstrated that dictionary adaptation is effective for the TEL task and corpora. Further work will require re-processing the collections to keep the document we lost. We will also need to come back to a true multilingual setting by solving the issue of weighting differently the basic bilingual lexicons and monolingual thesauri, according to the target collection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,118.01,423.02,128.72"><head>Table 1 :</head><label>1</label><figDesc>(Lost) Relevant Documents for each collection Collection # of relevant documents # of relevant documents not indexed we used the Open Office thesauri 2 . As multilingual resources, we used a probabilistic dictionary, called ELRAC, that is a combination of a very standard one (ELRA) and a lexicon automatically extracted from the parallel JRC-AC (Acquis Communautaire) Corpus. Finally, we carried out our experiments relying on the Lemur Toolkit [1].</figDesc><table coords="3,90.01,142.61,307.38,68.25"><row><cell>BL</cell><cell>2533</cell><cell>240</cell></row><row><cell>BNF</cell><cell>1339</cell><cell>108</cell></row><row><cell>ONB</cell><cell>1637</cell><cell>69</cell></row><row><cell>monolingual resource,</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,552.88,298.14,201.74"><head>Table 2 :</head><label>2</label><figDesc>Dictionary Adaptation Experimental Results in Mean Average Precision -(1) refers to the unrestricted collection, while (2) refers to the indexed collection</figDesc><table coords="4,90.00,552.88,298.14,201.74"><row><cell>the BL collection:</cell></row><row><cell>number of relevant documents entirely in German : 24</cell></row><row><cell>number of relevant documents in English and German : 78</cell></row><row><cell>number of relevant documents entirely in French : 4</cell></row><row><cell>number of relevant documents completely in English : 2066</cell></row><row><cell>number of relevant documents in French and English : 122</cell></row><row><cell>For the BNF collection:</cell></row><row><cell>number of relevant documents entirely in German : 2</cell></row><row><cell>number of relevant documents in French and German : 11</cell></row><row><cell>number of relevant documents entirely in French : 1008</cell></row><row><cell>number of relevant documents completely in English : 12</cell></row><row><cell>number of relevant documents in French and English : 198</cell></row><row><cell>For the ONB collection:</cell></row><row><cell>number of relevant documents entirely in German : 1241</cell></row><row><cell>number of relevant documents in French and German : 29</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,738.49,145.92,8.37"><p>Available on http://wt.jrc.it/lt/Acquis/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.24,737.66,243.24,8.37"><p>Available on http://wiki.services.openoffice.org/wiki/Dictionaries</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Aknowledgments</head><p>This work was partly supported by the <rs type="programName">IST Programme of the</rs> <rs type="funder">European Community</rs>, under the <rs type="projectName">SMART</rs> project, <rs type="grantNumber">FP6-IST-2005-033917</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_2cvXxJf">
					<idno type="grant-number">FP6-IST-2005-033917</idno>
					<orgName type="project" subtype="full">SMART</orgName>
					<orgName type="program" subtype="full">IST Programme of the</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,105.49,219.01,407.51,10.46;6,105.50,230.96,345.53,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,252.26,219.01,242.34,10.46">Xrce&apos;s participation to clef 2007 -domain specific track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-M</forename><surname>Renders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,105.50,230.96,315.65,10.46">Working Notes of CLEF 2007. Avalaible On-line on the CLEF Web Site</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.49,250.88,407.51,10.46;6,105.50,262.84,337.19,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,274.25,250.88,238.76,10.46;6,105.50,262.84,154.67,10.46">Embedding web-based statistical translation models in cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,268.44,262.84,77.72,10.46">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="381" to="419" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.49,282.76,407.51,10.46;6,105.50,294.72,354.52,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,211.16,282.76,301.84,10.46;6,105.50,294.72,100.84,10.46">A study of smoothing methods for language models applied to ad hoc to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,227.06,294.72,104.28,10.46">Proceedings of SIGIR&apos;01</title>
		<meeting>SIGIR&apos;01</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
