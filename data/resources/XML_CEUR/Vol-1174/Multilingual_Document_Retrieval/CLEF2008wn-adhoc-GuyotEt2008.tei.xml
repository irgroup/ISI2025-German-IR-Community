<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,115.56,98.29,364.08,12.64">UNIGE Experiments on Robust Word Sense Disambiguation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,159.24,139.24,60.63,8.96"><forename type="first">Jacques</forename><surname>Guyot</surname></persName>
							<email>jacques.guyot@unige.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Centre universitaire d&apos;informatique</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<addrLine>Route de Drize 7</addrLine>
									<postCode>1227</postCode>
									<settlement>Carouge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.28,139.24,57.83,8.96"><forename type="first">Gilles</forename><surname>Falquet</surname></persName>
							<email>gilles.falquet@unige.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Centre universitaire d&apos;informatique</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<addrLine>Route de Drize 7</addrLine>
									<postCode>1227</postCode>
									<settlement>Carouge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.08,139.24,66.04,8.96"><forename type="first">Saïd</forename><surname>Radhouani</surname></persName>
							<email>said.radhouani@unige.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Centre universitaire d&apos;informatique</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<addrLine>Route de Drize 7</addrLine>
									<postCode>1227</postCode>
									<settlement>Carouge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,365.64,139.24,70.21,8.96"><forename type="first">Karim</forename><surname>Benzineb</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre universitaire d&apos;informatique</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<addrLine>Route de Drize 7</addrLine>
									<postCode>1227</postCode>
									<settlement>Carouge</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,115.56,98.29,364.08,12.64">UNIGE Experiments on Robust Word Sense Disambiguation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FB9448CE12148DD8952A9BE4FC285D37</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This task was meant to compare the results of two different retrieval techniques: the first one was based on the words found in documents and query texts; the second one was based on the senses (concepts) obtained by disambiguating the words in documents and queries. The underlying goal was to come up with a more precise knowledge about the possible improvements brought by word sense disambiguation (WSD) in the information retrieval process. The proposed task structure was interesting in that it drew up a clear separation between the actors (humans or computers): those who provide the corpus, those who disambiguate it, and those who query it. Thus it was possible to test the universality and the interoperability of the methods and algorithms involved.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="842.0" lry="595.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="842.0" lry="595.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="842.0" lry="595.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="842.0" lry="595.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and Testing Data</head><p>The document corpus was created by merging two collections of English documents : LA Times 94 and Glasgow Herald 95 (166'000 documents with 470'000 unique words and 55 million word occurrences). This corpus was processed with two different word sense disambiguation algorithms: UBC <ref type="bibr" coords="1,370.20,418.48,31.27,8.96">[ubc07]</ref> and NUS <ref type="bibr" coords="1,443.40,418.48,28.95,8.96">[nus07]</ref>, resulting in two different sets. The disambiguation process replaced each occurrence of a term (composed by one or more words) by an XML element containing the term identifier, an extracted lemma, a part-of-speech (POS) tag <ref type="bibr" coords="1,70.92,458.20,99.44,8.96">(noun, verb, adjective…)</ref>, the original word form (WF) and a list of senses together with their respective scores. The senses were represented by WordNet 1.6 synset identifiers. For instance, the word "discovery" could be replaced by: &lt;TERM ID="C041-19" LEMA="discovery" POS="NN"&gt; &lt;WF&gt;discovery&lt;/WF&gt; &lt;SYNSET SCORE="0.33057320536529267" CODE="04475449-N"/&gt; ..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. &lt;/TERM&gt;</head><p>A training set of 150 queries (topics) was provided together with the expected results, as well as a testing set containing 160 queries. As usual, the queries included three parts: a title (T), a description (D) and a narrative (N). The English queries were processed with the UBC and NUS disambiguation algorithms, while the Spanish queries were disambiguated with the first sense heuristics (FSH), i.e. always choosing the first sense available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Indexer</head><p>To index the corpus, we chose the IDX-VLI indexer described in <ref type="bibr" coords="1,350.76,677.80,29.95,8.96">[gfb06]</ref> because it can gather a wealth of information <ref type="bibr" coords="1,122.16,691.00,62.75,8.96">(positions, etc.)</ref>, it has built-in operators and it is remarkably fast. Still, we only used the basic version of that indexer, i.e. we did not use any relevance feedback mechanism, context description or any other sophisticated tool of that sort. We thus avoided interfering with the direct results of the experiment and we facilitated the result analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collection processing</head><p>We developed and tested several document processing strategies on the provided collections. Those strategies were applied to each &lt;TERM&gt; element within each document:</p><p>• NAT : Keep only the word form of each element (i.e. rebuild the original text) • LEM : Keep only the lemma • POS : Keep the lemma and the part-of-speech tag • WSD : Keep only the synset with the best score \footnotemark<ref type="foot" coords="2,361.68,164.05,3.24,5.83" target="#foot_0">1</ref> .</p><p>• WSDL : Keep the best synset and the lemma.</p><p>During the indexing process the strategies were applied to all the terms, including numbers, except for the stopwords. Given the poor performance of the POS approach, we quickly gave up this option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic processing</head><p>The same translators were applied to the queries, with an extended stop-word list including words such as report, find, etc. For each topic we derived three queries:</p><formula xml:id="formula_0" coords="2,88.92,287.06,28.93,12.98">• T :</formula><p>Include only the title part • TD :</p><p>Include the title and description translated terms • TDN : Include the title, description and narrative translated terms.</p><p>In order to come up with a reasonably good base line we tested several approaches to build a Boolean pre-filter from a given topic (results are the mean average precision (MAP) on T):</p><p>• OR (25.5%) :</p><p>The logical OR of the terms (or lemmas) • AND (15.8%) : The logical AND of the terms • NEAR (15.2%) : The logical OR of all the pairs (t i NEAR t j ) where t i and t j are the query terms • AND-1 (23.6%): The logical OR of all the possible conjunctions of terms, except for the conjunction of all the terms.</p><p>The best results in terms of MAP were produced by the OR filtering, followed by the computation of a relevance score based on the Okapi BM25 weighting model (with default parameters). The test was carried out on the titles (T) of 150 training topics. More restrictive filtering schemes were tried out but did not perform any better, probably because of the relatively small size of the corpus.</p><p>Runs with word senses: For the disambiguation-based runs we tried out several other filtering schemes, including:</p><p>• OR (22.4%) :</p><p>The logical OR of the best synset corresponding to a topic term • AND (15.1%) : The logical AND of the best synset corresponding to a topic term • NEAR (12.5%) : The logical OR of all the pairs (s i NEAR s j ) where s i and s j are the best synsets corresponding to a topic term t i and t j • AND-1 (18.8%): The logical OR of all the possible conjunctions of synsets, except for the conjunction of all the synsets • HYPER (14.3%): The logical AND of each (s i OR h i ) where s i is the best synset corresponding to a topic term t i and h i is the direct hypernym of s i in WordNet • ORHYPER(18.43%): The logical OR of each (s i OR h i ) where s i is the best synset corresponding to a topic term t i and h i is the direct hypernym of s i in WordNet.</p><p>However, none of these strategies performed any better than the basic OR filter on terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result summary</head><p>The first table below shows the mean average precision (in percent) calculated on the training queries with different query processing options (disambiguation algorithm and part-of-topic selection) and different document processing options (disambiguation and translation). Of course, the &lt;TERM&gt; processing (NAT, LEM, WSD or WSDL) was always the same throughout the queries and the corpus for a given run.</p><p>The base line was the run with topic selection TDN and term selection LEM (i.e. the whole topic with stemming).</p><p>The second table shows the results of the testing queries, which are slightly better than those of the training queries (maybe the testing queries were somewhat easier).</p><p>The base line (LEM) for the Spanish queries was created by automatically translating the queries from Spanish into English.</p><p>The tests on the NUS corpus produced better results than those on the UBC one. Therefore most of the runs were performed on the NUS corpus, while the UBC corpus would be used to test the interoperability of the disambiguation processes. </p><formula xml:id="formula_1" coords="3,70.92,324.89,11.90,8.44">avg</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Findings and Discussion</head><p>In the tables above we note the following facts:</p><p>• Using the D and N parts-of-topics increases the precision in all cases (with and without WSD). This is probably due to the ranking method which benefits from the additional terms provided by D and N. • On the test run with UBC disambiguation, the senses alone (WSD) decrease the MAP: -4.6% on T queries and -3.1% on TDN. On training requests, adding the lemmas to the senses (WSDL) slightly improves the MAP (+0.6%). This is the only case where disambiguation brings any improvement.</p><p>• Using different disambiguation algorithms for the queries and the documents noticeably decreases the results. This should not happen if the algorithms were perfect. It shows that disambiguation acts as a kind of encoding process on the words, and obviously the best results are obtained when the same encoding, producing the same mistakes, is applied to both queries and documents. Thus, at this stage, the disambiguation algorithms are not interoperable.</p><p>We carefully analyzed about 50 queries to better understand what happened with the disambiguation process. For instance, the query with the title "El Niño and the weather" was disambiguated as follows (NUS):</p><p>• El was understood as the abbreviation el. of elevation • Niño was understood as the abbreviation Ni of nickel, probably because the parser failed on the non-ASCII character ñ • weather was correctly understood as the weather concept.</p><p>Although the disambiguation was incorrect, WSD was as good as LEM because the "encoding" was the same in the collection and in the query and there were few or no documents about nickel that could have brought up noise.</p><p>More generally, when the WSD results were better than the LEM ones, it was not due to semantic processing but to contingencies. For instance, the query title "Teenage Suicides" had a better score with WSD because teenage was not recognized! Thus the query became suicides, which is narrower than teenage OR suicide and, on this corpus, avoids retrieving a large amount of irrelevant documents about teenagers.</p><p>A few items of the test run are commented in Appendix A.</p><p>The poor performance on Spanish queries is due to 1) the above-mentioned lack of interoperability between the different WSD algorithms, and 2) the low quality of the Spanish WSD itself.</p><p>This can be illustrated with some examples: On Question 41: "Pesticide in baby food" is translated by "Pesticidas en alimentos para bebes" and is then converted into the FOOD and DRINK (verb) concepts because bebes is a conjugated form of beber, which is the Spanish verb for drink.</p><p>On Question 43: "El Niño and the weather" is translated by "El Niño y el tiempo" and is then converted into the CHILD and TIME concepts because Niño is the Spanish noun for child and tiempo is an ambiguous word meaning both time and weather.</p><p>Given those difficulties, outstanding results could not be expected.</p><p>Looking back on the questions and results, it can be noted that 1'793 documents were retrieved out of the 2'052 relevant ones, i.e. almost 90% of them. The core issue is to sort out the documents so as to reject those whose content does not match users' expectations.</p><p>A closer look at our results on the Training corpus showed that we got a pretty good performance on some of the requests. This does not mean that our search engine understood the said requests correctly; it is simply due to the fact that the corpus included only good matches for those requests, so it was almost impossible to find wrong answers.</p><p>For instance, on Question 50 about "the Revolt in Chiapas", we retrieved 106 documents out of the 107 relevant ones with an average precision of 87%. This is due to the fact that in the corpus, the Chiapas are only known for their revolt (in fact if we google the word "Chiapas" a good proportion of the results are currently about the Chiapas rebellion).</p><p>On the other hand, on Question 59: "Computer Viruses", our search engine retrieved 1 document on 1 with an average precision of 0.3%. This is because the 300 documents retrieved before the one we were looking for were indeed about viruses and computers, but did not mention any virus name or damage as was requested.</p><p>Therefore term disambiguation does not help the search engine to understand what kind of documents are expected. A question such as the one above requires the text to be read and understood in order to decide whether it is actually a correct match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Intuitively, Word Sense Disambiguation should improve the quality of information retrieval systems. However, as already observed in previous experiments, this is only true in some specific situations, for instance when the disambiguation process is almost perfect, or in limited domains. The observations presented here seem to support this statement. We propose two types of explanations:</p><p>1. When a query is large enough (more than one or two words), the probability that a document containing these words uses them with a meaning different from the intended one is very low. For instance, it is unlikely that a document containing mouse, cheese and cat is in fact about a computer mouse. This probably makes WSD useless in many situations. Such a request is similar in nature to the narrativebased tests. On the other hand, the WSD approach could make more sense when requests include only one or two words (which is the most frequent case in standard searches).</p><p>2. WSD is a very partial semantic analysis which is insufficient to really understand the queries. For instance, consider the query "Computer Viruses" whose narrative is "Relevant documents should mention the name of the computer virus, and possibly the damage it does". To find relevant documents, a system must recognize phrases which contain virus names ("the XX virus", "the virus named XX", "the virus known as XX", etc.). It should also recognize phrases describing damages ("XX erases the hard disk", "XX causes system crashes" but not "XX propagates through mail messages"). These tasks are very difficult to perform and they are far beyond the scope of WSD. Moreover, they require specific domain knowledge, as shown in <ref type="bibr" coords="5,237.96,585.52,23.35,8.96">[rf06]</ref> .</p><p>The modifications brought to our stop-word lists showed that our search engine is more sensitive to various adjustments of its internal parameters than to the use of a WSD system. Indeed, when we ran a new series of tests with English-only stop words (which eliminated some terms in the requests, such as "eu" and "un"), our new score for the LEM-TDN (which was our best result in this task) increased from 39.17% to 39.63%.</p><p>Finally, as we argued in <ref type="bibr" coords="5,169.20,671.56,27.17,8.96">[grf05]</ref>, conceptual indexing is a promising approach for language-independent indexing and retrieval systems. Although an efficient WSD is essential to create good conceptual indexes, we showed in <ref type="bibr" coords="5,70.92,698.08,28.27,8.96">[grf05]</ref> that ambiguous indexes (with several concepts for some terms) are often sufficient to reach a good multilingual retrieval performance, for the reasons mentioned above. </p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,77.64,738.21,446.91,8.10;2,70.92,748.53,183.21,8.10"><p>This amounts to considering that the disambiguation algorithm is "perfect". Alternatively we could have added all the synsets with a score greater than a given threshold.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix B: Analysis of some requests which performed better with WSD Case 1) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretation</head><p>Clearly here the WSD process improved the results because the search engine looked for the military_service concept instead of looking for military OR service. This is the ideal situation and we could have expected it to be the most frequent case, but it was not, as illustrated below.</p><p>Case 2) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretation</head><p>Two explanations may be proposed in this case: flooding was wrongly encoded but it still specifies the concept better than the flood lemma; and Holland was encoded as Netherlands, so the documents were badly classified because the search engine did not know that the two words are synonyms.</p><p>Case 4) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretation</head><p>When searching for the China-Taiwan concept using the lemmas, the engine found only 9 documents out of the expected 34, while the WSD process retrieved 26 documents. It is noted here that the encoding process made two mistakes: China-Taiwan was encoded as China only, and relation was encoded as causality. Yet the WSD average precision is still better because the same encoding mistakes were made systematically across the corpus and queries.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,104.88,112.36,419.50,8.96;6,70.92,125.56,139.41,8.96" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,290.16,112.36,139.65,8.96">Construire un moteur d&apos;indexation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guyot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Falquet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Benzineb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Hermes</publisher>
			<pubPlace>Paris</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technique et science informatique</note>
</biblStruct>

<biblStruct coords="6,101.88,148.72,422.37,8.96;6,70.92,162.04,453.33,8.96;6,70.92,175.24,453.31,8.96;6,70.92,188.44,118.05,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,281.88,148.72,238.46,8.96">Conceptual Indexing for Multilingual Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guyot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Radhouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Falquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,82.92,162.04,441.33,8.96;6,70.92,175.24,46.92,8.96">Accessing Multilingual Information Repositories: 6th Workshop of the Cross-Language Evaluation Forum, CLEF 2005</title>
		<title level="s" coord="6,409.56,175.24,114.68,8.96;6,70.92,188.44,29.27,8.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">4022</biblScope>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="6,107.52,211.60,416.74,8.96;6,70.92,224.92,453.34,8.96;6,70.92,238.12,273.09,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,297.84,211.60,226.42,8.96;6,70.92,224.92,207.95,8.96">NUS-PT: Exploiting Parallel Texts for Word Sense Disambiguation in the English All-Words Tasks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,289.08,224.92,235.18,8.96;6,70.92,238.12,113.23,8.96">Proc. of the 4th International Workshop on Semantic Evaluations (SemEval 2007)</title>
		<meeting>of the 4th International Workshop on Semantic Evaluations (SemEval 2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,100.68,261.28,423.78,8.96;6,70.92,274.60,453.49,8.96;6,70.92,287.80,23.61,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,245.16,261.28,263.37,8.96">Using External Knowledge to Solve Multi-Dimensional Queries</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Radhouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Falquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,70.92,274.60,346.63,8.96">Proc. 13th Intl Conf. on Concurrent Engineering Research and Applications (CE 2006)</title>
		<meeting>13th Intl Conf. on Concurrent Engineering Research and Applications (CE 2006)<address><addrLine>Antibes</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2006-09">2006. Sept. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.24,310.96,419.23,8.96;6,70.92,324.28,453.09,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,269.88,310.96,212.80,8.96">UBC-ALM: Decombining k-NN with SVD for WSD</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,492.00,310.96,32.47,8.96;6,70.92,324.28,293.23,8.96">Proc. of the 4th International Workshop on Semantic Evaluations (SemEval 2007)</title>
		<meeting>of the 4th International Workshop on Semantic Evaluations (SemEval 2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
