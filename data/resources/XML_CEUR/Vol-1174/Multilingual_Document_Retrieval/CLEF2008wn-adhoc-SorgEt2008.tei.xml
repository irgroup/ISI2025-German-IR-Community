<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,120.37,148.79,362.26,15.48;1,234.31,170.71,134.38,15.48">Cross-lingual Information Retrieval with Explicit Semantic Analysis</title>
				<funder ref="#_BtByHxX">
					<orgName type="full">German Research Foundation (DFG)</orgName>
				</funder>
				<funder>
					<orgName type="full">Multipla</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,241.25,204.99,48.19,8.64"><forename type="first">Philipp</forename><surname>Sorg</surname></persName>
							<email>sorg@aifb.uni-karlsruhe.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute AIFB</orgName>
								<orgName type="institution">University of Karlsruhe</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,296.15,204.99,65.60,8.64"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
							<email>cimiano@aifb.uni-karlsruhe.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute AIFB</orgName>
								<orgName type="institution">University of Karlsruhe</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,120.37,148.79,362.26,15.48;1,234.31,170.71,134.38,15.48">Cross-lingual Information Retrieval with Explicit Semantic Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5A04FBEE2D8C79668A3C012A86309386</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval Measurement</term>
					<term>Performance</term>
					<term>Experimentation Cross-language Information Retrieval</term>
					<term>Explicit Semantic Analysis</term>
					<term>Wikipedia</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We have participated on the monolingual and bilingual CLEF Ad-Hoc Retrieval Tasks, using a novel extension of the by now well-known Explicit Semantic Analysis (ESA) approach. We call this extension Cross-Language Explicit Semantic Analysis (CL-ESA) as it allows to apply ESA in a cross-lingual information retrieval setting. In essence, ESA represents documents as vectors in the space of Wikipedia articles, using the tfidf measure to capture how "important" a Wikipedia article is for a specific word. The interesting property of ESA is that arbitrary documents can be represented as a vector with respect to the Wikipedia article space. ESA thus replaces the standard BOW model for retrieval. In our cross-lingual extension of ESA, the cross-language links of Wikipedia are used in order to map the ESA vectors between different languages, thus allowing retrieval across languages. Our results are far behind the ones of other systems on the monolingual and ad-hoc retrieval tasks, but our motivation was to find out the potential of the CL-ESA approach using a first and unoptimized implementation thereof.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cross-language Information Retrieval (CLIR) can be described at an abstract level as the task of retrieving documents across languages. In some sense, the CLIR task represents one extreme case of the so called vocabulary mismatch problem, i.e. the problem that the vocabulary of a user query and the vocabulary of relevant documents can differ substantially. The bag-of-words (BOW) model notoriously suffers from the vocabulary mismatch problem as the different dimensions are inherently orthogonal, thus neglecting relations between different words in the same language as well as across languages. Therefore, the challenging task of retrieving documents to queries in other languages requires models going beyond the traditional bag-of-words model.</p><p>When tackling the task of retrieving documents across languages, there seem to be essentially two main paradigms:</p><p>1. Translation-based approaches which rely either on a translation of documents or queries. For the translation of queries, one typically relies on bilingual dictionaries (compare <ref type="bibr" coords="2,421.77,158.17,15.27,8.64" target="#b9">[10]</ref>, <ref type="bibr" coords="2,443.34,158.17,10.45,8.64" target="#b4">[5]</ref>).</p><p>2. Mapping of queries and documents into a multilingual space in which similarity between queries and documents can be computed uniformly across languages.</p><p>The first type of approaches is obviously highly dependent on the quality of the translation system used or the bilingual dictionary in question. Demner-Fusham et al. <ref type="bibr" coords="2,329.45,223.92,11.62,8.64" target="#b4">[5]</ref> have in particular shown that the coverage of the bilingual dictionary has a crucial impact on the retrieval task. As mentioned by Demner-Fushman et al., for a successful dictionary-based CLIR model, the following three steps need to be accomplished: <ref type="bibr" coords="2,501.39,247.83,11.62,8.64" target="#b0">(1)</ref> selection of the terms to be translated, (2) generation of a set of candidate translations, and (3) use of that set of candidate translations in the retrieval process.</p><p>Concerning the second type of approaches in which queries and documents are mapped into a multilingual space, there are two crucially different models:</p><p>• latent model: Instead of representing documents (and queries) with respect to the bag-of-word dimensions, some approaches compute "latent" concepts from the data and index documents with respect to these latent concepts. Latent concepts correspond to certain topics emerging bottom-up from the document collection. The most prominent technique here is latent semantic analysis (LSA) <ref type="bibr" coords="2,114.91,377.35,10.58,8.64" target="#b3">[4]</ref>. In fact, LSA has also been applied in cross-lingual IR settings (compare <ref type="bibr" coords="2,417.27,377.35,14.94,8.64" target="#b16">[17]</ref>). For this purposes, parallel texts are needed across languages in order to construct a matrix where the dimensions correspond to words in all languages considered. Dimensionality reduction is then applied to discover correlated words across languages. Queries and documents can then be represented in this "latent space" and retrieval can be performed in a standard fashion by calculating the cosine in this space.</p><p>• external category model: In contrast to retrieval models which build on latent topics or concepts, one can also choose a set of external categories, topics or concepts to define the dimensions of the vectors. These can be categories from existing thesauri, ontologies etc. The advantage is that the vectors then remain constant across different document collections, in particular also across languages. Such models presuppose that we are indeed able to index texts in various languages with respect to the multilingual space spanned by the external categories.</p><p>The latter approach based on indexing with respect to external categories is interesting in the sense that i) no parallel texts are required (e.g. in order to compute latent topics grouping words from different languages), and ii) no bilingual dictionaries are needed. Obviously, this is true only to some extent as the mapping into the external categories (across languages) might well require cross-lingual dictionaries. Gabrilovich and Markovitch <ref type="bibr" coords="2,209.44,574.61,11.62,8.64" target="#b6">[7]</ref> have for example recently presented an interesting approach in which Wikipedia articles are used as dimensions of the vectors, i.e. documents are indexed with respect to the Wikipedia article space. While Gabrilovich and Markovitch have applied this model to calculate semantic relatedness between words, this model extends straightforwardly to an IR setting, in which query and documents are mapped to a vector representing the Wikipedia article space (see for instance <ref type="bibr" coords="2,458.52,622.43,11.62,8.64" target="#b8">[9]</ref> and <ref type="bibr" coords="2,489.50,622.43,10.45,8.64" target="#b5">[6]</ref>).</p><p>An interesting characteristic of Wikipedia is that articles are linked across languages by bidirectional language links. Thus, we can in principle translate a document or query vector indexed with respect to the Wikipedia of language L i to language L j , thus extending straightforwardly into a cross-lingual retrieval task.</p><p>In this paper we investigate this idea closer and present an approach for cross-language IR based on Explicit Semantic Analysis. In particular, we present our system as it has been used on the CLEF monolingual and multilingual Ad-Hoc retrieval tasks. Further, we also present additional experiments on the Multext dataset conducted after the submission to the CLEF campaign in order to verify some of the parameter settings of our approach on another dataset. In order to be able to quantify the influence of the parameters, we have in particular conducted standard mating experiments on the Multext dataset.</p><p>The article is structured as follows: in the next section 2 we describe in more detail the ESA model and show how it can be used in a retrieval setting. In Section 3 we discuss how this model can be extended to a cross-lingual setting relying on the Wikipedia cross-language links. In section 4 we discuss some implementation details which are nevertheless important to understand how the overall system works on the task of cross-language IR. Finally, in Section 5 we present our results on the CLEF datasets as well as on the Multext corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Explicit Semantic Analysis (ESA)</head><p>Explicit Semantic Analysis (ESA) <ref type="bibr" coords="3,231.20,228.01,11.62,8.64" target="#b6">[7]</ref> attempts to index or classify a given text t with respect to a set of explicitly given external categories. It is in this sense that ESA is explicit compared to approaches which aim at representing texts with respect to latent topics or concepts, as done in Latent Semantic Analysis (LSA) (see <ref type="bibr" coords="3,137.96,263.87,10.58,8.64" target="#b3">[4]</ref>, <ref type="bibr" coords="3,155.49,263.87,14.94,8.64" target="#b10">[11]</ref>). Gabrilovich and Markovitch have outlined the general theory behind ESA and in particular described its instantiation to the case of using Wikipedia articles as external categories. We will basically build on this instantiation as described in <ref type="bibr" coords="3,293.48,287.78,11.62,8.64" target="#b6">[7]</ref> which we briefly summarize in the following.</p><p>In essence, Explicit Semantic Analysis takes as input a text t and maps it to a high-dimensional realvalued vector space. This vector space is spanned by a Wikipedia database W k = {a 1 , . . . , a n } in language L k such that each dimension corresponds to an article a i . This mapping is given by the following function:</p><formula xml:id="formula_0" coords="3,251.21,345.17,96.22,30.63">Φ k : T → R |W k | Φ k (t) := v 1 , . . . , v |W k |</formula><p>where |W k | is the number of articles in Wikipedia W k corresponding to language L k . The value v i in the ESA vector of t expresses the strength of association between t and the Wikipedia article a i . Based on a function as that defines the strength of association between words and Wikipedia articles, the values v i can be computed as the sum of the association strength of all words of t = w 1 , . . . , w s to the article a i :</p><formula xml:id="formula_1" coords="3,258.63,442.88,42.06,19.61">v i := wj ∈t</formula><p>as(w j , a i ) One approach to define such a association strength function as is to use a tf.idf function based on the Bag-of-Words (BOW) model of the Wikipedia articles. The association strength of word w j to article a i is then equal to the tf.idf value of w j in a i : as(w j , a i ) = tf.idf ai (w j )</p><p>In the literature, many different definitions of tf.idf functions based on the BOW model have been proposed (see <ref type="bibr" coords="3,147.54,549.09,10.45,8.64" target="#b0">[1]</ref>). The particular function that was used in our experiments is described in Section 4.</p><p>Essentially, for each article a i in Wikipedia, ESA sums up all the association strengths of each word w j appearing in the document. In this sense, the Semantic Interpreter applying ESA described in <ref type="bibr" coords="3,457.35,573.00,11.62,8.64" target="#b6">[7]</ref> essentially computes the function Φ. As output we thus get a vector representing the strength of association of our text t with respect to the articles in Wikipedia W k . Actually, this vector thus corresponds to a ranking of the Wikipedia articles according to importance or relevance for a text t.</p><p>Given the ESA framework, we can assess the similarity between two texts t i , t j ∈ T , between a query q and a text t i etc. For example, the standard cosine measure can be used to compare the vectors. In the remainder of this paper we will simply assume that the cosine is used to compare different vectors.</p><p>In fact, this framework is flexible to be applied to a variety of tasks, computing the similarity between:</p><p>• single words, which can be seen as singleton texts consisting of only one word. This can then be used to compute semantic relatedness between words as in <ref type="bibr" coords="3,349.30,688.57,10.58,8.64" target="#b6">[7]</ref>. Gabrilovich and Markovitch actually showed that their method performs better than LSI on the task of computing semantic relatedness between words.</p><p>• two documents (e.g. in a clustering task)</p><p>• a query and a document (e.g. in a retrieval. task)</p><p>In this paper we are concerned with a retrieval task, in which we are given a query q and need to rank the documents according to relevance. It should be clear from the above discussions that ESA straightforwardly extends to a retrieval scenario.</p><p>As a running example in this paper, we will use query 10.2452/460-AH ("Scary Movies") from the 2008 CLEF Ad-hoc retrieval dataset where our system performed remarkably well. In the following table we indicate the 10 top-ranked Wikipedia articles for the query in the three languages German, English and French: The top-10 ranked articles clearly differ between the languages. It is in particular interesting to observe that many results are actually named entities which clearly differ between languages due to a different cultural background. Consequently, the ESA vectors for the same query in different languages varies substantially, which is less optimal in a cross-language retrieval setting.</p><p>In the following section, we present our own extension to ESA called CL-ESA (Cross-language Explicit Semantic Analysis) <ref type="foot" coords="4,168.97,413.08,3.49,6.05" target="#foot_0">1</ref> , which represents a relatively straightforward extension of ESA to a cross-lingual setting. Our main aim in this paper is to discover if CL-ESA performs well in a cross-lingual retrieval setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cross-lingual ESA (CL-ESA)</head><p>A very interesting characteristic of Wikipedia, besides the overwhelming amount of information created dynamically and in a collaborative way, is the fact that articles are linked across languages. Cross-language links are those that link a certain article to a corresponding article in the Wikipedia database in another language. A previous analysis of this cross-lingual link structure between the German and English Wikipedia showed that 95% of these links are indeed bi-directional (see <ref type="bibr" coords="4,341.33,542.18,14.94,8.64" target="#b15">[16]</ref>). The analysis of French-English and French-German links showed similar results. In the following we therefore assume the existence of a mapping function m i→j that maps an article of Wikipedia W i to its corresponding article in Wikipedia W j .</p><p>In fact, given a text t ∈ T in language L i , it turns out that we can simply index this document with respect to any of the other languages L 1 , .., L n we consider by transforming the vector Φ i (t) into a corresponding vector in the vector space that is spanned by the articles of Wikipedia in the target language. Thus, given that we consider n languages, we have n 2 mapping functions of the type:</p><formula xml:id="formula_2" coords="4,254.89,647.39,92.72,11.72">Ψ i→j : R |Wi| → R |Wj |</formula><p>This mapping is calculated as follows:</p><formula xml:id="formula_3" coords="4,228.31,687.67,142.01,10.87">Ψ i→j v 1 , ..., v |Wi| = v 1 , ..., v |Wj | where v p = q∈{q * |mi→j (a q * )=ap} v q (1) with 1 ≤ p ≤ |W i |, 1 ≤ q ≤ |W j |.</formula><p>In case that i = j we thus have the identity function.</p><p>In order to get the ESA representation of a document t ∈ T in language L i with respect to Wikipedia W j we simply have to compute the function Ψ i→j (Φ i (t)).</p><p>In the following table, we give the top-ranked Wikipedia articles for our running example query together with the result of mapping the German and French vectors into the English Wikipedia space: To illustrate the actual overlap of the ESA vectors, the next table contains the positions of the first 10 matches of the i) English ESA vector using the query of the running example and ii) the German ESA vector mapped to the English ESA space. In this case, matches are common non-zero dimensions in the ESA vector. The positions of these matches show that the English vector and the mapped German vector have common non-zero dimensions, but the rank of these dimensions differs a lot. In an ideal setting these ranks should be equal in both vectors. Given the above settings, it should be straightforward to see how the actual retrieval works. The cosine between a query q i in language L i and a document d j in language L j is calculated as:</p><formula xml:id="formula_4" coords="5,216.41,614.89,170.18,9.65">cos(q i , d j ) := cos(Φ i (q i ), Ψ j→i (Φ j (d j )))</formula><p>This thus gives us an elegant retrieval model which is uniform across languages. A prerequisite for this model is certainly that we know the language of the query and of the different documents in order to know which mapping Ψ should be applied. We describe in the implementation section how we actually implemented a straightforward component for language detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>In this section we describe the implementation details we used for our experiments. In particular, we describe i) the document preprocessing (Section 4.1), ii) the actual ESA implementation that consists of article preprocessing, ESA vector computation and multi-lingual mapping (Section 4.2), iii) the identification method to identify the language of a document (Section 4.3), and iv) the overall retrieval process (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preprocessing of Documents</head><p>We used the following methods for the preprocessing of documents:</p><p>Tokenizer As tokenizer we used a standard white space tokenizer. All non-character tokens were deleted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stop-Word Filtering</head><p>We used standard stop word lists in the languages English, German and French to filter out stop words.</p><p>Stemmer All terms in the documents were stemmed using Snowball Stemmers<ref type="foot" coords="6,414.17,253.51,3.49,6.05" target="#foot_1">2</ref> available for many different languages including English, German and French.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ESA Implementation</head><p>The implementation of Cross-Lingual ESA can be divided into three steps. The first step is the preprocessing of the Wikipedia articles. This includes preprocessing of the article texts as well as the selection of articles that will be used for ESA indexing. The next step is the computation of the ESA vector, which depends on the choice of the association strenght (as) function that assigns strength of association between words of the documents and Wikipedia articles. The last step is the multi-lingual mapping of the ESA vector.</p><p>In the following, the implementation of all of these steps including different variations and parameters will be explained in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Wikipedia Article Preprocessing</head><p>The processing of the Wikipedia articles was done by using the Wikipedia tokenizer that is included in the Lucene<ref type="foot" coords="6,119.32,453.51,3.49,6.05" target="#foot_2">3</ref> software package and then using the same methods for stop word removal and stemming as in the preprocessing of the documents. The Wikipedia tokenizer removes all Wiki markup from the text, e.g. syntax for links, headings and font styles.</p><p>The selection of articles that were used as dimensions of the ESA vector was based on different criteria. First we filtered out all redirect articles and all category articles. Then all articles with less than 100 words or less than 5 incoming pagelinks were discarded. In our first experiments, we did not perform any further selection. The results of the CLEF ad hoc retrieval are based on these settings. In the subsequent experiments on the Multext dataset, we restrict the Wikipedia articles used for ESA indexing to those that have at least a language link to one of the two other languages we consider. For example, we only consider an article of the English Wikipedia if it has a cross-language link to the German or the French Wikipedia. In absolute numbers, we used 536, 896 English, 390, 027 German and 362, 972 French articles for the ESA indexing (Wikipedia snapshot of <ref type="bibr" coords="6,222.27,586.69,64.88,8.64">March 12, 2008)</ref>.</p><p>In the original ESA approach, Gabrilovich and Markovitch included more preprocessing and selection steps <ref type="bibr" coords="6,112.66,610.60,10.58,8.64" target="#b7">[8]</ref>. They added to the text for example the anchor text of incoming pagelinks and titles of redirects to an article. Some articles such as articles about years and similar were discarded. We have not made use of any additional similar heuristics in our implementation of the ESA/CL-ESA approach. Nevertheless, it would be interesting to study the influence of such additional heuristics in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">ESA Vector Computation</head><p>The computation of the ESA vector is based on an inverted index of the preprocessed selected Wikipedia articles. Each document of the dataset can then be treated as a query to this index. The retrieved articles with their weight can then be used to build the ESA vector.</p><p>The implementation of the index was done by using Lucene. As the function for computing the association strength between documents and articles, we used a customized implementation of the Lucene similarity function which computes the following function for a text t = w 1 , . . . , w l and a Wikipedia article a i of Wikipedia database W :</p><formula xml:id="formula_5" coords="7,90.00,167.54,314.51,124.50">as R (t, a i ) = (C t ) |a i | -1 wj ∈t tf ai (w j )idf (w j ) with C t = 1 wj ∈t idf (w j ) tf ai (w i ) = number of occurrences of w i in a i idf (w j ) = 1 + log number of articles containing w j |W | + 1</formula><p>The choice of as R is motivated by the good performance on IR tasks. We therefore assume that this association strength can be used for the computation of the values of the ESA vector. The factor</p><formula xml:id="formula_6" coords="7,488.19,309.25,24.32,14.46">|a i | -1</formula><p>constitutes a normalization by length of the article. The factor C(t) is only dependant on the query and does therefore not affect the relevance ranking of articles to the text t or the cosine computation.</p><p>In the experiments on the Multext dataset, we also used a different function that computes a bit valued ESA vector. This function as BIT is defined as follows:</p><formula xml:id="formula_7" coords="7,207.27,388.78,61.52,9.65">as BIT (t, a i ) =</formula><p>1 a i contains any w j ∈ t 0 else</p><p>For both functions, the number k of articles (dimensions) considered in order to compute the ESA vector is used as a parameter. In fact, it seems that for the computation of the ESA vector "less is more" as conveyed by the experiments described in <ref type="bibr" coords="7,218.19,440.35,10.58,8.64" target="#b5">[6]</ref>. However, this is only the case provided that we have a reasonable way of determining which articles are most suitable. In our approach we only set those values in the ESA vector corresponding to the k articles with the highest association strength to a document t. Thus, the vectors we consider are relatively sparse with |W | -k dimensions having zero values. When using as BIT to compute the ESA vector, the ranking of relevant articles for a text is still based on as R . As this ranking is used to select k articles, as BIT is not independent from as R . The objective of using as BIT however is to flatten the differences between the associated Wikipedia articles in the ESA vector.</p><p>Gabrilovich and Markovitch weighted the association strength by exploiting the pagelink structure of Wikipedia. It remains future work to adapt this method to our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Multi-lingual Mapping</head><p>As described above the multi-lingual mapping was done by using the cross-language links of Wikipedia. To use these links in an efficient way, some preprocessing is necessary. First we did a normalization of the target page titles of all cross-language links, as this is not done automatically in the Wikipedia database. Then we identified all cross-language links pointing to redirect pages and replaced them with language links to the article to which the redirect was leading.</p><p>In order to map the vectors from language L i to language L j we only use the cross-language links of Wikipedia W i pointing to W j . As our statistics showed that most of these links are bi-directional (95%) we did not include the links from W j to W i .</p><p>In some cases, two or more articles in W i contain a cross-language link to the same article in a ∈ W j . In this case, the new value of the ESA dimension corresponding to a was set to the sum of the values of all dimensions that correspond to the source articles in the original ESA vector (see Equation <ref type="formula" coords="7,450.67,712.71,3.60,8.64">1</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Language identification</head><p>In order to be able to compute the ESA vector for a document, the language of this document must be known as the computation is based on an index of a Wikipedia database in the document's language. Many document collections only contain documents in one language and thus no language identification is needed. In other cases, such as in the CLEF ad hoc retrieval task, the dataset contains documents in different languages.</p><p>In our implementation we first try to determine the language by using properties of the documents such as language annotations. If these are not available, we apply a simple heuristic to determine the language of document t as follows:</p><formula xml:id="formula_8" coords="8,204.40,416.25,193.00,23.23">lang(t) := max L k ∈{L1,...,Ln} minDim(Φ k (t)) maxDim(Φ k (t))</formula><p>where minDim( v) returns the value of the lowest dimension in vector v and maxDim( v) returns the highest correspondingly. The intuition behind this heuristic is that a small difference between the values of the lowest and highest dimension, which is computed by the share of these values, means that the document matches good to many Wikipedia articles and it can therefore be assumed that the document is of the same language as the used Wikipedia articles. Comparing a document to Wikipedia articles in another language, there will be some mathes but the value of lowest dimension will most probably be very small. While we have not done an extensive evaluation of this heuristic, a check showed that the quality of this heuristic is reasonable and sufficient for our purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Retrieval</head><p>The implementation of the multi-lingual retrieval task is described in Figure <ref type="figure" coords="8,410.01,581.95,4.98,8.64" target="#fig_0">1</ref> using pseudo code. In summary we first compute the ESA vector of all topics and then iterate over all documents in the dataset. The described workflow reduces the number of ESA vector computations substantially.</p><p>For the CLEF ad hoc retrieval task we were able to process the ONB dataset using all English, German and French topics in about 40 hours. The same task on the BL dataset had a runtime of approximately 60 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section, we describe the datasets used for the evaluation. Then we present the experiments together with the different parameter settings applied. Finally, we also analyze the results of our approach with respect to different parameters using alternative measures such as the overlap of retrieved documents for the same query in different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>The first dataset we used was the TEL dataset that was provided by The European Library in the context of the CLEF 2008 ad-hoc track. This dataset consists of library catalog records mainly in English, German and French but also some records in other languages. In our experiments, we used two parts of this dataset: The TEL English data provided by the British Library with mainly English records and the TEL German data provided by the Austrian National Library with mainly German records. All of these records consist of content information together with meta information about the publication. The title of the record is the only content information that is available for all records. Some records additionally contain some annotation terms. In our experiments we only used the available content information.</p><p>This dataset is challenging for IR tasks in different ways. First the text of the records is very short, only a few words for most records. Second, the dataset consists of records in different languages and retrieval methods need to consider relevant documents in all of these languages. The following examples show the complete content information of some records of the TEL English dataset:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Title or Subject</head><p>Annotation Terms Strength, fracture and complexity : an international journal.</p><p>Fracture mechanics, Strength of materials Studies in the anthropology of North American indians series.</p><p>-Lehrbuch des Schachspiels und Einfuehrung in die Problemkunst. Chess</p><p>The TEL English dataset contains 1,000,100 records, the TEL German dataset 869,353.</p><p>As second dataset we used the Multext JOC corpus <ref type="foot" coords="9,311.87,338.75,3.49,6.05" target="#foot_3">4</ref> . The original data of this corpus is composed of written questions asked by members of the European Parliament on a wide variety of topics and corresponding answers from the European Commission in 9 parallel versions, published as one section of the C Series of the Official Journal of the European Community of the year 1993. The parts corresponding to the languages of the Multext project (English, French, German, Italian and Spanish) were collected and prepared in collaboration with the MLCC project. For our experiments we used the English, German and French parts. This dataset contains 3126 question/answer pairs in each language which are aligned across the languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">CLEF Ad-hoc Experiments</head><p>The CLEF ad-hoc TEL task was divided into mono-lingual and bi-lingual tasks. 50 topics in the main languages English, German and French were provided. The topics consist of two fields, a short title containing 2-4 keywords and a description of the information item of interest in terms of 1-2 sentences.</p><p>The objective is to query the selected target collection using topics in the same language (mono-lingual run) or topics in a different language (bi-lingual run) and to submit the results in a list ranked with respect to decreasing relevance. In line with these objectives we submitted results of six different runs to CLEF 2008. These are the results of querying English, German and French topics to the TEL English dataset and English, German and French topics to the TEL German dataset.</p><p>The following parameter settings as described in the implementation section were used for these experiments:</p><p>ESA vector length We used different lengths of the ESA vector to represent topics and records. For the topics we used k = 10, 000, that means that 10,000 Wikipedia articles with the strongest association to a specific topic were used to build the ESA vector for this topic. For the records, we used k = 1000. The difference between the lengths is mainly due to performance issues. We were only able to process the huge amount of records by limiting the length of the ESA vectors for records to 1000 non-zero entries. As only 50 topics were provided, we were able to use more entries for the ESA vectors for topics. Our intention thereby was to improve recall of the retrieval by using more ESA dimensions.</p><p>Article selection In the results of the experiments submitted to CLEF, we only used the default article selection as described in the implementation section. One problem of this setting is the loss of many dimensions in the mapping process, as not all of the articles corresponding to a non-zero ESA vector entry have a corresponding cross-language link to the Wikipedia in the target language. In this case, the information about this dimension is lost in the mapping process.</p><p>The following table contains the CLEF 2008 results of our submitted experiments measured by the Mean Average Precision (MAP) quality measure:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Topic language MAP TEL English (BL) English 17.7% German 7.6% French 3.6% TEL German (ONB) English 6.7% German 9.6% French 5.1%</p><p>In addition to the submitted experiments we also conducted additional experiments on the TEL dataset to better quantify and understand the impact of certain parameters on the result quality. As we were not able to evaluate the results apart from the submitted ones, we decided to examine the result overlap for queries in different languages on the same dataset. This measure can be seen as a quality measure for the capability of retrieving relevant documents across languages. Ideally, queries in different languages should result in the same set of retrieved records. We computed the result overlap for two different settings. First we used the same settings as used in the submitted results. For the second set of experiments we further restricted the Wikipedia articles that were used for ESA indexing to articles with at least one language link to one of the two other languages considered. The following table contains the result overlaps for topic pairs in different languages on the TEL English dataset:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Article restriction</head><p>Topic language pair Average result overlap No restriction English -German 21% English -French 19% German -French 28% Articles with exiting English -German 39% cross-language link English -French 51% German -French 39%</p><p>The results show that we were able to substantially improve the retrieval methods according to the results overlap measure by restricting the Wikipedia articles. Our assumption is that the results on the retrieval task would also improve, but we did not manage to submit an additional run on time for CLEF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Mate Retrieval on Multext JOC Corpus</head><p>As described above, the part of the Multext JOC Corpus we used consists of 3126 question/answer pairs in English, German and French. All of these documents are aligned across languages in the sense that for all documents there exist a corresponding article in the other languages. This dataset can therefore be used for mate-retrieval experiments, which allow a direct assessment of different parameters. Mate retrieval is the task of using a document as query with the objective to identify its translated counterpart in a set of documents in another language. In this case the counterpart is known in advance enabling an automatic evaluation of the mate retrieval results.</p><p>Our main goal of the mate retrieval experiments was to optimize the parameters settings for CL-ESA. We ran the experiments for various parameter settings:</p><p>ESA vector length We used different k for the maximal number of non-zero dimensions of the ESA vector, namely k ∈ {1000; 10, 000; 100, 000}.</p><p>Article selection We only used articles with existing cross-language links for the ESA vector computation as described in the implementation section.</p><p>Text selection We used different text parts of the question/answer pairs in our experiments, namely subject, question and all text consisting of subject, question and response. We always compared identic parts of queries and documents, e.g. if we used the subject as query we only matched it to the subjects of the documents in the retrieval process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real vs. Bit vectors</head><p>In the experiments we examined the effect of using real valued ESA vectors versus bit valued ESA vectors.</p><p>As evaluation measure we used TOP- The results show that using the bit valued ESA vectors yields a big loss in performance at the mate retrieval task, independently of the text parts that were used. It seems therefore to be important to use the relevance of articles to the queries that is encoded in the real values of the ESA vector representation of queries.</p><p>Looking at the number of dimensions of the ESA vector that were used, 10,000 seems to be a good value for this parameter. Using more dimensions does not yield better precision. For queries consisting of question part of the documents and all of the text, the results are even worse.</p><p>Comparing the results using different text parts as queries the differences are not significantly different. As e.g. subjects only consist of a few words but the whole documents contain several sentences, this is an unexpected result. It seems that this method works good for short queries, but with longer queries more noise is added as well and the retrieval performance therefore is not getting much better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>The first approaches to Cross-lingual Information Retrieval (CLIR) were based on the translation of the query into the language of the target documents. Hull and Grefenstette presented a system that uses the term vector translation model <ref type="bibr" coords="11,210.28,607.52,15.27,8.64" target="#b9">[10]</ref>. All terms of the query are translated by looking them up in a bilingual dictionary. A problem of this approach is that many terms have multiple translations which are all added to the translated query. This leads to a loss of precision in the retrieval process. Demner-Fushman and Oard studied the effect of the size of the bilingual term list in dictionary based CLIR <ref type="bibr" coords="11,414.38,643.38,10.58,8.64" target="#b4">[5]</ref>. One of there results is that term lists with above 30,000 entries optimize the coverage of general vocabulary in their experiments. Additionally they showed that the translation of named entities is very important and substantially influences the retrieval quality. Because of that they suggest that supplemental techniques for named entity translation are useful even with large lexicons.</p><p>Another approach to CLIR is based on on Latent Semantic Indexing (LSI). LSI applied to text documents is a technique to reduce the vector representation <ref type="bibr" coords="11,319.79,715.11,10.58,8.64" target="#b2">[3]</ref>. Based on a training corpus Principal Component Analysis (PCA) on the co-occurrence matrix of words can be used to identify relevant dimensions and to construct a mapping of the original Bag-of-Words vector space to these new dimensions. For CLIR LSI can be applied by using a parallel corpus with documents in two languages for training. Parallel documents are therefore merged co-occurrences are computed across languages. The learned model can then be used for CLIR <ref type="bibr" coords="12,161.34,136.25,11.62,8.64" target="#b1">[2]</ref>  <ref type="bibr" coords="12,175.30,136.25,15.27,8.64" target="#b16">[17]</ref>. If a training corpus in multiple languages is available, containing versions of all documents in all languages, LSI can also be used for CLIR in many languages <ref type="bibr" coords="12,404.46,148.20,15.27,8.64" target="#b11">[12]</ref>.</p><p>Recently emerging approaches to CLIR use the Wikipedia database as background knowledge. Schoenhofen et al. <ref type="bibr" coords="12,138.07,172.11,16.60,8.64" target="#b14">[15]</ref> presented a system that translates queries based on a small dictionary and cross-language links in Wikipedia. Afterwards the terms of the translated query are mapped to Wikipedia articles. Different features of these articles are then used to filter the query terms that are used for retrieval. This approach is different to the presented approach as they use cross-language links to translate single query terms. In our approach these links are used to define a mapping of high dimensional vector spaces, that is used to map the ESA vector representation of the whole query.</p><p>Egozi et al. presented a system for monolingual IR using Wikipedia as background knowledge <ref type="bibr" coords="12,498.90,243.85,10.58,8.64" target="#b5">[6]</ref>. This work is highly relevant for this paper as they apply Explicit Semantic Analysis <ref type="bibr" coords="12,423.76,255.80,11.62,8.64" target="#b6">[7]</ref> to IR. Additionally they propose a method to improve the ESA mapping in regards to IR tasks based on Pseudo Relevance Feedback (PSF). This is done first performing standard Bag-of-Words retrieval with a query and then using these results to select relevant dimensions of the ESA vector representation of the same query. A future challenge will be to apply these techniques as well to multi-lingual IR based on the cross-lingual ESA approach we presented in this paper.</p><p>Another approach to use PRF in multi-lingual retrieval is described in by Qu et al <ref type="bibr" coords="12,429.62,327.53,15.27,8.64" target="#b13">[14]</ref>. They examined the effects of pre-translation feedback versus post-translation feedback and identified different errors that were induced through the query expansion.</p><p>After developing our approach and submitting this paper, our literature search discovered the paper by Potthast et al. <ref type="bibr" coords="12,147.93,375.35,15.27,8.64" target="#b12">[13]</ref>, who independently of us developed and presented the CL-ESA model before. In their paper, they perform extensive evaluations on two datasets: Wikipedia and the JRC Acquis dataset <ref type="foot" coords="12,488.26,385.64,3.49,6.05" target="#foot_4">5</ref> . We also intend to use this dataset in future experimental evaluation. The approaches also differ in the way the association between a text and a Wikipedia article is computed. While Potthast et al. use the cosine similarity between a document and a Wikipedia article as weight, we have simply used the tf.idf values for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we have presented our CL-ESA approach and the corresponding implementation with which we have participated in this year's CLEF campaign on the monolingual and bilingual Ad-Hoc retrieval tasks. In particular, we have presented a cross-lingual extension to the Explicit Semantic Analysis (ESA) approach of Gabrilovich and Markovitch. While the results are far from satisfactory, we think that there is still a lot of potential to improve the approach in future research. Questions which seem very important to us are in how far various measures for calculating the association strength between a word (or text) and a Wikipedia article as well as the selection of Wikipedia articles influence the overall results. The interesting experiments presented in <ref type="bibr" coords="12,195.52,574.71,11.62,8.64" target="#b5">[6]</ref> show that "less is more" in the sense that considering a small number of articles can be enough provided that they are selected appropriately. In direct future work, we plan to compare our method with LSI-based cross-lingual retrieval methods to find out more in detail about the performance of our approach, being able to better quantify the weaknesses of the current implementation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,190.49,263.59,222.02,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Pseudocode describing the retrieval algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,90.00,144.22,423.00,253.15"><head></head><label></label><figDesc>1 and TOP-10 Precision, that is the share of input document for which the mate was retrieved on position 1 or among the 10 best ranked results. The results for different text selection, ESA vector model and ESA vector lengths are presented in the following table:The results presented in the following table are retrieval results using German queries on English documents:</figDesc><table coords="11,205.78,209.30,191.43,188.07"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Precision</cell></row><row><cell>Text</cell><cell>Vector model</cell><cell>k</cell><cell cols="2">TOP-1 TOP-10</cell></row><row><cell>Subject</cell><cell>real values</cell><cell>1000</cell><cell>37%</cell><cell>70%</cell></row><row><cell></cell><cell></cell><cell>10,000</cell><cell>38%</cell><cell>69%</cell></row><row><cell></cell><cell></cell><cell>100,000</cell><cell>39%</cell><cell>66%</cell></row><row><cell></cell><cell>bit values</cell><cell>1000</cell><cell>30%</cell><cell>63%</cell></row><row><cell></cell><cell></cell><cell>10,000</cell><cell>25%</cell><cell>54%</cell></row><row><cell></cell><cell></cell><cell>100,000</cell><cell>15%</cell><cell>36%</cell></row><row><cell cols="2">Question real values</cell><cell>1000</cell><cell>33%</cell><cell>52%</cell></row><row><cell></cell><cell></cell><cell>10,000</cell><cell>44%</cell><cell>69%</cell></row><row><cell></cell><cell></cell><cell>100,000</cell><cell>41%</cell><cell>65%</cell></row><row><cell></cell><cell>bit values</cell><cell>1000</cell><cell>30%</cell><cell>40%</cell></row><row><cell></cell><cell></cell><cell>10,000</cell><cell>36%</cell><cell>63%</cell></row><row><cell></cell><cell></cell><cell>100,000</cell><cell>14%</cell><cell>37%</cell></row><row><cell>All text</cell><cell>real values</cell><cell>1000</cell><cell>29%</cell><cell>50%</cell></row><row><cell></cell><cell></cell><cell>10,000</cell><cell>46%</cell><cell>71%</cell></row><row><cell></cell><cell></cell><cell>100,000</cell><cell>45%</cell><cell>68%</cell></row><row><cell></cell><cell>bit values</cell><cell>1000</cell><cell>27%</cell><cell>49%</cell></row><row><cell></cell><cell></cell><cell>10,000</cell><cell>38%</cell><cell>65%</cell></row><row><cell></cell><cell></cell><cell>100,000</cell><cell>17%</cell><cell>40%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,104.35,709.40,408.65,6.91;4,90.00,718.87,423.00,6.91;4,90.00,728.33,423.00,6.91;4,90.00,737.80,423.00,6.91;4,90.00,747.26,335.22,6.91"><p>We would like to point out that we have developed and called our model CL-ESA independently of the CL-ESA approach described by Potthast et al.<ref type="bibr" coords="4,177.93,718.87,12.22,6.91" target="#b12">[13]</ref>. We discovered this work just after finishing our paper, so that CL-ESA is introduced here as a novel paradigm while it clearly has the CL-ESA approach of Potthast et al. as precedent. We thank the Web Technology &amp; Information Systems Group of Weimar University (in particular Martin Potthast) for bearing with us in spite of missing their work in the first place and for the exchange with respect to technical details related to the implementation of the ESA approach.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,104.35,735.64,133.90,5.61"><p>http://snowball.tartarus.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,104.35,745.36,114.77,5.61"><p>http://lucene.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="9,104.35,746.40,215.69,5.61"><p>http://aune.lpl.univ-aix.fr/projects/multext/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="12,104.35,698.47,182.22,5.61"><p>http://langtech.jrc.it/JRC-Acquis.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was funded by the <rs type="funder">Multipla</rs> project sponsored by the <rs type="funder">German Research Foundation (DFG)</rs> under grant number <rs type="grantNumber">38457858</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BtByHxX">
					<idno type="grant-number">38457858</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="13,111.58,134.43,369.95,8.82" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="13,262.19,134.43,119.10,8.59">Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.58,153.54,401.42,8.64;13,111.58,165.32,201.08,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,229.18,153.54,280.31,8.64">Using latent semantic indexing for multilanguage information retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">G</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,111.58,165.32,107.58,8.59">Computers and Humanities</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="413" to="429" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.58,184.43,401.42,8.64;13,111.58,196.20,401.42,8.82;13,111.58,208.34,86.61,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,134.74,196.38,143.36,8.64">Indexing by latent semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,285.47,196.20,223.42,8.59">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.58,227.27,401.42,8.64;13,111.58,239.05,401.42,8.82;13,111.58,251.00,122.02,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,161.53,239.23,147.15,8.64">Indexing by latent semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">W</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,319.04,239.05,193.96,8.59;13,111.58,251.00,28.80,8.59">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.58,270.12,401.42,8.64;13,111.58,281.89,401.42,8.82;13,111.58,293.85,153.83,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,263.54,270.12,249.46,8.64;13,111.58,282.07,120.14,8.64">The effect of bilingual term list size on dictionary-based crosslanguage information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,250.63,281.89,262.38,8.59;13,111.58,293.85,124.42,8.59">Proceedings of the 36th Annual Hawaii International Conference on System Sciences (HICSS&apos;03)</title>
		<meeting>the 36th Annual Hawaii International Conference on System Sciences (HICSS&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.58,312.96,401.42,8.64;13,111.58,324.74,401.42,8.82;13,111.58,336.69,55.61,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,325.69,312.96,187.31,8.64;13,111.58,324.91,96.89,8.64">Concept-based feature generation and selection for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Ofer</forename><surname>Egozi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,229.71,324.74,283.30,8.59;13,111.58,336.69,26.32,8.59">Proceedings of the Twenty-Third Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Twenty-Third Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.58,355.80,401.42,8.64;13,111.58,367.58,401.42,8.82;13,111.58,379.53,132.81,8.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,256.48,355.80,256.52,8.64;13,111.58,367.76,69.81,8.64">Computing semantic relatedness using wikipedia-based explicit semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,204.44,367.58,308.56,8.59;13,111.58,379.53,28.80,8.59">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.58,398.47,401.42,8.82;13,111.58,410.60,301.48,8.64" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="13,200.64,398.47,307.64,8.59">Feature Generation for Textual Information Retrieval using World Knowledge</title>
		<author>
			<persName coords=""><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">5767</biblScope>
			<pubPlace>Haifa, Istrael</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Israel Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="13,111.58,429.54,401.42,8.64;13,111.58,441.31,382.58,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,225.94,429.54,287.06,8.64;13,111.58,441.49,28.32,8.64">Text categorization with knowledge transfer from heterogeneous data sources</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,158.32,441.31,306.54,8.59">Proceedings of the Twenty-Third Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Twenty-Third Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.58,460.42,401.42,8.64;13,111.58,472.20,401.42,8.82;13,111.58,484.16,297.66,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,237.28,460.42,275.73,8.64;13,111.58,472.38,101.85,8.64">Querying across languages: A dictionary-based approach to multilingual information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,233.42,472.20,279.58,8.59;13,111.58,484.16,214.06,8.59">Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 19th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.58,503.27,401.42,8.64;13,111.58,515.05,362.85,8.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,231.44,503.27,281.56,8.64;13,111.58,515.22,216.79,8.64">A solution to plato&apos;s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,336.55,515.05,51.60,8.59">Psychol. Rev</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="211" to="240" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.58,534.16,401.42,8.64;13,111.58,546.11,333.66,8.64" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="13,243.97,534.16,195.33,8.64">Cross-language text retrieval with three languages</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<pubPlace>Durham, North Carolina</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Duke University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="13,111.58,565.05,401.42,8.64;13,111.58,577.00,401.42,8.64;13,111.58,588.78,370.93,8.82" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,319.20,565.05,189.24,8.64">A wikipedia-based multilingual retrieval model</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maik</forename><surname>Anderka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,111.58,588.78,276.53,8.59">Proceedings of the 30th European Conference on IR Research (ECIR)</title>
		<editor>
			<persName><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ian</forename><surname>Ruthven</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</editor>
		<meeting>the 30th European Conference on IR Research (ECIR)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.58,607.89,401.42,8.64;13,111.58,619.67,401.42,8.82" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,300.89,607.89,212.11,8.64;13,111.58,619.85,12.18,8.64">The effect of pseudo relevance feedback on mt-based clir</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Eilerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,139.96,619.67,344.08,8.59">Proceedings of the RIAO (Recherche d&apos;Information Assiste par Ordinateur) Conference</title>
		<meeting>the RIAO (Recherche d&apos;Information Assiste par Ordinateur) Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.58,638.78,401.42,8.64;13,111.58,650.56,192.42,8.82" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,347.04,638.78,165.97,8.64;13,111.58,650.73,37.60,8.64">Performing cross-language retrieval with wikipedia</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schoenhofen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Benzcur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Biro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Csalogany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,167.74,650.56,106.87,8.59">Proceedings of CLEF 2007</title>
		<meeting>CLEF 2007</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.58,669.67,401.42,8.64;13,111.58,681.44,401.43,8.82;13,111.58,693.40,74.33,8.82" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,269.23,669.67,243.77,8.64;13,111.58,681.62,114.99,8.64">Enriching the crosslingual link structure of wikipedia -a classification-based approach</title>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Sorg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,244.66,681.44,268.34,8.59;13,111.58,693.40,45.62,8.59">Proceedings of the AAAI 2008 Workshop on Wikipedia and Artifical Intelligence</title>
		<meeting>the AAAI 2008 Workshop on Wikipedia and Artifical Intelligence</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.58,712.51,401.42,8.64;13,111.58,724.29,401.42,8.82;13,111.58,736.24,94.32,8.82" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,373.28,712.51,139.73,8.64;13,111.58,724.47,120.41,8.64">Automatic cross-language retrieval using latent semantic indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Todd</forename><forename type="middle">A</forename><surname>Letsche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,251.51,724.29,261.49,8.59;13,111.58,736.24,65.53,8.59">Proceedings of the AAAI Symposium on CrossLanguage Text and Speech Retrieval</title>
		<meeting>the AAAI Symposium on CrossLanguage Text and Speech Retrieval</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
