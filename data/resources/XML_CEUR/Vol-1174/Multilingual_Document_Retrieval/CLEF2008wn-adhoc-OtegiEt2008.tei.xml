<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,108.96,97.87,385.08,17.23;1,98.28,119.83,406.40,17.23;1,209.64,141.67,183.56,17.23">IXA at CLEF 2008 Robust-WSD Task: using Word Sense Disambiguation for (Cross Lingual) Information Retrieval</title>
				<funder ref="#_cZkgYZQ">
					<orgName type="full">KNOW</orgName>
				</funder>
				<funder ref="#_b6KhY9Y">
					<orgName type="full">Basque Government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,203.28,176.03,61.25,9.96"><forename type="first">Arantxa</forename><surname>Otegi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IXA NLP Group</orgName>
								<orgName type="institution" key="instit2">University of the Basque Country Donostia</orgName>
								<address>
									<country>Basque Country</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.20,176.03,55.96,9.96"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IXA NLP Group</orgName>
								<orgName type="institution" key="instit2">University of the Basque Country Donostia</orgName>
								<address>
									<country>Basque Country</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,335.62,176.03,63.90,9.96"><forename type="first">German</forename><surname>Rigau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IXA NLP Group</orgName>
								<orgName type="institution" key="instit2">University of the Basque Country Donostia</orgName>
								<address>
									<country>Basque Country</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,108.96,97.87,385.08,17.23;1,98.28,119.83,406.40,17.23;1,209.64,141.67,183.56,17.23">IXA at CLEF 2008 Robust-WSD Task: using Word Sense Disambiguation for (Cross Lingual) Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B5327111F4BCF6BE1A7F1DDCC92DCA83</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>I.2 [Artificial Intelligence]: I.2.7 Natural Language Processing Measurement, Performance, Experimentation Robust Retrieval, CLIR, Word Sense Disambiguation, Query Expansion, Structured Queries</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the IXA NLP group at the CLEF 2008 Robust-WSD Task. This is our first time at CLEF, and we participated at both the monolingual (English) and the bilingual (Spanish to English) subtasks. We tried several query and document expansion and translation strategies, with and without the use of the word sense disambiguation results provided by the organizers. All expansions and translations were done using the English and Spanish wordnets as provided by the organizers and no other resource was used. We used Indri as the search engine, which we tuned in the training part. Our main goal was to improve (Cross Lingual) Information Retrieval results using WSD information, and we attained improvements in both mono and bilingual subtasks, although the improvement was only significant for the bilingual subtask. As a secondary goal, our best systems ranked 4th overall and 3rd overall in the monolingual and bilingual subtasks, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The objective of the robust task in previous editions was to give preference to systems which achieve good stable performance over all queries. This year it also included an additional goal: to test whether word sense disambiguation (WSD) can be used beneficially by retrieval systems. All our experiments have been focused on the second goal and we experimented on how WSD data can be exploited in order to improve retrieval. In this sense, we carried out different expansion and translation strategies of both the topics and documents with and without word sense information.</p><p>For this purpose, we used the open source Indri search engine, which is based on the inference network framework and supports structured queries <ref type="bibr" coords="1,318.75,691.55,9.98,9.96" target="#b0">[1]</ref>.</p><p>The remainder of this paper is organized as follows. Section 2 describes the experiments carried out, Section 3 presents the results obtained and, finally, Section 4 draws the conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiments</head><p>In short, our main experimentation strategy consisted on trying several expansion and translation strategies, all of which used the synonyms in the English and Spanish wordnets made available by the organizers as the sole resources (i.e., we did not use any other external resource), with and without word sense information. Our runs have consisted of different combination of expanded (translated) topics and documents. The steps of our retrieval system are the following.</p><p>We first expand and translate the documents and topics. In a second step we index the original, expanded and translated document collections. Then we tested different query expansion and translation strategies, and finally we search for the queries in the indexes in various combinations. We will see each in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Expansion and translation strategies</head><p>WSD data provided to the participants was based on WordNet version 1.6. Each word sense have a WordNet synset assigned with a score. Using those synset codes and the English and Spanish wordnets, we expanded both the documents and the topics. In this way, we generated different topic and document collections using different approaches of expansion and translation, as follows:</p><p>• Full expansion of English topics and documents: expansion to all synonyms of all senses.</p><p>• Best expansion of English topics and documents: expansion to all synonyms of only the highest scored sense for each word using the two different expansion collections using UBC and NUS disambiguation data (as provided by organizers).</p><p>• Full translation of English documents: translation from English to Spanish of all senses.</p><p>• Best translation of English documents: translation from English to Spanish of only the highest scored sense for each word using the two different translation collections using UBC and NUS disambiguation data.</p><p>• Translation of Spanish topics: translation from Spanish to English of the first sense for each word.</p><p>In the subsequent steps, we used different combinations of these expanded and translated collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Indexing</head><p>Once the collections had been pre-processed, they were indexed using Indri. While indexing, the Indri implementation of the Krovetz stemming algorithm was applied to document terms.</p><p>We created several indexes: one with the original collection words, and one with each collection created after applying different expansion (and translation) strategies, as explained in Section 2.1).</p><p>No stopword list was used, but only nouns, adjectives, verbs and numbers were indexed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Query construction</head><p>We constructed queries using the title and description topic fields. Based on the training topics, we excluded some words and phrased from the queries, such as find, describing, discussing, document, report for English and encontrar, describir, documentos, noticias, ejemplos for Spanish. After excluding those words and taking only nouns, adjectives, verbs and numbers, we constructed several queries for each topic as follows:</p><p>1. Only original words.</p><p>2. Both original words and all expansions for each word.</p><p>3. Both original words and only expansions for the best sense of each word. 4. Only translated words, using all translations for each word. If a word had not a translation, that original word was included in the query.</p><p>5. Only translated words, using translations for the best sense of each word. If a word had not a translation, that original word was included in the query.</p><p>The first three cases are for the monolingual runs, and the last one for the bilingual runs which translated the query.</p><p>In the first case, we constructed a simple query combining only the original words using the Indri operator #combine. In the other cases, we have used some of the operators available in the structured query language. For example, when we wanted to use original words as well as synonyms (obtained after expansion) in the same query, we constructed two subqueries (one with original words, and another one with the expanded words). Then we integrated both subqueries in the same query using the #weight operator and giving a weight of 0.6 to the original word's subquery and 0.4 to the other subquery. We used also the #syn operator to join the expanded words of each sense, as they are meant to be synonyms. In the case of full expansion, instead of #syn, we used #wsyn (weighted synonym). This operator allows to give different weights to synonyms. So, as in the previous case, we joined synonyms, and also, we weighted each synonymset with the score that the disambiguation system had assigned to each sense. Finally, multiword expressions, such as prime minister are added to the query joined with the #1 operator (ordered window).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Retrieval</head><p>We carried out several retrieval experiments combining different kinds of indexes with different kinds of queries. We used the training data to perform extensive experimentation, and chose the ones with best MAP results in order to created the test topic runs. The submitted runs are described in Section 3.</p><p>In some of the experiments we applied pseudo-relevance feedback (PRF) with these default parameters: fbDocs:10, fbTerms:50, fbMu:0 and fbOrigWeight: 0.5. Unfortunately, we did not have time to tune those parameters for the official deadline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Table <ref type="table" coords="3,117.34,521.75,5.03,9.96" target="#tab_0">1</ref> summarizes the results of our submitted runs, as follows:</p><p>• monolingual without WSD: En2EnNowsd original terms in topics; original terms in documents.</p><p>En2EnNowsdPsrel same as En2EnNowsd, but with PRF.</p><p>• monolingual with WSD: En2EnNusDocsPsrel original terms topics; both original and expanded terms in documents, using best sense according to NUS word sense disambiguation; PRF.</p><p>En2EnUbcDocsPsrel original terms topics; both original and expanded terms in documents, using best sense according to UBC word sense disambiguation; PRF.</p><p>En2EnFullStructTopNusDocsPsrel original and fully expanded terms in topics; both both original and expanded terms in documents, using best sense according to NUS word sense disambiguation; PRF. Es2EnNowsd original terms in topics (in Spanish); translated terms in documents (from English to Spanish).</p><p>Es2EnNowsdPsrel same as Es2EnNowsd, but with PRF.</p><p>• bilingual with WSD:</p><p>Es2EnNusDocsPsrel original terms in topics (in Spanish); translated terms in documents, using the best sense according to NUS word sense disambiguation; PRF.</p><p>Es2EnUbcDocsPsrel original terms in topics (in Spanish); translated terms in documents, using the best sense according to UBC word sense disambiguation; PRF.</p><p>Es2En1stTopsNusDocsPsrel translated terms in topics (from Spanish to English) for first sense in Spanish; both original and expanded terms of the best sense according to NUS disambiguation data; PRF.</p><p>Es2En1stTopsUbcDocsPsrel translated terms in topics (from Spanish to English) for first sense in Spanish; both original and expanded terms of the best sense according to UBC disambiguation data; PRF.</p><p>The results show that the use of WSD data has been effective. With respect to monolingual retrieval, En2EnUbcDocsPsrel obtains the best results from our runs, although no significant difference is found with respect to En2WnNowsdPsrel<ref type="foot" coords="4,355.92,496.99,3.95,6.37" target="#foot_0">1</ref> . Regarding the bilingual results, Es2En1stTopsUbcDocsPsrel is the best, and it is significantly better than Es2EnNowsdPsrel. These results confirm the results we got in the training data. Although not shown here, the results of using WSD are significantly better in the training data with respect to using all senses (full expansion).</p><p>Although it was not our main goal, our systems ranked high in the exercise, making the 7th best in the monolingual no-WSD subtask, 9th in monolingual using WSD, 5th best in the bilingual no-WSD subtask, and 1st in bilingual using WSD. Overall, our systems ranked 4th overall and 3rd overall in the monolingual and bilingual subtasks, respectively.</p><p>After analyzing the experiments and the results, we have found that the approach of expanding the documents works better than expanding the topics. The extensive experimentation that we performed on the use of structured queries did not yield better results than just expanding the documents.</p><p>In our experiments we did not make any effort to deal with hard topics, and we only paid attention to improvements in Mean Average Precision (MAP) metric. That is why the Geometric Mean Average Precision (GMAP) values are lower.</p><p>We have reported our experiments for the Robust-WSD Track at CLEF. All our runs ended up in good ranking, taking into account that these have been our first experiments in the field of information retrieval. This is remarkable, as we did not use any external resources, except the WSD information and Spanish and English wordnets provided by the organizers, and given that we did not do any proper parameter tuning (e.g. in the relevance feedback step) on the training part.</p><p>Our main goal was to get better (CL)IR results using WSD and we achieved it, obtaining remarkable gains in bilingual IR, and smaller gains in monolingual IR. We discovered that using the WSD information for documents was a good strategy, in contrast to most of previous IR work, which has focused on WSD of topics.</p><p>For the future we plan to improve the bilingual results, mainly incorporating external resources. We tried straightforward methods to exploit WSD information for the expansion and indexing of the documents. We would like to pursue more sophisticated methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,96.00,59.87,360.95,197.76"><head>Table 1 :</head><label>1</label><figDesc>Results for submitted runs• bilingual without WSD:</figDesc><table coords="4,96.00,59.87,360.95,146.28"><row><cell></cell><cell></cell><cell>runId</cell><cell>map</cell><cell>gmap</cell></row><row><cell cols="2">monolingual no WSD</cell><cell>En2EnNowsd</cell><cell cols="2">0.3534 0.1488</cell></row><row><cell></cell><cell></cell><cell>En2EnNowsdPsrel</cell><cell cols="2">0.3810 0.1572</cell></row><row><cell></cell><cell cols="2">with WSD En2EnNusDocsPsrel</cell><cell cols="2">0.3862 0.1541</cell></row><row><cell></cell><cell></cell><cell>En2EnUbcDocsPsrel</cell><cell cols="2">0.3899 0.1552</cell></row><row><cell></cell><cell></cell><cell cols="3">En2EnFullStructTopsNusDocsPsrel 0.3890 0.1532</cell></row><row><cell>bilingual</cell><cell>no WSD</cell><cell>Es2EnNowsd</cell><cell cols="2">0.1835 0.0164</cell></row><row><cell></cell><cell></cell><cell>Es2EnNowsdPsrel</cell><cell cols="2">0.1957 0.0162</cell></row><row><cell></cell><cell cols="2">with WSD Es2EnNusDocsPsrel</cell><cell cols="2">0.2138 0.0205</cell></row><row><cell></cell><cell></cell><cell>Es2EnUbcDocsPsrel</cell><cell cols="2">0.2100 0.0212</cell></row><row><cell></cell><cell></cell><cell>Es2En1stTopsNusDocsPsrel</cell><cell cols="2">0.2350 0.0176</cell></row><row><cell></cell><cell></cell><cell>Es2En1stTopsUbcDocsPsrel</cell><cell cols="2">0.2356 0.0172</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,105.24,696.81,316.50,7.32"><p>Paired Randomization Tests over MAPs with α=0.05 have been used along this work</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been supported by <rs type="funder">KNOW</rs> (<rs type="grantNumber">TIN2006-15049-C03-01</rs>) and <rs type="projectName">KYOTO</rs> (<rs type="grantNumber">ICT-2007-211423</rs>). <rs type="person">Arantxa Otegi</rs>'s work is funded by a PhD grant from the <rs type="funder">Basque Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_cZkgYZQ">
					<idno type="grant-number">TIN2006-15049-C03-01</idno>
					<orgName type="project" subtype="full">KYOTO</orgName>
				</org>
				<org type="funding" xml:id="_b6KhY9Y">
					<idno type="grant-number">ICT-2007-211423</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,105.47,348.23,407.20,9.96;5,105.48,360.23,406.99,9.96;5,105.48,372.11,22.88,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,319.03,348.23,193.64,9.96;5,105.48,360.23,86.02,9.96">Indri: A language model-based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,204.04,360.57,304.09,9.18">Proceedings of the International Conference on Intelligence Analysis</title>
		<meeting>the International Conference on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
