<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,177.24,115.72,260.83,12.93">CLEF 2008: Ad Hoc Track Overview</title>
				<funder ref="#_BjH7MvS">
					<orgName type="full">Ministry of Education</orgName>
				</funder>
				<funder ref="#_Vupg3TP">
					<orgName type="full">European Commission</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.32,152.97,57.60,9.96"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
							<email>e.agirre@ehu.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of the Basque Country</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.36,152.97,61.11,9.96"><forename type="first">Giorgio</forename><forename type="middle">M</forename><surname>Di</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.08,152.97,53.71,9.96"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.28,152.97,66.14,9.96"><forename type="first">Thomas</forename><surname>Mandl</surname></persName>
							<email>mandl@uni-hildesheim.de</email>
							<affiliation key="aff2">
								<orgName type="department">Information Science</orgName>
								<orgName type="institution">University of Hildesheim</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.28,164.97,54.45,9.96"><forename type="first">Carol</forename><surname>Peters</surname></persName>
							<email>carol.peters@isti.cnr.it</email>
							<affiliation key="aff3">
								<orgName type="institution">ISTI-CNR</orgName>
								<address>
									<addrLine>Area di Ricerca</addrLine>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,177.24,115.72,260.83,12.93">CLEF 2008: Ad Hoc Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5FCF1DFAFFBA299ED9CBB14C33A64E97</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 [Systems and Software]: Performance evaluation Experimentation, Performance, Measurement, Algorithms Multilingual Information Access, Cross-Language Information Retrieval, Word Sense Disambiguation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the objectives and organization of the CLEF 2008 ad hoc track and discuss the main characteristics of the tasks offered to test monolingual and cross-language textual document retrieval systems. The track was changed considerably this year with the introduction of new document collections consisting of library catalog records derived from The European Library, with a non-European target language, and with a task offering word sense disambiguated data for groups interested in the impact of natural language processing on the performance of information retrieval systems. The track was thus structured in three distinct streams denominated: TEL@CLEF, Persian@CLEF and Robust WSD. The results obtained for each task are presented and statistical analyses are given.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="842.0" lry="595.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="40" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="41" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="42" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="43" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="44" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="45" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="46" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="47" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="48" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="49" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="50" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="51" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="52" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="53" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ad hoc retrieval track is generally considered to be the core track in the Cross-Language Evaluation Forum (CLEF). The aim of this track is to promote the development of monolingual and cross-language textual document retrieval systems. From 2000 -2007, the track used exclusively collections of European newspaper and news agency documents <ref type="foot" coords="2,309.12,128.93,3.97,6.97" target="#foot_0">1</ref> . This year the focus of the track was considerably widened: we introduced very different document collections, a non-European target language, and an information retrieval (IR) task designed to attract participation from groups interested in natural language processing (NLP). The track was thus structured in three distinct streams:</p><p>-TEL@CLEF -Persian@CLEF -Robust WSD The first task offered monolingual and cross-language search on library catalog records and was organized in collaboration with The European Library (TEL) <ref type="foot" coords="2,162.72,266.93,3.97,6.97" target="#foot_1">2</ref> . The second task resembled the ad hoc retrieval tasks of previous years but this time the target collection was a Persian newspaper corpus.</p><p>The third task was the robust activity which this year used word sense disambiguated (WSD) data, and involved English documents and monolingual and cross-language search in Spanish.</p><p>In this paper we first present the track setup, the evaluation methodology and the participation in the different tasks (Section 2). We then describe the main features of each task and show the results (Sections 3 -5). Statistical testing is discussed in Section 6 and the final section provides a brief summing up.</p><p>For information on the various approaches and resources used by the groups participating in this track and the issues they focused on, we refer the reader to the other papers in the Ad Hoc section of these Working Notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Track Setup</head><p>The ad hoc track in CLEF adopts a corpus-based, automatic scoring method for the assessment of system performance, based on ideas first introduced in the Cranfield experiments in the late 1960s <ref type="bibr" coords="2,328.09,483.33,14.60,9.96" target="#b12">[13]</ref>. The tasks offered are studied in order to effectively measure textual document retrieval under specific conditions. The test collections are made up of documents, topics and relevance assessments. The topics consist of a set of statements simulating information needs from which the systems derive the queries to search the document collections. Evaluation of system performance is then done by judging the documents retrieved in response to a topic with respect to their relevance, and computing the recall and precision measures. The distinguishing feature of CLEF is that it applies this evaluation paradigm in a multilingual setting. This means that the criteria normally adopted to create a test collection, consisting of suitable documents, sample queries and relevance assessments, have been adapted to satisfy the particular requirements of the multilingual context. All language dependent tasks such as topic creation and relevance judgment are performed in a distributed setting by native speakers. Rules are established and a tight central coordination is maintained in order to ensure consistency and coherency of topic and relevance judgment sets over the different collections, languages and tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Documents</head><p>Each of the three ad hoc tasks this year used a different set of documents.</p><p>The TEL task used three collections:</p><p>-British Library (BL); 1,000,100 documents, 1.2 GB; -Bibliothéque Nationale de France (BNF); 1,000,100 documents, 1.3 GB; -Austrian National Library (ONB); 869,353 documents, 1.3 GB.</p><p>We refer to the three collections (BL, BNF, ONB) as English, French and German because in each case this is the main and expected language of the collection. However, each of these collections is to some extent multilingual and contains documents (catalog records) in many additional languages.</p><p>The TEL data is very different from the newspaper articles and news agency dispatches previously used in the CLEF ad hoc track. The data tends to be very sparse. Many records contain only title, author and subject heading information; other records provide more detail. The title and (if existing) an abstract or description may be in a different language to that understood as the language of the collection. The subject heading information is normally in the main language of the collection. About 66% of the documents in the English and German collection have textual subject headings, in the French collection only 37%. Dewey Classification (DDC) is not available in the French collection; negligible (¡0.3%) in the German collection; but occurs in about half of the English documents (456,408 docs to be exact).</p><p>Whereas in the traditional ad hoc task, the user searches directly for a document containing information of interest, here the user tries to identify which publications are of potential interest according to the information provided by the catalog card. When we designed the task, the question the user was presumed to be asking was "Is the publication described by the bibliographic record relevant to my information need?"</p><p>The Persian task used the Hamshahri corpus of 1996-2002 newspapers as the target collection. This corpus was made available to CLEF by the Data Base Research Group (DBRG) of the University of Tehran. Hamshahri is one of the most popular daily newspapers in Iran. The Hamshahri corpus is a Persian test collection that consists of 345 MB of news texts for the years 1996 to 2002 (corpus size with tags is 564 MB). This corpus contains more than 160,000 news articles about a variety of subjects and includes nearly 417000 different words. Hamshahri articles vary between 1KB and 140KB in size <ref type="foot" coords="3,383.28,634.01,3.97,6.97" target="#foot_2">3</ref> .</p><p>The robust task used existing CLEF news collections but with word sense disambiguation (WSD) added. The word sense disambiguation data was automatically added by systems from two leading research laboratories, UBC <ref type="bibr" coords="4,451.19,142.29,10.45,9.96" target="#b1">[2]</ref> and NUS <ref type="bibr" coords="4,157.80,154.17,14.60,9.96" target="#b11">[12]</ref>. Both systems returned word senses from the English WordNet, version 1.6.</p><p>The document collections were offered both with and without WSD, and included the following:</p><p>-LA Times 94 (with word sense disambiguated data); ca 113,000 documents, 425 MB without WSD, An excerpt for a document<ref type="foot" coords="4,270.36,276.05,3.97,6.97" target="#foot_3">4</ref> is shown in Figure <ref type="figure" coords="4,365.29,277.53,3.90,9.96" target="#fig_0">1</ref>, where each term in the document is followed by its senses with their respective scores as assigned by the automatic WSD system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topics</head><p>Topics in the CLEF ad hoc track are structured statements representing information needs. Each topic typically consists of three parts: a brief "title" statement; a one-sentence "description"; a more complex "narrative" specifying the relevance assessment criteria. Topics are prepared in xml format and identified by means of a Digital Object Identifier (DOI) <ref type="foot" coords="4,319.44,397.49,3.97,6.97" target="#foot_4">5</ref> of the experiment <ref type="bibr" coords="4,407.17,398.85,15.49,9.96" target="#b35">[35]</ref> which allows us to reference and cite them.</p><p>For the TEL task, a common set of 50 topics was prepared in each of the 3 main collection languages (English, French and German) plus Dutch and Spanish in response to demand. Only the Title and Description fields were released to the participants. The narrative was employed to provide information for the assessors on how the topics should be judged. The topic sets were prepared on the basis of the contents of the collections.</p><p>In ad hoc, when a task uses data collections in more than one language, we consider it important to be able to use versions of the same core topic set to query all collections. This makes it easier to compare results over different collections and also facilitates the preparation of extra topic sets in additional languages. However, it is never easy to find topics that are effective for several different collections and the topic preparation stage requires considerable discussion between the coordinators for each collection in order to identify suitable common candidates. The sparseness of the data made this particularly difficult for the TEL task and tended to lead to the formulation of topics that were quite broad in scope so that at least some relevant documents could be found in each collection. A result of this strategy is that there tends to be a considerable lack of evenness of distribution in relevant documents. For each topic, the results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;DOC&gt;</head><p>&lt;DOCNO&gt;GH950102-000000&lt;/DOCNO&gt; &lt;DOCID&gt;GH950102-000000&lt;/DOCID&gt; &lt;HEADLINE&gt; &lt;TERM ID="GH950102-000000-1" LEMA="alien" POS="JJ"&gt; &lt;WF&gt;Alien&lt;/WF&gt; &lt;SYNSET SCORE="0.6" CODE="01295935-a"/&gt; &lt;SYNSET SCORE="0.4" CODE="00984080-a"/&gt; &lt;/TERM&gt; &lt;TERM ID="GH950102-000000-2" LEMA="treatment" POS="NN"&gt; &lt;WF&gt;treatment&lt;/WF&gt; &lt;SYNSET SCORE="0.827904118008605" CODE="00735486-n"/&gt; &lt;SYNSET SCORE="0" CODE="03857483-n"/&gt; &lt;SYNSET SCORE="0.172095881991395" CODE="00430183-n"/&gt; &lt;SYNSET SCORE="0" CODE="05340429-n"/&gt; &lt;/TERM&gt; &lt;TERM ID="GH950102-000000-3" LEMA="be" POS="VBZ"&gt; &lt;WF&gt;is&lt;/WF&gt; &lt;SYNSET SCORE="0.0113384126222329" CODE="01787769-v"/&gt; &lt;SYNSET SCORE="0.181174635551023" CODE="01784339-v"/&gt; &lt;SYNSET SCORE="0.644489771431999" CODE="01775973-v"/&gt; &lt;SYNSET SCORE="0.00515927770112184" CODE="01666138-v"/&gt; &lt;SYNSET SCORE="0.0420541124242606" CODE="01775163-v"/&gt; &lt;SYNSET SCORE="0.00347951286819845" CODE="01840295-v"/&gt; &lt;SYNSET SCORE="0.0540524326594277" CODE="01811792-v"/&gt; &lt;SYNSET SCORE="0" CODE="01843641-v"/&gt; &lt;SYNSET SCORE="0.000119983202351671" CODE="01552250-v"/&gt; &lt;SYNSET SCORE="0.0418741376207331" CODE="01781222-v"/&gt; &lt;SYNSET SCORE="5.99916011758354e-05" CODE="01782836-v"/&gt; &lt;SYNSET SCORE="0.0161977323174756" CODE="01817610-v"/&gt; &lt;/TERM&gt; ... expected from the separate collections can vary considerably, e.g. in the case of the TEL task, a topic of particular interest to Britain, such as the example given in Figure <ref type="figure" coords="5,205.34,508.17,3.90,9.96" target="#fig_1">2</ref>, can be expected to find far more relevant documents in the BL collection than in BNF or ONB.</p><p>For the Persian task, 50 topics were created in Persian by the Data Base Research group of the University of Tehran, and then translated into English. The rule in CLEF when creating topics in additional languages is not to produce literal translations but to attempt to render them as naturally as possible. This was a particularly difficult task when going from Persian to English as cultural differences had to be catered for.</p><p>For example, Iran commonly uses a different calendar from Europe and reference was often made in the Persian topics to events that are well known to Iranian society but not often discussed in English. This is shown in the example of Figure <ref type="figure" coords="5,178.69,643.53,3.90,9.96">3</ref>, where the rather awkward English rendering evidences the uncertainty of the translator.</p><p>&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt; &lt;topic&gt; &lt;identifier&gt;10.2452/451-AH&lt;/identifier&gt; &lt;title lang="en"&gt;Roman Military in Britain&lt;/title&gt; &lt;title lang="de"&gt;Römisches Militär in Britannien&lt;/title&gt; &lt;title lang="es"&gt;El ejército romano en Britania&lt;/title&gt; &lt;title lang="fr"&gt;L'armée romaine en Grande-Bretagne&lt;/title&gt; &lt;title lang="nl"&gt;Romeinse Leger in Groot-Brittannie&lt;/title&gt; &lt;description lang="en"&gt;Find books or publications on the Roman invasion or military occupation of Britain.&lt;/description&gt; &lt;description lang="de"&gt;Finden Sie Bücher oder Publikationen über die römische Invasion oder das Militär in Britannien.&lt;/description&gt; &lt;description lang="es"&gt;Encuentre libros o publicaciones sobre la invasión romana o la ocupación militar romana en Britania.&lt;/description&gt; &lt;description lang="fr"&gt;Trouver des livres ou des publications sur l'invasion et l'occupation de la Grande-Bretagne par les Romains.&lt;/description&gt; &lt;description lang="nl"&gt;Vind boeken of publicaties over de Romeinse invasie of bezetting van Groot-Brittannie.&lt;/description&gt; &lt;/topic&gt; All topics were offered both with and without WSD. Topics in English were disambiguated by both UBC <ref type="bibr" coords="6,260.06,489.33,10.45,9.96" target="#b1">[2]</ref> and NUS <ref type="bibr" coords="6,314.56,489.33,15.49,9.96" target="#b11">[12]</ref> systems, yielding word senses from WordNet version 1.6. A large-scale disambiguation system for Spanish was not available, so we used the first-sense heuristic, yielding senses from the Spanish wordnet, which is tightly aligned to the English WordNet version 1.6 (i.e., they share synset numbers or sense codes). An excerpt for a topic<ref type="foot" coords="6,394.68,535.73,3.97,6.97" target="#foot_5">6</ref> is shown in Figure <ref type="figure" coords="6,134.76,549.09,3.90,9.96" target="#fig_2">4</ref>, where each term in the topic is followed by its senses with their respective scores as assigned buy the automatic WSD system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relevance Assessment</head><p>The number of documents in large test collections such as CLEF makes it impractical to judge every document for relevance. Instead approximate recall values are calculated using pooling techniques. The results submitted by the groups participating in the ad hoc tasks are used to form a pool of documents for each topic and language by collecting the highly ranked documents from selected runs according to a set of predefined criteria. Traditionally, the top 100 ranked documents from each of the runs selected are included in the pool; in such a case we say that the pool is of depth 100. This pool is then used for subsequent relevance judgments. After calculating the effectiveness measures, the results are analyzed and run statistics produced and distributed. The stability of pools constructed in this way and their reliability for postcampaign experiments is discussed in <ref type="bibr" coords="7,300.96,447.33,10.45,9.96" target="#b8">[9]</ref> with respect to the CLEF 2003 pools. New pools were formed in CLEF 2008 for the runs submitted for the TEL and the Persian mono-and bilingual tasks. Instead, the robust tasks used the original pools and relevance assessments from previous CLEF campaigns.</p><p>The main criteria used when constructing the pools were:</p><p>favour diversity among approaches adopted by participants, according to the descriptions of the experiments provided by the participants; -choose at least one experiment for each participant in each task, chosen among the experiments with highest priority as indicated by the participant; -add mandatory title+description experiments, even though they do not have high priority; -add manual experiments, when provided; -for bilingual tasks, ensure that each source topic language is represented.</p><p>One important limitation when forming the pools is the number of documents to be assessed. Last year, for collections of newspaper documents, we estimated that assessors could judge from 60 to 100 documents per hour, providing binary &lt;top&gt; &lt;num&gt;10.2452/141-WSD-AH&lt;/num&gt; &lt;EN-title&gt; &lt;TERM ID="10.2452/141-WSD-AH-1" LEMA="letter" POS="NNP"&gt; &lt;WF&gt;Letter&lt;/WF&gt; &lt;SYNSET SCORE="0" CODE="05115901-n"/&gt; &lt;SYNSET SCORE="0" CODE="05362432-n"/&gt; &lt;SYNSET SCORE="0" CODE="05029514-n"/&gt; &lt;SYNSET SCORE="1" CODE="04968965-n"/&gt; &lt;/TERM&gt; &lt;TERM ID="10.2452/141-WSD-AH-2" LEMA="bomb" POS="NNP"&gt; &lt;WF&gt;Bomb&lt;/WF&gt; &lt;SYNSET SCORE="0.888888888888889" CODE="02310834-n"/&gt; &lt;SYNSET SCORE="0" CODE="05484679-n"/&gt; &lt;SYNSET SCORE="0.  judgments: relevant / not relevant. Our estimate this year for the TEL catalog records was higher as these records are much shorter than the average newspaper article (100 to 120 documents per hour). In both cases, it can be seen what a time-consuming and resource expensive task human relevance assessment is. This limitation impacts strongly on the application of the criteria above -and implies that we are obliged to be flexible in the number of documents judged per selected run for individual pools.</p><p>This meant that this year, in order to create pools of more-or-less equivalent size (approx. 25,000 documents), the depth of the TEL English, French, and German pools and of the Persian pool was 60 <ref type="foot" coords="8,333.72,607.97,3.97,6.97" target="#foot_6">7</ref> .</p><p>Table <ref type="table" coords="9,178.68,118.29,4.98,9.96" target="#tab_3">1</ref> reports summary information on the 2008 ad hoc pools used to calculate the results for the main monolingual and bilingual experiments. In particular, for each pool, we show the number of topics, the number of runs submitted, the number of runs included in the pool, the number of documents in the pool (relevant and non-relevant), and the number of assessors.</p><p>The box plot of Figure <ref type="figure" coords="9,247.46,183.81,4.98,9.96" target="#fig_3">5</ref> compares the distributions of the relevant documents across the topics of each pool for the different ad hoc pools; the boxes are ordered by decreasing mean number of relevant documents per topic.</p><p>As can be noted, TEL English, French and German distributions appear similar and are asymmetric towards topics with a greater number of relevant documents. Both the English and French distributions show some upper outliers, i.e. topics with a greater number of relevant document with respect to the behaviour of the other topics in the distribution. These outliers are probably due to the fact that CLEF topics have to be able to retrieve relevant documents in all the collections; therefore, they may be considerably broader in one collection compared with others depending on the contents of the separate datasets.</p><p>For the TEL documents, we judged for relevance only those documents that are written totally or partially in English, French and German (and Spanish for searches on the English collection as we expected this language to be used only for ES to EN runs), e.g. a catalog record written entirely in Hungarian was counted as not relevant as it was of no use to our hypothetical user; however, a catalog record with perhaps the title and a brief description in Hungarian, but with subject descriptors in French, German or English was judged for relevance as it could be potentially useful. Our assessors had no additional knowledge of the documents referred to by the catalog records (or surrogates) contained in the collection. They judged for relevance on the information contained in the records made available to the systems. This was a non trivial task due to the lack of information present in the documents. During the relevance assessment activity there was much consultation between the assessors for the three TEL collections in order to ensure that the same assessment criteria were adopted by everyone.</p><p>As shown in the box plot of Figure <ref type="figure" coords="9,313.95,512.01,3.90,9.96" target="#fig_3">5</ref>, the Persian distribution presents a greater number of relevant documents per topic with respect to the other distributions and is more symmetric between topics with lesser or greater number of relevant documents. This greater symmetry in distribution of relevant documents is probably due to the fact that the topic set was created just on the basis of the contents of the Persian collection, rather than needing to reflect the contents of multiple collections. In addition, as can be seen from Table <ref type="table" coords="9,445.33,583.77,3.90,9.96" target="#tab_3">1</ref>, it has been possible to sample all the experiments submitted for the Persian tasks. This means that there were fewer unique documents per run and this fact, together with the greater number of relevant documents per topic suggests either that all the systems were using similar approaches and retrieval algorithms (however this is not so -see Section 4 below) or that the systems found the Persian topics quite easy.   The relevance assessment for the Persian results was done by the DBRG group in Tehran. Again, assessment was performed on a binary basis and the standard CLEF assessment rules were applied.</p><p>As has already been stated, the robust WSD task used existing relevance assessments from previous years. The relevance assessments regarding the training topics were provided to participants before competition time.</p><p>This year, we tried a slight improvement with respect to the traditional pooling strategy adopted so far in CLEF. During the topic creation phase, the assessors express their opinion about the relevance of the documents they inspect with respect to the topic. Although this opinion may change during the various discussions between assessors in this phase, we consider these indications as potentially useful in helping to strengthen the pools of documents that will be judged for relevance. They are thus added to the pools. However, the assessors are not informed of which documents they had previously judged in order not to bias them in any way.</p><p>Similarly to last year, in his paper, Stephen Tomlinson, has reported some sampling experiments aimed at estimating the judging coverage for the CLEF 2008 test collections. He finds that this tends to be lower than the estimates he produced for the CLEF 2007 collections. With respect to the TEL collections, the implication is that at best 50% to 70% of the relevant documents are included in the pools -and that most of the unjudged relevant documents are for the 10 or more queries that have the most known answers <ref type="bibr" coords="12,354.36,655.41,14.60,9.96" target="#b40">[40]</ref>. For Persian the coverage seems to be lower; this could be a result of the fact that all the Persian topics tend to be relatively broad. It is our intention to look more closely into the question of coverage of these pools by performing some post-workshop stability tests. The results will be reported in our Proceedings paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Result Calculation</head><p>Evaluation campaigns such as TREC and CLEF are based on the belief that the effectiveness of Information Retrieval Systems (IRSs) can be objectively evaluated by an analysis of a representative set of sample search results. For this, effectiveness measures are calculated based on the results submitted by the participants and the relevance assessments. Popular measures usually adopted for exercises of this type are Recall and Precision. Details on how they are calculated for CLEF are given in <ref type="bibr" coords="13,284.30,285.33,14.60,9.96" target="#b9">[10]</ref>. For the robust task, we used additional measures, see Section 5.</p><p>The individual results for all official ad hoc experiments in CLEF 2008 are given in the Appendices at the end of these Working Notes <ref type="bibr" coords="13,395.91,322.17,15.92,9.96" target="#b16">[17,</ref><ref type="bibr" coords="13,411.83,322.17,11.94,9.96" target="#b17">18,</ref><ref type="bibr" coords="13,423.77,322.17,11.94,9.96" target="#b18">19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Participants and Experiments</head><p>As shown in Table <ref type="table" coords="13,218.65,381.57,3.90,9.96" target="#tab_4">2</ref>, a total of 24 groups from 14 different countries submitted official results for one or more of the ad hoc tasks -a slight increase on the 22 participants of last year. Table <ref type="table" coords="13,292.70,405.45,4.98,9.96" target="#tab_5">3</ref> provides a breakdown of the number of participants by country <ref type="foot" coords="13,237.12,416.09,3.97,6.97" target="#foot_7">8</ref>A total of 289 runs were submitted with an increase of about 22% on the 235 runs of 2007. The average number of submitted runs per participant also increased: from 10.6 runs/participant of 2007 to 12.0 runs/participant of this year.</p><p>Participants were required to submit at least one title+description ("TD") run per task in order to increase comparability between experiments. The large majority of runs (215 out of 289, 74.40%) used this combination of topic fields, 27 (9.34%) used all fields <ref type="foot" coords="13,248.88,513.65,3.97,6.97" target="#foot_8">9</ref> , 47 (16.26%) used the title field. The majority of experiments were conducted using automatic query construction (273 out of 289, 94.47%) and only in a small fraction of the experiments (16 out 289, 5.53%) were queries been manually constructed from topics. A breakdown into the separate tasks is shown in Table <ref type="figure" coords="13,239.28,562.77,16.43,9.96" target="#fig_2">4(a)</ref>.</p><p>Seven different topic languages were used in the ad hoc experiments. As always, the most popular language for queries was English, with Farsi second. The number of runs per topic language is shown in Table <ref type="table" coords="13,388.24,599.61,4.20,9.96" target="#tab_6">4</ref>(b). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TEL@CLEF</head><p>The objective of this activity was to search and retrieve relevant items from collections of library catalog cards. The underlying aim was to identify the most effective retrieval technologies for searching this type of very sparse data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tasks</head><p>Two subtasks were offered: Monolingual and Bilingual. In both tasks, the aim was to retrieve documents relevant to the query. By monolingual we mean that the query is in the same language as the expected language of the collection. By bilingual we mean that the query is in a different language to the expected language of the collection. For example, in an EN → FR run, relevant documents (bibliographic records) could be any document in the BNF collection (referred to as the French collection) in whatever language they are written. The same is true for a monolingual FR → FR run -relevant documents from the BNF collection could actually also be in English or German, not just French.</p><p>In CLEF 2008, the activity we simulated was that of users who have a working knowledge of English, French and German (plus wrt the English collection also Spanish) and who want to discover the existence of relevant documents that can be useful for them in one of our three target collections. One of our suppositions was that, knowing that these collections are to some extent multilingual, some systems may attempt to use specific tools to discover this. For example, a system trying the cross-language English to French task on the BNF target collection but knowing that documents retrieved in English and German will also be judged for relevance might choose to employ an English-German as well as the probable English-French dictionary. Groups attempting anything of this type were asked to declare such runs with a ++ indication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Participants</head><p>13 groups submitted 153 runs for the TEL task: all groups submitted monolingual runs (96 runs out of 153); 8 groups also submitted bilingual runs (57 runs out of 153). Table <ref type="table" coords="16,215.53,499.53,4.45,9.96" target="#tab_6">4</ref>(a) provides a breakdown of the number of participants and submitted runs by task.</p><p>3.3 Results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Results</head><p>Table <ref type="table" coords="16,161.64,594.81,4.98,9.96" target="#tab_7">5</ref> shows the top five groups for each target collection, ordered by mean average precision. The table reports: the short name of the participating group; the mean average precision achieved by the experiment; the DOI of the experiment; and the performance difference between the first and the last participant. Figures <ref type="figure" coords="16,184.80,643.53,7.81,9.96">7,</ref><ref type="figure" coords="16,195.61,643.53,3.90,9.96">9</ref>, and 11 compare the performances of the top participants of the TEL Monolingual tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual Results</head><p>Table <ref type="table" coords="17,161.64,398.85,4.98,9.96" target="#tab_8">6</ref> shows the top five groups for each target collection, ordered by mean average precision. The table reports: the short name of the participating group; the mean average precision achieved by the experiment; the DOI of the experiment; and the performance difference between the first and the last participant. Figures 8, 10, and 12 compare the performances of the top participants of the TEL Bilingual tasks.</p><p>For bilingual retrieval evaluation, a common method is to compare results against monolingual baselines. We have the following results for CLEF 2008:</p><p>-X → EN: 90.99% of best monolingual English IR system; -X → FR: 56.63% of best monolingual French IR system; -X → DE: 53.15% of best monolingual German IR system.</p><p>While the best result for English, obtained with German topics, is very good and can be considered as state-of-the-art for a good cross-language system running on well-tested languages with reliable processing tools and resources such as English and German, the results for the other two target collections are fairly disappointing. We have no explanation for this at the present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Approaches</head><p>In the TEL experiments, all the traditional approaches to monolingual and crosslanguage retrieval were attempted by the different groups. Retrieval algorithms Fig. <ref type="figure" coords="20,273.37,723.27,9.01,8.13" target="#fig_1">12</ref>. Bilingual German included language models, vector-space and probabilistic approaches, and translation resources ranged from bilingual dictionaries, parallel and comparable corpora, to on-line MT systems and Wikipedia. Groups often used a combination of more than one resource.</p><p>One of the most interesting and new features of the TEL task was the multilinguality of the collections. Only about half of each collection was in the national language (English, French or German), with virtually all other languages represented by one or more entries in one or another of the collections. However, only a few groups took this into specific consideration trying to devise ways to address this aspect and, somewhat disappointingly, their efforts do not appear to have been particularly rewarded by improved performance.</p><p>An example of this is the group from the Technical University of Chemnitz, which had overall the best results in the bilingual tasks (1st for XtoEN; 2nd for XtoFR and DE) although they did not do so well in the monolingual tasks. This group attempted to tackle the multilinguality of the collections in several ways. First, they tried to identify the language of each record in the collections using a language detector. Unfortunately, due to an error, they were unable to use the indices created in this way <ref type="foot" coords="21,268.80,321.53,7.93,6.97" target="#foot_9">10</ref> . Second, in both their monolingual and crosslanguage experiments they implemented a retrieval algorithm which translated the query into the top 10 (in terms of occurrence) languages and merged these multilingual terms into a single query. They ran experiments weighting the query in different ways on the basis of estimated distribution of language content in the collections. In the monolingual experiments, rather disappointingly, the results showed that their purely monolingual baseline always out performed experiments with query translations and language weights. This finding was confirmed with the bilingual experiments where again the better results were achieved with the baseline configurations. They attributed their good overall results for bilingual to the superiority of the Google online translation service <ref type="bibr" coords="21,388.56,442.53,14.60,9.96" target="#b27">[27]</ref>.</p><p>Another group that attempted to tackle the multinguality of the target collections was Xerox. This group built a single index containing all languages (according to the expected languages which they identified as just English, French and German although as stated the collections actually contain documents in other languages as well). This, of course, meant that the queries also had to be issued in all three languages. They built a multilingual probabilistic dictionary and for each target collection gave more weight to the official language of the collection <ref type="bibr" coords="21,178.80,538.77,14.60,9.96" target="#b13">[14]</ref>. Although their results for both monolingual and bilingual experiments for the French and German collections were always within the top five; they were not quite so successful with the English collection.</p><p>However, most groups appear to have ignored the multilinguality of the single collections in their experiments. Good examples of this are three veteran CLEF groups, UniNE which had, overall the best monolingual results, JHU which appeared in the top five for all bilingual tasks, and Berkeley which figured in the top five for all experiments except for monolingual German. UniNe appeared to focus on testing different IR models and combination approaches whereas the major interest of JHU was on the most efficient methods for indexing. Berkeley tested a version the Logistic Regression (LR) algorithm that has been used very successfully in cross-language IR by Berkeley researchers for a number of years together with blind relevance feedback <ref type="bibr" coords="22,305.90,166.17,14.60,9.96" target="#b19">[20]</ref>, <ref type="bibr" coords="22,330.75,166.17,14.60,9.96" target="#b31">[31]</ref>, <ref type="bibr" coords="22,355.71,166.17,14.60,9.96" target="#b28">[28]</ref>.</p><p>As was mentioned in Section 2.1, the TEL data is structured data; participants were told that they could use all fields. Some groups attempted to exploit this by weighting the contents of different fields differently. See, for example <ref type="bibr" coords="22,465.13,202.05,15.49,9.96" target="#b29">[29]</ref> To sum up, from a preliminary scanning of the results of this task, it appears that the majority of groups took it as a traditional ad hoc retrieval task and applied traditional methods. However, it is far too early to confirm whether this is really the best approach to retrieval on library catalog cards. We expect that this issue will be discussed at the workshop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Persian@CLEF</head><p>This activity was coordinated in collaboration with the Data Base Research Group (DBRG) of Tehran University. It was the first time that CLEF offered a non-European language target collection. Persian is an Indo-European language spoken in Iran, Afghanistan and Tajikistan. It is also known as Farsi. However, the Academy of Persian Language and Literature has declared in an official pronouncement that the name "Persian" is more appropriate, as it has the longer tradition in the western languages and better expresses the role of the language as a mark of cultural and national continuity.</p><p>We chose Persian as our first non-European target language for a number of reasons: its challenging script (a modified version of the Arabic alphabet with elision of short vowels) which is written from right to left; its morphology (extensive use of suffixes and compounding); its political and cultural importance. However, the main influencing factor was the generous offer from DBRG to provide an important newspaper corpus (Hamshahri) as the target collection and to be responsible for the coordination of the activity. This collaboration has proved very fruitful and intellectually stimulating and we hope that it will continue in 2009.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tasks</head><p>The activity was organised as a typical ad hoc text retrieval task on newspaper collections. Two tasks were offered: monolingual retrieval; cross-language retrieval (English queries to Persian target) and 50 topics were prepared (see section 2.2). For each topic, participants had to find relevant documents in the collection and submit the results in a ranked list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Participants</head><p>Eight groups submitted 66 runs for the Persian task: all eight submitted monolingual runs (53 runs out of 66); 3 groups also submitted bilingual runs (13 runs out of 66). Five of the groups were formed of Persian native speakers, mostly from the University of Tehran; they were all first time CLEF participants. Unfortunately, at the time of writing we just have reports from four of these groups.</p><p>The other three groups were CLEF veterans with much experience in the CLEF ad hoc track. Table <ref type="table" coords="23,176.16,369.33,4.45,9.96" target="#tab_6">4</ref>(a) provides a breakdown of the number of participants and submitted runs by task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Table <ref type="table" coords="23,161.64,432.21,4.98,9.96" target="#tab_10">7</ref> shows the top five groups for each target collection, ordered by mean average precision. The table reports: the short name of the participating group; the mean average precision achieved by the experiment; the DOI of the experiment; and the performance difference between the first and the last participant.</p><p>Figures <ref type="figure" coords="23,185.52,480.09,10.02,9.96" target="#fig_0">13</ref> and<ref type="figure" coords="23,219.01,480.09,10.02,9.96" target="#fig_2">14</ref> compare the performances of the top participants of the Persian tasks.</p><p>For bilingual retrieval evaluation, a common method is to compare results against monolingual baselines. We have the following results for CLEF 2008:</p><p>-X → FA: 92.26% of best monolingual Farsi IR system. This appears to be in line with state-of-the-art performance for cross-language systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Approaches</head><p>As was to be expected a common theme in a number of the papers was the most effective way to handle the Persian morphology. The group with the best results in the monolingual task tested three approaches; no stemming, a light stemmer developed in-house, and a 4-gram indexing approach. Their best results were Fig. <ref type="figure" coords="24,274.69,723.27,9.01,8.13" target="#fig_2">14</ref>. Bilingual Persian achieved using their light stemmer which has been made freely available on their website. However, they commented that the loss in performance with the no stemming approach was not very great. This group also tested three probabilistic models: Okapi, DFR and statistical language model (LM). The best results were obtained with the latter two <ref type="bibr" coords="25,286.45,166.17,14.60,9.96" target="#b19">[20]</ref>. The group with the second best results compared several different forms of textual normalization: character n-grams, n-gram stems, ordinary words, words automatically segmented into morphemes, and a novel form of n-gram indexing based on n-grams with character skips. They found that that character 5-grams and skipgrams performed the best <ref type="bibr" coords="25,462.35,213.93,14.60,9.96" target="#b31">[31]</ref>. The findings of <ref type="bibr" coords="25,205.58,225.93,15.49,9.96" target="#b19">[20]</ref> were confirmed by <ref type="bibr" coords="25,305.79,225.93,14.60,9.96" target="#b40">[40]</ref>. This group also tested runs with no stemming, with the UniNE stemmer and with n-grams. Similarly, they reported that stemming had relatively little impact.</p><p>Somewhat surprisingly, most of the papers from the Iran-based groups do not provide much information wrt morphological analysis or stemming in their papers. One mentions the application of a light Porter-like stemmer but reported that the algorithm adopted was too simple and results did not improve <ref type="bibr" coords="25,443.29,298.29,10.00,9.96" target="#b6">[7]</ref>. Only one of these groups provides some detailed discussion of the impact of stemming. This group used a simple stemmer (PERSTEM <ref type="foot" coords="25,349.08,320.93,7.93,6.97" target="#foot_10">11</ref> and reported that in most cases stemming did improve performance but noted that this was in contrast with experiments conducted by other groups at the University of Tehran on on the same collection. They suggest that further experiments with different types of stemmers and stemming techniques are required in order to clarify the role of stemming in Persian text processing <ref type="bibr" coords="25,317.77,382.05,14.60,9.96" target="#b26">[26]</ref>. Two of the Persian groups also decided to annotate the corpus with part-of-speech tags in order to evaluate the impact of such information on the performance of the retrieval algorithms <ref type="bibr" coords="25,462.37,405.93,14.60,9.96" target="#b24">[24]</ref>, <ref type="bibr" coords="25,134.76,417.93,14.60,9.96" target="#b26">[26]</ref>. The results reported do not appear to show any great boost in performance.</p><p>Other experiments by the groups from Iran included an investigation into the effect of fusion of different retrieval technique. Two approaches were tested: combining the results of nine distinct retrieval methods; combining the results of the same method but with different types of tokens. The second strategy applied a vector space model and ran it with three different types of tokens namely 4grams, stemmed single terms and unstemmed single terms. This approach gave better results <ref type="bibr" coords="25,196.08,502.29,10.00,9.96" target="#b0">[1]</ref>.</p><p>For the cross-language task, the English topics were translated into Persian. As remarked above, the task of the translators was not easy as it was both a cross-language and also a cross-cultural task. The best result -again by a CLEF veteran participant -obtained 92% of the best monolingual performance. This is well in line with state-of-the-art performance for good cross-language retrieval systems. This group used an online machine translation system applied to the queries<ref type="foot" coords="25,165.12,585.29,7.93,6.97" target="#foot_11">12</ref>  <ref type="bibr" coords="25,176.76,586.65,14.60,9.96" target="#b31">[31]</ref>.</p><p>The other two submissions for the cross-language task were from Iran-based groups. We have received a report from just one of them. This group applied both query and document translation. For query translation they used a method based on the estimation of translation probabilities. In the document translation part they used the Shiraz machine translation system to translate the documents into English. They then created a Hybrid CLIR system by score-based merging of the two retrieval system results. The best performance was obtained with the hybrid system, confirming the reports of other researchers in previous CLEF campaigns, and elsewhere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Robust -WSD Experiments</head><p>The robust task ran for the third time at CLEF 2008. It is an ad-hoc retrieval task based on data of previous CLEF campaigns. The robust task emphasizes the difficult topics by a non-linear integration of the results of individual topics into one result for a system, using the geometric mean of the average precision for all topics (GMAP) as an additional evaluation measure <ref type="bibr" coords="26,402.05,282.21,15.46,9.96" target="#b37">[37,</ref><ref type="bibr" coords="26,417.51,282.21,11.60,9.96" target="#b41">41]</ref>. Given the difficulty of the task, training data including topics and relevance assessments was provided for the participants to tune their systems to the collection.</p><p>This year the robust task also incorporates word sense disambiguation information provided by the organizers to the participants. The task follows the 2007 joint SemEval-CLEF task <ref type="bibr" coords="26,251.06,342.09,10.00,9.96" target="#b2">[3]</ref>, and has the aim of exploring the contribution of word sense disambiguation to monolingual and cross-language information retrieval. Note that a similar exercise was also run in the question answering track at CLEF 2008 (see paper on the QA track on this working notes). The goal of the task is to test whether WSD can be used beneficially for retrieval systems, and thus participants were required to submit at least one baseline run without WSD and one run using the WSD annotations. Participants could also submit four further baseline runs without WSD and four runs using WSD.</p><p>The experiment involved both monolingual (topics and documents in English) and bilingual experiments (topics in Spanish and documents in English). In addition to the original documents and topics, the organizers of the task provided both documents and topics which had been automatically tagged with word senses from WordNet version 1.6 using two state-of-the-art word sense disambiguation systems, UBC <ref type="bibr" coords="26,256.91,497.49,10.45,9.96" target="#b1">[2]</ref> and NUS <ref type="bibr" coords="26,313.93,497.49,14.60,9.96" target="#b11">[12]</ref>. These systems provided weighted word sense tags for each of the nouns, verbs, adjectives and adverbs that they could disambiguate.</p><p>In addition, the participants could use publicly available data from the English and Spanish wordnets in order to test different expansion strategies. Note that given the tight alignment of the Spanish and English wordnets, the wordnets could also be used to translate directly from one sense to another, and perform expansion to terms in another language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Participants</head><p>Eight groups submitted 63 runs for the Robust tasks: all groups submitted monolingual runs (45 runs out of 63); 4 groups also submitted bilingual runs (18 runs out of 63). Moreover, 7 groups participated in the WSD tasks, submitting 40 out of 63 runs, 30 monolingual and 10 bilingual. Table <ref type="table" coords="27,354.90,312.57,4.45,9.96" target="#tab_6">4</ref>(a) provides a breakdown of the number of participants and submitted runs by task. Two further groups were late, so they are not included in the official results but they do have working notes papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Results</head><p>Table <ref type="table" coords="27,163.56,430.17,4.98,9.96" target="#tab_12">8</ref> shows the best results for this task. The performance difference between the best and the last (up to 5) placed group is given (in terms of average precision). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual Results</head><p>Table <ref type="table" coords="27,163.56,536.61,4.98,9.96" target="#tab_14">9</ref> shows the best results for this task. The performance difference between the best and the last (up to 5) placed group is given (in terms of average precision). All the experiments where from English to French.</p><p>For bilingual retrieval evaluation, a common method is to compare results against monolingual baselines. We have the following results for CLEF 2008:</p><p>-X → EN: 80.59% of best monolingual English IR system (MAP); -X → EN WSD: 52.38% of best monolingual English IR system (MAP);    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>In this section we will focus on the comparison between WSD and non-WSD runs. Overall, the best GMAP result in the monolingual system was for a run using WSD, but the best MAP was obtained for a non-WSD run. Several other participants were able to obtain their best MAP and GMAP scores using WSD information. In the bilingual experiments, the best results in MAP and GMAP were for non-WSD runs, but several participants were able to profit from the WSD annotations.</p><p>In the monolingual experiments, cf. Table <ref type="table" coords="30,335.42,426.81,3.90,9.96" target="#tab_12">8</ref>, the best results overall in both MAP and GMAP were for UNINE. Their WSD runs scored very similar to the non-WSD runs, with a slight decrease of MAP (0.16 percentage points) and a slight increase of GMAP (0.27 percentage points) <ref type="bibr" coords="30,354.26,462.69,14.60,9.96" target="#b19">[20]</ref>. The second best scoring team in MAP was UCM, which did attain MAP and GMAP improvements using WSD (from 38.34 MAP -15.28 GMAP in their best non-WSD run to 39.57 MAP -16.17 GMAP in their best WSD run) <ref type="bibr" coords="30,303.52,498.57,14.60,9.96" target="#b36">[36]</ref>. The third best scoring team in MAP was GENEVA who achieved lower scores on both MAP and GMAP using WSD information <ref type="bibr" coords="30,188.30,522.45,14.60,9.96" target="#b21">[22]</ref>. The fourth best team, IXA, obtained better MAP results using WSD information (from 38.10 to 38.99 MAP), but lower GMAP (from 15.72 to 15.52) <ref type="bibr" coords="30,179.04,546.33,14.60,9.96" target="#b34">[34]</ref>. Regarding the rest of participants, while UFRGS and UNIBA obtained improvements, KNOW-CENTER did not, and INAOE only submitted non-WSD runs. There were two additional groups (IRn and SINAI) that sent their results late. Both groups had their best scores for non-WSD systems. Please check the respective working notes reports for specific results.</p><p>In the bilingual experiments, cf. Table <ref type="table" coords="30,327.75,607.65,3.90,9.96" target="#tab_14">9</ref>, the best results overall in both MAP and GMAP were for UFRGS, with a system which did not use WSD annotations (36.39, compared to 21.77 MAP for their best run using WSD) <ref type="bibr" coords="30,462.38,631.53,14.60,9.96" target="#b15">[16]</ref>. The second scoring team, GENEVA also failed to profit from WSD annotations (30.36 compared to 9.70 MAP) <ref type="bibr" coords="30,280.46,655.41,14.60,9.96" target="#b21">[22]</ref>. The other two participating groups did obtain improvements, with IXA attaining 23.56 MAP with WSD (compared to 19.57 without) and UNIBA attaining 7.23 MAP) <ref type="bibr" coords="31,351.77,130.29,14.60,9.96" target="#b34">[34]</ref>, <ref type="bibr" coords="31,376.73,130.29,9.91,9.96" target="#b7">[8]</ref>.</p><p>All in all, the exercise showed that some teams did improve results using WSD annotations (up to aprox. 1 MAP points in monolingual and aprox. 4 MAP points in bilingual), providing the best GMAP results for the monolingual exercise, but the best results for the bilingual were for systems which did not use WSD (with a gap of aprox. 13 MAP points). In any case, further case-bycase analysis of the actual systems and runs will be needed in order to get more insight about the contribution of WSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Statistical Testing</head><p>When the goal is to validate how well results can be expected to hold beyond a particular set of queries, statistical testing can help to determine what differences between runs appear to be real as opposed to differences that are due to sampling issues. We aim to identify runs with results that are significantly different from the results of other runs. "Significantly different" in this context means that the difference between the performance scores for the runs in question appears greater than what might be expected by pure chance. As with all statistical testing, conclusions will be qualified by an error probability, which was chosen to be 0.05 in the following. We have designed our analysis to follow closely the methodology used by similar analyses carried out for Text REtrieval Conference (TREC) <ref type="bibr" coords="31,174.48,393.69,14.60,9.96" target="#b22">[23]</ref>.</p><p>We used the MATLAB Statistics Toolbox, which provides the necessary functionality plus some additional functions and utilities. We use the ANalysis Of VAriance (ANOVA) test. ANOVA makes some assumptions concerning the data be checked. Hull <ref type="bibr" coords="31,209.76,441.93,15.49,9.96" target="#b22">[23]</ref> provides details of these; in particular, the scores in question should be approximately normally distributed and their variance has to be approximately the same for all runs. Two tests for goodness of fit to a normal distribution were chosen using the MATLAB statistical toolbox: the Lilliefors test <ref type="bibr" coords="31,155.15,489.69,15.61,9.96" target="#b14">[15]</ref> and the Jarque-Bera test <ref type="bibr" coords="31,291.36,489.69,14.60,9.96" target="#b25">[25]</ref>. In the case of the CLEF tasks under analysis, both tests indicate that the assumption of normality is violated for most of the data samples (in this case the runs for each participant).</p><p>In such cases, a transformation of data should be performed. The transformation for measures that range from 0 to 1 is the arcsin-root transformation: arcsin √ x which Tague-Sutcliffe <ref type="bibr" coords="31,231.36,583.29,15.49,9.96" target="#b39">[39]</ref> recommends for use with precision/recall measures. Table <ref type="table" coords="31,176.64,595.65,10.02,9.96" target="#tab_3">10</ref> shows the results of both the Lilliefors and Jarque-Bera tests before and after applying the Tague-Sutcliffe transformation. After the transformation the analysis of the normality of samples distribution improves significantly, with some exceptions. The difficulty to transform the data into normally distributed samples derives from the original distribution of run performances which tend towards zero within the interval [0,1].</p><p>Table <ref type="table" coords="32,163.92,116.19,9.01,8.13" target="#tab_3">10</ref>. Lilliefors (LF) and Jarque-Bera (JB) test for each Ad-Hoc track with and without Tague-Sutcliffe (TS) arcsin transformation. Each entry is the number of experiments whose performance distribution can be considered drawn from a Gaussian distribution, with respect to the total number of experiment of the track. The value of alpha for this test was set to 5%. In the following sections, two different graphs are presented to summarize the results of this test. All experiments, regardless of topic language or topic fields, are included. Results are therefore only valid for comparison of individual pairs of runs, and not in terms of absolute performance. Both for the ad-hoc and robust tasks, only runs where significant differences exist are shown; the remainder of the graphs can be found in the Appendices <ref type="bibr" coords="32,384.41,442.89,15.87,9.96" target="#b16">[17,</ref><ref type="bibr" coords="32,400.28,442.89,11.90,9.96" target="#b17">18,</ref><ref type="bibr" coords="32,412.18,442.89,11.90,9.96" target="#b18">19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Track</head><p>The first graph shows participants' runs (y axis) and performance obtained (x axis). The circle indicates the average performance (in terms of precision) while the segment shows the interval in which the difference in performance is not statistically significant.</p><p>The second graph shows the overall results where all the runs that are included in the same group do not have a significantly different performance. All runs scoring below a certain group perform significantly worse than at least the top entry of the group. Likewise all the runs scoring above a certain group perform significantly better than at least the bottom entry in that group. To determine all runs that perform significantly worse than a certain run, determine the rightmost group that includes the run, all runs scoring below the bottom entry of that group are significantly worse. Conversely, to determine all runs that perform significantly better than a given run, determine the leftmost group that includes the run. All runs that score better than the top entry of that group perform significantly better.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,218.52,446.88,178.34,8.97"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of Robust WSD document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,157.20,289.80,300.64,8.97"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of TEL topic in all five languages: topic 10.2452/451-AH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,171.48,463.92,271.84,8.97"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example of Robust WSD topic: topic 10.2452/141-WSD-AH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,167.48,164.16,8.13,17.96;11,167.48,183.73,8.13,8.23;11,167.04,195.12,8.97,49.03;11,167.04,247.19,8.97,7.38;11,167.04,257.75,8.97,12.86;11,167.04,273.59,8.97,31.78;11,167.04,308.39,8.97,43.11;11,167.04,354.58,8.97,24.03;11,167.04,381.82,8.97,12.86;11,167.04,397.66,8.97,26.90;11,167.04,427.67,8.97,23.44"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Distribution of the relevant documents across the ad-hoc pools.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="12,195.00,361.44,225.25,8.97;12,140.52,115.91,334.36,231.37"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Topic creation interface of the DIRECT system.</figDesc><graphic coords="12,140.52,115.91,334.36,231.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="27,149.76,466.89,330.84,9.96;27,134.76,478.77,194.60,9.96"><head>Figures 15 and 16</head><label>16</label><figDesc>compare the performances of the top participants of the Robust Monolingual and Monolingual WSD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="27,149.76,643.53,330.84,9.96;27,134.76,655.41,190.49,9.96"><head>Figures 17 and 18</head><label>18</label><figDesc>Figures 17 and 18 compares the performances of the top participants of the Robust Bilingual and Bilingual WSD tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="29,225.36,713.64,164.64,8.97"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Robust Bilingual English WSD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="33,135.12,584.64,345.00,8.97"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Ad-Hoc TEL Monolingual English. The figure shows the Tukey T Test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,144.48,126.60,311.76,522.93"><head>Table 1 .</head><label>1</label><figDesc>Summary information about CLEF 2008 pools. TEL English Pool (DOI 10.2454/AH-TEL-ENGLISH-CLEF2008)</figDesc><table coords="10,144.48,161.16,311.76,488.37"><row><cell></cell><cell>28,104 pooled documents</cell></row><row><cell>Pool size</cell><cell>-25,571 not relevant documents -2,533 relevant documents</cell></row><row><cell></cell><cell>50 topics</cell></row><row><cell></cell><cell>21 out of 61 submitted experiments</cell></row><row><cell>Pooled Experiments</cell><cell>-monolingual: 13 out of 37 submitted experiments</cell></row><row><cell></cell><cell>-bilingual: 8 out of 24 submitted experiments</cell></row><row><cell>Assessors</cell><cell>3 assessors</cell></row><row><cell cols="2">TEL French Pool (DOI 10.2454/AH-TEL-FRENCH-CLEF2008)</cell></row><row><cell></cell><cell>24,530 pooled documents</cell></row><row><cell>Pool size</cell><cell>-23,191 not relevant documents -1,339 relevant documents</cell></row><row><cell></cell><cell>50 topics</cell></row><row><cell></cell><cell>14 out of 45 submitted experiments</cell></row><row><cell>Pooled Experiments</cell><cell></cell></row><row><cell></cell><cell>26,814 pooled documents</cell></row><row><cell>Pool size</cell><cell>-21,653 not relevant documents -5,161 relevant documents</cell></row><row><cell></cell><cell>50 topics</cell></row><row><cell></cell><cell>66 out of 66 submitted experiments</cell></row><row><cell>Pooled Experiments</cell><cell>-monolingual: 53 out of 53 submitted experiments</cell></row><row><cell></cell><cell>-bilingual: 13 out of 13 submitted experiments</cell></row><row><cell>Assessors</cell><cell>22 assessors</cell></row></table><note coords="10,247.68,360.24,203.88,8.97;10,247.68,371.16,189.60,8.97;10,144.60,386.79,43.56,8.13;10,240.96,387.00,42.50,8.97;10,192.24,401.43,228.28,8.13;10,144.48,435.75,41.12,8.13;10,240.96,414.72,101.55,8.97;10,247.68,429.84,133.23,8.97;10,247.68,440.76,112.23,8.97;10,240.96,455.88,35.91,8.97;10,144.60,485.43,93.71,8.13;10,240.96,471.96,143.54,8.97;10,247.68,486.96,208.56,8.97;10,247.68,498.00,189.60,8.97;10,144.60,513.63,43.56,8.13;10,240.96,513.84,42.50,8.97;10,211.44,528.15,189.76,8.13"><p>monolingual: 9 out of 29 submitted experiments bilingual: 5 out of 16 submitted experiments Assessors 3 assessors TEL German Pool (DOI 10.2454/AH-TEL-GERMAN-CLEF2008) Pool size 28,734 pooled documents -27,097 not relevant documents -1,637 relevant documents 50 topics Pooled Experiments 16 out of 47 submitted experiments monolingual: 10 out of 30 submitted experiments bilingual: 6 out of 17 submitted experiments Assessors 4 assessors Persian Pool (DOI 10.2454/AH-PERSIAN-CLEF2008)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,147.96,127.68,316.48,293.13"><head>Table 2 .</head><label>2</label><figDesc>CLEF 2008 ad hoc participants</figDesc><table coords="14,147.96,148.83,316.48,271.97"><row><cell>Participant</cell><cell>Institution</cell><cell>Country</cell></row><row><cell>chemnitz</cell><cell>Chemnitz University of Technology</cell><cell>Germany</cell></row><row><cell>cheshire</cell><cell>U.C.Berkeley</cell><cell>United States</cell></row><row><cell>geneva</cell><cell>University of Geneva</cell><cell>Switzerland</cell></row><row><cell>imag</cell><cell>Inst. For Infocomm Research</cell><cell>France</cell></row><row><cell>inaoe</cell><cell>INAOE</cell><cell>Mexico</cell></row><row><cell>inesc</cell><cell>INESC ID</cell><cell>Portugal</cell></row><row><cell>isi</cell><cell>Indian Statistical Institute</cell><cell>India</cell></row><row><cell>ixa</cell><cell>Univ. Basque Country</cell><cell>Spain</cell></row><row><cell>jhu-apl</cell><cell cols="2">Johns Hopkins University Applied Physics Lab United States</cell></row><row><cell>karlsruhe</cell><cell>University of Karlsruhe</cell><cell>Germany</cell></row><row><cell>know-center</cell><cell>Knowledge Relationship Discovery</cell><cell>Austria</cell></row><row><cell>opentext</cell><cell>Open Text Corporation</cell><cell>Canada</cell></row><row><cell>tehran-IRDB</cell><cell>IR-DB Research Group</cell><cell>Iran</cell></row><row><cell>tehran-NLP</cell><cell>NLP-Software Engineering Grad. Lab</cell><cell>Iran</cell></row><row><cell cols="2">tehran-NLPDB NLP-DB Research Group</cell><cell>Iran</cell></row><row><cell cols="2">tehran-NLPDB2 NLP-DB Group</cell><cell>Iran</cell></row><row><cell>tehran-SEC</cell><cell>School of Electrical Computing-1</cell><cell>Iran</cell></row><row><cell>twente</cell><cell>Univ. of Twente</cell><cell>Netherlands</cell></row><row><cell>ucm</cell><cell>Universidad Complutense de Madrid</cell><cell>Spain</cell></row><row><cell>ufrgs</cell><cell>Univ. Fed. do Rio Grande do Sul</cell><cell>Brazil</cell></row><row><cell>uniba</cell><cell>Universita' di Bari</cell><cell>Italy</cell></row><row><cell>unine</cell><cell>U.Neuchatel-Informatics</cell><cell>Switzerland</cell></row><row><cell>xerox</cell><cell>Xerox Reseearch -Data Mining</cell><cell>France</cell></row><row><cell>xerox-sas</cell><cell>Xerox SAS</cell><cell>Italy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,199.80,454.32,215.91,194.41"><head>Table 3 .</head><label>3</label><figDesc>CLEF 2008 ad hoc participants by country.</figDesc><table coords="14,243.00,475.47,126.41,173.25"><row><cell>Country</cell><cell># Participants</cell></row><row><cell>Austria</cell><cell>1</cell></row><row><cell>Brazil</cell><cell>1</cell></row><row><cell>Canada</cell><cell>1</cell></row><row><cell>France</cell><cell>2</cell></row><row><cell>Germany</cell><cell>2</cell></row><row><cell>India</cell><cell>1</cell></row><row><cell>Iran</cell><cell>5</cell></row><row><cell>Italy</cell><cell>2</cell></row><row><cell>Mexico</cell><cell>1</cell></row><row><cell>Netherlands</cell><cell>1</cell></row><row><cell>Portugal</cell><cell>1</cell></row><row><cell>Spain</cell><cell>2</cell></row><row><cell>Switzerland</cell><cell>2</cell></row><row><cell>United States</cell><cell>2</cell></row><row><cell>Total</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="15,139.32,115.32,341.32,233.65"><head>Table 4 .</head><label>4</label><figDesc>Breakdown of experiments into tracks and topic languages.</figDesc><table coords="15,139.32,139.80,341.32,209.17"><row><cell></cell><cell></cell><cell></cell><cell cols="2">(b) List of experiments by</cell></row><row><cell cols="2">(a) Number of experiments per track, participant.</cell><cell></cell><cell>topic language.</cell><cell></cell></row><row><cell>Track</cell><cell cols="2"># Part. # Runs</cell><cell cols="2">Topic Lang. # Runs</cell></row><row><cell>TEL Mono English</cell><cell>13</cell><cell>37</cell><cell>English</cell><cell>120</cell></row><row><cell>TEL Mono French</cell><cell>9</cell><cell>29</cell><cell>Farsi</cell><cell>51</cell></row><row><cell>TEL Mono German</cell><cell>10</cell><cell>30</cell><cell>German</cell><cell>44</cell></row><row><cell>TEL Bili English TEL Bili French TEL Bili German Mono Persian Bili Persian</cell><cell>8 5 6 8 3</cell><cell>24 16 17 53 13</cell><cell>French Spanish Dutch Portuguese Total</cell><cell>44 26 3 1 289</cell></row><row><cell>Robust Mono English Test</cell><cell>8</cell><cell>20</cell><cell></cell><cell></cell></row><row><cell>Robust Mono English Training</cell><cell>1</cell><cell>2</cell><cell></cell><cell></cell></row><row><cell>Robust Bili English Test</cell><cell>4</cell><cell>8</cell><cell></cell><cell></cell></row><row><cell>Robust Mono English Test WSD</cell><cell>7</cell><cell>25</cell><cell></cell><cell></cell></row><row><cell>Robust Mono English Training WSD</cell><cell>1</cell><cell>5</cell><cell></cell><cell></cell></row><row><cell>Robust Bili English Test WSD</cell><cell>4</cell><cell>10</cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell cols="2">289</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="16,124.08,115.32,364.80,234.21"><head>Table 5 .</head><label>5</label><figDesc>Best entries for the monolingual TEL tasks.</figDesc><table coords="16,124.08,136.47,364.80,213.05"><row><cell cols="2">Track Rank Participant</cell><cell>Experiment DOI</cell><cell>MAP</cell></row><row><cell></cell><cell>1st unine</cell><cell>10.2415/AH-TEL-MONO-EN-CLEF2008.UNINE.UNINEEN3</cell><cell>37.53%</cell></row><row><cell></cell><cell>2nd inesc</cell><cell>10.2415/AH-TEL-MONO-EN-CLEF2008.INESC.RUN3</cell><cell>36.23%</cell></row><row><cell>English</cell><cell>3rd chemnitz 4th jhu-apl</cell><cell>10.2415/AH-TEL-MONO-EN-CLEF2008.CHEMNITZ.CUT SIMPLE 10.2415/AH-TEL-MONO-EN-CLEF2008.JHU-APL.JHUMOEN4RF</cell><cell>35.61% 35.31%</cell></row><row><cell></cell><cell>5th cheshire</cell><cell cols="2">10.2415/AH-TEL-MONO-EN-CLEF2008.CHESHIRE.BKAHTELMENTDT2F 34.66%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>8.28%</cell></row><row><cell></cell><cell>1st unine</cell><cell>10.2415/AH-TEL-MONO-FR-CLEF2008.UNINE.UNINEFR3</cell><cell>33.27%</cell></row><row><cell></cell><cell>2nd xerox</cell><cell>10.2415/AH-TEL-MONO-FR-CLEF2008.XEROX.J1</cell><cell>30.88%</cell></row><row><cell>French</cell><cell>3rd jhu-apl 4th opentext</cell><cell>10.2415/AH-TEL-MONO-FR-CLEF2008.JHU-APL.JHUMOFR4 10.2415/AH-TEL-MONO-FR-CLEF2008.OPENTEXT.OTFR08TD</cell><cell>29.50% 25.23%</cell></row><row><cell></cell><cell>5th chesire</cell><cell cols="2">10.2415/AH-TEL-MONO-FR-CLEF2008.CHESHIRE.BKAHTELMFRTDT2FB 24.37%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>36.52%</cell></row><row><cell></cell><cell>1st opentext</cell><cell>10.2415/AH-TEL-MONO-DE-CLEF2008.OPENTEXT.OTDE08TDE</cell><cell>35.71%</cell></row><row><cell></cell><cell>2nd jhu-apl</cell><cell>10.2415/AH-TEL-MONO-DE-CLEF2008.JHU-APL.JHUMODE4</cell><cell>33.77%</cell></row><row><cell>German</cell><cell>3rd unine 4th xerox</cell><cell>10.2415/AH-TEL-MONO-DE-CLEF2008.UNINE.UNINEDE1 10.2415/AH-TEL-MONO-DE-CLEF2008.XEROX.T1</cell><cell>30.12% 27.36%</cell></row><row><cell></cell><cell>5th inesc</cell><cell>10.2415/AH-TEL-MONO-DE-CLEF2008.INESC.RUN3</cell><cell>22.97%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>55.46%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="17,124.08,115.32,399.71,234.21"><head>Table 6 .</head><label>6</label><figDesc>Best entries for the bilingual TEL tasks.</figDesc><table coords="17,124.08,136.47,399.71,213.05"><row><cell cols="2">Track Rank Participant</cell><cell>Experiment DOI</cell><cell>MAP</cell></row><row><cell></cell><cell>1st chemnitz</cell><cell>10.2415/AH-TEL-BILI-X2EN-CLEF2008.CHEMNITZ.CUT SIMPLE DE2EN</cell><cell>34.15%</cell></row><row><cell></cell><cell>2nd chesire</cell><cell>10.2415/AH-TEL-BILI-X2EN-CLEF2008.CHESHIRE.BKAHTELBFRENTDT2FB</cell><cell>28.24%</cell></row><row><cell>English</cell><cell>3rd ufrgs 4th twente</cell><cell>10.2415/AH-TEL-BILI-X2EN-CLEF2008.UFRGS.UFRGS BI SP EN2 10.2415/AH-TEL-BILI-X2EN-CLEF2008.TWENTE.FCW</cell><cell>23.15% 22.78%</cell></row><row><cell></cell><cell>5th jhu-apl</cell><cell>10.2415/AH-TEL-BILI-X2EN-CLEF2008.JHU-APL.JHUBIDEEN5</cell><cell>21.11%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>61.77%</cell></row><row><cell></cell><cell>1st chesire</cell><cell>10.2415/AH-TEL-BILI-X2FR-CLEF2008.CHESHIRE.BKAHTELBDEFRTDT2FB</cell><cell>18.84%</cell></row><row><cell></cell><cell>2nd chemnitz</cell><cell>10.2415/AH-TEL-BILI-X2FR-CLEF2008.CHEMNITZ.CUT SIMPLE EN2FR</cell><cell>17.54%</cell></row><row><cell>French</cell><cell>3rd jhu-apl 4th xerox</cell><cell>10.2415/AH-TEL-BILI-X2FR-CLEF2008.JHU-APL.JHUBINLFR5 10.2415/AH-TEL-BILI-X2FR-CLEF2008.XEROX.GER FRE J</cell><cell>17.46% 11.62%</cell></row><row><cell></cell><cell>5th xerox-sas</cell><cell>10.2415/AH-TEL-BILI-X2FR-CLEF2008.XEROX-SAS.CACAOENGFREPLAIN</cell><cell>6.78%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>177.87%</cell></row><row><cell></cell><cell>1st jhu-apl</cell><cell>10.2415/AH-TEL-BILI-X2DE-CLEF2008.JHU-APL.JHUBIENDE5</cell><cell>18.98%</cell></row><row><cell></cell><cell>2nd chemnitz</cell><cell cols="2">10.2415/AH-TEL-BILI-X2DE-CLEF2008.CHEMNITZ.CUT MERGED SIMPLE EN2DE 18.51%</cell></row><row><cell>German</cell><cell>3rd chesire 4th xerox</cell><cell>10.2415/AH-TEL-BILI-X2DE-CLEF2008.CHESHIRE.BKAHTELBENDETDT2FB 10.2415/AH-TEL-BILI-X2DE-CLEF2008.XEROX.FRE GER J</cell><cell>15.56% 12.05%</cell></row><row><cell></cell><cell>5th karlsruhe</cell><cell>10.2415/AH-TEL-BILI-X2DE-CLEF2008.KARLSRUHE.AIFB ONB EN</cell><cell>6.67%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>184.55%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="23,124.08,115.32,355.73,164.25"><head>Table 7 .</head><label>7</label><figDesc>Best entries for the Persian tasks.</figDesc><table coords="23,124.08,134.79,355.73,144.77"><row><cell>Track</cell><cell>Rank Participant</cell><cell>Experiment DOI</cell><cell>MAP</cell></row><row><cell></cell><cell>1st unine</cell><cell>10.2415/AH-PERSIAN-MONO-FA-CLEF2008.UNINE.UNINEPE2</cell><cell>48.98%</cell></row><row><cell></cell><cell>2nd jhu-apl</cell><cell>10.2415/AH-PERSIAN-MONO-FA-CLEF2008.JHU-APL.JHUFASK41R400</cell><cell>45.19%</cell></row><row><cell>Monolingual</cell><cell cols="3">3rd opentext 4th tehran-nlpdb2 10.2415/AH-PERSIAN-MONO-FA-CLEF2008.TEHRAN-NLPDB2.UTNLPDB3INEXPC2 28.83% 10.2415/AH-PERSIAN-MONO-FA-CLEF2008.OPENTEXT.OTFA08T 42.08%</cell></row><row><cell></cell><cell cols="2">5th tehran-nlpdb 10.2415/AH-PERSIAN-MONO-FA-CLEF2008.TEHRAN-NLPDB.UTNLPDB1MT</cell><cell>28.14%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>74.05%</cell></row><row><cell></cell><cell>1st jhu-apl</cell><cell>10.2415/AH-PERSIAN-BILI-X2FA-CLEF2008.JHU-APL.JHUENFASK41R400</cell><cell>45.19%</cell></row><row><cell></cell><cell cols="3">2nd tehran-nlpdb 10.2415/AH-PERSIAN-BILI-X2FA-CLEF2008.TEHRAN-NLPDB.UTNLPDB1BT4G 14.45%</cell></row><row><cell>Bilingual</cell><cell>3rd tehran-sec 4th -</cell><cell>10.2415/AH-PERSIAN-BILI-X2FA-CLEF2008.TEHRAN-SEC.CLDTDR -</cell><cell>12.88% -</cell></row><row><cell></cell><cell>5th -</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>250.85%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="27,109.92,115.32,403.13,166.05"><head>Table 8 .</head><label>8</label><figDesc>Best entries for the robust monolingual task.</figDesc><table coords="27,109.92,136.47,403.13,144.89"><row><cell>Track</cell><cell>Rank Participant</cell><cell>Experiment DOI</cell><cell>MAP GMAP</cell></row><row><cell></cell><cell>1st unine</cell><cell>10.2415/AH-ROBUST-MONO-EN-TEST-CLEF2008.UNINE.UNINEROBUST4</cell><cell>45.14% 21.17%</cell></row><row><cell></cell><cell>2nd geneva</cell><cell>10.2415/AH-ROBUST-MONO-EN-TEST-CLEF2008.GENEVA.ISILEMTDN</cell><cell>39.17% 16.53%</cell></row><row><cell>English</cell><cell>3rd ucm 4th ixa</cell><cell>10.2415/AH-ROBUST-MONO-EN-TEST-CLEF2008.UCM.BM25 BO1 10.2415/AH-ROBUST-MONO-EN-TEST-CLEF2008.IXA.EN2ENNOWSDPSREL</cell><cell>38.34% 15.28% 38.10% 15.72%</cell></row><row><cell></cell><cell>5th ufrgs</cell><cell>10.2415/AH-ROBUST-MONO-EN-TEST-CLEF2008.UFRGS.UFRGS R MONO2 TEST</cell><cell>33.94% 13.96%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>33.03% 51.64%</cell></row><row><cell></cell><cell>1st unine</cell><cell>10.2415/AH-ROBUST-WSD-MONO-EN-TEST-CLEF2008.UNINE.UNINEROBUST6</cell><cell>44.98% 21.54%</cell></row><row><cell></cell><cell>2nd ucm</cell><cell>10.2415/AH-ROBUST-WSD-MONO-EN-TEST-CLEF2008.UCM.BM25 BO1 CLAUSES 09</cell><cell>39.57% 16.17%</cell></row><row><cell>English WSD</cell><cell>3rd ixa 4th geneva</cell><cell>10.2415/AH-ROBUST-WSD-MONO-EN-TEST-CLEF2008.IXA.EN2ENUBCDOCSPSREL 10.2415/AH-ROBUST-WSD-MONO-EN-TEST-CLEF2008.GENEVA.ISINUSLWTDN</cell><cell>38.99% 15.52% 38.13% 16.25%</cell></row><row><cell></cell><cell>5th ufrgs</cell><cell cols="2">10.2415/AH-ROBUST-WSD-MONO-EN-TEST-CLEF2008.UFRGS.UFRGS R MONO WSD5 TEST 34.64% 14.17%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>29.84% 52.01%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="28,108.89,118.53,473.47,613.31"><head></head><label></label><figDesc>Fig. 16. Robust Monolingual English WSD.</figDesc><table coords="28,108.89,118.53,473.47,587.95"><row><cell cols="12">Ad-Hoc Robust Monolingual English Test Task Top 5 Participants -Standard Recall Levels vs Mean Interpolated Precision Ad-Hoc Robust Bilingual English Test Task Top 5 Participants -Standard Recall Levels vs Mean Interpolated Precision</cell></row><row><cell></cell><cell cols="2">100% 100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">unine [Experiment UNINEROBUST4; MAP 45.15%; Not Pooled] ufrgs [Experiment UFRGS_R_BI3_TEST; MAP 36.39%; Not Pooled]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">geneva [Experiment ISILEMTDN; MAP 39.17%; Not Pooled] geneva [Experiment ISIESENTD; MAP 30.36%; Not Pooled]</cell></row><row><cell></cell><cell cols="2">90% 90%</cell><cell></cell><cell></cell><cell cols="7">ucm [Experiment BM25_BO1; MAP 38.35%; Not Pooled] ixa [Experiment ES2ENNOWSDPSREL; MAP 19.57%; Not Pooled]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">ixa [Experiment EN2ENNOWSDPSREL; MAP 38.10%; Not Pooled] uniba [Experiment CROSS1TDNUS2F; MAP 2.57%; Not Pooled]</cell></row><row><cell></cell><cell cols="2">80% 80%</cell><cell></cell><cell></cell><cell cols="7">ufrgs [Experiment UFRGS_R_MONO2_TEST; MAP 33.95%; Not Pooled]</cell></row><row><cell></cell><cell cols="2">70% 70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">60% 60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision Precision</cell><cell cols="2">50% 50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">40% 40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">30% 30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">20% 20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">10% 10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0% 0% 0% 0%</cell><cell>10% 10%</cell><cell>20% 20%</cell><cell>30% 30%</cell><cell>40% 40%</cell><cell>50% 50%</cell><cell>60% 60%</cell><cell>70% 70%</cell><cell>80% 80%</cell><cell>90% 90%</cell><cell>100% 100%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall Recall</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Fig. 15. Robust Monolingual English. Fig. 17. Robust Bilingual English.</cell><cell></cell></row><row><cell cols="12">Ad-Hoc Robust Word Sense Disambiguation Bilingual English Test Task Top 5 Participants -Standard Recall Levels vs Mean Interpolated Precision</cell></row><row><cell cols="12">Ad-Hoc Robust Word Sense Disambiguation Monolingual English Test Task Top 5 Participants -Standard Recall Levels vs Mean Interpolated Precision 100%</cell></row><row><cell></cell><cell></cell><cell>100%</cell><cell></cell><cell></cell><cell cols="7">ixa [Experiment ES2EN1STTOPSUBCDOCSPSREL; MAP 23.56%; Not Pooled]</cell></row><row><cell></cell><cell></cell><cell cols="2">90%</cell><cell></cell><cell cols="7">ufrgs [Experiment UFRGS_R_BI_WSD1_TEST; MAP 21.77%; Not Pooled] unine [Experiment UNINEROBUST6; MAP 44.99%; Not Pooled] ucm [Experiment BM25_BO1_CLAUSES_09; MAP 39.57%; Not Pooled] geneva [Experiment ISIESPWSDTDN; MAP 9.70%; Not Pooled]</cell></row><row><cell></cell><cell></cell><cell>90%</cell><cell></cell><cell></cell><cell cols="7">ixa [Experiment EN2ENUBCDOCSPSREL; MAP 38.99%; Not Pooled] uniba [Experiment CROSSWSD12NUS2F; MAP 7.23%; Not Pooled]</cell></row><row><cell></cell><cell></cell><cell cols="2">80% 80%</cell><cell></cell><cell cols="7">geneva [Experiment ISINUSLWTDN; MAP 38.14%; Not Pooled] ufrgs [Experiment UFRGS_R_MONO_WSD5_TEST; MAP 34.65%; Not Pooled]</cell></row><row><cell></cell><cell></cell><cell cols="2">70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Precision</cell><cell cols="2">50% 60% 40% 50% Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">0% 0% 0% 0%</cell><cell>10% 10%</cell><cell>20% 20%</cell><cell>30% 30%</cell><cell>40% 40%</cell><cell>50% 50% Recall</cell><cell>60% 60%</cell><cell>70% 70%</cell><cell>80% 80%</cell><cell>90% 90%</cell><cell>100% 100%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Recall</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="30,92.88,115.32,434.45,166.05"><head>Table 9 .</head><label>9</label><figDesc>Best entries for the robust bilingual task.</figDesc><table coords="30,92.88,136.47,434.45,144.89"><row><cell>Track</cell><cell>Rank Participant</cell><cell>Experiment DOI</cell><cell>MAP</cell><cell>GMAP</cell></row><row><cell></cell><cell>1st ufrgs</cell><cell>10.2415/AH-ROBUST-BILI-X2EN-TEST-CLEF2008.UFRGS.UFRGS R BI3 TEST</cell><cell>36.38%</cell><cell>13.00%</cell></row><row><cell></cell><cell>2nd geneva</cell><cell>10.2415/AH-ROBUST-BILI-X2EN-TEST-CLEF2008.GENEVA.ISIESENTD</cell><cell>30.36%</cell><cell>10.96%</cell></row><row><cell>English</cell><cell>3rd ixa 4th uniba</cell><cell>10.2415/AH-ROBUST-BILI-X2EN-TEST-CLEF2008.IXA.ES2ENNOWSDPSREL 10.2415/AH-ROBUST-BILI-X2EN-TEST-CLEF2008.UNIBA.CROSS1TDNUS2F</cell><cell>19.57% 2.56%</cell><cell>1.62% 0.04%</cell></row><row><cell></cell><cell>5th -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell cols="2">1,321.09% 32,400.00%</cell></row><row><cell></cell><cell>1st ixa</cell><cell>10.2415/AH-ROBUST-WSD-BILI-X2EN-TEST-CLEF2008.IXA.ES2EN1STTOPSUBCDOCSPSREL</cell><cell>23.56%</cell><cell>1.71%</cell></row><row><cell></cell><cell>2nd ufrgs</cell><cell>10.2415/AH-ROBUST-WSD-BILI-X2EN-TEST-CLEF2008.UFRGS.UFRGS R BI WSD1 TEST</cell><cell>21.77%</cell><cell>5.14%</cell></row><row><cell>English WSD</cell><cell>3rd geneva 4th geneva</cell><cell>10.2415/AH-ROBUST-WSD-BILI-X2EN-TEST-CLEF2008.GENEVA.ISIESPWSDTDN 10.2415/AH-ROBUST-WSD-BILI-X2EN-TEST-CLEF2008.UNIBA.CROSSWSD12NUS2F</cell><cell>9.70% 7.23%</cell><cell>0.37% 0.16%</cell></row><row><cell></cell><cell>5th -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell cols="2">225.86% 3,112.50%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="34,134.76,115.32,345.74,20.01"><head>Table 11 .</head><label>11</label><figDesc>Ad-Hoc TEL Monolingual English. The table shows the first ten groups of the Tukey T Test.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.72,623.28,335.70,8.97;2,144.72,634.31,335.92,8.97;2,144.72,645.23,98.48,8.97"><p>Over the years, this track has built up test collections for monolingual and crosslanguage system evaluation in<ref type="bibr" coords="2,269.25,634.31,9.17,8.97" target="#b12">13</ref> European languages (see the Introduction to this volume for more details)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,137.52,655.12,3.65,4.17;2,144.72,656.16,175.71,8.97"><p><ref type="bibr" coords="2,137.52,655.12,3.65,4.17" target="#b1">2</ref> See http://www.theeuropeanlibrary.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.72,656.16,270.99,8.97"><p>For more information, see http://ece.ut.ac.ir/dbrg/hamshahri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.72,645.24,289.23,8.97"><p>Full sample and dtd are available at http://ixa2.si.ehu.es/clirwsd/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,144.72,656.66,88.95,8.27"><p>http://www.doi.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,144.72,656.16,289.23,8.97"><p>Full sample and dtd are available at http://ixa2.si.ehu.es/clirwsd/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="8,144.72,634.32,335.82,8.97;8,144.72,645.24,335.80,8.97;8,144.72,656.16,189.64,8.97"><p>Tests made on NTCIR pools in previous years have suggested that a depth of 60 in normally adequate to create stable pools, presuming that a sufficient number of runs from different systems have been included</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="13,144.72,623.28,335.86,8.97;13,144.72,634.31,335.78,8.97;13,144.72,645.23,134.15,8.97"><p>Two additional Spanish groups presented results after the deadline for the robust tasks; their results were thus not reported in the official list but their papers are included in this volume<ref type="bibr" coords="13,242.01,645.23,13.46,8.97" target="#b30">[30]</ref>,<ref type="bibr" coords="13,262.04,645.23,13.46,8.97" target="#b32">[32]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="13,144.72,656.16,281.67,8.97"><p>The narrative field was only offered for the Persian and Robust tasks.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="21,144.72,645.24,335.80,8.97;21,144.72,656.16,290.07,8.97"><p>This meant that they had to recreate their indices and perform all official experiments at the very last moment; this may have impacted on their results</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="25,144.72,645.24,161.99,8.97"><p>http://sourceforge.net/projects/perstem</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11" coords="25,144.72,656.16,199.78,8.97"><p>http://www.parstranslator.net/eng/translate.htm</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p>The TEL task was studied in order to provide useful input to The European Library (TEL); we express our gratitude in particular to <rs type="person">Jill Cousins</rs>, Programme Director, and <rs type="person">Sjoerd Siebinga</rs>, Technical Developer of <rs type="affiliation">TEL</rs>. <rs type="person">Vivien Petras</rs>, <rs type="affiliation">GESIS-IZ Social Science Information Centre, Germany</rs>, and <rs type="person">Nicolas Moreau</rs>, Evaluation and <rs type="institution">Language Resources Distribution Agency, France</rs>, were responsible for the creation of the topics and the supervision of the relevance assessment work for the ONB and BNF data respectively. We thank them for their valuable assistance.</p><p>We should also like to acknowledge the enormous contribution to the coordination of the Persian task made by the <rs type="institution">Data Base Research group of the University of Tehran</rs> and in particular to <rs type="person">Abolfazl AleAhmad</rs> and <rs type="person">Hadi Amiri</rs>. They were responsible for the preparation of the set of topics for the Hamshahri collection in Farsi and English and for the subsequent relevance assessments.</p><p>The robust task was partially funded by the <rs type="funder">Ministry of Education</rs> (project <rs type="grantNumber">KNOW TIN2006-15049</rs>) and the <rs type="funder">European Commission</rs> (project <rs type="grantNumber">KYOTO ICT-2007-211423</rs>). We would like to acknowledge the invaluable suggestions and practical assistance of <rs type="person">Arantxa Otegi</rs>, <rs type="person">German Rigau</rs> and <rs type="person">Piek Vossen</rs>. In particular, <rs type="person">Arantxa</rs> was responsible for the XML coding of the word sense disambiguation results. We also want to thank <rs type="person">Oier Lopez de Lacalle</rs>, who runs the <rs type="institution">UBC</rs> WSD system, and <rs type="person">Yee Seng Chan</rs>, <rs type="person">Hwee Tou Ng</rs> and <rs type="person">Zhi Zhong</rs>, who run the <rs type="institution">NUS</rs> WSD system. Their generous contribution was invaluable to run this exercise.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BjH7MvS">
					<idno type="grant-number">KNOW TIN2006-15049</idno>
				</org>
				<org type="funding" xml:id="_Vupg3TP">
					<idno type="grant-number">KYOTO ICT-2007-211423</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Experiment DOI Groups 10.2415/AH-TEL-MONO-EN-CLEF2008.UNINE.UNINEEN3 X 10.2415/AH-TEL-MONO-EN-CLEF2008.UNINE.UNINEEN2 X X 10.2415/AH-TEL-MONO-EN-CLEF2008.INESC.RUN3 X X 10.2415/AH-TEL-MONO-EN-CLEF2008.UNINE.UNINEEN1 X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.CHEMNITZ.CUT SIMPLE X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.JHU-APL.JHUMOENS X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.JHU-APL.JHUMOEN4RF X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.CHESHIRE.BKAHTELMENTDT2F X X X X 10.2415/AH-TEL-BILI-X2EN-CLEF2008.TWENTE.M X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.UNINE.UNINEEN4 X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.JHU-APL.JHUMOEN4 X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.JHU-APL.JHUMOEN5 X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.OPENTEXT.OTEN08TDE X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.INESC.RUN4 X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.INESC.RUN1 X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.INESC.RUN2 X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.OPENTEXT.OTEN08TD X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.XEROX.F X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.XEROX.J1 X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.OPENTEXT.OTEN08T X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.XEROX.S1 X X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.CHESHIRE.BKAHTELMENTT2FB X X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.UFRGS.UFRGS MONO EN2 X X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.ISI.IN EXPC2C10 X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.IMAG.IPAL02 X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.IMAG.IPAL01 X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.IMAG.IPAL03 X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.IMAG.IPAL04 X X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.UFRGS.UFRGS MONO EN1 X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.CHEMNITZ.CUT MULTI10 WX PLUSPLUS X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.OPENTEXT.OTEN08TDZ X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.XEROX-SAS.CACAOENGENGPLAIN X X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.KARLSRUHE.AIFB BL EN X X X 10.2415/AH-TEL-MONO-EN-CLEF2008.XEROX-SAS.CACAOENGENGEXPANDED X X 10.2415/AH-TEL-MONO-EN-CLEF2008. <ref type="bibr" coords="34,263.36,356.14,31.79,4.98">CHEMNITZ</ref>  Experiment DOI Groups 10.2415/AH-TEL-MONO-FR-CLEF2008.UNINE.UNINEFR1 X 10.2415/AH-TEL-MONO-FR-CLEF2008.UNINE.UNINEFR3 X 10.2415/AH-TEL-MONO-FR-CLEF2008.UNINE.UNINEFR2 X X 10.2415/AH-TEL-MONO-FR-CLEF2008.XEROX.J1 X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.JHU-APL.JHUMOFR4 X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.UNINE.UNINEFR4 X X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.JHU-APL.JHUMOFR5 X X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.JHU-APL.JHUMOFR4RF X X X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.XEROX.F X X X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.XEROX.S1 X X X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.OPENTEXT.OTFR08TD X X X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.XEROX.AF X X X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.OPENTEXT.OTFR08TDE X X X X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.CHESHIRE.BKAHTELMFRTDT2FB X X X X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.INESC.RUN3 X X X X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.OPENTEXT.OTFR08T X X X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.ISI.IN EXPC2C10 X X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.INESC.RUN4 X X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.JHU-APL.JHUMOFRS X X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.INESC.RUN1 X X X X X 10.2415/AH-TEL-MONO-FR-CLEF2008.OPENTEXT.OTFR08TDZ X X X X </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Experiment DOI Groups 10.2415/AH-TEL-BILI-X2FR-CLEF2008.CHESHIRE.BKAHTELBDEFRTDT2FB X 10.2415/AH-TEL-BILI-X2FR-CLEF2008.CHESHIRE.BKAHTELBESFRTDT2FB X 10.2415/AH-TEL-BILI-X2FR-CLEF2008.JHU-APL.JHUBINLFR5 X X 10.2415/AH-TEL-BILI-X2FR-CLEF2008.JHU-APL.JHUBIENFR5 X X 10.2415/AH-TEL-BILI-X2FR-CLEF2008.CHESHIRE.BKAHTELBENFRTDT2FB X X 10.2415/AH-TEL-BILI-X2FR-CLEF2008.CHEMNITZ.CUT SIMPLE EN2FR X X 10.2415/AH-TEL-BILI-X2FR-CLEF2008.JHU-APL.JHUBIDEFR5 X X 10.2415/AH-TEL-BILI-X2FR-CLEF2008.XEROX.PRF ENG FRE X X X 10.2415/AH-TEL-BILI-X2FR-CLEF2008.CHEMNITZ.CUT SIMPLE MULTI10 W1 EN2FR X X X 10.2415/AH-TEL-BILI-X2FR-CLEF2008.XEROX.GER FRE J X X X 10.2415/AH-TEL-BILI-X2FR-CLEF2008.XEROX.ENG FRE S X X 10.2415/AH-TEL-BILI-X2FR-CLEF2008.XEROX-SAS.CACAOENGFREPLAIN X X 10.2415/AH-TEL-BILI-X2FR-CLEF2008.XEROX-SAS.CACAOENGFREEXPANDED X X X 10.2415/AH-TEL-BILI-X2FR-CLEF2008.XEROX.PRF GER FRE X X 10.2415/AH-TEL-BILI-X2FR-CLEF2008.XEROX-SAS.CACAOGERFREPLAIN X 10.2415/AH-TEL-BILI-X2FR-CLEF2008.XEROX-SAS.CACAOGERFREEXPANDED X    X X X 10.2415/AH-TEL-BILI-X2DE-CLEF2008.XEROX.FRE GER J X X X X 10.2415/AH-TEL-BILI-X2DE-CLEF2008.CHESHIRE.BKAHTELBESDETDT2FB X X X X 10.2415/AH-TEL-BILI-X2DE-CLEF2008.XEROX.PRF2 X X X X X 10.2415/AH-TEL-BILI-X2DE-CLEF2008.CHEMNITZ.CUT MERGED SIMPLE MULTI10 W1 EN2DE X X X X X 10.2415/AH-TEL-BILI-X2DE-CLEF2008.XEROX.F X X X X 10.2415/AH-TEL-BILI-X2DE-CLEF2008.KARLSRUHE.AIFB ONB EN X X X X 10.2415/AH-TEL-BILI-X2DE-CLEF2008.KARLSRUHE.AIFB ONB FR X X X 10.2415/AH-TEL-BILI-X2DE-CLEF2008.XEROX.ENG F X X X 10.2415/AH-TEL-BILI-X2DE-CLEF2008.XEROX-SAS.CACAOENGGERPLAIN X X X 10.2415/AH-TEL-BILI-X2DE-CLEF2008.XEROX-SAS.CACAOENGGEREXPANDED X X 10.2415/AH-TEL-BILI-X2DE-CLEF2008.XEROX-SAS.CACAOFREGERPLAIN X X 10.2415/AH-TEL-BILI-X2DE-CLEF2008.XEROX-SAS.CACAOFREGEREXPANDED X Fig. <ref type="figure" coords="41,154.45,645.27,9.01,8.13">24</ref>. Ad-Hoc TEL Bilingual German. Experiments grouped according to the Tukey T Test.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>The ad hoc task this year has been almost completely renovated with new collections and new tasks. For all three tasks, we have been very happy with number of participants. However, it is really too soon to be able to provide any deep analysis of the results obtained. This is left to the post-workshop proceedings.</p><p>In any case, it is our intention to run all three tasks for a second year both in order to provide participants with another chance to test their systems after refinement and tuning on the basis of this year's experiments and also to be able to create useful and consolidated test collections. In particular, for both the TEL and Persian tasks, we intend to perform some experiments on this year's test collections in order to verify their stability. The results will be reported in the Proceedings.</p><p>From our first impressions of the results of the TEL task, it would appear that there is no need for systems to apply any dedicated processing to handle the specificity of these collections (very sparse, essentially multilingual data) and that traditional IR and CLIR approaches can perform well with no extra boosting. However, we feel that it is too early to make such assumptions; many more experiments are needed.</p><p>The Persian task continued in the tradition of the CLEF ad hoc retrieval tasks on newspaper collections. The first results seem to confirm that the traditional IR/CLIR approaches port well to "new" languages -where by "new" we intend languages which have not been subjected to a lot of testing and experimental IR studies previously.</p><p>The robust exercise had, for the first time, the additional goal of measuring to what extent IR systems could profit from automatic word sense disambiguation information. The conclusions are mixed: while some top scoring groups did manage to improve the results using WSD information by aprox. 1 MAP percentage point (aprox. 4 MAP percentage points in the cross-language exercise) and the best monolingual GMAP score was for a WSD run (0.27 percentage points), the best scores for the rest came from systems which did not use WSD information. Given the relatively short time that the participants had to try effective ways of using the word sense information we think that these results are positive, but we think that a subsequent evaluation exercise would be needed for participants to further develop their systems.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="51,142.88,202.08,337.72,8.97;51,151.56,213.11,329.06,8.97;51,151.56,224.03,60.62,8.97" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="51,220.66,213.11,255.74,8.97">Fusion of Retrieval Models at CLEF 2008 Ad-Hoc Persian Track</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Aghazade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Farzinvash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aleahmad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Oroumchian</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="51,142.88,235.43,337.61,8.97;51,151.56,246.36,328.94,8.97;51,151.56,257.39,228.87,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="51,297.79,235.43,182.69,8.97;51,151.56,246.36,18.21,8.97">UBC-ALM: Combining k-NN with SVD for WSD</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="51,190.68,246.36,289.83,8.97;51,151.56,257.39,61.60,8.97">Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval 2007)</title>
		<meeting>the 4th International Workshop on Semantic Evaluations (SemEval 2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="51,142.88,268.68,337.60,8.97;51,151.56,279.71,329.06,8.97;51,151.56,290.63,277.84,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="51,151.56,279.71,325.18,8.97">SemEval-2007 Task01: Evaluating WSD on Cross-Language Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Otegi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="51,163.08,290.63,148.97,8.97">Proceedings of CLEF 2007 Workshop</title>
		<meeting>CLEF 2007 Workshop<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="51,142.88,302.03,337.74,8.97;51,151.56,312.96,328.93,8.97;51,151.56,323.99,328.88,8.97;51,151.56,334.91,328.86,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="51,323.35,302.03,157.27,8.97;51,151.56,312.96,202.32,8.97">A Proposal to Extend and Enrich the Scientific Data Curation of Evaluation Campaigns</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Agosti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="51,230.00,323.99,250.44,8.97;51,151.56,334.91,79.02,8.97">Proc. 1st International Workshop on Evaluating Information Access (EVIA 2007)</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Sakay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Evans</surname></persName>
		</editor>
		<meeting>1st International Workshop on Evaluating Information Access (EVIA 2007)<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>National Institute of Informatics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="62" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="51,142.88,346.31,337.56,8.97;51,151.56,357.23,328.84,8.97;51,151.56,368.27,245.41,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="51,308.59,346.31,171.85,8.97;51,151.56,357.23,101.64,8.97">The Importance of Scientific Data Curation for Evaluation Campaigns</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Agosti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="51,381.41,357.23,98.99,8.97;51,151.56,368.27,58.20,8.97">DELOS Conference 2007 Working Notes</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Thanos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Borri</surname></persName>
		</editor>
		<meeting><address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ISTI-CNR, Gruppo ALI</publisher>
			<biblScope unit="page" from="185" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="51,142.88,379.67,337.50,8.97;51,151.56,390.59,328.94,8.97;51,151.56,401.51,328.86,8.97;51,151.56,412.55,329.05,8.97;51,151.56,423.47,329.05,8.97;51,151.56,434.39,328.90,8.97;51,151.56,445.43,22.85,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="51,309.31,379.67,171.07,8.97;51,151.56,390.59,134.47,8.97;51,437.15,401.51,43.27,8.97;51,151.56,412.55,329.05,8.97;51,151.56,423.47,169.43,8.97">Evaluation of Multilingual and Multi-modal Information Retrieval : Seventh Workshop of the Cross-Language Evaluation Forum (CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Agosti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="51,350.02,423.47,93.71,8.97">Revised Selected Papers</title>
		<title level="s" coord="51,450.23,423.47,30.38,8.97;51,151.56,434.39,140.62,8.97">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Stempfhuber</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006. 2007</date>
			<biblScope unit="volume">4730</biblScope>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
	<note>Scientific Data of an Evaluation Campaign: Do We Properly Deal With Them</note>
</biblStruct>

<biblStruct coords="51,142.88,456.71,337.49,8.97;51,151.56,467.75,256.90,8.97" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="51,458.38,456.71,21.99,8.97;51,151.56,467.75,167.86,8.97">Cross Language Experiments at Persian@CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aleahmad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kamalloo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zareh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rahgozar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Oroumchian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="51,142.88,479.15,337.73,8.97;51,151.56,490.07,159.12,8.97" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="51,309.66,479.15,170.95,8.97;51,151.56,490.07,90.01,8.97">UNIBA-SENSE at CLEF 2008: SEmantic N-levels Search Engine</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Semeraro</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="51,142.88,501.47,337.61,8.97;51,151.56,512.39,328.98,8.97;51,151.56,523.43,329.03,8.97;51,151.56,534.35,329.03,8.97;51,151.56,545.27,203.67,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="51,216.10,501.47,136.34,8.97">CLEF 2003 -Overview of results</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="51,276.31,512.39,204.23,8.97;51,151.56,523.43,329.03,8.97;51,151.56,534.35,148.87,8.97">Comparative Evaluation of Multilingual Information Access Systems: Fourth Workshop of the Cross-Language Evaluation Forum (CLEF 2003) Revised Selected Papers</title>
		<title level="s" coord="51,307.05,534.35,168.26,8.97">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="44" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="51,142.55,556.67,337.94,8.97;51,151.56,567.71,328.82,8.97;51,151.56,578.63,328.87,8.97;51,151.56,589.55,329.05,8.97;51,151.56,600.59,233.79,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="51,261.56,556.67,153.69,8.97">CLEF 2003 Methodology and Metrics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="51,322.37,567.71,158.01,8.97;51,151.56,578.63,328.87,8.97;51,151.56,589.55,180.07,8.97">Comparative Evaluation of Multilingual Information Access Systems: Fourth Workshop of the Cross-Language Evaluation Forum (CLEF 2003) Revised Selected Papers</title>
		<title level="s" coord="51,338.73,589.55,141.88,8.97;51,151.56,600.59,26.39,8.97">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="7" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="51,142.55,611.87,338.06,8.97" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="51,229.89,611.87,184.39,8.97">CACAO Project at the TEL@CLEF 2008 Task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bosca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dini</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="51,142.55,623.27,337.99,8.97;51,151.56,634.31,329.00,8.97;51,151.56,645.23,329.00,8.97;51,151.56,656.15,99.17,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="51,298.28,623.27,182.26,8.97;51,151.56,634.31,219.70,8.97">NUS-PT: Exploiting Parallel Texts for Word Sense Disambiguation in the English All-Words Tasks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="51,388.51,634.31,92.05,8.97;51,151.56,645.23,263.69,8.97">Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval 2007)</title>
		<meeting>the 4th International Workshop on Semantic Evaluations (SemEval 2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="52,142.55,119.04,337.96,8.97;52,151.56,130.08,329.11,8.97;52,151.56,140.99,227.90,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="52,213.69,119.04,191.90,8.97">The Cranfield Tests on Index Language Devices</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cleverdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="52,236.83,130.08,136.88,8.97">Readings in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Sparck</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Willett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename></persName>
		</editor>
		<meeting><address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publisher, Inc</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="47" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="52,142.55,152.51,338.10,8.97;52,151.56,163.44,60.62,8.97" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="52,272.72,152.51,203.70,8.97">XRCE&apos;s Participation to CLEF 2008 Ad-Hoc Track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-M</forename><surname>Renders</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="52,142.55,175.07,338.07,8.97;52,151.56,185.99,92.99,8.97" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Conover</surname></persName>
		</author>
		<title level="m" coord="52,215.01,175.07,137.33,8.97">Practical Nonparametric Statistics</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
	<note>1st edn</note>
</biblStruct>

<biblStruct coords="52,142.55,197.51,338.08,8.97;52,151.56,208.55,329.06,8.97;52,151.56,219.47,60.62,8.97" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="52,151.56,208.55,325.18,8.97">UFRGS@CLEF2008: Indexing Multiword Expressions for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Costa</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Geraldo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Orengo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">M</forename><surname>Villavicencio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="52,142.55,230.99,338.11,8.97;52,151.56,241.91,329.07,8.97;52,151.56,252.95,294.80,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="52,272.60,230.99,188.95,8.97">Appendix A: Results of the TEL@CLEF Task</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2008" />
	</analytic>
	<monogr>
		<title level="m" coord="52,298.62,241.91,177.24,8.97">Working Notes for the CLEF 2008 Workshop</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Borri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008-09-05">September 5. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="52,142.55,264.47,337.83,8.97;52,151.56,275.39,328.95,8.97;52,151.56,286.43,271.28,8.97" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="52,270.20,264.47,165.43,8.97">Appendix B: Results of the Persian Task</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2008" />
	</analytic>
	<monogr>
		<title level="m" coord="52,272.59,275.39,177.24,8.97">Working Notes for the CLEF 2008 Workshop</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Borri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008-09-05">September 5. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="52,142.55,297.95,337.95,8.97;52,151.56,308.87,328.95,8.97;52,151.56,319.79,271.28,8.97" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="52,264.92,297.95,159.68,8.97">Appendix C: Results of the Robust Task</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2008" />
	</analytic>
	<monogr>
		<title level="m" coord="52,264.55,308.87,184.07,8.97">Working Notes for the CLEF 2008 Workshop</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Borri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008-09-05">September 5. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="52,142.55,331.43,338.13,8.97;52,151.56,342.35,77.41,8.97" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="52,291.54,331.43,189.14,8.97;52,151.56,342.35,8.50,8.97">UniNE at CLEF2008: TEL, Perisan and Robust IR</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dolamic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fautsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="52,142.55,353.87,337.93,8.97;52,151.56,364.79,221.03,8.97" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="52,278.48,353.87,202.00,8.97;52,151.56,364.79,152.37,8.97">UFRGS@CLEF2008: Using Association Rules for Cross-Language Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Geraldo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">M</forename><surname>Orengo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="52,142.55,376.43,337.99,8.97;52,151.56,387.35,211.07,8.97" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="52,380.68,376.43,99.85,8.97;52,151.56,387.35,141.80,8.97">UNIGE Experiments on Robust Word sense Disambiguation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guyot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Falquet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Radhouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Benzineb</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="52,142.55,398.87,338.10,8.97" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="52,190.42,398.87,273.27,8.97">Using Statistical Testing in the Evaluation of Retrieval Experiments</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hull</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="52,151.56,409.79,329.08,8.97;52,151.56,420.83,329.06,8.97;52,151.56,431.75,240.05,8.97" xml:id="b23">
	<monogr>
		<title level="m" coord="52,347.91,409.79,132.73,8.97;52,151.56,420.83,329.06,8.97;52,151.56,431.75,52.73,8.97">Proc. 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1993)</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Korfhage</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Rasmussen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Willett</surname></persName>
		</editor>
		<meeting>16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1993)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="329" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="52,142.55,443.27,337.97,8.97;52,151.56,454.31,315.62,8.97" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="52,334.14,443.27,146.37,8.97;52,151.56,454.31,246.59,8.97">Investigation on Application of Local Cluster Analysis and Part of Speech Tagging on Persian Text</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jadidinejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Amiri</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="52,142.55,465.83,338.11,8.97;52,151.56,476.75,329.08,8.97;52,151.56,487.67,72.24,8.97" xml:id="b25">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Judge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">E</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lütkepohl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<title level="m" coord="52,430.17,465.83,50.49,8.97;52,151.56,476.75,174.62,8.97">Introduction to the Theory and Practice of Econometrics</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>2nd edn</note>
</biblStruct>

<biblStruct coords="52,142.55,499.31,337.94,8.97;52,151.56,510.23,329.06,8.97;52,151.56,521.15,60.62,8.97" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="52,233.74,510.23,243.00,8.97">Using Part of Speech tagging in Persian Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Karimpour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pishdad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aleahmad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Oroumchian</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="52,142.55,532.67,337.84,8.97;52,151.56,543.71,174.22,8.97" xml:id="b27">
	<monogr>
		<title level="m" type="main" coord="52,296.56,532.67,183.82,8.97;52,151.56,543.71,105.58,8.97">CLEF 2008 Ad-Hoc Track: On-line Processing Experiments with Xtrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kuersten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="52,142.55,555.23,338.13,8.97;52,151.56,566.15,49.10,8.97" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="52,201.58,555.23,260.57,8.97">Logistic Regression for Metadata: Cheshire takes on Adhoc-TEL</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Larson</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="52,142.55,577.67,337.84,8.97;52,151.56,588.71,248.25,8.97" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Borbinha</surname></persName>
		</author>
		<title level="m" coord="52,311.24,577.67,169.15,8.97;52,151.56,588.71,179.28,8.97">Technical University of Lisbon CLEF 2008 Submission (TEL@CLEF Monolingual Task)</title>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="52,142.55,600.23,338.03,8.97;52,151.56,611.15,328.88,8.97;52,151.56,622.19,129.23,8.97" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="52,442.66,600.23,37.91,8.97;52,151.56,611.15,328.88,8.97;52,151.56,622.19,60.14,8.97">SINAI at Robust WSD Task @ CLEF 2008: When WSD is a Good Idea for Information Retrieval Tasks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Martínez-Santiago</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Perea-Ortega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>García-Cumbreras</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="52,142.55,633.71,308.08,8.97" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="52,216.34,633.71,145.33,8.97">JHU Ad Hoc Experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Macnamee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="52,142.55,645.23,338.13,8.97;52,151.56,656.15,49.10,8.97" xml:id="b32">
	<monogr>
		<title level="m" type="main" coord="52,295.63,645.23,148.31,8.97">IRn in the CLEF Robust WSD Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Muñoz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="53,142.55,119.04,337.95,8.97;53,151.56,130.08,329.06,8.97;53,151.56,140.99,152.86,8.97" xml:id="b33">
	<monogr>
		<title level="m" type="main" coord="53,186.82,130.08,293.80,8.97;53,151.56,140.99,83.59,8.97">WikiTranslate: Query Translation for Cross-lingual Information Retrieval using only Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Overwijk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hauff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M G</forename><surname>De Jong</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="53,142.55,151.91,338.00,8.97;53,151.56,162.96,322.44,8.97" xml:id="b34">
	<monogr>
		<title level="m" type="main" coord="53,279.06,151.91,201.48,8.97;53,151.56,162.96,253.78,8.97">IXA at CLEF 2008 Robust-WSD Task using Word SenseDisambiguation for (Cross Lingual) Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Otegi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rigau</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="53,142.55,173.87,338.01,8.97;53,151.56,184.79,314.61,8.97" xml:id="b35">
	<monogr>
		<title level="m" type="main" coord="53,214.88,173.87,124.61,8.97">The DOI Handbook -Edition 4</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Paskin</surname></persName>
		</author>
		<idno type="DOI">10.1000/186[lastvisited2007</idno>
		<ptr target="http://dx.doi.org/10.1000/186[lastvisited2007" />
		<imprint>
			<date type="published" when="2006-08-30">August 30. 2006</date>
			<publisher>International DOI Foundation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="53,142.55,195.84,338.13,8.97;53,151.56,206.75,89.88,8.97" xml:id="b36">
	<monogr>
		<title level="m" type="main" coord="53,297.94,195.84,182.74,8.97;53,151.56,206.75,20.92,8.97">UCM-Y!R at CLEF 2008 Robust and WSD Tasks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Pérez-Agüera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="53,142.55,217.67,338.06,8.97;53,151.56,228.71,329.00,8.97;53,151.56,239.63,328.86,8.97" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="53,213.70,217.67,158.96,8.97">On GMAP: and Other Transformations</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="53,256.98,228.71,223.58,8.97;53,151.56,239.63,153.16,8.97">Proc. 15th International Conference on Information and Knowledge Management (CIKM 2006)</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Tsotras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>15th International Conference on Information and Knowledge Management (CIKM 2006)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="53,142.55,250.55,338.04,8.97;53,151.56,261.59,101.16,8.97" xml:id="b38">
	<monogr>
		<title level="m" type="main" coord="53,242.83,250.55,237.75,8.97;53,151.56,261.59,32.34,8.97">Cross-lingual Information Retrieval with Explicit Semantic Analysis</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sorg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="53,142.55,272.51,337.94,8.97;53,151.56,283.43,329.05,8.97;53,151.56,294.47,328.75,8.97" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="53,236.15,272.51,244.34,8.97;53,151.56,283.43,35.87,8.97">The Pragmatics of Information Retrieval Experimentation, Revisited</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tague-Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="53,342.87,283.43,133.86,8.97">Readings in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Sparck</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Willett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename></persName>
		</editor>
		<meeting><address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publisher, Inc</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="205" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="53,142.55,305.39,338.01,8.97;53,151.56,316.31,113.53,8.97" xml:id="b40">
	<monogr>
		<title level="m" type="main" coord="53,217.53,305.39,263.03,8.97;53,151.56,316.31,24.61,8.97">German, French, English and Persian Retrieval Experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="53,142.55,327.23,337.93,8.97;53,151.56,338.27,22.85,8.97" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="53,223.90,327.23,144.69,8.97">The TREC Robust Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="53,379.97,327.23,56.22,8.97">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="11" to="20" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
