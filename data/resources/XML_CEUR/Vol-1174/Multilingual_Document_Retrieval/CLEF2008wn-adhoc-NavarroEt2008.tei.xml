<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,123.66,147.76,355.74,17.23">IRn in the CLEF Robust WSD Task 2008</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,197.85,182.09,62.66,9.96"><forename type="first">Sergio</forename><surname>Navarro</surname></persName>
							<email>snavarro@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Processing and Information Systems Group</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.51,182.09,68.74,9.96"><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
							<email>llopis@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Processing and Information Systems Group</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.72,182.09,60.48,9.96"><forename type="first">Rafael</forename><surname>Mu√±oz</surname></persName>
							<email>rafael@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Processing and Information Systems Group</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,123.66,147.76,355.74,17.23">IRn in the CLEF Robust WSD Task 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D965B5DC3EA5B485154C9F8E3BAF76AB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.2 Information Storage H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation Information Retrieval, PRF, LCA, WordNet, Automatic Query Expansion, Relevance Feedback, WSD</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the Robust WSD Task within the CLEF 2008. The aim of this pilot task is exploring methods which can take profit of WSD information in order to improve the IR systems. In our approach we have used a passage based system jointly with a WordNet based expansion method for the collection documents and the queries using the two WSD systems runs provided by the organization. Furthermore we have experimented with two well known relevance feedback methods -LCA and PRF -, in order to figure out which is more suitable to take profit of the WSD query expansion based on Wordnet. Our best run has obtained a 4th place in the competition with a value of 0.4008 MAP. We conclude that LCA fits better than PRF to this task. And that our WSD expansion is useful for some query subsets. In future works we will study the features of the query subsets for which the performance of our system decreases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The aim of the CLEF Robust WSD Task task is exploring the contribution of Word Sense Disambiguation (WSD) to monolingual and multilingual Information Retrieval, in order to find successful methods to take profit of WSD information which helps the systems to increase their levels of robustness.</p><p>We are researching in the IR area, and it is so common to find -specially in collections of image annotations -documents which use narrow texts. It has a direct impact over the textual retrieval. Indeed, the problem of mismatch between a concept in a query and in a document, when it is expressed with different terms than found in the collection, is aggravated in this type of collections with small sized documents. Despite the fact that relevance feedback is a good tool for improving the results, it often shows unpredictable behaviour. Which makes us look for alternative and complementary methods. We believe that the solution pass through the use of external resources. Since that these narrow collections usually do not reflect as many relations between different related terms as in a standard collection -where usually there are more terms related that have at least a document where they are coocurring -.</p><p>Bearing in mind the efficiency of the system we have worked in a method which do not have a great cost in the retrieval phase for our system. Thus, we have used a simple strategy of term expansion for the collection documents and the queries, which is based on the WSD systems offered by the organization.</p><p>This paper is structured as follows: Firstly, it presents the main characteristics of the IRn system focusing on the documents and query expansion strategy, and the relevance feedback strategies, then it moves on to explain the experiments we have made to evaluate the system, and finally it describes the results and conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The IR-n System</head><p>In our approach, we used IR-n -an information retrieval system based on passages -. Passagebased IR systems treats each document as a set of passages, with each passage defining a portion of text or contiguous block of text. Unlike document-based systems, these systems can consider the proximity of words with each other, that appear in a document in order to evaluate their relevance <ref type="bibr" coords="2,132.91,345.44,9.96,9.96" target="#b3">[4]</ref>.</p><p>The IR-n passage-based system differs from other systems of the same category with regard to the method proposed for defining the passage -that is -using sentences as unit. Thus, passages are defined by a number of consecutive sentences in a document <ref type="bibr" coords="2,372.77,381.31,9.96,9.96" target="#b3">[4]</ref>.</p><p>IR-n uses stemmer and stopword lists to determine which information in a document will be used for retrieval. For a list of stemmers and stopwords used by IR-n, see www.unine.ch/infor/clef. IR-n uses several weighting models. Weighting models allow the quantification of the similarity between a text -a complete document or a passage in a document -and a query. Values are based on the terms that are shared by the text and query and on the discriminatory importance of each term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Expansion based on WordNet (WN) using WSD</head><p>This method, is an attempt to manage the information provided by a WSD system in order to overcome the problem of the mismatch between a concept in a query and in a document, and the problems derived from the natural language ambiguity.</p><p>The system expands terms within the queries and the collection documents. To carry out the expansion, it first selects the most likely WN synset returned by the WSD system -in the event of a tie it selects all the synsets with the maximum probability -. And afterwards, it generates the term expansion using all synonyms belonging to the selected synset/s .</p><p>In the phase of selecting the synset of a term, optionally IR-n can use two WSD systems in order to limit the synset selection only to those synsets which have been ranked as the most likely by one of the two WSD, and that at least has been ranked at second place by the other WSD system.</p><p>Finally, IR-n uses a parameter which allow, to configure the weight assigned for the terms added to the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relevance Feedback</head><p>Most IR systems use relevance feedback techniques <ref type="bibr" coords="2,316.36,689.07,9.96,9.96" target="#b2">[3]</ref>. These systems usually employ local feedback. The local feedback assumes that top-ranked documents are relevant. The added terms are, therefore, common terms from the top-ranked documents. Local feedback has become a widely used relevance feedback technique. Although, it can deter retrieval, in case most of the top-ranked documents are not relevant, results in TREC an CLEF conferences show that is an effective technique <ref type="bibr" coords="3,173.28,123.31,9.96,9.96" target="#b6">[7]</ref>.</p><p>In past works <ref type="bibr" coords="3,168.12,135.26,10.52,9.96" target="#b4">[5]</ref> we noticed that in spite of the improvements in the general results brought by the relevance feedback -we used PRF <ref type="bibr" coords="3,273.03,147.22,10.52,9.96" target="#b5">[6]</ref> relevance feedback strategy -, this process also adds wrong terms for the expansion in some of the cases. Therefore we decided to focus part of our efforts on finding an alternative strategy for the relevance feedback, Thus, we are comparing in this CLEF edition PRF with Local Context Analisy (LCA) <ref type="bibr" coords="3,352.49,183.08,9.96,9.96" target="#b6">[7]</ref>, as alternate strategy.</p><p>In the selection of terms, PRF gives more importance to those terms which have a higher frequency in the top relevant documents than in the whole collection. An alternative query expansion method relies on the Local Context Analysis (LCA), based on the hypothesis that a common term from the top-ranked relevant documents will tend to co-occur with all query terms within the top-ranked documents. That is an attempt to avoid including terms from top-ranked, non-relevant documents in the expansion. Furthermore, in the case of polysemous words, this method will help to retrieve documents more related to the sense of the query, since it is logical to think that the user will use words from the domain associated with this sense to complete the query. Indeed we think that in this year participation it could bebetter to use a method based on the terms of the query as LCA, since that the expanded terms based on WN used in the query and in all the documents, could boost performance of this relevance feedback strategy, improving its ability for skipping non relevant documents.</p><p>The IR-n architecture allows us to use query expansion based on either the most relevant passages or the most relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training</head><p>IR-n is a parameterizable system, which means that it can be adapted in line with the concrete characteristics of the task at hand. The parameters for this configuration are the number of sentences that form a passage, the weighting model to use, the type of expansion, the number of documents/passages on which the expansion is based, the weight used for the WN based expanded terms, the average number of words per document and the WSD system used.</p><p>This section describes the training process that was carried out in order to obtain the best possible features for improving the performance of the system.</p><p>The collections and resources are described first, and the next section describes specific experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>The organization has provided topics and document collections from previous CLEF campaigns -from year 2001 to year 2006 -which were annotated by two different systems for word sense disambiguation (WSD) developed by a group of the University of Barcelona (UBC) <ref type="bibr" coords="3,472.34,571.05,10.52,9.96" target="#b0">[1]</ref> and a group of the National University of Singapore (NUS) <ref type="bibr" coords="3,324.85,583.01,9.96,9.96" target="#b1">[2]</ref>. The documents are in English, and the topics in both English and Spanish.</p><p>The Table <ref type="table" coords="3,152.28,606.92,4.98,9.96" target="#tab_0">1</ref> and Table <ref type="table" coords="3,206.56,606.92,4.98,9.96" target="#tab_1">2</ref> show us the queries and collections used for training and test phases. Where the Topics No. column, is the range of queries used from the CLEF Ad-hoc task competition of the indicated competition year -CLEF Year -. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments</head><p>The experiment phase aims to establish the optimum values for the configuration of the system for the collection.</p><p>Below is a description of the input parameters of the system:</p><p>The Passage size (ps): Number of sentences in a passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weight Model (wm):</head><p>We used DFR weighting model.</p><p>Relevance Feedback (relFB): Indicating which relevance feedback uses the system -PRF or LCA.</p><p>Relevance Feedback parameters: If exp has value 1, this denotes we use relevance feedback based on passages. But, if exp has value 2, the relevance feedback is based on documents. Moreover, num denotes the number of passages or documents that the relevance feedback will use from the textual ranking and finally, term indicates the k terms extracted from the best ranked passages or documents from the original query.</p><p>WSD system used for the expansion of the Collection (WSDCOL): Indicate which WSD system has been used or if none has been used for the documents expansion.</p><p>WSD system used for the expansion of the Query (WSDQuery): Indicate which WSD system has been used or if none has been used for the query expansion.</p><p>Weight for the WN based Expanded Terms (wWN): Is the weight used for the expanded terms using WN within the query.</p><p>Our participation is limited to the English monolingual task. Thus, for the experiments we have worked with DFR as the weighting schema. We have taken this decision based on the good results obtained in previous works for this language <ref type="bibr" coords="4,318.45,533.71,9.96,9.96" target="#b4">[5]</ref>.</p><p>We started the experiments looking for which was the best configuration for the collection, in order to use it as baseline in our participation. Table <ref type="table" coords="4,332.61,557.61,4.98,9.96" target="#tab_2">3</ref> shows the best configurations obtained with our passages based system without using neither relevance feedback techniques nor query expansion based on WN. We have used the best run configuration -in MAP terms -, which uses a passage size of four sentences, as the base configuration for the training phase. The next experiments have added different combinations of values of relevance feedback and WN expansion parameters. In an attempt We can see that the worst results -under the baseline results -are obtained for those runs which use only expansion for the query -not for the collection -. The best run is the one which uses NUS WSD system for expand the query and the collection. An finally we saw the method of mixing the two WSD does not improve the run which only uses NUS WSD system. Due to time restriction we do not have results mixing WSD systems for the expansion of the collection.</p><p>next table -Table <ref type="table" coords="5,203.96,538.52,4.98,9.96" target="#tab_3">4</ref> -shows MAP and Recall values for the best runs for each combination of WSDCOL and WSDQuery parameters. As we forecasted we can observe that the best MAP results are obtained using LCA. Indeed, the major improvement respect PRF occurs with the run witch use NUS WSD system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results in 2008 Robust WSD Task</head><p>The organization of the task only have allowed to send 4 submission using WSD runs and 4 without using WSD. Thus, we have sent to the task two runs without WSD: the baseline, and the best run which used only LCA. And for the 4 WSD runs, we have sent the best run without relevance feedback and the three best runs using relevance feedback.</p><p>Due to problems related with using an incomplete test query set -we submitted our runs out of time -. Thus, it has made that our results does not appear between the official task results. Table <ref type="table" coords="6,117.27,476.10,4.98,9.96" target="#tab_5">6</ref> and Table <ref type="table" coords="6,171.95,476.10,4.98,9.96" target="#tab_6">7</ref> show the results obtained in the tasks Monolingual without WSD and in the task with WSD respectively. The results are ordered by MAP. Also, we can see in this table our ranking position in MAP terms within the competition results. The best run submitted by the participants without using WSD in the competiton has obtained a value of 0.4515 of MAP.</p><p>The best run submitted by the participants using WSD has obtained a value of 0.4499 of MAP. On the one hand, opposite to what happens in training phase, all the runs which have used WSD have obtained results for all the measures under the results of the run which have used LCA without WSD. On the other hand these results show us that LCA as in the training phase always improves the results respect the same configuration without its use. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We conclude from those results that in spite our WSD approach has showed good results with the training set, we have doubts about its suitability in general for all kind of queries. Since that we have obtained contradictory results in the competition. In future works we will try to research the causes of its behaviour with the competition query set, analysing the possible error sources -the method itself, the wordnet organization or errors in disambiguation by the WSD systemsand its relation with the features of the queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>This research has been partially funded by the Spanish Government within the framework of the TEXT-MESS (TIN-2006-15265-C06-01) project and by European Union (EU) within the framework of the QALL-ME project (FP6-IST-033860).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,201.42,118.27,200.19,9.96;5,112.88,139.03,377.17,259.44"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MAP evolution with WN expansion</figDesc><graphic coords="5,112.88,139.03,377.17,259.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,192.06,637.69,218.88,60.71"><head>Table 1 :</head><label>1</label><figDesc>Training Query Set and Data Collections CLEF Year Topics No.</figDesc><table coords="3,211.75,649.39,198.01,49.01"><row><cell></cell><cell></cell><cell>Collections</cell></row><row><cell>2001</cell><cell>41-90</cell><cell>L.A. Times 94</cell></row><row><cell>2002</cell><cell>91-140</cell><cell>L.A. Times 94</cell></row><row><cell>2004</cell><cell>201-250</cell><cell>Glasgow Herald 95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,150.65,118.27,301.69,60.72"><head>Table 2 :</head><label>2</label><figDesc>Test Query Set and Data Collections CLEF Year Topics No.</figDesc><table coords="4,341.25,129.97,54.95,9.95"><row><cell>Collections</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,219.72,611.96,163.55,83.48"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table coords="4,219.72,611.96,163.55,83.48"><row><cell>ps</cell><cell>c</cell><cell cols="3">Best Baseline Runs avgld map recall</cell></row><row><cell>5</cell><cell>5.5</cell><cell>750</cell><cell>0.3556</cell><cell>0.8874</cell></row><row><cell cols="2">4 5.5</cell><cell>750</cell><cell cols="2">0.3572 0.8865</cell></row><row><cell>3</cell><cell>6</cell><cell>850</cell><cell>0.3498</cell><cell>0.8840</cell></row><row><cell>2</cell><cell>8.5</cell><cell>750</cell><cell>0.3332</cell><cell>0.8806</cell></row><row><cell>1</cell><cell>2</cell><cell>750</cell><cell>0.3133</cell><cell>0.8679</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,104.94,581.26,302.18,168.42"><head>Table 4 :</head><label>4</label><figDesc>Best Runs without Relevance Feedback</figDesc><table coords="5,104.94,591.02,301.46,158.66"><row><cell>WSD</cell><cell>WSD</cell><cell></cell><cell></cell><cell></cell></row><row><cell>COL</cell><cell>Query</cell><cell>wWN</cell><cell>map</cell><cell>recall</cell></row><row><cell>no</cell><cell>no</cell><cell>0</cell><cell>0.3572</cell><cell>88.65</cell></row><row><cell>no</cell><cell>NUS</cell><cell>0.1</cell><cell>0.3565</cell><cell>89.67</cell></row><row><cell>no</cell><cell>UBC</cell><cell>0.1</cell><cell>0.3567</cell><cell>89.52</cell></row><row><cell>no</cell><cell>NUS+UBC</cell><cell>0.2</cell><cell>0.3669</cell><cell>92.02</cell></row><row><cell>NUS</cell><cell>NUS</cell><cell>0.2</cell><cell cols="2">0.3748 92.38</cell></row><row><cell>UBC</cell><cell>UBC</cell><cell>0.2</cell><cell>0.3649</cell><cell>91.31</cell></row><row><cell cols="2">NUS NUS+UBC</cell><cell>0.2</cell><cell>0.3745</cell><cell>92.26</cell></row><row><cell cols="2">UBC NUS+UBC</cell><cell>0.4</cell><cell>0.3644</cell><cell>90.60</cell></row><row><cell>In the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,135.36,739.72,377.63,9.96"><head>Table 5 ,</head><label>5</label><figDesc>we show the best results obtained using LCA and PRF with each one of the best runs of the Table4.</figDesc><table coords="6,139.34,140.19,324.30,169.56"><row><cell></cell><cell cols="6">Table 5: Best Relevance Feedback Runs WSD WSD</cell><cell></cell><cell></cell></row><row><cell cols="7">relFb COL Query wWN exp num k</cell><cell>map</cell><cell>recall</cell></row><row><cell>no</cell><cell>no</cell><cell>no</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.3572</cell><cell>0.8865</cell></row><row><cell>prf</cell><cell>no</cell><cell>no</cell><cell>0</cell><cell>2</cell><cell>5</cell><cell cols="2">15 0.3719</cell><cell>0.9040</cell></row><row><cell>lca</cell><cell>no</cell><cell>no</cell><cell>0</cell><cell>1</cell><cell>5</cell><cell cols="2">15 0.3833</cell><cell>0.9201</cell></row><row><cell>prf</cell><cell>NUS</cell><cell>no</cell><cell>0</cell><cell>2</cell><cell>5</cell><cell>5</cell><cell>0.3626</cell><cell>0.9381</cell></row><row><cell>lca</cell><cell>NUS</cell><cell>no</cell><cell>0</cell><cell>2</cell><cell>15</cell><cell cols="2">10 0.3845</cell><cell>0.9393</cell></row><row><cell>prf</cell><cell>NUS</cell><cell>NUS</cell><cell>0.2</cell><cell>2</cell><cell>10</cell><cell cols="2">10 0.3756</cell><cell>0.9417</cell></row><row><cell>lca</cell><cell>NUS</cell><cell>NUS</cell><cell>0.2</cell><cell>2</cell><cell>15</cell><cell cols="3">10 0.3949 0.9417</cell></row><row><cell>prf</cell><cell>UBC</cell><cell>no</cell><cell>0</cell><cell>2</cell><cell>10</cell><cell cols="2">10 0.3651</cell><cell>0.9250</cell></row><row><cell>lca</cell><cell>UBC</cell><cell>no</cell><cell>0</cell><cell>2</cell><cell>15</cell><cell cols="2">10 0.3668</cell><cell>0.9321</cell></row><row><cell>prf</cell><cell>UBC</cell><cell>UBC</cell><cell>0.2</cell><cell>2</cell><cell>10</cell><cell cols="2">10 0.3761</cell><cell>0.9262</cell></row><row><cell>lca</cell><cell>UBC</cell><cell>UBC</cell><cell>0.2</cell><cell>2</cell><cell>20</cell><cell cols="2">10 0.3803</cell><cell>0.9286</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,115.90,530.78,371.17,70.33"><head>Table 6 :</head><label>6</label><figDesc>Results in 2008 Robust WSD Task -Without WSD runs -</figDesc><table coords="6,115.90,540.55,371.17,60.57"><row><cell></cell><cell></cell><cell cols="2">WSD WSD</cell><cell>rk</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">WSD WSD CLEF</cell><cell></cell></row><row><cell>runName</cell><cell cols="3">relFb COL Query</cell><cell>map</cell><cell>map</cell><cell>gmap recall</cell></row><row><cell>TestIRnSinColLCA</cell><cell>lca</cell><cell>no</cell><cell>no</cell><cell>3</cell><cell cols="2">0.4008 0.1514 0.8851</cell></row><row><cell>TestIRnSinCol</cell><cell>no</cell><cell>no</cell><cell>no</cell><cell>10</cell><cell cols="2">0.36610 0.1473 0.8851</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,104.15,118.27,394.68,95.05"><head>Table 7 :</head><label>7</label><figDesc>Results in 2008 Robust WSD Task -WSD runs -</figDesc><table coords="7,104.15,128.03,394.68,85.28"><row><cell></cell><cell></cell><cell cols="2">WSD WSD</cell><cell>rk</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">WSD WSD CLEF</cell><cell></cell></row><row><cell>runName</cell><cell cols="3">relFb COL Query</cell><cell>map</cell><cell>map</cell><cell>gmap recall</cell></row><row><cell>TestIRnUBC 0.2 LCA</cell><cell>lca</cell><cell>UBC</cell><cell>UBC</cell><cell>13</cell><cell cols="2">0.3748 0.1361 0.8768</cell></row><row><cell>TestIRnNUSSoloCol LCA</cell><cell>lca</cell><cell>NUS</cell><cell>no</cell><cell>14</cell><cell cols="2">0.3726 0.1384 0.8722</cell></row><row><cell>TestIRnNUS 0.2 LCA</cell><cell>lca</cell><cell>NUS</cell><cell>NUS</cell><cell>15</cell><cell cols="2">0.3720 0.1389 0.8761</cell></row><row><cell>TestIRnNUS 0.2</cell><cell>no</cell><cell>NUS</cell><cell>NUS</cell><cell>16</cell><cell cols="2">0.3664 0.1471 0.8669</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.50,461.05,407.46,9.96;7,105.50,473.00,407.46,9.96;7,105.50,484.96,132.85,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,292.78,461.05,146.81,9.96">Combining k-nn with svd for wsd</title>
		<author>
			<persName coords=""><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oier</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,463.33,461.05,49.63,9.96;7,105.50,473.00,334.25,9.96">Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval 2007)</title>
		<meeting>the 4th International Workshop on Semantic Evaluations (SemEval 2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,504.48,407.47,9.96;7,105.50,516.43,407.49,9.96;7,105.50,528.39,374.11,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,311.64,504.48,201.32,9.96;7,105.50,516.43,169.63,9.96">Exploiting parallel texts for word sense disambiguation in the english all-words tasks</title>
		<author>
			<persName coords=""><forename type="first">Yee</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ng</forename><surname>Hwee Tou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhong</forename><surname>Zhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,296.28,516.43,216.71,9.96;7,105.50,528.39,165.28,9.96">Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval 2007)</title>
		<meeting>the 4th International Workshop on Semantic Evaluations (SemEval 2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,547.90,407.49,9.96;7,105.50,559.85,407.49,9.96;7,105.50,571.81,407.49,9.96;7,105.50,583.76,407.53,9.96;7,105.50,595.72,30.46,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,257.30,547.90,255.69,9.96;7,105.50,559.85,123.49,9.96">Combining Query Translation and Document Translation in Cross-Language Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,140.70,571.81,238.14,9.96">4th Workshop of the Cross-Language Evaluation Forum</title>
		<title level="s" coord="7,443.92,571.81,69.07,9.96;7,105.50,583.76,239.21,9.96">Lecture notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF; Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
	<note>Lecture notes in Computer Science</note>
</biblStruct>

<biblStruct coords="7,105.50,615.24,407.50,9.96;7,105.50,627.19,154.61,9.96" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
		</author>
		<title level="m" coord="7,184.23,615.24,298.91,9.96">IR-n: Un Sistema de Recuperacin de Informacin Basado en Pasajes</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Alicante</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="7,105.50,646.70,407.50,9.96;7,105.50,658.65,407.51,9.96;7,105.50,670.62,49.14,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,407.51,646.70,105.49,9.96;7,105.50,658.65,243.29,9.96">Information Retrieval of Visual Descriptions with IR-n System based on Passages</title>
		<author>
			<persName coords=""><forename type="first">Sergio</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rafael</forename><surname>Mu√±oz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elisa</forename><surname>Noguera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,486.36,658.65,26.64,9.96;7,105.50,670.62,18.52,9.96">CLEF 2007</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>In on-line Working Notes</note>
</biblStruct>

<biblStruct coords="7,105.50,690.13,407.50,9.96;7,105.50,702.08,278.42,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,281.29,690.13,158.15,9.96">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,450.37,690.13,62.63,9.96;7,105.50,702.08,181.62,9.96">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,721.60,407.49,9.96;7,105.50,733.55,265.18,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,242.88,721.60,270.11,9.96;7,105.50,733.55,68.23,9.96">Improving the effectiveness of information retrieval with local context analysis</title>
		<author>
			<persName coords=""><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,182.25,733.55,97.04,9.96">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="112" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
