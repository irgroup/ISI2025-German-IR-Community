<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,171.83,148.72,259.42,15.51">LIG at ImageCLEFphoto 2008</title>
				<funder ref="#_kBFCJ9a">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,263.79,182.19,75.44,9.96"><forename type="first">Philippe</forename><surname>Mulhem</surname></persName>
							<email>philippe.mulhem@imag.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">UJF</orgName>
								<orgName type="laboratory" key="lab2">UMR CNRS 5217</orgName>
								<orgName type="laboratory" key="lab3">Laboratoire d&apos;informatique de Grenoble</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,171.83,148.72,259.42,15.51">LIG at ImageCLEFphoto 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DD6A8097C4618961B8863FCFD4FDF48C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Text indexing and retrieval, Image indexing and retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This working notes describe the runs and results obtained by the LIG at ImageCLEFphoto 2008. The submitted runs are: two runs (text only and text+image) without diversification on classes, and two runs (text only and text+image) with class diversification were submitted. The text retrieval is based on language model of Information Retrieval, and the image part is processed using RGB histograms on 9 image blocks with a similarity value based on Jeffrey divergence. Results using text+image are obtained by a linear combination of normalized results on text and image. The diversification is based on clusters, according to the cluster given in the queries. When the cluster name is not directly extracted from the images (like city or country), we apply a visual clustering. Not surprisingly, the cluster recall at 20 (i.e., cr(20)) results are higher for the runs that include diversification. On the other hand, the precision at 20 and the mean average precision results are higher without diversification on our runs, for both text only and image+text results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the runs that where submitted at ImageCLEFphoto 2008 by the LIG (Laboratoire d'Informatique de Grenoble). The runs submitted deal with text only and text+image retrieval. The main idea behind our submissions is to make a first step into considering language models for text and for image. Another aspect of this work is to check the impact of image content on the quality of the retrieval. When considering clusters of images to ensure diversification of the results, we applied simple solutions based on the analysis of the images description when available, and we relied on visual clustering of the images when the clustering was not expressed in the location field of the image description. Among the four runs submitted, our best results are below the average ImageCLEFphoto results for the map (-5%) and the precision at 20 documents (-3%), and the results obtained for the clustering recall at 20 documents are above the average (+19%). This fact shows that the diversification based on simple features may impact positively the results.</p><p>The remaining of the paper is organized as follows. Section 2 describes the runs submitted by focusing on the text processing, the image processing and the fusion and the processing of text+image, section 3 details the results obtained, and we conclude in part 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Description of the runs</head><p>As descibed above, the runs submitted consider on ons sidetext only and text+image indexing and retrieval, and the use of diversification or not on other side. We describe in this section these different aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text processing</head><p>When considering text retrieval, it has been shown by <ref type="bibr" coords="2,335.95,284.23,10.52,9.96" target="#b0">[1]</ref> that language models (LM) of Information Retrieval, inspired by speech recognition, give results that are close to, or outperform, existing approaches (like Vector Space Model <ref type="bibr" coords="2,293.91,308.14,10.52,9.96" target="#b2">[3]</ref> or probabilistic models based on BM25 for instance <ref type="bibr" coords="2,121.34,320.09,10.29,9.96" target="#b1">[2]</ref>). Different kinds of Language Models already exist, and we chose to use a language model that expresses the probability for the query Q to be generated from a document model D by : P(Q|D). Such probability is computed using the probability of any term t i to be generated by the document, P(t i |D). One strong aspect of the LM is the use of smoothing leading to consider that a document that does not contain a term does not have a zero probability of generating this term. The smooting used in our experiments is the Dirichlet smoothing, that has been shown in citezhailaferty04 to be a good smothing method. Such smoothing is described using the following formula:</p><formula xml:id="formula_0" coords="2,104.94,423.65,117.01,24.34">P (t i |D) = tf(ti,D)+µ. tf(t i ,C)</formula><p>tf( * ,C) tf(ti,D)+µ where</p><p>• C is the corpus of documents considered,</p><p>• tf(x,D) denotes the term frequency of the term x in the document D,</p><p>• tf(x,D) denotes the term frequency of the term x in the corpus C,</p><p>• µ is a parameter defined experimentally that indicates the importance of the corpus on the smoothing.</p><p>A usual information retrieval preprocessing is performed: we remove stopwords from the image descriptions and from the queries, and we stem the texts using a Porter stemmer. We use the concatenation of the TITLE, DESCRIPTION and NOTES fields, in lower case, of the xml description of the images as their text description.</p><p>For the query processing, we used the query and the narrative as the text query, and the smae preprocessing is applied on this text.</p><p>As explained above, the query processing evaluates P(Q|D) as: P (Q|D) = P (t q,1 , ..., t q,nq |D)</p><formula xml:id="formula_1" coords="2,276.22,671.04,236.78,25.05">= i P (t q,i |D)<label>(1)</label></formula><p>The results are ranked according to the decreasing order of this probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image processing</head><p>The processing of the images computes histograms on image blocks. The result obtained on 9 blocks gave better results than taking the whole image in previous of our studies on personal photographs. Each image of the corpus is split into a 3x3 regular grid. For each of the blocks b I,i , 1 ≤ i ≤ 9 of an image I, we extract an RGB histogram, H I,i , 1 ≤ i ≤ 9. The choice of the RGB color space is due to the fact that it gives similar results than other color spaces that require more processing time to be computed. Each histogram has 512 bins, according to a 8×8×8 regular split of the RGB cube. Such size is a tradeof between too small histograms unable to discriminate visual elements, storage uasge and matching duration. Then, one global histogram GH I with 512×9=4602 bins is created for one image, by concatenating the 9 H I,i histograms. GH I is then normalized to 1.</p><p>The matching function between two images I and J uses the Jensen-Shannon divergence. This divergence is a symmetrical version of the Kullback-Leibler divergence. The formula of the Jensen-Shannon divergence between two histograms GH I and GH J is:</p><formula xml:id="formula_2" coords="3,98.13,293.67,414.87,28.35">JS(GH I ||GH J ) = 1 2 i GH I (i).log( GH I (i) GHI (i)+GHJ (i) 2 ) + 1 2 i GH J (i).log( GH J (i) GHI (i)+GHJ (i) 2 ) (2)</formula><p>The similarity S im (I, J) between two images I and J is defined as 1 -JS(GH I ||GH J ).</p><p>In imageCLEFphoto, the visual part of a query is composed of 3 images, each image representing one sample of what is expexted. We consider that the relevance of images in the corpus depends on the best relevance value wrt. each of the query images. The similarity between a set of images IS q = {I q,i } and one image J is then defined as the maximum of the similarity between J and each of the images of IS q .</p><p>The results are then ranked according to the decreasing order of this similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image+Text processing</head><p>Several ways may be used to represent mixed textual and visual data for retrieval. We consider that even if the internal representation for these two media are somewhat similar (distribution of probabilities), their inner nature refrain us to apply a kind of early fusion of them by concatenating the two distributions. Another reason is that "adding" two distributions does not necessarily create another one. That is why, in our run, we consider a late fusion defined as a linear combination of the two results (text and image) obtained.</p><p>In fact, the linear distribution is applied on normalized results, using for the text (resp. the image) the minimum and the maximum matching values to ensure the normalized results to be in [0, 1]. This linear combination is refined using the following heuristics: we assume that the best visual results (i.e., very similar to one of the query images), are often relevant, which is not the case for the text. So, we extend the matching function using a condition on the matching:</p><formula xml:id="formula_3" coords="3,116.59,602.71,396.41,24.80">matching(Q, D) = 1, if Sim v (Q v , D v ) &gt; t v α.Sim t (Q t , D t ) + (1 -α).Sim v (Q v , D v ) othewise<label>(3)</label></formula><p>with Q v (resp. Q t ) the visual (resp. textal) part of the query, and D v (resp. D t ) the visual (resp. textal) part of the document. The results are then ranked according to the decreasing order of this similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Diversification processing</head><p>As one task for the imageCLEFphoto this year was dedicated to study the capacity of the systems to provide diverse results for a query, and not only near duplicate results. To achieve this goal, we defined clusters according to several criteria:</p><p>• country: based on the LOCATION field of the corpus images description, we generated a set of clusters C country , that groups images by country name. C country contains 23 classes, with on average 952 images per cluster.</p><p>• city: based on the LOCATION field of the corpus images description, we generated a set of clusters C city , that groups images by city name and country when available. One cluster of C city contains all the images with no city. C city contains 511 classes, with on average 38 images per cluster.</p><p>• clusters on date where also built. Because such clusters were not used during the query processing we only mention them;</p><p>• visual: we used a KNN clustering, namely C visual , based on the visual description of the images (4608 dimension histograms). The target number of cluster is 500, i.e. similar to the number of clusters for the cities. There is 40 images per cluster on average.</p><p>Depending on the query un ser consideration, a clustering is chosen for the diversification process:</p><p>• Queries having a cluster name city are diversified using the C city clusters;</p><p>• Queries having a cluster name state or country are diversified using the C country clusters;</p><p>• Queries having another cluster name are diversified using visual clusters C visual .</p><p>We describe now the way the diversification is processed, according to a clustering C x :</p><p>1. From the top of the initial (not diversified) result list L o , we generate a diversified list L d that contains only the best representative of one cluster of C x according to the ranking of L o , until the diversified list contains 20 elements.</p><p>2. In a second step, we loop on L o from its top, and we add to L d the elements of L o that do not already belong to L d .</p><p>This process ensures that the 20 first results are from different clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We describe in this section the official results obtained by our approach. According to the parameters described in the previous parts, the λ of the text language model is set to 1500, using the Zettair system <ref type="bibr" coords="4,174.34,552.77,9.96,9.96" target="#b3">[4]</ref>. For the runs text+image, we set the threshold t v to 0.99 and the α used in the linear combination to 0.55 . We apply the diversification process explained earlier. To be able to study the advantages or drawbacks of the diversification, we submitted diversified and not diversified runs.</p><p>The figure <ref type="figure" coords="4,154.02,600.59,4.98,9.96">1</ref> presents the evolution of the precision for our results, and the figure 2 presents the cluster recall for our four submitted runs. The runs without diversification (NOCLUST) are presented with dotted lines in these two figures, where the plain lines correspond to diversified runs.</p><p>We analyse first the results regarding the precision of figure <ref type="figure" coords="4,367.27,648.41,3.87,9.96">1</ref>. Without diversification, we see in this figure that the text+image configuration outperforms the text only runs (+34% averaged on the 6 values given by the official results).</p><p>For the text only run, we see that for the precision after 15, 20 and 30 documents the precision value is roughly constant (0.2171, 0.2026 and 0.2103). This means that at 15 documents the systems gives 3.2 relevant documents, and at 30 documents the system gives 6.3 relevant documents on average. On the other hand, the text+image run shows a monotonic decreasing of the precision. Considering the diversified results, the text+image also outperfoms the text only based system by 46.32% (averaged on the 6 values given by the official results). For both text+image and text Figure <ref type="figure" coords="5,121.75,376.81,3.87,9.96">1</ref>: Precision for LIG runs for text only (TXT) and text+image (TXTIMG), without and with diversification only results, we see that the precision is increased between 20 and 30 documents. We explain this fact by our diversification process that manages in a different way the first 20 documents, and then only adds the retrieved documents: because the first documents have more chances to be relevant, such documents can be placed from the 20th document of the diversified list, increasing the precision. The diversification process lowers the precision values obtained, for both the text+image and the text only results. This can be explained by the fact that the diversification pushes up documents that are not in the first results of the initial list, and these documents are potentially less relevant. For the text+image results however, the difference after 30 documents is not large (0.2684 versus 0.2436), which means that on average only one additional relevant document is retrieved without diversification compared to the results after diversification.</p><p>We study then the cluster recall resuts presented in figure <ref type="figure" coords="5,361.08,562.11,3.87,9.96" target="#fig_0">2</ref>. As expected, the results obtained after diversification for text only (resp. text+image) outperform the results obtained without diversification (+9.6% for text only retrieval, +5.7% for text+image, averaged on the 8 values of the official results). At 20 documents, the difference is +16.7% for the text only, and +10.8% for the text+image run. We see on the figure2 that, here also, the text+image runs give better results than text only runs. However, after 50 documents, the difference beween diversified and undiversified results is very small for the text+image runs: +2.0% on average. In this case, the diversification is almost useless with respect to the cluster recall.</p><p>For the text only runs, a difference of 3.0% is achieved after 100 documents, leading to conclude that the undiversified text results benefit more from the diversification than the texte+image results. This is validated through the study of the average of the differences for diversified results versus undiversified results up to the 30th document (i.e., for 5, 10, 15, 20 and 30): the text only diversified run outperforms by +19.9% the undiversified text only, and the text+image diversified run outperforms by +11.2% the text+image undiversified results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The work described here lists the runs and results obtained by the LIG at ImageCLEFphoto 2008. We experimented text and text+image retrieval. For the text, we considered a language model using a Dirichlet smooting. For the image, we extracted RGB histograms of 9 blocks of the images, ane the matching was performed using a Jeffrey divergence. For the retrieval and text+image, we used a linear combination of the matching values for the text and for the images of the query. We applied a simple diversification scheme in a way to force multiple clusters to be given for the 20 first results. We submitted results with and without diversification. When comparing our different runs, we found out that text+image runs consistently outperform text only runs. For the precision values, undiversified results outeperfom diversified runs. For the cluster recall, diversified runs outperfom undiversified results. So, we conclude that precision and recall do not benefit both from the diversification process. Such antagonism between recall and precision is new in IR, but we think that studying the effect of our diversification process, and testing other ways to diversify (other clusters, smarter clustering, smarter reranking of the results) may help to increase both precision and recall, at least below 30 documents. The results obtained are below the average of the runs with and without diversification for the text only, according to the precision and the cluster recall. For the text+image runs, the results are below the average when considering the non-diversified run, but above the average when using diversification for the cluster recall.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,90.00,384.01,423.00,9.96;6,90.00,395.96,102.48,9.96;6,92.17,109.48,420.45,259.98"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Cluster Recall for LIG runs for text only (TXT) and text+image (TXTIMG), without and with diversification</figDesc><graphic coords="6,92.17,109.48,420.45,259.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,92.15,109.47,420.48,252.78"><head></head><label></label><figDesc></figDesc><graphic coords="5,92.15,109.47,420.48,252.78" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was partially supported by the <rs type="funder">French National Agency of Research</rs> (<rs type="grantNumber">ANR-06-MDCA-002</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kBFCJ9a">
					<idno type="grant-number">ANR-06-MDCA-002</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.50,133.27,407.51,9.96;7,105.50,145.23,233.41,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,259.37,133.27,236.81,9.96">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jay</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,105.50,145.57,134.11,9.18">Proceedings of the ACM SIGIR</title>
		<meeting>the ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,165.16,407.48,9.96;7,105.50,177.11,90.80,9.96" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,443.73,165.16,65.30,9.96">Okapi at trec-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>S E Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gatford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,197.04,407.49,9.96;7,105.50,208.99,22.68,9.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,250.81,197.37,194.42,9.18">Introduction to modern information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,228.91,281.85,9.96" xml:id="b3">
	<monogr>
		<ptr target="http://www.seg.rmit.edu.au/zettair/" />
		<title level="m" coord="7,105.50,228.91,110.04,9.96">The Zettair search engine</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
