<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,130.20,97.77,342.84,17.23;1,103.92,119.73,395.40,17.23">UPMC/LIP6 at ImageCLEFphoto 2008: on the exploitation of visual concepts (VCDT)</title>
				<funder ref="#_yaBs5kk">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,99.24,159.97,64.14,9.96"><forename type="first">Sabrina</forename><surname>Tollari</surname></persName>
						</author>
						<author>
							<persName coords="1,170.55,159.97,78.16,9.96"><forename type="first">Marcin</forename><surname>Detyniecki</surname></persName>
						</author>
						<author>
							<persName coords="1,256.48,159.97,75.93,9.96"><forename type="first">Ali</forename><surname>Fakeri-Tabrizi</surname></persName>
						</author>
						<author>
							<persName coords="1,339.66,159.97,82.62,9.96"><forename type="first">Massih-Reza</forename><surname>Amini</surname></persName>
						</author>
						<author>
							<persName coords="1,430.67,159.97,73.24,9.96"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire d&apos;Informatique de Paris</orgName>
								<orgName type="institution">Université Pierre et Marie Curie-Paris6</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR CNRS 7606</orgName>
								<address>
									<addrLine>104 avenue du président Kennedy</addrLine>
									<postCode>75016</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,130.20,97.77,342.84,17.23;1,103.92,119.73,395.40,17.23">UPMC/LIP6 at ImageCLEFphoto 2008: on the exploitation of visual concepts (VCDT)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0BE5686EAFAFDF632F09463E1026D27D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation Forest of Fuzzy Decision Trees, Cooccurrences Analysis, Visual Concepts, Multi-Class Multi-Label Image Classification, Multimodal Image Retrieval, TF-IDF, Language Model, WordNet</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this working note, we focus our efforts on the study of how to automatically extract and exploit visual concepts. First, in the Visual Concept Detection Task (VCDT), we look at the mutual exclusion and implication relations between VCDT concepts in order to improve the automatic image annotation by Forest of Fuzzy Decision Trees (FFDTs). In our experiments, the use of the relations do not improve nor worsen the quality of the annotation. Our best VCDT run is the 4th ones under 53 submitted runs (3rd team under 11 teams). Second, in the Photo Retrieval Task (ImageCLEFphoto), we use the FFDTs learn in VCDT task and WordNet to improve image retrieval. We analyse the influence of extracted visual concept models to the diversity and precision. This study shows that there is a clear improvement, in terms of precision or cluster recall at 20, when using the visual concepts explicitly appearing in the query.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this working notes, we comment the runs that were submitted, by the UPMC/LIP6, to the Visual Concept Detection Task (VCDT) and for the Photo Retrieval task (ImageCLEFphoto) of ImageCLEF 2008. For this challenge, we focus our efforts on the study of how to automatically extract and exploit visual concepts.</p><p>In the next section, we present our methods and results in VCDT task. In section 3, we describe the techniques we use in the ImageCLEFphoto task, especially how we use the VCDT concepts in this task and our diversification method. Finally, in the last section, we conclude. Automatic image annotation is a typical inductive machine learning approach. It has as starting point a set of correctly labeled examples used to train or build a model. In a second stage, the model is used to perform an automatic classification of any forthcoming examples, even if they have not been already met before. Inductive machine learning is a well-known research topic, with a large set of methods, one of the most common being the decision tree approach (DT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Forests of Fuzzy Decision Trees (FFDT)</head><p>One limitation when considering classical DTs is its robustness and threshold problems when dealing with numerical or imprecisely defined data. The introduction of fuzzy set theory, leading to the fuzzy decision tree approach (FDT), enables us to smooth out these negative effects. In general, inductive learning consists on raising from the particular to the general. A tree is built, from the root to the leaves, by successive partitioning the training set into subsets. Each partition is done by means of a test on an attribute and leads to the definition of a node of the tree (for more details, see <ref type="bibr" coords="2,167.75,406.93,10.23,9.96" target="#b2">[3]</ref>). When addressing unbalanced and large (in terms of dimension and size) data sets, it has been shown in <ref type="bibr" coords="2,225.13,418.93,10.45,9.96" target="#b3">[4]</ref> that it is interesting to combine several DTs, obtaining a Forest of Decision Trees. Moreover, when combining the results provided by several DTs the overall score becomes a degree of confidence in the classification. Note that is not the case for the scores provided by a single decision tree.</p><p>In the learning step, a forest of FDTs (FFDT) was constructed for each concept X of the VCDT challenge. A FFDT is composed of n FDTs. Each FDT F i of the forest is constructed based on a training set T i , each being a balanced random sample of the whole training set.</p><p>In the classification step, each image I is classified by means of each FDT F i . We obtain a degree d i (I, X) ∈ [0, 1] for the image to be a representation of the concept X. Thus, for each I, n degrees d i (I, X), i = 1 . . . n are obtained from the forest. Then all these degrees are aggregated by a simple vote, which mathematically corresponds to the sum of all the degrees: d(I, X) = n i=1 d i (I). Finally, to decide if an image presents a concept or not, we can use a threshold value t (with 0 ≤ t ≤ n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cooccurrences analysis</head><p>DTs learn each concept independently, but concepts can be related. For instance, when a scene can not be simultaneously indoor and outdoor, or if we observe that is overcast, it implies that we have the concept sky. Here, we propose to use cooccurrence analysis to automatically find these relations. Once we have discovered the relation, we need a rule to resolve the conflicting annotations. In fact, each concept is annotated by a FFDT, with a certain confidence degree. For instance, for each image, we will have a degree of having the concept outdoor and a certain degree of having indoor. We know that both can not appear simultaneously, something has to be done.</p><p>We propose to use simple rules. In this paper, we study two type of relations between concepts: exclusion and implication.</p><p>Exclusion discovery and rule To discover the exclusions, we need to look at what concept never appear together. For this, we calculate a cooccurrence matrix COOC. Since there may be some noise (e.g. annotation mistakes), we use a threshold α to decide which pair of concepts never appear together. Once we know which concepts are related, we apply a resolution rule to the scores provided by the FFDT. We choose the rule, that for mutually excluding concepts, eliminates (i.e. gives a confidence of zero) to the label having the lowest confidence. For instance, if we have outdoor with a degree of confidence of 42/50 and indoor with a degree of 20/50 then we will say that it is certainly not indoor and its degree should equal 0. This gives us the following algorithm:</p><p>• let COOC be the concept cooccurrence matrix</p><p>• for each test image I:</p><p>let d(I,X) be the FFDT degree of I for concept X for each couple of concepts (A,B) where</p><formula xml:id="formula_0" coords="3,155.40,244.69,289.11,25.92">COOC(A, B) ≤ α (discovery) if d(I,A) &gt; d(I,B) then d(I,A)=0 else d(I,B)=0 (resolution rule)</formula><p>Implication discovery and rule To discover implications, we need to look, by definition of the implication, at the cooccurrence of the absence of concepts and of the presence of concepts. The resulting cooccurrence matrix COOCNEG is non symmetric, which reflects the fact that one concept may imply another one, but the reciprocal may not be true. The resolution rule says that if a concept implies another one, the confidence degree of the latter should be at least equal to the former. Since there may be some noise, we use a threshold β to decide which concepts imply other ones. We obtain the following algorithm:</p><p>• let COOCNEG be the concept cooccurrence asymmetric matrix between a concept and the negation of an other concept</p><p>• for each test image I:</p><p>let d(I,X) be the FFDT degree of I for concept X </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual Descriptors</head><p>The visual descriptors used in this paper are exclusively color based. In order to obtain spatialrelated information, the images were segmented into 9 overlapping regions (see figure <ref type="figure" coords="3,460.45,514.09,3.88,9.96" target="#fig_0">1</ref>). For each region, we compute a color histogram in the HSV space. The number of bins of the histogram (i.e. numbers of colors) reflects the importance of the region by being valued. The large central region (the image without borders) represents the purpose of the picture. Two other regions, top and bottom, correspond to a spatial focus of these areas. We believe that they are particularly interesting for general concepts (i.e. not objects), as for instance: sky, sunny, vegetation, etc. The remaining regions (left and right top, left and right middle, left and right bottom) are described in terms of color difference between the right and the left. The idea is to explicit any systematic symmetries. In fact, objects can appear on either side. Moreover, decision trees are not able to automatically discover this type of relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">VCDT Experiments and Results</head><p>The VCDT corpus contains 1827 train images and 1000 test images. Exclusive and implication relations A preliminary step before building our runs is to study cooccurrence values to discover exclusions and implications.</p><p>For the 17 concepts, there are 136 cooccurrences values. Those values vary from 0 to 1443 (there are 1827 train images). We set α = 5 (two concepts are considered exclusive if at the maximum 5 of the 1827 training images were annotated as presenting the two concepts in the training sets). For the same reason, we set β = 5 (a concept implies an other concept if at the maximum 5 training images are not annoted by the first concept, but annoted by the second one).</p><p>Our system automatically discovered 25 exclusive relations (table <ref type="table" coords="4,395.76,307.09,4.46,9.96" target="#tab_0">1</ref>) and 12 implication relations (table <ref type="table" coords="4,143.89,319.09,3.88,9.96" target="#tab_1">2</ref>). We found not only most of the relations suggested in the schema describing the training data, but also several other ones. For the latter, some are logic and some are the result of the fact that some labels are not very frequent. On table 1, the concepts of the first column are mutually exclusive with each of the concepts of the second column (taken individually). We notice, for instance, that sunny and night never appear together, but also that there is never a beach and a road together. On table 2, each concept implies the concept in the second column. We found for instance that tree implies vegetation, but less trivially that water implies outdoor.</p><p>Description of runs In order to understand the effects of the cooccurrence analysis in a concept annotation task, we submitted the following six VCDT runs: runA B50trees100pc: degrees of confidence as direct results of the FFDT of each concept.</p><p>runB B50trees100pc T25: same as runA, but with a class decision based on a threshold t of 25 (the FFDTs' degrees varying from 0 to 50).</p><p>runC B50trees100pc COOC5: same as runA, filtered by the exclusion resolution rules.</p><p>runD B50trees100COOC5T25: same as runC, but with a decision threshold of t = 25.</p><p>runE B50trees100C5N5: same as runA, filtered by the exclusion and implication resolution rules.</p><p>runF B50trees100C5N5T25: same as runE, but with a decision threshold of t = 25.</p><p>Besides the submitted runs, for the completeness of this study, we calculated the different error rates for: runw same as runA, filtered by the inclusion resolution rules. runx same as runw, but with a decision threshold of t = 25. runy random degrees. runz same as runy, but with a decision threshold of t = 25.</p><p>In order to appreciate the effect of the implication and exclusion rules, we look at the results of the submitted runs. and the area under the curve in the ROC space: AUC). Based on these scores, the exclusion and implication rules seem to worsen the results provided by the FFDTs. We believe that this is due to the fact that these scores are not adapted to boolean classification (and our rules provide boolean decisions). The area under the curve and the equal error rate are interesting when the classification is accompanied by a degree of confidence. Moreover, this measure penalize boolean decision over degrees.</p><p>Thus, in order to analyze the real effect of the rules (on a decision framework), we propose to use an adapted measure, similar to the EER, the Normalized Score (NS). This score, used in <ref type="bibr" coords="5,499.69,375.73,10.00,9.96" target="#b0">[1]</ref>, corresponds to: sensitivity+specificity-1. Figure <ref type="figure" coords="5,299.90,387.61,4.98,9.96">2</ref> compares the NS varying t. In the case of simple classification (FFDT), the best threshold value is t = 25. It corresponds of a full vote of half of the decision trees. If we compare the NS for best threshold, then we observe that rules (exclusion, implication or both) do not improve nor worse the results. We think that this disappointing result may come from the condition of our resolution rules. In fact, if the FFDTs provide high scores for two concepts, it may not be a good idea to choose either one, because actually we do not really no -since there is a strong contradiction. Clearly a further study is needed.</p><p>If we tend to annotate the images easily (using a low threshold), then the use of the exclusion may be interesting to clean up and thus improve the results. An explanation is exclusion rules make less error when one of the two degrees d(I, A) and d(I, B) is very low. If we are rather strict in our decision to label an image with a concept (i.e. we have a high threshold t), then using the implication will improve the results. An explanation is implication rules work well when the two degrees d(I, A) and d(I, B) are both high. Overall, the combined use of inclusion and exclusion gives the best results for any threshold. Unfortunately, it does not outperform (just equals) the best results (for t ≤ 25).</p><p>3 Photo retrieval task (ImageCLEFphoto)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text retrieval using TD-IDF and Language Model</head><p>In ImageCLEFphoto, we use standard TF-IDF model and a language model (LM) as a base line for text analysis. The idea of language model is to estimate the probability of generation of document D for a given query Q, i.e P (D|Q). We suppose that the distribution of documents in corpus is uniform and also that words into a document are independent, then we have: P (D|Q) = qi P (q i |D) which declares a unigram model. The fact that each word of a query belongs or does not belong to a document could be used to rewrite the above probability as a multiplication of P s is the model for the words appear in a document and P u is the model for the words don't appear in a document. P u (q i |D) could be estimated by α d × P (q i |C) in which α d is constant factor and C is the corpus of documents. Thereafter, we can rewrite the formula of P (Q|D) as follows:</p><formula xml:id="formula_1" coords="6,158.76,489.85,285.49,27.93">log P (Q|D) = qi∈D log P s (q i |D) N (q i , d) + |q i | × log α d + i log P (q i |C).</formula><p>N (q i , d) has a normalization role as IDF in TF-IDF method. Simply, we can consider it as β d ×P (q i |C) in which β d is the normalization factor. To calculate P s (q i |D), we can use a smoothing method. In our approach, we used Jelinek-Mercer method <ref type="bibr" coords="6,346.82,548.29,10.00,9.96" target="#b6">[7]</ref>:</p><formula xml:id="formula_2" coords="6,187.32,568.81,228.49,10.65">P s (q i |D) = (1 -λ)P ml (q i |D) + λP (q i |C) , λ ∈ [0, 1]</formula><p>in which P ml is the maximum likelihood value. In order to make a decision, we put a threshold t to determine if an image contains the given concept according to the corresponding FFDT. First, if the name of a concept appears in the &lt;title&gt; element (VCDT filtering), we propose to filter the rank images list according to the FFDT of this concept. Second, if the name of a concept appears in the &lt;title&gt; element or in the list of synonyms (according to WordNet <ref type="bibr" coords="7,285.02,443.53,10.83,9.96" target="#b1">[2]</ref>) of the words in the &lt;title&gt; element (VCDTWN filtering), we also propose to filter the rank images list according to the FFDT of this concept. For example, the &lt;title&gt; of topic 5 is "animal swimming". Using only VCDT filtering, the system automatically determine that it must use the FFDT of the concept animal. If, in addition, we use WordNet (VCDTWN filtering), the system automatically determine that it must use the FFDT of the concept animal and of the concept water (because according to WordNet, the synonym of "swimming" is: "water sport, aquatics").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Using VCDT concepts in ImageCLEFphoto</head><p>For each query, we obtain a list of images ranked by their text relevance according to LM or TF-IDF text models. Then, using the decision of the FFDTs, we rerank the first 50 ranked images: the system browses the retrieves images from rank 1 to rank 50. If the degree of an image is lower than the threshold t, then this image is reranked at the end of the current 50 images list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Promote Diversity by fastly clustering visual space</head><p>For a given query similar documents are naturally closely ranked. When a user makes a query, he should want that the first relevant documents are as diverse as possible. So the ImageCLEFphoto 2008 task is very interesting to improve image retrieval, but the definition of diversity in the ImageCLEFphoto 2008 task is not very clear, in particular in term of granularity. In most cases, it is strongly related to the text.</p><p>For us, there are two kinds of diversification in the ImageCLEFphoto 2008. The first one is knowledge based: city, state, country, venue, landmark.... For this kind of diversification, the use of an ontology (one for country, an other for city...) seems to be a good idea <ref type="bibr" coords="7,449.40,693.13,10.00,9.96" target="#b4">[5]</ref> . Some clusters correspond to both categories: animal, vehicle type, sport.... For example, for the cluster animal, it is possible to distinguish animal in function of the type of animal in the text and also in function of some visual characteristics (like coat, scale, feathers...). As in real applications, it is not obvious to determine automatically which kind of diversification applying for a given query <ref type="bibr" coords="8,307.27,321.85,9.91,9.96" target="#b5">[6]</ref>, we choose to apply, for all query (even if it is suboptimal), the same kind of diversification (the visual one) by clustering the visual space.</p><p>Visual clustering has been studied for a long time now. Two approaches are generally proposed: data clustering and space clustering. The first approach requires lots of calculation time and should be adapted to distribution of the first images ranked by a given query. The second approach, since it is done independently of the data, is often less efficient, but can be applied extremely fast. We choose to cluster the visual space based on the hue dimension of the HSV space. For each image, we binarize its associated 8 bin hue histogram. Each binary vector correspond to a cluster. The number of clusters is 256 (not all are instantiated), a reasonable number for a re-ranking at P20.</p><p>We use the visual space clusters to rerank the 50 retrieve images. For each query, the system browses the retrieves images from rank 1 to rank 50. If an image has the same visual space cluster as an image of highest rank, then this image is reranked at the end of the current 50 images list. In this way, if in the 50 first images, there are n differrent visual space clusters, then at the end of the rerank process, the first n images correspond to strictly different visual space clusters. We call this diversification method: DIVVISU.</p><p>In order to have a point of comparison, we also propose to randomly permute the first 40 retrieve images. We call this naive method of diversification: DIVALEA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">ImageCLEFphoto Experiments and Results</head><p>The ImageCLEFphoto2008 corpus contains 20k images and 39 topics. Each image is associated with an alphanumeric caption stored in a semi-structured format. These captions include the title of the image, its creation date, the location at which the photograph was taken, the name of the photographer, a semantic description of the contents of the image (as determined by the photographer) and additional notes. In the text retrieval, we use all this elements.</p><p>From topics to queries ImageCLEFphoto topics contain different elements: &lt;title&gt;: the title of the topics, &lt;cluster&gt;: defines how the clustering of images should take place, and &lt;narr&gt;: a narrative description of the topics. Table <ref type="table" coords="8,280.11,643.09,4.98,9.96" target="#tab_3">4</ref> compares different strategies for the construction of text queries in function of the type of text description model (LM and TF-IDF). The results are evaluated using the ImageCLEFphoto 2007 ground-truth. For &lt;title&gt;, &lt;cluster&gt; or &lt;narr&gt;, we use the text of the corresponding element. For &lt;narr&gt;-not, we do not use the sentences of &lt;narr&gt; which contain the word "not". When considering just the &lt;title&gt;, the language model and TF-  IDF perform similarly. But, in general, using something more than just the &lt;title&gt; improves the quality (i.e. precision at 20) of results. We observe that TF-IDF has a stronger improvement, first when adding the narrative, and then when filtering the narrative (&lt;narr&gt;-not). According to P20 score, the best combination of elements for query construction in ImageCLEFphoto 2007 (not 2008) is &lt;title&gt;+&lt;narr&gt;-not. However, as we did not look at this scores before we submitted our runs, all the runs we submitted to the 2008 edition are based on the &lt;title&gt;+&lt;cluster&gt;+&lt;narr&gt;not combination. When we compare the results of run1 and run10 in 2007 and in 2008 (table <ref type="table" coords="9,501.39,355.57,3.88,9.96" target="#tab_4">5</ref>), we notice that the P20 results are not the same for run1 (0.190 in 2007, 0.185 in 2008), but are similar for run10. The 2007 ground truth and the 2008 one may be a little bit different.</p><p>The submitted runs We submitted 18 runs: 9 based on LM (run 1 to 9, noted Q3 in the name of runs) and 9 based on TF-IDF (run 10 to 18, noted r3tfidf in the name of runs). All use the content of &lt;title&gt;+&lt;cluster&gt;+&lt;narr&gt;-not to construct the query. The results are given in two tables and one figure: table <ref type="table" coords="9,228.02,427.33,4.98,9.96" target="#tab_4">5</ref> gives the methods used for each submitted run, table 6 compares VCDT and VCDTWN filtering, and figure <ref type="figure" coords="9,279.16,439.21,4.98,9.96" target="#fig_5">4</ref> compares DIVALEA and DIVVISU diversification.</p><p>VCDT and VCDTWN filtering To determine if an image should or not contains a visual concept, we choose to set the threshold t to the median of all the degrees values for a given concept (this value varies from 7.3 (overcast ) to 28.8 (outdoor )). We do not use cooccurrence analysis (neither exclusion nor implication rules) in the ImageCLEFphoto task because it was not conclusive in the VCDT task.</p><p>Table <ref type="table" coords="9,131.88,510.97,4.98,9.96" target="#tab_5">6</ref> shows that, for all topics, VCDT filtering improves P20 by 8% and VCDTWN filtering improves P20 by 4% in comparison to TF-IDF P20. Since our method depends on the presence of a concept in the text query, it does not apply to every topic. Using VCDT filtering, only 11 topics where filtered. Using VCDTWN filtering, 25 topics where modified. For the other topics, result images from text retrieval keep the same ranked. Thus, we separate the study into three groups: all the topics, the 11 topics modified by VCDT filtering and the 25 topics for which we applied VCDTWN filtering. On table 6, we observe an improvement on TF-IDF scores of +44% for P20 and +30% for the 11 topics modified by VCDT filtering, but not by VCDTWN filtering (+8% for P20 and -4% for CR20). Figure <ref type="figure" coords="9,277.35,606.61,4.98,9.96" target="#fig_3">3</ref> shows the P20 score for each topic. We notice that, by VCDT filtering, quite all the modified topics are improved, but by VCDTWN filtering, some topics are improved and others are worsened. Then, we conclude that the way we use WordNet is not adapted for this task. Further study is needed.</p><p>Diversification Figure <ref type="figure" coords="9,212.77,654.49,4.98,9.96" target="#fig_5">4</ref> compares diversification method scores. DIVALEA and DIVVISU give lower P20 than no diversification, but DIVVISU slightly improves CR20 (in average +2%). So our DIVVISU diversification method works slightly well for diversification, but lowers precision.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this working note, we focus our efforts on the study of how to automatically extract and exploit visual concepts. First, in VCDT task, we look at the mutual exclusion and implication relations between the concepts, in order to improve the automatic labelling. Our best VCDT run is the 4th ones under 53 submitted runs (3rd team under 11 teams). In our experiments, the use of the relations do not improve nor worsen the quality of the labeling. Second, in ImageCLEFphoto task, we analyse the influence of extracted visual concepts models to the diversity and precision, in a text retrieval context. This study shows that there is a clear improvement, in terms of precision or cluster recall at 20, when using the visual concepts explicitly appearing in the query. In our future researches, we will focus on how using image query to improve image retrieval using concept.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,118.32,186.73,366.34,9.96;2,188.28,59.16,226.00,113.03"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Image segmentation for the extraction of visual descriptors in VCDT task</figDesc><graphic coords="2,188.28,59.16,226.00,113.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,126.12,439.93,343.00,9.96;3,155.40,455.89,191.80,9.96"><head>-</head><label></label><figDesc>for each couple of concepts (A,B) where COOCN EG(A, B) ≤ β (discovery) d(I,B)=max(d(I,A),d(I,B)) (resolution rule)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,105.55,185.26,2.91,4.85;9,101.18,161.96,7.29,4.85;9,101.18,138.66,7.29,4.85;9,101.18,115.36,7.29,4.85;9,101.18,92.06,7.29,4.85;9,105.55,68.76,2.91,4.85;9,118.87,195.16,2.91,4.85;9,151.34,195.16,7.29,4.85;9,186.00,195.16,7.29,4.85;9,220.66,195.16,7.29,4.85;9,255.32,195.16,7.29,4.85;9,292.16,195.16,2.91,4.85;9,91.38,145.91,5.39,10.36;9,91.38,125.85,5.39,18.44;9,91.38,108.37,5.39,15.86;9,186.69,200.86,30.42,5.39;9,142.34,62.22,119.13,5.39;9,121.05,185.26,2.91,4.85;9,121.05,179.43,2.91,4.85;9,164.36,161.96,2.91,4.85;9,129.70,179.43,5.83,4.85;9,199.02,132.83,5.83,4.85;9,138.38,173.61,5.83,4.85;9,121.05,185.26,5.83,4.85;9,121.05,179.43,5.83,4.85;9,138.38,173.61,5.83,4.85;9,225.03,115.36,5.83,4.85;9,199.02,132.83,5.83,4.85;9,147.03,167.78,5.83,4.85;9,138.38,173.61,5.83,4.85;9,121.05,185.26,5.83,4.85;9,147.03,167.78,5.83,4.85;9,121.05,185.26,5.83,4.85;9,233.68,109.53,5.83,4.85;9,225.03,115.36,5.83,4.85;9,121.05,185.26,5.83,4.85;9,268.34,86.23,5.83,4.85;9,199.02,132.83,5.83,4.85;9,155.71,161.96,5.83,4.85;9,138.38,173.61,5.83,4.85;9,129.70,179.43,5.83,4.85;9,147.03,167.78,5.83,4.85;9,121.05,156.13,5.83,4.85;9,129.70,179.43,5.83,4.85;9,225.03,103.71,5.83,4.85;9,147.03,156.13,5.83,4.85;9,155.71,161.96,5.83,4.85;9,173.04,150.31,5.83,4.85;9,225.03,115.36,5.83,4.85;9,277.02,80.41,5.83,4.85;9,181.69,144.48,5.83,4.85;9,199.02,103.71,5.83,4.85;9,129.70,179.43,5.83,4.85;9,225.03,115.36,5.83,4.85;9,164.36,153.89,2.91,4.85;9,98.64,211.62,192.59,7.97;9,318.55,185.26,2.91,4.85;9,314.18,161.96,7.29,4.85;9,314.18,138.66,7.29,4.85;9,314.18,115.36,7.29,4.85;9,314.18,92.06,7.29,4.85;9,318.55,68.76,2.91,4.85;9,331.87,195.16,2.91,4.85;9,364.34,195.16,7.29,4.85;9,399.00,195.16,7.29,4.85;9,433.66,195.16,7.29,4.85;9,468.32,195.16,7.29,4.85;9,505.16,195.16,2.91,4.85;9,304.38,150.76,5.39,10.36;9,304.38,130.70,5.39,18.44;9,304.38,103.52,5.39,25.56;9,399.69,200.86,30.42,5.39;9,350.49,62.22,128.84,5.39;9,334.05,185.26,2.91,4.85;9,377.36,127.01,2.91,4.85;9,342.70,161.96,2.91,4.85;9,342.70,179.43,5.83,4.85;9,412.02,161.96,5.83,4.85;9,351.38,179.43,5.83,4.85;9,334.05,173.61,5.83,4.85;9,334.05,167.78,5.83,4.85;9,351.38,173.61,5.83,4.85;9,438.03,115.36,5.83,4.85;9,412.02,144.48,5.83,4.85;9,360.03,173.61,5.83,4.85;9,351.38,173.61,5.83,4.85;9,334.05,185.26,5.83,4.85;9,360.03,138.66,5.83,4.85;9,334.05,185.26,5.83,4.85;9,334.05,179.43,5.83,4.85;9,446.68,109.53,5.83,4.85;9,438.03,115.36,5.83,4.85;9,334.05,179.43,5.83,4.85;9,481.34,86.23,5.83,4.85;9,412.02,150.31,5.83,4.85;9,368.71,161.96,5.83,4.85;9,351.38,173.61,5.83,4.85;9,342.70,179.43,5.83,4.85;9,360.03,173.61,5.83,4.85;9,334.05,156.13,5.83,4.85;9,342.70,179.43,5.83,4.85;9,438.03,115.36,5.83,4.85;9,360.03,179.43,5.83,4.85;9,368.71,161.96,5.83,4.85;9,386.04,179.43,5.83,4.85;9,438.03,115.36,5.83,4.85;9,490.02,80.41,5.83,4.85;9,394.69,144.48,5.83,4.85;9,412.02,103.71,5.83,4.85;9,342.70,185.26,5.83,4.85;9,438.03,121.18,5.83,4.85;9,377.36,154.94,2.91,4.85;9,303.96,211.62,208.07,7.97"><head></head><label></label><figDesc>TF-IDF versus TD-IDF+VCDTWN cluster filtering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,90.00,240.01,423.06,9.96;9,90.00,252.01,254.37,9.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Details of P20 scores of each topic for TF-IDF and TF-IDF+concept filtering. The point labeled 0 corresponds to the average score of the 39 topics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,184.76,144.99,3.06,5.10;10,177.10,131.73,10.72,5.10;10,180.16,118.46,7.66,5.10;10,177.10,105.20,10.72,5.10;10,180.16,91.93,7.66,5.10;10,177.10,78.67,10.72,5.10;10,225.04,150.50,3.06,5.10;10,311.67,150.50,3.06,5.10;10,398.31,150.50,3.06,5.10;10,160.29,102.89,8.74,16.81;10,194.92,65.09,173.93,5.10;10,384.18,65.09,45.80,5.10;10,184.76,244.18,3.06,5.10;10,177.10,233.00,10.72,5.10;10,180.16,221.78,7.66,5.10;10,177.10,210.61,10.72,5.10;10,180.16,199.39,7.66,5.10;10,177.10,188.21,10.72,5.10;10,180.16,177.03,7.66,5.10;10,225.04,249.69,3.06,5.10;10,311.67,249.69,3.06,5.10;10,398.31,249.69,3.06,5.10;10,160.29,198.41,8.74,24.15;10,191.41,164.28,176.68,5.10;10,383.41,164.28,50.08,5.10"><head>1 .</head><label>1</label><figDesc>NODIV 2.DIVALEA 3.DIVVISU NO / VCDT / VCDTWN Average CR20=0.35</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,90.00,271.69,423.29,9.96;10,90.00,283.69,423.28,9.96;10,90.00,295.69,422.93,9.96;10,90.00,307.57,119.81,9.96"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of diversification methods 1. no diversification, 2. random diversification (DIVALEA) 3. diversification by visual space clustering (DIVVISU). For each diversification method, scores for TF-IDF only (1st bar), TF-IDF+VCDT (2nd bar) and TF-IDF+VCDTWN filtering (3rd bar) are given</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,667.93,423.27,33.96"><head>Table 1 :</head><label>1</label><figDesc>Automatically discovered exclusive relations. Concepts of the first column are mutually exclusive with each of the concepts of the second column between 68 to 1607 train images by concept). All the forests are composed of 50 trees. This task corresponds to a multi-class multi-label image classification.</figDesc><table coords="3,399.36,667.93,113.66,9.96"><row><cell>There are 17 concepts. A</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,194.42,689.65,318.73,9.96"><head>Table 2 :</head><label>2</label><figDesc>Table 3  gives the scores used in VCDT task (i.e. equal error rate: EER water, roadorpathway, tree, mountains, beach, sunny, partlycloudy, overcast Automatically discovered implication relations. Each of the concepts in the first column implies the concept in the second</figDesc><table coords="5,456.12,60.13,34.16,9.96"><row><cell>outdoor</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,90.00,248.29,423.13,21.84"><head>Table 3 :</head><label>3</label><figDesc>Results of VCDT task (EER: Equal Error Rate -AUC: Area under ROC curve). Runw, runx, runy and runz were not submitted, but were calculated for seek of completeness</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,90.00,63.83,423.10,375.60"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table coords="6,106.68,63.83,389.93,274.46"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">NormalizedScore Curves</cell></row><row><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FFDT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FFDT+Exclusion</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FFDT+Implication</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FFDT+Implication+Exclusion</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell>Random</cell></row><row><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>NormalizedScore</cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Threshold t</cell><cell></cell></row><row><cell></cell><cell cols="6">Figure 2: Normalized score for each run (averaged over all concepts)</cell></row><row><cell></cell><cell cols="3">Query construction using:</cell><cell></cell><cell></cell><cell>LM</cell><cell>TF-IDF</cell></row><row><cell cols="5">&lt;title&gt; &lt;cluster&gt; &lt;narr&gt; &lt;narr&gt;-not</cell><cell>P20</cell><cell>Gain %</cell><cell>P20</cell><cell>Gain %</cell></row><row><cell>X</cell><cell></cell><cell></cell><cell></cell><cell cols="3">0.178 -</cell><cell>0.179</cell><cell>-</cell></row><row><cell>X</cell><cell></cell><cell>X</cell><cell></cell><cell cols="3">0.192 +8</cell><cell>0.224</cell><cell>+25</cell></row><row><cell>X</cell><cell></cell><cell></cell><cell>X</cell><cell cols="3">0.190 +7</cell><cell>0.260</cell><cell>+45</cell></row><row><cell>X</cell><cell>X</cell><cell>X</cell><cell></cell><cell cols="3">0.183 +3</cell><cell>0.221</cell><cell>+23</cell></row><row><cell>X</cell><cell>X</cell><cell></cell><cell>X</cell><cell cols="3">0.190 +7(run1) 0.250 +40(run10)</cell></row></table><note coords="6,128.52,350.77,384.58,9.96;6,90.00,362.77,422.98,9.96;6,90.00,374.77,410.24,9.96;6,90.00,405.85,72.63,9.96;6,211.56,417.85,45.19,9.96;6,259.56,431.97,18.63,6.97;6,280.08,417.85,47.47,10.66;6,329.76,432.45,11.76,6.97;6,336.48,433.66,11.91,4.55;6,350.28,417.85,41.17,10.65"><p>Precision at 20 (P20) in function of the extracted text from topics to build the query. P20 was calculated using the ground truth of ImageCLEFphoto 2007 (not 2008). Only the runs using &lt;title&gt;+&lt;cluster&gt;+&lt;narr&gt;-not were submitted to ImageCLEFphoto 2008 (run1 and run10) two distribution: P (D|Q) = qi∈D P s (q i |D) × qi / ∈D P u (q i |D).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,90.00,635.17,423.19,69.72"><head>Table 5 :</head><label>5</label><figDesc>Previous works show that combining text and visual information improves image retrieval, but most of this work use an early or late fusion of visual and textual modality. Following the idea of VCDT and ImageCLEFphoto tasks, we propose to use VCDT visual concepts to filter Image-CLEFphoto text runs in order to answer if visual concept filtering can improve text only retrieval.The difficulty is to determine how to use the visual concepts of VCDT in ImageCLEFphoto 2008. In the VCDT task, we have obtained a FFDT by concept (see sections 2.1 and 2.4). Each The 18 submitted runs. VCDT filtering: using VCDT visual concepts, VCDTWN</figDesc><table coords="7,108.00,60.13,387.09,255.48"><row><cell></cell><cell></cell><cell cols="2">Filtering</cell><cell cols="2">Diversification</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">by concepts</cell><cell>DIV</cell><cell>DIV</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Text</cell><cell cols="5">VCDT WN ALEA VISU Modality</cell><cell>P20</cell><cell>CR20 MAP</cell></row><row><cell>run1</cell><cell>LM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TXT</cell><cell>0.185</cell><cell>0.247</cell><cell>0.123</cell></row><row><cell>run2</cell><cell>LM</cell><cell>X</cell><cell></cell><cell></cell><cell></cell><cell cols="2">TXTIMG 0.195</cell><cell>0.257</cell><cell>0.124</cell></row><row><cell>run3</cell><cell>LM</cell><cell>X</cell><cell>X</cell><cell></cell><cell></cell><cell cols="2">TXTIMG 0.176</cell><cell>0.248</cell><cell>0.122</cell></row><row><cell>run4</cell><cell>LM</cell><cell></cell><cell></cell><cell>X</cell><cell></cell><cell>TXT</cell><cell>0.165</cell><cell>0.272</cell><cell>0.112</cell></row><row><cell>run5</cell><cell>LM</cell><cell>X</cell><cell></cell><cell>X</cell><cell></cell><cell cols="2">TXTIMG 0.154</cell><cell>0.295</cell><cell>0.111</cell></row><row><cell>run6</cell><cell>LM</cell><cell>X</cell><cell>X</cell><cell>X</cell><cell></cell><cell cols="2">TXTIMG 0.146</cell><cell>0.248</cell><cell>0.114</cell></row><row><cell>run7</cell><cell>LM</cell><cell></cell><cell></cell><cell></cell><cell>X</cell><cell cols="2">TXTIMG 0.145</cell><cell>0.248</cell><cell>0.090</cell></row><row><cell>run8</cell><cell>LM</cell><cell>X</cell><cell></cell><cell></cell><cell>X</cell><cell cols="2">TXTIMG 0.151</cell><cell>0.254</cell><cell>0.091</cell></row><row><cell>run9</cell><cell>LM</cell><cell>X</cell><cell>X</cell><cell></cell><cell>X</cell><cell cols="2">TXTIMG 0.144</cell><cell>0.240</cell><cell>0.090</cell></row><row><cell cols="2">run10 TF-IDF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TXT</cell><cell>0.250</cell><cell>0.300</cell><cell>0.192</cell></row><row><cell cols="2">run11 TF-IDF</cell><cell>X</cell><cell></cell><cell></cell><cell></cell><cell cols="3">TXTIMG 0.269 0.313 0.194</cell></row><row><cell cols="2">run12 TF-IDF</cell><cell>X</cell><cell>X</cell><cell></cell><cell></cell><cell cols="2">TXTIMG 0.260</cell><cell>0.293</cell><cell>0.191</cell></row><row><cell cols="2">run13 TF-IDF</cell><cell></cell><cell></cell><cell>X</cell><cell></cell><cell>TXT</cell><cell>0.227</cell><cell>0.285</cell><cell>0.170</cell></row><row><cell cols="2">run14 TF-IDF</cell><cell>X</cell><cell></cell><cell>X</cell><cell></cell><cell cols="2">TXTIMG 0.232</cell><cell>0.260</cell><cell>0.172</cell></row><row><cell cols="2">run15 TF-IDF</cell><cell>X</cell><cell>X</cell><cell>X</cell><cell></cell><cell cols="2">TXTIMG 0.224</cell><cell>0.254</cell><cell>0.172</cell></row><row><cell cols="2">run16 TF-IDF</cell><cell></cell><cell></cell><cell></cell><cell>X</cell><cell cols="2">TXTIMG 0.204</cell><cell>0.305</cell><cell>0.135</cell></row><row><cell cols="2">run17 TF-IDF</cell><cell>X</cell><cell></cell><cell></cell><cell>X</cell><cell cols="3">TXTIMG 0.214 0.318 0.137</cell></row><row><cell cols="2">run18 TF-IDF</cell><cell>X</cell><cell>X</cell><cell></cell><cell>X</cell><cell cols="2">TXTIMG 0.212</cell><cell>0.299</cell><cell>0.133</cell></row></table><note coords="7,90.00,339.49,423.04,9.96;7,90.00,351.37,414.65,9.96;7,90.00,383.77,423.13,9.96"><p>filtering: using VCDT visual concepts extended using WordNet, DIVALEA: random permutation, DIVVISU: visual space clustering diversification. All runs are fully automatic (EN-EN-AUTO) of these FFDTs can give a degree that the corresponding visual concept appears in a new image.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,459.40,693.13,53.75,9.96"><head>Table 6 :</head><label>6</label><figDesc>Comparison of VCDT and VCDTWN filtering. For VCDT filtering, only 11 topics are modified. For VCDTWN, only 25 topics are modified application it's hard to determine which ontology applies to a given topic. The second one is based on visual information: weather condition, group composition, statue.... For this clusters, visual diversification should improve results</figDesc><table coords="7,459.40,693.13,53.75,9.96"><row><cell>, but in real</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment This work was partially supported by the <rs type="funder">French National Agency of Research</rs> (<rs type="grantNumber">ANR-06-MDCA-002 AVEIR</rs> project).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_yaBs5kk">
					<idno type="grant-number">ANR-06-MDCA-002 AVEIR</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,104.75,533.08,408.19,8.97;10,104.76,544.00,274.60,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,430.06,533.08,82.88,8.97;10,104.76,544.00,30.53,8.97">Matching words and pictures</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,143.16,544.00,153.68,8.96">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1107" to="1135" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.75,558.88,383.92,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,222.12,558.88,169.07,8.96">WordNet -An Electronic Lexical Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Bradford books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.75,573.63,408.32,8.97;10,104.76,584.56,148.59,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,268.18,573.63,225.07,8.97">An adaptable system to construct fuzzy decision trees</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Marsala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bouchon-Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,104.76,584.56,119.77,8.96">Proceedings of the NAFIPS&apos;99</title>
		<meeting>the NAFIPS&apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.75,599.43,408.12,8.97;10,104.76,610.36,302.07,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,239.24,599.43,273.63,8.97;10,104.76,610.36,39.31,8.97">Trecvid 2006: Forests of fuzzy decision trees for high-level feature extraction</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Marsala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Detyniecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,163.56,610.36,215.14,8.96">TREC Video Retrieval Evaluation Online Proceedings</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.75,625.23,408.04,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,180.92,625.23,102.40,8.97">LIG at ImageCLEFphoto</title>
		<author>
			<persName coords=""><forename type="first">Philippe</forename><surname>Mulhem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,322.80,625.23,161.04,8.96">Working Notes of ImageCLEFphoto2008</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.75,640.00,408.23,8.97;10,104.76,650.92,408.28,8.97;10,104.76,661.96,20.80,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,206.36,640.00,306.62,8.97;10,104.76,650.92,147.83,8.97">Web image retrieval on ImagEVAL: Evidences on visualness and textualness concept dependency in fusion model</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tollari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,276.72,650.92,231.50,8.96">ACM Conference on Image and Video Retrieval (CIVR)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.75,676.72,407.94,8.97;10,104.76,687.76,336.76,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,206.85,676.72,305.84,8.97;10,104.76,687.76,31.99,8.97">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,144.48,687.76,207.09,8.96">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
