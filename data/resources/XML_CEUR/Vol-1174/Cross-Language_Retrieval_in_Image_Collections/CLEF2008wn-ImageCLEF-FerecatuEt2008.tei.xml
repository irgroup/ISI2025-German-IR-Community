<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.20,148.92,307.05,15.49;1,184.56,170.88,233.96,15.49;1,161.04,192.72,281.25,15.49">TELECOM ParisTech at ImageClefphoto 2008: Bi-Modal Text and Image Retrieval with Diversity Enhancement</title>
				<funder ref="#_KFTjXA8">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,215.16,226.60,61.26,8.97;1,276.48,224.90,1.19,6.65"><forename type="first">Marin</forename><surname>Ferecatu</surname></persName>
							<email>marin.ferecatu@telecom-paristech.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Institut TELECOM</orgName>
								<orgName type="laboratory" key="lab1">CNRS LTCI</orgName>
								<orgName type="laboratory" key="lab2">UMR</orgName>
								<orgName type="institution">TELECOM ParisTech †</orgName>
								<address>
									<addrLine>5141 46, rue Barrault</addrLine>
									<postCode>75634</postCode>
									<settlement>Paris Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.56,226.60,56.89,8.97;1,385.44,224.90,1.30,6.65"><forename type="first">Hichem</forename><surname>Sahbi</surname></persName>
							<email>hichem.sahbi@telecom-paristech.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Institut TELECOM</orgName>
								<orgName type="laboratory" key="lab1">CNRS LTCI</orgName>
								<orgName type="laboratory" key="lab2">UMR</orgName>
								<orgName type="institution">TELECOM ParisTech †</orgName>
								<address>
									<addrLine>5141 46, rue Barrault</addrLine>
									<postCode>75634</postCode>
									<settlement>Paris Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.20,148.92,307.05,15.49;1,184.56,170.88,233.96,15.49;1,161.04,192.72,281.25,15.49">TELECOM ParisTech at ImageClefphoto 2008: Bi-Modal Text and Image Retrieval with Diversity Enhancement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7E0F131C43665405100F0675B4384DC7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Image retrieval, Reranking, Support Vector Machines, Hybrid Text and Image Search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe the participation of TELECOM ParisTech in the ImageClefphoto 2008 challenge. This edition focuses on promoting diversity in the results produced by the retrieval systems. Given the high level semantic content of the topics, search engines based solely on text or visual descriptors are unlikely to offer satisfactory results. Our system uses several text and visual descriptors, as well as several combination algorithms to improve the overall retrieval performance. The text part includes a collection of manually built boolean queries and a set of textual descriptors extracted automatically using dictionary filtering and dimensionality reduction. Text and visual descriptors are combined using two strategies: adhoc concatenation and re-ranking. Diversity makes it possible to reduce the redundancy in the final results and it is obtained using two techniques, threshold clustering and maxmin exploration. Several runs were submitted to the challenge, including individual (text or visual), combined, and with different settings of diversity. The results show that the combined runs outperform by a significant amount the individual runs. These results clearly corroborate (i) the complementarity of text and visual descriptors and (ii) the effectiveness of boolean queries suggesting promising future research directions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Stimulated by the exponential growth of multimedia contents, the interest in document indexing and retrieval has steadily increased in recent years. Although text based retrieval has been studied for several decades <ref type="bibr" coords="2,123.96,112.00,15.34,8.97" target="#b25">[26]</ref>, many problems remain unsolved <ref type="bibr" coords="2,278.04,112.00,15.24,8.97" target="#b19">[20]</ref>. Recently, text based retrieval emerged successfully in internet search <ref type="bibr" coords="2,151.32,124.00,15.34,8.97" target="#b16">[17]</ref>, and it usually relies on statistical text processing and tags collections. Content based image retrieval also developed rapidly in the last decade, motivated by the growing amount of image and video collections and by the need to organize, share and search those contents effectively and efficiently <ref type="bibr" coords="2,90.00,159.88,15.81,8.97" target="#b27">[28,</ref><ref type="bibr" coords="2,107.76,159.88,7.26,8.97" target="#b3">4]</ref>. Since neither world (text or image indexing and retrieval) offers a satisfactory bridge to solve the infamous semantic gap, more and more search engines employ hybrid text and visual representations in order to describe and search multimedia databases. In this context, the ImageClef challenge offers an excellent benchmark to test and compare several state of the art algorithms proposed by different participants 1 .</p><p>ImageClefphoto 2008 focuses on promoting diversity in the results produced by search engines, i.e., those which reduce the number of redundant elements in their output are preferred. As we shall see in §2, the query topics are very semantic so individual text or visual descriptors are not expected to offer satisfactory results. In this work, we investigate several combination strategies for text and visual descriptors as well as several algorithms for reducing the redundancy in the results returned by the system. In order to describe the textual content we use two representations: (1) a set of manually constructed boolean queries and (2) a set of automatically extracted vector representations based on dictionary filtering and dimensionality reduction. We also use several global image descriptors <ref type="bibr" coords="2,339.00,291.40,15.81,8.97" target="#b18">[19,</ref><ref type="bibr" coords="2,357.72,291.40,7.26,8.97" target="#b3">4]</ref>, which even though less performant than local ones <ref type="bibr" coords="2,152.28,303.28,15.24,8.97" target="#b23">[24]</ref>, have the advantage of being generic and computationally cheap (see §3).</p><p>We compare hybrid combination by concatenation and re-ranking, using both the Query By Example (QBE) paradigm and SVM learning. In order to eliminate the redundancy in the final results we employ two strategies: a modified version of Quality Thresholding algorithm and a maxmin exploration strategy. Our results show that combining the visual search results (even when using a small number of examples, e.g. three in the challenge) with the textual results improves significantly the overall performance. Clearly, the information provided by the two modalities are complementary. Furthermore, the manually prepared boolean queries provide a noticeable and consistent gain suggesting that semantic parsing and automatic extraction of boolean representations is a promising research direction.</p><p>The paper is organized as follows. We start by a short presentation of the ImageClef Photo Retrieval Task ( §2). Then, we describe our visual descriptors and the retrieval results obtained using image descriptors only ( §3). Manual queries are described in §4.1 while in §4.2 we present our automatic text feature extraction from raw data and their combination with the visual descriptors. In §5 we describe our diversification algorithm; we end the paper with a discussion and concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ImageCLEF Photo Retrieval Task</head><p>In this section we briefly describe the ImageClefphoto retrieval challenge, description that we use later to motivate our choices and to discuss various results. This year, ImageClefphoto focuses on promoting diversity in the results produced by search engines, i.e. those which eliminate (near-)duplicate documents are likely to produce a higher percentage of meaningful results. The performance measures used in the challenge are the precision at 20 (P20) defined as</p><formula xml:id="formula_0" coords="2,257.04,580.96,256.04,22.86">P 20 = Relevant(20) 20<label>(1)</label></formula><p>and the cluster-recall at 20 (CR20), or sub-topic recall:</p><formula xml:id="formula_1" coords="2,254.52,630.41,258.56,29.69">CR20 = 20 i=1 S(d i ) N S<label>(2)</label></formula><p>where for a given topic, the document d i is relevant for the set of sub-topics S(d i ) and N S is the total number of sub-topics <ref type="bibr" coords="2,180.00,680.56,15.24,8.97" target="#b30">[31]</ref>. The submitted runs were ranked according to the average of the two above measures.</p><p>The benchmark uses the IAPR-TC12 <ref type="bibr" coords="2,255.96,704.44,16.64,8.97" target="#b11">[12]</ref> data collection, which consists of 20,000 images associated with a set of XML annotations, including text descriptions, location tags, title and date. The challenge proposes 39 query topics, also available in XML format. Each topic is defined by a narrative block indicating the search target, a diversity criterion and a set of three image queries (Fig. <ref type="figure" coords="3,393.84,279.04,4.98,8.97" target="#fig_0">1</ref> shows one topic description and a database entry). A short analysis of these data reveals that the topics combine highly semantic concepts and complex logical structures. For each topic, three images are also supplied in order to enhance understanding and to help formulating visual queries, but they are not intended as a replacement of text descriptions. Indeed, due to the complex semantic definitions of the topics, image queries are unlikely to retrieve all relevant results. Thus, image examples are perhaps best used to enhance the results through their combination with the text description, and this is the approach we adopted for this challenge.</p><p>Different acronyms were used by participants and stand for the following runs and query types: IMG (image), TXT (text) and TXTIMG (combination text/image), MAN (manual) and AUTO (automatic).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Visual Content Description and Search</head><p>In this section we motivate our choice of image descriptors for the ImageClefphoto retrieval use case (see §3.1 and §3.2). We then present, in §3.3, our querying paradigms and we show in §3.4 some performance measurements, including a comparison with other ImageClef runs. We end this section with a brief discussion of the limits of visual queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>Extracting visual features that capture high level semantic content of an image is a difficult task. Indeed, the main challenge for content based retrieval systems is the infamous "semantic gap" that instantiates the discrepancy between the low level features used to represent the visual content and the high level concepts expected by the users <ref type="bibr" coords="3,177.84,549.40,10.60,8.97" target="#b3">[4]</ref>.</p><p>For some domain specific databases and applications, such as browsing fingerprint or face images, there exists enough a priori knowledge of the image content to be able to propose more accurate mathematical models. However, for generic databases, the task becomes much harder since there is no perfect and unique description of the visual content which agrees with the semantic<ref type="foot" coords="3,381.36,595.72,3.49,6.28" target="#foot_0">2</ref> . Therefore, many systems rely on holistic combined image descriptions such as color, texture and shape <ref type="bibr" coords="3,384.24,609.28,10.77,8.97" target="#b3">[4,</ref><ref type="bibr" coords="3,397.68,609.28,12.45,8.97" target="#b17">18,</ref><ref type="bibr" coords="3,412.80,609.28,13.28,8.97" target="#b9">10]</ref> in order to search for target images. This approach, even though ad-hoc, has been successfully applied for unstructured image databases (which lack text descriptions) through an interactive description of concepts using relevance feedback and machine learning techniques <ref type="bibr" coords="3,261.84,645.04,15.24,8.97" target="#b32">[33]</ref>.</p><p>Recent object recognition approaches rely on local descriptors (for example, image regions or interest points) in order to describe more precisely the visual content <ref type="bibr" coords="3,336.12,669.04,15.34,8.97" target="#b23">[24]</ref>. While local descriptors have many desirable properties, such as stability and invariance to common geometric and photometric transformations <ref type="bibr" coords="3,90.00,692.92,15.81,8.97" target="#b22">[23,</ref><ref type="bibr" coords="3,109.20,692.92,11.86,8.97" target="#b31">32]</ref>, they are resource (time and memory) demanding and cannot be easily extended to large-scale search engines. Moreover, although these algorithms can be tuned to perform well for some object cate-gories, they are less adapted to pictures involving deformable objects or context dependent scenes that are difficult to describe using individual rigid objects (for example, emotional states or esthetic impressions.)</p><p>Since ImageClefphoto query topics are very semantic and sometimes expressed as a combination of several concepts, they are not easily translated in terms of low level visual features. The large number of topics and concepts involved in their definition rules out the possibility to use specific visual models for each concept. Instead, we use global image descriptors as described below. While being less performant compared to local descriptions, they are more appropriate for our use case as (1) they have small memory footprint and thus fit into standard PCs without any specific storage requirements; and (2) they are very fast to compute as they involve simple distance measure operations, guaranteeing real time responses. Furthermore, as they do not include any a priori object model, they can be applied to any target category. Indeed, global descriptors have been shown to perform well in this framework, for example with SVMbased machine learning <ref type="bibr" coords="4,186.60,243.52,15.24,8.97" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Global Image Descriptors</head><p>As described in §3.1, we use global image descriptors in order to represent the visual content of images. More precisely, we use a combination of color, texture and shape features, as described below.</p><p>Color histograms: they provide a summary description of the color information but ignore spatial correlations between colors; thus, pixels having the same color distribution may not be similar in the context of their spatial neighborhoods <ref type="bibr" coords="4,201.48,338.56,15.24,8.97" target="#b29">[30]</ref>, <ref type="bibr" coords="4,223.20,338.56,10.60,8.97" target="#b0">[1]</ref>. Alternatively, given an image of size M × N , we weight each color instance by a measure related to its local context:</p><formula xml:id="formula_2" coords="4,209.40,371.30,184.24,30.45">h(c) = 1 M N M-1 x=0 N -1 y=0 w(x, y)δ(f (x, y) -c)</formula><p>where h(c) is frequency of color c and w(x, y) is a pixel-based weighting function. We use w(x, y) = ||∆(x, y)|| 2 , the Laplacian at the pixel (x, y), to emphasize corners and edges in the image and local color frequency to emphasize non-uniform regions.</p><p>Texture features: we use the power spectral density distribution in the complex plane. This has been shown to perform well when combined with color and shape histograms <ref type="bibr" coords="4,391.56,462.04,15.34,8.97" target="#b18">[19]</ref>. Roughly, a high energy spectrum concentrated at low frequencies highlights large scale informations in an image, while high frequencies correspond to textured regions (small scale details). Shape features: in order to describe the shape content of an image we use standard edge orientation histograms. First, edges are extracted from images, then the gradient is computed using only the edge pixels. The orientation of the gradient is quantized w.r. to the angle resulting into a histogram that is sensible to the general flow of lines in the image <ref type="bibr" coords="4,287.88,533.80,15.34,8.97" target="#b13">[14]</ref>. More details on image descriptors can be found in <ref type="bibr" coords="4,90.00,545.80,10.60,8.97" target="#b6">[7]</ref>.</p><p>Visual feature vectors are combined by concatenation and then, in order to reduce resource requirements and to avoid the curse of dimensionality, we apply linear PCA <ref type="bibr" coords="4,352.56,569.68,16.64,8.97" target="#b15">[16]</ref> and keep the 100 largest principal components (which preserves 95% of the energy of the signal.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Querying Paradigms</head><p>For each topic, we use the three query images combined with two different paradigms: (i) similarity search by minimum distance and (ii) SVM filtering. MinDistance Query: let B be the database and let Q = {q 1 , q 2 , q 3 } denote the query, here q 1 , q 2 and q 3 are the three images of the query topic. We extend the "query by example" search paradigm for multiple inputs by introducing a composed measure of dissimilarity between an image x ∈ B and Q:</p><formula xml:id="formula_3" coords="4,249.00,698.34,264.08,14.69">d(x, Q) = min i=1,2,3 d(x, q i ),<label>(3)</label></formula><p>This definition naturally follows from the fact that images in B close to one of the three query images {q i } are more likely to belong to the same topic. Instead of taking the min-value, one can consider a convex combination of distances; nevertheless, if {q i } are very distant in the description space, the resulting dissimilarity measure is uncorrelated with the semantic captured by the query topics. SVM Based Querying: Support Vector Machines (SVM) <ref type="bibr" coords="5,327.00,370.36,16.64,8.97" target="#b26">[27]</ref> have been applied with success to a great wealth of practical problems since their inception in early '90s by Vapnik <ref type="bibr" coords="5,393.84,382.36,15.34,8.97" target="#b28">[29]</ref>. In image retrieval, they provide state of the art results in many recognition tasks <ref type="bibr" coords="5,314.04,394.24,11.72,8.97" target="#b3">[4]</ref> and relevance feedback <ref type="bibr" coords="5,423.12,394.24,15.34,8.97" target="#b32">[33]</ref>. They use a linear combination of kernels as a decision function:</p><formula xml:id="formula_4" coords="5,243.12,424.34,269.96,30.57">f SV M (x) = N i=1 α i K(x, x i ),<label>(4)</label></formula><p>where K(•, •) is a positive definite kernel, {α i } are the signed Lagrange multipliers and {x i : α i = 0} are known as the support vectors (see <ref type="bibr" coords="5,226.80,477.52,15.81,8.97" target="#b26">[27,</ref><ref type="bibr" coords="5,245.04,477.52,13.28,8.97" target="#b28">29]</ref> for details). We trained our SVMs using Q = {q 1 , q 2 , q 3 } as positive examples. One may use one class SVMs for training, but their generalization performances were reported to be sub-optimal (with respect to standard SVMs) for image retrieval <ref type="bibr" coords="5,199.08,513.40,10.89,8.97" target="#b3">[4,</ref><ref type="bibr" coords="5,212.88,513.40,8.24,8.97" target="#b5">6]</ref> mainly when the positive classes contain few examples. In practice, we used, for each topic, standard two class SVMs trained on Q and random subsets of 10 negative examples taken from B. As the size of B is 20, 000, it is unlikely that some relevant images belong to these negative random subsets. While simple, this procedure proved to be effective in practice and produced better results compared to one class SVMs.</p><p>Once each topic SVM learnt<ref type="foot" coords="5,222.48,571.60,3.49,6.28" target="#foot_1">3</ref> , the system ranks the database according to the score given by Eq. 4 and returns the most positive examples. In all these experiments, we use the Laplacian kernel defined as K(x i , x j ) = expγ x ix j . This kernel was advocated in <ref type="bibr" coords="5,346.44,597.04,11.72,8.97" target="#b2">[3]</ref> for histogram-based image description and proved to provide better results than the usual Gaussian kernel for relevance feedback tasks <ref type="bibr" coords="5,493.92,609.04,15.34,8.97" target="#b14">[15]</ref>. Notice that the dominant term in its Taylor expansion corresponds to the triangular kernel, K(x i , x j ) = -γ x ix j , which is proved to be scale invariant with respect to the distribution of the data in the description space <ref type="bibr" coords="5,161.76,644.92,10.60,8.97" target="#b7">[8]</ref>. In practice, we found that a good setting of γ is 1. For consistency and comparison issues, we fix in all our experiments the kernel and its parameters. Of course, our results could be improved by fine tuning the kernel parameters or by exploring other kernels, but this is not in the scope of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Performance</head><p>In Fig. <ref type="figure" coords="5,119.16,715.96,4.98,8.97" target="#fig_1">2</ref> we present the average precision over all topics as a function of the number of results sent by the query engine. At this stage, we draw the following conclusions: 1. As expected, the SVM query procedure described above outperforms all the other paradigms (a gain of almost 10% when compared to QBE MinDist and One-class SVM).</p><p>2. One-class SVM performs about the same as the QBE MinDist. This can be explained by the fact that the number of learning examples is very small (only three) and thus, not enough to capture the complexity of the target topic.</p><p>These runs do not include diversity and were not submitted to the ImageCLEF-PRT (see §5). The two IMG runs we submitted (using only visual features), were ranked 2 nd and 3 rd tailing the 1 st rank run performances on the combined P20/CR20 measure (see §5). In terms of the P 20 measure (Eq 1), our two runs were ranked 4 th and 6 th which proves that the diversification algorithms, although lowering the P 20 measure, improved the overall performance. As an illustration, Fig <ref type="figure" coords="6,385.68,426.04,4.98,8.97" target="#fig_2">3</ref> shows the difference between SVM and direct MinDist similarity retrieval using the "birds flying" topic. As expected, SVM uniformly provides more consistent results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Limits of the Visual Search</head><p>Motivated by a wealth of practical applications, image retrieval by visual content has became a rapidly evolving research field, although breakthrough advances are still rare. State of the art results are far from satisfactory and search by image descriptors alone is unlikely to offer complete satisfaction for most practical task <ref type="bibr" coords="6,122.16,533.08,10.60,8.97" target="#b3">[4]</ref>. This state of progress clearly motivates the use of hybrid descriptions, for example by combining visual features with text and other available media (sound, music, meta-tags, etc.) Meanwhile, recent trends in research suggest that machine learning based search methods with relevance feedback provide excellent results for many tasks. In the following sections we describe our approach for ImageClefphoto challenge by using manually prepared boolean queries ( §4.1) and combined text and image content representations ( §4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hybrid Document Search</head><p>As mentioned in §3, using only visual descriptors is not enough to provide satisfactory results in this challenge. Our goal is to measure the gain when combining text and image features; in §4.1 we describe boolean queries and their combination with visual descriptors while in §4.2 we present our text descriptor based on dictionary filtering and dimensionality reduction. Both approaches are illustrated by examples of actual queries; we also present different results from the challenge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Boolean Queries</head><p>A short analysis of the query topics reveals complex and highly semantic concepts involving several objects and relations. One possible way to represent these topics in a principled way is to use boolean queries. Let us consider the 3 rd topic ("religious statue in the foreground"):</p><p>Relevant images will show a statue of one (or more) religious figures such as gods, angels, prophets etc. from any kind of religion in the foreground. Non-religious statues like war memorials or monuments are not relevant. Images with statues that are not the focus of the image (like the front view of church with many small statues) are not relevant. The statues of Easter Island are not relevant as they do not have any religious background. This can naturally be expressed using boolean operations involving concepts and operations: "(religious AND statue) AND NOT (memorial OR monument OR war) AND NOT (LOCATION 'Easter Island')". Boolean retrieval relies on the use of logical operators where the terms in queries are linked together using an algebra of simple operations (including AND, OR and NOT). Nevertheless, automatic extraction of boolean queries from raw text is known to be a difficult and still unsolved task <ref type="bibr" coords="7,333.60,424.36,15.81,8.97" target="#b10">[11,</ref><ref type="bibr" coords="7,351.48,424.36,11.86,8.97" target="#b19">20]</ref>. Hence, we choose to manually build these expressions from the raw text. This approach which is very popular in Internet search, has emerged as one of the easy to use standards in text retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Query Construction</head><p>We first introduce a small querying language adapted for the ImageClefphoto challenge. We use AND, OR and NOT to connect terms and we use a "LOCATION" specifier to makes it possible to filter documents by their locations (such as "country" or "city" tags). These informations can easily be extracted from the XML document descriptions.</p><p>For some topics, we filter and tag (as "BW": black and white) images depending on their grey level information. This is implemented using the saturation component in the HSV color space. For instance, the query "church AND BW AND NOT LOCATION France" seeks grey level pictures of churches not located in France.</p><p>Queries are created using a web interface. Using the raw text, the user interactively formulates boolean queries for different topics. This procedure is similar to relevance feedback as the user iteratively updates the boolean queries until the results are satisfactory. Fig. <ref type="figure" coords="7,324.72,612.52,4.98,8.97" target="#fig_3">4</ref> shows some results: (left) topic #2 "((church OR cathedral OR mosque) AND (towers) AND (three OR four OR five))", (middle) topic #11 "((LOCATION Russia) AND (BW))" and (right) topic #2 "(lighthouse AND (water OR sea OR ocean))".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Combination with Visual Descriptors</head><p>Boolean queries described earlier return a set of candidate relevant images which are not scored (and hence unranked) so it is not possible, at this stage, to use merging techniques in order to produce combined text/image ranks. Instead, we intersect the results of image and text queries. Notice that topics are very complex and difficult to express exactly using only boolean queries. The latter may produce relevant and (also) irrelevant results which are filtered using visual search <ref type="foot" coords="7,332.04,727.36,3.49,6.28" target="#foot_2">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Experiments</head><p>In our experiments, we extract the raw text, the LOCATION field from XML documents and the "BW" tag from the underlying images. Using these informations and the boolean queries, the search engine returns the results in real time. Notice again that the user is involved only in the construction of boolean queries so all further steps, are fully automatic. Fig. <ref type="figure" coords="8,273.24,166.60,4.98,8.97" target="#fig_4">5</ref> shows the evaluation of different querying schemes (text, image and combined text/image).</p><p>We clearly see that boolean queries outperform visual searches. This is predictable since the former are manually and interactively constructed while visual search is fully automatic. We also see from the results that combining visual and boolean searches boosts the precision by ∼ 20% and even though limited, the information provided by the visual descriptors is always informative. This result is slightly unexpected, because of the small number of positive examples (only three) and suggests that visual descriptors provide a complementary information w.r. to the boolean queries. We noticed that the influence of the visual descriptors decreases as the rank increases: at rank 50, combined text/image performs about the same as text only. This is due to the small number of examples available to compute the visual similarity, e.g. for lower ranks the results are less semantically correlated with the topics. Finally, we conclude that hybrid search always outperforms individual text or image searches.</p><p>The runs we submitted to the ImageClefphoto were similar, but they differ in terms of their diversity algorithms (see. §5) and were ranked 1 st , 2 nd and 3 rd in ImageClefphoto. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Automatic Queries</head><p>In this section, we compare several schemas that combine automatically extracted text descriptors and visual features. The automatic query description and querying is expected to be less performant than the manual one. As described below, we consider in these experiments both early and late descriptor merging techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Text Description</head><p>Text indexing and retrieval is a growing field and many existing state of the art techniques provide reasonably accurate results <ref type="bibr" coords="8,173.64,697.84,15.69,8.97" target="#b25">[26,</ref><ref type="bibr" coords="8,191.76,697.84,12.45,8.97" target="#b21">22,</ref><ref type="bibr" coords="8,206.64,697.84,11.86,8.97" target="#b16">17]</ref>. Nevertheless, most of them rely on the estimation of statistical measures and can only be applied on large datasets. For the IAPR-TC12 database, almost all the terms appear only once, so these statistical techniques are clearly not applicable. In order to extract meaningful information from short descriptions, semantic analysis and natural language parsing are necessary; however, these are known to be difficult and still unsolved problems <ref type="bibr" coords="8,287.52,745.60,15.69,8.97" target="#b20">[21,</ref><ref type="bibr" coords="8,305.76,745.60,11.86,8.97" target="#b19">20]</ref>.</p><p>Our goal is to investigate the performance improvement obtained by combining both textual and visual descriptions, so we adopted a simple vector space model in order to represent the text information. First, we eliminate the stop words, then we parse the list of terms with the Porter stemmer<ref type="foot" coords="9,433.32,134.32,3.49,6.28" target="#foot_3">5</ref>  <ref type="bibr" coords="9,437.16,135.88,15.24,8.97" target="#b24">[25]</ref>. We associate to each resulting term a coordinate in the feature space. As no relationships are considered between terms, we only keep the terms that are used in the definition of the query topics. A further step is considered in order to reduce the dimensionality based on a linear version of PCA. In this step, we only keep the first 100 principal components, corresponding to 98% of the total statistical variance of the text data. We measure dissimilarity between documents using the L1 distance (cosine distance produced similar results). Query topics are described in a similar way but they are pre-process ed in order to eliminate words common to all topics <ref type="foot" coords="9,113.88,218.08,3.49,6.28" target="#foot_4">6</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Combination with Visual Descriptors</head><p>We use two merging strategies: 'Early merging' refers to combining descriptors prior to querying while 'late merging' performs individual text and visual queries prior to combination. Early merging: we create hybrid descriptors by concatenation (cartesian product space) of text and the visual feature spaces. To ensure equal contributions in the final feature vector, we normalize individual features by the mean and the variance computed on the whole dataset. Late merging: for each document, we first run individual queries (text and visual) obtaining two ranking lists. Then each document is assigned a final rank based on the MINRANK scheme defined as</p><formula xml:id="formula_5" coords="9,247.68,358.26,265.40,9.89">r(I) = min(r V (I), r T (I))<label>(5)</label></formula><p>where i ∈ B is an image from the database B and r V (resp. r T ) is its visual (resp. text) rank. The intuition behind this combination strategy comes from the fact that the best rank should be preferred, e.g. low ranked documents are unlikely to be similar to the query topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Experiments</head><p>Fig. <ref type="figure" coords="9,108.48,449.56,4.98,8.97" target="#fig_6">6</ref> shows a comparison of the merging techniques. As we see, textual features extracted automatically outperform significantly the visual ones. We also notice that early combination by concatenation (cartesian product) produces slightly better results than the MINRANK late combination, but the difference is not significant. From these experiments, the best results were obtained by applying the MINRANK algorithm (as described previously) to the hybrid text/image and text only. We obtain ∼ 10% improvement w.r.t. to text retrieval. Nevertheless, this gain is less significant compared to boolean queries which are built manually (see §4.1).</p><p>Finally, this run is ranked 6 th in the ImageClefphoto AUTO TXTIMG and even though no diversification algorithm was used, it ranked 2 nd w.r. to the CR20 measure. This can be explained by the fact that the MINRANK mixes best ranks from both image and text results and since text and visual features are independently extracted, some reduction in the redundancy of the results is expected, i.e. the returned images do not belong to the same sub-categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Diversity and Clustering</head><p>As presented in §2, the measures used to evaluate the runs are precision and cluster-recall on the first 20 results (denoted as P20 and CR20 resp.) According to these measures, results should be both relevant and as diverse as possible. Notice that extra information is also provided for each query topic (specified by the field "&lt;cluster&gt;&lt;/cluster&gt;") about the targeted diversity criterion. In this section, we describe our diversity schemes and their impacts on the P20 and CR20 performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Text Clustering</head><p>Some of the diversity criteria required by the query topics can be directly extracted from the XML data. Each document contains a tag such as "city", "state" and "location" which can be used in order to filter and group documents. For that purpose and in order to keep a good balance between precision and diversity, we start with a larger selection of 40 results and we cluster them following the specified location tags. The output is formed by taking the 1 st element in each cluster, then the 2 nd , etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visual Diversification</head><p>Visual clustering is applied when the diversity criterion cannot be extracted from the XML tags associated to images. We consider two types of "visual diversity" algorithms, described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">MAXMIN Diversity</head><p>The MAXMIN diversity algorithm is based on the maximization of the mininum distance of a given document with respect to the (so far) selected results. Our algorithm starts by choosing the document with the best rank. Afterwards, it chooses the next document as the one which maximizes the distance with respect to the first. More precisely, let S be the candidate set (the initial window of size 40 in our case) and suppose that C ⊂ S is the set of already selected examples. Then, the next example is chosen as:</p><formula xml:id="formula_6" coords="10,232.32,575.22,280.76,14.93">x = argmax x k ∈S\C min xi∈C d(x k , x i )<label>(6)</label></formula><p>This procedure does not produce a clustering, but rather a permutation of the initial selection such as its prefixes are very diversified according to Eq. 6 with respect to the distance defined on the description space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">QT Clustering</head><p>Our second "diversification" method is based on visual clustering. We tested several standard clustering techniques, such as Fuzzy-K-Means <ref type="bibr" coords="10,234.60,667.12,11.60,8.97" target="#b4">[5]</ref> and Competitive Agglomeration <ref type="bibr" coords="10,377.88,667.12,10.60,8.97" target="#b8">[9]</ref>, but we obtained many clusters of large size and highly diverse semantic content. To control the size of the generated clusters, we developed a variant of the Quality Threshold algorithm <ref type="bibr" coords="10,268.80,691.00,15.24,8.97" target="#b12">[13]</ref>, described below. Let s denotes the cluster size defined as s = N/n C , where N , n C are respectively the number of images and the expected (fixed) number of clusters. The algorithm builds the clusters iteratively. First the center of the new cluster is chosen by minimizing the following criterion</p><formula xml:id="formula_7" coords="10,226.44,744.93,286.64,11.41">x t = argmin xi∈St R (KNN s (x i ; S t ))<label>(7)</label></formula><p>where KNN s (x i ; S t ) denotes the set of s nearest neighbors of x i in S t (S 1 = {x 1 , ..., x N }) and R the radius of the smallest sphere enclosing KNN s (x i ; S t ). The new cluster is built as C t = KNN s (x i ; S t ), and then removed from the remaining data, i.e. S t+1 = S t \C t . As for the text clustering, the final output is formed by taking the 1 st element in each cluster, then the 2 nd , etc., until all elements are exhausted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments</head><p>We submitted several runs to ImageClefphoto including the diversity settings described above. More specifically, 26 out of the 39 topics have "cluster tags" related to the location of the pictures; diversity is performed for these topics using text clustering while for the rest it is visual. We observed that applying a diversity schema lowers the P20 measure; however, as the CR20 measure increases, we expect the results to be more meaningful.</p><p>Visual Retrieval allows us to evaluate the impact of visual diversity (using QT or MAXMIN clustering). As already discussed in §3.4, the best "visual retrieval" performances were achieved using the two class SVM (P20=0.248) and without diversity. We submitted two runs to the challenge using QT and MAXMIN clustering and they are ranked 2 nd and 3 rd when using the combined P20 and CR20 score. Their P20 measure is, as expected, lower (0.20 and 0.17 respectively) when compared to the non-diversified results, and this suggests that diversity indeed produced an important CR20 gain.  Combined Text/Visual Search: in Fig. <ref type="figure" coords="11,268.44,504.52,4.98,8.97" target="#fig_8">7</ref> we examine the impact of different diversity algorithms on the P20 and CR20 measures for the combined text and image descriptors (see §4.2.2). First, we observe a loss of precision for both QT and MAXMIN algorithms. Nevertheless, the QT algorithm failed to produce better CR rankings when compared to the case without diversity. For this algorithm, we set the number of clusters to 20 and the initial selection contains the 40 first ranked results. As the size of different clusters is very small (i.e., s = 2), many clusters are similar and this affects the quality of diversity. Moreover, even for a perfect retrieval, there is simply not enough data (less than 100 in average) per class in order to generate consistent clustering. These results show that the space of query results is better explored and summarized using MAXMIN than QT clustering which suffers from the insufficient amount of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Perspectives</head><p>In this paper we presented our experiments investigating the performance of several combination techniques for image and text descriptors, on the ImageClefphoto challenge database. We compared early merging of descriptors by concatenation with late ranking combination obtained by separate queries on text and image features. We also described two schemes for reducing redundancy in the results returned by our search engine.</p><p>In our first conclusion, we found that even with very few images (three in the ImageClefphoto), our system was able to improve the results significantly. Moreover, the improvement is more significant in case of manually prepared boolean queries. This clearly indicates that good quality boolean queries are less likely to return noisy results with respect to the targeted topic. Automatic extraction of boolean queries from raw text is hence identified as a worthy to explore research direction, for instance by using Parts of Speech (POS) tagging and language parsing.</p><p>In our second conclusion, we noticed that using a diversification algorithm helped improving the ranking of our submitted runs. This is more noticeable for queries using only visual descriptors (see §3) where the proposed diversification schemes significantly improved the ranking of our runs (2 nd and 3 rd ). However, because of the limited size of ground truth classes (less than 100 images per topic), it is not possible, at this stage, to draw firm conclusions. Indeed, in a real search engine, where topics might be represented by millions of (possibly similar) images, we expect the obtained clusters to be much more consistent.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,131.04,247.00,341.13,8.97;3,117.36,108.88,215.70,122.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: IAPR-TC12 database entry description (left) and query topic format (right).</figDesc><graphic coords="3,117.36,108.88,215.70,122.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,90.00,304.96,422.94,8.97;5,90.00,316.84,35.73,8.97"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparative performance of two-class SVM, one-class SVM and similarity retrieval with MinDist.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,90.00,258.52,422.98,8.97;6,90.00,270.52,63.33,8.97;6,131.04,108.91,169.20,134.57"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: "Bird flying" search topic: comparative results for similarity retrieval by MinDist (left) and SVM retrieval (right).</figDesc><graphic coords="6,131.04,108.91,169.20,134.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,161.04,226.72,281.01,8.97;7,102.36,108.86,131.10,102.70"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Some examples of boolean retrieval (see the text for details).</figDesc><graphic coords="7,102.36,108.86,131.10,102.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,97.20,547.60,408.81,8.97"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Precision of boolean retrieval: text and image alone versus combined text and visual queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="10,146.40,309.52,310.41,8.97"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Precision results for several text and image combination techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="11,134.28,480.64,334.41,8.97"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Hybrid text and image descriptor: the effect of "diversification" of results.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,104.40,724.28,408.42,7.17;3,90.00,733.76,85.57,7.17"><p>This is due to lack of consensus about the underlying semantics, which usually depends on the context (even humans disagree when interpreting images.)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="5,104.40,746.96,412.78,7.17"><p>In our experiments we used the well known LIBSVM package<ref type="bibr" coords="5,300.72,746.96,8.42,7.17" target="#b1">[2]</ref>, see http://www.csie.ntu.edu.tw/˜cjlin/libsvm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="7,104.40,746.96,408.31,7.17"><p>In practice, visual queries return 1000 documents which are intersected with the results of text and scored using visual distances.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="9,104.40,704.55,196.78,6.26"><p>http://tartarus.org/˜martin/PorterStemmer</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="9,104.40,713.84,260.11,7.17"><p>For example, expressions like "relevant images" are used in all topic descriptions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="12,104.40,727.71,95.98,6.26"><p>http://aveir.lip6.fr</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">French National Research Agency (ANR)</rs> under the <rs type="projectName">AVEIR 7</rs> project, <rs type="grantNumber">ANR-06-MDCA-002</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_KFTjXA8">
					<idno type="grant-number">ANR-06-MDCA-002</idno>
					<orgName type="project" subtype="full">AVEIR 7</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,111.60,331.36,401.38,8.97;12,111.60,343.36,401.41,8.97;12,111.60,355.36,401.73,8.97;12,111.60,367.24,22.65,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,218.28,343.36,215.67,8.97">Ikona: Interactive generic and specific image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Nozha</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marin</forename><surname>Ferecatu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valérie</forename><surname>Franc ¸ois Fleuret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bertrand</forename><forename type="middle">Le</forename><surname>Gouet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hichem</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,453.12,343.36,59.89,8.97;12,111.60,355.36,327.61,8.97">Proceedings of the International workshop on Multimedia Content-Based Indexing and Retrieval</title>
		<meeting>the International workshop on Multimedia Content-Based Indexing and Retrieval</meeting>
		<imprint>
			<publisher>MMCBIR</publisher>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.60,387.16,175.53,8.97;12,309.60,387.16,203.48,8.97;12,111.60,399.16,28.05,8.97;12,160.68,399.16,236.85,8.97;12,418.44,399.16,94.57,8.97;12,111.60,411.16,164.13,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="12,309.60,387.16,203.48,8.97;12,111.60,399.16,24.04,8.97">LIBSVM: a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/˜cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>National Taiwan University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="12,111.60,431.08,401.70,8.97;12,111.60,442.96,344.73,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,330.36,431.08,182.94,8.97;12,111.60,442.96,77.69,8.97">Support-vector machines for histogram-based image classification</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Olivier Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><forename type="middle">N</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,196.56,442.96,155.96,8.97">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1055" to="1064" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.60,462.88,401.56,8.97;12,111.60,474.88,249.45,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,332.40,462.88,180.76,8.97;12,111.60,474.88,56.59,8.97">Image retrieval: Ideas, influences, and trends of the new age</title>
		<author>
			<persName coords=""><forename type="first">Ritendra</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhiraj</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,175.80,474.88,99.12,8.97">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="60" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.60,494.80,401.61,8.97" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="12,320.52,494.80,84.82,8.97">Pattern Classification</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Wiley Interscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.60,514.72,401.53,8.97;12,111.60,526.72,382.89,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,297.36,514.72,215.77,8.97;12,111.60,526.72,137.41,8.97">Semantic interactive image retrieval combining visual and conceptual content description</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ferecatu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Crucianu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,256.32,526.72,134.87,8.97">ACM Multimedia Systems Journal</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="309" to="322" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.60,546.64,401.70,8.97;12,111.60,558.52,398.61,8.97" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,178.56,546.64,334.74,8.97;12,111.60,558.52,43.31,8.97">Image retrieval with active relevance feedback using both visual and keyword-based descriptors</title>
		<author>
			<persName coords=""><forename type="first">Marin</forename><surname>Ferecatu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>INRIA-University of Versailles Saint Quentin-en-Yvelines</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="12,111.60,578.44,401.48,8.97;12,111.60,590.44,401.49,8.97;12,111.60,602.44,57.21,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,264.00,578.44,249.08,8.97;12,111.60,590.44,56.29,8.97">Scale-invariance of support vector machines based on the triangular kernel</title>
		<author>
			<persName coords=""><forename type="first">Fleuret</forename><surname>Franc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hichem</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,187.44,590.44,321.75,8.97">3rd International Workshop on Statistical and Computational Theories of Vision</title>
		<imprint>
			<date type="published" when="2003-10">October 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.60,622.36,401.68,8.97;12,111.60,634.24,402.09,8.97;12,111.60,646.24,22.65,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,278.28,622.36,235.00,8.97;12,111.60,634.24,72.44,8.97">A robust competitive clustering algorithm with applications in computer vision</title>
		<author>
			<persName coords=""><forename type="first">Hichem</forename><surname>Frigui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raghu</forename><surname>Krishnapuram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,190.32,634.24,255.23,8.97">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="450" to="465" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.60,666.16,401.46,8.97;12,111.60,678.16,377.85,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,306.24,666.16,184.55,8.97">Content-based image retrieval: An overview</title>
		<author>
			<persName coords=""><forename type="first">Theo</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,257.64,678.16,145.59,8.97">Emerging Topics in Computer Vision</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</editor>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.60,698.08,401.73,8.97;12,111.60,709.96,22.65,8.97" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,271.32,698.08,197.26,8.97">Information Retrieval: Algorithms and Heuristics</title>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">A</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ophir</forename><surname>Frieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,112.00,401.60,8.97;13,111.60,124.00,401.60,8.97;13,111.60,135.88,401.61,8.97;13,111.60,147.88,22.65,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,340.68,112.00,172.52,8.97;13,111.60,124.00,175.71,8.97">The iapr tc-12 benchmark: A new evaluation resource for visual information systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,318.60,124.00,194.60,8.97;13,111.60,135.88,396.57,8.97">Proceedings of International Workshop OntoImage&apos;2006 Language Resources for Content-Based Image Retrieval, held in conjuction with LREC&apos;06</title>
		<meeting>International Workshop OntoImage&apos;2006 Language Resources for Content-Based Image Retrieval, held in conjuction with LREC&apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,167.80,401.48,8.97;13,111.60,179.80,214.77,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,284.04,167.80,229.04,8.97;13,111.60,179.80,72.08,8.97">Exploring expression data: Identification and analysis of coexpressed genes</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Heyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kruglyak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yooseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,191.40,179.80,30.73,8.97">Genome</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1106" to="1115" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,199.72,401.49,8.97;13,111.60,211.60,181.41,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,227.40,199.72,281.61,8.97">Shape-based retrieval: a case study with trademark image databases</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vailaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,111.60,211.60,78.17,8.97">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1369" to="1390" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,231.52,401.70,8.97;13,111.60,243.52,401.61,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,411.00,231.52,102.30,8.97;13,111.60,243.52,58.47,8.97">Learning in region-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Feng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hong-Jiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,187.44,243.52,296.55,8.97">Proceedings of the IEEE International Symposium on Circuits and Systems</title>
		<meeting>the IEEE International Symposium on Circuits and Systems</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,263.44,269.61,8.97" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<title level="m" coord="13,161.76,263.44,120.85,8.97">Principal Component Analysis</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,283.36,401.62,8.97;13,111.60,295.36,176.61,8.97" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="13,263.40,283.36,249.82,8.97;13,111.60,295.36,35.39,8.97">Google&apos;s PageRank and Beyond: The Science of Search Engine Rankings</title>
		<author>
			<persName coords=""><forename type="first">Amy</forename><forename type="middle">N</forename><surname>Langville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carl</forename><forename type="middle">D</forename><surname>Meyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,315.28,401.60,8.97;13,111.60,327.16,401.60,8.97;13,111.60,339.16,91.29,8.97" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,282.24,315.28,230.96,8.97;13,111.60,327.16,86.02,8.97">Content-based multimedia information retrieval: State-ofthe-art and challenges</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Djeraba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,205.32,327.16,307.88,8.97;13,111.60,339.16,18.28,8.97">ACM Transactions on Multimedia Computing, Communication, and Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,359.08,401.53,8.97;13,111.60,371.08,142.29,8.97" xml:id="b18">
	<monogr>
		<title level="m" coord="13,327.48,359.08,185.65,8.97;13,111.60,371.08,84.31,8.97">Introduction to MPEG-7: Multimedia Content Description Interface</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Salembier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Sikora</surname></persName>
		</editor>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,391.00,401.48,8.97;13,111.60,402.88,171.45,8.97" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prabhakar</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hinrich</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schtze</surname></persName>
		</author>
		<title level="m" coord="13,386.28,391.00,126.80,8.97;13,111.60,402.88,24.86,8.97">Introduction to Information Retrieval</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,422.80,401.48,8.97;13,111.60,434.80,122.73,8.97" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="13,310.92,422.80,202.16,8.97;13,111.60,434.80,27.90,8.97">Foundations of Statistical Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hinrich</forename><surname>Schuetze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,454.72,401.53,8.97;13,111.60,466.72,129.09,8.97" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="13,407.52,454.72,105.61,8.97;13,111.60,466.72,29.79,8.97">Text Information Retrieval Systems</title>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">T</forename><surname>Meadow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename><forename type="middle">R</forename><surname>Boyce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><forename type="middle">H</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carol</forename><forename type="middle">L</forename><surname>Barry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,486.64,401.49,8.97;13,111.60,498.52,335.25,8.97" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="13,296.76,486.64,184.98,8.97">A performance evaluation of local descriptors</title>
		<author>
			<persName coords=""><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,491.40,486.64,21.69,8.97;13,111.60,498.52,227.39,8.97">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,518.56,401.53,8.97;13,111.60,530.44,170.13,8.97" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="13,394.08,518.56,119.05,8.97;13,111.60,530.44,44.19,8.97">Towards category-level object recognition</title>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">4170</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,550.36,353.25,8.97" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="13,162.48,550.36,167.36,8.97">An algorithm for suffix stripping, program</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,337.68,550.36,33.15,8.97">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,570.28,401.61,8.97;13,111.60,582.28,22.65,8.97" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="13,266.64,570.28,181.95,8.97">Introduction to Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,602.20,339.93,8.97" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="13,286.80,602.20,87.63,8.97">Learning with Kernels</title>
		<author>
			<persName coords=""><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,622.12,401.50,8.97;13,111.60,634.12,402.09,8.97;13,111.60,646.00,22.65,8.97" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="13,366.72,622.12,146.38,8.97;13,111.60,634.12,86.28,8.97">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,206.88,634.12,222.83,8.97">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,665.92,289.65,8.97" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="13,168.96,665.92,107.25,8.97">Statistical Learning Theory</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-09">September 1998</date>
			<publisher>John Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,685.84,401.38,8.97;13,111.60,697.84,401.61,8.97" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="13,281.28,685.84,231.70,8.97;13,111.60,697.84,39.34,8.97">Upgrading color distributions for image retrieval: can we do better?</title>
		<author>
			<persName coords=""><forename type="first">Constantin</forename><surname>Vertan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nozha</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,165.24,697.84,275.40,8.97">International Conference on Visual Information Systems (Visual2000)</title>
		<imprint>
			<date type="published" when="2000-11">November 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.60,717.76,401.48,8.97;13,111.60,729.76,401.50,8.97;13,111.60,741.64,276.09,8.97" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="13,272.16,717.76,240.92,8.97;13,111.60,729.76,98.07,8.97">Beyond independent relevance: methods and evaluation metrics for subtopic retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,236.76,729.76,276.34,8.97;13,111.60,741.64,215.91,8.97">Proceedings of the 26th Annual international ACM SIGIR Conference on Research and Development in informaion Retrieval</title>
		<meeting>the 26th Annual international ACM SIGIR Conference on Research and Development in informaion Retrieval</meeting>
		<imprint>
			<date type="published" when="2003-08">August 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,111.60,112.00,401.48,8.97;14,111.60,124.00,401.65,8.97;14,111.60,135.88,169.17,8.97" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="14,421.44,112.00,91.64,8.97;14,111.60,124.00,307.79,8.97">Local features and kernels for classification of texture and object categories: a comprehensive study</title>
		<author>
			<persName coords=""><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcin</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,427.08,124.00,86.17,8.97;14,111.60,135.88,75.99,8.97">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="238" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,111.60,155.92,401.86,8.97;14,111.60,167.80,198.81,8.97" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="14,282.00,155.92,231.46,8.97;14,111.60,167.80,24.56,8.97">Relevance feedback for image retrieval: a comprehensive review</title>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,143.64,167.80,78.16,8.97">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="536" to="544" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
