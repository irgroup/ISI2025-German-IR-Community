<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,75.24,82.13,444.86,12.58;1,144.24,113.33,306.75,12.58">PKU at ImageCLEF 2008: Experiments with Query Extension Techniques for Text-Based and Content-Based Image Retrieval</title>
				<funder ref="#_AzfJSA6">
					<orgName type="full">Chinese NSF</orgName>
				</funder>
				<funder ref="#_YMW2XRC #_VWqGAaD">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_ZSkdgs5">
					<orgName type="full">China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,123.54,137.82,37.51,9.02"><forename type="first">Zhi</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The Institute of Digital Media</orgName>
								<orgName type="department" key="dep2">School of EE &amp; CS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Graduate University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,178.65,137.82,61.58,9.02"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The Institute of Digital Media</orgName>
								<orgName type="department" key="dep2">School of EE &amp; CS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.05,137.82,49.77,9.02"><forename type="first">Yuanning</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The Institute of Digital Media</orgName>
								<orgName type="department" key="dep2">School of EE &amp; CS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Graduate University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.53,137.82,34.99,9.02"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<email>tliu@jdl.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The Institute of Digital Media</orgName>
								<orgName type="department" key="dep2">School of EE &amp; CS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Graduate University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.29,137.82,54.92,9.02"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The Institute of Digital Media</orgName>
								<orgName type="department" key="dep2">School of EE &amp; CS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,431.21,137.82,37.31,9.02"><forename type="first">Wen</forename><surname>Gao</surname></persName>
							<email>wgao@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The Institute of Digital Media</orgName>
								<orgName type="department" key="dep2">School of EE &amp; CS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,75.24,82.13,444.86,12.58;1,144.24,113.33,306.75,12.58">PKU at ImageCLEF 2008: Experiments with Query Extension Techniques for Text-Based and Content-Based Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F005D54014CE15B140BEF379CC984428</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present our solutions for the WikipediaMM task at ImageCLEF 2008. The aim of this task is to investigate effective retrieval approaches in the context of a large-scale and heterogeneous collection of Wikipedia images that are searched by textual queries (and/or sample images and/or concepts) describing a user's information need. We first experimented with a text-based image retrieval approach with query extension, where the expansion terms are automatically selected from a knowledge base that is (semi-)automatically constructed from Wikipedia. We show how this open, constantly evolving encyclopedia can yield inexpensive knowledge structures that are specifically tailored to effectively enhance the semantics of queries. Encouragingly, the experimental results rank in the first place among all submitted runs. The second approach we experimented with is content-based image retrieval (CBIR), in which we first train 1-vs-all classifiers for all query concepts by using the training images obtained by Yahoo! search, and then treat the retrieval task as visual concept detection in the given Wikipedia image set. By comparison, this approach performs better than other submitted CBIR runs.</p><p>Finally, we experimented with a cross-media image retrieval approach by combining and re-ranking text-based and content-based retrieval results. Despite the final experimental results were not formally submitted before the deadline, this approach performs remarkably better than the text-based retrieval or CBIR approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF 2008 contains five different tasks (i.e., photo retrieval, medical retrieval, visual concept detection, medical annotation, and WikipediaMM). In this paper we present our efforts in the WikipediaMM task of ImageCLEF 2008. This is our first year at ImageCLEF and the WikipediaMM task we participated in is also offered for the first time. We participated in all steps of the task, including topic creation, retrieval experiments, and relevance assessment.</p><p>The aim of WikipediaMM 2008 task is to investigate effective retrieval approaches in the context of a large-scale and heterogeneous collection of Wikipedia images that are searched by textual queries (and/or sample images and/or concepts) describing a user's information need. Towards this end, WikipediaMM 2008 task needs to deal with searching 75 topics from approximately 150,000 images in the Wikipedia collection. Roughly speaking, there are three challenges that the task participants must deal with: Scalability. A good retrieval approach should perform well on such a large image set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness.</head><p>A natural, and also maybe the most effective solution for this task is text-based image retrieval approach since each image in this dataset is associated with user-generated alphanumeric, unstructured metadata (e.g., a brief caption or description of the image). However, text-based retrieval methods should be robust to noisy textual description that many images may be annotated with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-modal Fusion.</head><p>Considering the fact that some images in this dataset have few or even no descriptive texts, it may be more preferable to combine text-based and content-based image retrieval (CBIR) for better performance. However, a pure combination of traditional text-based and content-based approaches is not adequate for dealing with the problem of Wikipedia image retrieval <ref type="bibr" coords="2,357.61,106.62,10.64,9.02" target="#b0">[1]</ref>. Therefore, how to effectively exploit the correlation between different modalities of retrieval clues remains challenging.</p><p>In this participation, we carry out three kinds of experiments. Our key idea is to experiment with different query extension techniques to help the retrieval system get close to users' real intent. In general, the oft-used approach of query extension is to add terms to queries or modify preliminary queries. In our experiments, query extension techniques are used in different situations. In the case of text-based image retrieval, the expansion terms are automatically selected from a knowledge base that is (semi-)automatically constructed from Wikipedia. This open, constantly evolving encyclopedia can yield inexpensive knowledge structures that are specifically tailored to effectively enhance the semantics of queries. Encouragingly, the experimental results rank in the first place among all submitted runs. In the case of CBIR, 1-vs-all classifiers are trained for all query concepts by using the training images obtained by Yahoo! search. Then the retrieval task can be implemented as visual concept detection in the given Wikipedia image set. Clearly, the training images obtained by Yahoo! search are used to enhance the image retrieval task. In the last case, we experimented with a cross-media image retrieval approach by combining and re-ranking text-based and content-based retrieval results. Here the CBIR results are used to modify preliminary text-based retrieval results. Despite the final experimental results were not formally submitted before the deadline, this approach performs remarkably better than the single text-based retrieval or CBIR approaches.</p><p>The rest of this paper is organized as follows. First of all, we present the architecture of our system for the WikipediaMM task in section 2. Then three different approaches are described respectively in the following three sections. The experimental results of our approaches are presented in section 6. Finally we draw a conclusion and propose our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Architecture</head><p>In our system, we implemented a retrieve engine as a test environment for all methods. Fig. <ref type="figure" coords="2,473.33,465.42,4.17,9.02" target="#fig_0">1</ref> shows the architecture of our system, illustrating how different components works together to generate retrieval results. The components include:  (4) Cross-Media Re-Ranking Module: Processing unit that combines the sets of returned images from CBIR and text-based retrieval modules, and then performs cross-media re-ranking to obtain the final retrieval results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Query Extension for Text Retrieval using Knowledge from Wikipedia</head><p>A natural solution for WikipediaMM 2008 task is text-based image retrieval method. To help the retrieval system get close to users' real intent, query extension techniques are often used by adding terms to queries or modifying preliminary queries. In this participation, we focus on how to automatically extract the expansion terms from a knowledge base that is (semi-)automatically constructed from Wikipedia. Organized with concept identified by URL (one user defined concept refers a single page) and links between concepts and external nodes, Wikipedia is not only a Web collection but also an online knowledge center which assembles all users' intelligences. Thereby, it is naturally attractive and promising that this open, and constantly evolving encyclopedia can yield inexpensive knowledge structures that are specifically tailored to effectively enhance the semantics of queries.</p><p>Recently, "Wikipedia mining" has been addressed as a new research area. WikiRelate <ref type="bibr" coords="3,422.47,278.22,11.68,9.02" target="#b1">[2]</ref> used links-based path length for computing relatedness for given concepts; Nakayama et al. <ref type="bibr" coords="3,356.22,293.82,11.68,9.02" target="#b2">[3]</ref> proposed a link mining method called PFIBF (Path Frequency -Inversed Backward link Frequency) as a base for Web thesaurus construction. However, none of work is made on using Wikipedia as the knowledge base in information retrieval. Fig. <ref type="figure" coords="3,89.57,340.62,5.01,9.02" target="#fig_1">2</ref> shows the system framework and data flow of our text-based retrieval approach. As mentioned above, we first construct a knowledge base from Wikipedia pages for query extension. Specifically, each non-administrative Wikipedia page is used as a term/concept describing individuals (e.g., Jingtao Hu), concepts (e.g., Emissions trading), locations (e.g., Big Ben), events (e.g., collapse of the World trade Center), and categories (e.g., microbiology). For a given term, the related terms can be easily extracted from the corresponding Wikipedia page. Thus given a textual query (and/or concept), the query constructer searches the knowledge base with the given query term and then extends the query with extracted terms. Finally, the extended query is given to the retrieval engine to generate the final search results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Knowledge Base Construction from Wikipedia for Query Extension</head><p>In our system, the construction of the knowledge base from Wikipedia includes the following steps:</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Near Pages Selection</head><p>We first index all titles of Wikipedia articles 1 , and then retrieve the Wikipedia pages with TF-IDF model. Only pages with a similarity score higher than threshold θ (θ is set to be 0.9 in our experiments) are chosen as the related pages of the input query.</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Page Keyphrase Extraction</head><p>Keyphrases play a key role in the Wikipedia knowledge base construction. In a Wikipeida page, keyphrases or keywords briefly describe the content of the concept. Thus they can be used to enhance the semantics of that concept. When facing roughly 2,480,000 Wikipedia pages, we are motivated to summarize concepts and measure the concept relatedness. Most existing keyphrase extraction algorithms, such as KEA <ref type="bibr" coords="4,414.67,153.42,10.63,9.02" target="#b3">[4]</ref>, are supervised learning methods which require human labeled training sets. However, it's laborious to build such an appropriate training set. Moreover, Wikipedia has various lengths of pages with complicated structure. Therefore, unsupervised keyphrase extraction method is more preferable.</p><p>In our system, we employ an unsupervised keyphrase extraction algorithm presented in our previous work <ref type="bibr" coords="4,510.31,215.82,10.64,9.02" target="#b4">[5]</ref>.</p><p>This algorithm uses several set-independent feature weights, treating text in a page as a semantic network.</p><p>Several structure variables of Small-World Network (SWN) are used to select key nodes from the network as keyphrases {( , ( ))}</p><formula xml:id="formula_0" coords="4,122.52,258.39,51.83,13.88">k k K t P t =</formula><p>, each with a probability score ( )</p><formula xml:id="formula_1" coords="4,323.34,261.61,15.23,10.67">k P t</formula><p>indicating the importance of the extracted keyphrase k t .</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Term Selection for Query Extension</head><p>However, the top-ranked keyphrases can not be directly added for query extension. For instance, when searching "saturn", term "moon" may be extracted as keyphrase with a high score, but "moon" can appear on many pages and should be considered more general. To address this problem, a statistical feature Inverse Backward link</p><formula xml:id="formula_2" coords="4,70.92,356.22,425.79,38.92">Frequency ( ibf ) is calculated as: log( ) ( ) N ibf bf t β = + ,<label>(1)</label></formula><p>where ( ) bf t is the number of backward links in which the link text contains term t, N denotes the total number of articles and β is a parameter in case ( ) bf t is zero.</p><p>Therefore, the final weight of a keyphrase can be computed as:</p><formula xml:id="formula_3" coords="4,102.78,452.38,391.71,15.03">( ) ( ) k t K k k w P t ibf t ∈ = ⋅ (2)</formula><p>Then the keyphrases with their normalized weights are combined with the original query to construct an extended query to be fed into the retrieve engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3</head><p>Examples for topic "daily show" and topic "Big Ben" in the knowledge base</p><p>For each topic, we can extract a knowledge tree which consists of the given topic, and keyphrases extracted from Wikipedia pages. Moreover, this knowledge tree can be pre-constructed or online constructed. By treating each query as a topic, the text-based retrieval system finds the topic in the knowledge base, and uses the weighted keyphrases as query extension to enhance the system performance. It should be noted that if no relevant keyphrases can be found in the base with respect to the query, it is very easy to perform the above steps online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TF-IDF Model for Text Retrieval</head><p>For the retrieve engine, we use the TF-IDF paradigm which is widely used in text mining and information retrieval. It is defined as follows:</p><p>| | ( , ) ( , ) ln ( )</p><formula xml:id="formula_4" coords="5,241.62,137.72,107.42,23.27">d w t d TF t d DF t = ×</formula><p>where TF stands for the frequency of term t in document d, DF for the document frequency of term t in the dataset. This traditional method represents the documents by vectors with the TF-IDF value as terms' weights.</p><p>In our text retrieval system, the score of query q and document d correlates to the dot-product between document and query vectors in VSM. Documents with larger scores are ranked higher in result sets. Then the scoring function is defined as follows:</p><formula xml:id="formula_5" coords="5,109.80,244.30,377.68,23.53">t in q ( , ) ( , )* ( , )* ( ) Score q d w t q TF t d IDF t = ∑ (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Content-Based Image Classification and Retrieval</head><p>In many real-world image datasets such as Wikipedia image collection, there are some images have few or even no descriptive texts. To address this problem, content-based image retrieval (CBIR) is used in our system. Given (2) Building Bag of Words (BOW) representation: SIFT (or Scale-invariant feature transform <ref type="bibr" coords="5,506.85,746.34,10.51,9.02" target="#b5">[6]</ref>), Dense-SIFT <ref type="bibr" coords="6,117.51,75.42,13.98,9.02" target="#b8">[9]</ref> and Color-Dense-SIFT are extracted from the training sets of all concepts. Then k-means algorithm is employed to quantized different types of features, forming a combined visual codebook for three types of features. All images are represented by a set of tokens of the visual words.</p><p>(3) Supervised training for each topic: Unsupervised probabilistic latent semantic analysis (pLSA) <ref type="bibr" coords="6,501.10,122.34,15.54,9.02" target="#b9">[10]</ref>is utilized to infer the latent topic distribution of the training images based on the BOW representation. Then support vector machine (SVM) is used to train a one-class classifier for each concept in the latent topic space.</p><p>Given the trained 1-vs-all classifiers for all query topics, the testing process includes the following three steps: (3) Visual concept detection: For each test image, compute the responds of the trained SVMs for different concepts. Concept is detected only when the corresponding respond is above a given threshold. For a concept based retrieval, test images are finally ranked according to their responds with respect to the concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Cross-media Re-ranking with Text and Visual Content</head><p>For better retrieval performance, we study cross-media image retrieval by combining both text-based retrieval and CBIR methodologies. In our system, we use the re-ranking scheme to combine the retrieval results of the two engines. Given a query topic, either text-based retrieval or CBIR engine can return a set of result images. By combining the two result sets, the returned images are re-ranked with weighting factors</p><p>)</p><formula xml:id="formula_7" coords="6,110.46,383.20,375.22,13.87">text text visual visual WeightedScore q d w Score q d w Score q d = + .<label>(4)</label></formula><p>Here the key idea is to compare the overlap of the returned images between results returned by each engine. Let {( , ) : , , }</p><formula xml:id="formula_8" coords="6,70.92,484.52,411.33,43.44">i j i j G d d d d i M j M = = &lt; &lt; ,<label>(5) where 1 2</label></formula><p>( , ) i j d d stands for an image both returned by the two engines. The numbers of overlap images in Top N ranked images are computed as:</p><formula xml:id="formula_9" coords="6,117.48,562.52,362.50,13.90">1 1 2 2 1 2 #{ : , }, #{ : , } i i j j H d d G i N H d d G j N = ∈ &lt; = ∈ &lt;<label>(6)</label></formula><p>and the weight of each engine is calculated as:</p><formula xml:id="formula_10" coords="6,117.54,605.93,358.30,35.88">/ 2 / / l l l l H N w H N σ σ + = + ∑ , (<label>7</label></formula><formula xml:id="formula_11" coords="6,475.84,616.38,3.89,9.02">)</formula><p>where l is the engine identifier and σ is an adjusting parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we describe our experiments for WikipediaMM task at the ImageCLEF 2008. Note that some of experimental results reported in this section were not formally submitted before the deadline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Methodology</head><p>Three sets of experiments were conducted. We first experimented with the text-based image retrieval approach with query extension. Text-based retrieval in different text source, with various query extension strategies were compared in the experiments. The second approach we experimented with is content-based image retrieval (CBIR). Finally, we experimented with the cross-media retrieval approach proposed in section 5.</p><p>In WikipediaMM task, the dataset consists of approximately 150,000 images and their text descriptions. The pre-processing of the dataset includes stop-words elimination and stemming. We developed a case-sensitive word splitter and maintained a stop-word list which takes into account frequent words of Wikipedia, such as "wikitemplate", "GFDL", "category". English words are stemmed by Porter Stemmer <ref type="bibr" coords="7,426.05,184.62,11.70,9.02" target="#b6">[7]</ref> which is used in the Apache Lucene Engine <ref type="bibr" coords="7,172.90,200.22,10.63,9.02" target="#b7">[8]</ref>. The text descriptions of the collection are indexed with Lucene once they are pre-processed. The TF-IDF model has been implemented as a baseline in our experiments.</p><p>The results are evaluated by MAP (Mean Average Precision), P@N (precision of top N images). Other evaluation measures include R-precision, and binary preference, etc. The ground-truth results are given in the evaluation phase of WikipediaMM task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Results with Text-Based Retrieval</head><p>The goal of the first set of experiments is to check how well different text-based retrieval methods with query extension perform.</p><p>Text-based retrieval in different text sources. Each image in the dataset has two text sources: image filename and image description. Thus text-based retrieval system can search in the two sources or their joint set. Table <ref type="table" coords="7,519.42,340.62,5.01,9.02" target="#tab_0">1</ref> shows the testing results on the given collection, and Fig. <ref type="figure" coords="7,302.54,356.22,5.01,9.02" target="#fig_6">5</ref> depicts the curves of precision at top N results. We can see that, by combining filename text and description to a single search field, text-based retrieval can achieve a MAP of 25.65%. This is much better than the retrieval models with only filename or text description.</p><p>Clearly, it's reasonable to include additional textual clues for retrieval. with TF-IDF similarity measure. And for QE-Link-N, we implemented the lf-ibf algorithm (Link Frequency-Inverse Backward Link Frequency) proposed in <ref type="bibr" coords="8,316.69,122.22,10.66,9.02" target="#b2">[3]</ref>. Table <ref type="table" coords="8,360.15,122.22,5.01,9.02" target="#tab_1">2</ref> shows the testing results on the given collection, and Fig. <ref type="figure" coords="8,150.66,137.82,5.01,9.02" target="#fig_7">6</ref> depicts the curves of precision at top N results.  In the experiments, we found that with a high threshold, the number of similar article titles is limited and there are few terms to be added to the query. From the experimental results shown in Table <ref type="table" coords="8,425.90,488.22,3.76,9.02" target="#tab_1">2</ref>, we can see that using similar article titles in Wikipedia with respect to the queries directly does not help improving the performance of text-based retrieval system.</p><p>Link structure mining of Wikipedia can be used in Web thesaurus construction or query expansion. However, due to the complexity of Wikipedia hyperlink network, there were too many noises in these links, including</p><p>Wikipedia system-generated links, and links that are rarely referenced. Therefore, we obtained even worse results than text-based retrieval without query extension.</p><p>For query expansion with the automatically constructed knowledge base, we found that the performance of the text-based retrieval system using keyphrases with just high probabilities is worse than text-based retrieval without any query extension. This means that keyphrases with general meanings have negative effects on retrieval results.</p><p>In contrast, by using the Keyword-IBF method, general keyphrases referenced by many articles are granted a lower weight in the extended query. As a consequence, this query extension method tends to choose terms that are more specific. The average improvement of all queries in this case is around 0.5% over text-based retrieval methods without query extension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text-based retrieval with query extension from the semi-automatically constructed knowledge base.</head><p>Text-based image retrieval approach with query extension can perform better than traditional text retrieval approach by adding expansion terms automatically selected from a knowledge base that is automatically constructed from Wikipedia. However, the retrieval performance of this approach depends on the quality of the constructed knowledge base. Due to the current restriction of keyphrase extraction algorithms and knowledge base construction models, the automatically constructed knowledge base is not good enough to practical applications in most cases. Therefore, we performed some manual confirmations and modifications to the automatically extracted knowledge base. Here we use QE-Semi to denote this query extension approach.  By reasonably manual confirmation of the knowledge base, we obtained an obvious improvement of the retrieval results. From Table <ref type="table" coords="9,151.62,454.20,5.01,9.02" target="#tab_2">3</ref> and Fig. <ref type="figure" coords="9,195.35,454.20,3.75,9.02" target="#fig_8">7</ref>, we can see that this approach performs much better than all other models. This</p><p>shows that query extension by using a knowledge construction procedure with a good keyphrase extraction algorithm and reasonably manual confirmation can remarkably improve the retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experimental Results with CBIR</head><p>For content-based image retrieval, the experimental results show that our BOW methods perform better than other CBIR systems. Compared with the text-based systems, our CBIR obtained a comparable MAP (0.1912 of CBIR vs. 0.21003 of text-based retrieval), and higher precisions in the top-ranked images (P@5=0.5333 and P@10= 0.448 of CBIR vs. P@5=0.405333 and P@10=0.346667 of text-based retrieval). Although visual content ambiguity hampers the overall performance (MAP) by returning images with similar low-level features, the experimental results show that learning visual models from Web images (e.g., from Yahoo! search) do help to rank the content-relevant images in higher places. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Experimental Results with Cross-media Re-ranking</head><p>In the last set of experiments, text-based and content-based image retrieval approaches are combined so as to achieve better performance. In the experiments, we set M 2 smaller than M 1 . This means that only the top-ranked images returned by the CBIR approach are included in the re-ranking phase since the lower-ranked images may have much higher probabilities to be noises in the CBIR system. Table <ref type="table" coords="10,362.97,137.82,5.01,9.02" target="#tab_4">5</ref> shows the testing results on the given collection, where ReRank-Text-Visual-N denotes the combination of CBIR and text-based retrieval without query extension, and ReRank-Semi-Visual-N denotes the combination of CBIR and text-based retrieval with query extension from the semi-automatically constructed knowledge base, and N denotes the corresponding parameter in formula ( <ref type="formula" coords="10,119.27,200.22,3.90,9.02" target="#formula_9">6</ref>) and <ref type="bibr" coords="10,146.51,200.22,10.62,9.02" target="#b6">(7)</ref>.   <ref type="formula" coords="10,381.10,615.60,3.90,9.02" target="#formula_9">6</ref>) and ( <ref type="formula" coords="10,411.87,615.60,3.55,9.02" target="#formula_10">7</ref>).</p><p>We used 6 values of N (N=10, 20, 40, 60, 80, 100) in formula ( <ref type="formula" coords="10,335.18,631.20,3.91,9.02" target="#formula_9">6</ref>) and ( <ref type="formula" coords="10,367.86,631.20,3.54,9.02" target="#formula_10">7</ref>). It's interesting to find that when N increases, the number of overlap images in top-N results of the two systems tends to be closer. In this case, the parameters 1 w and 2 w tends to 0.5 and the preliminary results of the two systems are more likely to be equally treated. As a consequence, the retrieval performance decreases when N increases. Therefore, we used N=10 in the following experiments.</p><p>As shown in Table <ref type="table" coords="10,148.72,709.20,5.01,9.02" target="#tab_4">5</ref> and fig. <ref type="figure" coords="10,189.76,709.20,3.76,9.02" target="#fig_9">8</ref>, the re-ranking approach performs remarkably better than the single text-based or content-based retrieval approaches. For the combination of CBIR and text-based retrieval without query extension, the average improvement of all the queries in ReRank-Text-Visual-10 is around 5.34% over the single text-based retrieval approach (25.6498% of MAP, as shown in Table <ref type="table" coords="11,349.74,75.42,3.63,9.02" target="#tab_0">1</ref>). While for the combination of CBIR and text-based retrieval with query extension from the semi-automatically constructed knowledge base, the average improvement for all the queries in ReRank-Semi-Visual-10 is around 1.403% over the single text-based retrieval approach (34.44% of MAP, as shown in Table <ref type="table" coords="11,257.29,122.22,3.61,9.02" target="#tab_2">3</ref>).</p><p>Fig. <ref type="figure" coords="11,89.57,137.82,5.01,9.02" target="#fig_11">9</ref> shows two examples of the re-ranking results. For query "golden gate bridge", text-based retrieval returns more relevant images than visual-based information retrieval. By re-ranking the two results, we get a better performance and a higher precision of top ranked images than the previous two methods. For query "Singapore", visual-based retrieval does not perform well, however, combining CBIR result with text-based retrieval can still improve the system performance.</p><p>We also observed that the re-ranked results have higher precision of top-ranked images than results returned by In conclusion, the cross-media re-ranking approach performs remarkably well. This indicates that cross-media fusion is definitely a promising direction to investigate effective retrieval approaches in the context of a large-scale and heterogeneous collection of images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Work</head><p>In this paper, we reported our solutions for WikipediaMM task at ImageCLEF 2008. We experimented with text-based, CBIR and cross-media image retrieval approaches with query extension. Encouragingly, the experimental results of our text-based approach rank in the first place among all submitted runs. Moreover, our CBIR approach also performs better than other submitted CBIR runs. Despite the final experimental results were not formally submitted before the deadline, our cross-media approach performs remarkably better than the single text-based or content-based retrieval approaches.</p><p>However, our experiments are just a first attempt towards effective cross-media retrieval in the context of a large-scale and heterogeneous collection of images. The query extension techniques can still be improved.</p><p>Specifically, our knowledge base construction procedure strongly depends on the keyphrase extraction algorithm.</p><p>Thus how to more effectively extract concepts and their relationships from Wikipedia is still a challenging future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,165.12,668.34,265.00,9.02;2,75.96,683.88,448.40,9.02;2,70.92,699.42,30.31,9.02;2,184.33,508.93,226.38,153.85"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. The architecture of our system for WikipediaMM 2008 task (1) Data Processing Module: Processing unit that performs several pre-processing tasks for the queries and the dataset.</figDesc><graphic coords="2,184.33,508.93,226.38,153.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,70.92,715.08,453.47,9.02;2,70.92,730.62,65.58,9.02;2,70.92,746.28,453.48,9.02;3,70.92,75.42,95.88,9.02"><head>( 2 )</head><label>2</label><figDesc>Text Retrieval Module: Retrieval subsystem that searches the dataset with textual queries and returns relevant images. (3) CBIR Module: Content-based image retrieval subsystem that searches the dataset with visual features and returns relevant images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,115.44,621.54,364.46,9.02;3,75.13,505.56,232.63,90.92"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Text-based retrieval with query extension. (a) System framework, and (b) Data flow.</figDesc><graphic coords="3,75.13,505.56,232.63,90.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,70.92,574.74,453.59,9.02;5,70.92,590.22,453.39,9.02;5,70.92,605.82,453.59,9.02;5,70.92,621.42,453.67,9.02;5,70.92,637.02,412.02,9.02"><head>Fig. 4 A( 1 )</head><label>41</label><figDesc>Fig. 4 A summary of our approach. Given the keyword for query concept "skyscraper", we first collect the top K Yahoo's search images to form the training set for concept learning. After feature extraction, quantization and pLSA modeling, a 1-vs-all SVM classifier is learned for the concept "skyscraper". During test stage, testing images from Wikipedia follows a similar process with the training stage, but uses the trained pLSA and SVM model for test. The results of the classifier are going to be used for image ranking for concept retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,70.92,184.74,453.45,9.02;6,70.92,200.22,237.50,9.02;6,70.92,215.94,453.50,9.02;6,70.92,231.42,121.38,9.02"><head>( 1 )( 2 )</head><label>12</label><figDesc>Representing Wikipedia images with BOW: After feature extraction and quantization, each test image is represented by the visual words from the trained codebook. Inferring latent topic distribution of test images: Based on the trained pLSA model, we infer the latent topic distribution of the test imagse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,79.08,417.83,2.91,5.24;6,73.02,419.43,30.38,9.06;6,117.18,417.83,2.91,5.24;6,110.52,419.43,414.04,9.06;6,82.80,439.58,2.75,4.96;6,72.90,441.00,33.50,9.02;6,123.84,439.58,2.75,4.96;6,113.34,441.00,183.55,9.02;6,309.36,439.39,2.89,5.21;6,329.28,439.39,2.89,5.21;6,359.40,439.39,2.89,5.21;6,332.88,441.00,2.50,9.01;6,308.94,446.47,1.61,5.21;6,303.66,441.00,54.48,9.01;6,314.34,437.78,7.13,12.26;6,341.63,437.78,5.49,12.26;6,363.78,441.00,20.14,9.02;6,397.08,439.40,2.89,5.20;6,418.14,439.40,2.89,5.20;6,451.74,439.40,2.89,5.20;6,422.22,441.00,2.50,9.01;6,397.26,446.47,1.61,5.21;6,390.78,441.00,5.01,9.01;6,411.54,441.00,38.28,9.01;6,402.60,437.78,7.13,12.26;6,433.31,437.78,5.49,12.26;6,456.90,441.00,67.49,9.02;6,70.92,465.39,127.80,9.06;6,145.80,486.14,16.57,5.20;6,178.98,486.14,2.89,5.20;6,199.44,486.14,2.89,5.20;6,229.98,486.14,2.89,5.20;6,263.10,486.14,2.89,5.20"><head>1 R and 2 R 1 M and 2 M</head><label>1212</label><figDesc>respectively denote the returned result sets of text-based and CBIR-based retrieval engines, and are their corresponding sizes. Let image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,91.02,657.12,413.33,9.02;7,198.66,514.26,198.00,136.08"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Experimental Results of Precision at top N results of text-only retrieval in different text sources.</figDesc><graphic coords="7,198.66,514.26,198.00,136.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,115.50,425.94,364.21,9.02;8,149.34,441.39,296.58,9.06;8,200.46,281.52,194.28,139.14"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Experimental Results of Precision at top N results of text-based retrieval with query expansion from the automatically constructed Wikipedia knowledge base.</figDesc><graphic coords="8,200.46,281.52,194.28,139.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="9,115.50,391.92,364.21,9.02;9,142.08,407.37,316.26,9.06;9,200.88,251.52,193.56,131.10"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Experimental Results of Precision at top N results of text-based retrieval with query extension from the semi-automatically constructed Wikipedia knowledge base.</figDesc><graphic coords="9,200.88,251.52,193.56,131.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="10,73.20,600.12,449.30,9.02;10,172.74,615.57,249.77,9.06;10,103.20,442.56,196.32,134.40"><head>Fig. 8</head><label>8</label><figDesc>Fig. 8 Performance of cross-media re-ranking: (a) Precision at top N results of cross-media re-ranking. (b) MAP of re-ranking with different values of N in formula (6) and<ref type="bibr" coords="10,408.32,615.60,10.64,9.02" target="#b6">(7)</ref>.</figDesc><graphic coords="10,103.20,442.56,196.32,134.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="11,70.92,231.42,453.61,9.02;11,70.92,247.02,453.59,9.02;11,70.92,262.62,453.46,9.02;11,70.92,278.22,176.62,9.02"><head></head><label></label><figDesc>text-based retrieval or CBIR. There are some possible reasons. Text-based retrieval can return more relevant images by searching keywords with image descriptions, while CBIR can obtain high precision of top-ranked images but too many noises in lower-ranked images. Thus combining CBIR with text-based retrieval can help increase the precision of top-ranked images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="11,70.92,621.54,453.64,9.02;11,70.92,637.02,202.76,9.02;11,381.00,472.68,131.16,93.60"><head>Fig. 9</head><label>9</label><figDesc>Fig. 9 Search results for (a) topic "Golden Gate Bridge" and (b) topic "Singapore" using text-only (left), visual-only (middle) and their combination (right).</figDesc><graphic coords="11,381.00,472.68,131.16,93.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,124.44,371.94,381.37,75.20"><head>Table 1 :</head><label>1</label><figDesc>The testing results of text-based retrieval in different text sources.</figDesc><table coords="7,124.44,389.04,381.37,58.10"><row><cell>Run ID</cell><cell>QE</cell><cell>Modality</cell><cell>MAP</cell><cell>P@5</cell><cell>P@10</cell><cell>R-Prec</cell></row><row><cell>text-filename</cell><cell>without</cell><cell>TXT</cell><cell cols="4">0.256498 0.442667 0.374667 0.292887</cell></row><row><cell>text</cell><cell>without</cell><cell>TXT</cell><cell>0.21003</cell><cell cols="3">0.405333 0.346667 0.254963</cell></row><row><cell>filename</cell><cell>without</cell><cell>TXT</cell><cell cols="2">0.155436 0.322667</cell><cell>0.244</cell><cell>0.223775</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,120.84,153.54,384.99,124.28"><head>Table 2 :</head><label>2</label><figDesc>The testing results of text retrieval with query extension techniques.</figDesc><table coords="8,120.84,170.64,384.99,107.18"><row><cell>Run ID</cell><cell>QE</cell><cell>Modality</cell><cell>MAP</cell><cell>P@5</cell><cell>P@10</cell><cell>R-Prec</cell></row><row><cell>QE-Title-20</cell><cell>with</cell><cell>TXT</cell><cell cols="4">0.256567 0.418667 0.362667 0.296697</cell></row><row><cell>QE-Title-10</cell><cell>with</cell><cell>TXT</cell><cell>0.255679</cell><cell>0.432</cell><cell>0.376</cell><cell>0.293633</cell></row><row><cell>QE-Link-20</cell><cell>with</cell><cell>TXT</cell><cell>0.227106</cell><cell>0.376</cell><cell cols="2">0.314667 0.253258</cell></row><row><cell>QE-Link-10</cell><cell>with</cell><cell>TXT</cell><cell>0.226688</cell><cell>0.368</cell><cell cols="2">0.318667 0.258306</cell></row><row><cell>QE-SW-20</cell><cell>with</cell><cell>TXT</cell><cell cols="2">0.236526 0.373333</cell><cell>0.336</cell><cell>0.261819</cell></row><row><cell>QE-SWIBF-20</cell><cell>with</cell><cell>TXT</cell><cell>0.260936</cell><cell>0.44</cell><cell cols="2">0.369333 0.285868</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,120.84,169.14,384.95,75.20"><head>Table 3 :</head><label>3</label><figDesc>The testing results of text retrieval query extension.</figDesc><table coords="9,120.84,186.24,384.95,58.10"><row><cell>Run ID</cell><cell>QE</cell><cell>Modality</cell><cell>MAP</cell><cell>P@5</cell><cell>P@10</cell><cell>R-Prec</cell></row><row><cell>text-filename</cell><cell>without</cell><cell>TXT</cell><cell cols="4">0.256498 0.442667 0.374667 0.292887</cell></row><row><cell>QE-SWIBF-20</cell><cell>with</cell><cell>TXT</cell><cell>0.260936</cell><cell>0.44</cell><cell cols="2">0.369333 0.285868</cell></row><row><cell>QE-Semi</cell><cell>with</cell><cell>TXT</cell><cell>0.3444</cell><cell>0.5733</cell><cell>0.476</cell><cell>0.3794</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,70.92,641.52,453.55,90.68"><head>Table 4 :</head><label>4</label><figDesc>The testing results of CBIR.</figDesc><table coords="9,70.92,658.62,453.55,73.58"><row><cell>Run ID</cell><cell>QE</cell><cell>Modality</cell><cell>MAP</cell><cell>P@5</cell><cell>P@10</cell><cell>R-Prec</cell></row><row><cell>CBIR run1</cell><cell>with</cell><cell>IMG</cell><cell>0.1912</cell><cell>0.5333</cell><cell cols="2">0.442667 0.292887</cell></row><row><cell>CBIR run2</cell><cell>with</cell><cell>IMG</cell><cell>0.1928</cell><cell>0.5307</cell><cell>0.4507</cell><cell>0.2295</cell></row><row><cell cols="7">It also should be noted that, our CBIR approach performs best in all submitted CBIR runs in WikipediaMM 2008</cell></row><row><cell>task.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,99.42,215.94,406.37,221.00"><head>Table 5 :</head><label>5</label><figDesc>The testing results of Cross-media Re-ranking.</figDesc><table coords="10,99.42,233.04,406.37,203.90"><row><cell>Run ID</cell><cell>QE</cell><cell>Modality</cell><cell>MAP</cell><cell>P@5</cell><cell>P@10</cell><cell>R-Prec</cell></row><row><cell>ReRank-Text-Visual-10</cell><cell>with</cell><cell cols="2">TXTIMG 0.309876</cell><cell>0.608</cell><cell cols="2">0.521333 0.338713</cell></row><row><cell>ReRank-Text-Visual-20</cell><cell>with</cell><cell cols="3">TXTIMG 0.303529 0.602667</cell><cell>0.512</cell><cell>0.342036</cell></row><row><cell>ReRank-Text-Visual-40</cell><cell>with</cell><cell cols="2">TXTIMG 0.297208</cell><cell>0.584</cell><cell>0.489333</cell><cell>0.33931</cell></row><row><cell>ReRank-Text-Visual-60</cell><cell>with</cell><cell cols="5">TXTIMG 0.292776 0.554667 0.473333 0.336636</cell></row><row><cell>ReRank-Text-Visual-80</cell><cell>with</cell><cell cols="5">TXTIMG 0.290972 0.538667 0.469333 0.334891</cell></row><row><cell>ReRank-Text-Visual-100</cell><cell>with</cell><cell cols="5">TXTIMG 0.288602 0.522667 0.462667 0.335717</cell></row><row><cell>ReRank-Semi-Visual-10</cell><cell>with</cell><cell cols="5">TXTIMG 0.358431 0.629333 0.514667 0.399319</cell></row><row><cell>ReRank-Semi-Visual-20</cell><cell>with</cell><cell cols="5">TXTIMG 0.356829 0.618667 0.514667 0.397366</cell></row><row><cell>ReRank-Semi-Visual-40</cell><cell>with</cell><cell cols="4">TXTIMG 0.351918 0.586667 0.501333</cell><cell>0.39878</cell></row><row><cell>ReRank-Semi-Visual-60</cell><cell>with</cell><cell cols="2">TXTIMG 0.348719</cell><cell>0.568</cell><cell>0.492</cell><cell>0.39878</cell></row><row><cell>ReRank-Semi-Visual-80</cell><cell>with</cell><cell cols="4">TXTIMG 0.348332 0.565333 0.490667</cell><cell>0.39878</cell></row><row><cell>ReRank-Semi-Visual-100</cell><cell>with</cell><cell cols="3">TXTIMG 0.348259 0.557333</cell><cell>0.488</cell><cell>0.39878</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,78.42,748.53,378.21,8.10"><p>The Wikipedia articles and other related sources can be downloaded from http://download.wikipedia.org.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work is supported by grants from <rs type="funder">Chinese NSF</rs> under contract No. <rs type="grantNumber">60605020</rs>, <rs type="programName">National Hi-Tech R&amp;D Program</rs> (<rs type="grantNumber">863</rs>) of <rs type="funder">China</rs> under contract No. <rs type="grantNumber">2006AA01Z320</rs> and <rs type="grantNumber">2006AA010105</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AzfJSA6">
					<idno type="grant-number">60605020</idno>
					<orgName type="program" subtype="full">National Hi-Tech R&amp;D Program</orgName>
				</org>
				<org type="funding" xml:id="_ZSkdgs5">
					<idno type="grant-number">863</idno>
				</org>
				<org type="funding" xml:id="_YMW2XRC">
					<idno type="grant-number">2006AA01Z320</idno>
				</org>
				<org type="funding" xml:id="_VWqGAaD">
					<idno type="grant-number">2006AA010105</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,85.83,278.19,438.61,9.06;12,85.92,293.79,234.79,9.06" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,239.17,278.22,267.59,9.02">Exploiting multi-context analysis in semantic image classification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,517.50,278.19,6.94,9.06;12,85.92,293.79,77.82,9.06">J. Zhejiang Univ. SCI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1268" to="1283" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.82,309.39,436.68,9.06;12,85.92,324.99,416.85,9.06" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,215.48,309.42,260.55,9.02">WikiRelate! Computing semantic relatedness using Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Strube</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,489.90,309.39,34.59,9.06;12,85.92,324.99,232.60,9.06">Proc. of National Conference on Artificial Intelligence (AAAI2006)</title>
		<meeting>of National Conference on Artificial Intelligence (AAAI2006)<address><addrLine>Boston, Mass</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
			<biblScope unit="page" from="1419" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,85.13,340.59,439.30,9.06;12,85.92,356.19,438.52,9.06;12,85.92,371.82,58.35,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,228.25,340.62,264.69,9.02">A thesaurus construction method from large scale web dictionaries</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nishio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,502.92,340.59,21.51,9.06;12,85.92,356.19,417.95,9.06">Proc. of IEEE International Conference on Advanced Information Networking and Applications (AINA2007)</title>
		<meeting>of IEEE International Conference on Advanced Information Networking and Applications (AINA2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="932" to="939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,85.62,387.42,438.80,9.02;12,85.92,402.99,293.78,9.06" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,378.85,387.42,145.56,9.02;12,85.92,403.02,39.18,9.02">KEA: practical automatic keyphrase extraction</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Paynter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Nevill-Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,135.36,402.99,215.38,9.06">Proc. of Fourth ACM Conference on Digital Libraries</title>
		<meeting>of Fourth ACM Conference on Digital Libraries</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,85.28,418.62,439.09,9.02;12,85.92,434.19,438.57,9.06;12,85.92,449.82,85.61,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,333.54,418.62,190.83,9.02;12,85.92,434.22,72.47,9.02">Keyphrase extraction using Semantic Networks Structure Analysis</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,169.32,434.19,259.31,9.06">Proc. of the sixth IEEE Int&apos;l. Conf. on Data Mining (ICDM 2006)</title>
		<meeting>of the sixth IEEE Int&apos;l. Conf. on Data Mining (ICDM 2006)<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,85.35,465.39,439.31,9.06;12,85.92,480.99,133.48,9.06" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,130.47,465.42,210.84,9.02">Object recognition from local scale-invariant feature</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,351.78,465.39,172.89,9.06;12,85.92,480.99,21.54,9.06">Proc. Int&apos;l Conf. Computer Vision (ICCV 1999)</title>
		<meeting>Int&apos;l Conf. Computer Vision (ICCV 1999)</meeting>
		<imprint>
			<date type="published" when="1999-09">Sep. 1999</date>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,85.13,496.62,407.25,9.02" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,127.79,496.62,127.59,9.02">The Porter Stemming Algorithm</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://www.tartarus.org/~martin/PorterStemmer" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,84.51,512.22,398.98,9.02" xml:id="b7">
	<monogr>
		<ptr target="http://lucene.apache.org/java/docs/index.html" />
		<title level="m" coord="12,84.51,512.22,182.26,9.02">Apache Software Foundation</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Apache Lucene</note>
</biblStruct>

<biblStruct coords="12,86.33,527.82,438.10,9.02;12,85.92,543.42,328.26,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,237.06,527.82,287.37,9.02;12,85.92,543.42,100.32,9.02">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,197.04,543.42,127.19,9.02">Proceedings of the IEEE CVPR</title>
		<meeting>the IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.81,559.02,430.66,9.02;12,85.92,574.62,72.55,9.02" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,158.91,559.02,274.16,9.02">Unsupervised learning by probabilistic latent semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,447.13,559.02,77.33,9.02">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="177" to="196" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
