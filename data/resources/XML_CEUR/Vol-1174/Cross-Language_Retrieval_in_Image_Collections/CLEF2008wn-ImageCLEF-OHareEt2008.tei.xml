<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,95.98,182.75,49.95,8.74"><forename type="first">Neil</forename><surname>O'hare</surname></persName>
							<email>nohare@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,154.12,182.75,57.98,8.74"><forename type="first">Peter</forename><surname>Wilkins</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.93,182.75,60.29,8.74"><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.13,182.75,74.30,8.74"><forename type="first">Eamonn</forename><surname>Newman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.57,182.75,78.32,8.74"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,472.60,182.75,72.02,8.74"><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DBA085901190767494FBCC1EC40EA858</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval Measurement</term>
					<term>Performance</term>
					<term>Experimentation Content-Based Image Retrieval</term>
					<term>Data Fusion</term>
					<term>Clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>DCU participated in the ImageCLEF 2008 photo retrieval task, submitting runs for both the English and Random language annotation conditions. Our approaches used text-based and image-based retrieval approaches to give baseline retrieval runs, with the highest-ranked images from these baseline runs clustered using K-Means clustering of the text annotations. Finally, each cluster was represented by its most relevant image and these images were ranked for the final submission. For random annotation language runs, we used TextCat 1 to identify German annotation documents, which were then translated into English using Systran Version:3.0 Machine Translator. We also compared results from these translated runs with untranslated runs. Our results showed that, as expected, runs that combine image and text outperform text alone and image alone. Our baseline image+text runs (i.e. without clustering) give our best MAP score, and these runs also outperformed the mean and median ImageCLEFPhoto submissions for CR@20. Clustering approaches consistently gave a large improvement in CR@20 over the baseline, unclustered results. Pseudo relevance feedback consistently improved MAP while also consistently decreasing CR@20. We also found that the performance of untranslated random runs was quite close to that of translated random runs for CR@20, indicating that we could achieve similar diversity in our results without translating the documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the CLEF2008 ImageCLEF photo retrieval task, our baseline retrieval system DCU used standard text retrieval, both with and without pseudo relevance feedback, and content-based image retrieval (CBIR) approaches based on MPEG-7 low level visual features and a combination of text retrieval and CBIR. K-Means clustering was then run on the outputs from these retrieval approaches in order to promote a more diverse set of images towards the top of the result list in keeping with this years version of the task, which is to promote diversity in image retrieval. For cross-language retrieval (i.e. random language runs) we used TextCat to classify documents as English or German, and then translated German documents to English using Systran. We also submitted runs based on indexing the random language documents without translating them, to explore whether it is necessary to translate non-English annotations in order to achieve diversity.</p><p>The remainder of this paper is organised as follows: Section 2 outlines that approaches that we used for both retrieval and clustering, and details our submitted runs; Section 3 gives our results, along with some preliminary analysis of them. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>Our approach for the ImageCLEF photo retrieval task this year can be broken down into 3 main phases, as follows:</p><p>• Retrieval. We first run a text-based and image-based retrieval algorithms to create a traditional ranked list of images, ordered by relevance to the query.</p><p>• Clustering. To improve the diversity of the results, the images towards the top of the result list are clustered, which will output groups of similar images.</p><p>• Cluster Representative selection and Final Ranking. The clusters are then ranked in order of relevance to the query, and one representative image from each cluster is output to the final result list.</p><p>Each of these is described in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Retrieval</head><p>Since the topic set for 2008 consists of a subset of 39 of the 60 topics used in ImageCLEFPhoto 2006 and 2007, we used the remaining 21 topics as a training set of topics for system development.</p><p>Although we did not have ground truth for these topics for diversity, we did have ground truth for retrieval, so we could use these topics to guide development of our baseline retrieval systems.. In the following subsections we outline our approaches used for text retrieval, image retrieval and combined text and image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Text Retrieval</head><p>For text retrieval we indexed the following field from the structured annotation of each photo: Title, Description, Notes and Location. The location field was matched to a world gazetteer built using data from the Geographic Names Information System [3] and the GEOnet Names Server [1]. This allows us to automatically expand the location information to Town, State/County, Country, Continent, instead of just the Town and Country provided. To formulating our queries, we made use of the title and narr fields from the topics. Since the narr field often includes information about non-relevant documents, we parsed the narr field to remove any sentences containing the phrase 'not relevant'. We perform text retrieval using the BM25 algorithm <ref type="bibr" coords="2,422.92,627.96,14.61,8.74" target="#b6">[12]</ref>, as implemented in the Terrier search engine platform <ref type="bibr" coords="2,256.12,639.92,14.61,8.74" target="#b5">[11]</ref>, using the following parameters: k1 = 1.2, k3 = 8 and b = 0.75. For runs that use pseudo relevance feedback, we used the diversion from randomness approach <ref type="bibr" coords="2,132.92,663.83,14.61,8.74" target="#b5">[11]</ref>, using the top 10 terms from the 3 documents for query expansion.</p><p>For random annotation language runs, the annotation documents were processed using TextCat [4], which is an implementation of the text categorization algorithm proposed by Cavnar &amp; Trenkle <ref type="bibr" coords="2,90.00,699.69,10.52,8.74" target="#b1">[7]</ref> [1]. TextCat uses a n-gram language model approach to language identification, i.e. a language is recognised through the identification of distinct n-grams which occur frequently in the language but seldom or not at all in other languages. After TextCat identified all the German sentences in the set of random language annotation, this content was translated from German to English using Systran Version:3.0 Machine Translator <ref type="bibr" coords="3,268.58,112.02,9.96,8.74">[5]</ref>. The set of translations were then merged with the other items in the dataset (ie the English sentences) and passed to the next stage of our system.</p><p>For text retrieval we used 3 language conditions (english, translated random and untranslated random), each with and without PRF, giving 6 distinct baseline text retrieval runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Image Retrieval</head><p>For our visual retrieval in ImageCLFEPhoto we make use of six global visual features which are defined in the MPEG-7 specification <ref type="bibr" coords="3,252.01,203.46,14.61,8.74" target="#b4">[10]</ref>. The following low-level features were used:</p><p>• Scalable Colour (SC): derived from a colour histogram defined in the HSV colour space.</p><p>It uses a Haar transform coefficient encoding, allowing scalable representation.</p><p>• Colour Structure (CS): based on colour histograms, represents an image by both the color distribution (similar to a color histogram) and the local spatial structure of the colour.</p><p>• Colour Layout (CL): compact descriptor which captures the spatial layout of the representative colours on a grid superimposed on an image.</p><p>• Colour Moments (CM): similar to Colour Layout, this descriptor divides an image into 4x4 subimages and for each subimage the mean and the variance on each LUV color space component is computed.</p><p>• Edge Histogram (EH): represents the spatial distribution of edges in an image, edges are categorized into five types: vertical, horizontal, 45 degrees diagonal, 135 degrees diagonal and non directional.</p><p>• Homogeneous Texture (HT): provides a quantitative representation using 62 numbers, consisting of the mean energy and the energy deviation from a set of frequency channels.</p><p>To compute an answer for a visual query, we take the topic images and extract from each their six Query-Terms (i.e. a representation of the image by each of the six features previously detailed). For each Query-Term we query its associated retrieval expert (i.e. visual index and ranking function) to produce a ranked list. The ranking metric for each feature is as specified by MPEG-7 and is typically a variation on Euclidian distance. For our experiments we produced 1000 results per Query-Term. Each ranked list is then weighted and the results from all ranked lists are normalized using MinMax <ref type="bibr" coords="3,244.37,498.79,9.96,8.74" target="#b2">[8]</ref>, then linearly combined using CombSUM <ref type="bibr" coords="3,441.21,498.79,9.96,8.74" target="#b2">[8]</ref>.</p><p>The weighting scheme we used for combination of visual experts is a query-dependant weighting scheme for expert combination which requires no training data <ref type="bibr" coords="3,372.38,522.70,14.61,8.74" target="#b7">[13]</ref>. This approach is based on the observation that, if we were to plot the normalized scores of an expert against that of scores of other experts used for a particular query, then the expert whose scores exhibited the greatest initial change correlated with that expert being the best performer for that query. While we acknowledge this observation is not universal, it has been shown emperically to improve retrieval performance. This technique was also employed in DCU's participation in ImageCLEFPhoto 2007 <ref type="bibr" coords="3,90.00,594.43,9.96,8.74" target="#b3">[9]</ref>.</p><p>For example, if our topic set has three query images, we will extract six features per image, resulting in the generation of 18 Query-Terms. Each of these is then queried against its respective retrieval expert to produce 18 ranked lists, then each ranked list is then individually weighted, using the aforementioned technique and linearly combined through data fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Combination of Image and Text Retrieval</head><p>As with the combination of visual features, image and text results are combined by weighting each ranked list, normalizing the results using MinMax <ref type="bibr" coords="3,306.87,697.82,10.52,8.74" target="#b2">[8]</ref> and then linearly combined using CombSUM <ref type="bibr" coords="3,90.00,709.78,9.96,8.74" target="#b2">[8]</ref>. Our results on the set of 21 training topics showed that, for the text and image combination, global weights of 0.7 for text and 0.3 for image outperformed the query-dependant weighting approach described above for MAP, so we used global weights for combining text results with image results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Clustering</head><p>The results of our baseline retrieval, whether text-based, image-based or a combination of the two, are then clustered to increase the diversity of the results. All of our clustering approaches use text information exclusively; we do not perform clustering on visual features. Since it was permitted in this task to inspect the cluster tag from the topic and create higher level cluster types, we classified the cluster tags into 3 categories: 'location', 'non-location' or 'general'. The 39 topics include 17 unique entries for the cluster tag. After classifying them into 3 categories we use a different subset of the fields from the structured annotation for clustering, as follows:</p><p>• Location: Topics for which only the location tag is used for clustering, corresponding to the cluster tags 'city', 'state', 'location', 'country', 'city national park' and 'venue'.</p><p>• Non-location: Topics for which the location tags is ignored for clustering, corresponding to the cluster tags 'animal', 'sport', 'bird', 'weather condition', 'vehicle type', 'composition' and 'group composition'.</p><p>• General: Topics for which all tags used for retrieval are also used for clustering: 'statue', 'venue', 'landmark', 'volcano' and 'tourist attraction'.</p><p>Apart from using a different subset of the annotation fields, each cluster type is treated identically in our subsequent clustering. We also submitted runs that did not classify the cluster tag, and treated all topics the same.</p><p>For clustering, we employ K-Means clustering using the Text Clustering Toolkit, a toolkit for clustering text documents using a number of standard algorithms <ref type="bibr" coords="4,389.12,378.49,9.96,8.74">[2]</ref>. Using annotation fields from one of the 3 classes defined above, we take the top X documents from our baseline retrieval algorithms and cluster them using K-Means. Rather than choose one single value for X, we varied this parameter in a number of runs, using values of 50, 100 and 150. We also varied k, the number of clusters, using 20, 30 and 40 clusters. An additional variant used the the Calinski-Harabasz index to automatically estimate the optimum number of clusters <ref type="bibr" coords="4,374.98,438.26,9.96,8.74" target="#b0">[6]</ref>.</p><p>Since we are clustering a small number of documents (ie. 150 or less), the tf-idf weighting scheme may not have enough documents to calculate reliable inverse document frequency scores. For this reason, we have used two separate approaches to term normalisation for cluster analysis: term frequency (tf) and term frequency / inverse document frequency (tf-idf).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cluster Ranking and Cluster Representative Selection</head><p>The final step is to rank all clusters in order of relevance to the query, and then select a representative image for each cluster to output to the final ranked list. To rank clusters, we take the simple approach of using the maximum individual image score within the cluster as the overall cluster score. We also use the same maximum image as the cluster representative, and our final output is k images (i.e. the number of clusters), corresponding to the most relevant image from each cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Description of Submitted Runs</head><p>For our submission to ImageCLEFPhoto 2008 we created 13 baseline retrieval runs as follows: 3 language conditions (english, translated and untranslated random) with and without pseudo relevance feedback for text only baselines; each of these 6 were combined with image retrieval to give 6 text-image baselines; additionally, we had 1 image-only baseline. These 13 baseline runs were used as input into clusterin using a number of parameter variations, creating a number of different runs. The parameters were: X, the number of documents to cluster (50, 100 or 150); k, the number of clusters (20, 30, 40 or automatic using the Calinski-Harabasz index); term normalisation method (tf or tf-idf); cluster classification (classification used or classification not used). This gives a total of 48 variations of clustering for each baseline submission. Since we cluster the image-only baseline using each of the 3 language conditions, meaning we cluster 13 baselines plus two addition language variants for the image baseline, we have 15x48 = 720 clustered runs and 13 baseline runs, giving a total of 733 runs submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Our results are summarised in Table <ref type="table" coords="5,251.62,178.74,3.87,8.74">1</ref>, which shows our baseline unclustered results and the best clustered variation for each baseline. As one would expect our best results are given by combining text retrieval with image retrieval, with the best MAP of 0.354 and the best P20 of 0.476 given by the English text and image run using pseudo relevance feedback. The best result for Cluster Recall at 20 (CR@20) is given by the English text and image with clustering, with a score of 0.552. The clustered runs perform poorly for MAP, although this is not a surprise since we only rank the top k for these runs, giving a truncated result list which would be expected to perform poorly on MAP, which is dependent on the entire ranked list.</p><p>We can see that, while pseudo relevance feedback leads to consistently better retrieval performance in terms of MAP and P@20, it also decreases diversity. Runs without feedback consistently perform better for CR@20, and this pattern can be observed both in clustered and unclustered runs. Combining image and text retrieval also gives a large improvement in diversity: for English language multimodal clustered runs, for example, the best performance for CR@20 is improved from 0.514 to 0.552, an improvement of 7% (for unclustered English runs, the improvement is 12%). Since image retrieval and text retrieval naturally retrieve different relevant documents, it is not a surprise that combining them gives a large improvement in CR@20. Combining image and text retrieval also gives a large improvement in P@20, although it only gives a modest improvement in MAP. The unclustered English text and image runs also show that it is possible to achieve good CR@20 score without using clustering: the CR@20 score of 0.455 for this run is 29% above the mean (0.455 compared with 0.353), without using any clustering. This unclustered run give our most consistent performance across all evaluation measures, performing above the mean for MAP, P@20 and CR@20: in fact, all our unclustered English and unclustered translated Random runs achieve this, and our untranslated random runs beat the mean when combined with image retrieval.</p><p>Comparing random language runs with English runs, the best random runs perform quite close to the English runs in terms of diversity, achieving a CR@20 score of 0.536, only 3% below the best English score. Our untranslated runs also show that, by effectively discarding 50% of the documents in the collection (although, for the text and image runs, some of these 'discarded' documents may be recovered if their image score is high enough), we can still maintain a similar level of diversity, with a score of 0.518 for the clustered run, only 3% below the score achieved if we translate the annotation documents. It is an open question where this is an effect of this particular test collection or whether in real world scenarios there would be a higher correlation between clusters and document languages.</p><p>As expected, the runs that use image retrieval as the baseline retrieval perform quite poorly, although the CR@20 scores that they achieve are not very far below the median, and at the moment it is not possible to compare them to image-only runs from other groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper we presented the approaches we used for the ImageCLEF 2008 Ad-hoc photographic retrieval task, along with a preliminary analysis of our results. This preliminary analysis showed that pseudo relevance feedback, while improving performance for standard retrieval measures, harms performances when it comes to diversity. Combining image with text retrieval gives a large improvement in diversity even when the improvement in overall retrieval (as measured by MAP) is modest. Clustering the results of the baseline retrieval algorithms gives a large improvement in diversity, while unsurprisingly harming overall retrieval performance. For the random language condition (where half of the documents are in English and half in German) we have shown that it is possible to maintain diversity in our results without translating the German annotations into English.</p><p>This paper represents a preliminary analysis of our results, and we plan to analyse our results in more detail. In particular, we plan to look in detail at how the clustering parameters that we varied in our submissions affect results. Also, we have not yet analysed the results from the runs that do not classify the clustering approach used, instead clustering for all topics using the entire annotation instead of just, say, the location tag. We plan to compare these unclassified results to the results presented here, to examine the level of diversity performance that we can achieve if we have no information about the clustering criteria. We would also like to conduct some topic by topic, particularly in order to distinguish those topics which rely on location-based clustering from those that do not.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,739.12,159.98,6.99"><p>http://odur.let.rug.nl/ vannoord/TextCat/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For each clustered run, the best submission for that modality and language pairing is shown. Scores in bold represent the best result for that language, translated, modality triple for a given evaluation measure. For the modality field, a separate value in brackets indicates the modality for the baseline retrieval: for example, ImgTxt(Img) means the the baseline retrieval value image-based, but since text was used for clustering the overall modality was ImgTxt.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,110.48,385.97,402.53,8.74;7,110.48,397.92,102.52,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,245.78,385.97,170.57,8.74">A dendrite method for cluster analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Calinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Harabasz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,427.24,385.97,85.77,8.74;7,110.48,397.92,38.85,8.74">Communications in Statistica</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,417.85,402.52,8.74;7,110.48,429.80,402.52,8.74;7,110.48,441.76,299.68,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,269.67,417.85,155.67,8.74">N-Gram-Based Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Cavnar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Trenkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,450.69,417.85,62.30,8.74;7,110.48,429.80,329.90,8.74">Proceedings of Third Annual Symposium on Document Analysis and Information Retrieval</title>
		<meeting>Third Annual Symposium on Document Analysis and Information Retrieval<address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<publisher>UNLV Publications/Reprographic</publisher>
			<date type="published" when="1994-04-13">11-13 April 1994</date>
			<biblScope unit="page" from="161" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,461.68,402.52,8.74;7,110.48,473.64,402.52,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,280.28,461.68,147.68,8.74">Combination of Multiple Searches</title>
		<author>
			<persName coords=""><forename type="first">Edward</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,451.18,461.68,61.82,8.74;7,110.48,473.64,218.71,8.74">Proceedings of the Third Text REtreival Conference (TREC-1994)</title>
		<meeting>the Third Text REtreival Conference (TREC-1994)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,493.56,402.52,8.74;7,110.48,505.52,402.52,8.74;7,110.48,517.47,333.89,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,203.07,505.52,134.25,8.74">Dcu and uta at imageclefphoto</title>
		<author>
			<persName coords=""><forename type="first">Anni</forename><surname>Jarvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Wilkins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomasz</forename><surname>Adamek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eija</forename><surname>Airio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gareth</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eero</forename><surname>Sormunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,379.35,505.52,133.65,8.74;7,110.48,517.47,212.61,8.74">ImageCLEF 2007 -The CLEF Cross Language Image Retrieval Track Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,537.40,402.52,8.74;7,110.48,549.35,193.13,8.74" xml:id="b4">
	<monogr>
		<title level="m" coord="7,349.24,537.40,163.77,8.74;7,110.48,549.35,130.49,8.74">Introduction to MPEG-7: Multimedia Content Description Language</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Salembier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Sikora</surname></persName>
		</editor>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,569.28,402.52,8.74;7,110.48,581.23,402.52,8.74;7,110.48,593.19,174.62,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,427.33,569.28,85.67,8.74;7,110.48,581.23,267.43,8.74">Research Directions in Terrier: a Search Engine for Advanced Retrieval on the Web</title>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,386.85,581.23,126.14,8.74;7,110.48,593.19,144.25,8.74">Novatica/UPGRADE Special Issue on Web Information Access</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,613.11,402.53,8.74;7,110.48,625.07,402.52,8.74;7,110.48,637.02,149.76,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,110.48,625.07,75.80,8.74">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,210.10,625.07,270.01,8.74">Proceedings of the Third Text Retrieval Conference (TREC-3)</title>
		<meeting>the Third Text Retrieval Conference (TREC-3)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,656.95,402.52,8.74;7,110.48,668.90,402.52,8.74;7,110.48,680.86,279.75,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,339.44,656.95,173.56,8.74;7,110.48,668.90,135.91,8.74">Using Score Distributions for Querytime Fusion in Multimedia Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Wilkins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,269.29,668.90,243.71,8.74;7,110.48,680.86,159.45,8.74">MIR 2006 -8th ACM SIGMM International Workshop on Multimedia Information Retrieval</title>
		<meeting><address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
