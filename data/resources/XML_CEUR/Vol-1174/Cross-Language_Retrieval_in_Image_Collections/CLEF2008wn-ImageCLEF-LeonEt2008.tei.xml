<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,91.83,151.38,419.36,12.38;1,156.37,173.29,290.25,12.38">Automatic System for Extraction of Content-Based Characteristics from Digital Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,110.55,206.49,57.95,6.64"><forename type="first">Gonzalo</forename><surname>León</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="institution">E.T.S.I. Informática -U.N.E.D</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,176.60,206.49,76.83,6.64"><forename type="first">José</forename><forename type="middle">Luis</forename><surname>Delgado</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="institution">E.T.S.I. Informática -U.N.E.D</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,261.52,206.49,83.73,6.64"><forename type="first">Covadonga</forename><surname>Rodrigo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="institution">E.T.S.I. Informática -U.N.E.D</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,353.26,206.49,67.74,6.64"><forename type="first">Fernando</forename><surname>López</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="institution">E.T.S.I. Informática -U.N.E.D</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,429.09,206.49,63.36,6.64"><forename type="first">Valentín</forename><surname>Sama</surname></persName>
							<email>vsama@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="institution">E.T.S.I. Informática -U.N.E.D</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,91.83,151.38,419.36,12.38;1,156.37,173.29,290.25,12.38">Automatic System for Extraction of Content-Based Characteristics from Digital Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">15D12166C4ACB8355949DBD5B4C51D56</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.4 [Image Processing and Computer Vision]: I.4.0 General [Query Languages]</term>
					<term>I.4.5 Reconstruction [Transform Methods]</term>
					<term>I.4.6 Segmentarion [Pixel Classication]</term>
					<term>I.4.7 Feature Measurement</term>
					<term>I.4.9 Applications</term>
					<term>I.5 [Pattern Recognition]: I.5.2 Design Methodology</term>
					<term>I.5.3 Clustering</term>
					<term>I.5.5 Implementation Algorithms, Experimentation, Measurement, Performance, Verication CBIR, Calibration, Validation, JAI</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we expose the development of a CBIR system (Content-based Image Retrieval) that is able to retrieve images from a corpus based upon the image content. In order to obtain such functionality, the system establishes a set of characteristics which will be automatically generated. This allows the system to univocally identify each image from the collection. The sort of characteristics is diverse and they are related to concepts such as entropy, Gabor lters and image size. After the calculation of characteristics of each image, a calibration process is performed whereby the system estimates the best weight for each characteristic. This estimation makes use of a calibration algorithm and a set of experiments, and the result is the inuence of each characteristic in the main function that is used for the retrieval process. The calibration process starts in an equally balanced situation (all the characteristics have the same inuence in the main function), and after several iterations the weight for each characteristic is xed. The following task is the image validation, where the modications to the main function are veried so as to ensure that the new function is better than the previous one. Finally, the image retrieval process is performed according to the Im-ageCLEFmed rules. The retrieval results have not been the expected ones, but we must say they are a good starting point that makes us establish several work lines for the future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image characterisation and the process of nding links amongst a collection of images is a well-known challenge that has been studied for a long time. This task can be addressed in two dierent ways: textual (based upon a set of textual metadatas related to each image) and graphical (based upon the image observation and its properties). The former requires an initial tagging process; hence the results will depend on the goodness of t. The latter is a more objective way of characterising an image, but requires some complex mathematical procedures which increase the diculty of obtaining results for its later analysis.</p><p>Focused on the second way and, more specically in the context of digital images, the work entails the extraction of signicant data from these images, being this collection of attributes sucient for identifying the images. This signicant data (from now on we will call them only characteristics) is a set of short-scope attributes, allowing the identication, or at least the characterisation, within a certain group. The sort of characteristics is very diverse, covering the range from the properties such as height and width until the statistical data of the transformed image.</p><p>The reason for extracting the characteristics instead of working with the original images lies in the big size that images usually have, and implies a very high time-consuming task that cannot be aorded. Besides, the characteristic has to be a tiny piece of information since its usefulness lies in the ability of making very often comparisons with other equivalent characteristics within the same image or from other ones.</p><p>Currently there are several image processing systems which pursue this aim. For instance, the GIFT (GNU Image-Finding Tool ) is an image retrieval system based upon the content (a.k.a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CBIR Content-based Image Retrieval</head><p>). Given a certain image, the system returns the most similar images ordered by relevance. GIFT is intended as a client-server application. This work is aimed at developing a system that is able to extract, from collection of images, a set of characteristics from each of them which will be further used for cataloguing and comparing purposes. The characteristics are only based upon the image content and not upon other data (textual metadata).</p><p>2 An approach to the system In the previous section we introduced the concept of characteristic as an attribute that allows, alone or in combination with other attributes, establishing the membership of an image to a certain group. Obtaining characteristics and its later use in the images identication has been applied in many elds, mainly in the biometric identication as it is exposed in <ref type="bibr" coords="2,397.96,419.66,10.51,6.64" target="#b1">[2,</ref><ref type="bibr" coords="2,412.22,419.66,7.01,6.64" target="#b2">3]</ref>, with great sucessful results.</p><p>The rst question that appears when developing the system is to know which the best option is amongst all the possibilities, and what has already been developed for this aim. Consequently there is a need in investigating the characteristics which are likely to be extracted from an image, and to what extent they are suitable for their comparison.</p><p>Another goal is to plan the system in a scalable way in terms of the characteristics extraction software. The system should not have a close architecture when the purpose is to add new characteristics. On the contrary, the addition of new characteristics should be easy as far as possible, avoiding the modication of the code since while developing the system, the goodness of the characteristics' results developed will be unknown.</p><p>In the same way, the system that is going to be developed should be able to: Store a collection of images (corpus ) and their related characteristics.</p><p>Store a collection of classes/groups or characteristics which can be applied to images and their inuence in later comparisons.</p><p>Compare the image's characteristics with those belonging to the images within the corpus.</p><p>Combine the comparison's results provided by the dierent characteristics when obtaining a unied measure of similarity, for its later sorting. Amongst all the existing options when developing our system, we have chosen a Java development starting from scratch, using the Java 1.5 Standard Edition (J2SE), focused on the program execution in dierent platforms (operating systems), with no need for recompiling, making the application to be executed in several places only moving the les from one machine to another.</p><p>A further reason justifying this choice is the great number of libraries which are available for Java, covering many dierent elds like digital image processing. In this sense, we can nd JAI (Java Advanced Imaging), that is an optional library (not included in the standard distribution J2SE) allowing the reading and processing of images for extracting their characteristics. Furthermore, the use of the concurrent paradigm by means of threads was also considered to choose Java as the selected technology.</p><p>The number and source/nature of the characteristics available for this study was very diverse, so there are some characteristics more connected to the image's source (for instance, the number of bits describing the color depth per pixel), that allow a direct mathematical and easier management, whereas there are some other characteristic obtained through a mathematical algorithm (for instance, Gabor Filters) upon the image and its later processing requires a better knowledge and produces an indirect application. Hence it is necessary to perform an analysis of these characteristics and evaluate whether they are useful or not, through a justied selection.</p><p>On the other hand it seems to be a logical thought the fact that the inuence of these characteristics is not the the same in the nal result. A priori, characteristics related to the image's dimension (for instance, height and width) should have a lower weight than other characteristics, such as the image's average power; nevertheless, this needs to be accepted in a relative way. For this reason a calibration process is required to ensure to what extent each characteristic impacts in the nal value of the comparison. The calibration process will be described in Section 4.</p><p>After performing some experimental assessments and addressing some indications shown in several articles (see <ref type="bibr" coords="3,176.17,303.03,10.50,6.64" target="#b6">[7,</ref><ref type="bibr" coords="3,189.70,303.03,7.75,6.64" target="#b4">5,</ref><ref type="bibr" coords="3,200.45,303.03,7.75,6.64" target="#b0">1,</ref><ref type="bibr" coords="3,211.21,303.03,7.19,6.64" target="#b3">4]</ref>), the nal number of characteristics was reduced to 9. The comparison procedure to be used is the one giving a value of 0 for those images with a very low similarity, whereas it gives a value close to 1 when the similarity is higher. In all the cases, the JAI (Java Advanced Imaging ) library has been used for the image reading and other related information. The dierent characteristics, grouped by the method to extract them, are shown in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Characteristics related to the Histogram</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Euclidean distance between two histograms</head><p>Given the histograms from two images, one measure of the similarity between the images could be the Euclidean distance between both histograms, transforming these histograms in a n-dimensional vector, where n is the number of colours. Let x and y be the histograms from the two images. The euclidean distance is dened as:</p><formula xml:id="formula_0" coords="3,256.51,472.46,89.48,30.32">D = n i=0 (x i -y i ) 2</formula><p>However, there is a problem in directly applying this formule. The chances for the pixel value in images with dierent sizes can be very variable and, at last, they can be the same image (if the images are not normalised in their size). For this reason the comparison is not performed with that value but the weight of such value. It is used the probability for such value, i.e. the numer of chances of each pixel's value divided by the total number of pixels. Since the similarity between images is inversely proportional to the euclidean distance, it is provided as the value for the similarity: 1 -D.</p><p>The histogram presents a problem while working with images in a grey-scale or a colour-scale.</p><p>The source of this problem lies in the number of histograms required to dene the image. In the grey-scale images only one histogram is needed whereas the colour-scale images need three histograms (one per band colour: red, green and blue). In this way, what it is executed is a comparison between histograms in a colour-scale image and, if a comparison between grey and colour images is performed, rstly there is a transformation of the colour histograms into a greyscale one, so that the comparison can be established at the same level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arithmetic average from the Histogram</head><p>Another interesting value to be considered is the arithmetic average of the pixels' values, easily calculated from the histogram. The value for the similarity between two images is obtained through the next formule 1 -|x1-x2| /x1+x2, where | x 1 -x 2 |is the absolute value of the dierence between both averages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entropy</head><p>There is another characteristic related to the histogram and obtained from it, named entropy.</p><p>It is dened as follows:</p><formula xml:id="formula_1" coords="4,242.76,159.36,117.48,30.32">H = - n i=1 p(x i ) • log 2 p(x i )</formula><p>where n is the number of dierent values that each pixel can have, and p(x i ) is the chance for the value at the position i. The entropy returns a unique real value for each image. In order to compare two images, the similarity value comes from the next formule:</p><formula xml:id="formula_2" coords="4,254.80,245.23,92.20,23.22">S H = 1 - | E 1 -E 2 | E 1 + E 2</formula><p>where E 1 y E 2 are the entropies of each image. As it can be stated, the result returned by the formule, the less dierence between the images, the closer value to 1. However, this formule does not take into account the case of entropies with dierent sign, leading to a non signicant value, since the addition of both entropies is not the addition of their magnitudes. To avoid this problem, in case of a negative entropy, the less value of both entropies is added to the negative one. This makes one of them to get the value of 0 while the other is necessarily positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local entropy</head><p>Since the entropy is a unique valor for the entire image, there is a lack of local information about the image, i.e. it does not dene the image in its dierent areas. To be more accurate at this point, a new characteristic was added, named "local entropy ". The procedure involves the division of the image in 36 areas of the same size, and the calculation of the entropy for each of these areas. The result is a vector used as a characteristic of the digital image.</p><p>To calculate the similarity between two local entropies vectors, the process consists of getting the local entropies for the same area within the two images, calculated as 1 -x /V M , wherexis the dierence between the entropies and V M is the maxium value. Empirically was set to 10. Finally the arithmetic average of the similarities was selected as the similarity between the two images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Characteristics related to the Gabor's lters</head><p>Gabor's lters are used in a digitalised image and provide information about its texture. They are commonly parametrised with the orientation and the spatial frequency, as it is stated in <ref type="bibr" coords="4,494.35,537.87,9.96,6.64" target="#b7">[8]</ref>.</p><p>This transformation lies in the application of a kernel created from the Gabor's function described over the digital image. For our purposes it is convenient to study the image in dierent orientations, wave lengths and dierences. Eight orientations have been used, from 0 to ( 7 /8) π, with variations of ( 1 /8) π. For the wave lengths four options were selected: 2.73, 5.47, 8.20, 10.93.</p><p>For the dierences, two options were taken into account: 0 and -( 1 /2) π. From each transformed image, the arithmetic average is obtained as well as the typical deviation; they are two dierent characteristics.</p><p>The whole similarity between two vectors (both arithmetic averages as well as typical deviations) is calculated through the arithmetic average (in absolute value) between the elements at each vector's position. The similarity S 1 is calculated as 1-x /V M , where x is the arithmetic average of the dierences and V M is the maximum value empirically set to 100. The nal similarity value is calculated as 0.1 • S 1 + 0.9 • S 2 , where 0.1 and 0.9 are the selected values coming from several experiments which were carried out about this lter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other characteristics</head><p>Other easier characteristics have been included in the system and are described as follows:</p><p>Colour Bits : Number of bits used to represent the image. In general, 8 and 16 bits are used for images in grey-scale and 24 bits for the full-colour images. The similarity is 1-|bc1-bc2| /bc1-bc2, where bc 1 y bc 2 are the bits of the rst and second images respectively.</p><p>Aspect Ratio: Proportion between the width and the height of each image. The similarity value is 1 -|a1-a2| /max(a1,a2), where a 1 and a 2 are the aspect ratios for the rst and the second images respectively.</p><p>Tiny Image : Direct comparison between images reduced to a size of 40x30 pixels. This characteristic, despite of being very simple, is highly eective when comparing images with a similar shape, as it is stated in <ref type="bibr" coords="5,272.80,209.49,9.96,6.64" target="#b5">[6]</ref>. The similarity value is calculated by the formule 1 -De /V M , where De is the euclidean distance and V M is the maximum value set to 12000. Size : This is the number of pixels. The similarity is returned by the next formule min(t1,t2) /max(t1,t2), where t 1 and t 2 are the sizes for the rst and the second images respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Calibration and Validation</head><p>As we forementioned in the previous sections, the calibration process was the one that leads to the modication of each characteristics' weight so as to increase the hit ratio in the group's prediction. It is possible for one characteristic to be better than any other one to classify an image, but this could not happen when working with any other dierent image. For this reason there is a need in nding an average weight of inuence in terms of nal similarity that allows an appropriate clasication for the greater number of images.</p><p>The calibration process is a non trivial task. It can be supposed that, through the entire collection of working images, it is selected a better weight that improves our classication process for the same collection of images. This could be understood like cheating and distortioning the results since the perfect classication is used in advance. In other words, if the weights are modied to let the Image X, that is known in advance it belongs to the Group Y, to be classied within the Group Y, it would not be honest to state that the classication has been carried out in the right way (the information to solve the problem has been used in advance and it should be unkown at that moment). This could be compared to asking someone to foretell a card that has been previously shown.</p><p>For this reason, the calibration requires a later process named validation. The validation allows the system to decide whether the modication of the weights enclosed in the calibration enhances or not the results. To achieve this goal, the validation process utilises a collection of images not used in the calibration, and a sort of validity is obtained for the weights calculated in the previous step.</p><p>To carry out the calibration process it was used the ImageCLEF information from the previous years. Specically the database used was casimage (http://pubimage.hcuge.ch) that was divided in disjoint groups. A total of 27 groups were extracted from the 57, discarding those with a few number of images or those very similar to existing ones.</p><p>The goal was to obtain the characteristics' weights that make the similiraty value, for each image within the a group, as high as possible. As a secondary objective, the similarity value for a certain image belonging to a group should be as low as possible when comparing to images belonging to other groups; this objective prevents the system classifying in a wrong way. These goals were achieved comparing all the images within the corpus amongst them, obtaining a collection of sorted descendant lists by similarity. For each list, it was calculated the prediction capacity to the group it belonged to, having the greatest capacity prediction those located in the rst positions of the list. If it was possible to infere, from such value for the prediction, that an image belonged to the group it really belonged to, then it was asserted that there had been a hit in the prediction.</p><p>At this point of the process, the weight of a characteristic was slightly increased and the step above was performed again. If the number of hits was stable or increased, the characteristic's weight was increased as well and the number of hits was re-calculated. If the number of hits decreased, the previous weight was restored and the process tried to increase the weight of other characteristic. This process was executing until covering the whole set of characteristics. We call this Calibration Algorithm .</p><p>The calibration algorithm is a very time-consuming task. Considering a corpus that consists of 5000 images, the system took 5 days in executing the algorithm. However, the algorithm has demonstrated to be the most suitable to obtain the best results compared to other algorithms evaluated. For a collection of images outside the corpus to calibrate the system, the hit ratio was increased from the 56% (that gave the same weight to all the characteristics) to the 86% with the weights coming from the algorithm.</p><p>In any case, and even being the computational cost very high, the calibration process is required since it is reasonable that the system improves future classications when the weights for the right classifcations are previously extracted (feedback). To start the study of the calibration process, some initial analysis were carried out taking as inputs the results of the comparisons between a group of images and the images within the repository (corpus). To manage this, the similarity values were extracted in order to include them in a database with the objective of graphically displaying this result. Figure <ref type="figure" coords="6,218.73,269.26,4.98,6.64">1</ref> shows the similarity distribution between images within the same group and images belonging to dierent groups, for the characteristic of local entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 Similarity Distribution for Local Entropy</head><p>It should be visually noted that the similarity amongst images within the same group (Groups 1-57) are placed, in average, above the similarities amongst dierent groups. This is a logical circumstance because it asserts that the similarity amongst images within the same group is higher than the one coming from dierent groups. This fact can be explained from the point of view of our system. We dened some criteria to decide whether an image was relevant or not, by means of a procedure that is described as follows:</p><p>for each topic, a list of images from the corpus was generated, ltered by similarity and sorted descendantly. Afterwards, for each image within the list, the typical deviation of each image and its previous ten similar ones was calculated. When this value was under 0,01, i.e. when the similarity of this image with the previous ten was very close, the list was "cut" in that point, and the images provided were the previous ones. The reason for selecting this method lies on the observation of the similarities in such ordered list and the fact that the similarity within the images at the rst position decreased signicantly, and after these rst position the way it decreased was softer.</p><p>Probably the use of a lower value would have been more suitable. Above all, perhaps we have not taken account of a very important factor, like the fact of that some "good" images, due to the calibration, may fall to the latest position of the list and be discarded. Besides, it seems to be logical that scanning the entire list, the number of images retrieved would be bigger; this possibility was discarded due to the lack of eciency and speed of the overall system. In any case, it will be a very relevant issue to consider for the future.</p><p>In the same way, the characteristics that have been used in the development should be revised (probably the addition of anyone else should be desirable and even required), as well as a signicant improvement of the eciency in the calibration process so as to get the best ts in less time; and nally, to make the process to be adapted depending on the source of the images, making the algoritm execute in a dierent and optimum fashion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,90.00,301.08,425.20,226.11"><head></head><label></label><figDesc></figDesc><graphic coords="6,90.00,301.08,425.20,226.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,90.00,617.79,423.00,121.18"><head>Table 1</head><label>1</label><figDesc>illustrates the values obtained for the similarities once the algorithm was executed</figDesc><table coords="6,90.00,629.75,290.88,41.87"><row><cell>through several iterations:</cell></row><row><cell>5 Analysis of the results and conclusions</cell></row></table><note coords="6,104.94,684.51,408.06,6.64;6,90.00,696.46,423.00,6.64;6,90.00,708.42,423.00,6.64;6,90.00,720.37,423.00,6.64;6,90.00,732.33,395.53,6.64"><p>In our rst participation with the ImageCLEF we can say we have not achieved the expected results. Compared to the rest of the groups, and analysing the metrics provided by the organisation, we have got the last position in most of the cases. The reason for this position lies on the number of extracted images for each topic, always under the number of images provided by the rest of participants (we have provided dozens while some other teams have provided thounsands).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,215.08,284.69,172.84,6.64"><head>Table 1 Calibration</head><label>1</label><figDesc>Algorithm results</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.50,596.14,407.51,6.64;7,105.50,608.10,407.50,6.64;7,105.50,620.05,45.51,6.64" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,496.13,596.14,16.88,6.64;7,105.50,608.10,241.86,6.64">Ipal knowledge-base medical image retrieval in imageclefmed</title>
		<author>
			<persName coords=""><forename type="first">X</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><forename type="middle">D</forename><surname>Raccoceanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thi</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Teodorescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vuillemot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
	<note type="report_type">IPAL French-Singaporean Joint Lab</note>
</biblStruct>

<biblStruct coords="7,105.50,634.63,407.50,6.64;7,105.50,646.58,407.51,6.64;7,105.50,658.54,407.50,6.64;7,105.50,670.49,407.50,6.64;7,105.50,682.45,254.46,6.64" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sánchez-Reíllo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sánchez-Ávila</surname></persName>
		</author>
		<title level="m" coord="7,305.40,634.63,207.60,6.64;7,105.50,646.58,337.48,6.64">Sistemas de identicación biométrica mediante patrón de iris utilizando representación multiescala e información de fase. (1</title>
		<imprint>
			<publisher>Departamento de Matemática Aplicada a las Tecnologías de la Información. E.T.S.I. Telecomunicación -Universidad Politécnica de Madrid (UPM) (2) Departamento de Ingeniería Eléctrica, Electrónica y Automática</publisher>
			<date>1</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>Universidad Carlos III de Madrid (UC3M)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,697.03,407.50,6.64;7,105.50,708.98,335.94,6.64" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,294.90,697.03,213.53,6.64">Identicación dactilar basada en ltros de gabor</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Suárez López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">Martín</forename><surname>Rodríguez</surname></persName>
		</author>
		<imprint>
			<publisher>Universidad de Vigo</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,723.56,407.51,6.64;7,105.50,735.51,407.51,6.64;7,105.50,747.47,316.02,6.64" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,344.58,723.56,168.42,6.64;7,105.50,735.51,357.46,6.64">image retrieval &amp; annotation tasks for the general photographic and medical image collections</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science &amp; Software Engineering, Concorcdia University</orgName>
		</respStmt>
	</monogr>
	<note>Cindi at imageclef</note>
</biblStruct>

<biblStruct coords="8,105.50,113.48,407.50,7.13;8,105.50,125.43,142.51,7.13" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,264.31,113.84,214.09,6.64">The clef 2004 cross-language image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanderson</forename><forename type="middle">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,486.37,113.48,26.63,7.13;8,105.50,125.43,18.50,7.13">CLEF 2004</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page">597613</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,141.18,407.51,6.64;8,105.50,153.14,334.07,6.64" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,306.27,141.18,206.73,6.64;8,105.50,153.14,34.66,6.64">Visual features for content-based medical image retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Heesch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rüger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howarth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yavlinski</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Imperial College London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Multimedia Information Retrieval Team</note>
</biblStruct>

<biblStruct coords="8,105.50,168.53,149.26,6.64;8,276.67,168.53,236.33,6.64;8,105.50,180.12,357.32,7.13" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,209.71,168.53,45.05,6.64;8,276.67,168.53,236.33,6.64;8,105.50,180.48,14.67,6.64">Using text image retrieval systems: Lic2m experiements at imageclef</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mollet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Besançon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,150.58,180.12,114.48,7.13">Grupo CEA-LIST/LI2</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">0</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">CM en Fontenay-aux-Roses -CEDEX</note>
</biblStruct>

<biblStruct coords="8,105.50,195.87,407.50,6.64;8,105.50,207.46,229.45,7.13" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,286.92,195.87,221.54,6.64">Comparison of texture featres based on gabor lters</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kruizinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Grigorescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,105.50,207.46,170.32,7.13">IEEE Transaction on Image Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
