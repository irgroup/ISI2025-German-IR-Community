<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.60,97.77,392.02,17.22;1,178.68,119.73,245.82,17.22;1,184.20,141.57,235.05,17.22">UPMC/LIP6 at ImageCLEF&apos;s WikipediaMM: An Image-Annotation Model for an Image Search-Engine</title>
				<funder ref="#_nfHWr7a">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.32,175.93,75.93,9.96"><forename type="first">Ali</forename><surname>Fakeri-Tabrizi</surname></persName>
						</author>
						<author>
							<persName coords="1,225.49,175.93,82.68,9.96"><forename type="first">Massih-Reza</forename><surname>Amini</surname></persName>
						</author>
						<author>
							<persName coords="1,316.57,175.93,64.02,9.96"><forename type="first">Sabrina</forename><surname>Tollari</surname></persName>
						</author>
						<author>
							<persName coords="1,387.76,175.93,73.07,9.96"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire d&apos;Informatique de Paris</orgName>
								<orgName type="institution">Université Pierre et Marie Curie-Paris6</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR CNRS 7606</orgName>
								<address>
									<addrLine>104 avenue du président Kennedy</addrLine>
									<postCode>75016</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.60,97.77,392.02,17.22;1,178.68,119.73,245.82,17.22;1,184.20,141.57,235.05,17.22">UPMC/LIP6 at ImageCLEF&apos;s WikipediaMM: An Image-Annotation Model for an Image Search-Engine</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4DC1628B2A8A7D3C14C4E02CED24114A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Visual Information Retrieval, TF-IDF, Language Model, Ranking on Data Manifolds</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present the LIP6 retrieval system which automatically ranks the most similar images to a given query constituted of both textual and/or visual information through a given textual-visual collection. The system first preprocesses the data set in order to remove stop-words as well as non-informative terms. For each given query, it then finds a ranked list of its most similar images using only their textual informations. Visual features are then used to obtain a second ranking list from a manifold and a linear combination of these two ranking lists gives the final ranking of images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF's WikipediaMM uses image collection created and employed by the INEX competition <ref type="bibr" coords="1,90.00,599.29,10.00,9.96">[1]</ref>. This image collection contains approximately 150,000 images that cover diverse topics of interest. These images are associated with unstructured and noisy textual annotations in English. There are 75 topics as the query to be searched on the collection. Each topic consists of textual data (and/or sample images and/or concepts) describing a user's (multimedia) information need. These data are given as the XML files in each one there is image textual information such as image title, image concept, image location path and a short text which describes the desired results, i.e it explains what could be the relevant result and what is the irrelevant results. The aim of this task is to try to find as many relevant images as possible from the Wikipedia image collection for a given textual-and/or-visual query.</p><p>In section 2, we propose a strategy to perform the retrieval process. Section 3 describes the models we used in our experiments and in section 4 we show the results we obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Strategy</head><p>We have a collection of textual-visual documents constituted of images and their corresponding textual notes. The aim is hence to find a list of ranked documents for each query in a such way that the most relevant documents have the highest ranks. A query can be in a textual or both textual and visual formats. In order to benefit from the whole given data, we introduce a retrieval strategy which combines two lists of ranked documents obtained separetely from textual and image informations for each given query. If a query is just described by a text, the system provides the ranked list obtained from the first textual strategy. In the following section we detail these different retreival strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>The textual strategy is based on the Term Frequency-Inverse Document Frequency Model <ref type="bibr" coords="2,483.24,266.53,10.45,9.96" target="#b0">[2]</ref> and the Language Model <ref type="bibr" coords="2,181.56,278.53,10.00,9.96" target="#b1">[3]</ref>. We run each of these models separately and then combine linearly their outputs. This stage follows a preprocessing step in which stop-words from the Glasgow University [4] stop-list are removed. We then use the porter stemming algorithm to reduce terms into their common stem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Term Frequency-Inverse Document Frequency Model</head><p>In the TF-IDF model <ref type="bibr" coords="2,189.36,360.61,9.91,9.96" target="#b0">[2]</ref>, each document in the collection and a query are represented by their associated vector of the length of the vocabulary. Each term w i in the vocabulary has a real-valued weight in x, where x is a document or a query, equal to the product of tf (w i , x) × idf (w i ) where</p><formula xml:id="formula_0" coords="2,146.16,405.73,302.17,24.22">tf (w i , x) = | {w|w ∈ x ∩ w = w i } | | {w|w ∈ x} | , idf (w i ) = log 1 + |D| 1 + | {d | |w i ∈ d} |</formula><p>and |D| representes the total number of documents in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Language Model</head><p>In the Language Model <ref type="bibr" coords="2,193.93,486.61,10.00,9.96" target="#b1">[3]</ref>, a query Q = [q 1 . . . q n ] is considered to be generated by a probabilistic model based on a given document D = [d 1 . . . d m ] and the aim is to estimate the strength of this generation via P (D|Q). The Bayes formula lets us extend the desired probability as follows:</p><formula xml:id="formula_1" coords="2,248.16,532.45,106.69,9.96">P (D|Q) = p(Q|D)P (D).</formula><p>At this stage, we suppose that the distribution of documents in the corpus is uniform and that document terms are independant which gives:</p><formula xml:id="formula_2" coords="2,250.20,589.69,102.64,21.10">P (D|Q) ∝ qi∈Q P (q i |D)</formula><p>This model is also known as the unigram model. By dissociating query terms occuring or not in D we get:</p><formula xml:id="formula_3" coords="2,212.88,647.17,300.16,21.58">P (D|Q) ∝ qi∈D P s (q i |D) × qi / ∈D P u (q i |D)<label>(1)</label></formula><p>P s is the probability model of occuring query terms in D and P u is the probability model nooccuring terms. P u (q i |D) could be estimated by α d × P (q i |C) in which α d is a constant factor and C is the collection of documents. Documents are then ranked according to the logarithm value of equation (1):</p><formula xml:id="formula_4" coords="3,155.76,94.21,291.49,28.29">log P (Q|D) = qi∈D log P s (q i |D) N (q i , d) + |q i | × log α d + qi / ∈D log P (q i |C).</formula><p>Where N (q i , d) is a normalization factor. For the estimation of P s (q i |D), we used the Jelinek-Mercer smoothing approach <ref type="bibr" coords="3,207.30,146.05,13.21,9.96" target="#b2">[5]</ref>:</p><formula xml:id="formula_5" coords="3,190.56,168.01,222.01,10.65">P s (q i |D) = (1 -λ)P ml (q i |D) + λP (q i |C) , λ ∈ [0, 1]</formula><p>in which P ml is the maximum likelihood estimation of the probability of presence of a query term</p><formula xml:id="formula_6" coords="3,90.00,201.85,280.09,36.21">q i in D P ml (q i |D) = |q i ∈ D| j=1...N |q j ∈ D| .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ranking on Data Manifolds</head><p>The goal of using Data Manifolds <ref type="bibr" coords="3,245.79,271.81,10.45,9.96" target="#b3">[6]</ref> is to rank the data with respect to their intrinsic global manifold structure revealed by their visual information. For many real world data types this model is superior to a local method, which rank data simply by pairwise Euclidean distances or inner products. The idea of model is as follows. First, we form a weighted network on the data (using a simple Kruskal's algorithm <ref type="bibr" coords="3,247.94,319.69,10.45,9.96" target="#b4">[7]</ref> by the weights corresponding to the Euclidean distances), and assign a positive ranking score to each query, ranked with respect to the queries, and zero to the rest. Afterward, all points spread their ranking score to their neighbors via the weighted network. The spread process is repeated until a global stable state is achieved, and all points except queries are ranked according to their final ranking scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preprocessing Phase</head><p>As the first step of preprocessing, all unuseful characters should be removed, like non-alphabetic ones. As we are sure that in Wikipedia task there exist only the English characters, we delete also the non-English characters. In next step of preprocessing, each word in the corpus existing in the anti-dictionary will be deleted. As third step, we try to retrieve the keywords and verbs' roots hidden in the combined terms. In this task, all the words appeared in the topics will create our keywords set. The idea is based on searching substrings in each word or combined term to find the hidden keywords through them. As we are sure that in Wikipedia task there exist only the English characters, we delete also the non-English characters. In next step of preprocessing, each word in the corpus existing in the anti-dictionary will be deleted. As third step, we try to retrieve the keywords and verbs' roots hidden in the combined terms. In this task, all the words appeared in the topics will create our keywords set. Our strategy based on searching substrings in each word or combined term to find the hidden keywords through them.</p><p>After this step, we remove the words with an appearance frequency less than 10 times in the corpus unless the word is a keyword. The reason why we do this step is to reduce the size of the corpus to make it easier to process the documents in the models. We lost a part of information, but it is not so important because we try to delete the least frequent words which have usually the least importance.</p><p>Afterward, we remove all documents containing less than 5 words for the reason that they have not so much information to be important in models' process. Here also, we prevent the documents containing any keywords being removed. During the preprocessing, we create the dictionary (vocabularies list) and we also index each document respectively to the dictionary to facilitate future calculation. Dictionary is created simply as a list of two columns: first column contains the words existing in the obtained corpus from the latter step. In second column, each line indicates the times of appearance of the word located in the corresponding line in first column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Description of the runs</head><p>All runs are fully automatic and do not use feedback nor query expansion. The visual features are composed of a simple global HSV histogram (8x3x3) and of the standard deviations of H, S and V. The visual vectors are so composed of 17 dimensions.</p><p>Run1 TFIDF A textual approach using TF-IDF. We used exclusively the text information here.</p><p>Each topic text (query) and document were characterized in the bag-of-word space using a TF-IDF encoding. By performing a cosines-similarity between the given query and documents, we had a value for each document represents the relevance. Descending sort on documents upon these obtained values made the most similar documents be ranked higher.</p><p>As the final result, we chose the 2000 first documents in the ranking list.</p><p>Run2 LM A textual approach using Language-Model. We used exclusively the text information again but this time we perform another textual model. For each topic we estimated the probability that every document in the collection generates the query using a unigram language model as explained in models section. At the end, the probability calculated for each document was counted as the relevance value. By sorting these values we obtained the ranked list of documents and we got the N first documents as the final result.</p><p>Run3 TFUSION TFIDF LM A textual approach using a fusion of the results of Language-Model and TF-IDF. For each query q, we obtained two lists of ranked documents according to the TF-IDF and the Language Model approaches as calculated in Run1 and Run2. The final ranked list for q is based on the fusion of the rank values upon these two lists. As the fusion in this part, we got 50% of each ranked list to combine.</p><p>Run4 visualonly A visual approach using Image only. We used exclusively the image information here. Each topic image (query) and document were characterized using global HSV image features. For each query, its most similar documents in the sense of the euclidean measure were chosen. We did not provide any results for those topics without an associated image query.</p><p>Run5 TIFUSION LMTF COS A texto-visual approach using Fusion Text-image (1). For each topic, we ranked documents upon two approaches considering only the text information as done in Run3. We got the 2000 first documents and try to re-rank them upon their similarity using image features, i.e the final rank of documents was obtained by re-sorting the 2000 first documents upon their similarity to the image(s) of the given topic. We used the Euclidean distances as the similarity measure.</p><p>Run6 TI LMTF MANIF A texto-visual approach using Fusion Text-image (2). For each topic, we ranked documents upon two approaches considering only the text information as done in Run3. We got the 2000 first ranked document and try to re-rank these 2000 document upon their similarity using image features. but here, we used manifold-based technique in place of the simple Euclidean distance. Briefly, we create the network of data manifold on base of these N documents, through them we search the most relevance to query and the values of data manifolds model was the final rank of documents.</p><p>Run7 TIFUSION LMTF MANIF A texto-visual approach using Fusion Text-image (3). In this part, we used two ranking lists obtained from Run5 and Run6 and the final ranking list was obtained from the fusion of rank values of these two lists. In the fusion in this part, we took 60% as the weight factor for the ranked list of Run5 and 40% as the weight factor for Run6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Discussion</head><p>Table <ref type="table" coords="5,117.96,253.21,4.98,9.96" target="#tab_0">1</ref> shows that Language Model (run2) gives better P5 and P10 scores than TFIDF model (run1). The reason why the probabilist model works better may be because WikipediaMM task is a case in which there are not the same keywords existing in query and in our database. Using a combination of results of TFIDF and Language Model (run3) leads to an improvement, because we can benefit of strength points of both models. The visual only run (run4) -we have submitted it as a visual base line -gives, as expected, the worst results and shows how difficult is the image retrieval task with visual features only. The ranking on data manifold model (run7), as shown in table 1, gives similar results than euclidean ones (run5). The reason why this model appeared not better than euclidean ones -as expected -is that it is more sensitive on feature extraction manner. Finally, the texto-visual approaches improve slightly P10 scores compare to text scores, but not MAP scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this working note, we focus our efforts on the study of how to find automatically the most similar images through a given textual-visual collection to a given query which is a collection of textual and/or visual information. First, we perform a preprocessing on the textual part, then for each given query, we use the textual information retrieval models which give us ranking lists. We also use the visual features to obtain another ranking list. A linear combination of these ranking lists give us the new ranking lists which benefit both of visual and textual part of information. As a new idea, we also perform the method of ranking on data manifold in place of using euclidean distance to find the similarity. As for several participants of WikipediaMM 2008, our text runs obtain better MAP than textovisual runs, but our texto-visual runs give better P10 scores, then using a combination of textual and visual information seems, in our case, improving the first results. That shows the difficulty to obtain a good texto-visual fusion. We use the same TF-IDF model and the same Language Model in ImageCLEFphoto 2008 as in WikipediaMM2008. Contrary to WikipediaMM task, in ImageCLEFphoto 2008, these two models give similar scores when the query is composed of the title words (see <ref type="bibr" coords="5,160.55,594.85,10.45,9.96" target="#b6">[8]</ref> for more details). The ranking on data manifold seems a promising way, but we have to improve our feature extraction manner.</p><p>As perspectives, we will study how the feature extraction manner influence the results obtain by the data manifold model. We will also consider the different parameters of language model and how the preprocessing phase could be improved in our future works. We also expect to investigate the effect of learning with both labeled and unlabeled data in the search phase <ref type="bibr" coords="5,437.16,654.61,10.00,9.96" target="#b7">[9]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,91.08,61.21,420.87,151.68"><head>Table 1 :</head><label>1</label><figDesc>WikipediaMM 2008 results. All runs are automatic and used no feedback no extension.</figDesc><table coords="5,468.48,61.21,35.11,9.96"><row><cell>Number</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was partially supported by the <rs type="funder">French National Agency of Research</rs> (<rs type="grantNumber">ANR-06-MDCA-002 AVEIR</rs> project).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_nfHWr7a">
					<idno type="grant-number">ANR-06-MDCA-002 AVEIR</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,105.48,169.81,407.66,9.96;6,105.48,181.81,252.27,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,275.80,169.81,233.48,9.96">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,105.48,181.81,179.58,9.96">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.49,201.73,407.78,9.96;6,105.48,213.61,228.52,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,239.32,201.73,228.11,9.96">A General Language Model for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,474.38,201.73,38.88,9.96;6,105.48,213.61,183.64,9.96">Research and Development in Information Retrieval</title>
		<imprint>
			<biblScope unit="page" from="279" to="280" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.49,253.45,407.50,9.96;6,105.48,265.45,292.15,9.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,271.60,253.45,241.39,9.96;6,105.48,265.45,142.24,9.96">A Study of Smoothing Methods for Language Models Applied to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.49,285.37,407.69,9.96;6,105.48,297.37,173.99,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,105.48,297.37,119.33,9.96">Ranking on Data Manifolds</title>
		<author>
			<persName coords=""><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,232.60,297.37,46.87,9.96">NIPS 2003</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.49,317.29,407.51,9.96;6,105.48,329.17,407.53,9.96;6,105.48,341.17,8.26,9.96" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,447.15,317.29,65.84,9.96;6,105.48,329.17,46.70,9.96">Introduction to Algorithms</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronald</forename><forename type="middle">L</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clifford</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press and McGraw-Hill</publisher>
		</imprint>
	</monogr>
	<note>Second Edition. Section 23</note>
</biblStruct>

<biblStruct coords="6,113.74,341.17,232.16,9.96" xml:id="b5">
	<monogr>
		<title level="m" coord="6,130.45,341.17,157.12,9.96">The algorithms of Kruskal and Prim</title>
		<imprint>
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.49,361.09,407.50,9.96;6,105.48,373.09,407.42,9.96;6,105.48,384.97,218.77,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,129.25,373.09,377.99,9.96">UPMC/LIP6 at ImageCLEFphoto 2008: on the exploitation of visual concepts (VCDT)</title>
		<author>
			<persName coords=""><forename type="first">Sabrina</forename><surname>Tollari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcin</forename><surname>Detyniecki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Fakeri-Tabrizi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Massih-Reza</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,117.96,385.42,174.31,9.07">Working Notes of ImageCLEFphoto2008</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.49,404.89,407.56,9.96;6,105.48,416.89,407.64,9.96;6,105.48,428.89,177.04,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,387.98,404.89,125.07,9.96;6,105.48,416.89,147.46,9.96">Learning Classification with Both Labeled and Unlabeled Data</title>
		<author>
			<persName coords=""><forename type="first">Jean-Noël</forename><surname>Vittaut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Massih-Reza</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,260.91,416.89,252.21,9.96;6,105.48,428.89,89.08,9.96">Proceedings of the 13th European Conference on Machine Learning (ECML&apos;02)</title>
		<meeting>the 13th European Conference on Machine Learning (ECML&apos;02)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="468" to="476" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
