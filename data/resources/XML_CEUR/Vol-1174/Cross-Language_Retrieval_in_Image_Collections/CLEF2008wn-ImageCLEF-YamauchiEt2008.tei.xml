<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,121.44,75.75,352.16,12.22;1,94.56,93.75,405.84,12.22">Meiji University at ImageCLEF2008 Photo Retrieval Task: Evaluation of Image Retrieval Methods Integrating Different Media</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,104.88,118.13,70.69,8.85"><forename type="first">Kosuke</forename><surname>Yamauchi</surname></persName>
							<email>yamauchi@cs.meiji.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
								<address>
									<addrLine>1-1-1 Higashimita, Tama-ku, Kawasaki-shi</addrLine>
									<postCode>214-8571</postCode>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,182.64,118.13,62.02,8.85"><forename type="first">Takuya</forename><surname>Nomura</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
								<address>
									<addrLine>1-1-1 Higashimita, Tama-ku, Kawasaki-shi</addrLine>
									<postCode>214-8571</postCode>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.24,118.13,43.87,8.85"><forename type="first">Keiko</forename><surname>Usui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
								<address>
									<addrLine>1-1-1 Higashimita, Tama-ku, Kawasaki-shi</addrLine>
									<postCode>214-8571</postCode>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.64,118.13,56.62,8.85"><forename type="first">Yusuke</forename><surname>Kamoi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
								<address>
									<addrLine>1-1-1 Higashimita, Tama-ku, Kawasaki-shi</addrLine>
									<postCode>214-8571</postCode>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,366.96,118.13,35.76,8.85"><forename type="first">Miki</forename><surname>Eto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
								<address>
									<addrLine>1-1-1 Higashimita, Tama-ku, Kawasaki-shi</addrLine>
									<postCode>214-8571</postCode>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,421.92,118.13,68.55,8.85"><forename type="first">Tomohiro</forename><surname>Takagi</surname></persName>
							<email>takagi@cs.meiji.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
								<address>
									<addrLine>1-1-1 Higashimita, Tama-ku, Kawasaki-shi</addrLine>
									<postCode>214-8571</postCode>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,121.44,75.75,352.16,12.22;1,94.56,93.75,405.84,12.22">Meiji University at ImageCLEF2008 Photo Retrieval Task: Evaluation of Image Retrieval Methods Integrating Different Media</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E69EAD631F20A9DCF44F332D239EC7D1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>Information Retrieval, Image Retrieval, Query Expansion, Conceptual Fuzzy Sets, Fuzzy Clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the Human Interface Laboratory of Meiji University in the ImageCLEF2008 photo retrieval task. We submitted eight retrieval runs taking two main approaches. The first approach combined Text-Based Image Retrieval (TBIR) and Context-Based Image Retrieval (CBIR). The second approach applied query expansion using conceptual fuzzy sets (CFS). A CFS is a method that uses the expression of meaning depending on the context, which an ordinary fuzzy set does not recognize. A conceptual dictionary is necessary to perform query expansion using CFS, and this is constructed by clustering. We propose here the use of query expansion with CFS, pseudo relevance feedback (PRF), and other techniques, for image retrieval that integrates different media, and we verify the utility of the system by explaining our experimental results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.275" lry="841.875"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.275" lry="841.875"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.275" lry="841.875"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.275" lry="841.875"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.275" lry="841.875"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.275" lry="841.875"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.275" lry="841.875"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.275" lry="841.875"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We present here our participation in the ImageCLEF2008 photo retrieval task. The task deals with answering 39 queries of variable complexity from a repository of 20,000 photographic images in the IAPR TC-12 photographic collection. The details of this task can be found in ImageCLEFphoto 2008 <ref type="bibr" coords="1,425.04,539.09,10.53,8.85" target="#b0">[1]</ref>.</p><p>We submitted eight retrieval runs this year taking two main approaches.</p><p>The first approach is an image retrieval system that integrates Text-Based Image Retrieval (TBIR) and Context-Based Image Retrieval (CBIR). We are taking this approach in order to overcome the difficulties that arise with these methods individually. For example, using TBIR, a person's subjectivity can easily be introduced, and furthermore, images without text cannot be retrieved. Using CBIR with the present technology, it is very difficult to see only the contents of the image and to make the computer understand what the image is. This method is also less accurate than TBIR. We expect that these difficulties can be avoided by combining TBIR and CBIR, and the accuracy of the image retrieval can be improved by the synergistic effect of different media used to solve these problems.</p><p>The second approach is query expansion using Conceptual Fuzzy Sets (CFS). Current search engines are powerful. However, several words are difficult to search. Takagi et al. <ref type="bibr" coords="1,363.60,665.57,11.76,8.85" target="#b1">[2]</ref> proposed resolving this problem by query expansion depending on the context using CFS. A CFS is a method that uses meaning expression depending on the context, which a fuzzy set does not recognize. We propose a system that depends more on query context than on query expansion for improving the packaging method of a conceptual fuzzy set.</p><p>This paper is organized as follows. In section 2, CFS and a method of query expansion using CFS are presented. Section 3 describes the details of all eight retrieval systems that we submitted. Section 4 describes the results of our experiment. In Section 5, we consider the results and discuss our study.</p><p>In this section, we explain what the conceptual fuzzy sets (CFS) is. Next, the method necessary to construct a conceptual dictionary in order to perform a query expansion using CFS is described, and finally, the query expansion technique is described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conceptual Fuzzy Sets</head><p>A CFS is based on the use theory of meaning propounded by Wittgenstein for the expression of meaning of a concept. According to this theory, the meaning of a word can be expressed by another word. Thus, the meaning of a word can be expressed by other words associated with one another. Also, the expression of the meaning of a word by another word makes a closed circular system. In CFS, the meaning of a word is expressed by the set of words and their activity values.</p><p>General fuzzy sets express phenomena without a clear boundary. However, when applying them to various realistic problems, they are not situation-dependent. The meaning of a concept changes in situation-dependent phenomena, and cannot correspond to a fixed expression. This problem occurs when the generality of knowledge is not obtained because the ambiguity of a fuzzy set is fixed, and the mechanism including background is not given. The essence of this problem is the expression of meaning.</p><p>Conceptual fuzzy sets solve situation-dependent problems by a recall mechanism (i.e., combined fuzzy sets) and the general versatility of knowledge by making a closed circular system based on the use theory of meaning.</p><p>In our system, we created a conceptual fuzzy set by the superpositioning of concepts. The concept component of a fuzzy set is called a prototype concept, and a set of prototype concepts is called a concept dictionary. The concept depending on the input context can be generated by superpositioning a prototype concept that is similar to the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Conceptual Dictionary</head><p>We constructed a conceptual dictionary to perform query expansion using conceptual fuzzy sets. The following methods are commonly used to construct a conceptual dictionary. i) Manual construction. ii) Clustering corpora, assuming one cluster is one concept. iii) Using an existing edifice of knowledge (e.g., Wordnet), and making a concept from words included in each directory, etc. However, the first method is a very tedious task, and the third method cannot be used to make a concept if the existing edifice of knowledge cannot be found. Thus, in our experiment, we constructed a concept dictionary by clustering.</p><p>We compared the precision of query expansion by constructing two concept dictionaries. We show each method for constructing these concept dictionaries below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Conceptual Dictionary using Fuzzy C-means</head><p>We used the fuzzy C-means method to construct the conceptual dictionary. The construction process was as follows.</p><p>1. From the data included in the corpus, make a word vector from the annotation text and a region ID vector from an image. Make a feature vector by combining the region ID vector and the word vector. 2. Cluster the feature vectors using the fuzzy C-means method; in this case, we set the number of clusters to be 1,000. 3. The fuzzy C-means method was used for clustering, and all vectors belonged to all clusters. Therefore, we set the threshold based on the membership value and limit the number of vectors that belong to one cluster. In addition, in a reorganization of the Term Frequency-Inverse Document Frequency (TF-IDF) method, we weight the feature vectors taking into consideration the degree of belonging to a cluster. First, we define Ej as a clustered element (i.e., feature vector in our experiment), and j expresses an identifier. Here, Ej has the same value of the degree of belonging to each cluster I (in our experiment, it was the same as the membership value), we define this as Belong(Ej, I), and we define TF as the multiplication of feature vectors taking into consideration Belong(Ej, I). TF is obtained with Eq. ( <ref type="formula" coords="2,133.11,695.09,3.51,8.85">1</ref>).</p><p>(1)</p><p>4. We define the word set from 3. As the prototype concept.</p><formula xml:id="formula_0" coords="2,137.04,717.81,133.22,19.46">j I E j E I E Belong TF j ) , (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Query Expansion using CFS</head><p>The technique of query expansion using CFS is described here. The flow of query expansion is shown below.</p><p>1. Extract a word vector from &lt;title&gt; and &lt;narr&gt; of topic, and extract a region ID vector from the image. Then, make a feature vector by combining the word vector and the region ID vector. 2. Calculate the degree of similarity of the feature vector of the query and each prototype concept in a concept dictionary using the cosine measure. 3. Make the query's concept by overlapping a number of similar prototype concepts N. The overlap of the prototype concept is calculated by Eq. ( <ref type="formula" coords="3,289.28,159.41,3.92,8.85">2</ref>) Here, Similarity is the cosine measure, C i is the number of prototype concept i, and we set N = 5.</p><p>(2)</p><p>4. Extract words that have a high score, and append this to the query. In our experiment, we extracted 15 words and appended these words to the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SYSTEM DESCRIPTION</head><p>In this section, details of the retrieval system that we submitted are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Integration System</head><p>This system is the bare system used as the base of all submitted systems, and it is considered to be the final retrieval result that integrates the retrieval results of TBIR and CBIR. This system was built based on the system of CINDY <ref type="bibr" coords="3,157.68,344.45,10.53,8.85" target="#b2">[3]</ref>, which participated in the ImageCLEF2006 photo retrieval task. First, the details of TBIR and CBIR are explained, and then the method of integration is described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Text-based Image Retrieval</head><p>We used Apache Lucene <ref type="bibr" coords="3,187.44,392.69,11.76,8.85" target="#b3">[4]</ref> as the TBIR method. Apache Lucene is an all-text retrieval engine developed as open source software. In Lucene, we can use text retrieval with TF-IDF, but we built Okapi BM25 <ref type="bibr" coords="3,493.68,404.21,11.76,8.85" target="#b4">[5]</ref> into Lucene for our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Content-based Image Retrieval</head><p>CBIR is depicted in Figure <ref type="figure" coords="3,195.36,453.65,15.12,8.85">1．</ref> Fig. <ref type="figure" coords="3,287.76,580.37,5.04,8.85">1</ref> CBIR CBIR consists of three retrieval modules called global retrieval，grid retrieval, and region retrieval. The retrieval results of the three retrieval modules are integrated by Eq. ( <ref type="formula" coords="3,353.67,611.09,3.51,8.85">3</ref>), and this integration result is considered to be the final retrieval result of CBIR. where result indicates the retrieval result of each retrieval module, and weight indicates weight. In this system, each weight is as follows: global_weight = 0.3, grid_weight = 0.6, region_weight = 0.1. The details of the three retrieval modules are described as follows.</p><formula xml:id="formula_1" coords="3,139.44,187.68,212.67,26.29">i N i i C Query C Similarity NewConcept ) , (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Retrieval</head><p>In the global retrieval module, first, the feature values of color (lab) and texture (wavelet transform) are extracted from the whole image. The distance scale between the images uses Earth Mover's Distance for the color and Euclidean distance for the texture. In this module, the weights of color and texture are as follows: Color_Weight = 0.9, Texture_Weight = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grid Retrieval</head><p>In the grid retrieval module, first, an image is divided into a 3×3 block. And, feature values of color (lab average, standard deviation) and texture (wavelet transform) are extracted for each block. The distance scale between the images uses Euclidean distance for both color and texture. In this module, the weight of color and texture are as follows: Color_Weight = 1.0, Texture_Weight = 0.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Region Retrieval</head><p>The region retrieval module first segments an image with the JSEG algorithm <ref type="bibr" coords="4,448.08,213.89,10.53,8.85" target="#b5">[6]</ref>. Then feature values are extracted from all regions. These features are normalized into Z-scores and combined into a 47-dimensional vector. The distance scale between the images uses the cosine measure. Table <ref type="table" coords="4,152.16,248.45,5.04,8.85" target="#tab_0">1</ref> lists the adopted features and their dimensions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Integration System</head><p>Here, an integrated part of the retrieval result is described. Figure <ref type="figure" coords="4,367.68,383.09,5.04,8.85">2</ref> outlines the flow of the Integration System.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2 Integration System</head><p>If a query is input, it is divided into an annotation and three images, and the annotation is passed to the TBIR, and the images are passed to the CBIR. TBIR described in 3.1.1 is executed using &lt;title&gt; and &lt;narr&gt; from the query annotation. At the same time, CBIR described in 3.1.2, is executed. Finally, the retrieval results of TBIR and CBIR are integrated by Eq. ( <ref type="formula" coords="4,210.15,574.13,3.51,8.85">4</ref>), and the final retrieval result is obtained. <ref type="bibr" coords="4,490.80,600.77,11.76,8.85" target="#b3">(4)</ref> where result indicates the retrieval result of each retrieval system, and weight indicates the weight. In this system, each weight is as follows: TBIR_weight = 0.5, CBIR_weight = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Integration System + CFS</head><p>This system combined CFS with the Integration System. Figure <ref type="figure" coords="4,342.24,675.41,5.04,8.85">3</ref>  If a query is input, query expansion using CFS is performed when TBIR is executed. The details of query expansion using CFS are described in 2.3 of Section 2. The result of TBIR using query expansion is integrated with the result of CBIR, and the final retrieval result is obtained. In this system, each weight is as follows: TBIR_Weight = 0.5, CBIR_Weight = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inter Media Pseudo Relevance Feedback</head><p>Inter Media Pseudo Relevance Feedback (IMPRF) is a query expansion system that is performed using an annotation of the image obtained through the retrieval result of CBIR, and that also executes TBIR. This system is based on the system of IPAL <ref type="bibr" coords="5,234.00,314.93,10.71,8.85" target="#b6">[7]</ref>, which participated in the ImageCLEF2006 photo retrieval task. Figure <ref type="figure" coords="5,106.56,326.45,5.04,8.85">4</ref> outlines the flow of IMPRF. Fig. <ref type="figure" coords="5,284.40,440.93,5.04,8.85">4</ref> IMPRF When a query is input, first CBIR is executed. Next, query expansion is performed using the retrieval result of CBIR. Query expansion is performed by extracting 15 words from &lt;TITLE&gt;, &lt;DESCRIPTION&gt;, &lt;NOTES&gt; and &lt;LOCATION&gt; of the annotations of the top 5 images of the retrieval result of CBIR. TBIR is executed using the words obtained by the query expansion. The retrieval results of TBIR and CBIR are integrated, and the final retrieval result is obtained. In this system, each weight is as follows: TBIR_weight = 0.5, CBIR_weight = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">IMPRF + PRF</head><p>This system combines pseudo relevance feedback (PRF) with IMPRF and is outlined in Fig. <ref type="figure" coords="5,456.24,553.01,3.78,8.85">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5 IMPRF + PRF</head><p>Once the result of integration is similarly obtained with IMPRF as explained in 3.3, PRF is performed from the integration result. PRF is carried out by extracting 15 words from &lt;TITLE&gt;, &lt;DESCRIPTION&gt;, &lt;NOTES&gt;, and &lt;LOCATION&gt; of the annotations of the top 5 images of the retrieval result. TBIR is executed using words obtained by PRF. The retrieval results of TBIR and CBIR are integrated, and the final retrieval result is obtained. In this system, each weight is as follows: TBIR_weight = 0.5, CBIR_weight = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">IMPRF + CFS</head><p>This system combines CFS with IMPRF and is outlined in Fig. <ref type="figure" coords="6,339.36,91.01,3.78,8.85">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 6 IMPRF + CFS</head><p>If a query is input, first CBIR is executed. Next, query expansion is performed using the retrieval result of CBIR. Query expansion is carried out by extracting 15 words from annotations of the top 5 images obtained in the retrieval result of CBIR. Moreover, query expansion is performed using CFS for the words obtained by query expansion, and TBIR is executed. Finally, the retrieval results of TBIR and CBIR are integrated, and the final retrieval result is obtained. In this system, each weight is as follows: TBIR_weight = 0.5, CBIR_weight = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">CMPRF + CFS + PRF</head><p>This system is Cross Media Pseudo Relevance Feedback (CMPRF), in which IMPRF is used twice. Figure <ref type="figure" coords="6,514.32,349.73,5.04,8.85">7</ref> outlines the flow of CMPRF + CFS + PRF. Fig. <ref type="figure" coords="6,254.16,528.53,5.04,8.85">7</ref> CMPRF + CFS + PRF When a query is input, CBIR is executed first. Query expansion is performed next using the retrieval result of CBIR. Query expansion is performed by extracting 15 words from the annotations of the top 5 images of the retrieval result of CBIR. Moreover, query expansion is performed using CFS for the words obtained by query expansion, and TBIR is executed. Next, feature values are extracted from the top 5 images of the retrieval result of TBIR, the average of the feature values is obtained from these, and CBIR is executed again. The retrieval results of TBIR and CBIR are integrated, and a retrieval result is obtained. Moreover, PRF is performed using this retrieval result. TBIR is executed again using words obtained by PRF. Finally, the retrieval results of TBIR and CBIR are integrated, and the final retrieval result is obtained. In this system, each weight is as follows: TBIR_weight = 0.5, CBIR_weight = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">TBIR + CFS</head><p>This system combines CFS with TBIR and uses a text only approach. In section 2.2.1, the use of word vector and region ID vector was explained in terms of constructing the conceptual dictionary. However, the conceptual dictionary is constructed using only a word vector because this system uses text only. In addition, when a query expansion is performed, a word vector is generated from &lt;title&gt; and &lt;narr&gt; of the query, and this is assumed to be a feature vector. TBIR is executed as explained in 3.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Auto Feature Weighting</head><p>Auto Feature Weighting (AFW) is an approach using image only. This system examines how similar three query images are when retrieving, and the weight of feature values is dynamically changed according to the similarity. Figure <ref type="figure" coords="7,149.28,114.05,5.04,8.85">8</ref> is an outline of the flow of AFW. Fig. <ref type="figure" coords="7,251.28,233.33,5.04,8.85">8</ref> Auto Feature Weighting One way this system differs from CBIR explained in 3.1.2 is that the weights of color and texture of the global retrieval and grid retrieval change dynamically. This is represented by Auto in Figure <ref type="figure" coords="7,448.08,262.61,3.66,8.85">8</ref>.</p><p>If three query images are input, the distance between each query image in color and texture is calculated. The weight is obtained from the input distance. Because the similarity of the color and texture features is small if the distance is large, the weight is reduced. Because the similarity of the features is large if the distance is small, the weight is increased.</p><p>In addition, an inverse number of the average value is obtained by Eq. ( <ref type="formula" coords="7,371.67,319.97,3.51,8.85">5</ref>). The reason for this is that a large weight is given for the features that look alike because they look alike due to the small value to calculate the distance, as stated above. <ref type="bibr" coords="7,490.80,389.09,11.76,8.85" target="#b4">(5)</ref> where similarity indicates the distance between each query image, and the subscript indicates the compared images. For example with color_similarity 1-2 , the distance in the color feature between the first and second images of the query image is shown.</p><p>Weight is obtained by Eq. ( <ref type="formula" coords="7,198.80,467.09,3.92,8.85">6</ref>) from ave_color_similarity and ave_texture_similarity that were calculated by Eq. ( <ref type="formula" coords="7,97.59,478.61,3.51,8.85">5</ref>). <ref type="bibr" coords="7,490.80,512.93,11.76,8.85" target="#b5">(6)</ref> Whenever a query is input, these operations are performed, and CBIR is executed after calculating the weights. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><formula xml:id="formula_2" coords="7,209.52,494.09,170.66,47.41">_ _ _ _ _ _ _ _ _ _ _ _</formula><p>The Integration System had the highest accuracy in P@20 and CR20. As for this result, it was understood that integrating different media, i.e., TBIR and CBIR, was conducive to the higher retrieval result. Moreover, when looking at the mean average precision (MAP), which can comprehensively assess how well the retrieval system performs, the top three systems perform query expansion using CFS. This indicates that performing query expansion using CFS produces a higher retrieval result. Here, the notable conclusion is TBIR + CFS. Although this system approach uses text only, it retrieves with very high accuracy on the whole. However, it is clear that of the systems we submitted, the best accuracy is obtained using IMPRF + CFS. Although an approach using image only is not very accurate, it is understood that accuracy improves by integrating different media. Furthermore, when query expansion is used abundantly with one system (e.g., CMPRF + CFS + PRF), query expansion does not produce good results, and a lot of the same words appear. Although query expansion is one technique for increasing retrieval accuracy, it is apparent that greater frequency of query expansion will not result in higher accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>We demonstrated that retrieval accuracy improved by performing query expansion using CFS in image retrieval that integrates different media.</p><p>However, future tasks remain, and these are as follows.</p><p>It is necessary to improve the accuracy because the accuracy of CBIR is low. Because it is clear that accuracy improves by integrating TBIR and CBIR, it is assumed that higher accuracy of CBIR will lead to a further rise in overall accuracy. In addition, it is thought that accuracy will increase by revising the method of integration because simply adding TBIR and CBIR is not sufficient (e.g., Eq. 4). In other words, the method of integration plays a very important role when TBIR and CBIR are integrated.</p><p>Also, another problem is how to determine the initial point of clustering when the conceptual dictionary is constructed. This system sets the initial point at random. It is thought that accuracy will fall outside the accuracy range if another corpus is used, but there is the possibility that accuracy will be higher or lower than the results reported here when the conceptual dictionary is constructed anew. It is necessary to consider using another fuzzy clustering technique if the accuracy changes each time the conceptual dictionary is constructed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,228.72,202.85,137.84,8.85"><head></head><label></label><figDesc>Fig. 3 Integration System + CFS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,114.72,265.73,365.76,85.65"><head>Table 1</head><label>1</label><figDesc>Feature values of region (numbers in parentheses indicate number of dimensions)</figDesc><table /><note coords="4,189.12,280.61,24.15,8.85;4,237.84,280.37,161.28,8.85;4,237.84,292.85,156.24,8.85;4,189.12,305.33,32.55,8.85;4,237.84,305.09,93.36,8.85;4,189.12,317.33,26.07,8.85;4,237.84,317.09,99.60,8.85;4,189.12,329.09,210.72,9.09;4,189.12,342.53,17.43,8.85;4,237.84,342.29,84.48,8.85"><p>Color RGB average (3), standard deviation (3) Lab average (3), standard deviation (3) Texture Wavelet transform (24) Shape Z-Fourier descriptors (8) Position X and Y coordinates of median point (2) Size Number of pixels (1)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,84.96,358.11,418.95,392.15"><head>Table 2</head><label>2</label><figDesc>lists the experimental results of the systems submitted.</figDesc><table coords="7,86.64,358.11,417.27,392.15"><row><cell>ave</cell><cell>_</cell><cell cols="2">color</cell><cell>_</cell><cell cols="2">simlarity</cell><cell></cell><cell>color</cell><cell>_</cell><cell cols="3">similarity</cell><cell>1</cell><cell>2</cell><cell></cell><cell cols="2">color</cell><cell>_</cell><cell cols="2">1 similarity</cell><cell>1</cell><cell>3</cell><cell>color</cell><cell>_</cell><cell>similarity</cell><cell>2</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3</cell></row><row><cell>ave</cell><cell>_</cell><cell cols="3">texture</cell><cell>_</cell><cell cols="2">simlarity</cell><cell cols="3">texture</cell><cell>_</cell><cell cols="3">similarity</cell><cell>1</cell><cell>2</cell><cell cols="3">texture</cell><cell>_</cell><cell>1 similarity</cell><cell>1</cell><cell>3</cell><cell>texture</cell><cell>_</cell><cell>similarity</cell><cell>2</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell></row><row><cell cols="3">Weight</cell><cell cols="3">color</cell><cell>ave</cell><cell></cell><cell>color</cell><cell></cell><cell></cell><cell cols="6">similarity color ave</cell><cell cols="4">texture similarity ave</cell><cell>similarity</cell></row><row><cell cols="3">Weight</cell><cell cols="3">texture</cell><cell>ave</cell><cell></cell><cell cols="2">color</cell><cell></cell><cell></cell><cell cols="6">similarity texture ave</cell><cell cols="3">texture similarity ave</cell><cell>similarity</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="15">Table 2 Experimental results of photo retrieval task</cell></row><row><cell cols="3">RunID</cell><cell></cell><cell></cell><cell></cell><cell cols="5">Language</cell><cell></cell><cell cols="5">Modality</cell><cell></cell><cell></cell><cell cols="2">P@20</cell><cell>CR20</cell><cell>MAP</cell><cell>GMAP</cell></row><row><cell cols="6">Integration System</cell><cell></cell><cell cols="3">EN -EN</cell><cell></cell><cell></cell><cell cols="5">TXTIMG</cell><cell></cell><cell></cell><cell cols="2">0.4282</cell><cell>0.3975</cell><cell>0.2940</cell><cell>0.1823</cell></row><row><cell cols="7">Integration System + CFS</cell><cell cols="3">EN -EN</cell><cell></cell><cell></cell><cell cols="5">TXTIMG</cell><cell></cell><cell></cell><cell cols="2">0.4218</cell><cell>0.3297</cell><cell>0.2966</cell><cell>0.1697</cell></row><row><cell cols="3">IMPRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">EN -EN</cell><cell></cell><cell></cell><cell cols="5">TXTIMG</cell><cell></cell><cell></cell><cell cols="2">0.3949</cell><cell>0.3223</cell><cell>0.2711</cell><cell>0.1522</cell></row><row><cell cols="5">IMPRF + PRF</cell><cell></cell><cell></cell><cell cols="3">EN -EN</cell><cell></cell><cell></cell><cell cols="5">TXTIMG</cell><cell></cell><cell></cell><cell cols="2">0.4038</cell><cell>0.2861</cell><cell>0.2921</cell><cell>0.1368</cell></row><row><cell cols="5">IMPRF + CFS</cell><cell></cell><cell></cell><cell cols="3">EN -EN</cell><cell></cell><cell></cell><cell cols="5">TXTIMG</cell><cell></cell><cell></cell><cell cols="2">0.4231</cell><cell>0.3226</cell><cell>0.3032</cell><cell>0.1676</cell></row><row><cell cols="7">CMPRF + CFS + PRF</cell><cell cols="3">EN -EN</cell><cell></cell><cell></cell><cell cols="5">TXTIMG</cell><cell></cell><cell></cell><cell cols="2">0.3974</cell><cell>0.2858</cell><cell>0.2870</cell><cell>0.1364</cell></row><row><cell cols="4">TBIR + CFS</cell><cell></cell><cell></cell><cell></cell><cell cols="3">EN -EN</cell><cell></cell><cell></cell><cell cols="4">TXT</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.4051</cell><cell>0.3966</cell><cell>0.3015</cell><cell>0.1844</cell></row><row><cell cols="3">AFW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">EN -EN</cell><cell></cell><cell></cell><cell cols="4">IMG</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.0051</cell><cell>0.0212</cell><cell>0.0005</cell><cell>0.0002</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,92.16,435.17,256.70,8.85" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Imageclefphoto</surname></persName>
		</author>
		<ptr target="http://www.imageclef.org/2008/photo" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.16,458.21,432.00,8.85;8,92.16,469.73,432.12,8.85;8,92.16,481.25,22.68,8.85" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,351.46,458.21,172.70,8.85;8,92.16,469.73,35.57,8.85">Conceptualized Queries for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Yan-Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiroshi</forename><surname>Sekiya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomohiro</forename><surname>Takagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,138.96,469.73,380.64,8.85">Proc. of North American Fuzzy Information Processing Society Annual Conference (NAFIPS)</title>
		<meeting>of North American Fuzzy Information essing Society Annual Conference (NAFIPS)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.16,504.29,432.63,8.85;8,92.16,515.57,432.48,8.85;8,92.16,527.09,86.52,8.85" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,382.16,504.29,142.63,8.85;8,92.16,515.57,373.23,8.85">CINDI at ImageCLEF 2006: Image Retrieval &amp; Annotation Tasks for the General Photographic and Medical Image Collections</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Varun</forename><surname>Sood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bipin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prabir</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,476.88,515.57,47.76,8.85;8,92.16,527.09,57.14,8.85">CLEF 2006 Working Notes</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.16,550.13,175.11,8.85" xml:id="b3">
	<monogr>
		<ptr target="http://lucene.apache.org/" />
		<title level="m" coord="8,92.16,550.13,60.62,8.85">Apache Lucene</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.16,573.17,432.39,8.85;8,92.16,584.69,329.64,8.85" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,500.43,573.17,24.13,8.85;8,92.16,584.69,40.31,8.85">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Micheline</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,156.00,584.69,236.06,8.85">Proceedings of the Third Text Retrieval Conference (TREC)</title>
		<meeting>the Third Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.16,607.73,432.15,8.85;8,92.16,619.25,285.48,8.85" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,222.96,607.73,293.08,8.85">Unsupervised segmentation of color-texture regions in images and video</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,92.16,619.25,235.35,8.85">Transactions of Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.16,642.05,432.39,8.85;8,92.16,653.57,432.12,8.85;8,92.16,665.09,22.68,8.85" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,447.92,642.05,76.63,8.85;8,92.16,653.57,309.02,8.85">IPAL Inter-Media Pseudo-Relevance Feedback Approach to ImageCLEF 2006 Photo Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Maillot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Pierre</forename><surname>Chevallet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vlad</forename><surname>Valea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joo Hwee</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,412.08,653.57,108.02,8.85">CLEF 2006 Working Notes</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
