<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,117.84,98.73,367.17,15.51;1,94.20,120.69,414.41,15.51;1,200.40,142.53,202.01,15.51">Affinity propagation promoting diversity in visuo-entropic and text features for CLEF Photo retrieval 2008 campaign</title>
				<funder ref="#_MVj5wRy">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,223.80,176.03,68.44,9.96"><forename type="first">Herve</forename><surname>Glotin</surname></persName>
							<email>glotin@univ-tln.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire des sciences de l&apos;information et des systemes</orgName>
								<orgName type="institution" key="instit1">UMR CNRS</orgName>
								<orgName type="institution" key="instit2">Universite&apos; Sud Toulon-Var France</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.63,176.03,72.54,9.96"><forename type="first">Zhongqiu</forename><surname>Zhao</surname></persName>
							<email>zhongqiuzhao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire des sciences de l&apos;information et des systemes</orgName>
								<orgName type="institution" key="instit1">UMR CNRS</orgName>
								<orgName type="institution" key="instit2">Universite&apos; Sud Toulon-Var France</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,117.84,98.73,367.17,15.51;1,94.20,120.69,414.41,15.51;1,200.40,142.53,202.01,15.51">Affinity propagation promoting diversity in visuo-entropic and text features for CLEF Photo retrieval 2008 campaign</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8B637C486BAE103ED33A7BC6981F7F36</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Infor-mation Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Rank Fusion, Image Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop for the CLEF PHOTO 2008 task a new visual features using various pixel projections for training SVMs, allowing us to produce image retrieval and clustering using affinity propagation. To heighten the diversity of the top of the retrieval results, we put the images with the lowest rank in each cluster into the top. The LSIS run which used only the visual information is at the 6th best team rank in the AUTO IMG run type. For AUTO TXTIMG runs, we merge by simple harmonic or arithmetic average our visual ranks to the textual ranks of the LIG language model participating to the AVEIR consortium. Then we also perform the affinity propagation and the reranking on this TXTIMG run, which gives complementary information to the AVEIR consortium, helping in producing the third best AUTO TXTIMG run (after XEROX). We discuss on the clustering performance of the various run types, and then we give some perspectives for enhancing such diversity image retrieval system. If affinity propagation clustering seems efficient for promoting visual diversity, our results show that clustering process itself should merge independant textual and visual clustering informations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction to ImageCLEF2008 Photo Retrieval Task</head><p>ImageCLEF <ref type="bibr" coords="1,147.43,681.11,10.55,9.96">[2]</ref> is the cross-language image retrieval track run as part of the Cross Language Evaluation Forum (CLEF) campaign. This track evaluates retrieval of images described by text captions based on queries in a different language; both text and image matching techniques are potentially exploitable. The photo retrieval task of ImageCLEF2008 is taking a different approach to evaluation by studying image clustering. A good image search engine ensures that duplicate or near duplicate documents retrieved in response to a query are hidden from the user. Ideally the top results of a ranked list contains diverse items representing different sub-topics within the results. Providing this functionality is particularly important when a user types in a query that is either poorly specified or ambiguous; a common query in image search. Given such a query, a search engine that retrieves a diverse, yet relevant set of images at the top of a ranked list is more likely to satisfy its users <ref type="bibr" coords="2,198.37,157.07,10.55,9.96" target="#b0">[1,</ref><ref type="bibr" coords="2,208.92,157.07,7.03,9.96">2]</ref>.</p><p>Participants to ImageClef Photo run each provided topic on their image search system and produce a ranking that in the top 20, holds as many relevant images that are representative of the different sub-topics within the results. The definition of what consitutes diversity varies across the topics [2], indicated by a topic tag, "cluster" giving what the clustering criteria the evaluators use. For each topic in the ImageCLEFPhoto set, relevant images are manually clustered into sub-topics and relevance judgements will be augmented to indicate which cluster an image belongs to. For example if a topic asks for images of beaches in Brazil, clusters are formed based on location; if a topic asks for photos of animals, clusters are formed based on animal type.</p><p>The CLEF image challenge is running on the image collection of the IAPR TC-12 photographic collection provided for this task consists of 20,000 still natural images (plus 20,000 corresponding thumbnails) taken from locations around the world and comprising an assorted cross-section of still natural images <ref type="bibr" coords="2,157.98,300.59,9.98,9.96">[2]</ref>. This includes pictures of different sports and actions, photographs of people, animals, cities, landscapes and many other aspects of contemporary life. Each image is associated with an alphanumeric caption stored in a semi-structured format. These captions include the title of the image, its creation date, the location at which the photograph was taken, the name of the photographer, a semantic description of the contents of the image (as determined by the photographer) and additional notes. Figure <ref type="figure" coords="2,283.64,360.35,5.03,9.96" target="#fig_0">1</ref> shows an example for the image collection and the topic list is given in table <ref type="table" coords="2,204.04,372.23,3.90,9.96" target="#tab_0">1</ref>.</p><p>These paper first describes LSIS entropic features, LS-SVM and affinity propagation. Then we precise the LSIS runs method, before to detail and compare the results in the last section. The conclusion gives finally some strategies to enhance the clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LSIS Profil Entropic Feature Extraction</head><p>An important step in content-based image retrieval (CBIR) system is the extraction of discriminant visual feature that are fast to compute. Information theory and Cognitive sciences can provide some inspiration for developping such feature.</p><p>Among the many visual features that have been studied, the distribution of color pixels in an image is the most common visual feature studied. The standard representation of color for content-based indexing in image databases is the color histogram. A different color representation is based on the information theoretic concept of entropy. Such entropic feature can simply equal the entropy of the pixel distribution of the image, as proposed in <ref type="bibr" coords="2,357.28,546.59,9.98,9.96" target="#b1">[3]</ref>. A more theoretical presentation of this kind of image entropy feature, accompanied by a practical description of its merits and limitations compared to color histograms, has been given in <ref type="bibr" coords="2,353.78,570.47,9.98,9.96" target="#b2">[4]</ref>.</p><p>We propose in <ref type="bibr" coords="2,172.15,582.47,11.00,9.96" target="#b3">[5,</ref><ref type="bibr" coords="2,183.16,582.47,7.34,9.96" target="#b4">6]</ref> a new feature equal to the pixel 'profil' entropy. A pixel profil can be a simple arithmetic mean in horizontal (or vertical) direction. The advantage of such feature is to combine raw shape and texture representations in a low cpu cost feature. These feature, associated to mean and color std, reached the second best rank in the official ImagEval 2006 campaing (see www.imageval.org and <ref type="bibr" coords="2,191.55,630.35,10.29,9.96" target="#b4">[6]</ref>).</p><p>In this paper we extend these features using another projection to get the pixel profil. We then propose also to use the harmonic mean of the pixel of each lign or column. The idea is that the object or pixel region distribution, which is lost in arithmetic mean projection, could be partly catch by the harmonic mean. These two projections are then expected to give complementary and/or concept dependant informations. We detail below the algorithm of the Profil Entropy New num. of each topic  Let I be an image, or any rectangular subpart of an image. For each normalized color (L = R + G + B, r = R/L, and g = G/L), we first calculate two orthogonal profils by the projections of the pixels of I. We consider two simple orthogonal projection axes : the horizontal axis X (noted Π X ), versus the vertical one Y (noted Π Y ). The projection operator is either the arithmetic mean (noted 'Ar', then the projection is noted Π Ar X ), as illustrated in Figure <ref type="figure" coords="4,404.20,434.03,3.90,9.96" target="#fig_1">2</ref>, or the harmonic mean of the pixels on each column or each lign of I (noted 'Ha', then we have Π Ha X ). Then, we estimate the probability distribution function (pdf) of each profil according to <ref type="bibr" coords="4,499.39,457.91,9.98,9.96" target="#b5">[7]</ref>. Considering that the sources are ergodic, we finaly calculate each PEF equal to the normalized entropy (H(pdf )/log(#bins(pdf ))). We detail below each steps of the PEF extraction. Let be op the selected projection, for each color of I of L(I) ligns and C(I) columns : Φ op X (I) = p df (Π op X (I)), over nbin X (I) = round( C(I)) bins, where Π op X is the vertical projection with operator op, P EF</p><formula xml:id="formula_0" coords="4,90.00,539.37,275.85,49.96">X (I) = H(Φ op X (I))/log(nbin X (I)). Φ op Y (I) = p df (Π op Y (I)), over nbin Y (I) = round( L(I)) bins, P EF Y (I) = H(Φ op Y (I))/log(nbin Y (I)).</formula><p>We add to these P EF a the image entropic feature <ref type="bibr" coords="4,327.26,601.43,10.55,9.96" target="#b1">[3,</ref><ref type="bibr" coords="4,337.81,601.43,7.03,9.96" target="#b2">4]</ref>: p df (I) = pdf of all the pixels of I over nbin XY (I) = nbin X (I) * nbin Y (I) bins,</p><formula xml:id="formula_1" coords="4,90.00,624.83,170.38,12.97">P EF . (I) = H( p df (I))/log(nbin XY (I)).</formula><p>We finaly complete the PEF features by the usual mean and standard deviation of each normalized color of I. Like in our VCDT IAPR CLEF system <ref type="bibr" coords="4,343.11,663.35,9.98,9.96" target="#b6">[8]</ref>, we can calculate the PEF into three horizontal (versus vertical) subimages. For each, we have 3 bands and 3 different PEF for each of the 3 colors, plus their mean and variance, thus we have 3 * 3 * 3 + 3 * 3 * 2 = 45 dimensions for vertical and horizontal subimage features, for a total of 90 features by images for one projection type. Details can be found in <ref type="bibr" coords="5,221.31,73.43,9.98,9.96" target="#b6">[8]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Support Vector Machines</head><p>In this task, we used the support vector machine (SVM) to implement image retrieval. The working mechanism of the SVM <ref type="bibr" coords="5,230.66,422.15,15.58,9.96" target="#b11">[13]</ref> is first to map the data into a higher dimensional input space by some kernel functions, and then to learn a separating hyperspace to maximize the margin. Currently, because of its good generalization capability, this technique has been widely applied in many areas such as face detection, image retrieval, and so on. The SVM is typically based on an ε-insensitive cost function, meaning that approximation errors smaller than will not increase the cost function value. This results in a quadratic convex optimization problem. So instead of using an ε-insensitive cost function, a quadratic cost function can be used. The least squares support vector machines (LS-SVM) are reformulations to the standard SVMs which lead to solving linear KKT systems instead <ref type="bibr" coords="5,187.23,517.79,14.67,9.96" target="#b12">[14]</ref>, it is then computationally attractive.</p><p>In our experiments we use LS-SVM with the RBF kernel</p><formula xml:id="formula_2" coords="5,225.72,547.76,151.56,11.55">K(x 1 -x 2 ) = exp(-|x 1 -x 2 | 2 /σ 2 )</formula><p>. So there is a corresponding parameter, σ , to be tuned. A large value of σ 2 indicates a stronger smoothing. Moreover, there is another parameter, γ, needing tuning to find the tradeoff between to stress minimizing of the complexity of the model and to stress good fitting of the training data points.</p><p>In the experiment, we train for each topic an hundred of SVM with different σ and γ, and we selected the best SVM using a validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Affinity Propagation</head><p>We first tried to use Clef Visual Concept models to clusterize the top 20 answers, but the result was not interesting, thus we changed for a recent clustering method : the affinity propagation clustering. The advantages of affinity propagation clustering <ref type="bibr" coords="6,300.99,73.43,7.99,9.96" target="#b7">[9]</ref><ref type="bibr" coords="6,308.98,73.43,4.00,9.96" target="#b8">[10]</ref><ref type="bibr" coords="6,308.98,73.43,4.00,9.96" target="#b9">[11]</ref><ref type="bibr" coords="6,312.98,73.43,11.99,9.96" target="#b10">[12]</ref> over other clustering methods lie in that it's more stable for different initializations. In affinity propagation clustering, two kinds of message are exchanged between data points, each of which takes into account a different kind of competition. Messages can be combined at any stage to decide which points are exemplars and, for every other point, which exemplar it belongs to. The "responsibility" r(i, k), sent from data point i to candidate exemplar point k, reflects the accumulated evidence for how well-suited point k is to serve as the exemplar for point i, taking into account other potential exemplars for point i. The "availability" a(i, k), sent from candidate exemplar point k to point i, reflects the accumulated evidence for how appropriate it would be for point i to choose point k as its exemplar, taking into account the support from other points that point k should be an exemplar. To begin with, the availabilities are initialized as a(i, k) = 0, and the responsibilities are initialized as r(i, k) = 0. Then, the responsibilities and availabilities are iteratively computed as:</p><formula xml:id="formula_3" coords="6,165.72,226.55,271.24,82.57">r(i, k) ← s(i, k) -max k =k {a(i, k ) + s(i, k )} a(i, k) ← min{0, r(k, k) + i =i&amp;i =k max{0, r(i , k)}}, f or i = k a(k, k) ← i =k max{0, r(i , k)}</formula><p>where s(i, k) reflects the similarity between the data points i and k. For all i = k , s(i, k) can be set to be the negative Euclidean distance, namely, s(i, k) = -x i -x k 2 ; while for all i = k, s(k, k) is a varying parameter, and the initialized values of s(k, k) for all ks are set to be equal to each other because all data points are equally suitable as exemplars. The affinity propagation takes a real number s(k, k) as its input. The number of identified exemplars (number of clusters) is influenced by the initialized value of s(k, k) . As reported in the literature <ref type="bibr" coords="6,421.06,376.67,14.67,9.96" target="#b13">[15]</ref>, the shared value of s(k, k) is set as the median of the input similarities (resulting in a moderate number of clusters) or their minimum (resulting in a small number of clusters). However, the true number of clusters may be a widely changeful value, but not exactly the moderate number or the small number. So in our design, we set the initialized value of s(k, k) varying from min i,j s(i, j) to their maximum max i,j s(i, j) , namely:</p><formula xml:id="formula_4" coords="6,195.84,458.15,211.17,15.13">s(k, k) = min i,j s(i, j) + α(max i,j s(i, j) -min i,j s(i, j))</formula><p>where α ∈ [0, 1].</p><p>In photo task, we set α to be 0.2 in order that the number of clusters we get is approximately equal to 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>AVEIR (Automatic annotation and Visual concept Extraction for Image Retrieval) is the name of a project supported by the French National Agency of Research (ANR-06-MDCA-002). A consortium of four French CNRS research laboratories are involved in this project <ref type="bibr" coords="6,453.76,587.51,14.67,9.96" target="#b15">[17]</ref>. In order to compare the state of the art, each of the partners participated individually to ImageCLEFphoto, and to analyze if the fusion of runs, based on different strategies, can bring diversity, a submission under the label AVEIR was proposed. <ref type="bibr" coords="6,252.06,623.39,17.19,9.96" target="#b13">[15,</ref><ref type="bibr" coords="6,269.25,623.39,12.89,9.96" target="#b14">16,</ref><ref type="bibr" coords="6,282.14,623.39,12.89,9.96" target="#b15">17]</ref>.</p><p>In our experiments, we computed the weighted averages of two ranks : our visual system described in this paper, and the LIG TXTIMG <ref type="bibr" coords="6,299.02,647.27,15.58,9.96" target="#b13">[15]</ref> using a model language after Porter process.</p><p>The process we adopt to implement the image retrieval in photo task is shown in fig. <ref type="figure" coords="6,485.22,659.27,3.90,9.96">3</ref>, and depicted by the following steps:</p><p>Step 1) According to the keywords of each topic, perform the text retrieval on the XML text database, and then get the TXT rank (please refer to the LIG Photo Clef paper <ref type="bibr" coords="6,442.85,695.03,14.78,9.96" target="#b13">[15]</ref>).</p><p>Figure <ref type="figure" coords="7,158.01,242.51,7.79,9.96">3:</ref> The training framework of our image retrieval system, t is fixed to 0.5</p><p>Step 2) Extract the visual features from the training image data using our extraction method; train and generate an hundrer of SVMs with different parameters.</p><p>Step 3) Use the first 20 images in TXT ranks as the positive samples, and the others as the negative samples to construct the validation set; select the best one among the SVMs.</p><p>Step 4) Extract the visual features from the visual image database using our extraction method; use the best SVM as the tool to perform the image retrieval and produce the rank result called IMG rank.</p><p>Step 5) Merge IMG and TXT rank into Rank-without-Clustering, where 't' in the figure denotes the text ratio, in our experiments t=0.5.</p><p>Step 6) Perform the clustering on the top 1000 images in Rank-without-Clustering for each topic, using affinity propagation.</p><p>Step 7) Select the image with the lowest rank in each cluster, put these images in their old order from Rank-without-Clustering and on the top of 'Final Rank'; then they are followed by the others in the old order.</p><p>We submitted 15 runs, one of which used only the visual information, others used both text and visual information. We make the fusions with either arithmetic or harmonic means.</p><p>Evaluation are based on two measures: precision at 20 and instance recall at rank 20 (also called S-recall) [2], which calculates the percentage of different clusters represented in the top 20. It will be important to maximise both measures: simply getting lots of relevant images from one cluster or filling the ranking with diverse, but non-relevant images, will result in a poor overall effectiveness score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visual only run</head><p>The first run is a visual only run. We simply train several SVMs on the training set, optimised with the TEXT AVEIR preprocess without clustering. Then we make 20 clusters using affinity propagation on the top 1000 images for each topic and we place the best (the image with the lowest rank) of each cluster to the top 20 (nearly), and we keep the rest of the list as the order before the clustering. This baseline LSIS IMG+CLUSTER (RUN1) is the 6th best team run in the Auto IMG run type [2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Image and Text fusion</head><p>We use LIG TXTIMG to train SVMs and to make fusions, then we make 20 clusters using affinity propagation on the top 1000 images for each topic after the fusion of visual and LIG TXTIMG and we place the best (the image with the lowest rank) of each cluster to the top 20 (nearly), and we keep the rest of the list as before the clustering. It is important to note that here the visual and text information are merged before the clustering. We will show in the end of the paper that it could be more efficient to make affinity propagation on visual and on textual ranking, and then to merge them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Conclusion</head><p>The results for visual on only LSISrun1(IMG) with clustering are given in table 2. For comparison we give the results of the other runs of the same type submitted to CLEF. The LSIS visual only system seems quite efficient : using the low dimensional PEF features the LSIS team rank in the IMG run type is 6th. Moreover, this visual information, giving complementary information, enhanced the AVEIR consortium run (third rank at the IMGTXT run type).</p><p>The results for combination of visual with textual informations (IMG+TXT), with (run12) or not (run0) clustering, are given in table <ref type="table" coords="8,267.22,503.75,3.90,9.96">3</ref>. For comparison we give the results of few runs of the same type submitted to CLEF, and some runs of AVEIR consortium in which our IMG+TXT LSISrun0 has been merged.</p><p>Because of the large variation of the considered topics (see tab. 1), the clustering evaluation must be analysed at the topic level. We then analyse for each topic in the next figure the CR20 after affinity propagation of each topics (numeroted from 1 to 39) for LSIS run1 (IMG) vs LSIS run12 (TXT IMG), see fig 4. The correlation between the two runs is low (0.3). We see clearly that some topics like 14 or 28 are difficult to cluster, contrary to 6 or 33 topics. Moreover if for most of the topics the CR20 is improved, we see the inversed for some topics.</p><p>To detail the impact of the text information to the cluster quality, we plot in fig. <ref type="figure" coords="8,468.32,611.39,5.03,9.96" target="#fig_4">5</ref> the gain values for each run between these two runs. We see then clearly that the global gain of nearly 68% is not uniform over each topic. If the majority of the CR20 are, some topics are better clusterised by affinity propagation using only the visual ranking. These variations need more research for being well interpreted.</p><p>The next figure shows the gain of CR20 from the LSIS visual only TXTIMG run to the LSIS TXTIMG+ affinity propagation CLUSTER (see fig. <ref type="figure" coords="8,322.36,683.15,3.88,9.96" target="#fig_6">6</ref>). The global gain is low (3.7%) but again the variation for each topic is high : if for some topics the text information itself allows an efficient implicit image clustering, the visual clustering alone is fondamental for some other topics. This could be explained by the fact that some clusters are more or less high level clusters. In other terms, some clusters may be more cultural (cities, country,...) than being only based on visual criteria.</p><p>Even if affinity propagation clustering seems efficient for promoting visual diversity, our results show that visual and textual information brings complementary clustering informations, which should be weigthed according to each topic. We shown that linear weighted fusion was efficient for topic retrieval in ImagEval campaign <ref type="bibr" coords="9,274.27,439.31,9.98,9.96" target="#b4">[6]</ref>. In this paper the visual and text information are merged before the clustering, so we can't weight textual and visual clustered ranks. It may be more efficient to make affinity propagation separatly on visual and on textual ranks, and then to merge them. Further works will be conducted for designing such simple clustering linear weighting fusion schemes.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,201.00,342.35,200.95,9.96;4,111.12,58.98,380.70,269.00"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An example for the image collection</figDesc><graphic coords="4,111.12,58.98,380.70,269.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,90.00,322.91,422.77,9.96;5,90.00,334.91,288.83,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the horizontal and vertical profils using simple arithmetic projection (or sum) of each normalized color r = R/L, g = G/L, L = R + G + B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,90.00,311.27,422.63,9.96;9,90.00,323.27,272.88,9.96"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effect of TXT to the CR20 given for each topics (numeroted from 1 to 39) for LSIS run 1 (IMG+CLUSTER) versus run 12 (TXT+IMG+CLUSTER).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,177.91,275.97,3.29,5.48;10,210.97,275.97,3.29,5.48;10,242.41,275.97,6.59,5.48;10,275.52,275.97,6.59,5.48;10,308.59,275.97,6.59,5.48;10,341.65,275.97,6.59,5.48;10,374.77,275.97,6.59,5.48;10,407.83,275.97,6.59,5.48;10,440.94,275.97,6.59,5.48;10,164.49,270.98,13.34,5.48;10,167.79,236.19,10.04,5.48;10,174.55,201.45,3.29,5.48;10,171.25,166.71,6.59,5.48;10,167.94,131.96,9.88,5.48;10,167.94,97.22,9.88,5.48;10,167.94,62.48,9.88,5.48;10,156.19,233.84,5.48,11.19;10,156.19,222.98,5.48,9.21;10,156.19,161.77,5.48,57.92;10,156.19,141.03,5.48,19.09;10,156.19,92.65,5.48,46.74;10,205.89,283.02,212.98,5.48"><head></head><label></label><figDesc>) LSIS-CLUS-IMGTXT against LSIS-CLUS-IMG topics renumeroted from 1 to 39. gain &gt; 200 are repres. as 200. Global gain = 67.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,90.00,304.31,422.73,9.96;10,90.00,316.31,323.97,9.96"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effect of TXT to the CR20. Gains of the CR20 of each topics (numeroted from 1 to 39) for LSIS run1 (IMG+CLUSTER) versus run12 (TXT+IMG+CLUSTER).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="11,174.61,278.56,3.33,5.54;11,208.08,278.56,3.33,5.54;11,239.89,278.56,6.67,5.54;11,273.41,278.56,6.67,5.54;11,306.88,278.56,6.67,5.54;11,340.34,278.56,6.67,5.54;11,373.86,278.56,6.67,5.54;11,407.32,278.56,6.67,5.54;11,440.84,278.56,6.67,5.54;11,164.37,273.51,10.17,5.54;11,171.21,231.30,3.33,5.54;11,167.87,189.10,6.67,5.54;11,164.52,146.89,10.00,5.54;11,164.52,104.68,10.00,5.54;11,164.52,62.48,10.00,5.54;11,155.97,220.85,5.54,20.65;11,155.97,160.56,5.54,58.63;11,155.97,139.57,5.54,19.32;11,155.97,98.76,5.54,39.14;11,239.94,285.70,141.10,5.54"><head></head><label></label><figDesc>) LSIS-CLUS-IMGTXT against LSIS-IMGTXT topics renumeroted from 1 to 39. The global gain=3.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,90.00,307.19,422.76,9.96;11,90.00,319.07,422.81,9.96;11,90.00,331.07,222.50,9.96"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Effect of Affinity Propagation to IMG+TXT CR20 score. Gains of the CR20 of each topics (numeroted from 1 to 39) for LSIS (IMG+TXT) run versus LSIS run12 (TXT+IMG+CLUSTER). Gains &gt; 200 are = 200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,153.48,58.94,296.10,169.08"><head></head><label></label><figDesc></figDesc><graphic coords="7,153.48,58.94,296.10,169.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,147.84,124.31,320.62,514.20"><head>Table 1 :</head><label>1</label><figDesc>Topics definitions (and numerotations) of the PhotoClef 2008</figDesc><table coords="3,312.52,124.31,93.77,9.96"><row><cell>Topic short definition</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,90.00,61.43,455.43,256.56"><head>Table 2 :</head><label>2</label><figDesc>Best team run for Automatic visual only (AUTO IMG). LSIS is in the top 6.</figDesc><table coords="8,90.00,73.67,455.43,244.32"><row><cell cols="2">Final Avg.</cell><cell cols="2">P20 CR20</cell><cell cols="2">group with run</cell><cell>P20 CR20 MAP</cell></row><row><cell cols="4">Rank Rank Rank Rank</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell cols="2">DCU EN-EN-AUTO-IMG.txt</cell><cell>0,237 0,324 0,107</cell></row><row><cell>2</cell><cell>3,5</cell><cell>4</cell><cell>3</cell><cell cols="2">PTECH EN-EN-AUTO-IMG-AINQN.run</cell><cell>0,200 0,318 0,086</cell></row><row><cell>3</cell><cell>5</cell><cell>1</cell><cell>9</cell><cell cols="3">NTU IMG-EN-AUTO-NOFB-TXTIMG.result 0,309 0,178 0,210</cell></row><row><cell>4</cell><cell>5,5</cell><cell>5</cell><cell>6</cell><cell cols="2">IPAL 01V-4RUNS-EQWEIGHT</cell><cell>0,199 0,234 0,084</cell></row><row><cell>5</cell><cell>6</cell><cell>8</cell><cell>4</cell><cell cols="2">Ottawa UOt05-EN-EN-AUTO-IMG-KM.txt</cell><cell>0,159 0,269 0,069</cell></row><row><cell>6</cell><cell>7</cell><cell>9</cell><cell>5</cell><cell cols="3">LSIS EN-EN-AUTO-IMG-AUTO-GLOZHA-1 0,128 0,237 0,062</cell></row><row><cell>7</cell><cell>7,5</cell><cell>7</cell><cell>8</cell><cell cols="2">CLaC IR-EN-EN-AUTO-IMG.txt</cell><cell>0,161 0,215 0,055</cell></row><row><cell>8</cell><cell>8,5</cell><cell>10</cell><cell>7</cell><cell cols="2">MMIS cbir+brfHaiming.txt</cell><cell>0,123 0,229 0,033</cell></row><row><cell cols="7">Table 3: Fusion results of LSIS and of some reference runs (AUTO IMGTXT) with or without</cell></row><row><cell>clustering.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Group</cell><cell cols="2">Cluster or not P20 CR20 MAP</cell></row><row><cell cols="5">CLEFphoto2008 average of the 100 run from the 25 groups</cell><cell>CL</cell><cell>0.320 0.350 0.219</cell></row><row><cell></cell><cell></cell><cell cols="3">LIG TXTIMG (for AVEIR)</cell><cell>NO CL</cell><cell>0.303 0.380 0.212</cell></row><row><cell></cell><cell cols="4">LSIS TXTIMG (for AVEIR -run0)</cell><cell>NO CL</cell><cell>0.292 0.383 0.155</cell></row><row><cell></cell><cell cols="4">LSIS TXTIMG+CLUSTER (run12)</cell><cell>CL</cell><cell>0.300 0.400 0.160</cell></row><row><cell cols="5">AVEIR fusion = average(PTECH, LSIS, LIG, LIP6 runs)</cell><cell>CL</cell><cell>0.420 0,463 0,303</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Best run (XEROX)</cell><cell>CL</cell><cell>0.511 0.426 0.366</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>We thank <rs type="person">P. Mulhem</rs> from <rs type="affiliation">LIG</rs> for having provided the IMGTXT runs in the AVEIR consortium image CLEF. This work was partially supported by the <rs type="funder">French National Agency of Research</rs> (<rs type="grantNumber">ANR-06-MDCA-002</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MVj5wRy">
					<idno type="grant-number">ANR-06-MDCA-002</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.51,620.51,402.26,9.96;9,110.52,632.51,402.24,9.96;9,110.52,644.51,402.08,9.96;9,110.52,656.39,137.73,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,383.65,620.51,129.12,9.96;9,110.52,632.51,255.53,9.96">The IAPR TC-12 Benchmark: A New Evaluation Resource for Visual Information Systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,385.56,632.51,127.20,9.96;9,110.52,644.51,402.08,9.96;9,110.52,656.39,37.76,9.96">Proceedings of Int. Workshop OntoImage2006 Language Resources for Content-Based Image Retrieval, in conjuction with LREC&apos;06</title>
		<meeting>Int. Workshop OntoImage2006 Language Resources for Content-Based Image Retrieval, in conjuction with LREC&apos;06<address><addrLine>Genova</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.51,347.87,402.15,9.96;10,110.52,359.75,395.42,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,177.29,347.87,335.37,9.96;10,110.52,359.75,133.27,9.96">Saliency maps and attention selection in scale and spatial coordinates: An information theoretic approach</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,263.16,359.75,212.23,9.96">Proc. of 5th Int. Conference on Computer Vision</title>
		<meeting>of 5th Int. Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.51,379.31,402.24,9.96;10,110.52,391.19,402.14,9.96;10,110.52,403.19,402.14,9.96;10,110.52,415.19,36.30,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,278.60,379.31,234.15,9.96;10,110.52,391.19,402.14,9.96;10,110.52,403.19,28.68,9.96">Content based image retrieval and information theory: A generalized approach, in Special Topic Is-sue on Visual Based Retrieval Systems and Web Mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barhen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,147.59,403.19,316.91,9.96">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="page" from="841" to="853" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.51,434.63,402.13,9.96;10,110.52,446.63,402.35,9.96" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,161.13,434.63,351.51,9.96;10,110.52,446.63,56.31,9.96">Robust Information Retrieval and perception for a scaled Lego-Audio-Video multistructuration</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>University Sud Toulon-Var</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Thesis of habilitation for research direction</note>
</biblStruct>

<biblStruct coords="10,110.51,466.07,402.16,9.96;10,110.52,478.07,402.16,9.96;10,110.52,489.95,22.88,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,228.04,466.07,284.63,9.96;10,110.52,478.07,205.12,9.96">Web image retrieval on imageval: Evidences on visualness and textualness concept dependency in fusion model</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tollari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,334.64,478.07,173.85,9.96">ACM Int Conf on Image Video Retrieval</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.51,509.51,402.20,9.96;10,110.52,521.51,244.08,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,188.22,509.51,324.49,9.96;10,110.52,521.51,19.88,9.96">On estimation of entropy and mutual information of continuous distributions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Moddemeijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,137.62,521.51,74.32,9.96">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="246" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.51,540.95,402.16,9.96;10,110.52,552.95,402.11,9.96;10,110.52,564.83,22.88,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,200.81,540.95,311.86,9.96;10,110.52,552.95,62.92,9.96">Profil Entropic visual Features for Visual Concept Detection in CLEF 2008 campaign</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,194.68,552.95,150.88,9.96">Working Notes of ImageCLEF2008</title>
		<meeting><address><addrLine>Danmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>in conjuction with ECDL</note>
</biblStruct>

<biblStruct coords="10,110.51,584.39,402.25,9.96;10,110.52,596.27,402.38,9.96;10,110.52,608.27,402.36,9.96;10,110.52,620.27,186.05,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,258.94,584.39,253.82,9.96;10,110.52,596.27,65.93,9.96">Less is more: probabilistic models for retrieving fewer relevant documents</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,196.26,596.27,316.65,9.96;10,110.52,608.27,181.77,9.96;10,110.52,620.27,42.86,9.96">Proceedings of the 29th Annual int. ACM SIGIR Conference on Research and Development in information Retrieval</title>
		<meeting>the 29th Annual int. ACM SIGIR Conference on Research and Development in information Retrieval<address><addrLine>Seattle, Washington, USA; New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006-08-06">2006. August 06 -11, 2006</date>
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;06</note>
</biblStruct>

<biblStruct coords="10,110.50,639.71,402.14,9.96;10,110.52,651.71,402.16,9.96;10,110.52,663.59,307.38,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,330.41,639.71,166.22,9.96">Diversifying the image retrieval results</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.52,651.71,306.25,9.96;10,192.28,663.59,82.43,9.96">Proceedings of the 14th Annual ACM int. Conference on Multimedia</title>
		<meeting>the 14th Annual ACM int. Conference on Multimedia<address><addrLine>Santa Barbara, CA, USA; New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006-10-23">2006. Oct. 23-27</date>
			<biblScope unit="page" from="707" to="710" />
		</imprint>
	</monogr>
	<note>MULTIMEDIA &apos;06</note>
</biblStruct>

<biblStruct coords="10,110.50,683.15,402.17,9.96;10,110.52,695.03,402.16,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,335.22,683.15,177.46,9.96;10,110.52,695.03,195.04,9.96">Beyond independent relevance: methods and evaluation metrics for subtopic retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,326.22,695.03,155.34,9.96">Proceedings of the 26th Annual int</title>
		<meeting>the 26th Annual int</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.50,395.39,402.23,9.96;11,110.52,407.27,62.07,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,208.82,395.39,238.34,9.96">Clustering by Passing Messages Between Data Points</title>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Freyb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,455.79,395.39,30.00,9.96">Science</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="page" from="972" to="976" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.50,427.19,293.12,9.96" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m" coord="11,183.99,427.19,110.93,9.96">Statistical learning theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.50,447.11,402.36,9.96;11,110.52,459.11,196.57,9.96" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="11,295.62,447.11,217.24,9.96;11,110.52,459.11,110.78,9.96">Least Squares Support Vector Machine Classifiers Neural Processing Letters</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="293" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.50,479.03,402.14,9.96;11,110.52,491.03,262.20,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,215.81,479.03,177.68,9.96">LIG working notes on ImageCLEFphoto</title>
		<author>
			<persName coords=""><forename type="first">Philippe</forename><surname>Mulhem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,435.93,479.03,76.72,9.96;11,110.52,491.03,70.57,9.96">Working Notes of ImageCLEF2008</title>
		<meeting><address><addrLine>Danmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
	<note>in conjuction with ECDL</note>
</biblStruct>

<biblStruct coords="11,110.50,510.95,402.28,9.96;11,110.52,522.83,402.09,9.96;11,110.52,534.83,76.61,9.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,267.72,510.95,245.06,9.96;11,110.52,522.83,110.66,9.96">Hybrid text and visual document retrieval for imageclef 2008 photo retrieval task</title>
		<author>
			<persName coords=""><forename type="first">Marin</forename><surname>Ferecatu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hichem</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,243.34,522.83,153.52,9.96">Working Notes of ImageCLEF2008</title>
		<meeting><address><addrLine>Danmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>in conjuction with ECDL</note>
</biblStruct>

<biblStruct coords="11,110.50,554.75,402.26,9.96;11,110.52,566.75,402.40,9.96;11,110.52,578.63,402.28,9.96;11,110.52,590.63,235.95,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,164.22,554.75,124.08,9.96;11,331.10,554.75,73.49,9.96">AVEIR at ImageCLEFphoto</title>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
			<affiliation>
				<orgName type="collaboration">Consortium</orgName>
			</affiliation>
		</author>
		<author>
			<persName coords=""><forename type="first">Hichem</forename><surname>Sahbi</surname></persName>
			<affiliation>
				<orgName type="collaboration">Consortium</orgName>
			</affiliation>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhong-Qiu</forename><surname>Zhao</surname></persName>
			<affiliation>
				<orgName type="collaboration">Consortium</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,398.32,578.63,114.48,9.96;11,110.52,590.63,44.11,9.96">Working Notes of Image-CLEF2008</title>
		<editor>
			<persName><forename type="first">Sabrina</forename><surname>Tollari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marcin</forename><surname>Detyniecki</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marin</forename><surname>Ferecatu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Philippe</forename><surname>Herve' Glotin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Massih-Reza</forename><surname>Mulhem</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ali</forename><surname>Amini</surname></persName>
		</editor>
		<editor>
			<persName><surname>Fakeri-Tabrizi</surname></persName>
		</editor>
		<meeting><address><addrLine>Danmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>the fusion of runs. in conjuction with ECDL 2008</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
