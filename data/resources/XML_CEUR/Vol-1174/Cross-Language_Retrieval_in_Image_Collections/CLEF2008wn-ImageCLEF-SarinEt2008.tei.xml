<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.50,88.37,418.27,26.83;1,171.53,110.29,252.22,26.83">Targeting Diversity in Photographic Retrieval Task with Commonsense Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,176.06,151.63,117.03,18.20"><forename type="first">Supheakmungkol</forename><surname>Sarin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Global Information and Telecommunication Studies</orgName>
								<orgName type="institution">Waseda University</orgName>
								<address>
									<addrLine>1011 Okuboyama, Nishi-Tomida, Honjo-shi</addrLine>
									<postCode>367-0035</postCode>
									<settlement>Saitama-ken</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,319.75,151.63,99.46,18.20"><forename type="first">Wataru</forename><surname>Kameyama</surname></persName>
							<email>wataru@waseda.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Global Information and Telecommunication Studies</orgName>
								<orgName type="institution">Waseda University</orgName>
								<address>
									<addrLine>1011 Okuboyama, Nishi-Tomida, Honjo-shi</addrLine>
									<postCode>367-0035</postCode>
									<settlement>Saitama-ken</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.50,88.37,418.27,26.83;1,171.53,110.29,252.22,26.83">Targeting Diversity in Photographic Retrieval Task with Commonsense Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">539DB9A8758D6B7DD064C3F6662454C3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages Query Languages Algorithms, Experimentation, Performance Commonsense knowledge, Image Retrieval, Diversity, AnalogySpace, Query/Document Expansion, Rerank</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image search engines have a very limited usefulness since it is still dicult to provide dierent users with what they are searching for. This is because most research eorts to date have only been concentrating on relevancy rather than diversity which is also a quite important factor, given that the search engine knows nothing about the user's context. In this paper, we describe our approach for ImageCLEF 2008 photographic retrieval task. The novelty of our technique is the use of AnalogySpace [3], the reasoning technique over commonsense knowledge for document and query expansion, which aims to increase the diversity of the results. Our proposed technique combines AnalogySpace mapping with other two mappings namely, location and full-text. We then re-rank the resulting images from the mapping by trying to eliminate duplicate and near duplicate results in the top 20. We present our preliminary experiments and the results conducted using the IAPR TC-12 photographic collection with 20,000 natural still photographs. The results show that our integrated method with AnalogySpace yields slightly better performance in terms of cluster recall and the number of relevant photographs retrieved. We nally identify the weakness in our approach and ways on how the system could be optimized and improved.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The aordability of digital camera and the ease of use of content publishing tool have pushed for the rapid growth of everyday photographs on the web with a large percentage coming from the amateur photographers. These published amateur photographs usually come with either a short description or a few keywords. This shows potentials for image retrieval system to provide better resulting images.</p><p>Unfortunately, image search engines have very limited usefulness since it is still dicult to provide dierent users with what they are searching for. Usually dierent people issuing the same query are looking for dierent images. A good image search engine should not produce top results in the ranked list that contain only relevant items of a single theme, but rather diverse items representing sub-topics within the results, yet keeping high level of relevancy. comprises ve tasks related to image retrieval and annotation techniques, namely, photographic retrieval, medical retrieval, general photographic concept detection, medical automatic image annotation, and image retrieval task from a collection of Wikipedia images. We present our development and contributions to the rst task of which the goal is to promote diversity in the top ranked list of resulting images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach and Implementation</head><p>Using surrounding text of the images or annotation as a means to interpret them is a classic research methodology. To date, however, most research eorts have only been concentrating on relevancy than diversity. The latter is also a quite important factor since the search engine usually knows nothing about the user. Furthermore, most of the time, people solve the problem through selecting some keywords and features of images to represent the photograph rather than trying to understand the semantic nature of annotation and the query. In this paper, we approach these problems as follows:</p><p>• To enable diversity, we use commonsense knowledge as a tool for term expansion. We consider ConceptNet <ref type="bibr" coords="2,151.76,491.44,10.51,6.64" target="#b7">[8]</ref> as our commonsense knowledge database. ConceptNet is made up of a network of everyday concepts that have been automatically generated from English sentences of the Open Mind Common Sense corpus. The corpus has been handcrafted by the general public since 2000 <ref type="bibr" coords="2,95.77,527.30,9.96,6.64" target="#b8">[9]</ref>. Those concepts are connected by one of about twenty relationships such as IsA , PartOf , locationAt , Desires , CapableOf , UsedFor , etc. We use ConceptNet for diversity purposes because a term can be expanded to its contextually related concepts that are not necessarily its synonyms. Furthermore, those related concepts reect the commonsense way of people's thinking and how they relate concepts since they are input by human beings with a specic purpose. For instance, drink coee relates to wake up, yawn, read newspaper, etc. However, diversity should not come as a compensation of relevancy. Therefore, we also try to maintain the level of precision by combining the former with both full-text and location matching.</p><p>• Re-ranking technique is performed afterward to re-rank the results of the previous step by trying to eliminate duplicate and near duplicate results.</p><p>Figure <ref type="figure" coords="2,102.10,670.09,4.98,6.64" target="#fig_0">1</ref> illustrates the process of our proposed approach. The ow can be divided into two major steps, namely, matching and re-ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Matching</head><p>As shown in Figure <ref type="figure" coords="2,157.98,739.92,3.87,6.64" target="#fig_0">1</ref>, we introduce three kinds of matching between query and annotation of the image, namely location, AnalogySpace, and full-text. We begin by parsing the annotation to get location named entities. GATE is used for this purpose <ref type="bibr" coords="3,70.87,199.54,14.61,6.64" target="#b9">[10]</ref>. Then, we establish a location hierarchy from the annotation before we perform the matching. For instance, Lima is expanded to Lima &gt; &gt; Peru &gt; &gt; South America. Location names found in image annotations and query topics are expressed as sets with prepositions found in the query as a matching condition. To do this, we simply create two sets of prepositions namely, include set and exclude set.</p><p>Prepositions in include set are such as 'in', 'of ', 'along', 'on', 'near', 'by', 'in', etc., while the other set includes prepositions such as 'out of ', 'outside', etc. For example, in the query Sport stadium outside Australia , outside serves as an excluding condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">AnalogySpace matching</head><p>AnalogySpace is a vector space representation of commonsense knowledge built on the top of ConceptNet using Principal Component Analysis <ref type="bibr" coords="3,231.84,336.69,9.96,6.64" target="#b2">[3]</ref>. This representation can be used as a reasoning tool as it reveals large-scale patterns in the data while smoothing over noise. In our case, we use an implementation of AnalogySpace called Divisi <ref type="bibr" coords="3,190.66,360.60,15.50,6.64">[11]</ref> to create ad-hoc category for each annotation and query. We then match the query against the annotation. The degree of similarity between the two ad-hoc categories is the dot product of matrices of the shared similar concepts and features.</p><p>Since ConceptNet depends on sentences contributed from human, it does not contain all the terms a dictionary has. To cope up with unknown terms, we use their synonym and hypernym. We create the set of expanded term for the unknown term using its Wordnet's synsets and hypernym regardless of its part of speech. However, we only choose one term as our replacement for the unknown term. The best term is the term that is most uniform to other terms of the annotation. This is achieved via dot product of the matrix of an ad-hoc category created from a combination of other terms of the annotation, against the ad-hoc categories created from each term from the expanded set if it exists in ConceptNet. We chose the term that has the highest similarity score. Figure <ref type="figure" coords="3,307.33,484.99,4.98,6.64" target="#fig_1">2</ref> shows the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">full-text matching</head><p>Vector Space Model is used to represent the annotations and query topics. Term frequency is used for our vector space model. Each document is represented as a vector, where each dimension corresponds to the frequency of a given term. In our case, terms are reduced to their stems.</p><p>Some terms from query topics might not be found in the index of the annotation documents. To cope up with this, we expand unknown query terms with their synsets and hypernym from WordNet. We select top three terms among the set of synonyms found. AnalogySpace is used to compute the similarity score between the unknown term and its synonyms.</p><p>The similarity distance between a document vector and a query vector is expressed as cosine distance.</p><p>Figure <ref type="figure" coords="3,102.29,643.76,4.98,6.64" target="#fig_2">3</ref> illustrates the technique.</p><p>Finally, we normalize each matching score according to its maximum and minimum value. The total matching score is expressed as the product of all the three matching scores. This is the simplest way to combine the scores and yet make the large dierences count for even more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Re-ranking</head><p>In this step, the results from the rst step are re-ranked according to their semantic similarity by giving penalty to the ones with high similarity between each other. We calculate full-text and location similarity. Same as in the matching process between query topic and photograph annotation, boolean logic is used for location similarity calculation, while vector space model is used for full-text similarity calculation. We compute the total pair distance of images as the product of both distance scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Re-rank</head><p>The similarity distance score obtained can be used to lter and re-rank the preliminary results. We use a method called Hill Climbing to nd a threshold of similarity distance that can help optimize both the precision and diversity. We introduce a loop where Hill Climbing starts with a random threshold and looks for the set of solutions which are better from its neighbors. The loop goes on until we obtain the best compromise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Process</head><p>Organizers of ImageCLEF 2008 provide participants with a collection of annotated images, together with query topics. Participants use these resources with their retrieval systems and submit to the organizers the identiers of the relevant documents for each query topic. Then, the organizers evaluate the result set of each submission from every participant and rank submissions according to standard evaluation measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset</head><p>The collection of images used for ImageCLEF 2008 is the IAPR TC-12 photo collection consisting of 20,000 natural images taken from locations around the world <ref type="bibr" coords="4,303.63,605.42,9.96,6.64" target="#b1">[2]</ref>. The collection includes images of various sports and actions, photos of people, animals, cities, landscapes and many other aspects of contemporary life.</p><p>Each image is also associated with an alphanumeric caption stored in a semi-structured format. These captions include the title of the image, its creation date, the location at which the photograph was taken, a semantic description of the contents of the image by the photographer and some additional notes. Table <ref type="table" coords="4,70.87,665.19,4.98,6.64" target="#tab_0">1</ref> shows the example of a photograph and its metadata. In our system, we use only the title, description, and location parts of the metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Query</head><p>There are a total of 39 queries used in this study ranging from the very specic to the very abstract ones with dierent levels of diculty. Here are some of the query topics: "animal swimming", "destinations in Venezuela", "church with more than two towers", "sunset over water", etc. Query topics are provided as a structured information. It is composed of the query title, cluster, narration of how relevant images should be, and some examples of relevant image les. Table <ref type="table" coords="5,341.14,439.68,4.98,6.64" target="#tab_1">2</ref> shows the example of a query topic. In our system, we use only the topic title.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Measurement techniques</head><p>To ensure both relevancy and diversity, the evaluation is based principally on two measures, namely, precision at 20, and instance recall at rank 20 <ref type="bibr" coords="5,274.43,521.82,9.96,6.64" target="#b3">[4]</ref>. The technique is a relatively new evaluation methodology that considers results of a query as interdependence rather than a standalone. A good engine will produce results that maximize the two measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussions</head><p>We present below the results of the four runs that we submitted to ImageCLEF photographic task 2008.</p><p>1. AnalogySpace: In this run, we combine location matching and AnalogySpace.</p><p>2. Full-text: In this run, we simply use location matching and full-text search.</p><p>3. Full-text (no query expansion) + AnalogySpace: In this run, we combine location matching, fulltext matching, and AnalogySpace matching.</p><p>4. Full-text (with query expansion) + AnalogySpace: The same as the previous one, we combine the three matching. We further expand the terms of query topics in full-text matching with their synsets and hypernym. notice that there is only a slight improvement in recall when introducing AnalogySpace. Table <ref type="table" coords="6,490.61,481.99,4.98,6.64" target="#tab_3">4</ref> shows that AnalogySpace helps to gain a little bit better cluster recall at 20 over the conventional full-text vector space model. The number of relevant images retrieved also increases as shown in Table <ref type="table" coords="6,445.94,505.90,3.87,6.64" target="#tab_4">5</ref>. However, Table <ref type="table" coords="6,70.87,517.86,4.98,6.64" target="#tab_2">3</ref> and 5 show that the precision at 20 and the Mean Average Precision (MAP) which is the summary of recall and precision do not produce better result with AnalogySpace. Therefore, the results are not yet conclusive. We also notice that the improvement happens only when there is no query expansion in the full-text matching.</p><p>We still believe that ConceptNet could help enriching diversity in the resulting images. To our understanding, the reason why we could not achieve better results is because of the fact that there are lots of terms that ConceptNet does not cover. When we try to expand those unknown terms using WordNet, we only introduce noise. That is because WordNet's synsets contain all the synonyms of the word from all its possible senses. We did not implement any sense disambiguation. We did not even check the part of speech. Therefore, most of the time, the replacement only twists the meaning of the original word since we do not select the most appropriate sense of the word. Moreover, we limit the number of selected synonym to only one in AnalogySpace term expansion, and only up to three in our full-text query expansion. This reduces the coverage of the meanings. Due to limited time, content-based technology was not taken into consideration. Should we have incorporated another content-based pair similarity distance in the re-ranking step, we might be able to get better resulting images. Hence, we are planning to tackle these issues in our future works.</p><p>Image search at the major search engines today largely relies on looking at words that are used around images on the pages that host them, in image le names, and in ALT text associated with them. No real image recognition is done by any of the majors. Datta et al. have recently produced a complete survey of the current image related techniques <ref type="bibr" coords="7,286.68,125.34,9.96,6.64" target="#b5">[6]</ref>. Hsu et al <ref type="bibr" coords="7,356.39,125.34,10.51,6.64" target="#b0">[1]</ref> have used ConceptNet as tool for query and document expansion in image retrieval task. Nevertheless, in doing this, the authors only use spatial relationship function to nd the concepts that co-exist in space of the real world. Google recently introduced VisualRank a method that guesses how the images would be linked together, with those being most similar having more virtual links to each other. As a result, the most "linked to" images are calculated to rank rst <ref type="bibr" coords="7,173.62,185.11,9.96,6.64" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>User satisfaction is not solely a function of relevancy. When nothing is known about the user, diversity plays an important role in getting the results that user would like to see. We present a novel approach to enable rich diversity in the results by incorporating commonsense knowledge expansion and result re-ranking through elimination of duplicate and near duplicate results. The presented results are just our preliminary ones. Even they are not conclusive yet, they pave the way to help us to improve our current system. We are now working to address the weak points that we have discussed earlier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,193.04,234.67,209.19,6.64;2,120.67,56.69,362.84,161.04"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Flow Diagram of System Architecture</figDesc><graphic coords="2,120.67,56.69,362.84,161.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,223.27,134.83,148.74,6.64;3,70.87,56.69,453.55,61.20"><head>Figure 2</head><label>2</label><figDesc>Figure 2: AnalogySpace Matching</figDesc><graphic coords="3,70.87,56.69,453.55,61.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,234.96,198.23,125.36,6.64;4,120.67,56.69,362.84,124.59"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Full-text Matching</figDesc><graphic coords="4,120.67,56.69,362.84,124.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,125.30,68.54,358.70,122.23"><head>Table 1 :</head><label>1</label><figDesc>Example of a photograph of the collection and its attached metadata</figDesc><table coords="5,217.81,108.41,266.19,82.36"><row><cell>DocNo</cell><cell>annotations/37/37394.eng</cell></row><row><cell>Title</cell><cell>The Saint Basil's Cathedral</cell></row><row><cell>Description</cell><cell>a cathedral with crosses on many onion domes;</cell></row><row><cell></cell><cell>people, trees, a statue and snow on a square in</cell></row><row><cell></cell><cell>front of it; a grey sky in the background;</cell></row><row><cell>location</cell><cell>Moscow, Russia</cell></row><row><cell>Date</cell><cell>February 2001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,81.84,259.76,432.32,91.72"><head>Table 2 :</head><label>2</label><figDesc>Example of a query topic</figDesc><table coords="5,81.84,281.48,424.57,46.09"><row><cell>Num</cell><cell>2</cell></row><row><cell>Title</cell><cell>Church with more than two towers</cell></row><row><cell>Cluster</cell><cell>City</cell></row><row><cell>Narr</cell><cell>Relevant images will show a church, cathedral or a mosque with three or more towers.</cell></row></table><note coords="5,130.41,332.88,383.74,6.64;5,130.41,344.84,346.22,6.64"><p>Churches with only one or two towers are not relevant. Buildings that are not churches, cathedrals or mosques are not relevant even if they have more than two towers.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,77.24,67.32,436.48,82.24"><head>Table 3 :</head><label>3</label><figDesc>Precision (P) at the top n results</figDesc><table coords="6,77.24,90.86,22.33,6.64"><row><cell>Runs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,77.24,174.22,435.50,105.90"><head>Table 4 :</head><label>4</label><figDesc>Cluster Recall (CR) at the top n results</figDesc><table coords="6,77.24,197.02,435.50,83.10"><row><cell>Run</cell><cell>CR5</cell><cell>CR10</cell><cell>CR15</cell><cell>CR20</cell><cell>CR30</cell><cell>CR50</cell><cell>CR100</cell><cell>CR1000</cell></row><row><cell>AnalogySpace</cell><cell>0.09</cell><cell>0.13</cell><cell>0.16</cell><cell>0.21</cell><cell>0.24</cell><cell>0.27</cell><cell>0.35</cell><cell>0.67</cell></row><row><cell>Full-text</cell><cell>0.14</cell><cell>0.21</cell><cell>0.25</cell><cell>0.28</cell><cell>0.35</cell><cell>0.46</cell><cell>0.55</cell><cell>0.81</cell></row><row><cell>Full-text (no query</cell><cell>0.14</cell><cell>0.21</cell><cell>0.25</cell><cell>0.31</cell><cell>0.38</cell><cell>0.46</cell><cell>0.54</cell><cell>0.82</cell></row><row><cell>expansion) + AnalogySpace</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Full-text (with query</cell><cell>0.13</cell><cell>0.19</cell><cell>0.22</cell><cell>0.25</cell><cell>0.33</cell><cell>0.42</cell><cell>0.52</cell><cell>0.8</cell></row><row><cell>expansion) + AnalogySpace</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,70.87,314.82,453.54,127.99"><head>Table 5 :</head><label>5</label><figDesc>Other measurements: Number of Relevant Retrieved images (NumRelRet), Number of Relevant</figDesc><table coords="6,70.87,326.78,453.54,116.03"><row><cell cols="6">images (NumRel), Mean Average Precision (MAP), Geometric Mean Average Precision (GMAP), Blind</cell></row><row><cell>RElevance Feedback (BREF)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell>NumRelRet</cell><cell>NumRel</cell><cell>MAP</cell><cell>GMAP</cell><cell>BREF</cell></row><row><cell>AnalogySpace</cell><cell>1247</cell><cell>2401</cell><cell>0.14</cell><cell>0.01</cell><cell>0.51</cell></row><row><cell>Full-text</cell><cell>1420</cell><cell>2401</cell><cell>0.21</cell><cell>0.06</cell><cell>0.64</cell></row><row><cell>Full-text (no query expansion) +</cell><cell>1451</cell><cell>2401</cell><cell>0.2</cell><cell>0.06</cell><cell>0.65</cell></row><row><cell>AnalogySpace</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Full-text (with query expansion) +</cell><cell>1462</cell><cell>2401</cell><cell>0.2</cell><cell>0.04</cell><cell>0.65</cell></row><row><cell>AnalogySpace</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,70.87,470.04,453.55,6.64"><head>Table 3 ,</head><label>3</label><figDesc><ref type="bibr" coords="6,108.63,470.04,3.87,6.64" target="#b3">4</ref>, and 5 show the precision, cluster recall, and other measures respectively. From the results, we</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,91.34,376.33,433.06,6.64;7,91.34,388.28,205.00,6.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,277.24,376.33,220.39,6.64">Information retrieval with commonsense knowledge</title>
		<author>
			<persName coords=""><forename type="first">Ming-Hung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,505.43,376.33,18.98,6.64;7,91.34,388.28,160.18,6.64">Proceedings of the 29th ACM SIGIR &apos;06</title>
		<meeting>the 29th ACM SIGIR &apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">651652</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.34,408.21,433.07,6.64;7,91.34,420.16,433.07,6.64;7,91.34,432.12,433.07,6.64;7,91.34,444.07,220.77,6.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,376.80,408.21,147.61,6.64;7,91.34,420.16,245.45,6.64">The IAPR TC-12 Benchmark: A New Evaluation Resource for Visual Information Systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,356.24,420.16,168.17,6.64;7,91.34,432.12,433.07,6.64;7,91.34,444.07,37.65,6.64">Proceedings of International Workshop OntoImage&apos;2006 Language Resources for Content-Based Image Retrieval, held in conjunction with LREC&apos;06</title>
		<meeting>International Workshop OntoImage&apos;2006 Language Resources for Content-Based Image Retrieval, held in conjunction with LREC&apos;06<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05-22">2006. 22 May 2006</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.34,464.00,433.06,6.64;7,91.34,475.95,259.91,6.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,332.15,464.00,192.25,6.64;7,91.34,475.95,126.19,6.64">AnalogySpace: Reducing the Dimensionality of Common Sense Knowledge</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henry</forename><surname>Lieberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,302.00,475.95,49.25,6.64">AAAI 2008</title>
		<meeting><address><addrLine>Chicago, Illinois</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.34,495.88,433.07,6.64;7,91.34,507.83,433.07,6.64;7,91.34,519.79,433.07,6.64;7,91.34,531.75,147.17,6.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,323.74,495.88,200.67,6.64;7,91.34,507.83,178.29,6.64">Beyond independent relevance: methods and evaluation metrics for subtopic retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Laerty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,292.09,507.83,232.32,6.64;7,91.34,519.79,326.15,6.64;7,191.83,531.75,42.81,6.64">Proceedings of the 26th Annual international ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 26th Annual international ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07-28">2003. July 28 -August 01, 2003</date>
		</imprint>
	</monogr>
	<note>SIGIR &apos;03</note>
</biblStruct>

<biblStruct coords="7,91.34,551.67,433.06,6.64;7,91.34,563.63,236.81,6.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,314.70,551.67,209.71,6.64;7,91.34,563.63,169.96,6.64">ConceptNet 3: a Flexible, Multilingual Semantic Network for Common Sense Knowledge</title>
		<author>
			<persName coords=""><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,269.63,563.63,35.28,6.64">RANLP</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.34,583.55,433.06,6.64;7,91.34,595.51,308.66,6.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,276.32,583.55,248.09,6.64;7,91.34,595.51,14.73,6.64">Image Retrieval: Ideas, Inuence, and Trends of the New Age</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,114.30,595.51,110.64,6.64">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.34,615.43,433.07,6.64;7,91.34,627.39,179.31,6.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,235.96,615.43,157.71,6.64">PageRank for Product Image Search</title>
		<author>
			<persName coords=""><forename type="first">Yushi</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shumeet</forename><surname>Bajula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,413.96,615.43,110.44,6.64;7,91.34,627.39,175.44,6.64">Proceedings of the World Wide Web Conference. ACM WWW &apos;08</title>
		<meeting>the World Wide Web Conference. ACM WWW &apos;08</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.34,647.31,433.06,6.64;7,91.34,659.27,433.07,6.64;7,91.34,671.22,19.92,6.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,273.23,647.31,251.17,6.64;7,91.34,659.27,132.18,6.64">ConceptNet 3: a Flexible, Multilingual Semantic Network for Common Sense Knowledge</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,232.54,659.27,291.87,6.64">Proceedings of Recent Advances in Natural Languages Processing</title>
		<meeting>Recent Advances in Natural Languages Processing</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.34,691.15,433.07,6.64;7,91.34,703.10,433.07,6.64;7,91.34,715.06,433.07,6.64;7,91.34,727.01,243.86,6.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,365.16,691.15,159.25,6.64;7,91.34,703.10,146.22,6.64">Open mind commonsense: knowledge acquisition from the general public</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E T</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W L</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,248.31,703.10,276.10,6.64;7,91.34,715.06,349.91,6.64">Proceedings of the First International Conference on Ontologies, Databases, and Applications of Semantics for Large Scale Information Systems</title>
		<title level="s" coord="7,450.34,715.06,74.06,6.64;7,91.34,727.01,94.37,6.64">Lecture Notes in Computer Science No</title>
		<meeting>the First International Conference on Ontologies, Databases, and Applications of Semantics for Large Scale Information Systems<address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2519</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.34,61.68,433.06,6.64" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,370.64,61.68,153.77,6.64">GATE: a framework and graphical</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Tablan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
