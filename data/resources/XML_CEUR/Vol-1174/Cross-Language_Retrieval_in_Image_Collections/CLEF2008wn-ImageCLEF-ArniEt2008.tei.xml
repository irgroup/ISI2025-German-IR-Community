<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,178.92,92.39,238.65,12.58;1,212.76,108.47,171.07,12.58">Overview of the ImageCLEFphoto 2008 Photographic Retrieval Task</title>
				<funder>
					<orgName type="full">Marie Curie University (UPMC)</orgName>
				</funder>
				<funder ref="#_Hfy2Utb">
					<orgName type="full">EU-</orgName>
				</funder>
				<funder ref="#_Sp9hJRZ">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,154.80,139.44,52.60,9.02"><forename type="first">Thomas</forename><surname>Arni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sheffield University</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,215.54,139.44,49.75,9.02"><forename type="first">Paul</forename><surname>Clough</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sheffield University</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.49,139.44,65.80,9.02"><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sheffield University</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,362.06,139.44,76.43,9.02"><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Victoria University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,178.92,92.39,238.65,12.58;1,212.76,108.47,171.07,12.58">Overview of the ImageCLEFphoto 2008 Photographic Retrieval Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9AE91BFA4B64E7735DC603717A991104</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Performance Evaluation, IAPR TC-12 Benchmark, Image Retrieval, Diversity, Clustering 1</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ImageCLEFphoto 2008 is an ad-hoc photo retrieval task and part of the ImageCLEF evaluation campaign. This task provides both the resources and the framework necessary to perform comparative laboratory-style evaluation of visual information retrieval systems. In 2008, the evaluation task concentrated on promoting diversity within the top 20 results from a multilingual image collection. This new challenge attracted a record number of submissions: a total of 24 participating groups submitting 1,042 system runs. Some of the findings include that the choice of annotation language is almost negligible and the best runs are by combining concept and content-based retrieval methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The evaluation of multilingual image retrieval systems (i.e. where associated texts are in languages different from written queries) has been the focus of ImageCLEF since its inception in 2003. The track has evolved over the years to address different domains (e.g. cultural heritage, medical imaging and Wikipedia), and different kinds of tasks (e.g. ad-hoc retrieval, automatic annotation and clustering). The focus of the ImageCLEFphoto task in 2008 has been to promote diversity in the top n results (see section 1.2). The resources provided enable system-centred evaluation for multilingual and diversity-based visual information retrieval based on a collection of "general" photographs (see section 2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Evaluation Scenario</head><p>The evaluation scenario is similar to the classic TREC<ref type="foot" coords="1,285.90,602.95,3.24,5.83" target="#foot_0">1</ref> ad-hoc retrieval task: simulation of the situation in which a system knows the set of documents to be searched, but cannot anticipate the particular topic that will be investigated (i.e. the search topics are not known to the system in advance) <ref type="bibr" coords="1,386.63,628.02,10.64,9.02" target="#b5">[6]</ref>. The goal of the simulation is: given an alphanumeric statement (and/or sample images) describing a user's information need, find as many relevant images as possible from the given collection (with the query language either being identical or different from that used to describe the images). For 2008, the scenario is slightly different in that systems must return relevant images from as many different sub-topics as possible (i.e. promote diversity) in the top n results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Evaluation Objective for 2008</head><p>The main objective of ImageCLEFphoto for 2008 comprised the evaluation of ad-hoc multilingual visual information retrieval systems from a general collection of annotated photographs (i.e. image with accompanying semi-structured captions such as the title, location, description, date or additional notes). However, this year focused on a particular aspect of retrieval: diversity of the results set (see section 1.3). More recently, research in image search has concentrated on ensuring that duplicate or near-duplicate documents retrieved in response to a query are hidden from the user. This should ideally lead to a ranked list where images are both relevant and diverse. In 2007, the task considered maximising the number of relevant documents in the resulting ranked list. In 2008, the task is to promote diversity in the top n results, which has been shown to better satisfy a user's information need <ref type="bibr" coords="2,141.60,116.46,10.84,9.02" target="#b7">[8,</ref><ref type="bibr" coords="2,156.06,116.46,8.36,9.02" target="#b8">9]</ref> (people often type in the same query but prefer to see results which represent different aspects of the results set). Hence, providing a diverse results list is especially important when a user types in a query that is either poorly specified or ambiguous.</p><p>This new challenge allows for the investigation of a number of research questions, including the following:</p><p>• Is it possible to promote diversity within the top n results?</p><p>• Which approaches work best at promoting diversity?</p><p>• Does promoting diversity reduce the number of relevant images in the top n results?</p><p>• Can "standard" text retrieval methods be used to promote diversity?</p><p>• How does the retrieval performance compare between bilingual and multilingual annotations?</p><p>One major goal of ImageCLEFphoto 2008 was to attract participants from various backgrounds and with different research interests. The collection developed for the 2008 task, in our view, provides a resource that can be used to evaluate both concept and content-based approaches for image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">An Example of Diversity</head><p>To illustrate what a diverse results set looks like, consider the following example. Given the search topic "images of typical Australian animals", using traditional ranking methods (commonly based on the Probability Ranking Principle) will produce a result calculated on the similarity between query and documents. This often leads to a set of results that contains groups of similar documents. Figure <ref type="figure" coords="2,314.39,355.08,5.01,9.02">1</ref> shows a typical example of the kind of results one could expect in the top 10 using traditional ranking methods. However, going down the ranked list one finds other types of animals such as koala bears.</p><p>The 2008 ImageCLEFphoto task is to promote diversity in the top n results by including at least one relevant document from each sub-topic within the first n results (i.e. pictures of different animals in the top n). A more diverse (and arguably improved) results set is illustrated in Figure <ref type="figure" coords="2,334.07,655.74,3.77,9.02" target="#fig_0">2</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Collection</head><p>The IAPR TC-12 Benchmark consists of 20,000 colour photographs taken from locations around the world and comprises a varying cross-section of still natural images. Figure <ref type="figure" coords="3,333.99,432.96,5.01,9.02">3</ref> illustrates a number of sample images from a selection of categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sports Landscapes Animals People Figure 3: Sample images from the IAPR TC-12 collection</head><p>The majority of images have been provided by Viventura<ref type="foot" coords="3,319.92,594.73,3.24,5.83" target="#foot_2">3</ref> , an independent travel company that organises adventure and language trips to South America. Travel guides accompany the tourists and maintain a daily online diary including photographs of trips made and general pictures of each location including accommodation, facilities and ongoing social projects. In addition to these photos, a number of photos from a personal archive have also been added to form the collection used in ImageCLEF. The collection is publicly available for research purposes and, unlike many existing photographic collections, can be used to evaluate image retrieval systems. The collection is general in content with many different images of similar visual content, but varying illumination, viewing angle and background. This makes it a challenge for the successful application of techniques involving visual analysis.</p><p>Each image in the collection has a corresponding semi-structured caption consisting of the following six fields: (1) a unique identifier, (2) a title, (3) a free-text description of the semantic and visual contents of the image, (4) notes for additional information, <ref type="bibr" coords="3,179.20,734.70,11.62,9.02" target="#b4">(5)</ref> where and (6) when the photo was taken. Figure <ref type="figure" coords="3,395.05,734.70,5.01,9.02">4</ref> shows a sample image with its corresponding textual annotation (in English).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: Sample image caption</head><p>By using a custom-built application for managing the images, various subsets of the collection can be generated with respect to a variety of particular parameters (e.g. using a selected subset of caption fields). For 2008, the following data was provided:</p><p>• Annotation language: two sets of annotations in (1) English and (2) Random. In the random set, the annotation language was randomly selected from for each of the images (i.e. annotations are either German or English image captions).</p><p>• Caption fields: all caption fields were provided for the 2008 task.</p><p>• Annotation completeness: each image caption exhibited the same level of annotation completenessthere were no images without annotations (as experimented with in 2006). The participants were granted access to the data set on 22 nd April 2008 and had exactly one month to familiarise themselves with the new subset. Most participants had to modify their standard retrieval systems in order to generate diverse results in the top n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query Topics</head><p>From an existing set of 60 topics, 39 were selected and distributed to participants (Table <ref type="table" coords="4,433.75,449.10,4.18,9.02" target="#tab_1">1</ref>) representing varying search requests (many of these are realistic and based on queries extracted during log file analysis -see <ref type="bibr" coords="4,501.53,460.62,11.67,9.02" target="#b3">[4]</ref> for more detailed information). We found that for the new retrieval challenge (promoting diversity), not all of the existing topics were suitable and therefore some were removed (see <ref type="bibr" coords="4,352.89,483.60,11.62,9.02" target="#b0">[1]</ref> for further details). Although 21 topics were removed, the remaining 39 topics are well-balanced, diverse and should present a retrieval challenge to participants wishing to use either text and/or low-level visual analysis techniques for creating clusters.</p><p>Similar to TREC, the query topics were provided as structured statements of user needs. The full description of a topic consists of (1) a topic titles (2) a topic narrative, (3) a newly added cluster type and (4) three example relevant images for that topic. An additional field was added called cluster type, which was augmented for easier assessment of the clusters as well as to facilitate the quantification of the result set diversity <ref type="bibr" coords="4,460.09,564.00,10.64,9.02" target="#b0">[1]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relevance Assessments</head><p>The relevance assessments, with the exception of removing any additional images considered as non-relevant, are exactly the same as in year 2007. No pooling of the images was carried out in 2008. Information about relevance assessments from previous years can be found in <ref type="bibr" coords="5,276.62,454.14,10.63,9.02" target="#b1">[2]</ref>. To enable diversity to be quantified, it was necessary to classify images relevant to a given topic to one or more sub-topics or clusters. This was performed by two assessors. In case of inconsistent judgements, a third assessor was used to resolve the inconsistencies. The resulting cluster assessment judgements are then used in combination with the normal relevance assessment to determine the retrieval effectiveness of each submitted system run (for further details see <ref type="bibr" coords="5,426.71,500.10,10.52,9.02" target="#b0">[1]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Generating the Results</head><p>Once the relevance judgements and the cluster relevance assessments were completed, the performance of individual systems and approaches can be evaluated. The results for submitted runs were computed using the latest version of trec eval 4 , as well as a custom-built tool to calculate diversity of the results set. Submissions were evaluated using two metrics: (1) precision at rank 20 (P20) and ( <ref type="formula" coords="5,331.40,583.86,3.89,9.02">2</ref>) cluster recall at rank 20 (CR20). Rank 20 was selected as the cut-off point to measure precision and cluster recall because most online image retrieval engines (e.g. Google, Yahoo! and AltaVista) display 18 to 20 images by default. Further measures considered included uninterpolated (arithmetic) Mean Average Precision (MAP), Geometric Mean Average Precision (GMAP) to test system robustness and binary preference (bpref), which is a good indicator of how complete relevance judgements are. To enable an absolute comparison between individual runs, a single metric is required: the F1-measure was used to combine scores from P20 and CR20 (representing the harmonic mean of P20 and CR20).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of Participation and Submissions</head><p>In 2008, 43 groups registered for ImageCLEFphoto (32 in 2007, 36 in 2006), with 24 groups eventually submitting a total of 1,042 runs (all of which were evaluated by the organisers). This is an increase in the number of runs from previous years (20 groups submitting 616 runs in 2007, 12 groups submitting 157 runs in 2006, and 11 groups 349 runs in 2005 respectively). Table <ref type="table" coords="5,285.20,732.84,5.01,9.02">2</ref> provides an overview of the participating groups, the corresponding number of submitted runs and whether they are new or returning participants. The 24 participating Table <ref type="table" coords="6,92.88,597.90,3.91,9.02">2</ref>: Participating groups Increased participation might be an indicator for (1) the growing need for evaluation of visual information retrieval from more general photographic collections, (2) the growing need for comparative evaluation of diversity and/or (3) an interest by researchers world-wide to participate in evaluation events such as ImageCLEFphoto.</p><p>Although the total number of runs has risen, the geometric mean of runs per participating group is slightly lower than in 2007 (12.4 in 2008 / 13.8 in 2007). The reason for the increasing number of total runs is mainly due to the larger number of submissions from Dublin City University (DCU), who submitted a total of 733 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of Submissions</head><p>Overall, 1042 runs were submitted and categorised with respect to the following dimensions: (1) annotation language, (2) modality (text only, image only or combined) and (3) run type (automatic or manual). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This section provides an overview of results with respect to the various submission dimensions (1) annotation language, (2) retrieval modality and (3) run type. The task for the participants was to maximise the number of relevant images in the top 20 results. At the same time the relevant images in the top 20 results should be from as many different sub-topics as possible. Simply getting lots of relevant images from one sub-topic or filling the ranking with diverse, but non-relevant images, results in a poor overall effectiveness score. Measures such as MAP are not suitable since it does not take into account diversity. To determine the diversity of a result set, S-Recall (sub-topic recall) proposed by Zhai et al <ref type="bibr" coords="7,267.11,372.72,11.68,9.02" target="#b4">[5]</ref> was used. S-recall at rank K is defined as the percentage of sub-topics covered by the first K documents in the list:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-recall at K</head><p>( )</p><formula xml:id="formula_0" coords="7,275.58,407.55,91.66,36.06">A n i d subtopics K i 1 = ∪ ≡</formula><p>where d i represents the i th document, subtopics(d i ) the number of sub-topics d i belongs to, and n A the total number of sub-topics in a particular topic. Thus the evaluation is based on two measures: precision at 20 and cluster recall at rank 20 (S-recall). As previously mentioned, it was important to maximise both measures in order to get a high overall ranking. To provide a single measure of effectiveness, we used the F1-measure (harmonic mean) to combine P20 and CR20:</p><formula xml:id="formula_1" coords="7,217.14,508.72,130.77,29.51">F1-measure = | | ) 20 20 ( ) 20 20 ( 2 CR P CR P + × ×</formula><p>The order of the diverse and relevant documents within the first top 20 result is not considered for the calculation of the cluster recall. This means that relevant documents from different sub-topics can be in a random order, without affecting the cluster recall score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results by annotation language</head><p>Tables <ref type="table" coords="7,97.42,620.10,5.01,9.02" target="#tab_4">4</ref> and<ref type="table" coords="7,123.21,620.10,5.01,9.02" target="#tab_5">5</ref> show the runs which achieved highest F1-measure scores for the two annotation languages: ENG and RND. Taking into account that only two groups submitted 495 runs with a random annotation language, the result shows the same trend as in previous years: the highest monolingual run still outperforms the highest bilingual run, which consists of a random annotation language. However, as in previous years, the margin of difference is low and can be attributed to significant progress of the translation and retrieval methods using these languages. The best performing runs using random annotations performed with an F1-measure score at 97.4% of the highest monolingual run. Hence, the language barrier is no longer a critical factor in achieving good retrieval results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results by Retrieval Modality</head><p>In 2006 and 2007, the results showed that by combining visual features from the image and semantic knowledge derived from the captions offered optimum performance for retrieval from a general photographic collection with fully annotated images <ref type="bibr" coords="8,164.78,571.26,10.84,9.02" target="#b1">[2,</ref><ref type="bibr" coords="8,179.04,571.26,7.23,9.02" target="#b2">3]</ref>. As indicated in Table <ref type="table" coords="8,284.07,571.26,3.76,9.02" target="#tab_6">6</ref>, the results of ImageCLEFphoto 2008 show that this also applies for our modified task, which promotes diversity in the results set. However, contrary to 2007 (24% MAP improvement over averages for combining techniques over solely text-based approaches), the improvement is not as clearly visible when combining visual features from the image and semantic information. The difference between "Mixed" and "Text Only" runs is across the averages from all runs, and differs only marginally. However, looking at the best runs in each modality, the "Mixed" runs (F1-Measure = 0.4650) outperform the "Text Only" runs by 16% (F1-measure = 0.4008). Purely content-based approaches still lag behind, although with a smaller gap than in previous years. The best "Image Only" runs (F1-Measure = 0.3396) is higher than both averages for the "Mixed" and "Text only" runs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision at 20</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results by Run Type</head><p>Table <ref type="table" coords="9,93.25,78.84,5.01,9.02" target="#tab_7">7</ref> shows the average scores and the standard deviations across all systems runs with respect to the run type. Unsurprisingly, F1-Measure results of manual approaches are significantly higher than purely automatic runs. All submitted manual runs are done with English annotation, whereas the average of the automatic runs is both from English as well as Random annotation. However, as previously shown the translation does not have a big impact and can therefore be neglected. In case of the automatic runs the F1-measure is practically identical for the English (ENG) annotations and those with the language randomly selected (RND).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision at 20</head><p>Cluster Recall at 20 F1-measure (P20/CR20) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Approaches Used by Participants</head><p>Some of the participating groups started by using a baseline run, carried out using different weighting methods (e.g. BM25, DFR, LM), with or without query expansion (e.g. using Local Content Analysis, Pseudo Relevance Feedback, thesaurus-based query expansion, Conceptual Fuzzy Sets, using a location hierarchy, and using Wordnet), and using content-and/or concept-based retrieval methods. The aim of this initial step was obtaining the best possible ranking (i.e. maximising the number of relevant documents returned in the top n). The most common following step was to re-rank the initial baseline run in order to promote diversity. One approach of re-ranking is to cluster the top n documents into sub-topics or clusters and then select the highest ranked document in each cluster and promote higher in the ranked list (i.e. to the top n). Clustering was mostly based on the associated textual information using various clustering algorithms (e.g. k-means, k-medoids, knn-density, and latent dirichlet allocation) and different weighting parameters. Some groups also tried to re-rank results using Maximal Marginal Relevance. Other approaches included merging different kind of runs (e.g. calculating image ranking with average/min/mean) or combining scores (novelty/ranking score) to get a diverse and relevant results list. Overall, a majority of approaches applied post-processing methods in one way or another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper has reported on the 2008 ImageCLEFphoto task, a general photographic ad-hoc retrieval task. The focus this year is different from this year and based on promoting diversity in the top n results. The challenge for participants was to maximise both the number of relevant images, as well as the number of sub-topics represented within the top 20 results. The 2008 task attracted a record number of submissions: 24 participating groups submitting a total of 1,042 system runs. The participants were provided with a subset of the IAPR TC-12 Benchmark: 20,000 colour photographs and two sets of semi-structured annotations in (1) English and (2) one set whereby the annotation language was randomly selected from English and German for each of the images. To measure the diversity of a ranked list, the existing collection was augmented with cluster assessments. Cluster assessments describe to which sub-topic a relevant image belongs to. Participants experimented with both contentand concept-based retrieval techniques. The main findings of this year include:</p><p>• Bilingual retrieval performs nearly as well as monolingual retrieval;</p><p>• Combining concept and content-based retrieval methods improves retrieval performance;</p><p>• A large number of participants used visual retrieval techniques (similar to previous years).</p><p>ImageCLEFphoto will continue to provide resources to the retrieval and computational vision communities to facilitate standardised laboratory-style testing of image retrieval systems. While these resources have predominately been used by systems applying a concept-based retrieval approach thus far, the number of participants who are using content-based retrieval techniques at ImageCLEFphoto is still increasing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,68.28,263.58,341.50,9.02;3,255.00,176.10,86.46,64.80"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example top 10 results set with a more diverse range of Australian animals</figDesc><graphic coords="3,255.00,176.10,86.46,64.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,68.28,564.00,460.03,135.68"><head>Table 1 :</head><label>1</label><figDesc>The cluster type in topic 48 is vehicle (in the &lt;cluster&gt; tag), which clearly defines how relevant images from this topic should be clustered. Different from previous years, topics were available in English only. Topics for the ImageCLEFphoto 2008 task.</figDesc><table coords="4,470.73,564.00,57.58,9.02"><row><cell>. Below is an</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,68.28,727.56,460.16,43.46"><head>Table 3</head><label>3</label><figDesc>Of all retrieval approaches, 61.2% involved the use of image retrieval (53.4% in 2007 and 31% in 2006), 79% of all groups used content-based (i.e. visual) information in their runs (60% in2007 and 58% in 2006). Almost all of the runs (99.7%) were automatic (i.e. involving no human intervention); only 3 submitted runs were manual. Only one participating group made use of additional data, which was available from the Visual Concept Detection Task 5 .</figDesc><table coords="7,71.10,128.88,453.68,108.80"><row><cell>Dimensions</cell><cell>Type</cell><cell>Runs</cell><cell cols="2">2008 Groups</cell><cell>Runs</cell><cell cols="2">2007 Groups</cell><cell>2006 Runs Groups</cell></row><row><cell cols="2">EN Annotation language RND</cell><cell cols="2">514 495</cell><cell>24 2</cell><cell cols="2">271 32</cell><cell>17 2</cell><cell>137</cell><cell>2</cell></row><row><cell></cell><cell>Text Only</cell><cell cols="2">404</cell><cell>22</cell><cell cols="2">167</cell><cell>15</cell><cell>121</cell><cell>2</cell></row><row><cell>Modality</cell><cell>Mixed (text and image)</cell><cell cols="2">605</cell><cell>19</cell><cell cols="2">255</cell><cell>13</cell><cell>21</cell><cell>1</cell></row><row><cell></cell><cell>Image Only</cell><cell cols="2">33</cell><cell>11</cell><cell cols="2">52</cell><cell>12</cell></row><row><cell>Run type</cell><cell>Manual Automatic</cell><cell cols="2">3 1039</cell><cell>1 25</cell><cell cols="2">19 455</cell><cell>3 19</cell><cell>142</cell><cell>2</cell></row></table><note coords="6,68.28,739.02,460.16,9.02;6,68.28,750.54,460.16,9.02;6,68.28,762.00,459.99,9.02;7,68.28,59.04,198.33,9.02"><p>provides an overview of all submitted runs according to these dimensions. Most submissions (96.8%) used the provided image annotations, with 22 groups submitting a total of 404 purely concept-based (textual) runs and 19 groups a total of 605 runs using a combination of content-based (visual) and concept-based features. A total of 11 groups submitted 33 purely content-based runs.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,68.28,252.30,184.26,9.02"><head>Table 3 :</head><label>3</label><figDesc>Submission overview by dimensions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,68.28,770.94,192.67,9.02"><head>Table 4 :</head><label>4</label><figDesc>Systems with the highest F1-Measure for English annotations</figDesc><table coords="8,71.10,59.17,454.45,425.20"><row><cell>Query language</cell><cell>Annotation language</cell><cell>Group</cell><cell>Run-ID</cell><cell cols="2">Run type Modality</cell><cell>P20</cell><cell>CR20</cell><cell>F1-Measure</cell></row><row><cell>English</cell><cell>English</cell><cell>PTECH</cell><cell>PTECH-EN-EN-MAN-TXTIMG-MMBQI.run</cell><cell>MAN</cell><cell>TXTIMG</cell><cell cols="2">0.6885 0.6801</cell><cell>0.6843</cell></row><row><cell>English</cell><cell>English</cell><cell>PTECH</cell><cell>PTECH-EN-EN-MAN-TXTIMG-MMBMI.run</cell><cell>MAN</cell><cell>TXTIMG</cell><cell cols="2">0.6962 0.6719</cell><cell>0.6838</cell></row><row><cell>English</cell><cell>English</cell><cell>PTECH</cell><cell>PTECH-EN-EN-MAN-TXT-MTBTN.run</cell><cell>MAN</cell><cell>TXT</cell><cell cols="2">0.5756 0.5814</cell><cell>0.5785</cell></row><row><cell>English</cell><cell>English</cell><cell>XRCE</cell><cell>xrce_tilo_nbdiv_15</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.5115 0.4262</cell><cell>0.4650</cell></row><row><cell>English</cell><cell>English</cell><cell>DCU</cell><cell cols="2">DCU-EN-EN-AUTO-TXTIMG-qe.txt AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.4756 0.4542</cell><cell>0.4647</cell></row><row><cell>English</cell><cell>English</cell><cell>XRCE</cell><cell>xrce_tilo_nbdiv_10</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.5282 0.4146</cell><cell>0.4646</cell></row><row><cell>English</cell><cell>English</cell><cell>XRCE</cell><cell>xrce_cm_nbdiv_10</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.5269 0.4111</cell><cell>0.4619</cell></row><row><cell>English</cell><cell>English</cell><cell>DCU</cell><cell>DCU-EN-EN-AUTO-TXTIMG.txt</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.4628 0.4546</cell><cell>0.4587</cell></row><row><cell>English</cell><cell>English</cell><cell>XRCE</cell><cell>xrce_cm_mmr_07</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.5282 0.4015</cell><cell>0.4562</cell></row><row><cell>English</cell><cell>English</cell><cell>XRCE</cell><cell>xrce_tfidf_nbdiv_10</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.5115 0.4081</cell><cell>0.4540</cell></row><row><cell>Query language</cell><cell>Annotation language</cell><cell>Group</cell><cell>Run-ID</cell><cell cols="2">Run type Modality</cell><cell>P20</cell><cell>CR20</cell><cell>F1-Measure</cell></row><row><cell>English</cell><cell>RND</cell><cell>DCU</cell><cell>DCU-EN-RND-AUTO-TXTIMG-tr.txt</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.4397 0.4673</cell><cell>0.4531</cell></row><row><cell>English</cell><cell>RND</cell><cell>DCU</cell><cell>DCU-EN-RND-AUTO-TXTIMG-tr-qe.txt</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.4423 0.4529</cell><cell>0.4475</cell></row><row><cell>English</cell><cell>RND</cell><cell>DCU</cell><cell>DCU-EN-RND-AUTO-TXTIMG-tr-d50-k40-tf-all.txt</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.4038 0.4967</cell><cell>0.4455</cell></row><row><cell>English</cell><cell>RND</cell><cell>DCU</cell><cell>DCU-EN-RND-AUTO-TXTIMG-tr-d50-k40-tfidf-all.txt</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.3974 0.4948</cell><cell>0.4408</cell></row><row><cell>English</cell><cell>RND</cell><cell>DCU</cell><cell>DCU-EN-RND-AUTO-TXTIMG-tr-d50-kx-tfidf-all.txt</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.3897 0.5049</cell><cell>0.4399</cell></row><row><cell>English</cell><cell>RND</cell><cell>DCU</cell><cell>DCU-EN-RND-AUTO-TXTIMG-tr-d50-kx-tf-qe-all.txt</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.4013 0.4806</cell><cell>0.4374</cell></row><row><cell>English</cell><cell>RND</cell><cell>DCU</cell><cell>DCU-EN-RND-AUTO-TXTIMG-tr-d50-kx-tf-all.txt</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.3910 0.4936</cell><cell>0.4363</cell></row><row><cell>English</cell><cell>RND</cell><cell>DCU</cell><cell>DCU-EN-RND-AUTO-TXTIMG-tr-d50-k40-tfidf-qe-all.txt</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.4013 0.4766</cell><cell>0.4357</cell></row><row><cell>English</cell><cell>RND</cell><cell>DCU</cell><cell>DCU-EN-RND-AUTO-TXTIMG-tr-d50-kx-tfidf-qe-all.txt</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.3897 0.4768</cell><cell>0.4289</cell></row><row><cell>English</cell><cell>RND</cell><cell>DCU</cell><cell>DCU-EN-RND-AUTO-TXTIMG-tr-d50-k40-tf-qe-all.txt</cell><cell>AUTO</cell><cell>TXTIMG</cell><cell cols="2">0.3897 0.4678</cell><cell>0.4252</cell></row></table><note coords="7,68.28,770.94,192.67,9.02"><p>5 http://www.imageclef.org/2008/iaprconcepts</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,68.28,499.08,364.25,9.02"><head>Table 5 :</head><label>5</label><figDesc>Systems with the highest F1-Measure for Random annotations (German / English)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,68.28,687.90,445.35,90.08"><head>Table 6 :</head><label>6</label><figDesc>Results by retrieval modality</figDesc><table coords="8,71.46,687.90,442.17,65.06"><row><cell>Modality</cell><cell>Mean</cell><cell>SD</cell><cell cols="2">Cluster Recall at 20 Mean SD</cell><cell cols="2">F1-measure (P20/CR20) Mean SD</cell></row><row><cell>Mixed</cell><cell>0.2538</cell><cell>0.1023</cell><cell>0.3998</cell><cell>0.0977</cell><cell>0.3034</cell><cell>0.0932</cell></row><row><cell>Text Only</cell><cell>0.2431</cell><cell>0.0590</cell><cell>0.3915</cell><cell>0.0819</cell><cell>0.2957</cell><cell>0.0576</cell></row><row><cell>Image Only</cell><cell>0.1625</cell><cell>0.1138</cell><cell>0.2127</cell><cell>0.1244</cell><cell>0.1784</cell><cell>0.1170</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,68.28,165.90,441.14,133.46"><head>Table 7 :</head><label>7</label><figDesc>Results by run type</figDesc><table coords="9,71.10,165.90,438.32,109.82"><row><cell>Technique</cell><cell>Mean</cell><cell>SD</cell><cell>Mean</cell><cell>SD</cell><cell>Mean</cell><cell>SD</cell></row><row><cell>Manual</cell><cell>0.6534</cell><cell>0.0675</cell><cell>0.6445</cell><cell>0.0548</cell><cell>0.6489</cell><cell>0.0610</cell></row><row><cell>Automatic</cell><cell>0.2456</cell><cell>0.0873</cell><cell>0.3899</cell><cell>0.0975</cell><cell>0.2955</cell><cell>0.0829</cell></row><row><cell>Automatic RND Only</cell><cell>0.2353</cell><cell>0.0651</cell><cell>0.4191</cell><cell>0.0731</cell><cell>0.2992</cell><cell>0.0679</cell></row><row><cell>Automatic ENG Only</cell><cell>0.2609</cell><cell>0.0990</cell><cell>0.3731</cell><cell>0.1002</cell><cell>0.2994</cell><cell>0.0879</cell></row><row><cell>Automatic IMG Only</cell><cell>0.1625</cell><cell>0.1138</cell><cell>0.2127</cell><cell>0.1244</cell><cell>0.1784</cell><cell>0.1170</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,74.04,770.94,76.13,9.02"><p>http://trec.nist.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,82.44,759.48,80.89,9.02"><p>http://www.iapr.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,82.44,770.94,99.91,9.02"><p>http://www.viventura.de/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>groups are affiliated to 21 different institutions in 11 countries. New participants submitting in 2008 include joint work from four <rs type="institution">French labs (AVEIR)</rs>, <rs type="institution">University of Waseda (GITS), Laboratory of Informatics of Grenoble (LIG)</rs>, <rs type="institution">System and Information Science Lab (LSIS)</rs>, <rs type="institution">Meiji University (Meiji)</rs>, <rs type="institution">University of Ottawa (Ottawa)</rs>, <rs type="institution">Telecom ParisTech (PTECH)</rs>, <rs type="institution">University of Sheffield (Shef)</rs>, <rs type="institution">University of Alicante (TEXTMESS)</rs> and <rs type="institution">Piere &amp;</rs> <rs type="funder">Marie Curie University (UPMC)</rs>. In total, 65% of the participants in 2007 returned and participated in 2008.</p></div>
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Michael Grubinger</rs> for providing the data collection and queries which formed the basis of the ImageCLEFPhoto task for 2008. Work undertaken in this paper is supported by the <rs type="funder">EU-</rs>funded <rs type="projectName">TrebleCLEF</rs> project (Grant agreement: <rs type="grantNumber">215231</rs>) and by the project <rs type="projectName">Multimatch</rs> (contract <rs type="grantNumber">IST-2005-2.5.10</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Hfy2Utb">
					<idno type="grant-number">215231</idno>
					<orgName type="project" subtype="full">TrebleCLEF</orgName>
				</org>
				<org type="funded-project" xml:id="_Sp9hJRZ">
					<idno type="grant-number">IST-2005-2.5.10</idno>
					<orgName type="project" subtype="full">Multimatch</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,91.68,161.16,436.60,9.02;10,91.68,172.68,436.68,9.02;10,91.68,184.14,167.26,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,326.26,161.16,202.02,9.02;10,91.68,172.68,59.97,9.02">Creating a test collection to evaluate diversity in image retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Arni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,170.10,172.68,358.26,9.02;10,91.68,184.14,162.39,9.02">Proceedings of the Workshop on Beyond Binary Relevance: Preferences, Diversity, and Set-Level Judgments, held at SIGIR2008</title>
		<meeting>the Workshop on Beyond Binary Relevance: Preferences, Diversity, and Set-Level Judgments, held at SIGIR2008</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.68,201.66,436.57,9.02;10,91.68,213.12,436.67,9.02;10,91.68,224.64,67.83,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,358.59,201.66,169.66,9.02;10,91.68,213.12,109.94,9.02">Overview of the ImageCLEFPhoto 2007 photographic retrieval task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,222.26,213.12,188.06,9.02">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">2007. 19-21 September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.68,242.10,436.65,9.02;10,91.68,253.62,436.66,9.02;10,91.68,265.08,436.69,9.02;10,91.68,276.60,392.34,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,408.64,242.10,119.69,9.02;10,91.68,253.62,231.91,9.02">Overview of the ImageCLEF 2006 photographic retrieval and object annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,344.31,253.62,184.03,9.02;10,91.68,265.08,398.54,9.02">Evaluation of Multilingual and Multi-modal Information Retrieval: Seventh Workshop of the Cross-Language Evaluation Forum (CLEF 2006)</title>
		<title level="s" coord="10,497.79,265.08,30.58,9.02;10,91.68,276.60,136.97,9.02">Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006-09-19">2006. September 19-21 2006</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="10,91.68,294.06,436.71,9.02;10,91.68,305.58,436.65,9.02;10,91.68,317.04,93.66,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,241.98,294.06,218.84,9.02">On the Creation of Query Topics for ImageCLEFPhoto</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,479.45,294.06,48.94,9.02;10,91.68,305.58,350.45,9.02">Proceedings of the third MUSCLE / ImageCLEF workshop on image and video retrieval evaluation</title>
		<meeting>the third MUSCLE / ImageCLEF workshop on image and video retrieval evaluation<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">2007. 19-21 September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.68,334.56,234.09,9.02;10,383.23,334.56,145.03,9.02;10,91.68,346.02,314.78,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,294.71,334.56,31.06,9.02;10,383.23,334.56,145.03,9.02;10,91.68,346.02,114.75,9.02">Beyond relevance: Methods and evaluation metrics for subtopic retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,223.30,346.02,133.59,9.02">Proceedings of ACM SIGIR 2003</title>
		<meeting>ACM SIGIR 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.68,363.54,436.74,9.02;10,91.68,374.99,373.51,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,257.50,363.54,254.69,9.02">Overview of the Seventh Text REtrieval Conference (TREC-7)</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,91.68,374.99,156.26,9.02">The Seventh Text Retrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-11">1998. November 1998</date>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.68,392.51,436.61,9.02;10,91.68,403.97,436.75,9.02;10,91.68,415.49,436.55,9.02;10,91.68,426.95,10.05,9.02;10,101.76,424.87,6.48,5.83;10,110.76,426.96,43.29,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,368.59,392.51,159.70,9.02;10,91.68,403.97,210.45,9.02">The IAPR-TC12 Benchmark: A New Evaluation Resource for Visual Information Systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,320.52,403.97,207.91,9.02;10,91.68,415.49,330.36,9.02">International Workshop OntoImage&apos;2006 Language Resources for Content-Based Image Retrieval, held in conjunction with LREC&apos;06</title>
		<meeting><address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05-22">2006. 22 nd May 2006</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.68,444.48,436.79,9.02;10,91.68,455.94,436.68,9.02;10,91.68,467.46,224.09,9.02" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,260.68,444.48,155.95,9.02">Diversifying the image retrieval results</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,433.61,444.48,94.86,9.02;10,91.68,455.94,222.51,9.02;10,91.68,467.46,76.23,9.02">Proceedings of the 14th Annual ACM international Conference on Multimedia</title>
		<meeting>the 14th Annual ACM international Conference on Multimedia<address><addrLine>Santa Barbara, CA, USA; New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006-10-23">2006. October 23 -27, 2006</date>
			<biblScope unit="page" from="707" to="710" />
		</imprint>
	</monogr>
	<note>MULTIMEDIA &apos;06</note>
</biblStruct>

<biblStruct coords="10,91.68,484.92,436.68,9.02;10,91.68,496.44,436.65,9.02;10,91.68,507.90,436.70,9.02;10,91.68,519.42,114.58,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,252.48,484.92,275.89,9.02;10,91.68,496.44,41.21,9.02">Less is more: probabilistic models for retrieving fewer relevant documents</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,155.06,496.44,373.27,9.02;10,91.68,507.90,154.34,9.02;10,457.35,507.90,39.79,9.02">Proceedings of the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval</title>
		<meeting>the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval<address><addrLine>Seattle, Washington, USA; New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006-08-06">2006. August 06 -11, 2006</date>
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;06</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
