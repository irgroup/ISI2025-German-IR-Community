<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,98.88,97.87,404.93,17.23;1,155.16,119.83,292.58,17.23">Concept Content Based Wikipedia WEB Image Retrieval using CLEF VCDT 2008</title>
				<funder ref="#_5EWdhYu">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,223.80,154.07,72.42,9.96"><forename type="first">Zhongqiu</forename><surname>Zhao</surname></persName>
							<email>zhongqiuzhao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire des sciences de l&apos;information et des systemes</orgName>
								<orgName type="institution" key="instit1">UMR CNRS</orgName>
								<orgName type="institution" key="instit2">Universite&apos; Sud Toulon-Var France</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.61,154.07,68.56,9.96"><forename type="first">Herve</forename><surname>Glotin</surname></persName>
							<email>glotin@univ-tln.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire des sciences de l&apos;information et des systemes</orgName>
								<orgName type="institution" key="instit1">UMR CNRS</orgName>
								<orgName type="institution" key="instit2">Universite&apos; Sud Toulon-Var France</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,98.88,97.87,404.93,17.23;1,155.16,119.83,292.58,17.23">Concept Content Based Wikipedia WEB Image Retrieval using CLEF VCDT 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F6CB8051E362D5983A8D5891ED3CE3C3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Infor-mation Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Rank Fusion, Image Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One challenge for this Wikipedia task is the training of visual models. We propose in this paper to link each topics one or few visual concepts of the Visual Concept Detection (VCDT) CLEFimage08 task, even if three topics do not fit VCDT concepts. We use the same models and features than in our VCDT systems. We show that our visual IMG NOFB run is the second best model in this campaign for this run type. So it can be concluded that our VCDT visual concept partly fit this task. Moreover we show that even a simple boolean text analysis overcomes the best IMG NO FEEDBACK run, which has 0.0037 MAP, against 0.399 for our TXT NOFB text run. This emphases the fact that visual retrieval for Wiki task is very difficult.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEF's 2008 wikipediaMM task provides a test bed for the system-oriented evaluation of visual information retrieval from a collection of Wikipedia images. The aim is to investigate retrieval approaches in the context of a larger scale and heterogeneous collection of images (similar to those encountered on the Web) that are searched for by users with diverse information needs.</p><p>The given images are associated with unstructured and noisy textual annotations in English. This is an ad-hoc image retrieval task; the evaluation scenario is thereby similar to the classic TREC ad-hoc retrieval task and the ImageCLEFphoto task: simulation of the situation in which a system knows the set of documents to be searched, but cannot anticipate the particular topic that will be investigated (i.e. topics are not known to the system in advance). The goal of the simulation is: given a textual query (and/or sample images and/or concepts) describing a user's Figure <ref type="figure" coords="2,208.17,248.63,3.90,9.96">1</ref>. An example image and its associated metadata (multimedia) information need, find as many relevant images as possible from the wikipedia image collection.</p><p>The characteristics of the (INEX MM) wikipedia image collection allow for the investigation of the following objectives: how well do the retrieval approaches cope with larger scale image collections? How well do the retrieval approaches cope with noisy and unstructured textual annotations? How well do the content-based retrieval approaches cope with images that cover diverse topics and are of varying quality? How well can systems exploit and combine different modalities given a user's multimedia information need? Can they outperform monomodal approaches like query-by-text, query-by-concept or query-by-image? In the context of INEX MM 2006-2007, mainly text-based retrieval approaches have been examined. This task is done to attract more visually-oriented approaches and most importantly, multimodal approaches that investigate the combination of evidence from different modalities.</p><p>In this paper we present our strategy to retrieve relevant documents using a concept-contentbased retrieval method in reusing the Visual Concept models we built for the CLEF VCDT 2008 task, without exclusively training the svms for this task. Then we test a very simple text analysis for a multimodal fusion. Results are showing that the visual process works badly, and does not contribute to the fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Material 3 Using Visual Concepts Detection (VCDT) Clef08 models</head><p>A possible source of information to help participants in the retrieval tasks was, For each image, the classification scores for the 101 different MediaMill <ref type="bibr" coords="2,341.55,688.79,10.55,9.96" target="#b0">[1]</ref> concepts provided by University of Amsterdam (UvA). The UvA classifier is trained on manually annotated TRECVID video data and the concepts are selected for the broadcast news domain.</p><p>Anyway, visual concept information can be given by a visual concept training as specified in the VCDT CLEF task. We choose this last solution to get an idea of which level of visual information we can get from few visual concept classes.</p><p>There are 17 VCDT topics that correspond to most of the 75 concept-topics as depicted in Table <ref type="table" coords="3,116.62,462.47,3.90,9.96" target="#tab_0">1</ref>. Only few wiki concepts are not described by VCDT : "map" and "charts" and "cartoons", this will decrease the global visual classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Visual Feature Extraction</head><p>An important step in content-based image retrieval (CBIR) system is the extraction of discriminant visual feature that are fast to compute. Information theory and Cognitive sciences can provide some inspiration for developping such feature.</p><p>Among the many visual features that have been studied, the distribution of color pixels in an image is the most common visual feature studied. The standard representation of color for contentbased indexing in image databases is the color histogram. A different color representation is based on the information theoretic concept of entropy. Such entropic feature can simply equal the entropy of the pixel distribution of the image, as proposed in <ref type="bibr" coords="3,324.67,612.95,9.98,9.96" target="#b2">[3]</ref>. A more theoretical presentation of this kind of image entropy feature, accompanied by a practical description of its merits and limitations compared to color histograms, has been given in <ref type="bibr" coords="3,304.38,636.83,9.98,9.96" target="#b3">[4]</ref>. We propose <ref type="bibr" coords="3,376.32,636.83,11.00,9.96" target="#b4">[5,</ref><ref type="bibr" coords="3,387.32,636.83,7.34,9.96" target="#b5">6]</ref> a new feature equal to the pixel 'profil' entropy. A pixel profil can be a simple arithmetic mean in horizontal (or vertical) direction. The advantage of such feature is to combine raw shape and texture representations in a low cpu cost feature. These feature, associated to mean and color std, reached the second best rank in the official ImagEval 2006 campaing (see www.imageval.org). For CLEF we extend these features using another projection to get the pixel profil. We use the harmonic mean of the pixel of each ligne or column. The idea is that the object or pixel region distribution, which is lost in arithmetic mean projection, could be partly catch by the harmonic mean. These two projections are then expected to give complementary and/or concept dependant informations. Details can be found in <ref type="bibr" coords="4,129.59,109.31,9.98,9.96" target="#b4">[5]</ref>. The extraction is summarized.</p><p>Let I be an image, or any rectangular subpart of an image.</p><p>For each normalized color (L = R + G + B, r = R/L, andg = G/L), we first calculate two orthogonal profils by the projections of the pixels of I. We consider two simple orthogonal projection axes : the horizontal axis X (noted Π X ), versus the vertical one Y (noted Π Y ). The projection operator is either the arithmetic mean (noted 'Ar', then the projection is noted Π Ar X ), as illustrated in Figure <ref type="figure" coords="4,194.32,180.95,3.90,9.96" target="#fig_1">2</ref>, or the harmonic mean of the pixels on each column or each ligne of I (noted 'Ha', then we have Π Ha X ). Then, we estimate the probability distribution function (pdf) of each profil according to <ref type="bibr" coords="4,499.39,204.95,9.98,9.96" target="#b6">[7]</ref>. Considering that the sources are ergodic, we finaly calculate each PEF equal to the normalized entropy (H(pdf )/log(#bins(pdf ))). We detail below each steps of the PEF extraction.</p><p>Let be op the selected projection, for each color of I of L(I) lignes and C(I) columns : And we finaly complete the PEF features by the usual mean and standard deviation of each normalized color of I.</p><formula xml:id="formula_0" coords="4,105.00,285.95,95.62,14.46">Φ op X (I) = p df (Π op X (I)),</formula><p>We can calculate the PEF into three horizontal subimages (see Glotin Zhao VCDT CLEF papers for details). We note such PEF '='. We also calculate the PEF in three vertical subimages, we note these PEF ' '.</p><p>For each, we have 3 bands and 3 different PEF for each of the 3 colors, plus their mean and variance, thus we have 3 * 3 * 3 + 3 * 3 * 2 = 45 dimensions for '=' or for ' ' features. We note '+' the feature concatenation of '=' and ' ' features, which has then 90 dimensions. Considering the two mean type, the PEF concatenation without repetition of the mean and std color are quite compact with a total of 126 dimensions (= 2 (subimages type '=' or ' ') * 3 (bands by subimages type) * 3 (rgL) * 4 (=4 Π types = (X or Y) * (Ar or Ha) ) + 1 (=H(I)) + 2 (= mean and std))).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Text Retrieval</head><p>Because of the very small length of the xml text data associated to the image, we selected almost 500 words defining the requests. We count these word occurences in each xml file. The text score for each xml file and each query is then simple the sum of theses occurences for each selected words of the query. This lead to a very fast text classification : for the whole 150 000 xml files only few minutes are necessary on a pentium IV. Our goal was not to produce a very efficient text retrieval system, but to compare our visual concept approach to the most basic text retrieval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>The support vector machine (SVM) <ref type="bibr" coords="5,249.23,363.59,10.55,9.96" target="#b7">[8]</ref> first maps the data into a higher dimensional input space by some kernel functions, and then to learn a separating hyperspace to maximize the margin. Currently, because of its good generalization capability, this technique has been widely applied in many areas such as face detection, image retrieval, and so on. The SVM is typically based on an ε-insensitive cost function, meaning that approximation errors smaller than will not increase the cost function value. This results in a quadratic convex optimization problem. So instead of using an ε-insensitive cost function, a quadratic cost function can be used. The least squares support vector machines (LS-SVM) <ref type="bibr" coords="5,214.10,447.35,10.55,9.96" target="#b8">[9]</ref> are reformulations to the standard SVMs which lead to solving linear KKT systems instead. It is computationally attractive.</p><p>In our experiments, the RBF kernel</p><formula xml:id="formula_1" coords="5,225.72,491.83,151.56,12.01">K(x 1 -x 2 ) = exp(-|x 1 -x 2 | 2 /σ 2 )</formula><p>is selected as the kernel function of our LS-SVM. So there is a corresponding parameter, σ , to be tuned. A large value of σ 2 indicates a stronger smoothing. Moreover, there is another parameter, γ, needing tuning to find the tradeoff between to stress minimizing of the complexity of the model and to stress good fitting of the training data points. We trained 100 SVMs with different parameter values for each topic, and selected the best SVM using the validation set.</p><p>For Image or Text classification we do not make any Feedback nor Query Expansion techniques. So our runs are for NOFB type. In our experiments, we computed the average of the ranks of TXT and VISUAL. The process we adopt to implement the image retrieval in photo task is shown in Figure <ref type="figure" coords="5,133.06,634.67,5.03,9.96" target="#fig_2">3</ref> and depicted as the following steps:</p><p>Step 1) According to the keywords of each topic, perform the text retrieval on the WIKI XML text database, and then get the rank result which is called Rank-Text.</p><p>Step 2) Split the VCDT labeled image dataset into 2 sets, namely training image dataset and Step 3) Extract the visual features from the training image data using our extraction method; train and generate lots of SVMs with different parameters.</p><p>Step 4) Use the validation set to select the best one among the SVMs.</p><p>Step 5) Extract the visual features from the WIKI visual image database using our extraction method; use the best SVM as the tool to perform the image retrieval and produce the rank result called Rank-Visual.</p><p>Step 6) Merge Rank-Visual and Rank-Text into Final Rank using the weights, where 't' denotes the text ratio (t is fixed = 0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>The results for the wikipediaMM task have been computed with the trec eval tool (version 8.1). The submitted runs have been corrected (where necessary) so as to correpond to valid runs in the correct TREC format. The following corrections have been made: The runs comply with the TREC fomat as specified in the submission guidelines for the task When a topic contains an image example that is part of the wikipediaMM collection, this image is removed from the retrieval results, i.e., we are seeking relevant images that the users are not familiar with (as they are with the images they provided as examples). When an image is retrieved more than once for a given topic, only its highest ranking for that topic is kept and the rest are removed (and the ranks in the retrieval results are appropriately fixed).</p><p>The interpolated recall precision averages are shown in Figure <ref type="figure" coords="6,381.83,604.67,3.90,9.96" target="#fig_3">4</ref>, and the summary statistics for the runs sorted by MAP are shown in Table <ref type="table" coords="6,302.95,616.67,3.90,9.96" target="#tab_1">2</ref>, from which we see that TXT method did the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Our visual IMG NOFB run is the second best model in this campaign. So it can be concluded that our PEF VCDT visual concept partly fit this task. We show that even a simple text analysis  overcomes the best IMG NO FEEDBACK from Imperial College which has 0.0037 MAP, against 0.399 for our TXT NOFB text run. This emphases the fact that visual retrieval for Wiki task is very difficult. This can explain why our 3 basic fusions methods TXTIMG did not improve the TXT run. One can then conclude that the Feedback seems necessary for such task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,204.10,288.59,163.55,10.26;4,90.00,300.59,36.21,9.96;4,126.24,298.29,8.03,6.33;4,126.24,305.97,6.48,6.33;4,138.12,300.59,185.34,9.96;4,90.00,312.47,76.04,10.26;4,166.08,310.17,8.03,6.33;4,166.08,317.97,6.48,6.33;4,174.60,312.47,85.42,10.26;4,105.00,336.47,7.19,9.96;4,112.20,334.17,8.03,6.33;4,112.20,341.85,4.68,6.33;4,120.72,333.83,51.92,12.60;4,172.68,334.17,8.03,6.33;4,172.68,341.85,4.68,6.33;4,181.20,336.47,184.65,10.26;4,90.00,348.35,75.32,10.26;4,165.36,346.05,8.03,6.33;4,165.36,353.85,4.68,6.33;4,173.88,348.35,84.70,10.26;4,105.00,372.23,222.27,10.26;4,94.80,381.59,338.86,12.90;4,90.00,394.67,170.38,12.90"><head></head><label></label><figDesc>over nbin X (I) = round( C(I)) bins, where Π op X is the vertical projection with operator op, P EFX (I) = H(Φ op X (I))/log(nbin X (I)). Φ op Y (I) = p df (Π op Y (I)), over nbin Y (I) = round( L(I)) bins, P EF Y (I) = H(Φ op Y (I))/log(nbin Y (I)).We add to these P EF a the usual entropic feature : p df (I) = pdf of all the pixels of I over nbin XY (I) = nbin X (I) * nbin Y (I) bins, P EF . (I) = H( p df (I))/log(nbin XY (I)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,90.00,287.51,422.77,9.96;5,90.00,299.51,288.83,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the horizontal and vertical profils using simple arithmetic projection (or sum) of each normalized color r = R/L, g = G/L, L = R + G + B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,169.80,268.43,263.15,9.96;6,111.12,58.94,380.70,195.01"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The Framework for Image Retrieval of Each Topic</figDesc><graphic coords="6,111.12,58.94,380.70,195.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,195.36,379.67,212.31,9.96;7,132.24,111.83,338.40,253.47"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Interpolated Recall Precision Averages</figDesc><graphic coords="7,132.24,111.83,338.40,253.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,153.48,58.97,296.10,175.17"><head></head><label></label><figDesc></figDesc><graphic coords="2,153.48,58.97,296.10,175.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,108.36,61.43,386.10,305.40"><head>Table 1 .</head><label>1</label><figDesc>The table of all the wiki concepts and the links with VCDT CLEF 2008 topics</figDesc><table coords="3,108.36,73.69,386.10,293.14"><row><cell>WIKI CONCEPTS</cell><cell>VCDT TOPICS</cell></row><row><cell>'aircraft'</cell><cell>'sky'</cell></row><row><cell>'boat'</cell><cell>'water'</cell></row><row><cell>'building'</cell><cell>'buildings'</cell></row><row><cell>'car'</cell><cell>'road'</cell></row><row><cell>'cartoon', 'drawing cartoon', 'drawing', 'graphics'</cell><cell>NONE</cell></row><row><cell>'charts'</cell><cell>NONE</cell></row><row><cell>'cloud'</cell><cell>'partly cloudy' or 'overcast'</cell></row><row><cell>'crowd'</cell><cell>'person'</cell></row><row><cell>'desert'</cell><cell>NO 'vegetation' and NO 'water' and</cell></row><row><cell></cell><cell>NO 'beach' and NO 'buildings'</cell></row><row><cell>'dog'</cell><cell>'animal'</cell></row><row><cell>'female'</cell><cell>'person'</cell></row><row><cell>'grass'</cell><cell>'vegetation'</cell></row><row><cell>'hu jintao'</cell><cell>'person'</cell></row><row><cell>'maps'</cell><cell>NONE</cell></row><row><cell>'mountain'</cell><cell>'mountains'</cell></row><row><cell>'outdoor'</cell><cell>'outdoor'</cell></row><row><cell>'people', 'people marching'</cell><cell>'person'</cell></row><row><cell>'sports'</cell><cell>'person'</cell></row><row><cell>'studio'</cell><cell>'day'</cell></row><row><cell>'tree'</cell><cell>'tree'</cell></row><row><cell>'vegetation'</cell><cell>'vegetation'</cell></row><row><cell>'waterbody'</cell><cell>'water' and 'person'</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,90.00,504.61,474.98,145.66"><head>Table 2 .</head><label>2</label><figDesc>The run results, with the 2 other runs of the campaign with visual NOFB runs showing that our visual concept approach is good for this run type. The basic text retrieval and fusion are also given for comparison.</figDesc><table coords="7,96.00,504.61,468.98,99.94"><row><cell>Team</cell><cell>Run</cell><cell cols="4">Modality MAP P@5 P@10 R-prec Bpref</cell></row><row><cell>imperial</cell><cell>visual only run AUTO NOFB</cell><cell>IMG</cell><cell>0.0037 0.0187 0.0147</cell><cell>0.0108</cell><cell>0.0086</cell></row><row><cell>LSIS utoulon</cell><cell>LSIS-IMG-AUTO-NOFB</cell><cell>IMG</cell><cell>0.0020 0.0027 0.0027</cell><cell>0.0049</cell><cell>0.0242</cell></row><row><cell>upmc-lip6</cell><cell>visual only run AUTO NOFB</cell><cell>IMG</cell><cell>0.0010 0.0107 0.0080</cell><cell>0.0053</cell><cell>0.0038</cell></row><row><cell>LSIS utoulon</cell><cell>LSIS-TXT-method1</cell><cell>TXT</cell><cell>0.0399 0.0507 0.0467</cell><cell>0.0583</cell><cell>0.0662</cell></row><row><cell cols="4">LSIS utoulon LSIS4-TXTIMG-AUTO-NOFB TXTIMG 0.0296 0.0347 0.0307</cell><cell>0.0421</cell><cell>0.0578</cell></row><row><cell cols="2">LSIS utoulon LSIS-IMGTXT-AUTO-NOFB</cell><cell cols="2">TXTIMG 0.0260 0.0267 0.0253</cell><cell>0.0349</cell><cell>0.0547</cell></row><row><cell cols="2">LSIS utoulon LSIS-TXTIMG-AUTO-NOFB</cell><cell cols="2">TXTIMG 0.0233 0.0533 0.0480</cell><cell>0.0300</cell><cell>0.0542</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was partially supported by the <rs type="funder">French National Agency of Research</rs> (<rs type="grantNumber">ANR-06-MDCA-002</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5EWdhYu">
					<idno type="grant-number">ANR-06-MDCA-002</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,105.47,218.75,407.23,9.96;8,105.48,230.75,407.20,9.96;8,105.48,242.75,407.19,9.96;8,105.48,254.63,273.93,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,105.48,230.75,389.19,9.96">The challenge problem for automated detection of 101 semantic concepts in multimedia</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-M</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,105.48,242.75,407.19,9.96;8,105.48,254.63,23.99,9.96">MULTIMEDIA 06: Proceedings of the 14th annual ACM international conference on Multimedia</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.47,274.55,407.20,9.96;8,105.48,286.55,407.17,9.96;8,105.48,298.55,407.16,9.96;8,105.48,310.43,372.97,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,242.87,274.55,145.39,9.96">The INEX 2006 Multimedia Track</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,197.32,286.55,315.32,9.96;8,105.48,298.55,255.37,9.96">Advances in XML Information Retrieval:Fifth International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX</title>
		<title level="s" coord="8,391.42,298.55,121.22,9.96;8,105.48,310.43,262.03,9.96">Lecture Notes in Computer Science/Lecture Notes in Artificial Intelligence (LNCS/LNAI</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Trotman</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.47,330.35,407.19,9.96;8,105.48,342.35,407.17,9.96;8,105.48,354.35,22.88,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,173.21,330.35,339.45,9.96;8,105.48,342.35,132.79,9.96">Saliency maps and attention selection in scale and spatial coordinates: An information theoretic approach</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,257.15,342.35,251.18,9.96">Proc. of 5th International Conference on Computer Vision</title>
		<meeting>of 5th International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.47,374.27,407.28,9.96;8,105.48,386.15,407.18,9.96;8,105.48,398.15,407.18,9.96;8,105.48,410.15,36.30,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,276.20,374.27,236.55,9.96;8,105.48,386.15,407.18,9.96;8,105.48,398.15,28.68,9.96">Content based image retrieval and information theory: A generalized approach, in Special Topic Is-sue on Visual Based Retrieval Systems and Web Mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barhen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,143.03,398.15,320.51,9.96">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="page" from="841" to="853" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.47,430.07,407.41,9.96;8,105.48,441.95,401.63,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,190.37,430.07,322.51,9.96;8,105.48,441.95,39.43,9.96">Profil Entropic visual Features for Visual Concept Detection in CLEF 2008 campaign</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,165.56,441.95,149.92,9.96">Working Notes of ImageCLEF2008</title>
		<meeting><address><addrLine>Danmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>in conjuction with ECDL</note>
</biblStruct>

<biblStruct coords="8,105.47,461.87,407.17,9.96;8,105.48,473.87,407.39,9.96" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,156.93,461.87,355.71,9.96;8,105.48,473.87,56.31,9.96">Robust Information Retrieval and perception for a scaled Lego-Audio-Video multistructuration</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>University Sud Toulon-Var</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Thesis of habilitation for research direction</note>
</biblStruct>

<biblStruct coords="8,105.47,493.79,407.22,9.96;8,105.48,505.79,216.98,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,180.42,493.79,328.15,9.96">On estimation of entropy and mutual information of continuous distributions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Moddemeijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,105.48,505.79,74.32,9.96">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="246" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.47,525.71,293.12,9.96" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m" coord="8,179.08,525.71,110.93,9.96">Statistical learning theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.47,545.63,407.40,9.96;8,105.48,557.51,196.57,9.96" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,293.47,545.63,219.40,9.96;8,105.48,557.51,110.90,9.96">Least Squares Support Vector Machine Classifiers Neural Processing Letters</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="293" to="300" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
