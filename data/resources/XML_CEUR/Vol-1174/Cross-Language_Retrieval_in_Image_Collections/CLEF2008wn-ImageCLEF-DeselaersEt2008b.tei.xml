<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.50,148.86,325.99,15.15;1,228.01,170.78,146.99,15.15">The Visual Concept Detection Task in ImageCLEF 2008</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,213.97,204.67,78.65,8.74"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
							<email>deselaers@cs.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.78,204.67,64.79,8.74"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
							<email>hanbury@prip.tuwien.ac.at</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computer-Aided Automation</orgName>
								<orgName type="laboratory">PRIP</orgName>
								<orgName type="institution">Vienna University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.50,148.86,325.99,15.15;1,228.01,170.78,146.99,15.15">The Visual Concept Detection Task in ImageCLEF 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">43AE78083E824BE80AF484262AA10325</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>visual concept detection</term>
					<term>image annotation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Visual Concept Detection Task (VCDT) of ImageCLEF 2008 is described. A database of 2,827 images were manually annotated with 17 concepts. Of these, 1,827 were used for training and 1,000 for testing the automated assignment of categories. In total 11 groups participated and submitted 53 runs. The runs were evaluated using ROC curves, from which the Area Under the Curve (AUC) and Equal Error Rate (EER) were calculated. For each concept, the best runs obtained an AUC of 80% or above.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Searching for images is, despite intensive research on alternative methods in the last 20 years, still a task which is mainly done based on textual information. For a long time, searching for images based on text was the most feasible method because on the one hand, the number of images to be searched was rather restricted, and on the other hand, only few people needed to access huge repositories of images. Both of these conditions have changed. The number of available images is growing more rapidly than ever due to the falling prices of high-end imaging equipment for professional use and of digital cameras for consumer use. Publicly available image databases such as Google picassa and Flickr have become major sites of interest on the Internet.</p><p>Nevertheless, accessing images is still a tedious task because sites such as Flickr do not allow images to be accessed based on their content but only based on the annotations that users create. These annotations are commonly disorganised, not very precise, and multilingual. Access problems can be addressed by improving the textual access methods, but none of these improvements can ever be perfect as long as the users do not annotate their images perfectly, which is very unlikely. Therefore, content-based methods have to be employed to improve access methods to digitally stored images.</p><p>A problem with content-based methods is that they are often costly and cannot be applied in real-time. An intermediate step is to automatically create textual labels based on the images' content. To make these labels as useful as possible, frequently occurring visual concepts should be annotated in a standard manner.</p><p>In the visual concept detection task (VCDT) of ImageCLEF 2008, the aim was to apply labels of frequent categories in the photo retrieval task to the images and evaluate how well automated visual concept annotation algorithms function. Additionally, participants of the VCDT could create annotations for all images used in the photo retrieval task, which were provided to the participants of this task.  In the following, we describe the visual concept detection task of ImageCLEF 2008, the database used, the methods of the participating groups, and the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Database and Task Description</head><p>As database for the ImageCLEF 2008 visual concept detection task, a total of 2,827 images were used. These are taken from the same pool of images used to create the IAPR-TC12 database <ref type="bibr" coords="2,499.71,464.41,9.96,8.74" target="#b1">[2]</ref>, but are not included in the IAPR-TC12 database used in the ImageCLEF photo retrieval task.</p><p>The visual concepts were chosen based on concepts used in work on visual concept annotation. They were organised in a hierarchy, shown in Figure <ref type="figure" coords="2,321.74,500.28,3.87,8.74" target="#fig_0">1</ref>.</p><p>As for the object detection task in 2007 <ref type="bibr" coords="2,278.77,512.23,9.96,8.74" target="#b0">[1]</ref>, a web interface was created for manual annotation of the images by the concepts. Annotation was mainly carried out by undergraduate students at the RWTH Aachen University and by the track coordinators. A general opinion expressed by the annotators was that the concept annotation required more time than the object annotation of 2007. The number of images that were voluntarily annotated this year was also significantly smaller than the 20,000 images annotated by object labels in 2007.</p><p>Of the 2,827 manually annotated images, 1,827 were distributed with annotations to the participants as training data. The remaining 1,000 images were provided without labels as test data. The participants' task was to apply labels to these 1,000 images.</p><p>Table <ref type="table" coords="2,132.55,619.83,4.98,8.74" target="#tab_1">1</ref> gives an overview of the frequency of the 17 visual concepts in the training data and in the test data and Figure <ref type="figure" coords="2,211.71,631.78,4.98,8.74" target="#fig_1">2</ref> gives an example image for each of the categories.</p><p>For each run, results for each concept were evaluated by plotting ROC curves. The results for each concept were summarised by two values: the area under the ROC curve (AUC) and the Equal Error Rate (EER). The latter is the error rate at which the false positive rate is equal to the false negative rate. Furthermore, for each run, the average AUC and average EER over all concepts were calculated.  LSIS. The Laboratory of Information Science and Systems, France submitted 7 runs using a structural feature combined with several other features using multi-layer perceptrons.</p><p>MMIS. The Multimedia and Information Systems Group of the Open University, UK submitted 4 runs using CIELAB and Tamura features and combinations of these.</p><p>Makerere. The Faculty of Computing and Information Technology, Makerere University, Uganda submitted one run using luminance, dominant colors, and different texture and shape features which are classified using a nearest neighbour classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RWTH. The Human Language Technology and Pattern Recognition Group from RWTH Aachen</head><p>University, Germany submitted one run using a patch-based bag-of-visual words approach using a log-linear classifier.</p><p>TIA. The Group for Machine Learning for Image Processing and Information Retrieval from the National Institute of Astrophysics, Optics and Electronics, Mexico submitted 7 runs using global and local features with support vector machines and random forest classifiers.</p><p>UPMC. The University Pierre et Marie Curie in Paris, France submitted 5 runs using fuzzy decision forests.</p><p>XRCE. The Textual and Visual Pattern Analysis group from the Xerox Research Center Europe in France submitted two runs using multi-scale, regular grid, patch-based image features and a Fisher-Kernel Vector classifier.</p><p>budapest. The Datamining and Websearch Research Group, Hungarian Academy of Sciences, Hungary submitted 13 runs using a wide variety of different features, classifiers, and combinations.</p><p>The average EER and average AUC for each submitted run are given in Table <ref type="table" coords="5,456.72,199.69,3.87,8.74" target="#tab_2">2</ref>. From this table, it can be seen that the best overall runs were submitted by XRCE.</p><p>Table <ref type="table" coords="5,132.66,223.60,4.98,8.74" target="#tab_3">3</ref> shows a breakdown of the results per concept. For each concept, the best and worst EER and AUC are shown, along with the average EER and AUC over all runs submitted. The best results were obtained for all concepts by XRCE, with budapest doing equally well on the night concept. The AUC per concept for all the best runs is 80.0% or above. Among the best results, the concepts having the highest scores are indoor and night. The concept with the worst score among the best results is road or pathway, most likely due to the high variability in the appearance of this concept. The concept with the highest average score, in other words, the concept that was detected best in most runs is sky. Again, the concept with the worst average score is road or pathway.</p><p>Only one group participating in the photo retrieval task of ImageCLEF made use of the concept annotation created by participants of the VCDT. The group from Universit Pierre et Marie Curie in Paris, France used the annotation in 12 of their 19 runs. However, the runs of that group were not ranked very highly and thus it is hard to judge how big the impact was. Their best two runs used the annotation and have a P (20) value of slightly over 0.26 while their third best run, which did not use these data has a P (20) value of 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper summarises the ImageCLEF 2008 Visual Concept Detection Task. The aim was to automatically annotate images with concepts, with a list of 17 hierarchically organised concepts provided. The results demonstrate that this task can be solved reasonably well, with the best run having an average AUC over all concepts of 90.66%. Six further runs obtained AUCs between 80% and 90%. When evaluating the runs on a per concept basis, the best run also obtained an AUC of 80% or above for every concept. Concepts for which automatic detection was particularly successful are: indoor/outdoor , night, and sky. The worst results were obtained for the concept road or pathway.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,90.00,343.89,423.00,8.74;2,90.00,355.85,22.69,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The visual concept hierarchy as used in the visual concept detection task of ImageCLEF 2008.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,186.83,660.19,229.33,8.74;3,274.71,553.21,53.58,80.37"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example images for each of the categories.</figDesc><graphic coords="3,274.71,553.21,53.58,80.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,118.94,423.00,414.10"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the frequency of topics in the training and test data.</figDesc><table coords="4,90.00,132.75,317.38,251.68"><row><cell cols="2">number category</cell><cell cols="2">train [%] test [%]</cell></row><row><cell>00</cell><cell>indoor</cell><cell>9.9</cell><cell>10.2</cell></row><row><cell>01</cell><cell>outdoor</cell><cell>88.0</cell><cell>88.1</cell></row><row><cell>02</cell><cell>person</cell><cell>43.8</cell><cell>44.9</cell></row><row><cell>03</cell><cell>day</cell><cell>82.0</cell><cell>81.9</cell></row><row><cell>04</cell><cell>night</cell><cell>3.7</cell><cell>2.3</cell></row><row><cell>05</cell><cell>water</cell><cell>23.1</cell><cell>21.7</cell></row><row><cell>06</cell><cell>road or pathway</cell><cell>20.0</cell><cell>19.4</cell></row><row><cell>07</cell><cell>vegetation</cell><cell>52.5</cell><cell>51.7</cell></row><row><cell>08</cell><cell>tree</cell><cell>29.3</cell><cell>30.8</cell></row><row><cell>09</cell><cell>mountains</cell><cell>14.3</cell><cell>13.8</cell></row><row><cell>10</cell><cell>beach</cell><cell>4.4</cell><cell>3.7</cell></row><row><cell>11</cell><cell>buildings</cell><cell>45.5</cell><cell>43.6</cell></row><row><cell>12</cell><cell>sky</cell><cell>66.9</cell><cell>69.3</cell></row><row><cell>13</cell><cell>sunny</cell><cell>12.3</cell><cell>13.1</cell></row><row><cell>14</cell><cell>partly cloudy</cell><cell>22.7</cell><cell>22.2</cell></row><row><cell>15</cell><cell>overcast</cell><cell>19.6</cell><cell>21.4</cell></row><row><cell>16</cell><cell>animal</cell><cell>5.6</cell><cell>5.8</cell></row><row><cell cols="2">3 Results from the Evaluation</cell><cell></cell><cell></cell></row></table><note coords="4,90.00,396.67,423.00,8.74;4,90.00,408.62,107.96,8.74;4,90.00,426.58,423.00,8.77;4,114.91,438.57,398.10,8.74;4,114.91,450.52,249.48,8.74;4,90.00,469.45,423.00,8.77;4,114.91,481.43,398.09,8.74;4,114.91,493.39,345.98,8.74;4,90.00,512.32,423.00,8.77;4,114.91,524.30,320.63,8.74"><p><p><p>In total 11 groups participated and submitted 53 runs. Below, we briefly describe the methods employed by each group: CEA-LIST. The Lab of applied research on software-intensive technologies of the CEA, France submitted 3 runs using different image features accounting for color and spatial layout with nearest neighbour and support vector machine classifiers.</p>HJ FA. The Microsoft Key Laboratory of Multimedia Computing and Communication of the University of Science and Technology, China submitted one run using color and SIFT descriptors which are combined and classified using a nearest neighbour classifier.</p>IPAL I2R. The IPAL French-Singaporean Joint Lab of the Institute for Infocomm Research in Singapore submitted 8 runs using a variety of different image descriptors.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,150.80,124.00,301.39,617.36"><head>Table 2 :</head><label>2</label><figDesc>Average EER and Average AUC of all participating groups.</figDesc><table coords="6,153.78,124.00,295.44,593.74"><row><cell>group</cell><cell>run</cell><cell cols="2">EER [%] AUC [%]</cell></row><row><cell>CEA_LIST</cell><cell>CEA_LIST_2</cell><cell>29.71</cell><cell>71.44</cell></row><row><cell>CEA_LIST</cell><cell>CEA_LIST_3</cell><cell>41.43</cell><cell>34.25</cell></row><row><cell>CEA_LIST</cell><cell>CEA_LIST_4</cell><cell>29.04</cell><cell>73.40</cell></row><row><cell>HJ_FA</cell><cell>HJ_Result</cell><cell>45.07</cell><cell>19.96</cell></row><row><cell>IPAL_I2R</cell><cell>I2R_IPAL_Cor_Run1</cell><cell>40.02</cell><cell>62.62</cell></row><row><cell>IPAL_I2R</cell><cell>I2R_IPAL_Edge_Run2</cell><cell>45.71</cell><cell>55.79</cell></row><row><cell>IPAL_I2R</cell><cell>I2R_IPAL_HIST_Run4</cell><cell>31.83</cell><cell>73.80</cell></row><row><cell>IPAL_I2R</cell><cell>I2R_IPAL_Linear_Run5</cell><cell>36.09</cell><cell>68.65</cell></row><row><cell>IPAL_I2R</cell><cell>I2R_IPAL_Texture_Run</cell><cell>39.22</cell><cell>62.93</cell></row><row><cell>IPAL_I2R</cell><cell>I2R_IPAL_model_Run6</cell><cell>33.93</cell><cell>72.01</cell></row><row><cell>IPAL_I2R</cell><cell>IPAL_I2R_FuseMCE_R7</cell><cell>31.17</cell><cell>74.05</cell></row><row><cell>IPAL_I2R</cell><cell>IPAL_I2R_FuseNMCE_R8</cell><cell>29.71</cell><cell>76.44</cell></row><row><cell>LSIS</cell><cell>GLOT-methode23_LSIS_evaOK</cell><cell>26.56</cell><cell>79.92</cell></row><row><cell>LSIS</cell><cell>new_kda_results.txt</cell><cell>25.88</cell><cell>80.51</cell></row><row><cell>LSIS</cell><cell>FusionA_LSIS.txt</cell><cell>49.29</cell><cell>50.84</cell></row><row><cell>LSIS</cell><cell>FusionH_LSIS.txt</cell><cell>49.38</cell><cell>50.20</cell></row><row><cell>LSIS</cell><cell>MLP1_LSIS_GLOT</cell><cell>25.95</cell><cell>80.67</cell></row><row><cell>LSIS</cell><cell>MLP1_vcdt_LSIS</cell><cell>25.95</cell><cell>80.67</cell></row><row><cell>LSIS</cell><cell>method2_LSIS</cell><cell>26.61</cell><cell>79.75</cell></row><row><cell>MMIS</cell><cell>MMIS_Ruihu</cell><cell>41.05</cell><cell>62.50</cell></row><row><cell>MMIS</cell><cell>ainhoa</cell><cell>28.44</cell><cell>77.94</cell></row><row><cell>MMIS</cell><cell>alexei</cell><cell>28.82</cell><cell>77.65</cell></row><row><cell>MMIS</cell><cell>combinedREPLACEMENT</cell><cell>31.90</cell><cell>73.69</cell></row><row><cell>Makerere</cell><cell>MAK</cell><cell>49.25</cell><cell>30.83</cell></row><row><cell>RWTH</cell><cell>PHME</cell><cell>20.45</cell><cell>86.19</cell></row><row><cell>TIA</cell><cell>INAOE-kr_00_HJ_TIA</cell><cell>42.93</cell><cell>28.90</cell></row><row><cell>TIA</cell><cell>INAOE-kr_04_HJ_TIA</cell><cell>47.12</cell><cell>17.58</cell></row><row><cell>TIA</cell><cell>INAOE-lb_01_HJ_TIA</cell><cell>39.12</cell><cell>42.15</cell></row><row><cell>TIA</cell><cell>INAOE-psms_00_HJ_TIA</cell><cell>32.09</cell><cell>55.64</cell></row><row><cell>TIA</cell><cell>INAOE-psms_02_HJ_TIA</cell><cell>35.90</cell><cell>47.07</cell></row><row><cell>TIA</cell><cell>INAOE-rf_00_HJ_TIA</cell><cell>39.29</cell><cell>36.11</cell></row><row><cell>TIA</cell><cell>INAOE-rf_03_HJ_TIA</cell><cell>42.64</cell><cell>26.37</cell></row><row><cell>UPMC</cell><cell>LIP6-B50trees100C5N5</cell><cell>27.32</cell><cell>71.98</cell></row><row><cell>UPMC</cell><cell>LIP6-B50trees100C5N5T25</cell><cell>28.93</cell><cell>53.78</cell></row><row><cell>UPMC</cell><cell>LIP6-B50trees100COOC5T25</cell><cell>28.83</cell><cell>54.19</cell></row><row><cell>UPMC</cell><cell>LIP6-B50trees100pc</cell><cell>24.55</cell><cell>82.74</cell></row><row><cell>UPMC</cell><cell>LIP6-B50trees100pc_COOC5</cell><cell>27.37</cell><cell>71.58</cell></row><row><cell>UPMC</cell><cell>LIP6-B50trees100pc_T25</cell><cell>26.20</cell><cell>57.09</cell></row><row><cell>XRCE</cell><cell>TVPA-XRCE_KNN</cell><cell>16.65</cell><cell>90.66</cell></row><row><cell>XRCE</cell><cell>TVPA-XRCE_LIN</cell><cell>19.29</cell><cell>88.73</cell></row><row><cell>budapest</cell><cell>acad-acad-logreg1</cell><cell>37.36</cell><cell>66.39</cell></row><row><cell>budapest</cell><cell>acad-acad-logreg2</cell><cell>37.12</cell><cell>66.53</cell></row><row><cell>budapest</cell><cell>acad-acad-lowppnn</cell><cell>36.07</cell><cell>67.15</cell></row><row><cell>budapest</cell><cell>acad-acad-lowppnpnn</cell><cell>32.46</cell><cell>73.05</cell></row><row><cell>budapest</cell><cell>acad-acad-medfi</cell><cell>32.47</cell><cell>73.57</cell></row><row><cell>budapest</cell><cell>acad-acad-mednofi</cell><cell>32.10</cell><cell>74.18</cell></row><row><cell>budapest</cell><cell>acad-acad-medppnn</cell><cell>37.01</cell><cell>59.30</cell></row><row><cell>budapest</cell><cell>acad-acad-medppnpnn</cell><cell>32.47</cell><cell>73.61</cell></row><row><cell>budapest</cell><cell>acad-acad-mixed</cell><cell>38.34</cell><cell>63.80</cell></row><row><cell>budapest</cell><cell>acad-budapest-acad-glob1</cell><cell>45.72</cell><cell>52.78</cell></row><row><cell>budapest</cell><cell>acad-budapest-acad-glob2</cell><cell>31.14</cell><cell>74.90</cell></row><row><cell>budapest</cell><cell>acad-budapest-acad-lowfi</cell><cell>32.48</cell><cell>73.03</cell></row><row><cell>budapest</cell><cell>acad-budapest-acad-lownfi</cell><cell>32.44</cell><cell>73.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,127.77,319.40,347.46,228.53"><head>Table 3 :</head><label>3</label><figDesc>Overview of the results per concept.</figDesc><table coords="7,127.77,333.20,347.46,214.73"><row><cell></cell><cell></cell><cell>best</cell><cell cols="2">average</cell><cell cols="2">worst</cell></row><row><cell># concept</cell><cell cols="2">EER AUC group</cell><cell cols="4">EER AUC EER AUC</cell></row><row><cell>00 indoor</cell><cell>8.9</cell><cell>97.4 XRCE</cell><cell>28.0</cell><cell>67.6</cell><cell>46.8</cell><cell>2.0</cell></row><row><cell>01 outdoor</cell><cell>9.2</cell><cell>96.6 XRCE</cell><cell>30.6</cell><cell>70.5</cell><cell>54.6</cell><cell>13.3</cell></row><row><cell>02 person</cell><cell>17.8</cell><cell>89.7 XRCE</cell><cell>35.9</cell><cell>62.2</cell><cell>53.0</cell><cell>0.4</cell></row><row><cell>03 day</cell><cell>21.0</cell><cell>85.7 XRCE</cell><cell>35.4</cell><cell>64.9</cell><cell>52.5</cell><cell>9.7</cell></row><row><cell>04 night</cell><cell>8.7</cell><cell>97.4 XRCE/budapest</cell><cell>27.6</cell><cell>72.5</cell><cell>73.3</cell><cell>0.0</cell></row><row><cell>05 water</cell><cell>23.8</cell><cell>84.6 XRCE</cell><cell>38.1</cell><cell>57.8</cell><cell>53.0</cell><cell>3.2</cell></row><row><cell>06 road/pathway</cell><cell>28.8</cell><cell>80.0 XRCE</cell><cell>42.6</cell><cell>50.7</cell><cell>56.8</cell><cell>0.0</cell></row><row><cell>07 vegetation</cell><cell>17.6</cell><cell>89.9 XRCE</cell><cell>33.9</cell><cell>67.4</cell><cell>49.7</cell><cell>30.7</cell></row><row><cell>08 tree</cell><cell>18.9</cell><cell>88.3 XRCE</cell><cell>36.1</cell><cell>62.8</cell><cell>59.5</cell><cell>1.0</cell></row><row><cell>09 mountains</cell><cell>15.3</cell><cell>93.8 XRCE</cell><cell>33.1</cell><cell>61.2</cell><cell>55.8</cell><cell>0.0</cell></row><row><cell>10 beach</cell><cell>21.7</cell><cell>86.8 XRCE</cell><cell>35.8</cell><cell>57.6</cell><cell>51.4</cell><cell>0.0</cell></row><row><cell>11 buildings</cell><cell>17.0</cell><cell>89.7 XRCE</cell><cell>37.4</cell><cell>60.8</cell><cell>64.0</cell><cell>0.5</cell></row><row><cell>12 sky</cell><cell>10.4</cell><cell>95.7 XRCE</cell><cell>24.0</cell><cell>78.6</cell><cell>50.8</cell><cell>37.3</cell></row><row><cell>13 sunny</cell><cell>9.2</cell><cell>96.4 XRCE</cell><cell>30.3</cell><cell>66.5</cell><cell>55.4</cell><cell>0.0</cell></row><row><cell>14 partly cloudy</cell><cell>15.4</cell><cell>92.1 XRCE</cell><cell>37.5</cell><cell>58.9</cell><cell>55.5</cell><cell>0.0</cell></row><row><cell>15 overcast</cell><cell>14.1</cell><cell>93.7 XRCE</cell><cell>32.1</cell><cell>67.6</cell><cell>61.5</cell><cell>0.0</cell></row><row><cell>16 animal</cell><cell>20.7</cell><cell>85.7 XRCE</cell><cell>38.2</cell><cell>54.2</cell><cell>58.4</cell><cell>0.0</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,105.50,584.19,407.50,8.74;5,105.50,596.15,407.50,8.74;5,105.50,608.10,407.50,8.74;5,105.50,620.06,363.50,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,374.88,608.10,138.12,8.74;5,105.50,620.06,85.89,8.74">Overview of the imageclef 2007 object retrieval task</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Viitaniemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Benczúr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Daróczy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">Escalante</forename><surname>Balderas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hernández Gracidas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">Marin</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Stöttinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,212.42,620.06,176.46,8.74">Proceedings of the CLEF 2007 Workshop</title>
		<meeting>the CLEF 2007 Workshop</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="5,105.50,639.98,407.50,8.74;5,105.50,651.94,407.50,8.74;5,105.50,663.89,265.49,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,435.81,639.98,77.19,8.74;5,105.50,651.94,306.15,8.74">The IAPR TC-12 benchmark -a new evaluation resource for visual information systems</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,434.45,651.94,78.56,8.74;5,105.50,663.89,151.67,8.74">Proceedings of the International Workshop OntoImage</title>
		<meeting>the International Workshop OntoImage</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
