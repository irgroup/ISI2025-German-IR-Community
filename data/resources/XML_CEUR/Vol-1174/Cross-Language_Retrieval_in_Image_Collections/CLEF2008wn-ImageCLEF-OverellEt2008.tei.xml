<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,183.03,148.86,236.95,15.15;1,90.00,170.78,423.00,15.15">MMIS at ImageCLEF 2008: Experiments combining different evidence sources</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,108.54,204.67,61.47,8.74"><forename type="first">Simon</forename><surname>Overell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia &amp; Information Systems Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,180.56,204.67,70.46,8.74"><forename type="first">Ainhoa</forename><surname>Llorente</surname></persName>
							<email>a.llorente@open.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Knowledge Media Institute The Open University</orgName>
								<address>
									<addrLine>Milton Keynes</addrLine>
									<postCode>MK7 6AA</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">INFOTECH Unit ROBOTIKER-TECNALIA</orgName>
								<orgName type="institution">Parque Tecnológico</orgName>
								<address>
									<addrLine>Edificio 202 E-48170</addrLine>
									<settlement>Zamudio, Bizkaia</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.92,204.67,54.66,8.74"><forename type="first">Haiming</forename><surname>Liu</surname></persName>
							<email>h.liu@open.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Knowledge Media Institute The Open University</orgName>
								<address>
									<addrLine>Milton Keynes</addrLine>
									<postCode>MK7 6AA</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,333.13,204.67,31.97,8.74"><forename type="first">Rui</forename><surname>Hu</surname></persName>
							<email>r.hu@open.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Knowledge Media Institute The Open University</orgName>
								<address>
									<addrLine>Milton Keynes</addrLine>
									<postCode>MK7 6AA</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.65,204.67,46.35,8.74"><forename type="first">Adam</forename><surname>Rae</surname></persName>
							<email>a.rae@open.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Knowledge Media Institute The Open University</orgName>
								<address>
									<addrLine>Milton Keynes</addrLine>
									<postCode>MK7 6AA</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,432.56,204.67,54.66,8.74"><forename type="first">Jianhan</forename><surname>Zhu</surname></persName>
							<email>jianhanzhu@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Knowledge Media Institute The Open University</orgName>
								<address>
									<addrLine>Milton Keynes</addrLine>
									<postCode>MK7 6AA</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,228.68,218.62,50.78,8.74"><forename type="first">Dawei</forename><surname>Song</surname></persName>
							<email>d.song@open.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Knowledge Media Institute The Open University</orgName>
								<address>
									<addrLine>Milton Keynes</addrLine>
									<postCode>MK7 6AA</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.62,218.62,56.91,8.74"><forename type="first">Stefan</forename><surname>Rüger</surname></persName>
							<email>s.rueger@open.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Multimedia &amp; Information Systems Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Knowledge Media Institute The Open University</orgName>
								<address>
									<addrLine>Milton Keynes</addrLine>
									<postCode>MK7 6AA</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,183.03,148.86,236.95,15.15;1,90.00,170.78,423.00,15.15">MMIS at ImageCLEF 2008: Experiments combining different evidence sources</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7B481F3DEEB25490D9A6C639F8D5EFD0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation Content Based Image Retrieval, Geographic Retrieval, Data Fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the work of the MMIS group at ImageCLEF 2008. The results for three tasks are presented: Visual Concept Detection Task (VCDT), ImageCLEFphoto and ImageCLEFwiki. We combine image annotations, CBIR, textual relevance and a geographic filter using our generic data fusion method. We also compare methods for BRF and clustering.</p><p>Our top performing method in the VCDT enhances supervised learning by modifying probabilities based on a matrix that shows how terms appear together. Although it occurred in the top quartile of submitted runs, the enhancement did not provide a statistically significant improvement.</p><p>In the ImageCLEFphoto task we demonstrate that evidence from image retrieval can provide a contribution to retrieval; however we are yet to find a way of combining text and image evidence in a way to provide an improvement over the baseline. Due to the relative performances of difference evidences in ImageCLEFwiki and our failure to improve over a baseline we conclude that text is the dominant feature in this collection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we describe the experiments of the MMIS group at ImageCLEF'08. We participated in three tasks: Visual Concept Detection Task (VCDT), ImageCLEFphoto and ImageCLEFwiki.</p><p>All experiments were performed in a single framework of independently testable and tuneable modules. The framework is described in detail in Section 2. The experiments conducted and individual runs submitted for each task are described in Sections 3, 4 and 5. Finally we present our conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System</head><p>Our system framework is shown in Figure <ref type="figure" coords="2,283.85,248.38,3.88,8.74" target="#fig_0">1</ref>. The text elements of the corpus are indexed as a bag-of-words, analysed geographically and stored in a geographic index. Texture and colour features are extracted from images to form feature indexes, these features are further analysed to automatically annotate the images. This allows us to compare query images to our index using both semantic annotations and low-level features.</p><p>Blind Relevance Feedback (BRF) is employed across media types similarly to <ref type="bibr" coords="2,447.74,308.16,9.97,8.74" target="#b8">[9]</ref>. In training experiments we found the text results to have the highest precision of all individual media types. Because of this we use the top text results as feedback for the Image Retrieval engine to provide an additional Image BRF rank.</p><p>Our intermediate format is the standard TREC format. This allows us to evaluate and tune each module independently. The results of all the independent modules are combined in the data fusion module. The data fusion module combines both the ranks provided by the image and text query engine, and the filters provided by the geographic and annotation query engines. The difference between a rank and a filter is all elements in a filter are considered of equal relevance.</p><p>In the ImageCLEFphoto task we are evaluated on the novelty of our top results and provided with a subject which this novelty will be judged with respect to. We have clustered our top results using the geographic index, image annotations and text index.</p><p>Further details on the individual modules and tuning are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Feature Extractor</head><p>Content-Based Image Retrieval (CBIR) provides a way to browse or search images from large image collections based on visual similarity. CBIR is normally performed by computing the dissimilarity between the object images and query images based on their multidimensional representations in content feature spaces, for example, colour, texture and structure. In this section, we are going to introduce the key issues of the image search applied to our three tasks.</p><p>2.1.1 Feature Extraction.</p><p>ImageCLEFphoto Task. Colour feature is the most commonly used visual feature e.g. HSV, RGB <ref type="bibr" coords="2,114.83,601.96,14.62,8.74" target="#b13">[14]</ref>. In <ref type="bibr" coords="2,149.41,601.96,10.52,8.74" target="#b7">[8]</ref> Colour feature HSV outperforms the texture feature Gabor and structure feature Konvolution on CBIR . Thus we use HSV in our CBIR system. HSV is a cylindrical colour space. Its representation appears to be more intuitive to humans than the hardware representation of RGB. The hue coordinate H is angular and represents the colour, the saturation S represents the pureness of the colour and is the radial distance, finally the brightness V is the vertical distance.</p><p>We extract the HSV feature globally for every image in the query and test sets.</p><p>ImageCLEFwiki Task. We used the Gabor feature for the ImageCLEFwiki task. Gabor is a texture feature generated using Gabor wavelets <ref type="bibr" coords="2,296.40,699.59,9.96,8.74" target="#b6">[7]</ref>. Here we decompose each image into two scales and four directions.  <ref type="bibr" coords="4,219.63,135.93,10.52,8.74" target="#b3">[4]</ref> is the most complete colour space specified by the International Commission on Illumination (CIE). Its three coordinates represent the lightness of the colour (L * ), its position between red/magenta and green (a * ) and its position between yellow and blue (b * ).</p><p>The Tamura texture feature is computed using three main texture features called "contrast", "coarseness", and "directionality". Contrast aims to capture the dynamic range of grey levels in an image. Coarseness has a direct relationship to scale and repetition rates and it was considered by Tamura et al. <ref type="bibr" coords="4,164.27,207.66,15.50,8.74" target="#b15">[16]</ref> as the most fundamental texture feature and finally, directionality is a global property over a region.</p><p>The process for extracting each feature is as follows, each image is divided into nine equal rectangular tiles, the mean and second central moment feature per channel are calculated in each tile. The resulting feature vector is obtained after concatenating all the vectors extracted in each tile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Dissimilarity Measure.</head><p>ImageCLEFphoto Task. The χ 2 statistic is a statistical measure that compares two objects in a distributed manner and basically assumes that the feature vector elements are samples. The dissimilarity measure is given by</p><formula xml:id="formula_0" coords="4,240.61,354.62,272.40,30.32">d χ 2 (A, B) = n i=1 (a i -m i ) 2 m i ,<label>(1)</label></formula><formula xml:id="formula_1" coords="4,273.45,391.68,239.55,22.31">m i = a i + b i 2<label>(2)</label></formula><p>where A = (a 1 , a 2 , ..., a n ) and B = (b 1 , b 2 , ..., b n ) are the query vector and test object vector respectively. It measures the difference of the query vector (observed distribution) from the mean of both vectors (expected distribution) <ref type="bibr" coords="4,262.40,443.61,16.25,8.74" target="#b19">[20]</ref>. The χ 2 statistic was chosen as it was one of the consistently best performing dissimilarity measures in our former research <ref type="bibr" coords="4,415.86,455.56,9.97,8.74" target="#b7">[8]</ref>.</p><p>ImageCLEFwiki Task. We use the City Block distance for the ImageCLEFwiki task, to compute the distance between a query image and each test image. The City Block distance belongs to the Minkowski family, which is given by:</p><formula xml:id="formula_2" coords="4,240.29,524.36,272.71,30.32">d p (A, B) = ( n i=1 |a i -b i | p ) 1 p ,<label>(3)</label></formula><p>when the parameter p equals one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Search Method.</head><p>ImageCLEFphoto Task. In the ImageCLEFphoto task, every query topic includes three independent example images a, b, c. The probability of an object image x being a relevant match for a query topic is determined by the joint probability of the relevance between the images a, b, c in the query topic and the object image x. According to probability theory, the joint result is given by</p><formula xml:id="formula_3" coords="4,219.66,677.43,293.34,8.74">D(abc, x) = d(a, x) × d(b, x) × d(c, x),<label>(4)</label></formula><p>where D(abc, x) is the distance between a query topic including images a, b and c with an image x. d(a, x), d(b, x) and d(c, x) are distances between the three images a, b, c and the image x, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Blind Relevance Feedback.</head><p>We employed Blind Relevance Feedback (BRF) for CBIR in a similar fashion to <ref type="bibr" coords="5,437.39,130.41,9.97,8.74" target="#b8">[9]</ref>. Two different BRF method were applied in the ImageCLEFphoto task.</p><p>• The first BRF method takes the top seven results from the text retrieval results as new query examples, and the seven ranked result are combined by the search method introduced in Section 2.1.3.</p><p>• The second BRF method was employed in both the ImageCLEFphoto and ImageCLEFwiki tasks. The final ranked result of this BRF is the sum of the ranks of the top five examples from the text result with equal weights. The detail of this combining method are described in Section 2.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Annotator</head><p>The Image Annotator is the core part of the Visual Concept Detection Task (VCDT) which aims to create a model able to detect the presence or absence of 17 visual concepts in the images of the collection. The input is a training set of 1825 images that have already been annotated with words coming from a vocabulary of 17 visual concepts. The output is the annotations. We use as a baseline for this module the framework developed by Yavlinsky et al. <ref type="bibr" coords="5,475.87,336.09,15.49,8.74" target="#b18">[19]</ref> who used global features together with a non-parametric density estimation.</p><p>The process can be described as follows. First, images are segmented into nine equal tiles, and then, low-level features are extracted. The features used to model the visual concept densities are a combination of colour CIELAB and texture Tamura, as explained in Section 2.1.</p><p>The next step is to extract the same feature information from an unseen picture in order to compare it with all the previously created models (one for each concept). The result of this comparison yields a probability value of each concept being present in each image.</p><p>Then we modify some of these probabilities using additional knowledge from the image context in order to improve the accuracy of the final annotations.</p><p>The context of the images is computed using a co-occurrence matrix where each cell represents the number of times two visual concepts appear together annotating an image of the training set.</p><p>The underlying idea of this algorithm is to detect incoherence between words with the help of this correlation matrix. Once incoherence between words has been detected, the probability of the word associated to the lowest probability will be lowered, as well as all the words which are semantically similar.</p><p>Among the many uses of the concept "semantic similarity," we refer to the definition by Miller and Charles <ref type="bibr" coords="5,148.37,539.33,15.50,8.74" target="#b9">[10]</ref> who consider it as the degree of contextual interchangeability or the degree to which one word can be replaced by another in a certain context. Consequently, two words are similar if they refer to entities that are likely to co-occur together like "mountains" and "vegetation", "beach" and "water", "buildings" and "road", etc. As shown in Figure <ref type="figure" coords="5,485.67,575.19,3.88,8.74" target="#fig_1">2</ref>, our vocabulary of 17 visual concepts adopt a hierarchical structure. In the first level we find two general concepts like "indoor" and "outdoor" which are mutually exclusive while in lower levels of the hierarchy we find more specific concepts that are subclasses of the previous ones. Some concepts can belong to more than one class, for instance, a "person" can be part of an "indoor" or "outdoor" scene but others are mutually exclusive, a scene can not represent "day" and "night" at the same time.</p><p>Thus, by modifying the probability values of some concepts, annotations are produced by selecting the concepts with the highest probability. The output of this module, the annotations, will be the input for the Annotation Query Engine and the Document Cluster Generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Annotation Query Engine</head><p>The input for this module is textual queries which are the concepts of our vocabulary and the annotations produced by the Image Annotator module. Given a query term, we represent the top n images annotated by it following the standard TREC format. This constitutes one of the inputs for the Data Fusion module. Retrieval performance is evaluated with the mean-average precision (MAP) on the whole vocabulary of terms, which is the average precision, over all queries, at the ranks where recall changes (where relevant items occur).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Text Indexer</head><p>Our text retrieval system is based on Apache Lucene <ref type="bibr" coords="6,334.84,409.26,14.61,8.74" target="#b14">[15]</ref>. Text fields are pre-processed by a customised analyser similar to Lucene's default analyser: text is split at white space into tokens, the tokens are then converted to lower case, stop words discarded and stemmed with the "Snowball Stemmer". The processed tokens are held in Lucene's inverted index.</p><p>We use only the English meta-data and queries (monolingual retrieval). In ImageCLEFphoto both the title and location fields are searched as text. We do not use the notes field as in previous training experiments we have found this gives worse results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Geographic Indexer</head><p>We process the text fields with Sheffield University's General Architecture for Text Engineering (GATE) <ref type="bibr" coords="6,147.93,539.22,9.97,8.74" target="#b0">[1]</ref>. The bundled Information Extraction Engine, ANNIE, performs named entity recognition, extracting named entities and tagging them as locations. Our disambiguation system matches these placenames to unique locations in the Getty Thesaurus of Geographical Names (TGN) <ref type="bibr" coords="6,123.55,575.09,9.97,8.74" target="#b4">[5]</ref>.</p><p>In ImageCLEFwiki we compare two different disambiguation methods, both based on a geographic co-occurrence model mined from Wikipedia <ref type="bibr" coords="6,316.83,599.00,14.61,8.74" target="#b12">[13]</ref>. The first method (MR), builds a default gazetteer based on statistics from Wikipedia on which locations are most referred to by each placename. It is not context aware and disambiguates every placename with the same name to the same location. For example every reference to Cambridge will be matched to Cambridge, Massachusettes regardless of context.</p><p>The second method (Neighbourhoods), builds neighbourhoods of trigger words from Wikipedia. Depending which trigger words occur in the vicinity of an ambiguous placename dictates which location it will be disambiguated as. For example if Oxford occurs in the context of Cambridge, Cambridge will be disambiguated as Cambridge, Cambridgeshire as Oxford is a trigger word of Cambridge, Cambridgeshire.</p><p>As the ImageCLEFphoto corpus contains minimal referrences to ambiguous placenames we only use the MR method.</p><p>To query our geographic index we extract locations from the query and based on the topological data contained in the TGN return a filter of all the documents mentioning either the query locations or locations within the query locations. For example a query location of the "United States" will return all the documents mentioning the United States (or synonyms such as "USA", "the States" etc.) and all documents mentioning states, counties, cities and towns within the United States.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Document Cluster</head><p>We only employ clustering in ImageCLEFphoto. We propose a simple method of re-ordering the top of our rank based on document annotations. We consider three sources of annotations: Automated annotations assigned to images, words matched to WordNet and locations extracted from text (described in Section 2.4). WordNet is a freely available semantic lexicon of 155,287 words mapping to 117,659 semantic senses <ref type="bibr" coords="7,282.27,253.94,14.61,8.74" target="#b16">[17]</ref>. In our experiments we compare two sources of annotations: automated image annotations (Image clustering) and words matched to WordNet (WordNet clustering).</p><p>In Image clustering all the images have been annotated with concepts coming from the Corel ontology. This ontology was created using SUMO (Suggested Upper Merged Ontology) <ref type="bibr" coords="7,477.80,301.76,15.50,8.74" target="#b11">[12]</ref> and enriching it with a taxonomy of animals created by the University of Michigan <ref type="bibr" coords="7,427.53,313.71,14.61,8.74" target="#b10">[11]</ref>. After that, the ontology was populated with the vocabulary of 374 terms used for annotating the Corel dataset. Among many categories, we can find animal, vehicle and weather.</p><p>For example, if the cluster topic is "animal" we will split the ranked list of results into sub ranked lists, one corresponding to every type of animal and an additional uncategorised rank. These ranks will then be merged to form a new rank, where all the documents at rank 1 in a sub list appear first, followed by the documents at rank 2, followed by rank 3 etc. The documents of equal rank in the sublists are ranked amongst themselves based on their rank in the original list. This way the document at rank 1 in the original list remains at rank 1 in the re-ranked list. We only maximise the diversity of the top 20 documents, after the 20th document the other documents maintain their ordering in the original list.</p><p>Similarly in WordNet clustering we build clusters of sub-categories of animal, bird, sport, vehicle and weather. We match these to the bag-of-words for each document contained in the text index. For example if the cluster topic is sport we will have a sub-ranked list containing every document that mentions "tennis". If for image clustering the cluster topic is not animal , vehicle or weather, or for WordNet clustering the topic is not animal, bird, sport, vehicle or weather, we default to location clustering. In this case we have a sublist for every different location a document is annotated with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Data Fusion</head><p>With multiple sources of evidence being provided by the different query engines, a method of combining these data was required. Each query engine produced an output rank of data set images ordered with respect to their relevance to the input query. Not all engines gave values for the entire data set-some gave ranks of relevant sub-sets of the main data set. We adopted a group consensus function based on the Borda Count method <ref type="bibr" coords="7,446.64,611.05,10.51,8.74" target="#b5">[6,</ref><ref type="bibr" coords="7,461.34,611.05,7.75,8.74" target="#b1">2]</ref> as it was simple to implement and fair to the input data, but it was extended by introducing per-rank scaling parameters.</p><p>Data were split into two categories which were treated differently; ranks and filters. Ranks were processed without adjustment and directly compared to each other during the combination process. Filters were used to filter out non-relevant results from an intermediate combined rank stage. This was used, for example, with the output of the geographic query engine, where it was judged more appropriate to 'filter in' explicitly determined relevant results than to treat it like a rank.</p><p>The final output of the data fusion algorithm was the combined result of the multiple input ranks and filters. Table <ref type="table" coords="8,117.29,240.28,3.88,8.74">1</ref>: The weights and penalisation values for the various ranks used to produce the final runs of the system. The upper part of the table shows the parameters for the ImageCLEFwiki Task, whereas the lower shows the parameters for the ImageCLEFphoto Task.</p><p>The rank weights described in Stage 2 below were crucial for an effective combination process. We used a convex parameter vector W where the individual weights w i were restrained by:</p><formula xml:id="formula_4" coords="8,279.86,332.23,228.90,30.32">n i=1 w i = 1 (<label>5</label></formula><formula xml:id="formula_5" coords="8,508.76,342.64,4.24,8.74">)</formula><p>where n is the number of pure ranks to combine. This function then defined our parameter space within which we had to search to find the more appropriate weights for the rank combination process. We performed a brute force search of the entire parameter space to find the optimum weights. We used past years' data for training and optimum sets of weights were considered those that gave the maximum mean average precision (MAP). The final weights are given in Table <ref type="table" coords="8,287.23,430.47,4.98,8.74">1</ref> for the two tasks that used rank combination; the ImageCLEFwiki and the ImageCLEFphoto tasks.</p><p>The parameter values used by the filter stage were also important. Unlike the pure rank weights that were used to multiply an entire rank, the filters were used by penalising those values that were not present in the filter. These penalisation weights p i , making up weight vector P , were subject to the same constraints and were derived in a similar way to the pure rank weights by exhaustively searching all possible values up to a limit which was defined as when the variation in the parameter yielded no change in the final rank's MAP value greater than 0.0001. The overall process is described here as a five stage algorithm:</p><p>1. Read in rank data Data was provided by the query engines in ordered ranked lists with those elements that were judged to be most relevant by the engine appearing first, with each rank denoted R i . Each of the m elements was assigned a value based on its position in the list, so the first element was given '1', and increased consecutively down the list until the last element had a value of m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Multiply by rank weights</head><p>Each query engine output list was scaled by multiplying every rank value in the list by its parameter value w i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sum ranks</head><p>The scaled ranks were then combined by producing a new rank R where every element r in each R i was replaced by value r . Stages 2 and 3 can be summarised thus:</p><formula xml:id="formula_6" coords="8,286.40,707.18,222.36,30.32">r = m i=1 w i r i (<label>6</label></formula><formula xml:id="formula_7" coords="8,508.76,717.60,4.24,8.74">)</formula><p>where m is the number of ranks to combine and w i is an element of the weight vector W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Filter rank</head><p>The newly combined intermediary rank R was then subjected to the output of each filter engine to produce rank R . For each element in the rank R , any element that was not found in the filter data had its rank value penalised by that rank's parameter value as described by filter function f (r). This pushed less relevant elements further down the ordered rank.</p><p>f (r i ) = r i r i present in filter r i p i r i not present in filter ( <ref type="formula" coords="9,504.51,187.20,4.24,8.74">7</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Sort rank</head><p>The filtered rank R was then sorted to ensure that the rank values were in ascending order. The sorted rank is then the output of the combination stage of the overall system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VCDT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiments</head><p>The objective of the Visual Concept Detection Task (VCDT) is to detect the presence or absence of 17 visual concepts in the 1000 images that constitute the test set. In addition to that, some confidence scores are provided once an object is detected. The higher the value the greater the confidence of the presence of the object in the image.</p><p>For the VCDT task, we submitted four different algorithms, all of them correspond to automatic runs dealing with visual information. The second run uses statistical information about visual concepts co-occurring together in addition to the visual information, and the final run is a combination of the other three.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Automated Image Annotation Algorithm</head><p>This algorithm corresponds to the work carried out by Yavlinsky et al. <ref type="bibr" coords="9,414.93,443.10,14.62,8.74" target="#b18">[19]</ref>. Their method is based on a supervised learning model that uses a Bayesian approach together with image analysis techniques. The algorithm exploits simple global features together with robust non-parametric density estimation using the technique of kernel smoothing in order to estimate the probability of the words belonging to a vocabulary being present in each one of the pictures of the test set. This algorithm was previously tested with the Corel dataset and the Getty collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Enhanced Automated Image Annotation Algorithm</head><p>This second algorithm is described in detail in Section 2.2. The submitted run is based on an enhanced version of the algorithm described in the previous section. The input is the annotations achieved by the algorithm developed by Yavlinsky et al. together with a matrix that represents the probabilities of all the words of the vocabulary being present in the images. This algorithm was also tested on the Corel collection of 5,000 pictures and a vocabulary of 374 words obtaining statistical significant results (5%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Dissimilarity Measures Algorithm</head><p>The third algorithm follows a simple approach based on dissimilarity measures. Given one test image, we compute its global distance (Cityblock distance) to the mean of the training images which share one common keyword. The smaller the distance value the higher the probability for a test image to be annotated by the keyword of that category. Our submitted result is based on the combination of two results from two single feature spaces. One is the CIELAB colour feature. For each pixel, we compute the CIELAB colour values. Within each tile the mean and the second moment are computed for each channel. The other is the Tamura texture feature. The combination here is a simple additive combination of the probability for each test image for each category. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Combined Algorithm</head><p>This submitted run is based on the combination of all the other runs submitted by this team, in addition to one extra combined result formed from the output of a feature extraction algorithm for the Tamura and CIELAB features. This was a simple additive combination which, through testing was shown to be useful when used in combination with the other algorithm outputs. By testing the output of each individual algorithm on subsets of the training data they were produced with, a rough indication of performance of the algorithm per concept was derived. These figures then allowed an automated system to pick for each concept the best performing algorithm's output and combine it into a new result set. The individual algorithms' outputs were not adjusted or scaled in any way (other than the pre-combined result set mentioned above).</p><p>In our testing routines based on splitting the available training data into training and evaluation sets, the combination performed marginally better than the individual component algorithm outputs. This was due to selecting those algorithms which performed better at certain concepts and classes of concepts. Further work will be carried out to more robustly take advantage of concept classification when combining algorithm results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluations and Results</head><p>The evaluation metric followed by the ImageCLEF organisation is based on ROC curves. Initially, a receiver operating characteristic (ROC) curve was used in signal detection theory to plot the sensitivity versus (1 -specificity) for a binary classifier as its discrimination threshold is varied. Later on, ROC curves <ref type="bibr" coords="10,190.11,479.04,10.51,8.74" target="#b2">[3]</ref> were applied to information retrieval in order to represent the fraction of true positives (TP) against the fraction of false positives (FP) in a binary classifier. The Equal Error Rate (EER) is the error rate at the threshold where FP=FN. The area under the ROC curve, AUC, is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. The results obtained by the four algorithms developed by our group are represented in Table <ref type="table" coords="10,301.75,538.82,3.88,8.74" target="#tab_0">2</ref>. Our best result corresponds to the "Enhanced Automated Image Annotation" algorithm as seen in Figure <ref type="figure" coords="10,352.10,550.77,3.88,8.74" target="#fig_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis</head><p>Our best algorithm was previously tested with the Corel dataset, a collection of 5,000 images and 374 terms obtaining a statistically significant (5%) improvement over the baseline approach followed by Yavlinksy et al. <ref type="bibr" coords="10,216.90,620.96,14.62,8.74" target="#b18">[19]</ref>. However, with the IAPR collection and the vocabulary of 17 terms used in VCDT, the results although better were not statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ImageCLEFphoto</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments</head><p>In this section, we describe the nine submitted runs. Img. This run is a pure CBIR. The dissimilarity value between an image example and an object image is computed by χ 2 Statistics measure based on their HSV colour feature space. Probability theory is adapted for combining the three dissimilarity values of the image examples in each topic with an object image. The final result is the Mean Average Precision (MAP) of 39 query topics.</p><p>ImgBrfWeightingIterative. This run is a combination of image evidence and text evidence. BRF is employed in this run. The top seven examples from ranked text results of iteration one were taken as new image examples for each query topic. The same search method as with "Img" is used in this run. The final result is the combination of the result ranks of this run and the ranks of "Img" using weight 0.6 and 0.4.</p><p>ImgBrfWeighting. The differences between this run and "ImgBrfWeightingIterative" is that the BRF rank for this run was produced by summing the ranks of the first five results of the text engine's results when the original query was used as input.</p><p>ImgTxt / TxtGeo / ImgTxtGeo. These three runs are the combination of image evidence and text evidence, text evidence and geographic evidence, image evidence and text evidence and geographic evidence, respectively. These evidences were merged by the data fusion process as described in Section 2.6. The ranks' and filters' weights are derived through exhaustive search of their parameter spaces. The final combination is made with optimal weights (see Table <ref type="table" coords="11,473.92,678.83,3.88,8.74">1</ref>).</p><p>ImgTxtGeo-ImageCluster. This run combines the text, image and geographic data as with the ImgTxtGeo run and then clusters the result using the Image Clustering method described in Section 2.5. ImgTxtGeo-MetaCluster. This run combines the text, image and geographic data as with the ImgTxtGeo run and then clusters the result using the WordNet Clustering method described in Section 2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Table <ref type="table" coords="12,117.92,351.86,4.98,8.74" target="#tab_1">3</ref> shows the result of nine runs submitted for the ImageCLEFphoto task, from which the following observations can be made. Firstly, with respect to MAP, the BRF method which multiplied seven ranks outperforms the BRF method summing five ranks. Secondly, with respect to the Geometric Mean Average Precision (GMAP), combining text and image data outperforms plain text data. As GMAP emphasises performance in the worst case, this shows that image retrieval definitely has a contribution to the combined result; the reason for the lower MAP is simply because we have not determined the best way to combine the two evidences yet.</p><p>Unfortunately there was no significant difference between either of our clustering methods and the inclusion of geographic information actually gave us worse results than our text baseline so we can draw no further conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ImageCLEFwiki</head><p>The ImageCLEFwiki task aims to investigate retrieval approaches in the context of a larger scale and heterogeneous collection of images. The dataset of this task is a wikipedia image collection, which contains 151,518 images created and employed by the INEX Multimedia track in 2006-2007 <ref type="bibr" coords="12,90.00,550.09,14.62,8.74" target="#b17">[18]</ref>. 75 topics are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments</head><p>In this section, we describe the six submitted runs.</p><p>SimpleText. This run is our baseline -pure text based search. The detailed description can be found in Section 2.3.</p><p>TextGeoNoContext. This run is a combination of text evidence and geographic evidence. It uses the MR method or placename disambiguation described in Section 2.4, which is not context aware. The geographic filter and text rank were combined by multiplying the rank of each document in the text rank not appearing in the geographic filter by a penalisation value of 3.0 (detailed in Section 2.6). Image. This run is a pure content based image search. Gabor texture features were extracted from the test collection and used to form a high dimensional space. The query images were compared to the corpus images using the Cityblock distance.</p><p>ImageText. This run is a combination of text evidence and image evidence. The two were combined using a convex combination of ranks (Section 2.6) using weights 0.3 and 0.7.</p><p>ImageTextBRF. This run is a combination of text evidence and image evidence. The same text and image retrieval system are used as in the previous section. We use the top five results from the text retrieval results as query images for our BRF system. The three were combined using a convex combination of ranks using weights 0.25, 0.1 and 0.65 for Image, BRF and Text relevance respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Table <ref type="table" coords="13,117.40,477.37,4.98,8.74" target="#tab_2">4</ref> shows the results of our six runs.</p><p>From the results we can see that our baseline (the "SimpleText" run) outperformed all the other methods based on the majority of performance measures. While the pure content based image search run gives us the worst performance. There were negligable differences whether we used context based placename disambiguation or not. Similarly only negligable differences were seen whether blind relevance feedback was employed or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Our conclusions from ImageCLEF'08 are limited as none of our experiments achieved a statistically significant improvement over a baseline. Discussions of the results are provided below.</p><p>VCDT. The enhanced automated image annotation method performed well appearing in the top quartile of all methods submitted, however it failed to provide significant improvement over the automated image annotation method. An explanation for this can be found in the small number of terms of the vocabulary that hinders the functioning of the algorithm and another in the nature of the vocabulary itself, where instead of incoherence we have mutually exclusive terms and almost no semantically similar terms.</p><p>ImageCLEFphoto. As mentioned in Section 4.2, despite the combination of Text and Image evidence performing worse than the text baseline with respect to MAP, the superior GMAP shows that Image evidence is improving performance in the worst case. In future work we would like to explore further data fusion methods and find a way to take full advantage of this additional evidence without undermining text retrieval where it is performing well.</p><p>ImageCLEFwiki. Our text baseline outperformed all other retrieval methods. From this we conclude that text is by far the dominant feature on retrieval for this heterogeneous collection. The fact that blind relevance feedback made negligible positive or negative difference to image retrieval combined with the very low retrieval results for image retrieval alone, implies simple features can contribute little to retrieval on this collection. Geographic retrieval offered some improvement in some measures (p@10 and R-prec.), but not statistically significant. We can only conclude that a geographically aware system could provide some improvement, but due to the short length of the documents, context based placename disambiguation will be unlikely to provide a significant improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,218.28,686.01,166.43,8.74;3,90.00,167.55,436.05,493.38"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our Application Framework</figDesc><graphic coords="3,90.00,167.55,436.05,493.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,209.87,295.29,183.25,8.74;6,146.62,108.86,309.75,161.35"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hierarchy of the visual concepts</figDesc><graphic coords="6,146.62,108.86,309.75,161.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,178.36,364.10,246.27,8.74;11,126.95,108.86,349.10,230.16"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ROC curves for our best annotation algorithm</figDesc><graphic coords="11,126.95,108.86,349.10,230.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,166.05,110.43,270.90,79.62"><head>Table 2 :</head><label>2</label><figDesc>Comparative results of MMIS group</figDesc><table coords="10,349.72,110.43,71.45,8.74"><row><cell>EER</cell><cell>AUC</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,168.51,110.43,265.98,138.20"><head>Table 3 :</head><label>3</label><figDesc>Results of submitted runs of ImageCLEFphoto task</figDesc><table coords="12,184.43,110.43,234.14,116.73"><row><cell>ID</cell><cell>MAP</cell><cell cols="2">GMAP CR20</cell></row><row><cell>Txt</cell><cell cols="2">0.0923 0.008</cell><cell>0.1926</cell></row><row><cell>Img</cell><cell cols="2">0.0256 0.0071</cell><cell>0.0449</cell></row><row><cell cols="3">ImgBrfWeightingIterative 0.0326 0.0089</cell><cell>0.2286</cell></row><row><cell>ImgBrfWeighting</cell><cell cols="2">0.0217 0.0058</cell><cell>0.1894</cell></row><row><cell>ImgTxt</cell><cell cols="2">0.0715 0.0241</cell><cell>0.2231</cell></row><row><cell>TxtGeo</cell><cell cols="2">0.0696 0.0025</cell><cell>0.2061</cell></row><row><cell>ImgTxtGeo</cell><cell>0.078</cell><cell>0.0291</cell><cell>0.2516</cell></row><row><cell cols="3">ImgTxtGeo-ImageCluster 0.0465 0.0221</cell><cell>0.2388</cell></row><row><cell>ImgTxtGeo-MetaCluster</cell><cell cols="2">0.0461 0.0258</cell><cell>0.2503</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="13,90.00,110.43,423.00,168.03"><head>Table 4 :</head><label>4</label><figDesc>Results of runs of ImageCLEFwiki taskTextGeoContext. This run is a combination of text evidence and geographic evidence. It uses the context aware Neighbourhood method or placename disambiguation described in Section 2.4. Text and geographic evidence are combined in the same way as the TextGeoNoContext run.</figDesc><table coords="13,158.17,110.43,286.67,80.87"><row><cell>Run ID</cell><cell>MAP</cell><cell>P@5</cell><cell cols="2">P@10 R-prec.</cell><cell>Bpref</cell></row><row><cell>SimpleText</cell><cell cols="3">0.1918 0.3707 0.3240</cell><cell cols="2">0.2362 0.2086</cell></row><row><cell cols="4">TextGeoNoContext 0.1896 0.3760 0.3280</cell><cell cols="2">0.2358 0.2052</cell></row><row><cell>TextGeoContext</cell><cell cols="3">0.1896 0.3760 0.3280</cell><cell cols="2">0.2357 0.2052</cell></row><row><cell>Image</cell><cell cols="3">0.0037 0.0187 0.0147</cell><cell cols="2">0.0108 0.0086</cell></row><row><cell>ImageText</cell><cell cols="3">0.1225 0.2293 0.2213</cell><cell cols="2">0.1718 0.1371</cell></row><row><cell>ImageTextBRF</cell><cell cols="3">0.1225 0.2293 0.2213</cell><cell cols="2">0.1718 0.1371</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="14,110.48,299.97,402.51,8.74;14,110.48,311.93,363.06,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="14,422.33,299.97,90.66,8.74;14,110.48,311.93,150.52,8.74">Developing language processing components with GATE</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tablan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ursu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bontcheva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>University of Sheffield</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="14,110.48,331.50,402.52,8.74;14,110.48,343.45,402.52,8.74;14,110.48,355.41,205.14,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,251.00,331.50,262.01,8.74;14,110.48,343.45,85.92,8.74">Variants of the Borda Count method for combining ranked classifier hypotheses</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Erp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Schomaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,409.06,343.45,103.93,8.74;14,110.48,355.41,174.74,8.74">International Workshop on Frontiers in Handwriting Recognition</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Ding</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.48,374.97,402.51,8.74;14,110.48,386.93,22.69,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,173.93,374.97,142.77,8.74">An introduction to ROC analysis</title>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,324.85,374.97,117.85,8.74">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.48,406.50,402.52,8.74;14,110.48,418.45,125.37,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,218.86,406.50,208.81,8.74">Mathematical morphology in the CIELAB space</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,436.02,406.50,76.98,8.74;14,110.48,418.45,41.52,8.74">Image Analysis &amp; Stereology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="201" to="206" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.48,438.02,402.51,8.74;14,110.48,449.98,58.67,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,165.28,438.02,176.73,8.74">User&apos;s Guide to the TGN Data Releases</title>
		<author>
			<persName coords=""><surname>Harping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,352.46,438.02,138.68,8.74">The Getty Vocabulary Program</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>0 edition</note>
</biblStruct>

<biblStruct coords="14,110.48,469.54,402.52,8.74;14,110.48,481.50,270.92,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,283.23,469.54,225.21,8.74">Decision combination in multiple classifier systems</title>
		<author>
			<persName coords=""><forename type="first">T K</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J J</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S N</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,110.48,481.50,184.44,8.74">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.48,501.07,402.52,8.74;14,110.48,513.02,191.35,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,221.53,501.07,203.50,8.74">Robust texture features for still-image retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Howarth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,432.90,501.07,80.10,8.74;14,110.48,513.02,73.54,8.74">Vision, Image and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">152</biblScope>
			<biblScope unit="page" from="868" to="874" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.48,532.59,402.51,8.74;14,110.48,544.55,374.46,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,311.04,532.59,201.95,8.74;14,110.48,544.55,91.18,8.74">Comparing dissimilarity measures for contentbased image retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rüger</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Uren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,222.38,544.55,173.21,8.74">Asian Information Retrieval Symposium</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="44" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.48,564.12,402.52,8.74;14,110.48,576.07,402.52,8.74;14,110.48,588.03,22.69,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,319.55,564.12,193.45,8.74;14,110.48,576.07,202.58,8.74">IPAL inter-media pseudo-relevance feedback approach to imageCLEF 2006 photo retrieval</title>
		<author>
			<persName coords=""><surname>Maillot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chevallet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J H</forename><surname>Valea</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,338.44,576.07,97.39,8.74">CLEF 2006 Workshop</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Working notes</note>
</biblStruct>

<biblStruct coords="14,110.48,607.59,402.52,8.74;14,110.48,619.55,210.62,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,259.44,607.59,194.35,8.74">Contextual correlates of semantic similarity</title>
		<author>
			<persName coords=""><forename type="first">G A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W G</forename><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,466.75,607.59,46.26,8.74;14,110.48,619.55,146.52,8.74">Journal of Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.48,639.12,402.52,8.74;14,110.48,651.07,217.98,8.74" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C S</forename><surname>Espinosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T A</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dewey</surname></persName>
		</author>
		<ptr target="http://animaldiversity.org" />
		<title level="m" coord="14,462.06,639.12,50.94,8.74;14,110.48,651.07,57.37,8.74">The animal diversity web</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.48,670.64,402.51,8.74;14,110.48,682.60,402.52,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,208.93,670.64,155.76,8.74">Towards a standard upper ontology</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Niles</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,388.88,670.64,124.11,8.74;14,110.48,682.60,174.94,8.74">International Conference on Formal Ontology in Information Systems</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="2" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.48,702.17,402.52,8.74;14,110.48,714.12,174.26,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,215.20,702.17,187.05,8.74">Geographic co-occurrence as a tool for GIR</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Overell</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,424.46,702.17,88.54,8.74;14,110.48,714.12,144.10,8.74">CIKM Workshop on Geographic Information Retrieval</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.48,733.69,402.52,8.74;14,110.48,745.64,285.96,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,242.74,733.69,265.97,8.74">Evaluation of key frame based retrieval techniques for video</title>
		<author>
			<persName coords=""><forename type="first">M J</forename><surname>Pickering</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,110.48,745.64,188.81,8.74">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="235" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.48,112.02,402.52,8.74" xml:id="b14">
	<monogr>
		<ptr target="http://lucene.apache.org/java/docs/" />
		<title level="m" coord="15,110.48,112.02,98.49,8.74">Apache Lucene Project</title>
		<imprint>
			<date type="published" when="2007-08-01">1 August 2007, 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.48,131.95,402.52,8.74;15,110.48,143.90,224.04,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="15,281.29,131.95,227.30,8.74">Textural features corresponding to visual perception</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yamawaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,110.48,143.90,132.18,8.74">Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="460" to="473" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.48,163.83,42.23,8.74;15,178.05,163.83,46.59,8.74;15,294.67,163.83,42.37,8.74;15,367.89,163.83,26.01,8.74;15,419.24,163.83,27.40,8.74;15,471.99,163.83,41.02,8.74;15,110.48,175.78,176.37,8.74" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="15,294.67,163.83,42.37,8.74;15,367.89,163.83,26.01,8.74;15,419.24,163.83,27.40,8.74;15,471.99,163.83,36.46,8.74">WordNet, online lexical database</title>
		<ptr target="http://www.cogsci.princeton.edu/∼wn/" />
		<imprint/>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.48,195.71,402.52,8.74;15,110.48,207.66,402.53,8.74;15,110.48,219.62,22.69,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="15,247.13,195.71,137.29,8.74">The inex 2006 multimedia track</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,203.48,207.66,231.23,8.74">Advances in XML Information Retrieval: INEX 2006</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Trotman</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.48,239.54,402.51,8.74;15,110.48,251.50,402.52,8.74;15,110.48,263.45,183.26,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="15,289.56,239.54,223.43,8.74;15,110.48,251.50,199.64,8.74">Automated image annotation using global features and robust non-parametric density estimation</title>
		<author>
			<persName coords=""><surname>Yavlinsky</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schofield</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,332.57,251.50,180.43,8.74;15,110.48,263.45,84.96,8.74">International ACM Conference on Image and Video Retrieval</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.48,283.38,402.52,8.74;15,110.48,295.33,324.71,8.74" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="15,196.86,283.38,240.58,8.74">Evaluation of similarity measurement for image retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,456.07,283.38,56.93,8.74;15,110.48,295.33,226.21,8.74">International Conference on Neural Networks &amp; Signal Processing</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="928" to="931" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
