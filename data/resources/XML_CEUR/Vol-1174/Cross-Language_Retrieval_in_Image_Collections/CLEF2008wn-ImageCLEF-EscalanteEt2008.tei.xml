<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,95.38,148.86,412.24,15.15">TIA-INAOE&apos;s Participation at ImageCLEF 2008</title>
				<funder ref="#_SAY4JUD">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_eNZrF6K">
					<orgName type="full">CONACyT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,137.88,182.75,73.43,8.74"><forename type="first">H</forename><forename type="middle">Jair</forename><surname>Escalante</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.05,182.75,77.28,8.74"><forename type="first">Jesús</forename><forename type="middle">A</forename><surname>González</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.33,182.75,88.75,8.74"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Hernández</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.30,182.75,59.04,8.74"><forename type="first">Aurelio</forename><surname>López</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,159.34,196.70,64.89,8.74"><forename type="first">Manuel</forename><surname>Montes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.45,196.70,72.85,8.74"><forename type="first">Eduardo</forename><surname>Morales</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.24,196.70,57.27,8.74"><forename type="first">Luis</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.35,196.70,65.32,8.74"><forename type="first">Luis</forename><surname>Villaseñor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,95.38,148.86,412.24,15.15">TIA-INAOE&apos;s Participation at ImageCLEF 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8902556F954A1821C8A4DDA358A8B1E9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 [Information Systems and Applications]: Information Search and Retrieval-Retrieval models</term>
					<term>Selection process</term>
					<term>Information Filtering Performance, Experimentation Multimedia image retrieval, Visual-concept detection, Annotation-based document expansion, Late fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the INAOE's research group on machine learning for image processing and information retrieval from México. This year we proposed two approaches for the photographic retrieval task. First, we studied the annotation-based expansion of documents for image retrieval. This approach consists of automatically assigning labels to images by using supervised machine learning techniques. Labels are used for expanding the manual annotations of images. Then, we build a text-based retrieval method that uses the expanded annotations. Experimental results give evidence that the expansion could be helpful for improving retrieval performance and diversifying results. However, it is not trivial to determine the best way for combining labels with the other information available. In our second formulation we adopted a late fusion approach to combine the outputs of several heterogeneous retrieval methods. Our aim was to take advantage of the diversity, complementariness and redundancy of documents through ranked lists obtained with different methods and using distinct information. We consider content-based, text-based, annotationbased, visual-concept-based and multi-modal retrieval methods. The fusion of methods achieved competitive performance to that of the best ImageCLEF2008 entries. The heterogeneousness of the retrieval methods proved to be useful for diversifying the retrieval results. For further diversifying the results of our methods we developed a simple strategy based on topic modeling with latent Dirichlet allocation. This technique resulted very helpful for some configurations, though degraded the performance for others. This is mainly due to the quality of the initial retrieval results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the participation of the INAOE's research group on machine learning for image processing and information retrieval (TIA) in the photographic retrieval task of ImageCLEF 2008. This year we submitted a total of 16 runs comprising diverse configurations of the two formulations we adopted. In the first we used automatic image annotation (AIA) methods for expanding the manual annotations of images. Under this formulation a region-level AIA method was used for assigning labels to regions in segmented images. The labels were then combined with the manual annotations of images and the expanded annotations were indexed and queried by using a standard text-based retrieval model. Our assumption is that the labels may provide complementary yet redundant information that can be helpful for improving retrieval performance. One should note that although this method was first proposed by our team for ImageCLEF 2007 <ref type="bibr" coords="2,499.72,241.44,9.96,8.74" target="#b7">[8]</ref>, in this work we performed experiments with a larger and better training set of annotated regions. Further, we used a different method (in-development) for annotating the images. Experimental results give evidence of slight improvements by using the annotation-based expansion. Interestingly, the diversity of results is increased by using the expanded annotation. These results give evidence that the use of labels generated by AIA methods can be helpful for enhancing retrieval performance. However, it is not trivial to determine the best way for combining labels with the other available information (i. e. image-content and manual annotations).</p><p>In our second formulation we considered the late fusion of heterogeneous methods <ref type="bibr" coords="2,473.87,337.08,9.96,8.74" target="#b8">[9]</ref>. This approach consists of combining the outputs of independent retrieval methods of diverse nature and based on different sources. Opposed to previous late fusion approaches our formulation considered several retrieval methods per modality, that are different to each other. Our aim was to take advantage of the diversity, complementariness and redundancy of documents through ranked lists of documents obtained with different methods and using distinct information. We considered content-based, text-based, annotation-based, visual-concept-based and multi-modal retrieval methods. A simple weighting scheme allowed us to effectively combine information from diverse sources. Despite the performance of independent retrieval methods is not good, the late fusion approach achieved competitive performance. Further, the heterogeneousness of the retrieval methods proved to be useful for diversifying the retrieval results. We report a few experiments with per-modality and hierarchical fusion, better results were obtained with the latter strategy. Further experiments and a more detailed analysis with this approach are reported elsewhere <ref type="bibr" coords="2,459.64,480.54,9.96,8.74" target="#b8">[9]</ref>.</p><p>The focus of this year photographic retrieval task was on diversifying retrieval results. In order to make varied the results of the late fusion approach we developed a simple strategy based on topic modeling with latent Dirichlet allocation (LDA). The proposed approach consists of finding LDA-topics among the retrieved documents using LDA. LDA-topics can be considered clusters of documents with similar semantic content. Then a single document is selected as representative of each LDA-topic. Representative documents are collocated at the top positions of the ranked list of documents. This technique diversified the results for some configurations of our methods, although it degraded the performance for others. This can be due to the quality of the initial retrieval result. We are currently working in an improved version of the LDA approach for the diversification of retrieval results.</p><p>The rest of this paper is organized as follows. In the next Section we briefly introduce the photographic retrieval task. Then, in Section 3, we present the annotation-based approach to image retrieval. Next, in Section 4, we describe the heterogeneous late fusion approach. Next, in Section 5, the LDA approach for diversification of retrieval results is presented. Then, in Section, 6 we report experimental results of our runs. Finally in Section 7 we present conclusions and discuss current and future work directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ad-hoc photographic retrieval</head><p>This paper presents developments and contributions for the photographic retrieval task of Image-CLEF 2008. The goal of this task is the following: given an English statement describing an user information need, find as many relevant images as possible from the given document collection <ref type="bibr" coords="3,90.00,281.14,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="3,103.89,281.14,7.01,8.74" target="#b5">6]</ref>. Organizers provide participants with a collection of annotated images, together with some topics describing information needs. The collection of documents used for ImageCLEF2008 is the IAPR TC-12 Benchmark <ref type="bibr" coords="3,223.26,305.05,14.61,8.74" target="#b11">[12]</ref>. Each query topic consists of a fragment of text describing a single information need, together with three sample images visually similar to the desired relevant images <ref type="bibr" coords="3,123.01,328.96,9.96,8.74" target="#b5">[6]</ref>. Participants use topics content for creating queries that are used with their retrieval systems. Systems runs are then evaluated by the organizers using standard evaluation measures from information retrieval <ref type="bibr" coords="3,205.32,352.87,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="3,218.73,352.87,7.01,8.74" target="#b5">6]</ref>. The focus of this year photographic retrieval task was on diversification of retrieval results. Therefore, organizers encourage participants to submit runs in which the top-x document are both relevant and as diverse as possible. Retrieval methods with explicit mechanisms for diversifying results are supposed to obtain better results. For further information we refer the reader to the respective overview paper <ref type="bibr" coords="3,319.95,400.69,9.96,8.74" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Annotation-based document expansion</head><p>In this section we describe our AIA-based approach to image retrieval. AIA is the task of assigning semantic labels to images <ref type="bibr" coords="3,204.99,467.41,14.61,8.74" target="#b18">[19]</ref>. The main goal of AIA is to allow un-annotated image collections to be searched by keywords. Labels can be assigned either at image-level or at region-level. In the former, labels are assigned to the image as a whole, while in the latter labels are assigned to regions in segmented images. The latter approach can be more useful than the former, because AIA methods can take advantage of spatial context. In this paper we considered region-level AIA methods for expanding manual annotations of images. The annotation-based document expansion (ABDE) approach is depicted in Figure <ref type="figure" coords="3,265.84,539.14,3.87,8.74" target="#fig_0">1</ref>.</p><p>All of the images in the IAPR-TC12 collection were automatically segmented and visual features were extracted from each region. Using a training set of annotated regions and a multi-class classifier all of the regions in the segmented collection were labeled. For each image, labels were used as the expansion of the original annotation. The expanded annotation was considered as a textual document and a text-based retrieval model was used for indexing the documents. The textual statement in each topic was used as query for the retrieval model. Based on previous work we selected as retrieval engine a vector space model (VSM) with a combination of augmentednormalized term-frequency and entropy for weighting documents <ref type="bibr" coords="3,370.40,634.78,14.61,8.74" target="#b22">[23]</ref>. We used the TMG-Matlab R toolbox for the implementation of all of the text-based retrieval methods <ref type="bibr" coords="3,412.68,646.74,14.61,8.74" target="#b22">[23]</ref>. In the rest of this section we provide additional details of the training set we used and the annotation method we considered.</p><p>For our experiments with ABDE in ImageCLEF 2007 we faced several issues (due to the training set we used) that made difficult the correct application of ABDE <ref type="bibr" coords="3,419.73,694.56,9.96,8.74" target="#b7">[8]</ref>. For this work we considered a better training set composed of about 7000 manually segmented images from the IAPR-TC12 collection. This training set is being created as an effort of INAOE-TIA for providing the community with an extended IAPR-TC12 benchmark that can be useful for studying the use of AIA methods in image retrieval <ref type="bibr" coords="3,230.30,742.38,9.96,8.74" target="#b6">[7]</ref>. The images in the training set have been carefully segmented following a well defined methodology. Each image is associated with one of 276 labels that have been arranged into a hierarchical structure that facilitates the annotation process. A total of 37,047 regions have been considered for our experiments. Sample images from our training set are shown in Figure <ref type="figure" coords="4,162.97,290.88,3.87,8.74" target="#fig_1">2</ref>. The following features were extracted from each region: area, boundary/area, width and height of the region, average and standard deviation in x and y, convexity, average, standard deviation and skewness in both color spaces RGB and CIE-Lab, for a total of 27 features. The training set is therefore composed of features-label pairs.</p><p>We used a simple knn classifier as baseline AIA method. Additionally, we considered a recently developed method for improving the quality of annotations <ref type="bibr" coords="4,361.50,350.66,14.61,8.74" target="#b12">[13]</ref>. This postprocessing method (referred to as MRFS) is based on a Markov random field that uses spatial relationships between regions for maximizing the coherence of the annotation for each image. The energy function of this random field takes into account a relevance weight obtained from knn and probabilities that reflect the relationships between labels and spatial relationships. For further details we refer the reader to follow the references <ref type="bibr" coords="4,224.39,410.43,14.61,8.74" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Late fusion of heterogeneous retrieval methods</head><p>In this section we describe our late fusion approach to image retrieval. Late fusion of independent retrieval methods (LFIRM) is one of the simplest and most widely used approaches for combining visual and textual information in the retrieval process <ref type="bibr" coords="4,327.09,489.11,15.50,8.74" target="#b19">[20,</ref><ref type="bibr" coords="4,345.73,489.11,7.75,8.74" target="#b5">6,</ref><ref type="bibr" coords="4,356.61,489.11,12.73,8.74" target="#b14">15,</ref><ref type="bibr" coords="4,372.48,489.11,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="4,383.37,489.11,12.73,8.74" target="#b13">14,</ref><ref type="bibr" coords="4,399.24,489.11,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="4,415.11,489.11,12.73,8.74" target="#b15">16,</ref><ref type="bibr" coords="4,430.96,489.11,11.62,8.74" target="#b16">17]</ref>. This approach consists of building several retrieval systems (i. e. independent retrieval models, hereafter IRMs) using subsets of the same collection of documents. At querying time, each IRM returns a list of documents relevant to a given query. The output of the different IRMs is then combined for obtaining a single list of ranked documents, see Figure <ref type="figure" coords="4,330.25,536.93,3.87,8.74" target="#fig_2">3</ref>. A common problem with this approach is that usually a single IRM is considered for each modality <ref type="bibr" coords="4,347.82,548.88,15.50,8.74" target="#b16">[17,</ref><ref type="bibr" coords="4,366.13,548.88,12.73,8.74" target="#b19">20,</ref><ref type="bibr" coords="4,381.67,548.88,7.75,8.74" target="#b5">6,</ref><ref type="bibr" coords="4,392.24,548.88,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="4,402.80,548.88,12.73,8.74" target="#b13">14,</ref><ref type="bibr" coords="4,418.35,548.88,12.73,8.74" target="#b20">21,</ref><ref type="bibr" coords="4,433.90,548.88,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="4,449.44,548.88,11.62,8.74" target="#b15">16]</ref>. The latter fact limits the performance of LFIRM because, despite the potential diversity of documents due to the IRMs, there is little, if any, redundance through the IRMs and therefore the combination is not effective <ref type="bibr" coords="4,128.63,584.75,15.50,8.74" target="#b19">[20,</ref><ref type="bibr" coords="4,147.60,584.75,7.01,8.74" target="#b5">6]</ref>. Some LFIRM systems consider multiple IRMs for each modality, however, most of these IRMs are very homogeneous. That is, these methods are variations of a same retrieval model using different parameters or meta-data for indexing <ref type="bibr" coords="4,351.77,608.66,15.50,8.74" target="#b16">[17,</ref><ref type="bibr" coords="4,370.59,608.66,7.75,8.74" target="#b5">6,</ref><ref type="bibr" coords="4,381.66,608.66,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="4,392.72,608.66,11.62,8.74" target="#b13">14]</ref>.</p><p>In this work we proposed the combination of heterogeneous IRMs through the LFIRM approach for multimedia image retrieval. We call this approach HLFIRM (heterogeneous LFIRM).</p><p>Heterogeneousness is important because it can be useful for providing diverse, complementary and redundant lists of documents to the LFIRM approach, reducing the retrieval problem to that of effectively combining lists of ranked documents. For merging the lists we assigned a score to each document in the lists and ranked them in descending order of this score. The combined list was formed by keeping the top-y ranked documents. We assigned a score W to each document d j in at least one of N lists L {1,...,N } of ranked documents as described by Equation (1):  where i indexes the N available lists of documents; ψ(x, H) is the position of document x in ranked list H; 1 a is an indicator function that takes the unit value when a is true and α i ( N k=1 α k = 1) is the relevance weighting for IRM i. Each list L i is the output of one of the IRMs we considered, these are shown in Table <ref type="table" coords="5,201.39,506.55,3.87,8.74" target="#tab_0">1</ref>. In the rest of this section we describe these heterogeneous IRMs.</p><formula xml:id="formula_0" coords="4,208.49,725.11,304.51,29.15">W (dj) = N i=1 1 d j ∈L i × N i=1 αi × 1 ψ(dj, Li)<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image-based IRMs</head><p>Two image-based methods were considered for HLFIRM, these are FIRE and VCDTR-X (rows 1 and 2 in Table <ref type="table" coords="5,157.16,564.79,3.87,8.74" target="#tab_0">1</ref>). FIRE is a content-based image retrieval (CBIR) system that works under the query-by-example formulation <ref type="bibr" coords="5,225.56,576.74,14.61,8.74" target="#b10">[11]</ref>. FIRE uses the sample images from the topics for querying. Since we are only interested in the output of the IRMs we used the FIRE baseline run provided by ImageCLEF 2007 organizers <ref type="bibr" coords="5,230.80,600.65,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="5,244.70,600.65,11.62,8.74" target="#b10">11]</ref>. In ImageCLEF 2007 the FIRE run we use was ranked at position 377 out of 474.</p><p>VCDTR-X is a novel IRM that uses visual-concepts identified in images for retrieval. Visual concepts are indeed labels assigned by image-level AIA methods; the method used for generating the labels is described in <ref type="bibr" coords="5,205.38,648.47,14.61,8.74" target="#b17">[18]</ref>. We used the concepts 1 provided by the Xerox Research Center Europe group (XRCE) for building a retrieval model that indexes such concepts. All images (including topic images) were automatically annotated by using this method. The assigned annotations were then used for building a VSM with boolean weighting. Queries for VCDTR-X were the automatic annotations assigned to topic images. No textual information was considered under this formulation. The annotation vocabulary is composed of 17 keywords that describe visual aspects of the images. VCDTR-X is the IRM of the worst performance among those described in Table <ref type="table" coords="6,117.40,112.02,3.87,8.74" target="#tab_0">1</ref>, its MAP performance is very close to zero <ref type="bibr" coords="6,315.13,112.02,9.96,8.74" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-modal IRMs</head><p>Four multi-modal IRMs (rows 5-8 in Table <ref type="table" coords="6,284.12,158.30,4.43,8.74" target="#tab_0">1</ref>) of different nature were considered for HLFIRM. ABDE methods are two variants of the method described in Section 3. The first one uses the knn classifier for annotating images, while the second uses the MRFS approach for improving the labeling process. IRMs in rows 3 and 4 of Table <ref type="table" coords="6,319.89,194.16,4.98,8.74" target="#tab_0">1</ref> are multi-modal methods proposed for the ImageCLEF 2007 competition <ref type="bibr" coords="6,222.96,206.12,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="6,236.25,206.12,7.01,8.74" target="#b7">8]</ref>. IMFB-07 applies inter-media relevance feedback, a technique where the input for a text-based system is obtained from the output of a CBIR system combined with the original textual query <ref type="bibr" coords="6,229.53,230.03,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="6,243.76,230.03,12.73,8.74" target="#b15">16,</ref><ref type="bibr" coords="6,260.19,230.03,7.01,8.74" target="#b7">8]</ref>. This was our best-ranked entry for ImageCLEF 2007, and for that reason we considered it for this work. LF-07 is an LFIRM run that combines the outputs of a textual method and an CBIR system <ref type="bibr" coords="6,313.99,253.94,9.96,8.74" target="#b7">[8]</ref>. The textual-method performs Web-based query expansion, a technique in which each topic-statement is used as a query for Google R the top-20 snippets are then attached to the original query <ref type="bibr" coords="6,333.81,277.85,9.96,8.74" target="#b7">[8]</ref>. The CBIR system was the FIRE run described in the latter section. This was the run of our group with the highest recall, and that is why we considered for this work. One should note that IMFB-07 and LF-07 were not among the top ranked entries in ImageCLEF2007. These were ranked 41 and 82 in the overall ranked list and achieved a MAP of 0.1986 and 0.1701 respectively. However, in Section 6 we show experimental results that show that these runs resulted very useful for the HLFIRM approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Text-based IRMs</head><p>Text-based IRMs (rows 7-17 in Table <ref type="table" coords="6,254.74,383.90,4.43,8.74" target="#tab_0">1</ref>) are variants of a VSM using different weighting schemas. All of these methods index the available text in image annotations by using different weighting strategies (see Table <ref type="table" coords="6,184.30,407.81,3.87,8.74" target="#tab_0">1</ref>). For querying, these methods use the textual statements of topics (including the cluster and narrative fields). We considered ten textual IRMs because, traditionally, textual methods have outperformed both image-based and multi-modal IRMs in past ImageCLEF campaigns <ref type="bibr" coords="6,138.87,443.68,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="6,152.82,443.68,7.01,8.74" target="#b5">6]</ref>. However, the individual performance of textual IRMs is worst than that of the ABDE method <ref type="bibr" coords="6,158.49,455.63,9.96,8.74" target="#b8">[9]</ref>.</p><p>As we can see we have considered a variety of methods that can offer diversity, redundancy and complementariness of documents, opposed to previous work on LFIRM that use single-modality IRMs <ref type="bibr" coords="6,117.25,491.50,15.50,8.74" target="#b16">[17,</ref><ref type="bibr" coords="6,136.00,491.50,12.73,8.74" target="#b19">20,</ref><ref type="bibr" coords="6,151.98,491.50,7.75,8.74" target="#b5">6,</ref><ref type="bibr" coords="6,162.99,491.50,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="6,174.00,491.50,12.73,8.74" target="#b13">14,</ref><ref type="bibr" coords="6,189.98,491.50,12.73,8.74" target="#b20">21,</ref><ref type="bibr" coords="6,205.96,491.50,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="6,221.95,491.50,11.62,8.74" target="#b15">16]</ref>. These features resulted very useful for HLFIRM that achieved competitive retrieval performance. Further, the use of HLFIRM resulted useful for diversifying retrieval results. All of the IRMs were built by the authors, although some of them were based on methods developed by other research groups <ref type="bibr" coords="6,308.05,527.37,15.50,8.74" target="#b17">[18,</ref><ref type="bibr" coords="6,327.90,527.37,11.62,8.74" target="#b10">11]</ref>. One should note that the individual performance of all of the IRMs we considered is not competitive. Individual IRMs would be ranked at the middle (or near the end) of the overall list of ranked entries for ImageCLEF 2008. However, even with this limitation the best entries with HLFIRM were among the top ranked runs, see Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Diversifying retrieval results</head><p>The focus of this year photographic retrieval task was on diversification of retrieval results. Diversity of retrieved documents is important because it facilitates the search process to users. Experimental results (see <ref type="bibr" coords="6,191.10,653.86,43.58,8.74">Section 6)</ref> give evidence that HLFIRM is able to diversify retrieval results by itself. However, in order to further increase the variety of documents at the first positions, we developed a diversification approach based on LDA. This approach was applied for each topic as a postprocessing step. We applied this technique to our runs with the late fusion method because this method seemed more promising than the ABDE technique. For each topic we considered the ranked list of documents returned by a retrieval model (in our case we used the output of the HLFIRM approach). LDA is a probabilistic modeling tool widely used in text analysis, image annotation and classification <ref type="bibr" coords="7,133.43,213.48,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="7,148.59,213.48,11.62,8.74" target="#b21">22]</ref>. For text modeling, LDA assumes that documents are mixtures of unknown LDA-topics. LDA-Topics are nothing but probability distributions of words over documents that characterize semantic themes. LDA-Topics are estimated from a collection of documents by using Gibbs sampling or variational inference <ref type="bibr" coords="7,262.80,249.35,9.96,8.74" target="#b2">[3]</ref>. Since documents are mixtures of topics we can always calculate the probability of each document given an LDA-topic P (w|z i ). In this work we associate each document w to the topic that maximizes the latter probability (i. e. argmax i P (w|z i )). In this way, each document is associated to a single LDA-topic, which can be considered a cluster. In this work we used the topic-modeling toolbox due to Steyvers et al. that implements a Gibbs sampling algorithm for inference <ref type="bibr" coords="7,234.60,309.13,14.61,8.74" target="#b21">[22]</ref>.</p><p>For diversifying retrieval results we considered the documents returned by each retrieval model to a query-topic. Considering these documents we used the LDA toolbox for obtaining k LDAtopics (for our experiments we fixed k=20 because the top 20 documents are evaluated in Image-CLEF 2008). Documents were grouped according the LDA-topic they belong to. Then a single document was selected from each LDA-topic as representative of it. The representative document was selected according its relevance weight in the list of ranked documents returned by the retrieval model. The k representative documents were collocated at the top of a new ranked lists of documents. The rest of the documents returned by the retrieval model were collocated below the k documents, in the new list, according their initial relevance weight. In this way diverse yet highly relevant documents are considered at the begining of the ranked lists. Intuitively, a relevant document from each theme (LDA-topic) is put at the top of the new ranked list of documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental results</head><p>In this section we report the results of the runs we submitted to ImageCLEF 2008 for evaluation. A total of 16 runs were submitted comprising diverse configurations of the approaches we adopted. For each configuration we show the precision at 20 documents retrieved (p20), mean average precision (MAP), cluster recall at 20 documents retrieved (c20) and the number of relevant documents retrieved (Rel-Ret). We also show the average (Avg.) of p20, MAP and c20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">ABDE entries</head><p>First we analyze the performance of ABDE under different settings. The results of our runs with ABDE are shown in Table <ref type="table" coords="7,210.05,601.45,3.87,8.74" target="#tab_1">2</ref>. For all of these entries we used the same weighting schema in the retrieval model, see Section 3. Baseline is an VSM that uses only the original image annotations. ABDE-Manual is an VSM that expands the original annotations by using the labels from our training set (i. e. only were considered manually assigned labels). ABDE-knn uses as expansion the labels of our training set plus the labels assigned with a knn classifier (for those images that have not been manually labeled yet). ABDE-MRFS is a run where the labels assigned by knn are further improved with the MRFS approach, see Section 3.</p><p>As we can see slightly better results were obtained with the runs that adopted the ABDE approach. The highest MAP and Rel-Ret is obtained by using the manual labels only. While the precision is slightly higher in the ABDE-knn run. An interesting result is that the best cluster-recall performance is obtained by the ABDE-MRFS entry. The difference is significant with respect to the Baseline run. This means that using the labels improved with the MRFS method increases the diversity of results at the first positions, even when the MAP is low. Furthermore, ABDE-MRFS is the entry with the highest average performance, offering the best tradeoff between retrieval performance and diversity of results. This result suggest that the labeling improvement due to the MRFS method is indeed useful for improving AIA accuracy. Although, a deeper analysis is required to confirm the latter.</p><p>The results shown in Table <ref type="table" coords="8,224.98,258.17,4.98,8.74" target="#tab_1">2</ref> give evidence that the use of labels generated with AIA methods can be helpful for enhancing the retrieval process. However, the improvements due to the ABDE method are still small. Furthermore, the performance obtained with the best ABDE-entry (i. e. ABDE-MRFS ) is competitive to methods that only used text for ImageCLEF 2008. However, when compared to multi-modal methods its performance is not that competitive. Therefore, better strategies for combining labels and annotations must be developed. Anyway, despite the mild performance of ABDE methods (when compared to other multi-modal methods) these methods resulted very useful when their outputs were combined with the HLFIRM approach, as described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">HLFIRM entries</head><p>We performed experiments with several configurations for the HLFIRM approach described in Section 4. First we tried the simple combination of all of the IRMs described in Table <ref type="table" coords="8,479.79,412.05,4.98,8.74" target="#tab_0">1</ref> (All ). Then we applied the late fusion approach for combining IRMs that only use text (LF-TXT ) and IRMs that use images, either image-only or image+text (LF-VIS ). For these configurations we fixed the relevance weights of IRMs to α i = 1, in this way equal weights are assigned to each IRM, see Equation <ref type="bibr" coords="8,174.88,459.87,11.62,8.74" target="#b0">(1)</ref>. Then we tried the hierarchical HLFIRM of IRMs. For this configurations we applied the late fusion approach to the already (per-modality) fused LF-TXT and LF-VIS runs. For the hierarchical fusion we performed experiments with the following weighting schemas. HLF-EW is a run that assigns the same weight (i. e. α i in Equation ( <ref type="formula" coords="8,402.08,495.73,4.15,8.74" target="#formula_0">1</ref>)) to both lists LF-TXT and LF-VIS. HLF-0.8/0.2 assigns a weight of 0.8 to the LF-VIS list and of 0.2 to the LF-TXT run. HLF-0.2/0.8 assigns a weight of 0.2 to the LF-VIS list and of 0.8 to the LF-TXT run. The results of all of these configurations are shown in Table <ref type="table" coords="8,333.89,531.60,3.87,8.74" target="#tab_2">3</ref>.</p><p>As we can see our results are mixed, though all of the results are very competitive. The highest precision is obtained with the fusion of IRMs that use images either alone or combined with text (LF-VIS ). This is an interesting result because the individual performance of these methods is poor, see Sections 4.1 and 4.2. Therefore, the HLFIRM approach is effectively combining the outputs of visual IRMs, taking advantage of the diversity and redundancy of results through the individual lists. The performance of text-based methods (LF-TXT ) is not bad at all. In fact the LF-TXT run is among the top-5 ranked entries of methods that only used text. This is also an interesting result because this method is very easy to implement and to apply in practice. It only requires building several text-based retrieval models and combining its outputs by using Equation <ref type="bibr" coords="8,90.00,651.15,11.62,8.74" target="#b0">(1)</ref>. No complex processes or natural language processing tools were required.</p><p>The highest MAP and the best average performance is obtained with by the HLF-0.8/0.2 entry. This is our best entry in ImageCLEF 2008, ranked among the top-15 runs. This is another interesting result because traditionally in late fusion image retrieval better results are obtained by weighting high text-based methods <ref type="bibr" coords="8,262.34,698.97,15.50,8.74" target="#b16">[17,</ref><ref type="bibr" coords="8,281.72,698.97,12.73,8.74" target="#b19">20,</ref><ref type="bibr" coords="8,298.34,698.97,7.75,8.74" target="#b4">5,</ref><ref type="bibr" coords="8,309.99,698.97,7.75,8.74" target="#b5">6,</ref><ref type="bibr" coords="8,321.62,698.97,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="8,333.27,698.97,12.73,8.74" target="#b13">14,</ref><ref type="bibr" coords="8,349.88,698.97,12.73,8.74" target="#b20">21,</ref><ref type="bibr" coords="8,366.51,698.97,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="8,383.13,698.97,11.62,8.74" target="#b15">16]</ref>. This result suggest that, despite their poor performance (see Sections 4.1 and 4.2), the visual-based methods are more helpful for the HLFIRM approach. However, text-based methods are also useful for improving retrieval performance. Cluster-recall is high for most entries in Table <ref type="table" coords="8,396.12,734.84,3.87,8.74" target="#tab_2">3</ref>, giving evidence that the HLFIRM approach can be useful for diversifying retrieval results. This is due to the fact that with HLFIRM the top ranked documents of each IRM are collocated at the top positions of the fused list, since the IRMs are different in nature, they use to retrieve different documents at the first positions. Therefore, the documents at the first positions in the fused list are diverse. The entry HLF-0.2/0.8 is ranked 7 th in c20 over entries using text and images, even when we did not have applied any explicit method for diversifying results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Diversifying results of HLFIRM entries</head><p>We applied the LDA approach described in Section 5 as postprocessing to diversify results of the late fusion entries. Results of this experiment are shown in Figure <ref type="figure" coords="9,379.14,423.82,3.87,8.74" target="#fig_3">4</ref>, since the impact of the LDA technique is on the diversity of results we only plot the c20 measure.</p><p>As we can see the LDA approach resulted useful only for 2 out of 6 runs (All and LF-VIS ). The best c20 performance is now achieved with the All run. This entry obtained a c20 of 0.4291, which is ranked at the 4 th position in the ranked list of entries that use image and text (in c20). However, the average performance of the All entry is now of 0.2995. In fact the average performance of all of the runs is decreased by using the LDA approach.</p><p>This postprocessing resulted useful for All and LF-VIS because for these runs the LDA-topics were effectively identified and the initial ranking was also useful for identifying the representative document of each LDA-topic. For the rest of the entries the LDA approach could not improve the diversity of results. This is due to the quality of the initial list of retrieved documents. Thus even when the LDA approach could effectively find LDA-topics the representative document identified for each LDA-topic is not relevant to the original query. Results obtained with the LDA approach suggest that this technique can be helpful for improving the diversity of retrieval retrieval results. However, it is clear that the average performance of methods decreases by adopting this technique. Therefore, either the initial list of retrieved documents needs to be improved or the LDA technique needs to be modified in order to effectively diversify results for retrieval methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have described the participation of INAOE-TIA research group in the photographic retrieval task of ImageCLEF2008. This year we adopted two approaches to the multimedia retrieval problem: annotation-based document expansion (ABDE) and late fusion of independent-heterogeneous retrieval methods (HLFIRM). Further, we proposed a technique based on topic modeling with latent Dirichlet allocation for diversifying retrieval results. Experimental results with the ABDE method give evidence that the use of automatic annotations can be useful for improving retrieval performance. However, the slight improvements in retrieval performance show that either the ABDE approach may be not the best way for combining labels/annotations or that better annotation methods are required. An interesting result with ABDE is that diversity of results increased significantly by using the labels obtained with our MRFS approach. Showing the annotation effectiveness of MRFS and that the automatic annotations introduced diversity into the retrieval process. These results motivate further research on several directions, including the improvement of image annotation methods, the study of different strategies for combining automatic and manual annotations for multimedia retrieval and analyzing the potential diversity/complementariness/redundancy offered by automatic annotations.</p><p>Results with the HLFIRM approach confirm that late fusion is a very useful approach to image retrieval. Despite the individual performance of the methods we considered was not good, our runs with HLFIRM showed competitive performance to that of the best ranked entries in ImageCLEF 2008. An interesting finding is that better results were obtained by assigning a higher weight to visual retrieval methods instead to textual ones. Furthermore, the use of heterogenous methods allowed the HLFIRM approach to implicitly diversify the retrieval results. Current work with the HLFIRM approach consists of using high-performance individual retrieval methods for the fusion and studying different ways to measure the potential diversity/complementariness/redundancy of individual retrieval methods. Our results with the LDA technique show that it may be useful for further diversifying results. However, it is also very possible to damage the performance of the retrieval results with this technique. Nevertheless, our results motivate further research on the diversification technique.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,211.07,237.36,176.77,8.74;3,131.43,108.86,340.15,113.38"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Diagram of the ABDE approach.</figDesc><graphic coords="3,131.43,108.86,340.15,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,149.75,223.19,299.95,8.74;4,214.80,108.86,113.38,99.21"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample images from the segmented-annotated IAPR-TC12 collection [7].</figDesc><graphic coords="4,214.80,108.86,113.38,99.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,90.00,233.37,423.00,8.74;5,90.00,243.38,88.00,6.12;5,188.12,108.86,226.76,113.38"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Graphical diagram of the LFIRM approach. The output of different IRMs is combined for obtaining a single list of ranked documents.</figDesc><graphic coords="5,188.12,108.86,226.76,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,90.00,264.71,423.00,8.74;9,90.00,276.35,346.85,7.86;9,188.12,108.86,226.77,141.73"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Cluster recall at 20 documents (c20) for the HLFIRM entries described in Table 3 with (red dashed-line) and without (blue solid-line) the diversification technique based on LDA.</figDesc><graphic coords="9,188.12,108.86,226.77,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,90.00,261.30,423.00,184.61"><head>Table 1 :</head><label>1</label><figDesc>List</figDesc><table coords="5,218.04,261.30,165.41,140.03"><row><cell cols="2">ID Name</cell><cell>Modality</cell><cell>Description</cell></row><row><cell>1</cell><cell>FIRE</cell><cell>IMG</cell><cell>CBIR</cell></row><row><cell>2</cell><cell>VCDTR-X</cell><cell>IMG</cell><cell>VCDT</cell></row><row><cell>3</cell><cell>IMFB-07</cell><cell cols="2">TXT+IMG WQE+IMFB</cell></row><row><cell>4</cell><cell>LF-07</cell><cell cols="2">TXT+IMG WQE+LF</cell></row><row><cell>5</cell><cell>ABDE-1</cell><cell cols="2">TXT+IMG ABIR</cell></row><row><cell>6</cell><cell>ABDE-2</cell><cell cols="2">TXT+IMG ABIR</cell></row><row><cell>7</cell><cell>TBIR-1</cell><cell>TXT</cell><cell>VSM t/f</cell></row><row><cell>8</cell><cell>TBIR-2</cell><cell>TXT</cell><cell>VSM n/e</cell></row><row><cell>9</cell><cell>TBIR-3</cell><cell>TXT</cell><cell>VSM a/g</cell></row><row><cell>10</cell><cell>TBIR-4</cell><cell>TXT</cell><cell>VSM a/e</cell></row><row><cell>11</cell><cell>TBIR-5</cell><cell>TXT</cell><cell>VSM n/g</cell></row><row><cell>12</cell><cell>TBIR-5</cell><cell>TXT</cell><cell>VSM t/g</cell></row><row><cell>13</cell><cell>TBIR-6</cell><cell>TXT</cell><cell>VSM n/f</cell></row><row><cell>14</cell><cell>TBIR-7</cell><cell>TXT</cell><cell>VSM a/f</cell></row><row><cell>15</cell><cell>TBIR-8</cell><cell>TXT</cell><cell>VSM t/e</cell></row><row><cell>17</cell><cell>TBIR-9</cell><cell>TXT</cell><cell>VSM t/g</cell></row></table><note coords="5,144.06,415.88,368.94,6.12;5,90.00,423.85,423.00,6.12;5,90.00,431.82,423.00,6.12;5,90.00,439.79,269.11,6.12"><p>of the IRMs we considered in this work . From rows 7 and on, column 4 describes the local/global weighting schemas for a VSM. Abbreviations are as follows: WQE, web-based query expansion; IMFB, inter-media relevance feedback; LF, Late fusion; t, term-frequency; f, inverse document-frequency; n, augmented normalized term-frequency; e, entropy; a, alternate log; g, global-frequency/term-frequency; l, logarithmic frequency.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,90.00,110.06,423.00,69.94"><head>Table 2 :</head><label>2</label><figDesc>Performance of INAOE-TIA entries with ABDE evaluated in ImageCLEF2008. The best result of each measure is shown in bold.</figDesc><table coords="7,179.07,110.06,244.86,39.61"><row><cell>Run</cell><cell>p20</cell><cell>MAP</cell><cell>c20</cell><cell>Avg.</cell><cell>Rel-Ret</cell></row><row><cell>Baseline</cell><cell>0.3295</cell><cell>0.2625</cell><cell>0.3493</cell><cell>0.3137</cell><cell>1906</cell></row><row><cell>ABDE-Manual</cell><cell>0.3333</cell><cell>0.2648</cell><cell>0.351</cell><cell>0.3163</cell><cell>1913</cell></row><row><cell>ABDE-knn</cell><cell>0.3397</cell><cell>0.2554</cell><cell>0.3582</cell><cell>0.3177</cell><cell>1886</cell></row><row><cell>ABDE-MRFS</cell><cell>0.3295</cell><cell>0.2546</cell><cell>0.3733</cell><cell>0.3191</cell><cell>1882</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,90.00,110.06,423.00,77.60"><head>Table 3 :</head><label>3</label><figDesc>Performance of INAOE-TIA entries with the late fusion approach that were evaluated in ImageCLEF2008.</figDesc><table coords="8,182.77,110.06,237.46,56.35"><row><cell>Run</cell><cell>p20</cell><cell>MAP</cell><cell>c20</cell><cell>Avg.</cell><cell>Rel-Ret</cell></row><row><cell>All</cell><cell>0.3782</cell><cell>0.3001</cell><cell>0.4058</cell><cell>0.3613</cell><cell>1946</cell></row><row><cell>LF-TXT</cell><cell>0.341</cell><cell>0.2706</cell><cell>0.3815</cell><cell>0.3311</cell><cell>1885</cell></row><row><cell>LF-VIS</cell><cell>0.4141</cell><cell>0.2923</cell><cell>0.3864</cell><cell>0.3642</cell><cell>1966</cell></row><row><cell>HLF-EW</cell><cell>0.3795</cell><cell>0.303</cell><cell>0.3906</cell><cell>0.3577</cell><cell>1970</cell></row><row><cell>HLF-0.8/0.2</cell><cell>0.391</cell><cell>0.3066</cell><cell>0.4033</cell><cell>0.3667</cell><cell>1978</cell></row><row><cell>HLF-0.2/0.8</cell><cell>0.3731</cell><cell>0.2949</cell><cell>0.4175</cell><cell>0.3619</cell><cell>1964</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,105.24,739.13,224.23,6.99"><p>Available from http://www.imageclef.org/2008/iaprconcepts</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We would like to thank the organizers of ImageCLEF2008 because of their support. This work was partially supported by <rs type="funder">CONACyT</rs> under project grant <rs type="grantNumber">61335</rs> and scholarship <rs type="grantNumber">205834</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_eNZrF6K">
					<idno type="grant-number">61335</idno>
				</org>
				<org type="funding" xml:id="_SAY4JUD">
					<idno type="grant-number">205834</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,109.31,424.01,403.69,7.86;10,109.32,434.97,242.10,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,331.41,424.01,181.60,7.86;10,109.32,434.97,51.54,7.86">Overview of the imageclef 2008 photographic retrieval task</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Arni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,180.31,434.97,111.28,7.86">Working Notes of the CLEF</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.31,449.91,403.69,7.86;10,109.32,460.87,228.15,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,226.65,449.91,286.35,7.86">Merging results from different media: Lic2m experiments at imageclef</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Besancon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Millet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,145.91,460.87,132.88,7.86">Working notes of the CLEF 2005</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.31,475.81,334.04,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,144.38,475.81,158.68,7.86">Probabilistic Models of Text and Images</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>U.C. Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="10,109.31,490.76,403.69,7.86;10,109.32,501.72,381.28,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,208.41,490.76,304.59,7.86;10,109.32,501.72,191.01,7.86">Approaches of using a word-image ontology and an annotated image corpus as intermedia for cross-language image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,319.49,501.72,111.28,7.86">Working Notes of the CLEF</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.31,516.66,403.68,7.86;10,109.32,527.62,393.98,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,387.17,516.66,125.83,7.86;10,109.32,527.62,203.46,7.86">Overview of the imageclef 2006 photographic retrieval and object annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,332.19,527.62,111.28,7.86">Working Notes of the CLEF</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.31,542.56,403.68,7.86;10,109.32,553.52,392.41,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,387.17,542.56,125.83,7.86;10,109.32,553.52,107.37,7.86">Overview of the imageclef 2007 photographic retrieval task</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,236.14,553.52,45.01,7.86">CLEF 2007</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5152</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.31,568.47,403.69,7.86;10,109.32,579.42,403.68,7.86;10,109.32,590.38,159.94,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,180.59,579.42,225.74,7.86">The segmented and annotated IAPR-TC12 benchmark</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">Jair</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villase Nnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,417.96,579.42,95.03,7.86;10,109.32,590.38,131.52,7.86">Submitted to Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.31,605.33,403.69,7.86;10,109.32,616.29,403.68,7.86;10,109.32,627.25,403.69,7.86;10,109.32,638.20,125.89,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,136.57,616.29,316.34,7.86">Towards annotation-based query and document expansion for image retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">Jair</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villase Nnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,164.03,627.25,119.43,7.86">Proceedings of the CLEF 2007</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting>the CLEF 2007<address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5152</biblScope>
			<biblScope unit="page" from="546" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.31,653.15,403.69,7.86;10,109.32,664.11,403.69,7.86;10,109.32,675.07,366.15,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,359.01,653.15,153.99,7.86;10,109.32,664.11,123.24,7.86">Late fusion of heterogeneous methods for multimedia image retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">Jair</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,254.46,664.11,258.54,7.86;10,109.32,675.07,81.70,7.86">MIR08: Proceedings of the 2008 ACM Multimedia Information Retrieval Conference</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada, Forthcomming</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10">October 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.32,690.01,403.68,7.86;10,109.32,700.97,316.89,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,391.05,690.01,121.95,7.86;10,109.32,700.97,125.77,7.86">Ipal at imageclef 2007 mixing features, models and knowledge</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Chevallet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,255.10,700.97,111.28,7.86">Working Notes of the CLEF</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.32,715.91,403.68,7.86;10,109.32,726.87,403.69,7.86;10,109.32,737.83,52.25,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,301.55,715.91,211.45,7.86;10,109.32,726.87,263.17,7.86">Fire in imageclef 2007: Support vector machines and logistic regression to fuse image descriptors in for photo retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,445.77,726.87,21.55,7.86">LNCS</title>
		<imprint>
			<biblScope unit="volume">5152</biblScope>
			<date type="published" when="2008">2008</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,109.32,112.70,403.68,7.86;11,109.32,123.66,403.69,7.86;11,109.32,134.62,162.10,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,332.22,112.70,180.77,7.86;11,109.32,123.66,158.09,7.86">The iapr tc-12 benchmark: A new evaluation resource for visual information systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,289.44,123.66,223.56,7.86;11,109.32,134.62,77.76,7.86">Proc. of the Intl. Workshop OntoImage&apos;2006 Language Resources for CBIR</title>
		<meeting>of the Intl. Workshop OntoImage&apos;2006 Language Resources for CBIR<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,109.32,149.56,403.68,7.86;11,109.32,160.52,403.68,7.86;11,109.32,171.48,217.23,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,237.75,149.56,275.25,7.86;11,109.32,160.52,68.32,7.86">Markov random fields and spatial information to improve automatic image annotation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,196.91,160.52,311.88,7.86">Proc. of the the 2007 Pacific-Rim Symposium on Image and Video Technology</title>
		<meeting>of the the 2007 Pacific-Rim Symposium on Image and Video Technology</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4872</biblScope>
			<biblScope unit="page" from="879" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,109.32,186.42,403.68,7.86;11,109.32,197.38,221.27,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,385.70,186.42,127.31,7.86;11,109.32,197.38,30.72,7.86">University of alicante in imageclef</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Izquierdo-Beviá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tomás</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saiz-Noeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Luis</forename><surname>Vicedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,159.48,197.38,111.28,7.86">Working Notes of the CLEF</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,109.32,212.33,403.68,7.86;11,109.32,223.29,386.77,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,255.43,212.33,257.58,7.86;11,109.32,223.29,152.07,7.86">Comparison of visual features and fusion techniques in automatic detection of concepts from news video</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rautiainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Seppdnen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,280.95,223.29,122.61,7.86">Proceedings of the IEEE ICME</title>
		<meeting>the IEEE ICME</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="932" to="935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,109.32,238.23,403.68,7.86;11,109.32,249.19,361.11,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,337.57,238.23,175.43,7.86;11,109.32,249.19,170.84,7.86">Ipal inter-media pseudo-relevance feedback approach to imageclef 2006 photo retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Maillot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Chevallet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Valea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,299.32,249.19,111.28,7.86">Working Notes of the CLEF</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,109.32,264.13,403.68,7.86;11,109.32,275.09,360.69,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,309.16,264.13,203.84,7.86;11,109.32,275.09,169.49,7.86">Uned at imageclef 2005: Automatically structured queries with named entities over metadata</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Peinado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>López-Ostenero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,298.90,275.09,111.28,7.86">Working Notes of the CLEF</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,109.32,290.03,403.68,7.86;11,109.32,300.99,403.68,7.86;11,109.32,311.95,135.98,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,218.12,290.03,243.32,7.86">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,479.82,290.03,33.18,7.86;11,109.32,300.99,299.61,7.86">Proceedings of the 2007 Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2007 Conference on Computer Vision and Pattern Recognition<address><addrLine>Minneapolis, Minnesota, US</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007-06">June 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,109.32,326.90,403.68,7.86;11,109.32,337.86,175.69,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,282.84,326.90,230.16,7.86;11,109.32,337.86,11.91,7.86">Image retrieval: Ideas, influences, and trends of the new age</title>
		<author>
			<persName coords=""><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><forename type="middle">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,129.28,337.86,100.88,7.86">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,109.31,352.80,403.69,7.86;11,109.32,363.76,403.69,7.86;11,109.32,374.72,262.88,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,283.74,352.80,229.26,7.86;11,109.32,363.76,158.58,7.86">Analyzing the performance of visual, concept and text features in content-based video retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rautiainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tapio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,285.99,363.76,227.01,7.86;11,109.32,374.72,82.63,7.86">MIR &apos;04: Proc. of the 6th ACM workshop on Multimedia information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="197" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,109.31,389.66,403.69,7.86;11,109.32,400.62,395.92,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,288.53,389.66,206.08,7.86">Early versus late fusion in semantic video analysis</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,109.32,400.62,231.80,7.86">Proc. of the 13th Annual ACM Conference on Multimedia</title>
		<meeting>of the 13th Annual ACM Conference on Multimedia<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="399" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,109.31,415.56,403.69,7.86;11,109.32,426.52,158.81,7.86" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="11,232.50,415.56,280.50,7.86;11,109.32,426.52,49.20,7.86">Latent Semantic Analysis: A Road to Meaning, chapter Probabilistic topic models</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Laurence Erlbaum</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,109.31,441.47,403.69,7.86;11,109.32,452.43,351.90,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="11,252.21,441.47,260.79,7.86;11,109.32,452.43,81.39,7.86">Tmg: A matlab toolbox for generating term-document matrices from text collections</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zeimpekis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gallopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,209.99,452.43,120.71,7.86">Recent Advances in Clustering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="187" to="210" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
