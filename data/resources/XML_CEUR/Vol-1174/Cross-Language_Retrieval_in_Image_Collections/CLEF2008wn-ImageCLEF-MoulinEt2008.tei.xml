<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,174.00,148.63,255.30,15.51">UJM at ImageCLEFwiki 2008</title>
				<funder ref="#_BZSKXY5 #_GHRxR38 #_zV3hGdS">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,125.40,182.13,79.73,9.96"><forename type="first">Christophe</forename><surname>Moulin</surname></persName>
							<email>christophe.moulin@univ-st-etienne.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">UMR</orgName>
								<address>
									<postCode>5516</postCode>
									<settlement>Saint-Étienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,213.15,182.13,52.50,9.96"><forename type="first">Cécile</forename><surname>Barat</surname></persName>
							<email>cecile.barat@univ-st-etienne.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">UMR</orgName>
								<address>
									<postCode>5516</postCode>
									<settlement>Saint-Étienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.38,182.13,57.35,9.96"><forename type="first">Mathias</forename><surname>Géry</surname></persName>
							<email>mathias.gery@univ-st-etienne.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">UMR</orgName>
								<address>
									<postCode>5516</postCode>
									<settlement>Saint-Étienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,338.55,182.13,87.19,9.96"><forename type="first">Christophe</forename><surname>Ducottet</surname></persName>
							<email>ducottet@univ-st-etienne.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">UMR</orgName>
								<address>
									<postCode>5516</postCode>
									<settlement>Saint-Étienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,433.68,182.13,82.59,9.96"><forename type="first">Christine</forename><surname>Largeron</surname></persName>
							<email>largeron@univ-st-etienne.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">UMR</orgName>
								<address>
									<postCode>5516</postCode>
									<settlement>Saint-Étienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,174.00,148.63,255.30,15.51">UJM at ImageCLEFwiki 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">424A2EC08A1C5B91D83D705F7D79CF6B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports our multimedia information retrieval experiments carried out for the ImageCLEF track (ImageCLEFwiki). The task is to answer to user information needs, i.e. queries which may be composed of several modalities (text, image, concept) with ranked lists of relevant documents. The purpose of our experiments is twofold: firstly, our overall aim is to develop a multimedia document model combining text and/or image modalities. Secondly, we aim to compare results of our model using a multimedia query with a text only model.</p><p>Our multimedia document model is based on a vector of textual and visual terms. The textual terms correspond to words. The visual ones result from local colour descriptors which are automatically extracted and quantized by k-means, leading to an image vocabulary. They represent the colour property of an image region. To perform a query, we compute a similarity score between each document vector (textual + visual terms) and the query using the Okapi method based on the tf.idf approach.</p><p>We have submitted 6 runs either automatic or manual, using textual, visual or both information. Thanks to these 6 runs, we aim to study several aspects of our model, as the choice of the visual words and local features, the way of combining textual and visual words for a query and the performance improvements obtained when adding visual information to a pure textual model. Concerning the choice of the visual words, results show us that they are significant in some cases where the visualness of the query is meaningful. The conclusion about the combination of textual and visual words is surprising. We obtain worth results when we add directly the text to the visual words. Finally, results also inform that visual information bring complementary relevant documents that were not found with the text query. These initial results are promising and encourage the development of our multimedia model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The capacity of data storage increases constantly making possible the collection of large amount of information of all kinds, as texts, images, videos or combinations of them. In order to retrieve documents in such amount of data, information retrieval techniques tailored for the data types are required.</p><p>The ImageCLEF collection consists of multimedia documents made up of text and images. In this paper, we present our participation to the ImageCLEFwiki task. Our research goals are twofold: First, we aim to propose a multimedia document model combining text and image modalities adapted for multimedia retrieval. Second, we want to study the performance of our model compared to a text retrieval approach. In order to benefit from our long time experience with textual model, we develop a vector-based model composed of textual and visual terms. The textual terms correspond to words of the text. The visual terms are obtained through a bag of words approach. Local colour descriptors are extracted from images and quantized by k-means leading to an image vocabulary.</p><p>After presenting our model, we will describe the runs we submitted. Then, we will comment on the results we obtained and conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Visual and textual document model</head><p>ImageCLEFwiki is a multimedia collection where documents are composed of text and image. User needs are represented by queries ("topics"), which are also multimedia (text, image and concept). Hence a multimedia document model is necessary to handle such a collection. We focus our work on combining textual and visual information without using the concept field of the topics. Before explaining our model, we will describe the collection data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Description of the data: ImageCLEF Wikipedia collection</head><p>The ImageCLEF Wikipedia collection is composed of 151'519 multimedia XML documents and 75 multimedia topics. The documents are made up of an image and a short text. Images are in a common format (jpeg and png) and their sizes are heterogeneous. They depict either drawings, paintings or screenshots. They are in colour or in black and white. The textual part of a document is unstructured and consists of a description of the image, information about the Wikipedia user who has uploaded the image, or the copyright of the image. The average number of words per document is about 33 words.</p><p>It is also possible to get the whole Wikipedia article from which each image is extracted. The ImageCLEF Wikipedia collection provides 75 topics whose only 28 contains an image as example of the expected answer. For the text part, these queries were composed of about 2 or 3 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Textual representation model</head><p>One of the most known document model in textual information retrieval is the vector space model introduced by Salton and al. <ref type="bibr" coords="2,260.92,521.73,10.00,9.96" target="#b1">[2]</ref>. This model is based on a textual vocabulary T = {t 1 , ..., t j , ...t |T | }. Each document is represented as a vector of weights d i = (w i,1 , ..., w i,j , ..., w i,|T | ) where w i,j is the weight of the term t j in the document d i . In order to calculate the weight of a term t j in a document d i , a tf.idf formula is usually applied. The tf i,j (term frequency) measures the relative frequency of a term t j in a document d i . We have used the one defined in the Okapi formula from Robertson and Jones <ref type="bibr" coords="2,244.94,582.69,9.91,9.96" target="#b0">[1]</ref>:</p><formula xml:id="formula_0" coords="2,224.76,603.69,152.32,27.97">tf i,j = (k 1 + 1) * n i,j n i,j + k 1 * (1 -b + b * |di| davg )</formula><p>where k 1 = 1.2 and b = 0.75 are two constants empirically defined in the Okapi formula, n i,j is the occurrence of the term t j in the document d i , |d i | is the size of the document d i and d avg is the average size of all documents in the corpus. The idf j (inverse document frequency) measures the discriminatory power of a term t j and is defined as <ref type="bibr" coords="2,136.56,690.45,9.91,9.96" target="#b0">[1]</ref>:</p><formula xml:id="formula_1" coords="2,246.48,701.37,108.90,24.01">idf j = log |D| -df j + 0.5 df j + 0.5</formula><p>where |D| is the size of the corpus and df j is the number of documents in which the term t j occurs at least one time.</p><p>The weight w i,j is then obtained by multiplying tf i,j and idf j . This weight is higher when the term t j is frequent in the document d i but rare in the others. In our case, the size of our vocabulary T is 217'323 after applying a Porter stemming. The indexing has been performed with the Lemur software<ref type="foot" coords="3,126.00,146.45,3.97,5.45" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual representation model</head><p>In order to combine the visual information with the textual one, the images are also represented by a vector of visual words. It is therefore necessary to create a visual vocabulary V {v 1 , ..., v j , ..., v |T | }.</p><p>Our method consists in partitioning all images into 16x16 grids, a minimum of 8x8 pixels being required for each cell. It leads to about 256 cells per image, or about 38 million over all images.</p><p>For each cell, we compute a feature vector that contains the colour properties of the region. The vector is a 6 dimensional vector. The 6 dimensions correspond to the mean and the standard deviation for We apply a k-means algorithm over 4 millions of cells randomly selected within the 38 millions of cells to obtain 2'000 visual terms, which correspond to our visual vocabulary V . 2'000 has been chosen arbitrarily while 4 millions corresponded to the maximum number of cells we could compute due to the complexity of the k-means. Each visual term represents a cluster of feature vectors.</p><p>Then, each new image can be represented using a vector of visual terms. It is decomposed into a 16x16 grid and the local features are computed. Each cell is then assigned to the closest visual term from our visual vocabolary V , using the euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Using the model described in the last section, we present our approach for multimedia documents retrieval from multimedia queries. Then we describe the runs we submitted to ImageCLEFwiki in order to evaluate our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Queries and matching</head><p>As mentionned before, the ImageCLEFwiki topics are composed of text, image and concept modalities. However, our model is designed to take only into account text and image modalities. Our retrieval approach consists in computing a similarity score between each document d i and the query q k using the Okapi method <ref type="bibr" coords="3,239.42,557.61,10.00,9.96" target="#b0">[1]</ref>. Documents are then ranked according to their scores. The following expression is used to compute the score:</p><formula xml:id="formula_2" coords="3,216.84,592.89,168.29,20.97">score(q k , d i ) = uj ∈q k tf i,j * idf j * qtw k,j</formula><p>where qtw k,j is defined as:</p><formula xml:id="formula_3" coords="3,249.96,636.45,100.97,23.89">qtw k,j = (k 3 + 1) * n k,j k 3 + n k,j</formula><p>where k 3 = 7 is a constant defined in the okapi formula and where n k,j represents the occurrence of the term u j in the query q k . This score is higher when the term u j is frequent in the document d i but rare in the others and is weighted by the occurrence of u j in the query.</p><p>Let us insist on the fact that the term u j can be either a textual term t j or a visual term v j and that queries can be composed of textual terms only, visual terms only or both textual and visual terms which allows to perform text only queries, image only queries or multimedia queries.</p><p>The textual terms used for queries are those provided with topics. When visual terms are used, they are extracted either from the topic images or from the collection images. This will be detailed in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Submitted runs</head><p>We have submitted 6 runs to ImageCLEFwiki 2008, labelled from run 01 to run 06. Part of them are automatic (auto), others required manual selection of some relevant documents (man). Thanks to these 6 runs, we aim to study several aspects of our model, as the choice of the visual words and local features, the combination of textual and visual words for a query and the performance improvements obtained when adding visual information to a pure textual model. All these runs are summed up in table <ref type="table" coords="4,196.93,265.17,3.90,9.96">1</ref>.</p><formula xml:id="formula_4" coords="4,112.56,287.85,377.29,96.49">run name first query (R 1 ) run type R 1 use second query (R 2 ) results LaHC run01 t auto - - R 1 LaHC run02 t auto v 10 v t R 2 LaHC run03 t man v 100 v t R 2 LaHC run04 t auto man - v 100 t+ v q if i q exists v t else R 2 LaHC run05 t man v 100 t+v t R 2 LaHC run06 t auto v 10 v t R 1 ∩ R 2</formula><p>• t: text only query: uj ∈ q ∩ T • R1: first query results (baseline results); R2: second query results</p><p>• auto: automatic run; man: selection of relevant images by user</p><p>• v10: automatic selection of the first 10 results from R1</p><p>• v100: manual selection of the relevant documents in the first 100 results from R1</p><p>• iq: query image</p><p>• vq: visual words extracted from the query image</p><p>• vt: v10 or v100</p><p>Table <ref type="table" coords="4,262.80,523.65,3.90,9.96">1</ref>: Summary of the runs</p><p>We define a baseline that corresponds to a pure text model. It uses only textual terms for the query and scoring of documents. This run is run 01 and its results are noted R 1 . We did not use neither feedback nor query expansion for this automatic run.</p><p>All other runs exploit both textual and visual information of documents. They consist in two successive queries. The first one corresponds to the baseline (R 1 ), while the second one is a visual or textual and visual query, either automatic or manual (R 2 ). R 2 is automatic when visual terms are extracted automatically from some images: we chose to select all the visual words of the top 10 retrieved documents issued from the baseline (v 10 ), assuming these results as relevant. R 2 is manual when the user is asked to select relevant documents among the first 100 results of R 1 (v 100 ). There is no limit in the number of selected documents.</p><p>Run 02 and run 06 are automatic and only visual runs. For run 02, all the results of the second query are given as answer (R 2 ) while for run 06 an intersection is performed between the results of the first and second query (R 1 ∩ R 2 ). This intersection is interesting as it emphasises the gain of the visual information use. Indeed, if the intersection is not null, it means that the visual query leads us to find documents which were not in the baseline results.</p><p>Run 03, run 04 and run 05 are manual runs. Run 03 is visual, while run 04 and run 05 are multimedia. For run 03, all the visual words of the selected images are used to perform the second query. For run 04 and run 05 we did a query expansion in order to analyse the combination of textual and visual information (t+ v 100 ). We kept the textual words of the initial query and added some visual words. For run 05, these words come from the manual selected images. The run 04 proceeds as run 05, unless a query image is provided for the considered topic. In that case, the second query is composed of the visual words extracted from the query image. Thanks to these two runs, we can study the influence of the number of relevant images used for queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Table <ref type="table" coords="5,117.36,261.81,4.98,9.96" target="#tab_0">2</ref> shows that our best textual run (LaHC run01) ranks us 6th on 12 participants. This run is ranked 22th on the whole 77 runs. These results provide a basis to evaluate our approach on different aspects: the choice of the visual words, the way to combine textual and visual words and the performance improvements obtained when adding visual information to a pure textual model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank</head><p>Concerning the choice of the visual words, results from run 02 and run 03 show us that they are meaningful in some particular cases. Indeed, using only the visual words, we are able to retrieve relevant documents which were not used for the query. For example, in run 03, we retrieved 42 relevant documents for the query "blue flowers" whereas we had just selected 9 images. However there are some topics for which the results are bad. Only a third of the topics leads to more relevant documents than the number used for the query. This is due to the visualness of the query. It is obvious that for a query like "blue flower", the visual information is more useful than for a query like "peace anti-war protest". Moreover, we observe from the results of run 04 and run 05 that only one image is insufficient to obtain good results. For topics which provide an image query, the results are always worth than the obtained results with several selected images. This can be explained by the fact that image query were not enough representative. Furthermore it is obvious that one image can not be enough expressive compared to several relevant images. The conclusion about the combination of textual and visual words is surprising. The comparison between run 03 and run 05 tells us that adding directly the text to the same visual words leads to worth results. The explanation is not obvious, but it seems to be that the proportion between textual and visual parts is unbalanced. Where we only have 2 or 3 textual words, we have several thousand of visual words.</p><p>Results also show that visual information bring complementary relevant documents that were not found with the text query. These initial results are promising and encourage the development of our multimedia model. The comparison between run 01, run 02 and run 06 inform us that 214 new relevant documents were retrieved with the visual query only over all topics. To take an example, 32 documents were found with run 01 and 30 with run 02. The intersection of both results coming from run 06 gives 13 shared documents. Thus, 17 relevant documents were retrieved with the visual information only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>For future work, as our local image features are basic, other features such as texture and edge information would surely lead to performance improvements. We also plan to automatically select the number of visual words using machine learning approaches. Finally, we aim to combine more efficiently textual and visual information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,162.96,275.78,6.00,6.33;3,150.60,283.22,30.50,6.33;3,182.52,277.17,2.77,9.96;3,202.56,275.78,6.20,6.33;3,190.20,283.22,30.50,6.33;3,225.96,277.17,16.10,9.96;3,246.84,275.78,30.50,6.33;3,252.24,283.49,19.92,5.45;3,282.48,277.17,230.60,9.96;3,90.00,289.17,82.81,9.96"><head></head><label></label><figDesc>3 * 255 where R, G and B are the red, green and blue components of the cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,90.00,294.45,423.16,320.76"><head>Table 2 :</head><label>2</label><figDesc>Best textual run of each participantAs we can see on table 3, our best text+image run (LaHC run03) ranks us 4th on 7 participants. This run, ranked 57th on the whole 77 runs, is outperformed by our best textual run (LaHC run01).</figDesc><table coords="5,97.80,294.45,407.58,320.76"><row><cell></cell><cell cols="2">Participant</cell><cell>Run</cell><cell cols="2">Feedback/Expansion MAP</cell><cell>P@10</cell></row><row><cell>1</cell><cell>upeking</cell><cell></cell><cell>zzhou3</cell><cell></cell><cell>QE</cell><cell>0.3444 0.4760</cell></row><row><cell>3</cell><cell>ualicante</cell><cell></cell><cell>IRnNoCamel</cell><cell></cell><cell>NOFB</cell><cell>0.2700 0.3893</cell></row><row><cell>4</cell><cell>cea</cell><cell></cell><cell>ceaTxt</cell><cell></cell><cell>QE</cell><cell>0.2632 0.4427</cell></row><row><cell>11</cell><cell>sztaki</cell><cell></cell><cell>bp acad textonly qe</cell><cell></cell><cell>QE</cell><cell>0.2546 0.3720</cell></row><row><cell>13</cell><cell>cwi</cell><cell></cell><cell>cwi lm txt</cell><cell></cell><cell>NOFB</cell><cell>0.2528 0.3427</cell></row><row><cell>22</cell><cell>curien</cell><cell></cell><cell>LaHC run01</cell><cell></cell><cell>NOFB</cell><cell>0.2453 0.3680</cell></row><row><cell>30</cell><cell>chemnitz</cell><cell></cell><cell>cut-txt-a</cell><cell></cell><cell>NOFB</cell><cell>0.2166 0.3440</cell></row><row><cell>44</cell><cell>imperial</cell><cell></cell><cell>SimpleText</cell><cell></cell><cell>NOFB</cell><cell>0.1918 0.3240</cell></row><row><cell>48</cell><cell>irit</cell><cell></cell><cell>SigRunText</cell><cell></cell><cell>NOFB</cell><cell>0.1652 0.2880</cell></row><row><cell>52</cell><cell>ugeneva</cell><cell></cell><cell>unige text baseline</cell><cell></cell><cell>NOFB</cell><cell>0.1440 0.2053</cell></row><row><cell>56</cell><cell cols="3">upmc-lip6 TFUSION TFIDF LM</cell><cell></cell><cell>NOFB</cell><cell>0.1193 0.2160</cell></row><row><cell>70</cell><cell>utoulon</cell><cell></cell><cell>LSIS TXT method1</cell><cell></cell><cell>NOFB</cell><cell>0.0399 0.0467</cell></row><row><cell cols="2">Rank Participant</cell><cell></cell><cell>Run</cell><cell></cell><cell>Feedback/Expansion MAP</cell><cell>P@10</cell></row><row><cell>10</cell><cell>sztaki</cell><cell></cell><cell>bp acad avg5</cell><cell></cell><cell>NOFB</cell><cell>0.2551 0.3653</cell></row><row><cell>27</cell><cell>chemnitz</cell><cell></cell><cell>cut-mix-qe</cell><cell></cell><cell>QE</cell><cell>0.2195 0.3627</cell></row><row><cell>53</cell><cell>imperial</cell><cell></cell><cell>ImageText</cell><cell></cell><cell>NOFB</cell><cell>0.1225 0.2213</cell></row><row><cell>57</cell><cell>curien</cell><cell></cell><cell>LaHC run03</cell><cell></cell><cell>FB</cell><cell>0.1174 0.2613</cell></row><row><cell>62</cell><cell>upmc-lip6</cell><cell></cell><cell>TIFUSION LMTF COS</cell><cell></cell><cell>NOFB</cell><cell>0.1050 0.2267</cell></row><row><cell>68</cell><cell>upeking</cell><cell></cell><cell>zhou ynli tliu1.1</cell><cell></cell><cell>FBQE</cell><cell>0.0603 0.0040</cell></row><row><cell>71</cell><cell>utoulon</cell><cell cols="3">LSIS4-TXTIMGAUTONOFB</cell><cell>NOFB</cell><cell>0.0296 0.0307</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,193.44,627.09,216.29,9.96"><head>Table 3 :</head><label>3</label><figDesc>Best text+image run of each participant</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,746.03,113.50,7.35"><p>http://www.lemurproject.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,105.24,737.23,166.78,7.51"><p>LIMA project: http://liris.cnrs.fr/lima/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,105.24,746.71,233.82,7.51"><p>WI project: http://www.web-intelligence-rhone-alpes.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>This work is supported by the <rs type="projectName">LIMA</rs> project 2 and the <rs type="projectName">Web Intelligence</rs> project 3 which are <rs type="projectName">2 Rhône-Alpes region</rs> projects.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_BZSKXY5">
					<orgName type="project" subtype="full">LIMA</orgName>
				</org>
				<org type="funded-project" xml:id="_GHRxR38">
					<orgName type="project" subtype="full">Web Intelligence</orgName>
				</org>
				<org type="funded-project" xml:id="_zV3hGdS">
					<orgName type="project" subtype="full">2 Rhône-Alpes region</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,105.48,668.73,407.75,9.96;6,105.48,680.61,312.36,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,129.49,680.61,65.04,9.96">Okapi at trec-3</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Micheline</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aarron</forename><surname>Gull</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marianna</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,215.28,680.95,113.36,9.18">Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.48,699.45,407.58,9.96;6,105.48,711.45,123.62,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,269.21,699.45,192.96,9.96">A vector space model for automatic indexing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,470.51,699.79,42.55,9.18;6,105.48,711.79,19.45,9.18">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
