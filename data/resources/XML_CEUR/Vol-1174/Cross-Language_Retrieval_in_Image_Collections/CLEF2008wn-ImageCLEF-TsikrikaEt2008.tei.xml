<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.42,148.86,300.16,15.15;1,216.31,170.78,170.39,15.15">Overview of the wikipediaMM task at ImageCLEF 2008</title>
				<funder ref="#_ChUUMxy">
					<orgName type="full">European</orgName>
				</funder>
				<funder>
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_muru5pB">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder ref="#_ZDatwzs">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,218.15,204.67,80.64,8.74"><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
							<email>theodora.tsikrika@cwi.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">CWI</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.95,204.67,54.44,8.74"><forename type="first">Jana</forename><surname>Kludas</surname></persName>
							<email>jana.kludas@cui.unige.ch</email>
							<affiliation key="aff1">
								<orgName type="laboratory">CUI</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.42,148.86,300.16,15.15;1,216.31,170.78,170.39,15.15">Overview of the wikipediaMM task at ImageCLEF 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">49168795A276CDBBDEE1F99F416304FA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation ImageCLEF, Wikipedia image collection, image retrieval, evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The wikipediaMM task provides a testbed for the system-oriented evaluation of ad-hoc retrieval from a collection of Wikipedia images. It became a part of the ImageCLEF evaluation campaign in 2008 with the aim of investigating the use of visual and textual sources in combination to improve the retrieval performance. This paper presents an overview over the wikipediaMM 2008 task's resources, topics, assessments, participants' approaches, and main results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The wikipediaMM task provides a testbed for the system-oriented evaluation of multimedia information retrieval from a collection of Wikipedia (http://www.wikipedia.org/) images which has been previously used in the <ref type="bibr" coords="1,215.42,594.66,48.37,8.74">INEX 2006</ref><ref type="bibr" coords="1,268.59,594.66,19.19,8.74">INEX -2007</ref> Multimedia track <ref type="bibr" coords="1,371.60,594.66,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="1,385.98,594.66,7.01,8.74" target="#b3">4]</ref>. The aim is to investigate mono-media and cross-media retrieval approaches in the context of a large scale and heterogeneous collection of images (similar to those encountered on the Web), with unstructured and noisy textual annotations in English, that are searched for by users with diverse information needs.</p><p>It is an ad-hoc image retrieval task with an evaluation scenario similar to the classic TREC ad-hoc retrieval task and the ImageCLEFphoto task: simulation of the situation in which a system knows the set of documents to be searched, but cannot anticipate the particular topic that will be investigated (i.e. topics are not known to the system in advance). The goal of the simulation is: given a textual query (and/or sample images and/or concepts) describing a user's multimedia information need to find as many relevant images as possible from the (INEX MM) wikipedia image collection. A multimedia retrieval approach in that case should be able to combine the relevance of different media types into a single ranking that is presented to the user. In this first year of the task, the focus in on monolingual retrieval.</p><p>The paper is organized as follows. First, we introduce the task resources (the image collection and other available resources), the topics, and assessments (Sections 2-4). Section 5 presents the approaches employed by the different participants, while Section 6 summarizes their main results. Section 7 concludes the paper and provides an outlook on next year's task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task resources</head><p>The resources used for the wikipediaMM task are based on Wikipedia data. The following resources were available to the participants:</p><p>(INEX MM) wikipedia image collection: The collection consists of approximately 150,000</p><p>JPEG and PNG images provided by Wikipedia's users. Each image is associated with user generated alphanumeric, unstructured metadata in English. These metadata usually contain a brief caption or description of the image, the Wikipedia user who uploaded the image, and the copyright information. These descriptions are highly heterogeneous and of varying length. Further information about the image collection can be found in <ref type="bibr" coords="2,464.96,296.30,9.96,8.74" target="#b4">[5]</ref>. Image classification scores: For each image, the classification scores for the 101 different Medi-aMill concepts are provided by UvA <ref type="bibr" coords="2,271.64,629.64,9.97,8.74" target="#b2">[3]</ref>. The UvA classifier is trained on manually annotated TRECVID video data and the concepts are selected for the broadcast news domain.</p><p>Image features: For each image, the set of the 120D feature vectors that has been used to derive the above image classification scores is available <ref type="bibr" coords="2,350.99,673.48,9.97,8.74" target="#b0">[1]</ref>. Participants can use these feature vectors to custom-build a content-based image retrieval (CBIR) system, without having to pre-process the image collection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topic Format</head><p>The wikipediaMM topics are multimedia queries that can consist of a textual, visual, and conceptual part, with the latter two parts being optional.</p><p>&lt;title&gt; query by keywords &lt;concept&gt; query by concept (optional) &lt;image&gt; query by image content (optional) &lt;narrative&gt; description of query in which the definitive definition of relevance and irrelevance are given</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">&lt;title&gt;</head><p>The topic &lt;title&gt; simulates a user who does not have (or does not want to use) example images or other visual information. The query expressed in the topic &lt;title&gt; is therefore a text-only query. This profile is likely to fit most users searching digital libraries.</p><p>Upon discovering that their &lt;title&gt; query returned many irrelevant hits, users might decide to add visual information and formulate a multimedia query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">&lt;concept&gt;</head><p>This field is directly related to the concepts for which classification results are provided as an additional source of information (see 2), i.e., they are restricted to the 101 MediaMill concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">&lt;image&gt;</head><p>The second type of visual evidence are example images, which can be taken from outside or inside Wikipedia and can be of any common format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">&lt;narrative&gt;</head><p>A clear and precise description of the information need is required in order to unambiguously determine whether or not a given image fulfils the given need. In a test collection this description is known as the narrative. It is the only true and accurate interpretation of a user's needs. Precise recording of the narrative is important for scientific repeatability -there must exist a definitive description of what is and is not relevant to the user. To aid this, the &lt;narrative&gt; should explain not only what information is being sought, but also the context and motivation of the information need, i.e., why the information is being sought and what work-task it might help to solve.</p><p>The three different types of information sources (textual terms, visual examples and visual concepts) can be used in any combination. For each field more than one entry can be specified. It is up to the systems how to use, combine or ignore this information; the relevance of a result item does not directly depend on them, but it is decided by manual assessments based on the &lt;narrative&gt;. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topic Development</head><p>The topics in the wikipediaMM task have been mainly developed by the participants. Altogether, 12 of the participating groups submitted 70 candidate topics. The 35 topics used in <ref type="bibr" coords="5,460.97,142.36,47.38,8.74">INEX 2006</ref><ref type="bibr" coords="5,90.00,154.32,19.93,8.74">INEX -2007</ref> Multimedia were also added to the candidate topic pool. The task organizers judged all topics in the pool towards their "visuality" as proposed in <ref type="bibr" coords="5,358.02,166.27,9.97,8.74" target="#b1">[2]</ref>, where instead of the "neutral" option, a "textual" one was used, leading to the following classification of candidate topics:</p><p>visual: topics that have visual properties that are highly discriminating for the problem (e.g., "blue flower"). Therefore, it is highly likely that CBIR systems are able to deal with them.</p><p>textual: topics that often consist of proper nouns of persons, buildings, locations, etc. (e.g., "Da Vinci paintings"). As long as the images are correctly annotated, text-only approaches are likely to suffice.</p><p>semantic: topics that have a complex set of constraints, need world knowledge or contain ambiguous terms (e.g., "labor demonstrations"). It is highly likely that no modality alone is effective.</p><p>The candidate topics were classified by the organisers; for the old INEX topics, the results of the INEX participants' runs were also used to aid this classification. The final topic set is listed in Table <ref type="table" coords="5,117.98,338.78,4.98,8.74" target="#tab_1">1</ref> and consists of 75 topics: 5 visual (topic ids: 1-5), 35 textual (topic ids: 6-40), and 35 semantic (topic ids: 41-75). Table <ref type="table" coords="5,245.36,350.74,4.98,8.74" target="#tab_2">2</ref> shows some statistics on the topics. Not all topics contain visual/multimedia information (i.e., image examples or visual concepts); this corresponds well with realistic scenarios, since users who express multimedia information needs do not necessarily want to employ visual evidence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Assessments</head><p>The wikipediaMM task is an image retrieval task, where an image with its metadata is either relevant or not (binary relevance). We adopted TREC-style pooling of the retrieved images with a pool depth of 100, resulting in pools of between 753 and 1850 images with a mean and median both around 1290. The evaluation was performed by the participants of the task within a period of 4 weeks after the submission of runs. The 13 groups that participated in the evaluation process used a web-based interface previously employed in the INEX Multimedia and TREC Enterprise tracks. Each participating group was assigned 4-5 topics and an effort was made to ensure that most of the topics were assigned to their creators. This was achieved in 76% of the assignments for the topics created by this year's participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participants</head><p>A total of 12 groups submitted 77 runs. Table <ref type="table" coords="5,310.12,709.78,4.98,8.74" target="#tab_3">3</ref> gives an overview of the resources used by the submitted runs. Most of the runs are textual only approaches, but compared to the INEX Multimedia track, there is a rise in fusion approaches that combine text and images, text and concepts, and all three modalities. Digital Media Institute, Peking University, China (7 runs). They investigated: (i) a text-based approach with query expansion, where the expansion terms are automatically selected from a knowledge base that is (semi-)automatically constructed from Wikipedia, (ii) a content-based visual approach, where they first train 1-vs-all classifiers for all queries by using the training images obtained by Yahoo! search, and then treat the retrieval task as a visual concept detection in the given Wikipedia image set, and (iii) a cross-media approach that combines and re-ranks the text-and content-based retrieval results.</p><p>CEA LIST, France (2 runs). Their approach is based on a query reformulation using concepts that are semantically related to those in the initial query. For each interesting entity in the query, they used Wikipedia and WordNet to extract related concepts, which were further ranked based on their perceived usefulness. A small list of visual concepts was used to rerank the answers to queries that included them, implicitly or explicitly. They submitted two automatic runs, one based on query reformulation only and one combining query reformulation and visual concepts.</p><p>NLP and Information Systems, University of Alicante, Spain (24 runs). They employed their textual passage-based retrieval system as their baseline approach, which was enhanced by a module that decomposes the (compound) file names in camel case notation into single terms, and by a module that performs geographical query expansion. They also investigated Probabilistic Relevance Feedback and Local Context Analysis techniques.</p><p>Data Mining and Web Search, Hungarian Academy of Sciences (8 runs). They used their own retrieval system to experiment with a text-based approach, that uses BM25 and query expansion based on blind relevance feedback, and its combination with a segmentbased visual approach.</p><p>Database Architectures and Information Access, CWI, Netherlands (2 runs). They used a language modelling approach based on purely textual evidence and also incorporated a length prior to bias retrieval towards images with longer descriptions than the ones retrieved by the language model.</p><p>Laboratoire Hubert Curien, Université Jean Monnet, Saint-Etienne, France (6 runs).</p><p>They used a vector space model to compute similarities between vectors of both textual and visual terms; the textual part is a term vector of the terms' BM25 weights and the visual part a 6-dimensional vector of clusters of colour features. The applied both manual and blind relevance feedback to a text-based run in order to expand the query with visual terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dept. of Computer Science and Media, Chemnitz University of Technology (4 runs).</head><p>They used their Xtrieval framework based on both textual and visual features, and also made use of the provided resources (concepts and features). They experimented with text-based retrieval, its combination with a visual approach, the combination of all three modalities, and thesaurus-based query expansion. They also investigated the efficiency of the employed approaches.</p><p>Multimedia and Information Systems, Imperial College, UK (6 runs). They examined textual features, visual features, and their combination. Their text-based approach was combined with evidence derived from a geographic co-occurrence model mined from Wikipedia, which aimed at disambiguating geographic references either in a context-dependent or a context-independent manner. Their visual-based approach, employing Gabor texture features and the Cityblock distance as a similarity measure, was combined with the text-based approach and with blind relevance feedback using a convex combination of ranks.</p><p>SIG-IRIT, Toulouse, France (4 runs). They explored the use of images' names as evidence in text-based image retrieval. They used them in isolation, by computing a similarity score between the query and the name of images using the vector space model, and in combination with textual evidence, either by fusing the ranking of a text-based approach (based on the XFIRM retrieval system) with the ranking produced by the name-based technique, or by using the text-based approach with an increase in the weight of terms in the image name.</p><p>Computer Vision and Multimedia, Université de Genève, Switzerland (2 runs). Our approach is based the preference ranking option of the SVM light library developed by the Cornell University. The first run, which is our baseline, uses only text features that were extracted form the Wikipedia collection and the queries. The second run applies a feature selection to the high dimensional textual feature vector based on the features relevance to each query. The results were disappointing, which is due to that our system is still under development.</p><p>UPMC/LIP6 -Computer Science Lab, Paris, France (7 runs). They investigated (i) text-based retrieval, using a tf.idf approach, a language modelling framework and their combination based on the ranks of retrieved images, and (ii) the combination of textual and visual evidence, by reranking the text-based results using visual similarity scores (Euclidean distance and a manifold-based technique, both based on HSV features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSIS, UMR CNRS &amp; Université Sud</head><p>Toulon-Var, France (5 runs). They applied the same techniques they used for the Visual Concept Detection task at ImageCLEF 2008, by relating each of the wikipediaMM topics to one or more visual concepts from that task. They also fused these visual-based rankings with the results of a text-based approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Table <ref type="table" coords="7,118.47,530.45,4.98,8.74" target="#tab_4">4</ref> shows the top 30 runs ranked by their Mean Average Precision (MAP); the complete result list of results can be found at: http://www.imageclef.org/2008/wikimm-results.</p><p>We first analyse the performance per modality by comparing the average performance over runs that employ the same type of resources. For every analysis, we take into account only the top 75% of all runs so as to exclude noise by removing erroneous and buggy runs. Table <ref type="table" coords="7,479.35,578.27,4.98,8.74" target="#tab_5">5</ref> shows the average performance and standard deviation with respect to modality.</p><p>According to the MAP measure, the best performing runs fuse text and concepts, followed by the runs that fuse text and images, and the text-only baseline. The latter two perform almost identically. A similar result is obtained with the precision after R (= number of relevant) documents are retrieved (R-prec.). Again, the TxtCon runs outperform all others. But contrary to the MAP, here all runs that use multi-modal fusion achieve better results than the text-only baseline, even though TxtImgCon does this only by a small margin.</p><p>An actual surprise comes for the precision at 20 documents, where the visual-only based runs (only 2 in the top 75%) outperform by large the fusion approaches TxtCon and TxtImgCon, and the text-only baseline. So, good content-based retrieval of images can return highly relevant documents in the top ranks. The problem seems to be the fusion with the textual and conceptual evidence, since TxtImg and TxtImgCon fusion approaches cannot outperform the other approaches. But in general it can be said, that this year the fusion approaches perform surprisingly well, which is mostly due to the success of incorporating concepts.    We further analyse the performance per topic. Figure <ref type="figure" coords="9,339.93,466.35,4.98,8.74" target="#fig_1">2</ref> shows for each topic the average MAP over all runs in the top 75%, over only the text-based ones among them, and over only the visualbased ones among them. The text-based runs outperform the visual-based ones in about half (54%) the topics. Table <ref type="table" coords="9,167.32,502.22,4.98,8.74">7</ref> presents the top 10 topics, where the visual runs outperform the textual, and vice versa. In the textual top 10, there are a lot of topics with proper nouns. Additionally, about half of them, do not have an example image in the topic (see Table <ref type="table" coords="9,391.37,526.13,3.88,8.74" target="#tab_1">1</ref>), which explains the good performance of text-based approaches. The topics in the top 10 of the visual-based runs contain more general concepts. These could also be topics that are not well annotated in the collection. Oddly, the visual top 10 also includes topics without image examples, which suggests that some visual feedback or query expansion has been used by some of the participants.</p><p>Figure <ref type="figure" coords="9,138.20,585.90,4.98,8.74" target="#fig_2">3</ref> compares the text baseline with the 3 fusion approaches: (1) text/concept, (2) text/visual and (3) text/concept/visual. The fusion of text and concepts outperforms the textonly baseline for 62% of the topics, the fusion of text and images for about half the topics (49%), and the fusion of all 3 modalities for 44% of the topics. We again create a list of the top 10 topics for which each fusion approach outperforms the text-only baseline. The lists for the fusion of text/visual and text/concept/visual have many entries in common. All 3 lists contain topics with proper nouns, as well as topics with general concepts. The clear distinction that was visible before between the visual and textual runs is now blurred. So, in summary fusion approaches that use several modalities are catching up with the text-based approaches. Especially the performance of the text/concept fusion approaches is impressively good. The visual hints help mainly for topics that incorporate an image example, but can also improve the overall performance. With the help of our participants, we both developed and assessed a diverse set of 75 multimedia topics. The results show that the dominance of text-based image retrieval is coming to an end; multi-modal fusion approaches help to improve the retrieval performance in this domain.</p><p>Our main focus for next year remains the same: researching the combination of evidence from different modalities in a "standard" ad-hoc image retrieval task. Possible new directions for 2009 include the addition of multilinguality in form of multi-lingual topics (and if possible annotations), and access to the context of the images, i.e., the Wikipedia web pages that contain them. We also aim at providing new sets of classification scores and low-level features, so that participants can concentrate their research effort on information fusion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,90.00,584.71,423.00,8.74;2,96.38,317.95,423.02,250.25"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example Wikipedia image+metadata from the (INEX MM) wikipedia image collection.</figDesc><graphic coords="2,96.38,317.95,423.02,250.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,180.61,265.70,241.79,8.74;9,90.00,108.86,425.19,141.73"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of different modalities per topic.</figDesc><graphic coords="9,90.00,108.86,425.19,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,187.29,265.70,228.42,8.74;10,90.00,108.86,425.19,141.73"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: performance of different fusion approaches</figDesc><graphic coords="10,90.00,108.86,425.19,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,149.73,423.00,571.33"><head>Table 1 :</head><label>1</label><figDesc>Topics</figDesc><table coords="4,90.00,149.73,423.00,571.33"><row><cell cols="7">for the ImageCLEF 2008 wikipediaMM task: ids, titles, and whether they include</cell></row><row><cell cols="7">visual information (Yes/No) in the form of image examples (IMG) and visual concepts (CON).</cell></row><row><cell>ID Topic title</cell><cell cols="3">IMG CON ID Topic title</cell><cell></cell><cell cols="2">IMG CON</cell></row><row><cell>blue flower</cell><cell>Y</cell><cell>Y</cell><cell>2 sea sunset</cell><cell></cell><cell>N</cell><cell>N</cell></row><row><cell>ferrari red</cell><cell>Y</cell><cell>Y</cell><cell>4 white cat</cell><cell></cell><cell>Y</cell><cell>Y</cell></row><row><cell>silver race car</cell><cell>N</cell><cell>Y</cell><cell>6 potato chips</cell><cell></cell><cell>N</cell><cell>N</cell></row><row><cell>spider web</cell><cell>Y</cell><cell>N</cell><cell cols="2">8 beach volleyball</cell><cell>Y</cell><cell>Y</cell></row><row><cell>surfing</cell><cell>Y</cell><cell>Y</cell><cell cols="2">10 portrait of Jintao Hu</cell><cell>Y</cell><cell>Y</cell></row><row><cell>map of the United States</cell><cell>N</cell><cell>Y</cell><cell cols="2">12 rabbit in cartoons</cell><cell>Y</cell><cell>Y</cell></row><row><cell>DNA helix</cell><cell>Y</cell><cell>Y</cell><cell cols="2">14 people playing guitar</cell><cell>Y</cell><cell>Y</cell></row><row><cell>sars china</cell><cell>Y</cell><cell>N</cell><cell cols="2">16 Roads in California</cell><cell>Y</cell><cell>N</cell></row><row><cell>race car</cell><cell>N</cell><cell>Y</cell><cell cols="2">18 can or bottle of beer</cell><cell>N</cell><cell>N</cell></row><row><cell>war with guns</cell><cell>N</cell><cell>N</cell><cell>20 hunting dog</cell><cell></cell><cell>N</cell><cell>Y</cell></row><row><cell>oak tree</cell><cell>Y</cell><cell>Y</cell><cell cols="2">22 car game covers</cell><cell>Y</cell><cell>N</cell></row><row><cell>british trains</cell><cell>Y</cell><cell>N</cell><cell cols="2">24 peace anti-war protest</cell><cell>Y</cell><cell>Y</cell></row><row><cell>daily show</cell><cell>N</cell><cell>Y</cell><cell cols="2">26 house architecture</cell><cell>N</cell><cell>Y</cell></row><row><cell>baseball game</cell><cell>Y</cell><cell>N</cell><cell cols="2">28 cactus in desert</cell><cell>Y</cell><cell>Y</cell></row><row><cell>pyramid</cell><cell>Y</cell><cell>Y</cell><cell>30 video games</cell><cell></cell><cell>N</cell><cell>N</cell></row><row><cell>bridges</cell><cell>N</cell><cell>N</cell><cell>32 mickey mouse</cell><cell></cell><cell>Y</cell><cell>N</cell></row><row><cell>Big Ben</cell><cell>N</cell><cell>N</cell><cell>34 polar bear</cell><cell></cell><cell>N</cell><cell>N</cell></row><row><cell>George W Bush</cell><cell>Y</cell><cell>N</cell><cell>36 Eiffel tower</cell><cell></cell><cell>N</cell><cell>N</cell></row><row><cell>Golden gate bridge</cell><cell>Y</cell><cell>N</cell><cell cols="2">38 Da Vinci paintings</cell><cell>Y</cell><cell>N</cell></row><row><cell>skyscraper</cell><cell>Y</cell><cell>Y</cell><cell>40 saturn</cell><cell></cell><cell>Y</cell><cell>N</cell></row><row><cell>ice hockey players</cell><cell>N</cell><cell>Y</cell><cell cols="2">42 labor demonstrations</cell><cell>N</cell><cell>Y</cell></row><row><cell>mountains under sky</cell><cell>N</cell><cell>Y</cell><cell cols="2">44 graph of a convex func-</cell><cell>Y</cell><cell>Y</cell></row><row><cell></cell><cell></cell><cell></cell><cell>tion</cell><cell></cell><cell></cell><cell></cell></row><row><cell>paintings related to cu-</cell><cell>Y</cell><cell>Y</cell><cell cols="2">46 London parks in daylight</cell><cell>Y</cell><cell>Y</cell></row><row><cell>bism</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>maple leaf</cell><cell>Y</cell><cell>Y</cell><cell cols="2">48 a white house with a gar-</cell><cell>Y</cell><cell>Y</cell></row><row><cell></cell><cell></cell><cell></cell><cell>den</cell><cell></cell><cell></cell><cell></cell></row><row><cell>plant</cell><cell>Y</cell><cell>Y</cell><cell cols="2">50 stars and nebulae in the</cell><cell>Y</cell><cell>N</cell></row><row><cell></cell><cell></cell><cell></cell><cell>dark sky</cell><cell></cell><cell></cell><cell></cell></row><row><cell>views of Scottish lochs</cell><cell>Y</cell><cell>N</cell><cell>52 Cambridge</cell><cell>university</cell><cell>Y</cell><cell>N</cell></row><row><cell></cell><cell></cell><cell></cell><cell>buildings</cell><cell></cell><cell></cell><cell></cell></row><row><cell>military aircraft</cell><cell>N</cell><cell>Y</cell><cell cols="2">54 winter landscape</cell><cell>N</cell><cell>N</cell></row><row><cell>animated cartoon</cell><cell>N</cell><cell>Y</cell><cell cols="2">56 London city palaces</cell><cell>N</cell><cell>Y</cell></row><row><cell>people riding bicycles</cell><cell>Y</cell><cell>N</cell><cell>58 sail boat</cell><cell></cell><cell>Y</cell><cell>Y</cell></row><row><cell>dancing couple</cell><cell>N</cell><cell>Y</cell><cell>60 atomic bomb</cell><cell></cell><cell>Y</cell><cell>Y</cell></row><row><cell>Singapore</cell><cell>N</cell><cell>N</cell><cell>62 cities by night</cell><cell></cell><cell>Y</cell><cell>Y</cell></row><row><cell>star galaxy</cell><cell>N</cell><cell>N</cell><cell cols="2">64 football stadium</cell><cell>N</cell><cell>N</cell></row><row><cell>famous buildings of</cell><cell>N</cell><cell>Y</cell><cell>66 historic castle</cell><cell></cell><cell>N</cell><cell>N</cell></row><row><cell>Paris</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bees with flowers</cell><cell>Y</cell><cell>Y</cell><cell cols="2">68 pyramids in Egypt</cell><cell>Y</cell><cell>Y</cell></row><row><cell>mountains with snow</cell><cell>N</cell><cell>Y</cell><cell cols="2">70 female players beach vol-</cell><cell>Y</cell><cell>Y</cell></row><row><cell>under sky</cell><cell></cell><cell></cell><cell>leyball</cell><cell></cell><cell></cell><cell></cell></row><row><cell>children riding bicycles</cell><cell>N</cell><cell>N</cell><cell>72 civil aircraft</cell><cell></cell><cell>N</cell><cell>Y</cell></row><row><cell>bridges at night</cell><cell>Y</cell><cell>Y</cell><cell cols="2">74 gothic cathedral</cell><cell>Y</cell><cell>Y</cell></row><row><cell>manga female character</cell><cell>N</cell><cell>Y</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,159.78,418.35,283.44,81.86"><head>Table 2 :</head><label>2</label><figDesc>Statistics for the ImageCLEF 2008 wikipediaMM topics</figDesc><table coords="5,183.54,431.70,235.92,68.51"><row><cell>Number of topics</cell><cell>75</cell></row><row><cell>Average number of terms in title</cell><cell>2.64</cell></row><row><cell>Number of topics with image(s)</cell><cell>43</cell></row><row><cell>Number of topics with concept(s)</cell><cell>45</cell></row><row><cell>Number of topics with both image and concept</cell><cell>28</cell></row><row><cell>Number of topics with text only</cell><cell>15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,90.00,120.39,423.00,126.93"><head>Table 3 :</head><label>3</label><figDesc>Resources used by the 77 submitted runsBelow we briefly describe the appproaches investigated by the groups that participated in the ImageCLEF 2008 wikipediaMM task:</figDesc><table coords="6,209.54,133.74,183.93,68.51"><row><cell>textual</cell><cell>Txt</cell><cell>35</cell></row><row><cell>visual</cell><cell>Img</cell><cell>5</cell></row><row><cell>concept</cell><cell>Con</cell><cell>0</cell></row><row><cell>textual/visual</cell><cell>TxtImg</cell><cell>22</cell></row><row><cell>textual/concept</cell><cell>TxtCon</cell><cell>13</cell></row><row><cell cols="3">textual/visual/concept TxtImgCon 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,95.98,170.18,391.15,303.49"><head>Table 4 :</head><label>4</label><figDesc>Results for the top 30 runs</figDesc><table coords="8,95.98,182.75,391.15,290.92"><row><cell>Group</cell><cell>Run</cell><cell>Modality</cell><cell>FB/QE</cell><cell>MAP</cell><cell>P@10</cell><cell>P@20</cell><cell>R-prec.</cell></row><row><cell>upeking</cell><cell>zzhou3</cell><cell>Txt</cell><cell>QE</cell><cell cols="3">0.3444 0.4760 0.3993</cell><cell>0.3794</cell></row><row><cell>cea</cell><cell>ceaTxtCon</cell><cell>TxtCon</cell><cell>QE</cell><cell cols="3">0.2735 0.4653 0.3840</cell><cell>0.3225</cell></row><row><cell>ualicante</cell><cell>IRnNoCamel</cell><cell>Txt</cell><cell>NOFB</cell><cell cols="3">0.2700 0.3893 0.3040</cell><cell>0.3075</cell></row><row><cell>cea</cell><cell>ceaTxt</cell><cell>Txt</cell><cell>QE</cell><cell cols="3">0.2632 0.4427 0.3673</cell><cell>0.3080</cell></row><row><cell>ualicante</cell><cell>IRnNoCamelLca</cell><cell>Txt</cell><cell>FB</cell><cell cols="3">0.2614 0.3587 0.3167</cell><cell>0.2950</cell></row><row><cell>ualicante</cell><cell>IRnNoCamelGeog</cell><cell>Txt</cell><cell>QE</cell><cell cols="3">0.2605 0.3640 0.2913</cell><cell>0.3000</cell></row><row><cell>ualicante</cell><cell>IRnConcSinCamLca</cell><cell>TxtCon</cell><cell>FB</cell><cell cols="3">0.2593 0.3493 0.2900</cell><cell>0.3016</cell></row><row><cell>ualicante</cell><cell>IRnConcSinCam</cell><cell>TxtCon</cell><cell>NOFB</cell><cell cols="3">0.2587 0.3627 0.2900</cell><cell>0.2975</cell></row><row><cell>ualicante</cell><cell>IRnNoCamelLcaGeog</cell><cell>Txt</cell><cell>FBQE</cell><cell cols="3">0.2583 0.3613 0.3140</cell><cell>0.2922</cell></row><row><cell>sztaki</cell><cell>bp acad avg5</cell><cell>TxtImg</cell><cell>NOFB</cell><cell cols="3">0.2551 0.3653 0.2773</cell><cell>0.3020</cell></row><row><cell>sztaki</cell><cell>bp acad textonly qe</cell><cell>Txt</cell><cell>QE</cell><cell cols="3">0.2546 0.3720 0.2907</cell><cell>0.2993</cell></row><row><cell cols="3">ualicante IRnConcSinCamLcaGeog TxtCon</cell><cell>FBQE</cell><cell cols="3">0.2537 0.3440 0.2853</cell><cell>0.2940</cell></row><row><cell>cwi</cell><cell>cwi lm txt</cell><cell>Txt</cell><cell>NOFB</cell><cell cols="3">0.2528 0.3427 0.2833</cell><cell>0.3080</cell></row><row><cell>sztaki</cell><cell>bp acad avgw glob10</cell><cell>TxtImg</cell><cell>NOFB</cell><cell cols="3">0.2526 0.3640 0.2793</cell><cell>0.2955</cell></row><row><cell>sztaki</cell><cell>bp acad avgw glob10 qe</cell><cell>TxtImg</cell><cell>QE</cell><cell cols="3">0.2514 0.3693 0.2833</cell><cell>0.2939</cell></row><row><cell cols="2">ualicante IRnConcSinCamGeog</cell><cell>TxtCon</cell><cell>QE</cell><cell cols="3">0.2509 0.3427 0.2787</cell><cell>0.2924</cell></row><row><cell>sztaki</cell><cell>bp acad glob10 qe</cell><cell>TxtImg</cell><cell>QE</cell><cell cols="3">0.2502 0.3653 0.2833</cell><cell>0.2905</cell></row><row><cell>sztaki</cell><cell>bp acad glob10</cell><cell>TxtImg</cell><cell>NOFB</cell><cell cols="3">0.2497 0.3627 0.2780</cell><cell>0.2955</cell></row><row><cell>cwi</cell><cell>cwi lm lprior txt</cell><cell>Txt</cell><cell>NOFB</cell><cell cols="3">0.2493 0.3467 0.2787</cell><cell>0.2965</cell></row><row><cell>sztaki</cell><cell>bp acad avg5</cell><cell>TxtImg</cell><cell>NOFB</cell><cell cols="3">0.2491 0.3640 0.2773</cell><cell>0.2970</cell></row><row><cell>sztaki</cell><cell>bp acad avgw qe</cell><cell>TxtImg</cell><cell>QE</cell><cell cols="3">0.2465 0.3640 0.2780</cell><cell>0.2887</cell></row><row><cell>curien</cell><cell>LaHC run01</cell><cell>Txt</cell><cell>NOFB</cell><cell cols="3">0.2453 0.3680 0.2860</cell><cell>0.2905</cell></row><row><cell cols="2">ualicante IRnConcSinCamPrf</cell><cell>TxtCon</cell><cell>FB</cell><cell cols="3">0.2326 0.2840 0.2700</cell><cell>0.2673</cell></row><row><cell cols="2">ualicante IRnNoCamelPrf</cell><cell>Txt</cell><cell>FB</cell><cell cols="3">0.2321 0.3107 0.2800</cell><cell>0.2665</cell></row><row><cell cols="2">ualicante IRnNoCamelPrfGeog</cell><cell>Txt</cell><cell>FBQE</cell><cell cols="3">0.2287 0.3120 0.2787</cell><cell>0.2611</cell></row><row><cell cols="2">ualicante IRnConcSinCamPrfGeog</cell><cell>TxtCon</cell><cell>FBQE</cell><cell cols="3">0.2238 0.2853 0.2673</cell><cell>0.2561</cell></row><row><cell cols="2">chemnitz cut-mix-qe</cell><cell cols="2">TxtImgCon QE</cell><cell cols="3">0.2195 0.3627 0.2747</cell><cell>0.2734</cell></row><row><cell cols="2">ualicante IRnConcepto</cell><cell>TxtCon</cell><cell>NOFB</cell><cell cols="3">0.2183 0.3213 0.2520</cell><cell>0.2574</cell></row><row><cell cols="2">ualicante IRn</cell><cell>Txt</cell><cell>NOFB</cell><cell cols="3">0.2178 0.3760 0.2507</cell><cell>0.2569</cell></row><row><cell cols="2">chemnitz cut-txt-a</cell><cell>Txt</cell><cell>NOFB</cell><cell cols="3">0.2166 0.3440 0.2833</cell><cell>0.2695</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,121.25,594.04,360.49,106.57"><head>Table 5 :</head><label>5</label><figDesc>Results per modality over all topics.</figDesc><table coords="8,121.25,607.39,360.49,93.22"><row><cell>Modality</cell><cell>MAP Mean</cell><cell>SD</cell><cell>P@20 Mean</cell><cell>SD</cell><cell>R-prec. Mean SD</cell></row><row><cell>All top 75% runs</cell><cell cols="5">0.2176 0.045 0.2702 0.044 0.2595 0.046</cell></row><row><cell>Txt in top 75% runs</cell><cell cols="5">0.2137 0.051 0.2744 0.045 0.2544 0.052</cell></row><row><cell>Img in top 75% runs</cell><cell cols="3">0.1920 0.001 0.3227</cell><cell>0</cell><cell>0.2302 0.001</cell></row><row><cell>TxtCon in top 75% runs</cell><cell cols="5">0.2316 0.025 0.2770 0.036 0.2724 0.026</cell></row><row><cell>TxtImg in top 75% runs</cell><cell cols="5">0.2182 0.055 0.2437 0.047 0.2642 0.053</cell></row><row><cell cols="6">TxtImgCon in top 75% runs 0.2093 0.006 0.2770 0.003 0.2598 0.007</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,90.00,299.58,423.00,141.64"><head>Table 6 :</head><label>6</label><figDesc>Top 10 topics for which the textual-only runs outperform the visual-only and vice versa.</figDesc><table coords="9,175.28,312.53,252.44,128.69"><row><cell>visual</cell><cell>textual</cell></row><row><cell>(63) star galaxy</cell><cell>(61) Singapore</cell></row><row><cell>(67) bees with flowers</cell><cell>(31) bridges</cell></row><row><cell cols="2">(71) children riding bicycles (10) portrait of Jintao Hu</cell></row><row><cell>(59) dancing couple</cell><cell>(25) daily show</cell></row><row><cell>(13) DNA helix</cell><cell>(30) video games</cell></row><row><cell>(36) Eiffel tower</cell><cell>(24) peace anti-war protest</cell></row><row><cell>(57) people riding bicycles</cell><cell>(35) George W Bush</cell></row><row><cell>(41) ice hockey players</cell><cell>(66) historic castle</cell></row><row><cell>(4) white cat</cell><cell>(51) views of Scottish lochs</cell></row><row><cell>(7) spider web</cell><cell>(68) pyramids in Egypt</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p><rs type="person">Theodora Tsikrika</rs> was supported by the <rs type="funder">European Union</rs> via the <rs type="funder">European Commission</rs> project <rs type="projectName">VITALAS</rs> (contract no. <rs type="grantNumber">045389</rs>). <rs type="person">Jana Kludas</rs> was funded by the <rs type="funder">European</rs> project <rs type="projectName">MultiMATCH</rs> (<rs type="grantNumber">EU-IST-STREP</rs>#<rs type="grantNumber">033104</rs>). The authors would also like to thank <rs type="person">Thomas Deselaers</rs> for invaluable technical support and all the groups participating in the relevance assessment process.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_muru5pB">
					<idno type="grant-number">045389</idno>
					<orgName type="project" subtype="full">VITALAS</orgName>
				</org>
				<org type="funded-project" xml:id="_ChUUMxy">
					<idno type="grant-number">EU-IST-STREP</idno>
					<orgName type="project" subtype="full">MultiMATCH</orgName>
				</org>
				<org type="funding" xml:id="_ZDatwzs">
					<idno type="grant-number">033104</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,105.50,133.84,407.51,8.74;11,105.50,145.80,407.51,8.74;11,105.50,157.75,407.51,8.74;11,105.50,169.71,282.92,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,193.91,145.80,298.01,8.74">Robust scene categorization by learning image statistics in context</title>
		<author>
			<persName coords=""><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan-Mark</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cor</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,105.50,157.75,402.40,8.74">Proceedings of the 2006 Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>the 2006 Conference on Computer Vision and Pattern Recognition Workshop<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">105</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,105.50,189.63,407.50,8.74;11,105.50,201.59,407.50,8.74;11,105.50,213.54,107.34,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,287.56,189.63,220.92,8.74">On the creation of query topics for imageclefphoto</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,117.79,201.59,395.21,8.74;11,105.50,213.54,46.48,8.74">Proceedings of the Third MUSCLE / ImageCLEF Workshop on Image and Video Retrieval Evaluation</title>
		<meeting>the Third MUSCLE / ImageCLEF Workshop on Image and Video Retrieval Evaluation</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,105.50,233.47,407.51,8.74;11,105.50,245.42,407.51,8.74;11,105.50,257.38,407.51,8.74;11,105.50,269.33,243.61,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,187.09,245.42,325.91,8.74;11,105.50,257.38,47.05,8.74">The challenge problem for automated detection of 101 semantic concepts in multimedia</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcel</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan-Mark</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,174.69,257.38,333.64,8.74">Proceedings of the 14th annual ACM international conference on Multimedia</title>
		<meeting>the 14th annual ACM international conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,105.50,289.26,407.50,8.74;11,105.50,301.21,407.50,8.74;11,105.50,313.17,407.50,8.74;11,105.50,325.12,337.55,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,300.07,289.26,151.51,8.74">The INEX 2007 multimedia track</title>
		<author>
			<persName coords=""><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thijs</forename><surname>Westerveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,157.40,313.17,355.59,8.74;11,105.50,325.12,264.28,8.74">Proceedings of the 6th International Workshop of the Initiative for the Evaluation of XML Retrieval (INEX 2007), Revised and Selected Papers</title>
		<editor>
			<persName><forename type="first">Norbert</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mounia</forename><surname>Lalmas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Trotman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</editor>
		<meeting>the 6th International Workshop of the Initiative for the Evaluation of XML Retrieval (INEX 2007), Revised and Selected Papers</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Focused access to XML documents</note>
</biblStruct>

<biblStruct coords="11,105.50,345.05,407.50,8.74;11,105.50,357.00,407.50,8.74;11,105.50,368.96,407.51,8.74;11,105.50,380.91,366.52,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,282.17,345.05,146.90,8.74">The INEX 2006 multimedia track</title>
		<author>
			<persName coords=""><forename type="first">Thijs</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roelof</forename><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,312.93,357.00,200.07,8.74;11,105.50,368.96,407.51,8.74;11,105.50,380.91,55.96,8.74">Advances in XML Information Retrieval, Proceedings of the 5th International Workshop of the Initiative for the Evaluation of XML Retrieval (INEX 2006)</title>
		<title level="s" coord="11,169.61,380.91,100.69,8.74">Revised Selected Papers</title>
		<editor>
			<persName><forename type="first">Norbert</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mounia</forename><surname>Lalmas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Trotman</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4518</biblScope>
			<biblScope unit="page" from="331" to="344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
