<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,211.32,56.61,189.72,11.87">TAU MIPLAB at ImageClef 2008</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,223.32,70.59,30.32,8.50"><forename type="first">U</forename><surname>Avni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,261.48,70.59,51.12,8.50"><forename type="first">J</forename><surname>Goldberger</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bar-Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,334.08,70.59,51.72,8.50"><forename type="first">H</forename><surname>Greenspan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,211.32,56.61,189.72,11.87">TAU MIPLAB at ImageClef 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">18F947EDAE332D7FAAD9C812858AFE49</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of Tel Aviv University Medical Image Processing Laboratory group at the ImageClef 2008 medical retrieval and medical annotation tasks. In both tasks we have used the bag-of-words approach for image representation. We submitted two purely visual automatic runs to the medical retrieval task, which used different normalization in the feature extraction stage. Images were converted to a histogram of visual words, and were compared using L1 distance. Our best run was ranked first among the automatic visual based retrieval systems, with MAP score of 0.042. For the medical annotation task we submitted four runs, all used support-vector-machines trained on the visual word histograms. The runs differ in image resolution, and in the way classifiers of two resolutions were combined. In this task our result was second best among the participating groups, with error scores between 105.75 and 117.17.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the last several years, "patch-based" representations and "bag-of-features" classification techniques have been proposed for general object recognition tasks <ref type="bibr" coords="1,258.13,324.03,24.96,8.50">[1 -6]</ref>. In these approaches, a shift is made from the pixel entity to a "patch" -a small window centered on the pixel. In its most simplified form, raw pixel values (intensities) within the window are used as the components of the feature vector. It is possible to take the patch information as a collection of pixel values, or to shift the representation to a different set of features based on the pixels, such as SIFT features <ref type="bibr" coords="1,519.48,356.55,10.13,8.50" target="#b6">[7]</ref>, and reduce the dimensionality of the representation via dimensionality reduction techniques, such as principlecomponent analysis (PCA) <ref type="bibr" coords="1,183.12,378.15,10.04,8.50" target="#b7">[8]</ref>.</p><p>A very large set of patches are extracted from an image. Each small patch shows a localized "glimpse" at the image content; the collection of thousands and more such patches, randomly selected, have the capability to identify the entire image content (similar to a puzzle being formed from its pieces). A dictionary of words is learned over a large collection of patches, extracted from a large set of images. Once a global dictionary is learned, each image is represented as a collection of words (also known as a "bag of words", or "bag of features"), using an indexed histogram over the defined words. The matching between images, or between an image and an image class, can then be defined as a distance measure between the representative histograms. In categorizing an image as belonging to a certain image class, well-known classifiers, such as the k-nearest neighbor and support-vector machines (SVM) <ref type="bibr" coords="1,450.96,475.59,10.13,8.50" target="#b8">[9]</ref>, are used.</p><p>Patch-based methods have evolved from texton methods in texture analysis <ref type="bibr" coords="1,382.44,497.31,10.26,8.50" target="#b0">[1,</ref><ref type="bibr" coords="1,395.40,497.31,7.80,8.50" target="#b1">2]</ref> and were motivated from the text processing world <ref type="bibr" coords="1,151.68,508.11,10.13,8.50" target="#b2">[3]</ref>. In the classical bag-of-features approach, spatial information and geometrical relationship between patches is lost. Recent works have shown that including the spatial information as additional features per patch may provide additional mage characterization strength. The patch-based, bag-of-features approach is simple, computationally efficient, and shows robustness to occlusions and spatial variations. Using this approach, a substantial increase in performance capabilities in general computer-vision object and scene classification tasks has been demonstrated [e.g., <ref type="bibr" coords="1,158.16,562.23,6.90,8.50" target="#b3">4,</ref><ref type="bibr" coords="1,169.80,562.23,6.84,8.50" target="#b4">5]</ref>. Motivated by these works, and the by success of works based on similar approach in ImageClef2007 challenges <ref type="bibr" coords="1,182.52,573.15,15.06,8.50" target="#b9">[10,</ref><ref type="bibr" coords="1,200.04,573.15,12.48,8.50" target="#b10">11]</ref> we have developed a retrieval and classification system for large medical databases, and put it to the test in ImageClefMed 2008 tasks.</p><p>In this task we are presented with 30 query topics, each with one or more example images, and a short textual description in several languages. Our objective is to return a ranked set of images from a database of over 66,000 images, sorted by their relevance to the presented queries. Some query examples are seen in Figure <ref type="figure" coords="2,486.24,111.75,3.51,8.50">4</ref>. We have submitted two purely visual automatic runs to this task: TAU_norm and TAU_orig. The two runs differ in the normalization process in the feature extraction stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Method</head><p>We model an image as a collection of local patches, where a patch is a small rectangular sub region of the image. Each patch is represented as a codeword index out of a finite vocabulary of visual codewords. Images are compared and classified based on this discrete and compact representation.</p><p>We selected a random set of 400 images from the database, and sampled patches of a fixed size of 9x9 pixels with a grid of 6 pixels spacing. We then computed a covariance matrix of this set of roughly 2,000,000 patches, and applied PCA to find its eigenvectors. The 6 vectors with the highest energy are shown in Figure <ref type="figure" coords="2,417.00,227.91,3.57,8.50" target="#fig_0">2</ref>. These eigenvectors are later used as a base for the rest of the patches in the database. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Feature extraction</head><p>In TAU_norm run, sampled patches are normalized to have 0 mean and 1 variance. This procedure gives some invariance to lighting and contrast. The normalized patch gray level is dimensionally reduced to 6 features using the basis vectors calculated previously. Patch mean gray level was lost in the normalization and PCA process; hence it is added as an additional seventh feature.</p><p>In TAU_orig, sampled patches undergo PCA dimensionality reduction to 7 features, without the initial normalization step. In this case the mean gray level is intrinsically included as the first PCA component.</p><p>In both runs, in order to preserve spatial information, the patch center coordinates were added as two additional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Dictionary</head><p>A dictionary of visual words is built using k-means clustering algorithm on the set of sampled patches. The k-means is performed in the feature space, using the L2 distance. Clusters centers initialized by an iterative process, that selects the patch which is furthest from previously selected centers as a new cluster center. Figure <ref type="figure" coords="2,427.44,526.11,4.68,8.50">3</ref> shows the centers retuned from the clustering process, after the patches were converted back to image space, and placed in their x,y coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3: Dictionary layout</head><p>Once a dictionary is ready, we sample every image in the database, and represent the image as an indexed histogram of visual words. In this step images are sampled with a denser grid, using a spacing of 2 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Query process</head><p>For image comparison distance measures between the representative histograms are used. Retrieved images are ranked by the distance between a target histogram and the histogram of the query image. When there are several query images we use the minimal distance between the target and the query set. Experiments on the ImageClef medical annotation challenge database indicated that L1 is a simple and effective distance measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experiments and Results</head><p>The runs were ranked according to mean average precision (MAP), which is the arithmetic mean of average precision (AP) values over the 30 individual topics. AP is calculated by averaging the precision in the top n retrieved images, where the values of n are taken after each relevant image is returned. In addition bpref score <ref type="bibr" coords="3,465.00,458.55,15.72,8.50" target="#b11">[12]</ref> is calculated, and precision scores using 5, 10, 15, 20 and 30 retrieved images.</p><p>Results of our two submitted runs are shown in Table <ref type="table" coords="3,305.76,491.07,3.51,8.50" target="#tab_0">1</ref>. TAU_norm run ranked first among the automatic visual retrieval runs, according to MAP, bpref, P5, P10, P15 and P20 scores, and second according to P30 score. TAU_orig run ranked 4 th according to MAP score, indicating that normalizing the patch variance improves the retrieved results. Using a system that is entirely visual based gives quantitative results which are overall much reduced as compared to text-based systems and mixed runs. This can be seen in the systems comparative table in which all visual based systems are ranked last (provided by the competition organizers). Using the MAP score, the proposed system above is ranked 95 out of 113 total runs. The system is computationally efficient, with average retrieval time of less than 400ms per query on a dual quad-core Intel Xeon 2.33 GHz.</p><p>It is interesting to note that on several query topics the proposed system proved highly accurate: On visual topics number 6 and 7 the TAU_norm system ranked 3 rd and 17 th , respectively. On the mixed topics number 15 and 16 it ranked 1 st and 3 rd , respectively, among the entire runs. These query topics are displayed in Fig. <ref type="figure" coords="4,438.96,77.19,3.45,8.50">4</ref>, and the retrieval result returned by TAU_norm for query 15 is displayed in Fig. <ref type="figure" coords="4,294.12,87.99,3.45,8.50">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 4: Query topics on which our system was effective</head><p>The topics in Figure <ref type="figure" coords="4,173.16,499.11,4.68,8.50">4</ref> display body parts with distinct visual features. Our system performed well on these queries because its parameters were tuned using the ImageClef medical annotation challenge database, and as such it specialized in identifying body parts in x-ray images.</p><p>16. Show me all x-ray images containing one or more fractures.</p><p>15. Show me chest x-ray images of cases with tuberculosis 7. Show me images of a knee x-ray 6. Show me images of a frontal head MRI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5: Top 15 retrieved images, for topic #15. Top left image is the query sample image. On this query topic</head><p>TAU_norm run had the highest MAP score from the entire submissions, textual, visual, or mixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Medical Image Annotation</head><p>In this task we are presented with 12,089 classified x-ray images, and our aim is to classify a set of 1000 previously unseen images, using the hierarchical IRMA code. We submitted four runs to this task, based on the same bag-of-words image representation presented in the previous section, with support-vector-machine classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Method</head><p>For the annotation challenge we used a dense sampling grid, a patch was extracted around every pixel. The dictionary size in this section was larger, with 700 visual words. Feature extraction, dictionary building and image representation was carried out as in the TAU_norn retrieval run, described in sections 2.1.1 and 2.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Run Descriptions</head><p>Four different runs were provided, as described next. In svm_full a support vector machine classifier is trained directly on the image word histograms, using one-vs-one technique for multi-class classification, with radial basis function kernel. Each IRMA code in the training set is treated as a different category label.</p><p>In svm_small the same method as in svm_full is applied to a 4 times scaled down version of the image, while the patch size remains 9x9 pixels.</p><p>In svm_vote the full scale and 1/4 scale classifiers are merged by summing up the votes for each category as was returned from the one-vs-one SVMs.</p><p>In svm_prob we calculate a probabilistic output of the SVM classifiers using <ref type="bibr" coords="5,396.84,686.19,14.54,8.50" target="#b12">[13]</ref>, and multiply the probabilities from the full scale and 1/4 scale classifiers. The categories are then sorted by their combined probability. We finally return an IRMA code from the entire hierarchy that minimizes the expected error score on the 5 most probable categories. When the most probable categories have comparable probabilities, the IRMA code returned using this method is often partial, with '*' replacing letters at some location along the IRMA code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments and Results</head><p>As in ImageCLEF 2007, the medical automatic annotation task rated the classification success according to a penalty score that takes into account the hierarchical structure of the IRMA code-the penalty is greater for errors made in higher levels of the hierarchy.</p><p>The results of our runs are shown in Table <ref type="table" coords="6,257.16,160.83,3.45,8.50" target="#tab_1">2</ref>. Our four submissions ranked in places 7 to 10 out of the 24 submitted runs, with error scored between 105.75 and 117.17. This result trails only runs from the Idiap research institute group, which submitted the first 6 best runs, and had the best results last year <ref type="bibr" coords="6,345.24,182.43,14.54,8.50" target="#b9">[10]</ref>. Our top ranking runs, svm_full and svm_prob, had a very close score, although they are defined quite differently. The errors in svm_full were due to misclassifications at some point in the IRMA code, where in svm_prob the errors where often due to partial classification.</p><p>In this challenge there was a slight advantage to svm_full submission over the rest of the runs, meaning that in our case combining the output of a lower resolution classifier didn't improve the error score.</p><p>The total running time for the whole system, training and classification, was approximately 40 minutes on the full resolution images, and 3 minutes on the 1/4 scaled down images. Times were measured on dual quad-core Intel Xeon 2.33 GHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Summary</head><p>We presented an image retrieval and classification system for large medical databases, based on compact bag-offeatures image representation. The system achieves comparatively good results in the ImageClefMed 2008 challenges, while maintaining efficient computation times. These qualities enable effective scaling to larger image collections.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,259.56,340.43,93.04,8.45;2,284.76,252.72,80.28,80.64"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: PCA components</figDesc><graphic coords="2,284.76,252.72,80.28,80.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,104.04,536.51,404.29,72.49"><head>Table 1 :</head><label>1</label><figDesc>Ranking of submitted medical image retrieval runs</figDesc><table coords="3,104.04,562.18,404.29,46.83"><row><cell>Rank</cell><cell>Run</cell><cell>MAP bpref P5</cell><cell>P10 P15 P20 P30</cell></row><row><cell>(purely visual)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="3">TAU_MIPLAB-TAU_norm 0.042 0.094 0.220 0.170 0.169 0.162 0.146</cell></row><row><cell>4</cell><cell>TAU_MIPLAB-TAU_orig</cell><cell cols="2">0.031 0.077 0.160 0.143 0.133 0.123 0.112</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,188.40,219.23,235.25,86.29"><head>Table 2 :</head><label>2</label><figDesc>Ranking of submitted medical image annotation runs</figDesc><table coords="6,191.88,244.90,228.36,60.63"><row><cell>Rank</cell><cell>Run</cell><cell>Error score</cell></row><row><cell>7</cell><cell>TAU-BIOMED-svm_full</cell><cell>105.75</cell></row><row><cell>8</cell><cell>TAU-BIOMED-svm_prob</cell><cell>105.86</cell></row><row><cell>9</cell><cell>TAU-BIOMED-svm_vote</cell><cell>109.37</cell></row><row><cell>10</cell><cell>TAU-BIOMED-svm_small</cell><cell>117.17</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,91.35,81.51,420.09,8.50;7,104.76,92.31,155.22,8.50" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,221.52,81.51,289.92,8.50;7,104.76,92.31,74.12,8.50">Representing and recognizing the visual appearance of materials using threedimensional textons</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,185.04,92.35,17.33,8.37">IJCV</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.35,114.03,439.29,8.50;7,104.76,124.71,16.38,8.50" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,240.72,114.03,185.72,8.50">Texture classification: are filter banks necessary?</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,439.20,114.07,30.19,8.37">CVPR</title>
		<imprint>
			<biblScope unit="volume">03</biblScope>
			<biblScope unit="page" from="691" to="698" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.35,146.43,426.53,8.50;7,104.76,157.35,212.82,8.50" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,236.62,146.43,272.99,8.50">Video Google: A Text Retrieval Approach to Object Matching in Videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,104.76,157.39,149.32,8.37">Proc. Ninth Int&apos;l Conf. Computer Vision</title>
		<meeting>Ninth Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1470" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.35,178.95,440.57,8.50;7,104.76,189.75,220.38,8.50" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,229.80,178.95,265.59,8.50">A Bayesian Hierarchical Model for Learning Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,501.60,178.99,30.32,8.37;7,104.76,189.79,180.28,8.37">Proc. of IEEE Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="524" to="531" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.35,211.47,415.23,8.50" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,192.96,211.47,223.28,8.50">Sampling strategies for bag-of-features image classification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,432.36,211.51,24.08,8.37">ECCV</title>
		<imprint>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page" from="406" to="503" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.35,233.07,437.33,8.50;7,104.76,243.99,137.88,8.50" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,258.60,233.07,270.08,8.50;7,104.76,243.99,53.56,8.50">Towards optimal bag-of-features for object categorization and semantic video retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C-W &amp;</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,164.16,244.03,20.84,8.37">CIVR</title>
		<imprint>
			<biblScope unit="volume">2007</biblScope>
			<biblScope unit="page" from="494" to="501" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.35,265.59,412.59,8.50" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,169.92,265.59,200.25,8.50">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="middle">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,386.28,265.63,42.10,8.37">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.35,287.19,355.47,8.50" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,189.96,287.23,154.23,8.37">Neural Networks for Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.35,308.79,357.03,8.50" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,178.56,308.79,156.19,8.50">The Nature of Statistical Learning Theory</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,95.56,330.51,402.76,8.50;7,104.76,341.31,348.42,8.50" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,284.88,330.51,213.44,8.50;7,104.76,341.31,78.72,8.50">CLEF2007 Image Annotation Task: an SVM-based Cue Integration Approach</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,200.52,341.31,170.91,8.50">Working Notes of the 2007 CLEF Work-shop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,95.56,363.03,414.56,8.50;7,104.76,373.83,288.42,8.50" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,207.00,363.03,259.20,8.50">Sparse patch-histograms for object classification in cluttered images</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,483.00,363.07,27.12,8.37;7,104.76,373.83,16.95,8.50">DAGM 2006</title>
		<title level="s" coord="7,130.68,373.87,131.78,8.37">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4174</biblScope>
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,95.56,395.55,409.28,8.50" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,241.32,395.55,184.57,8.50">Retrieval evaluation with incomplete information</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,432.12,395.55,44.15,8.50">SIGIR 2004</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,95.56,417.15,429.02,8.50;7,104.76,428.07,133.38,8.50" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,263.52,417.15,256.75,8.50">A Note on Platt&apos;s Probabilistic Outputs for Support Vector Machines</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,104.76,428.11,67.97,8.37">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
