<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.68,151.79,309.75,12.58">Clustering for Photo Retrieval at Image CLEF 2008</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,162.36,190.14,52.38,9.02"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Engineering</orgName>
								<orgName type="institution">University of Ottawa</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,221.52,190.14,57.32,9.02"><forename type="first">Marc</forename><surname>Stogaitis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Engineering</orgName>
								<orgName type="institution">University of Ottawa</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.09,190.14,69.30,9.02"><forename type="first">François</forename><surname>Deguire</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Engineering</orgName>
								<orgName type="institution">University of Ottawa</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,378.49,190.14,65.82,9.02"><forename type="first">Muath</forename><surname>Alzghool</surname></persName>
							<email>alzghool@site.uottawa.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Engineering</orgName>
								<orgName type="institution">University of Ottawa</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.68,151.79,309.75,12.58">Clustering for Photo Retrieval at Image CLEF 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">074DDF8BFFD5003AC124ADE3D03AB8EC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information retrieval</term>
					<term>image retrieval</term>
					<term>photographs</term>
					<term>text retrieval</term>
					<term>kmeans clustering</term>
					<term>agglomerative clustering</term>
					<term>WordNet-based clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the first participation of the University of Ottawa group in the Photo Retrieval task at Image CLEF 2008. Our system uses the following components: Lucene for text indexing and LIRE for image indexing. We experiment with several clustering methods in order to retrieve images from diverse clusters. The clustering methods are: k-means clustering, hierarchical clustering, and our own method based on WordNet. We present results for thirteen submitted runs, in order to compare retrieval based on text description, to image-only retrieval, and to merged retrieval, and to compare results for the different clustering methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents the first participation of the University of Ottawa group in the photo retrieval track at Image CLEF 2008. This year's task focused on clustering images in order to retrieve images from different clusters. We present our system, followed by results for the submitted runs. We worked only with the English part of the collection (English captions and queries). The research questions that we are investigating include: what happens if we index only the text captions, only the images, or the captions and the images; what is the performance of the system with and without clustering. We investigate different types of clustering. First, the k-means clustering algorithm, then hierarchical clustering in three variants: based on average link similarity, complete link, and single link. Then we try our own clustering method, based on searching words from the query and from the text caption in the WordNet lexical knowledge base <ref type="bibr" coords="1,220.66,619.92,10.61,9.02" target="#b0">[1]</ref>. We present four versions of this algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>The University of Ottawa's Image Retrieval system was built with off-the-shelf components. For text retrieval we used Lucene<ref type="foot" coords="2,312.12,188.56,3.00,5.40" target="#foot_0">1</ref> and for image retrieval we use LIRE <ref type="foot" coords="2,463.68,188.56,3.00,5.40" target="#foot_1">2</ref> .</p><p>Lucene is an open-source text search engine that we have used in order to match the text captions of the queries with text description of the images from the collection. This tool provided us with a wide variety of options that we were able to use to quickly create our index and, later on, to perform the appropriate queries on it. It also provided us with parsers that we used in order to automatically remove the stopwords from the provided queries. We added our own parsing component that removed phrases from the narrative fields such as: "the relevant images should not include …". LIRE is the tool that we have used in order to perform the content-based image search. The main advantage of this tool for us is that it is compatible with Lucene, thus providing a good solution for the problem at hand. Lucene and LIRE were designed to work together. Furthermore, these tools are also fairly simple to use, also providing straightforward methods for indexing and searching.</p><p>We have used a data fusion technique to merge the text retrieval with the image retrieval based on the method proposed in <ref type="bibr" coords="2,296.94,372.66,10.60,9.02" target="#b2">[3]</ref>. Their method, called combMNZ, sums up all the scores of a document multiplied by the number of non-zero scores of the document, as in formula 1:</p><p>(1)</p><formula xml:id="formula_0" coords="2,149.82,414.09,131.16,27.43">∑ ∈ * = schemes IR i i n score combMNZ</formula><p>where score i is the similarity score of the document for the indexing technique used to retrieve this document (text or image indexing), and n is the number of non-zero scores of the document.</p><p>Since there are there are two indexing techniques based on text and image retrieval from different systems, these systems will generate different ranges of similarity scores, so it is necessary to normalize the similarity scores of the document. Lee <ref type="bibr" coords="2,459.00,522.54,11.70,9.02" target="#b1">[2]</ref> proposed a normalization method by utilizing the maximum and minimum scores for each weighting scheme as defined by formula 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MinScore MaxScore</head><formula xml:id="formula_1" coords="2,163.38,565.65,241.54,30.15">MinScore score Score Normalized - - = (2)</formula><p>We have adapted combMNZ to carry a weight for each indexing technique. Our data fusion model uses a fusion formula that we call WCombMNZ represented by formula 3.</p><formula xml:id="formula_2" coords="3,136.74,156.69,245.05,27.43">∑ ∈ * = schemes indexing i i i n Score Normalized W WCombMNZ * (3)</formula><p>where W i is a predefinded weight associated with each indexing technique's results, n is the number of non-zero scores of the document, and the NormalizedScore i is calculated by formula 2 as described before.</p><p>We have tried different weights (W i ) for each indexing scheme, as shown in the next section, usually giving more weight to the text retrieval.</p><p>We added a clustering component that clusters the text captions. Our text clustering component was implemented with the use of the Dragon<ref type="foot" coords="3,396.42,287.86,3.00,5.40" target="#foot_2">3</ref> Toolkit. Our text clustering works as follows: using the Dragon Toolkit, a second index of all image annotations, indexed by the doc number field, was created. When performing a clustering operation during a search, the program goes through the Lucene results, extracts their doc number, and uses them to generate a list of Dragon Toolkit documents. The Dragon Toolkit algorithm is then called, which clusters the data based on its own index (the second index described above).</p><p>The way we integrated the results of the clustering component into the system was by post processing the search results for the test queries. After the clusters were formed, the results were re-ranked so that as many as possible distinct clusters were retrieved in the first 20 results, keeping only the first image from each cluster. By the first image from a cluster we mean the image with the highest similarity score. The other images from the same cluster were re-arranged below the limit of first 20, in the order of their scores.</p><p>Our clustering component used the k-means clustering algorithm, a hierarchical agglomerative clustering algorithm, and a new clustering method that we designed based on WordNet search.</p><p>The WordNet-based runs work as follows. The algorithm takes as input the ranked results list that was created by our standard text/image search. It then cycles through each word of the first document, looking for words that match the current clustering criteria. For example, if the query indicates that we should cluster based on animals, then the algorithm will try to spot a word in the document that it recognizes as an animal (for example, the word "dog"). This is accomplished by performing a recursive search up the WordNet semantic relation called hypernym ("is a" relation). We also included the "is an instance of" relation together with the hypermyms. Here is an example of this process: suppose we have the sentence "Run fast fox and don't look back". Also assume that our clustering criterion is "animal". The algorithm would first take the word "run" and, for each of the possible word senses, recursively look through its hypernym ancestry to see if it can find the word "animal". In this case, it would not find it. It would then move on to the next word, "fast". After not finding it for this word, it would move on to the word "fox". In this case, it would find "animal" somewhere in its hypernym tree and would therefore be able to return the word "fox" as being related to the cluster. Once the algorithm has identified the relevant word which describes the current document in relation to the cluster, it then checks if we have already entered a result in the top 20 results with that word. If we have not, it adds this result to the current list of top results. If it is already in the list, it holds on to this result and will eventually append it lower in the list (after the first 20 results). The performance of the algorithm seems pretty good. It is particularly good at detecting locations, countries, states etc. To help enhance this capability, the algorithm looks at the &lt;location&gt; field of the documents first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>Table <ref type="table" coords="4,150.88,283.98,5.01,9.02" target="#tab_0">1</ref> shows the results of the thirteen submitted results on the test topics/queries. The evaluation measure we report are standard measures computed with the trec_eval script: MAP (Mean Average Precision) and the precisions at 20 documents. In addition, we report the number of distinct clusters included in the first 20 documents, which was the main focus of this year's task.</p><p>Next, we describe the first 9 runs.</p><p>Run1 is a text only search followed by re-arranging using the k-means clustering algorithm.</p><p>Run2 merged the results of four different retrieval runs, one with text only and one for each of the three sample images (similarity with that image). The results were merged with the following weights: 70% for the text retrieval, and 10% for each image retrieval.</p><p>Run3 is the same as Run2, but with the following weights: 85% for the text retrieval, and 5% for each image retrieval.</p><p>Run4 is the same as Run3 but with the following weights: 55% for the text retrieval, and 15% for each image retrieval.</p><p>Run5 was an image only run. It was done by assigning the following weights: 0% for the text retrieval, and 33% for each image retrieval.</p><p>Run6 is a run in which the clustering step was disabled so that the order that we present is the same as from a text &amp; image retrieval only run. The weights that were used are: 55% for the text retrieval, and 15% for each image retrieval.</p><p>Run7 uses the Hierarchical Clustering algorithm instead of the K-Means. It uses Average Linkage to measure similarity. The weights that were used are: 55% for the text retrieval, and 15% for each image retrieval.</p><p>Run8 uses the Hierarchical Clustering algorithm instead of the K-Means. It uses Complete Linkage to measure similarity. The weights that were used are: 55% for the text retrieval, and 15% for each image retrieval.</p><p>Run9 uses the Hierarchical Clustering algorithm instead of the K-Means. It uses Single Linkage to measure similarity. The weights that were used are: 55% for the text retrieval, 15% for each image retrieval.</p><p>Three more runs use a different way of re-arranging the data that is based on WordNet, as explained at the end of the previous section. Below is an explanation of the next three runs that we performed using our WordNet-based clustering algorithm.</p><p>Run10 was a basic WordNet run where the algorithm functions as mentioned above.</p><p>Run11 was the same as Run10, except that instead of using the clustering field from the query directly, we process it a bit (basically, we only take the first word of the clustering criterion). This helped fix a few exceptions that were found in the clustering queries, such as a cluster called "vehicle type", which would not be recognized by WordNet. WordNet will however recognize vehicle and should be able to cluster properly with that term.</p><p>Run12 is the same as Run11 with the added constraint that we only take the first 3 definitions of a word sense when looking for hypernym links. This helped fixed problems with things like the word "blue" being recognized as an animal since its 7th word sense according to WordNet is a "butterfly".</p><p>In the previous algorithm, there were cases when none of the terms of a document were found to match the category, according to WordNet. In the previous runs, we just kept the result where it was. However, another option was to only put results that had matches in WordNet in the 20 first documents. Run 13 tries this second option. Therefore, Run13 is similar to Run12 except that we now make sure that the first 20 results returned contain words that are found to match the clustering criterion according to WordNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion of the results</head><p>The first five runs used the k-means clustering algorithm, Run1 used only the text, and Run5 used only the images. We can see that using only the images the performance is the lowest. Text-only retrieval was pretty good, and using a combination of text and image retrieval brings a small improvement over text-only. Among the first 5 runs, the best weights for merging text and image retrieval results were for Run4, namely 55% for the text retrieval, and 15% for each image retrieval. These weights for merging results were kept for the remaining runs, with good results.</p><p>Run6, with no clustering achieved the best P@20 and MAP core, and a score of 0.3970 from CR@20. Adding clustering we reduces P@20 and MAP score, but some of the methods improved the number of distinct clusters retrieved in the first 20.</p><p>The four WordNet-based clustering methods worked best in terms of retrieving many relevant clusters, especially the version from Run 10, which archived 0.4102 for CR@20. The hierarchical clustering was the worst, especially the complete link and single link.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Our system used merged results from Lucene for text retrieval and Lire for images search. We incorporated a clustering component. We experimented with the k-means algorithm, agglomerative clustering, and a WordNet-based clustering algorithm. Our experiments showed that text retrieval works well, and adding image similarity brings a bit of improvement. In terms of retrieving many different clusters, our WordNet-based algorithm worked best.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,124.74,149.43,346.01,260.22"><head>Table 1 .</head><label>1</label><figDesc>Results of the thirteen submitted runs. All of them are for English queries and English captions of the images (EN-EN). They are all automatic, no manual feedback was involved (AUTO). TXT indicates that the image annotation text was used during the search. IMG indicates that the images themselves were used during the search. The names of the runs also contain a code for the clustering algorithm (KM for k-means clustering; HA -hierarchical clustering, average linkage; HC -hierarchical clustering, complete linkage; HS -hierarchical clustering, single linkage; WN -WordNet-based clustering; NO -no clustering).</figDesc><table coords="6,134.70,240.72,314.10,168.94"><row><cell>Run name</cell><cell>CR@20</cell><cell>P@20</cell><cell>MAP</cell></row><row><cell>UOt01_EN_EN_AUTO_TXT_KM</cell><cell>0.3684</cell><cell>0.3333</cell><cell>0.2314</cell></row><row><cell>UOt02_EN_EN_AUTO_TXTIMG_KM</cell><cell>0.3767</cell><cell>0.3474</cell><cell>0.2429</cell></row><row><cell>UOt03_EN_EN_AUTO_TXTIMG_KM</cell><cell>0.3716</cell><cell>0.3436</cell><cell>0.2384</cell></row><row><cell>UOt04_EN_EN_AUTO_TXTIMG_KM</cell><cell>0.3970</cell><cell>0.3615</cell><cell>0.2479</cell></row><row><cell>UOt05_EN_EN_AUTO_IMG_KM</cell><cell>0.2694</cell><cell>0.1590</cell><cell>0.0693</cell></row><row><cell>UOt06_EN_EN_AUTO_TXTIMG_NO</cell><cell>0.3970</cell><cell>0.3654</cell><cell>0.2490</cell></row><row><cell>UOt07_EN_EN_AUTO_TXTIMG_HA</cell><cell>0.1488</cell><cell>0.0705</cell><cell>0.1665</cell></row><row><cell>UOt08_EN_EN_AUTO_TXTIMG_HC</cell><cell>0.3149</cell><cell>0.1333</cell><cell>0.1869</cell></row><row><cell>UOt09_EN_EN_AUTO_TXTIMG_HS</cell><cell>0.1204</cell><cell>0.0590</cell><cell>0.1625</cell></row><row><cell>UOt10_EN_EN_AUTO_TXTIMG_WN</cell><cell>0.4102</cell><cell>0.2756</cell><cell>0.1382</cell></row><row><cell>UOt11_EN_EN_AUTO_TXTIMG_WN</cell><cell>0.4038</cell><cell>0.2679</cell><cell>0.1391</cell></row><row><cell>UOt12_EN_EN_AUTO_TXTIMG_WN</cell><cell>0.4027</cell><cell>0.2705</cell><cell>0.1372</cell></row><row><cell>UOt13_EN_EN_AUTO_TXTIMG_WN</cell><cell>0.4069</cell><cell>0.2026</cell><cell>0.1965</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,130.26,665.37,252.19,8.10"><p>Lucene text search engine library, http://lucene.apache.org/java/docs/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,130.26,675.93,219.99,8.10;2,124.74,686.25,138.05,8.10"><p>LIRE open source java content-based image retrieval library, http://www.semanticmetadata.net/lire/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,130.02,686.25,220.52,8.10"><p>Dragon Toolkit http://www.dragontoolkit.org/textcluster.asp</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,128.12,299.91,342.41,8.10;7,142.74,310.29,18.00,8.10" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,246.57,299.91,157.00,8.10">WordNet, An Electronic Lexical Database</title>
		<editor>Fellbaum, Christiane</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.12,320.61,342.51,8.10;7,142.74,330.99,327.91,8.10;7,142.74,341.31,327.85,8.10;7,142.74,351.63,123.59,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,207.38,320.61,263.25,8.10;7,142.74,330.99,28.67,8.10">Combining multiple evidence from different properties of weighting schemes</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,188.54,330.99,282.10,8.10;7,142.74,341.31,245.33,8.10">Proceedings of the Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the the 18th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>Seattle, Washington, United States</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995">1995. 1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.12,362.01,342.60,8.10;7,142.74,372.33,192.85,8.10" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,264.21,362.01,125.34,8.10">Combination of Multiple Searches</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology Special Publication</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
