<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,111.73,148.86,379.53,15.15;1,182.35,170.78,238.30,15.15">CLEF2008 Image Annotation Task: an SVM Confidence-Based Approach</title>
				<funder ref="#_jUQczCR">
					<orgName type="full">Hasler Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Blanceflor Boncompagni Ludovisi Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,170.35,204.67,73.90,8.74"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
							<email>ttommasi@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Institute Centre</orgName>
								<address>
									<addrLine>Du Parc, Rue Marconi 19</addrLine>
									<postBox>P. O. Box 592</postBox>
									<postCode>CH-1920</postCode>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.87,204.67,81.24,8.74"><forename type="first">Francesco</forename><surname>Orabona</surname></persName>
							<email>forabona@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Institute Centre</orgName>
								<address>
									<addrLine>Du Parc, Rue Marconi 19</addrLine>
									<postBox>P. O. Box 592</postBox>
									<postCode>CH-1920</postCode>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.88,204.67,70.76,8.74"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
							<email>bcaputo@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Institute Centre</orgName>
								<address>
									<addrLine>Du Parc, Rue Marconi 19</addrLine>
									<postBox>P. O. Box 592</postBox>
									<postCode>CH-1920</postCode>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,111.73,148.86,379.53,15.15;1,182.35,170.78,238.30,15.15">CLEF2008 Image Annotation Task: an SVM Confidence-Based Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9460C8A2671C6493224892A79DFD3AA0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Automatic Image Annotation, Cue Integration, Confidence Estimation, Virtual Examples, Support Vector Machines, Kernel Methods</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the algorithms and results of our participation to the medical image annotation task of ImageCLEFmed 2008. Our previous experience in the same task in 2007 suggests that combining multiple cues with different SVM-based approaches is very effective in this domain. Moreover it points out that local features are the most discriminative cues for the problem at hand. On these basis we decided to integrate two different local structural and textural descriptors. Cues are combined through simple concatenation of the feature vectors and through the Multi-Cue Kernel. The trickiest part of the challenge this year was annotating images coming mainly from classes with only few examples in the training set. We tackled the problem on two fronts: (1) we introduced a further integration strategy using SVM as an opinion maker. It consists in combining the first two opinions on the basis of a technique to evaluate the confidence of the classifier's decisions. This approach produces class labels with "don't know" wildcards opportunely placed; (2) we enriched the poorly populated training classes adding virtual examples generated slightly modifying the original images. We submitted several runs considering different combination of the proposed techniques. Our team was called "idiap". The run using jointly the low cueintegration technique, the confidence-based opinion fusion and the virtual examples, scored 74.92 ranking first among all submissions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The rapid development of new medical image acquisition techniques and the widespread of computerized equipment to save transfer and store medical imagery in digital format have led to the need for new methods to manage and archive this data. Average-sized radiology departments produce nowadays several tera-bytes of data annually. Automatic image annotation systems turn out to be important tools to manage big databases, in avoiding manual classification errors and helping in image retrieval. The ImageCLEFmed challenge is an international event which gives the possibility to different research groups to benchmark their image annotation approaches. The aim is to find out how well current techniques can identify image modality, body orientation, body region and biological system examined based on the images.</p><p>In 2008 the ImageCLEFmed annotation task provided participants with 12076 x-ray images as training data spread across 197 classes. The task consisted in assigning the correct label to 1000 test images. To recognize these images, an automatic annotation system have to face two major problems: the intra-class variability vs inter-class similarity, and the data unbalance. The first problem arises from the fact that images belonging to the same visual class might look very different while images that belong to different visual classes might look very similar. The second one is connected to the natural disposition of organs in the human body and the frequency of diseases, which causes that some parts of the body are more likely to be object of image acquisition. The ImageCLEFmed organizers decided to focus on this second problem introducing in the training set 82 classes with a maximum of 6 images each and preparing a test set mainly with images from this low populated classes.</p><p>This paper describes the algorithms submitted by the "idiap" team as its second participation to the CLEF benchmark competition <ref type="foot" coords="2,249.35,383.33,3.97,6.12" target="#foot_0">1</ref> . Last year we proposed different cue-integration approaches based on Support Vector Machine (SVM, <ref type="bibr" coords="2,270.58,396.85,10.29,8.74" target="#b1">[2]</ref>), using global and local features. They proved robust and able to tackle the inter-vs-intra class variability problem. Our run based on the use of the Multi-Cue Kernel (MCK, <ref type="bibr" coords="2,201.88,420.76,15.50,8.74" target="#b17">[18]</ref>) ranked first in 2007. After the competition we compared the results obtained by MCK with a scheme consistent in concatenating the different feature vectors. Results showed that the two methods do not produce significatively different results <ref type="bibr" coords="2,424.94,444.68,14.61,8.74" target="#b17">[18]</ref>.</p><p>This year we decided to reuse both the above described methods changing the selected features into two different types of local descriptors: Scale Invariant Feature Transform (SIFT, <ref type="bibr" coords="2,478.52,468.59,10.80,8.74" target="#b7">[8]</ref>) and Local Binary Pattern (LBP, <ref type="bibr" coords="2,221.64,480.54,14.76,8.74" target="#b10">[11]</ref>). We also propose two strategies to tackle the unbalancing problem. On one hand we explore a technique to estimate the confidence of the classifier's decision and when it is not considered reliable, a soft decision is made using SVM as an opinion maker and combining its first two opinions to produce a less specific label. This approach was derived from the label hierarchical structure and the possibility to insert a "don't know" in some point in it. On the other hand we created examples for the classes with few images to enrich them. The new images were produced as slightly modified copies of the original ones through translation, rotation and brightness changes.</p><p>We submitted several runs, the results show that the classification performance increases passing from the use of a single cue (idiap-LBP score 128.58; idiap-SIFTnew score 100.27) to that of multiple cues (LOW lbp siftnew score 93.20), from the use of a hard decision (idiap-MCK pix sift score 313.01; LOW lbp siftnew score 93.20) to a soft decision through confidence based opinion fusion (idiap-MCK pix sift 2MARG score 227.82; LOW 2MARG score 83.79) and gets even better adding virtual examples in low populated classes (idiap-LOW MULT 2MARG score 74.92).</p><p>The rest of the paper is organized as follows: section 2 describes the feature extracted from images and the two methods used to combine them. Section 3 gives details on the confidence based opinion fusion, while section 4 explains how we multiplied images to create virtual examples. Section 5 reports the experimental procedure adopted and the results obtained. Conclusions and outlook are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Cue Integration</head><p>The aim of the automatic image annotation task is to classify images into a set of classes based on the hierarchical IRMA code <ref type="bibr" coords="3,234.99,145.80,9.96,8.74" target="#b6">[7]</ref>. This code distinguishes images along the modality, body orientation, body region and biological system axis and errors in the annotation are counted depending on the level at which the mistake is made. Greater penalty is applied for incorrect classification then for a less specific one in the hierarchy. For each image the error ranges from 0 to 1 respectively if the image is correctly classified or if the predicted label is completely wrong. The error is normalized axis wise so that each axis contributes with a maximum of 0.25 to the score. It is also possible to assign a "don't know" label that counts half respect to an error.</p><p>We propose to extract a set of features from each image and to use then SVM to classify them. In the previous editions of the challenge, top-performing methods were based on the assumption that images consist of parts which can be modelled more or less independently. That methods used local features which thus seem to be the most discriminative cues for medical image annotation <ref type="bibr" coords="3,90.00,277.30,15.50,8.74" target="#b15">[16,</ref><ref type="bibr" coords="3,108.41,277.30,7.01,8.74" target="#b8">9]</ref>. Our past experience confirms this assumption, so this year we decided to explore two local approaches using SIFT and LBP based descriptors. We considered them separated and combined through two different integration schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Extraction</head><p>In 2007 for the medical annotation task we defined and used a modified version of the classical SIFT descriptor that we called modSIFT <ref type="bibr" coords="3,273.44,359.45,14.62,8.74" target="#b17">[18]</ref>. Patches were randomly sampled from images and the descriptor considered points at only one octave, discarding rotation invariance. We explored the "bag of words" approach for classification. This is based on the idea that it is possible to transform the images into a set of prespecified visual words, and to classify the images using the statistics of appearance of each word as feature vectors. We built the vocabulary randomly sampling 30 points from each training image and extracting a modSIFT in each point. The visual words were created using an unsupervised K-means clustering algorithm with K=500, that means considering a vocabulary with 500 elements. The feature vector for each image was then defined dividing the image in four parts, randomly extracting 1500 modSIFTs in each subimage, quantizing the resulting distribution of descriptors in the vocabulary and converting it into four histogram of votes that were then put in a row to build the feature vector of 2000 elements.</p><p>The two runs based on this feature ranked third and fourth in 2007, so we decided to reuse it doing only a slight modification, inspired by the approach in <ref type="bibr" coords="3,379.36,502.91,9.97,8.74" target="#b5">[6]</ref>. We added to the original feature vector the histogram obtained extracting modSIFT from the entire image. Actually we obtained this new part of the feature simply adding the four original histograms together and then concatenating the obtained values to the starting vector producing a vector of 2500 elements. We can say that doing this we are considering the image at two different space levels and in our preliminary tests this simple method brought a gain of about 2 score points.</p><p>Another approach that we explored was considering local texture features. As the x-ray images do not contain any color information, texture features play an important role for this task and in the past challenge editions they were used by several groups <ref type="bibr" coords="3,362.04,598.55,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="3,376.50,598.55,7.01,8.74" target="#b8">9]</ref>. We chose the Local Binary Pattern operator, a powerful method well known for its successes in face recognition and object classification <ref type="bibr" coords="3,147.59,622.46,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="3,160.80,622.46,12.73,8.74" target="#b19">20]</ref> and which recently achieved good results also in the medical area <ref type="bibr" coords="3,457.94,622.46,15.49,8.74" target="#b18">[19,</ref><ref type="bibr" coords="3,476.13,622.46,11.62,8.74" target="#b11">12]</ref>. The LBP basic idea is to build a a binary code that describes the local texture pattern in a circular region thresholding each neighborhood on the circle by the gray value of its center. After choosing the dimension of the radius R and the number of points P to be considered on each circle, the images are scanned with the LBP operator pixel by pixel and the outputs are accumulated into a discrete histogram. The operator is gray-scale invariant, moreover we used the rotational invariant LBP version which considers the uniform patterns (LBP riu2 P,R , see Figure <ref type="figure" coords="3,405.90,694.19,3.88,8.74" target="#fig_0">1</ref>). Our preliminary results on a validation set showed that the best way to use LBP on the medical image database at hand was combining in a two dimensional histogram LBP  Here we show just 9 of them corresponding to the uniform patterns with 2 spatial transition i.e. bitwise 0/1 changes (riu2). In the figure black and white circles correspond to the bit values of 0 and 1 in the 8-bit output of the LBP operator <ref type="bibr" coords="4,494.73,326.18,14.62,8.74" target="#b10">[11]</ref>. four parts, one vector is extracted from each subimage and from the central area and then they are concatenated producing a vector of 3240 elements (see Figure <ref type="figure" coords="4,378.73,380.48,3.88,8.74">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Low and Mid Level Integration Schemes</head><p>In the computer vision and pattern recognition literature some authors have suggested different methods to combine information derived from different cues. They can all be reconducted to one of these three approaches: high-level, mid-level and low-level integration <ref type="bibr" coords="4,437.78,450.67,14.62,8.74" target="#b12">[13]</ref>. In the lowlevel integration scheme, image data or the corresponding features are combined together before classification; in the mid-level integration the different feature descriptors are kept separated but they are integrated in a single classifier generating the final hypothesis; finally a high-level cue integration starts from the output of two or more classifiers dealing with complementary information. The hypothesis are then combined together to achieve a consensus decision.</p><p>Considered our results in the ImageCLEF 2007 <ref type="bibr" coords="4,318.61,522.40,14.61,8.74" target="#b17">[18]</ref>, we decided to use again the Multi-Cue Kernel as mid-level integration scheme and the concatenation of feature vectors as low-level integration.</p><p>The Multi-Cue Kernel is a linear combination of kernels each dealing with a single feature.</p><p>Figure <ref type="figure" coords="4,122.03,718.73,3.88,8.74">2</ref>: A schematic drawing which shows how we built the texture feature vector combining the 1-dimensional histograms produced by the LBP operators in 2-dimensional histograms.</p><p>Suppose that for each image I i , we extract a set of P different cues, T p (I i ), p = 1 . . . P . Hence we have P different training sets and a corresponding set of P kernels K p , p = 1 . . . P . The Multi-Cue Kernel between two images, I i and I j , is defined as</p><formula xml:id="formula_0" coords="5,213.94,156.85,299.06,30.20">K M C (I i , I j ) = P p=1 a p K p (T p (I i ), T p (I j )) .<label>(1)</label></formula><p>where a p ∈ + are weighting factors found through cross validation while determining the optimal separating hyperplane.</p><p>On the other hand, in the low-level scheme, the single features vectors are combined in a unique vector, that is normalized to have sum equal to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classification</head><p>For the classification step we used an SVM with an exponential χ 2 as kernel, for both the local structural and textural approaches and the cue-integration methods:</p><formula xml:id="formula_1" coords="5,216.43,313.42,296.58,30.32">K(X, Y ) = exp -γ N i=1 (X i -Y i ) 2 X i + Y i .<label>(2)</label></formula><p>The parameter γ was tuned through cross-validation. This kernel has been successfully applied for histogram comparison and it has been demonstrated to be positive definite <ref type="bibr" coords="5,445.91,365.72,9.97,8.74" target="#b3">[4]</ref>, thus it is a valid kernel. In our experiments we used also the linear, RBF and histogram intersection kernel but all of them gave worse results than the χ 2 . Even if the labels are hierarchical, we have chosen to use the standard multi-class approaches. This choice is motivated by the finding that, with our features, the error score was higher using an axis-wise classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Confidence Based Opinion Fusion</head><p>As previously described, the evaluation scheme for the medical image annotation task addresses the hierarchical structure of the IRMA code by allowing the classifier to decide a "don't know" at any level of the code, independently for each of the four axes. To effectively support this scheme, models which estimate the classifier's confidence in its decision could be useful, a fortiori if we consider the high unbalancing of the classes in the training set.</p><p>Discriminative classifiers usually do not provide any out-of-the-box solution for estimating confidence of the decision, but in some cases they can be transformed in opinion makers on the basis of the value of the used discriminative function. This gives the possibility to derive confidence information and hypothesis ranking from the produced opinions. In case of SVM, it can be done considering the distances between the test samples and the hyperplanes. The evaluation results very efficient due to the use of kernel functions and does not require additional processing in the training phase.</p><p>In the One-vs-All multiclass extension of SVM, if M is the number of classes, M SVMs are trained each separating a single class from all remaining ones. The decision is then based on the distances of the test sample, x, to the M hyperplanes, D j (x), j = 1 . . . M . The final output is the class corresponding to the hyperplane for which the distance is largest:</p><formula xml:id="formula_2" coords="5,256.77,678.62,256.23,18.59">j * = argmax j=1...M D j (x) .<label>(3)</label></formula><p>If now we think of the confidence as a measure of unambiguity of the decision, we can define it as the difference between the maximal and the next largest distance:</p><formula xml:id="formula_3" coords="5,220.56,741.77,292.45,14.66">C(x) = D j * (x) - max j=1...M,j =j * D j (x) .<label>(4)</label></formula><p>The value C(x) can be thresholded for obtaining a binary confidence information. Confidence is then assumed if C(x) &gt; τ for threshold τ . Hence what we did was considering the first two margins produced by SVM corresponding to the distances of the test samples from the two closest hyperplanes. If the decision is not confident, that is C(x) &lt; τ , then the label corresponding to the first two opinions are compared and where they differ we put a "don't know" term. We looked for the best threshold considering the results obtained in the preliminary validation phase and we adopt that for the subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adding Virtual Examples</head><p>To achieve good results in machine learning based classification, it is important to use training data which are sufficient not only in quality but also in quantity. For an SVM, working with classes very sparsely populated means that during the training phase, it is forced to individuate the best hyperplane which separate classes with few examples, to all the rest of the training set. Obviously the work done by the classifier in that condition can't be considered really reliable. To improve the reliability of the classification, we thought to enrich the poorly populated classes.</p><p>Using virtual examples, i.e. artificially created images, is a well known method to expand the training data in an automatic way on the basis of a prior knowledge <ref type="bibr" coords="6,390.73,322.06,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="6,409.55,322.06,12.73,8.74" target="#b13">14,</ref><ref type="bibr" coords="6,425.60,322.06,11.63,8.74" target="#b14">15]</ref>.</p><p>In one of their publications <ref type="bibr" coords="6,223.49,334.01,9.96,8.74" target="#b4">[5]</ref>, people who collected and organized the IRMA database suggest that reasonably small transformations of certain image objects do not affect the class membership. So we produced modified copies of the released images in the subsequent way:</p><p>• each side increased of 100 pixels;</p><p>• each side increased of 50 pixels;</p><p>• each side decreased of 50 pixels;</p><p>• left rotation of 40 degrees;</p><p>• right rotation of 40 degrees;</p><p>• left rotation of 20 degrees;</p><p>• right rotation of 20 degrees;</p><p>• left shift of 50 pixels;</p><p>• right shift of 50 pixels;</p><p>• up shift of 50 pixels;</p><p>• down shift of 50 pixels;</p><p>• left (50 pixels) + up (50 pixels) shift;</p><p>• left (50 pixels) + down (50 pixels) shift;</p><p>• right (50 pixels) + up (50 pixels) shift;</p><p>• right (50 pixels) + down (50 pixels) shift;</p><p>• brightness increased (gray scale enhanced adding 20 to the original gray level and putting to 255 values higher than 255);</p><p>• brightness decreased (gray scale lowered subtracting 20 to the original gray level and putting to 0 the obtained negative values).</p><p>Thus for each of the images belonging to poorly populated classes we produced 17 copies using matlab scripts.</p><p>Before starting our validation experiments, we studied in-depth how to divide the released database to consider the high unbalancing between classes. We decided to separate the training images in:</p><p>• rich set: images belonging to classes with more than 10 elements. A total of 11947 images divided in 115 classes;</p><p>• poor set: images belonging to classes with less than 10 elements. A total of 129 images divided in 82 classes.</p><p>From the first group we built 5 disjoint sets, rich train i /rich test i , each with of 11372/575 images, where the test sets were created randomly extracting five images for each of the 115 classes. On the other hand, we used the whole poor set as a second test set. In this way, although the classes with few images are not considered in the training phase, we can evaluate the performance of the classifier to assign to that images the corresponding nearest class in the hierarchy. So we trained the classifier on the rich train i set and tested both on the rich test i and on the poor set, for each of the 5 splits. The error score was evaluated using the program released by the ImageCLEF organizers. The score values were normalized by the number of images in the corresponding test set, producing two average error scores. They were then multiplied by 500 and summed together to produce the value of the score on the test set of the challenge hypothesizing that it would have been constituted half by images from the rich set and half by images form the poor set. The expected value of the score is then defined as the average of the scores obtained on the 5 splits. Each parameter in our methods was found optimizing this expected score.</p><p>Our validation experiments started considering the local structural and textural approaches separately, applying the proposed experimental setup. We used the above procedure to select the best kernel parameters for the single-cue SVMs, giving the lowest combined error score described above. We adopted the same procedure for our validation experiments with the two cue-integration schemes.</p><p>On top of these experiments we applied, as second step, the confidence based opinion fusion technique described in Section 3. Both the single-cue and the multiple-cue runs were executed using the One-vs-All SVM multiclass extension and saving a file containing for each test image the values of the distances from the separating hyperplanes. We considered these files related to the rich and poor test sets produced by the classification with the best parameters found in the previous phase. The first two higher margins for every test images were subtracted and the difference compared to the threshold τ varying in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]. The best threshold was considered that producing the lowest expected score, using the procedure described above.</p><p>To evaluate the effect of introducing virtual examples in the classes belonging to the poor set we divided it in two parts:</p><p>• poor one: images belonging to classes with only one element. A total of 53 images from 53 classes;</p><p>• poor more: images belonging to classes with more than one element. A total of 76 images from 29 classes.</p><p>We only considered the second group and created 6 poor more train i /poor more test i splits of 29/47 images, where the train sets were defined extracting one image from each of the 29 classes.</p><p>We also introduced virtual examples as described in Section 4 such that each poor more train set was enriched with 29*17=493 images. Then we combined these sets joining rich train i and poor more train j to build the training set and testing separately on rich test i and poor more test j with i = 1, 2, 3, 4, 5 and j = 1, 2, 3, 4, 5, 6. We executed experiments with this setup and the best kernel parameters obtained form the previous single and multiple-cue experiments. The described procedure, for each i, j couple produced again two classification outputs. The error scores were normalized and combined as described above. We also repeated this group of experiments without Finally we applied the confidence based decision fusion on the output of the just presented experiments with the virtual examples in the training set. In this way we obtained our lowest expected score both for the single-cue and the multiple-cue approaches. Note that, independently of the selected feature or combination of features, applying together our two proposed methods always improves the score.</p><p>All the parameters of the validation phase were then used to run our submission experiments on the 1000 unlabelled images of the challenge test set using all the 12076 images of the original dataset as training. We submitted 9 runs. One of them (idiap-MCK pix sift) consisted simply in repeating our 2007 winner run, that is combining modSIFT and pixel features through MCK using exactly the same parameters of last year <ref type="bibr" coords="8,270.10,449.53,14.61,8.74" target="#b16">[17]</ref>. As expected, this run ranked last this year, due to the fact that the dataset varied a lot respect of 2007 and a new search for all the parameters was needed. It is interesting to note that simply applying the confidence based opinion fusion on the this run (idiap-MCK pix sift 2MARG) we have a gain in score of 85. <ref type="bibr" coords="8,390.38,485.40,8.48,8.74" target="#b18">19</ref>.</p><p>Considering that our validation results did not show great differences between the low-level and the mid-level integration scheme we decided to use just the low-level cue-integration scheme for sake of simplicity. We submitted only one MCK run using both the confidence based opinion fusion and the virtual examples. Hence the remaining runs consisted in:</p><p>• using the two new cues separately (idiap-SIFTnew, idiap-LBP);</p><p>• applying cue-integration (idiap-LOW lbp siftnew);</p><p>• combining cue-integration with the confidence based opinion fusion (idiap-LOW 2MARG);</p><p>• combining cue-integration with the introduction of virtual examples in the training set (idiap-LOW MULT);</p><p>• combining cue-integration with the confidence based opinion fusion and the introduction of virtual examples in the training set (idiap-LOW MULT 2MARG, idiap-MCK MULT 2MARG).</p><p>The ranking, name and score of our submitted runs together with the score gain respect to the best run of other participants are listed in Table <ref type="table" coords="8,304.47,679.42,3.88,8.74" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper presents a combination of three different strategies to face the medical image annotation in a highly unbalanced database with great inter-vs-intra class variability. The first consists in combining cues through two different SVM approaches corresponding to a low-level and a mid-level integration scheme. The second allows to estimate the confidence of the classifier decision and, on this basis, to assign to a test image the class label corresponding to the hard decision of the classifier, or to a combination of the labels related to the first two produced opinions. The third consists in enlarging the training set through virtual examples defined as modified copies of the images in the less populated classes. The method obtained combining the low-level cue-integration scheme together with the confidence based opinion fusion and the introduction of virtual examples obtained a score of 74.92 ranking first among all submissions.</p><p>This work can be extended in many ways. First, it could be interesting to understand if the low-level cue-integration scheme results still better then the mid-level one when the number of combined cues grows. We could for example analyze what happens adding to the two presented local features a global one. Second, we would like to integrate the confidence estimation and the cue integration in a unique strategy. The classifier should measure its own level of confidence and, in case of uncertainty, to seek for extra information considering multiple cues, so to increase its own knowledge only when necessary. Third, here we have introduced virtual examples modifying the original images through translation, rotation and brightness changes. The results prove the effectiveness of this strategy, but we wonder if it is possible to avoid this passage. A solution could be to design different features which are able to capture the information coming from all the modified copies. This would make the classification accurate even working with few images. Finally, in this work we used the hierarchical structure of the data only to put in the image label some "don't know" terms. Moreover our preliminary results using the axis-wise classification did not produce good results. We want to study more deeply the hierarchical structure to understand if it is possible to exploit it to produce better classification performance. Future work will explore these directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,90.00,266.41,423.00,8.74;4,90.00,278.36,423.00,8.74;4,90.00,290.32,423.00,8.74;4,90.00,302.27,142.00,8.74;4,233.39,300.70,6.73,6.12;4,232.00,306.79,10.31,6.12;4,247.23,302.27,265.77,8.74;4,90.00,314.23,423.00,8.74;4,90.00,326.18,423.00,8.74;4,132.30,108.86,338.40,142.43"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: a) The basic LBP operator. b) Circularly symmetric neighbor set with radius of 1 pixel and 8 points on the circle. Samples that do not exactly match the pixel grid are obtained via interpolation. c) The rotation invariant (ri) binary patterns that can occur in the circular symmetric neighbor set of LBP ri 8,1 are 36.Here we show just 9 of them corresponding to the uniform patterns with 2 spatial transition i.e. bitwise 0/1 changes (riu2). In the figure black and white circles correspond to the bit values of 0 and 1 in the 8-bit output of the LBP operator<ref type="bibr" coords="4,494.73,326.18,14.62,8.74" target="#b10">[11]</ref>.</figDesc><graphic coords="4,132.30,108.86,338.40,142.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,90.00,110.82,423.00,227.89"><head>Table 1 :</head><label>1</label><figDesc>Ranking of our submitted runs, name, score and gain respect to the best run of the other participants. The extension MULT stands for image multiplication, that isthe use of virtual examples. 2MARG stands for the combination of the first two SVM margins for the confidence based opinion fusion.introducing the virtual examples and the score resulted lower of about 4 points on average showing that the addition of virtual elements is useful for the classification task.</figDesc><table coords="8,178.78,110.82,245.45,128.69"><row><cell cols="2">Rank Name</cell><cell>Score</cell><cell>Gain</cell></row><row><cell>1</cell><cell cols="2">idiap-LOW MULT 2MARG 74.92</cell><cell>30.83</cell></row><row><cell>2</cell><cell>idiap-LOW MULT</cell><cell>83.45</cell><cell>22.30</cell></row><row><cell>3</cell><cell>idiap-LOW 2MARG</cell><cell>83.79</cell><cell>21.96</cell></row><row><cell>4</cell><cell cols="2">idiap-MCK MULT 2MARG 85.91</cell><cell>19.84</cell></row><row><cell>5</cell><cell>idiap-LOW lbp siftnew</cell><cell>93.20</cell><cell>12.55</cell></row><row><cell>6</cell><cell>idiap-SIFTnew</cell><cell cols="2">100.27 5.48</cell></row><row><cell>7</cell><cell>TAU-BIOMED-svm full</cell><cell cols="2">105.75 0</cell></row><row><cell>11</cell><cell>idiap-LBP</cell><cell cols="2">128.58 -22.83</cell></row><row><cell>19</cell><cell cols="3">idiap-MCK pix sift 2MARG 227.82 -122.07</cell></row><row><cell>24</cell><cell>idiap-MCK pix sift</cell><cell cols="2">313.01 -207.26</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,714.66,7.75,6.99"><p>In</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2007" xml:id="foot_1" coords="2,135.58,714.66,172.14,6.99"><p>the name was "BLOOM" due to our sponsors.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is part of the <rs type="projectName">EMMA</rs> project and was supported by the <rs type="funder">Hasler Foundation</rs> and by the <rs type="funder">Blanceflor Boncompagni Ludovisi Foundation</rs> (www.haslerstiftung.ch, www.blanceflor.se). The support is gratefully acknowledged.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_jUQczCR">
					<orgName type="project" subtype="full">EMMA</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.48,520.43,402.51,8.74;9,110.48,532.39,402.52,8.74;9,110.48,544.34,104.06,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,300.58,520.43,212.41,8.74;9,110.48,532.39,122.94,8.74">Face description with local binary patterns: application to face recognition</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,244.95,532.39,264.03,8.74">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,564.27,402.52,8.74;9,110.48,576.22,300.46,8.74" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<title level="m" coord="9,268.35,564.27,244.65,8.74;9,110.48,576.22,140.94,8.74">An Introduction to Support Vector Machines (and Other Kernel-Based Learning Methods)</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,596.15,402.51,8.74;9,110.48,608.10,382.09,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,393.47,596.15,119.52,8.74;9,110.48,608.10,132.33,8.74">The CLEF 2005 automatic medical image annotation task</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,251.38,608.10,181.95,8.74">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,628.03,402.51,8.74;9,110.48,639.98,394.38,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,345.51,628.03,167.48,8.74;9,110.48,639.98,30.61,8.74">Spectral grouping using the Nyström method</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,150.60,639.98,257.82,8.74">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="214" to="225" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,659.91,402.52,8.74;9,110.48,671.86,402.53,8.74;9,110.48,683.82,22.69,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,409.25,659.91,103.75,8.74;9,110.48,671.86,240.13,8.74">A statistical framework for model-based image retrieval in medical applications</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dahmen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">B</forename><surname>Wein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,359.70,671.86,91.95,8.74">J. Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="68" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,703.75,402.51,8.74;9,110.48,715.70,402.52,8.74;9,110.48,727.66,331.01,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,337.94,703.75,175.05,8.74;9,110.48,715.70,234.67,8.74">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,367.28,715.70,145.72,8.74;9,110.48,727.66,221.27,8.74">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,112.02,402.51,8.74;10,110.48,123.98,402.53,8.74;10,110.48,135.93,242.01,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,439.89,112.02,73.11,8.74;10,110.48,123.98,188.84,8.74">The IRMA code for unique classification of medical images</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohnen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">B</forename><surname>Wein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,326.79,123.98,186.22,8.74;10,110.48,135.93,49.93,8.74">Proc. of International Society for Optical Engineering</title>
		<meeting>of International Society for Optical Engineering<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05">May 2003</date>
			<biblScope unit="page" from="440" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,155.86,402.52,8.74;10,110.48,167.81,402.52,8.74;10,110.48,179.77,181.02,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,171.99,155.86,234.43,8.74">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,430.94,155.86,82.06,8.74;10,110.48,167.81,215.17,8.74">Proc. of the International Conference on Computer Vision (ICCV)</title>
		<meeting>of the International Conference on Computer Vision (ICCV)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,199.69,402.52,8.74;10,110.48,211.65,402.53,8.74;10,110.48,223.60,250.60,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,460.29,199.69,52.71,8.74;10,110.48,211.65,315.18,8.74">Overview of the ImageCLEFmed 2006 medical retrieval and medical annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Deserno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,445.89,211.65,61.18,8.74">Proc. of CLEF</title>
		<title level="s" coord="10,110.48,223.60,152.10,8.74">Lecture Notes in Computer Science</title>
		<meeting>of CLEF</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="595" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,243.53,402.52,8.74;10,110.48,255.48,349.20,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,271.78,243.53,241.22,8.74;10,110.48,255.48,108.31,8.74">Incorporating prior information in machine learning by creating virtual examples</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,240.37,255.48,59.45,8.74">Proc. of IEEE</title>
		<meeting>of IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2196" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,275.41,402.51,8.74;10,110.48,287.36,402.52,8.74;10,110.48,299.32,185.12,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,308.94,275.41,204.05,8.74;10,110.48,287.36,224.92,8.74">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,345.42,287.36,167.58,8.74;10,110.48,299.32,88.67,8.74">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,319.24,402.51,8.74;10,110.48,331.20,402.52,8.74;10,110.48,343.15,402.52,8.74;10,110.48,355.11,106.33,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,326.27,319.24,186.72,8.74;10,110.48,331.20,186.79,8.74">False positive reduction in mammographic mass detection using local binary patterns</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lladó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Freixenet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,321.43,331.20,191.57,8.74;10,110.48,343.15,138.59,8.74">Proc. of the Medical Image Computing and Computer-Assisted Intervention</title>
		<title level="s" coord="10,258.64,343.15,157.43,8.74">Lecture Notes in Computer Science</title>
		<meeting>of the Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4791</biblScope>
			<biblScope unit="page" from="286" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,375.03,402.52,8.74;10,110.48,386.99,203.05,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,266.39,375.03,242.16,8.74">Identity verification using speech and face information</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,110.48,386.99,106.27,8.74">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="449" to="480" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,406.91,402.52,8.74;10,110.48,418.87,345.20,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,164.56,406.91,302.46,8.74">Virtual examples for text classification with support vector machines</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sasano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,490.33,406.91,22.67,8.74;10,110.48,418.87,314.84,8.74">Proc. of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,438.79,402.52,8.74;10,110.48,450.75,398.56,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,288.20,438.79,224.81,8.74;10,110.48,450.75,37.69,8.74">Incorporating invariances in support vector learning machines</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,169.75,450.75,188.78,8.74">Proc. of Artificial Neural Network -ICANN</title>
		<meeting>of Artificial Neural Network -ICANN</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1112</biblScope>
			<biblScope unit="page" from="47" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,470.67,402.51,8.74;10,110.48,482.63,402.52,8.74;10,110.48,494.59,298.88,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,458.62,470.67,54.38,8.74;10,110.48,482.63,205.51,8.74">Local versus global features for content-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kosaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aisen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Broderick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,335.08,482.63,177.92,8.74;10,110.48,494.59,186.62,8.74">Proc. of the IEEE Workshop on Content-Based Access of Image and Video Libraries</title>
		<meeting>of the IEEE Workshop on Content-Based Access of Image and Video Libraries</meeting>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
			<biblScope unit="page" from="30" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,514.51,402.51,8.74;10,110.48,526.47,402.53,8.74;10,110.48,538.42,101.36,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,310.21,514.51,202.78,8.74;10,110.48,526.47,133.57,8.74">CLEF2007: Image annotation task: an svmbased cue integration approach</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,253.07,526.47,189.12,8.74">Working Notes of the ImageCLEFmed 2007</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,558.35,402.51,8.74;10,110.48,570.30,243.26,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,299.94,558.35,213.05,8.74;10,110.48,570.30,45.28,8.74">Discriminative cue integration for medical image annotation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,164.71,570.30,118.27,8.74">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,590.23,402.52,8.74;10,110.48,602.18,402.52,8.74;10,110.48,614.14,100.79,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,349.87,590.23,163.13,8.74;10,110.48,602.18,106.13,8.74">Robustness of local binary patterns in brain MR image analysis</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Unay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jasinschi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ercil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,236.44,602.18,272.39,8.74">Proc. of the IEEE Engineering in Medicine and Biology Society</title>
		<meeting>of the IEEE Engineering in Medicine and Biology Society</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="2098" to="2101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,634.06,402.52,8.74;10,110.48,646.02,402.53,8.74;10,110.48,657.97,318.31,8.74" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,338.19,634.06,174.81,8.74;10,110.48,646.02,183.95,8.74">Real-time object classification in video surveillance based on appearance learning</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">T</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,318.26,646.02,194.75,8.74;10,110.48,657.97,174.80,8.74">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
