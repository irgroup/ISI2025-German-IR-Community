<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,126.96,98.73,348.92,15.51;1,116.52,120.69,369.65,15.51">Profil Entropic visual Features for Visual Concept Detection in CLEF 2008 campaign</title>
				<funder ref="#_5cH4rU5">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,223.80,154.07,68.44,9.96"><forename type="first">Herve</forename><surname>Glotin</surname></persName>
							<email>glotin@univ-tln.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire des sciences de l&apos;information et des systemes</orgName>
								<orgName type="institution" key="instit1">UMR CNRS</orgName>
								<orgName type="institution" key="instit2">Universite&apos; Sud Toulon-Var France</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.63,154.07,72.54,9.96"><forename type="first">Zhongqiu</forename><surname>Zhao</surname></persName>
							<email>zhongqiuzhao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire des sciences de l&apos;information et des systemes</orgName>
								<orgName type="institution" key="instit1">UMR CNRS</orgName>
								<orgName type="institution" key="instit2">Universite&apos; Sud Toulon-Var France</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,126.96,98.73,348.92,15.51;1,116.52,120.69,369.65,15.51">Profil Entropic visual Features for Visual Concept Detection in CLEF 2008 campaign</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EFD310D947A25074EA50A2A6AB591926</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Infor-mation Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Rank Fusion, Image Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this task, we used only visual information to implement the VCDT task. We defined and compared two simple projection operators : the harmonic and arithmetic means. We proposed a new kind of compact features based on the entropy of pixels projection. These features, called Profil Entropy Features (PEF), were added to usual color means and variances, and then were fed to SVM classifiers for the detection of 17 visual concepts on the IARPR images during the CLEF 2008 campaign. The simple arithmetic mean projection is at the 4th best rank at the official test over 53 runs of around 20 laboratories. We show that the harmonic projection gives complementary information, and that its simple early fusion with arithmetic PEF yields to the third best rank system. As the runs of the other teams used state of the art SIFT an color histogram visual features, it could be concluded that PEF are efficient. Moreover, PEF are fast with around 10 images computed per second on usual pentium.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction to Visual Concept Detection Clef2008 Task</head><p>ImageCLEF (http://www.imageclef.org/ImageCLEF2008) is the cross-language image retrieval track run as part of the Cross Language Evaluation Forum (CLEF) campaign. This track evaluates retrieval of images described by text captions based on queries in a different language; both text and image matching techniques are potentially exploitable. The visual concept detection task (VCDT) is one of five tasks of ImageCLEF 2008, whose goal is to identify visual concepts that would help in solving the photographic retrieval task in ImageCLEF 2008. Therefore, the VCDT task provides a training database of approximately 1,800 images which are classified according to the concept hierarchy described in Figure <ref type="figure" coords="2,389.98,567.95,5.03,9.96">1</ref> along with their classification. Only these data may be used to train retrieval models. Figure <ref type="figure" coords="2,387.47,579.95,5.03,9.96">2</ref> shows the examples for the 17 topics to find in the IAPR images database. So the retrieval task contains totally 17 topics. The test database consists of 1,000 images, for each of which participating groups are required to determine the presence/absence of the concepts.</p><p>In this task, we use the LS support vector machine (LS-SVM) to implement image retrieval, which will be detailed in Section 4.</p><p>An important step in content-based image retrieval (CBIR) system is the extraction of discriminant visual feature that are fast to compute. Information theory and Cognitive sciences can provide some inspiration for developping such feature.</p><p>Among the many visual features that have been studied, the distribution of color pixels in an image is the most common visual feature studied. The standard representation of color for content-based indexing in image databases is the color histogram. A different color representation is based on the information theoretic concept of entropy. Such entropic feature can simply equal the entropy of the pixel distribution of the image, as proposed in <ref type="bibr" coords="3,357.27,166.91,9.98,9.96" target="#b0">[1]</ref>. A more theoretical presentation of this kind of image entropy feature, accompanied by a practical description of its merits and limitations compared to color histograms, has been given in <ref type="bibr" coords="3,353.78,190.91,9.98,9.96" target="#b1">[2]</ref>.</p><p>We propose in <ref type="bibr" coords="3,173.23,202.79,10.55,9.96" target="#b2">[3]</ref> a new feature equal to the pixel 'profil' entropy. A pixel profil can be a simple arithmetic mean in horizontal (or vertical) direction. The advantage of such feature is to combine raw shape and texture representations in a low cpu cost feature. These feature, associated to mean and color std, reached the second best rank in the official ImagEval 2006 campaing (see www.imageval.org and <ref type="bibr" coords="3,191.54,250.67,10.29,9.96" target="#b3">[4]</ref>).</p><p>In this paper we extend these features using another projection to get the pixel profil. We then propose also to use the harmonic mean of the pixel of each ligne or column. The idea is that the object or pixel region distribution, which is lost in arithmetic mean projection, could be partly catch by the harmonic mean. These two projections are then expected to give complementary and/or concept dependant informations. We detail below the extraction algorithm of these Profil Entropy Feature (PEF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PEF Algorithm</head><p>Let I be an image, or any rectangular subpart of an image.</p><p>For each normalized color (L = R + G + B, r = R/L, andg = G/L), we first calculate two orthogonal profils by the projections of the pixels of I. We consider two simple orthogonal projection axes : the horizontal axis X (noted Π X ), versus the vertical one Y (noted Π Y ). The projection operator is either the arithmetic mean (noted 'Ar', then the projection is noted Π Ar X ), as illustrated in Figure <ref type="figure" coords="3,194.33,428.39,3.90,9.96" target="#fig_1">3</ref>, or the harmonic mean of the pixels on each column or each ligne of I (noted 'Ha', then we have Π Ha X ). Then, we estimate the probability distribution function (pdf) of each profil according to <ref type="bibr" coords="3,499.39,452.39,9.98,9.96" target="#b4">[5]</ref>. Considering that the sources are ergodic, we finaly calculate each PEF equal to the normalized entropy (H(pdf )/log(#bins(pdf ))). We detail below each steps of the PEF extraction.</p><p>Let be op the selected projection, for each color of I of L(I) lignes and C(I) columns :</p><formula xml:id="formula_0" coords="3,90.00,533.39,277.65,74.22">Φ op X (I) = p df (Π op X (I)), over nbin X (I) = round( C(I)) bins, where Π op X is the vertical projection with operator op, P EF X (I) = H(Φ op X (I))/log(nbin X (I)). Φ op Y (I) = p df (Π op Y (I)), over nbin Y (I) = round( L(I)) bins, P EF Y (I) = H(Φ op Y (I))/log(nbin Y (I)).</formula><p>We add to these P EF a the usual entropic feature : p df (I) = pdf of all the pixels of I over nbin XY (I) = nbin X (I) * nbin Y (I) bins, P EF . (I) = H( p df (I))/log(nbin XY (I)).</p><p>And we finaly complete the PEF features by the usual mean and standard deviation of each normalized color of I. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">PEF concatenations</head><p>We can calculate the PEF into three horizontal subimages as illustrated in Figure <ref type="figure" coords="4,443.34,360.23,3.90,9.96" target="#fig_3">4</ref>. We note such PEF '='. We also calculate the PEF in three vertical subimages, we note these PEF ' '.</p><p>For each, we have   The support vector machine (SVM) <ref type="bibr" coords="5,246.59,273.47,11.00,9.96" target="#b5">[6,</ref><ref type="bibr" coords="5,257.60,273.47,7.34,9.96" target="#b6">7]</ref> first maps the data into a higher dimensional input space by some kernel functions to learn a separating hyperspace to maximize the margin. Currently, because of its good generalization capability, this technique has been widely applied in many areas such as face detection, image retrieval, and so on <ref type="bibr" coords="5,335.65,309.35,10.55,9.96" target="#b7">[8,</ref><ref type="bibr" coords="5,346.20,309.35,7.03,9.96" target="#b8">9]</ref>. The SVM is typically based on an ε-insensitive cost function, meaning that approximation errors smaller than will not increase the cost function value. This results in a quadratic convex optimization problem. So instead of using an ε-insensitive cost function, a quadratic cost function can be used. The least squares support vector machines (LS-SVM) <ref type="bibr" coords="5,212.90,357.11,15.58,9.96" target="#b9">[10]</ref> are reformulations to the standard SVMs which lead to solving linear KKT systems instead. It is computationally attractive.</p><p>In our experiments, the RBF kernel</p><formula xml:id="formula_1" coords="5,225.72,401.71,151.56,11.89">K(x 1 -x 2 ) = exp(-|x 1 -x 2 | 2 /σ 2 )</formula><p>is selected as the kernel function of our LS-SVM. So there is a corresponding parameter, σ , to be tuned. A large value of σ 2 indicates a stronger smoothing. Moreover, there is another parameter, γ, needing tuning to find the tradeoff between to stress minimizing of the complexity of the model and to stress good fitting of the training data points. We train a total of 100 SVMs with different parameter values for each topic, and then we selected the best SVM using the validation set. In the experiments, we used the LS-SVMlab1.5 toolbox, which can be downloaded from http://www.esat.kuleuven.ac.be/sista/lssvmlab/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>The process we adopt to implement the image retrieval in VCDT task is shown in Figure <ref type="figure" coords="5,491.44,551.39,3.90,9.96" target="#fig_4">5</ref>. It can also be depicted as the following steps:</p><p>Step 1) Split the VCDT labeled image dataset into 2 sets, namely training image dataset and validation set.</p><p>Step 2) Extract the visual features from the training image data using our extraction method; train and generate lots of SVM (or in the original run Kernel Discrimant Analysis or MlP) with different parameters.</p><p>Step 3) Use the validation set to select the best model</p><p>Step 4) Extract the visual features from the VCDT test image database using our extraction method; and then use the best model to find the best discriminant feature.</p><p>Step 5) Sort the test images by the distances from the positive training images and produce the final rank result.  We show in Figure <ref type="figure" coords="6,195.42,473.03,5.03,9.96" target="#fig_6">6</ref> the Area Under the curve (AUC) gains, calculated using the CLEF evaluation tool, for the differents PEF.</p><p>In the top of Figure <ref type="figure" coords="6,199.61,496.91,5.03,9.96" target="#fig_6">6</ref> : gain (in %) of AUC using Arithmetic projection PEF from the = subimages compared to the AUC using PEF from the subimages. This shows that the choice of subimages type (horizontal or vertical subimages) could be optimized according to the topics.</p><p>Middle of Figure <ref type="figure" coords="6,178.87,532.79,5.03,9.96" target="#fig_6">6</ref> : gain (in %) of AUC using concatenated Arithmetic PEF ('Ar+') compared to concatenated Harmonic PEF ('Ha+'). This shows that Ar PEF are more discriminant for most of the considered topics, except for 'tree' and 'mountains', where 'Ha' PEF are better.</p><p>Bottom of Figure <ref type="figure" coords="6,185.93,568.67,5.03,9.96" target="#fig_6">6</ref> : gain (in %) of AUC using concatenated PEF (Ar+ and Ha+) against 'Ha+' alone. The gain are all positive and significant, showing the completion between Ar and Ha projection operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and conclusion</head><p>We finaly compared PEF scores to the 4 best team which participated to Clef VCDT 2008. Figure <ref type="figure" coords="6,90.00,659.27,5.03,9.96" target="#fig_7">7</ref> gives for each topic the classification error (= the complementary of the usual area under the curve = 1 -Area Under the Curve). In average the results of our 126 PEF features (denoted by 'LSIS') are at the third rank into the initial official campaing (the average of the 17 topic errors is the given at index 18 in fig. <ref type="figure" coords="6,231.26,695.03,3.88,9.96" target="#fig_7">7</ref>). Xerox system is the best, certainly including SIFT features and large reference images database (see Xerox paper in this workshop). The usual perceptual color histograms features, of around 200 dimensions, that has been partly used by UPMC (see workshop note) seem similar or little less discriminant than PEF.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,151.08,231.83,300.79,9.96;2,153.48,58.97,296.10,158.37"><head>Figure 1 .Figure 2 :</head><label>12</label><figDesc>Figure 1. The class hierarchy of the VCDT task of ImageCLEF 2008</figDesc><graphic coords="2,153.48,58.97,296.10,158.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,90.00,287.51,422.77,9.96;4,90.00,299.51,288.83,9.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of the horizontal and vertical profils using simple arithmetic projection (or sum) of each normalized color r = R/L, g = G/L, L = R + G + B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,187.00,384.11,325.68,9.96;4,90.00,396.11,422.60,9.96;4,90.00,407.99,422.78,9.96;4,90.00,419.99,422.64,9.96;4,90.00,431.87,422.75,9.96;4,90.00,443.87,422.49,9.96"><head></head><label></label><figDesc>3 bands and 3 different PEF for each of the 3 colors, plus their mean and variance, thus we have 3 * 3 * 3 + 3 * 3 * 2 = 45 dimensions for '=' or for ' ' features. We note '+' the feature concatenation of '=' and ' ' features, which has then 90 dimensions. Considering the two mean type, the PEF concatenation without repetition of the mean and std color are quite compact with a total of 126 dimensions (= 2 (subimages type '=' or ' ') * 3 (bands by subimages type) * 3 (rgL) * 4 (=4 Π types = (X or Y) * (Ar or Ha) ) + 1 (=H(I)) + 2 (= mean and std))).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,90.00,633.71,422.68,9.96;4,90.00,645.71,44.24,9.96;4,230.81,474.62,140.75,144.55"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of the 3 subimages of type '=' (horizontal). The type ' ' is simple its transpose.</figDesc><graphic coords="4,230.81,474.62,140.75,144.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,175.68,219.83,251.40,9.96;5,111.12,58.95,380.70,146.39"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The framework for image retrieval of each topic</figDesc><graphic coords="5,111.12,58.95,380.70,146.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,116.30,173.41,4.83,8.04;6,159.42,173.41,4.83,8.04;6,202.60,173.41,4.83,8.04;6,245.79,173.41,4.83,8.04;6,288.98,173.41,4.83,8.04;6,329.77,173.41,9.67,8.04;6,372.96,173.41,9.67,8.04;6,416.15,173.41,9.67,8.04;6,459.33,173.41,9.67,8.04;6,502.52,173.41,9.67,8.04;6,106.30,166.09,9.91,8.04;6,111.38,139.06,4.83,8.04;6,111.38,112.03,4.83,8.04;6,106.52,85.08,9.67,8.04;6,94.12,138.65,8.04,16.43;6,94.12,117.88,8.04,18.36;6,94.12,101.94,8.04,13.52;6,251.44,183.77,123.17,8.04;6,116.30,285.94,4.83,8.04;6,159.42,285.94,4.83,8.04;6,202.60,285.94,4.83,8.04;6,245.79,285.94,4.83,8.04;6,288.98,285.94,4.83,8.04;6,329.77,285.94,9.67,8.04;6,372.96,285.94,9.67,8.04;6,416.15,285.94,9.67,8.04;6,459.33,285.94,9.67,8.04;6,502.52,285.94,9.67,8.04;6,106.30,278.62,9.91,8.04;6,111.38,251.59,4.83,8.04;6,111.38,224.56,4.83,8.04;6,106.52,197.61,9.67,8.04;6,94.12,251.18,8.04,16.43;6,94.12,230.41,8.04,18.36;6,94.12,214.47,8.04,13.52;6,249.92,296.30,126.14,8.04;6,116.30,398.54,4.83,8.04;6,159.42,398.54,4.83,8.04;6,202.60,398.54,4.83,8.04;6,245.79,398.54,4.83,8.04;6,288.98,398.54,4.83,8.04;6,329.77,398.54,9.67,8.04;6,372.96,398.54,9.67,8.04;6,416.15,398.54,9.67,8.04;6,459.33,398.54,9.67,8.04;6,502.52,398.54,9.67,8.04;6,111.38,391.22,4.83,8.04;6,111.38,364.20,4.83,8.04;6,106.52,337.17,9.67,8.04;6,106.52,310.14,9.67,8.04;6,94.34,363.71,8.04,16.43;6,94.34,342.94,8.04,18.36;6,94.34,327.00,8.04,13.52;6,233.91,408.90,158.28,8.04"><head></head><label></label><figDesc>Ar+ U Ha+] on Ha+ ) for each topic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,193.08,433.43,216.78,9.96"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The relative gains for the different PEF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="7,90.00,408.11,422.63,9.96;7,90.00,419.99,422.78,9.96;7,90.00,431.99,422.89,9.96;7,90.00,443.99,422.64,9.96;7,90.00,455.87,391.14,9.96"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The different scores (=1-Area Under the Curve) for the 17 topics and the 4 best teams at the Clef VCDT task. The new fusion of the PEF features (LSIS) after the official campaign are at the third rank into the initial CLEF results. Xerox (see XEROX notes in this workshop, maybe using SIFT features) is the best. Usual color histograms features (around 200 dimensions, see this workshop note) used by UPMC seem similar or little less discriminant than PEF.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was partially supported by the <rs type="funder">French National Agency of Research</rs> (<rs type="grantNumber">ANR-06-MDCA-002</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5cH4rU5">
					<idno type="grant-number">ANR-06-MDCA-002</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,110.51,643.67,402.15,9.96;7,110.52,655.55,402.13,9.96;7,110.52,667.55,22.88,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,177.29,643.67,335.37,9.96;7,110.52,655.55,131.71,9.96">Saliency maps and attention selection in scale and spatial coordinates: An information theoretic approach</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,260.40,655.55,247.94,9.96">Proc. of 5th International Conference on Computer Vision</title>
		<meeting>of 5th International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.51,61.43,402.24,9.96;8,110.52,73.43,402.14,9.96;8,110.52,85.31,402.14,9.96;8,110.52,97.31,36.30,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,278.60,61.43,234.15,9.96;8,110.52,73.43,402.14,9.96;8,110.52,85.31,28.68,9.96">Content based image retrieval and information theory: A generalized approach, in Special Topic Is-sue on Visual Based Retrieval Systems and Web Mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barhen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,147.58,85.31,316.91,9.96">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="page" from="841" to="853" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.51,117.23,402.13,9.96;8,110.52,129.23,402.35,9.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,161.13,117.23,351.51,9.96;8,110.52,129.23,56.31,9.96">Robust Information Retrieval and perception for a scaled Lego-Audio-Video multistructuration</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>University Sud Toulon-Var</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Thesis of habilitation for research direction</note>
</biblStruct>

<biblStruct coords="8,110.51,149.15,402.16,9.96;8,110.52,161.03,402.16,9.96;8,110.52,173.03,22.88,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,228.04,149.15,284.63,9.96;8,110.52,161.03,205.12,9.96">Web image retrieval on imageval: Evidences on visualness and textualness concept dependency in fusion model</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tollari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,334.64,161.03,173.85,9.96">ACM Int Conf on Image Video Retrieval</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.51,192.95,402.20,9.96;8,110.52,204.95,244.08,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,188.22,192.95,324.49,9.96;8,110.52,204.95,19.88,9.96">On estimation of entropy and mutual information of continuous distributions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Moddemeijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,137.62,204.95,74.32,9.96">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="246" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.51,224.87,373.32,9.96" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m" coord="8,184.00,224.87,172.67,9.96">The nature of statistical learning theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.51,244.79,293.12,9.96" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m" coord="8,184.00,244.79,110.93,9.96">Statistical learning theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.51,264.71,402.40,9.96;8,110.52,276.59,376.68,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,254.16,264.71,225.05,9.96">Face detection using spectral histograms and SVMs</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">A</forename><surname>Waring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,488.81,264.71,24.10,9.96;8,110.52,276.59,243.75,9.96">IEEE Transactions on Systems, Man, and Cybernetics, Part B</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="467" to="476" />
			<date type="published" when="2005-06">2005. June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.51,296.63,402.23,9.96;8,110.52,308.51,402.27,9.96;8,110.52,320.51,72.50,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,261.82,296.63,247.06,9.96">Support vector machine active learning for image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chang</forename><surname>Edward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,123.00,308.51,341.64,9.96">Proceedings of the ninth ACM international conference on Multimedia Ottawa</title>
		<meeting>the ninth ACM international conference on Multimedia Ottawa<address><addrLine>Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="107" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.50,340.43,402.36,9.96;8,110.52,352.31,196.57,9.96" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,295.62,340.43,217.24,9.96;8,110.52,352.31,110.78,9.96">Least Squares Support Vector Machine Classifiers Neural Processing Letters</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="293" to="300" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
