<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.08,77.05,323.24,12.19;1,247.50,95.06,100.31,12.19">Increasing Relevance and Diversity in Photo Retrieval by Result Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,209.70,131.45,81.00,10.46"><forename type="first">Yih-Chen</forename><surname>Chang</surname></persName>
							<email>ycchang@nlg.csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University Taipei</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,314.03,131.45,71.66,10.46"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
							<email>hhchen@csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University Taipei</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.08,77.05,323.24,12.19;1,247.50,95.06,100.31,12.19">Increasing Relevance and Diversity in Photo Retrieval by Result Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BC2AA4CA7EDCB50FD3292768E1C038CD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper considers the strategies of query expansion, relevance feedback and result fusion to increase both relevance and diversity in photo retrieval. In the text-based retrieval only experiments, the run with query expansion has better MAP and P20 than that without query expansion, and only has 0.85% decrease in CR20. Although relevance feedback run increases both MAP and P20, its CR20 decreases 10.18% compared with non-feedback run. It shows that relevance feedback brings in relevant but similar images, thus diversity may be decreased. The run with both query expansion and relevance feedback is the best in the four text-based runs. In the content-based retrieval only experiments, the run without feedback outperforms the run with feedback. The latter has 10.84%, 9.13%, and 20.46% performance decrease in MAP, P20, and CR20. In the fusion experiment, integrating text-based and content-based retrieval not only reports more relevant images, but also more diverse ones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the photo retrieval task of ImageCLEF 2008, the focus is shifted from cross language image retrieval to promote diversity. Besides relevance, retrieving diverse items representing different subtopics is also concerned. How to balance the relevance and diversity is challenging. This paper studies the strategies of query expansion and relevance feedback in text-based and content-based retrieval, and shows how to merge the results of text and image queries to increase both relevance and diversity.</p><p>This paper is organized as follows. Sections 2, 3 and 4 present text-based retrieval, content-based retrieval and combination of both, respectively. Section 5</p><p>shows the runs submmited for formal evaluation in the photo retrieval task, and discsses the effects of different retrieval and fusion strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Text-Based Retrieval</head><p>In text-based retrieval, we consider the strategies of query expansion and relevance feedback. Assume the text corpus T is composed of n terms, t 1 , t 2 , …, t n , and a query Q contains m query terms, q 1 , q 2 , …, q m . We expand Q in the following way.</p><p>(1) For each corpus term t i and query term q j , compute P(q j | t i )=P(t i ,q j )/ P(t i ).</p><p>(2) For each corpus term t i , compute OverlapNum(t i ,Q) defined below.</p><formula xml:id="formula_0" coords="2,161.88,203.81,255.35,11.10">OverlapNum(t i ,Q) = cardinality{q | q∈Q, P(q | t i )&gt;0}</formula><p>(3) For all t i ∈T, if &gt; thd, then t i will be added to new query Q'. In the experiments, thd is set to 1. In other words, the original query terms which also appear in the corpus will be added into Q'.</p><formula xml:id="formula_1" coords="2,215.34,219.76,159.27,29.69">∑ = × m j i i j ) Q , t ( ) t | q ( P 1 OverlapNum</formula><p>We adopt Lemur as our text IR system. The weighting function is BM25 with parameters (K1=1.2, B=0.75, K3=7). For relevance feedback, we select the top-10 terms of the highest BM25 scores from the top-5 retrieved documents, and add them to the query. The expanded terms have 1/2 weight of the original query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Content-Based Retrieval</head><p>For each image g i , we extract two kinds of features: SizeFeature(g i ) and</p><p>ColorFeature(g i ). These two functions are defined below.</p><p>(1)</p><formula xml:id="formula_2" coords="2,137.88,437.45,218.74,29.10">SizeFeature(g i ) = 0, if height(g i )&gt;width(g i ) = 1, if height(g i )&lt;=width(g i )</formula><p>(2) ColorFeature(g i ): divide g i into 32×32 blocks, and extract their RGB values. The similarity of two images, g i and g j , is computed as follows.</p><p>(1) Compute the color similarity of g i and g j based on their color features.</p><p>ColorSimilar(g i , g j ) = number of blocks in g i and g j , whose R, G and B value differences are not larger than 10.</p><p>(2) Compute the size similarity of g i and g j based on their size features.</p><p>If SizeFeature(g i ) and SizeFeature(g j ) is the same, then SizeSimilar(g i , g j )=1.5. Otherwise, SizeSimilar(g i , g j )=1.0.</p><p>(3) The similarity of g i and g j is in terms of SizeSimilar and ColorSimilar: Similar(g i , g j )= SizeSimilar(g i , g j ) × ColorSimilar(g i , g j )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Combining Text-based and Content-based Retrieval</head><p>In image retrieval, we compute the similarities of the query images and all the images in the data set and select the most similar image for media mapping <ref type="bibr" coords="2,455.65,707.45,49.88,10.46;2,89.88,725.45,63.77,10.46" target="#b0">(Chen and Chang, 2006)</ref>. The corresponding text description of the reported image is regarded as a text query for further retrieval. The results of text-based and content-base retrieval are merged in the following way. We normalize the scores of the two result lists by the corresponding top-1 scores <ref type="bibr" coords="3,299.40,95.45,155.41,10.46" target="#b1">(Tsai, Wang, and Chen, 2008)</ref>, i.e., the normalized scores will be within 0 and 1, and merge the lists with the same weights by their normalized scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Discussion</head><p>We submit 7 runs shown below for the formal evaluation.</p><p>(1) NTU-EN-EN-AUTO-NOFB-TXT This run is baseline. We employ Lemur for text-based retrieval without query expansion and relevance feedback.</p><p>(     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,123.20,257.45,162.56,10.46;3,137.88,275.45,353.87,10.46;3,113.88,293.45,209.19,10.46;3,137.88,311.45,340.59,10.46;3,113.88,329.45,191.87,10.46;3,137.88,347.45,367.65,10.46;3,137.88,365.45,95.31,10.46;3,113.88,383.45,221.50,10.46;3,137.88,401.45,367.67,10.46;3,137.88,419.45,367.52,10.46;3,137.88,437.45,230.15,10.46;3,113.88,455.45,202.53,10.46;3,137.88,473.45,367.57,10.46;3,137.88,491.45,132.64,10.46;3,113.88,509.45,215.21,10.46;3,137.88,527.45,367.58,10.46;3,137.88,545.45,169.82,10.46;3,113.88,563.45,391.63,10.46;3,89.88,581.45,415.61,10.46;3,89.88,599.45,310.76,10.46"><head></head><label></label><figDesc>) NTU-EN-EN-AUTO-FB-TXT This run employs Lemur for text-based retrieval with relevance feedback. (3) NTU-EN-EN-AUTO-QE-NOFB-TXT This run employs Lemur for text-based retrieval with query expansion. (4) NTU-EN-EN-AUTO-QE-FB-TXT This run employs Lemur for text-based retrieval with query expansion and relevance feedback. (5) NTU-IMG-EN-AUTO-NOFB-TXTIMG. This run employs content-based retrieval first, then adopts media mapping to transform the image query to text query, and employs Lemur for text-based retrieval without relevance feedback. (6) NTU-IMG-EN-AUTO-FB-TXTIMG This run is similar to NTU-IMG-EN-AUTO-NOFB-TXTIMG except that relevance feedback is done. (7) NTU-EN-EN-AUTO-QE-FB-TXTIMG This run merges the results of NTU-IMG-EN-AUTO-FB-TXTIMG and NTU-EN-EN-AUTO-QE-FB-TXT. The evaluation of the formal runs is based on mean average precision (MAP), precision at 20 (P20) and instance recall at rank 20 (CR20), which calculates the percentage of different clusters represented in the top 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,89.88,599.45,415.69,154.46"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="3,463.81,599.45,41.68,10.46"><row><cell>lists the</cell></row></table><note coords="3,89.88,707.45,415.66,10.46;3,89.88,725.45,415.69,10.46;3,89.88,743.45,390.50,10.46"><p>similar images, thus diversity may be decreased. The run with both query expansion and relevance feedback is better than the other three runs. Compared with baseline, it has 33.79%, 44.44%, and 0.27% increase in MAP, P20 and CR20, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,95.64,77.57,414.59,154.96"><head>Table 1 . Comparisons of Runs Employing Text Query Only</head><label>1</label><figDesc></figDesc><table coords="4,95.64,96.18,414.59,136.35"><row><cell>Runs</cell><cell cols="2">Feedback Expansion</cell><cell>MAP</cell><cell>P20</cell><cell>CR20</cell></row><row><cell>EN-EN-AUTO-NOFB-TXT</cell><cell>No</cell><cell>No</cell><cell>0.1790</cell><cell>0.2077</cell><cell>0.2602</cell></row><row><cell>EN-EN-AUTO-QE-NOFB-TXT</cell><cell>No</cell><cell>Yes</cell><cell>0.1967</cell><cell>0.2244</cell><cell>0.2580</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(+9.88%)</cell><cell>(+8.04%)</cell><cell>(-0.85%)</cell></row><row><cell>EN-EN-AUTO-FB-TXT</cell><cell>Yes</cell><cell>No</cell><cell>0.2122</cell><cell>0.2692</cell><cell>0.2337</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(+18.54%)</cell><cell>(29.61%)</cell><cell>(-10.18%)</cell></row><row><cell>EN-EN-AUTO-QE-FB-TXT</cell><cell>Yes</cell><cell>Yes</cell><cell>0.2395</cell><cell>0.3000</cell><cell>0.2609</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(+33.79%)</cell><cell>(+44.44%)</cell><cell>(+0.27%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,89.88,260.45,415.72,244.46"><head>Table 2</head><label>2</label><figDesc>lists the experimental results of employing sample images. In the experiments, 3 example images are considered. The run without feedback outperforms the run with feedback. The latter has 10.84%, 9.13%, and 20.46% performance decrease in MAP, P20, and CR20. The possible reason of the drop in precision is the top-5 retrieved images for feedback may be very specific. That may introduce noises. Consider topic 43, sunset over water, as an example. The correct image should contain both sunset and water. The query without feedback is "Sunset at the sea the dark outlines of a mountain in the foreground the sun is rising over the sea behind it a light orange sky in the background peru". In the top-5 retrieved images, only one contains both scenes, but all of them contain sunset scene. There</figDesc><table coords="4,89.88,440.45,415.72,64.46"><row><cell>are 34 relevant images in the result list before feedback, and only 25 relevant images</cell></row><row><cell>after feedback. The MAP decreases from 0.1776 to 0.0535 after feedback. CR20</cell></row><row><cell>decreases more than MAP and P20. It shows pure relevance feedback is harmful to</cell></row><row><cell>diversity.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,95.28,530.57,409.26,83.20"><head>Table 2 . Comparisons of Runs Employing Image Query Only</head><label>2</label><figDesc></figDesc><table coords="4,95.28,548.93,409.26,64.84"><row><cell>Run</cell><cell>Feedback</cell><cell>MAP</cell><cell>P20</cell><cell>CR20</cell></row><row><cell>IMG-EN-AUTO-NOFB-TXTIMG</cell><cell>No</cell><cell>0.2103</cell><cell>0.3090</cell><cell>0.1779</cell></row><row><cell>IMG-EN-AUTO-FB-TXTIMG</cell><cell>Yes</cell><cell>0.1875</cell><cell>0.2808</cell><cell>0.1415</cell></row><row><cell></cell><cell></cell><cell>(-10.84%)</cell><cell>(-9.13%)</cell><cell>(-20.46%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,89.88,640.43,415.70,82.46"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="4,89.88,640.43,415.70,82.46"><row><cell>compares the performance of employing text query only, image query</cell></row><row><cell>only, and both. The fusion run, which achieves MAP 0.2809, P20 0.3769 and CR20</cell></row><row><cell>0.2763, is the best of our 7 submitted runs in the formal evaluation. It shows that</cell></row><row><cell>integrating text-based and content-based retrieval not only reports more relevant</cell></row><row><cell>images, but also more diverse ones.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,89.88,77.57,415.73,228.86"><head>Table 3 . Comparisons of Runs Employing Text Query, Image Query and Both</head><label>3</label><figDesc>This paper considers query expansion, relevance feedback and result fusion to deal with relevance and diversity in image retrieval. Query expansion is useful to increase the precsion in text-based retrieval, but has a little negative effect on the diversity. Relevance feedback is harmful to diversity when this strategy is used independently or in single type of queries. Text-based and content-based retrievals have their own special capability, so that both relevance and diversity are improved.</figDesc><table coords="5,89.88,95.93,409.62,102.62"><row><cell>Run</cell><cell>Feedback</cell><cell>MAP</cell><cell>P20</cell><cell>CR20</cell></row><row><cell>EN-EN-AUTO-QE-FB-TXT</cell><cell>Yes</cell><cell>0.2395</cell><cell>0.3000</cell><cell>0.2609</cell></row><row><cell>IMG-EN-AUTO-FB-TXTIMG</cell><cell>Yes</cell><cell>0.1875</cell><cell>0.2808</cell><cell>0.1415</cell></row><row><cell>EN-EN-AUTO-QE-FB-TXTIMG</cell><cell>Yes</cell><cell>0.2809</cell><cell>0.3769</cell><cell>0.2763</cell></row><row><cell>6. Conclusion</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,89.88,349.97,415.56,10.46;5,107.88,367.97,391.32,10.46;5,499.20,365.08,6.21,6.96;5,107.88,385.97,397.59,10.46;5,107.88,403.97,319.67,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,335.88,349.97,169.55,10.46;5,107.88,367.97,276.10,10.46">Language Translation and Media Transformation in Cross-Language Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yih-Chen</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,403.74,367.97,95.46,10.46;5,499.20,365.08,6.21,6.96;5,107.88,385.97,265.65,10.46">Proceedings of 9 th International Conference on Asian Digital Libraries</title>
		<title level="s" coord="5,177.23,403.97,169.24,10.46">Lecture Notes in Computer Science</title>
		<meeting>9 th International Conference on Asian Digital Libraries<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-11-27">2006. November 27-30, 2006</date>
			<biblScope unit="volume">4312</biblScope>
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,89.88,421.97,415.56,10.46;5,107.88,439.97,392.28,10.46;5,500.16,437.08,5.34,6.96;5,107.88,457.97,397.62,10.46;5,107.88,475.97,43.02,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,394.81,421.97,110.63,10.46;5,107.88,439.97,264.89,10.46">A Study of Learning a Merge Model for Multilingual Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu-Ting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,387.78,439.97,112.38,10.46;5,500.16,437.08,5.34,6.96;5,107.88,457.97,235.76,10.46">Proceedings of the 31 st Annual International ACM SIGIR Conference</title>
		<meeting>the 31 st Annual International ACM SIGIR Conference<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-07-24">2008. 20-24 July 2008</date>
			<biblScope unit="page" from="195" to="202" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
