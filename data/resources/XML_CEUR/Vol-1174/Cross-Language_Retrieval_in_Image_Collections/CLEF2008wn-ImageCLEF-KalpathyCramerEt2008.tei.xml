<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,106.12,76.50,394.42,13.00">Multimodal Medical Image Retrieval: OHSU at ImageCLEF 2008</title>
				<funder ref="#_jvFctg7">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_F6QTwMN">
					<orgName type="full">NLM</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.68,114.96,111.69,9.04"><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology</orgName>
								<orgName type="institution">Oregon Health &amp; Science University</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.26,114.96,59.71,9.04"><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
							<email>bedricks@ohsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology</orgName>
								<orgName type="institution">Oregon Health &amp; Science University</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.80,114.96,51.13,9.04"><forename type="first">William</forename><surname>Hatt</surname></persName>
							<email>hattb@ohsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology</orgName>
								<orgName type="institution">Oregon Health &amp; Science University</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,395.46,114.96,58.94,9.04"><forename type="first">William</forename><surname>Hersh</surname></persName>
							<email>hersh@ohsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology</orgName>
								<orgName type="institution">Oregon Health &amp; Science University</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,106.12,76.50,394.42,13.00">Multimodal Medical Image Retrieval: OHSU at ImageCLEF 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8E83A314D62CC945858AB813E4A264F8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>Measurement, Performance, Experimentation Image Retrieval, Performance Evaluation, Image Classification, Medical Imaging Medical Image Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present results from Oregon Health &amp; Science University's participation in the medical image retrieval task of ImageCLEF 2008. We created a web-based retrieval system built on a full-text index of the annotations using a Ruby on Rails framework. The text-based search engine was implemented in Ruby using Ferret, a port of Lucene. In addition to this textual index of annotations, supervised machine learning techniques using visual features were used to classify the images based on image acquisition modality. All images were annotated with the purported modality. Our system provides the user with a number of search options including those for limiting the search to the desired modality, UMLS-based term expansion and Natural Language Processing based techniques. Purely textual runs as well as mixed runs using the purported modality were submitted. We also submitted interactive runs using a number of user specified search options. Latent semantic analysis of the visual features was used to reorder results. The use of the UMLS Metathesaurus increased our recall. However, our system is primarily geared towards precision. Consequently, many of our multimodal automatic runs using the custom parser as well as interactive runs had high early precision. Our runs also performed well using the bpref metric, a measure that is more robust in the case of incomplete judgments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image retrieval systems do not currently perform as well as their text counterparts <ref type="bibr" coords="1,429.64,690.00,10.66,9.04" target="#b0">[1]</ref>. Medical and other image retrieval systems have historically relied on annotations or captions associated with the images for indexing the retrieval system. The last few decades have seen numerous advancements in the area of contentbased image retrieval (CBIR) <ref type="bibr" coords="1,196.36,724.80,10.83,9.04" target="#b1">[2,</ref><ref type="bibr" coords="1,207.19,724.80,7.22,9.04" target="#b2">3]</ref>. Although CBIR systems have demonstrated success in fairly constrained medical domains including pathology, dermatology, chest radiology, and mammography, they have demonstrated poor performance when applied to databases with a wide spectrum of imaging modalities, anatomies and pathologies <ref type="bibr" coords="2,180.28,86.16,11.03,9.04" target="#b0">[1,</ref><ref type="bibr" coords="2,191.31,86.16,7.36,9.04" target="#b3">4,</ref><ref type="bibr" coords="2,198.67,86.16,7.36,9.04" target="#b4">5,</ref><ref type="bibr" coords="2,206.02,86.16,7.36,9.04" target="#b5">6]</ref>.</p><p>Retrieval performance has shown demonstrable improvement by fusing the results of textual and visual techniques. This has especially been shown to improve early precision <ref type="bibr" coords="2,364.14,109.20,10.82,9.04" target="#b6">[7,</ref><ref type="bibr" coords="2,374.96,109.20,7.21,9.04" target="#b7">8]</ref>. The medical image retrieval task within ImageCLEF (ImageCLEFmed) 2008 campaign is TREC-style <ref type="bibr" coords="2,358.14,120.72,11.67,9.04" target="#b8">[9]</ref> and provides a forum and set of test collections for the medical image retrieval community to use to benchmark their algorithms on a set of queries. The ImageCLEF campaign has, since 2003, been a part of the Cross Language Evaluation Forum (CLEF) <ref type="bibr" coords="2,72.04,155.52,12.21,9.04" target="#b8">[9,</ref><ref type="bibr" coords="2,84.25,155.52,12.21,9.04" target="#b9">10,</ref><ref type="bibr" coords="2,96.47,155.52,12.21,9.04" target="#b10">11]</ref> which is derived from the Text Retrieval Conference (TREC, trec.nist.gov).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Description of our Adaptive Medical Image Retrieval System</head><p>The ImageCLEF 2008 medical image retrieval test collection consists of about 66,000 medical images and annotations associated with them. This collection is a set of images and captions from Radiology and Radiographics , two RSNA journals. We have created a flexible database schema that allows us to easily incorporate new collections while facilitating retrieval using both text and visual techniques. The captions and titles in the collection are currently indexed and we continue to add indexable fields for incorporating visual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Database and Web Application</head><p>The data distribution included an XML file with the image ID, the captions of the images, the titles of the journal articles in which the image had appeared and the PubMed ID of the journal article. In addition, a compressed file containing the approximately 66,000 images was provided. We used the Ruby programming language, with the open source Ruby On Rails web application framework 1, 2 . A PostgreSQL relational database was used to store mapping between the images and the various fields associated with the image. The title, full caption and precise caption, as provided in the data distribution, were indexed. The user interface for the search engine is given below in Fig. <ref type="figure" coords="2,356.19,409.68,3.89,9.04">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 User Interface for OHSU search engine</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Processing and Analysis</head><p>The image itself has important visual characteristics such as color and texture that can help in the retrieval process. We created additional tables in the database to store image information that was created using a variety of image processing techniques in MATLAB <ref type="foot" coords="3,253.72,158.33,3.00,5.51" target="#foot_2">3</ref> . These include color and intensity histograms as well as measures of texture using gray-level co-occurrence matrices and discrete cosine transforms 8 . These features can be used to find images that are visually similar to the query image. We used this in the interactive, mixed mode to reorder the images obtained from the textual search such that images that are visually similar to an image marked relevant by the user are returned at the top of the list.</p><p>Images that may have had information about the imaging modality or anatomy or view associated with them as part of the DICOM header can lose that information when the image is compressed to become a part of a teaching or on-line collection, as the image format used by these collections is usually compressed JPEG. In previous work <ref type="bibr" coords="3,131.08,251.28,10.66,9.04" target="#b7">[8]</ref>, we described a modality classifier that can identify the imaging modality for medical images. We extended that work to the new dataset used for ImageCLEF 2008. Our system as previously described relied on a training set of modality-labeled images for its supervised learning. In 2008, we did not use any external database for training the modality classifier. Instead, a parser was written to extract the modality from the image caption. Images for which a single modality was parsed were used as the training set for the modality classifier. Grey scale images are classified into a set of modalities including x-rays, CT, MRI, ultrasound and nuclear medicine. Color image classes include gross pathology, microscopy, and endoscopy. The rest of the dataset (i.e., images for which zero or more than one modalities were parsed) was classified using the above classifier. We created two fields in the database for the modality that were indexed by our search engine. The first field contained the modality as extracted by the text parser, and the second contained the modality resulting from the classification process using visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Query Parser and Search Engine</head><p>The system presents a variety of search options to the user including Boolean OR, AND, and "exact match.". There are also options to perform fuzzy searches, as well as a custom query parser. A critical aspect of our system is the query parser, written in Ruby. Ferret, a Ruby port of the popular Lucene system, was used in our system as the underlying search engine <ref type="foot" coords="3,235.24,467.45,3.00,5.51" target="#foot_3">4</ref> . The custom query parser first performs stop word removal using a modified stop word list. The custom query parser is highly configurable, and the user has several configuration options from which to choose. The first such option is modality limitation. If the user selects this option, the query is parsed to extract the desired modality, if available. Using the modality fields described in the previous section, only those images that are of the desired modality are returned. We expect this to improve the precision as only images of the desired modality will be included within the result set. However, there might be a loss in recall if the modality extraction and classification process is not accurate.</p><p>The system is linked to the UMLS Metathesaurus. The second configuration option allows the user to perform manual or automatic query expansion using synonyms from the Metathesarus. In the manual mode, a list of synonyms is presented to the user, which the user can choose to add to the query. In the automatic mode, all synonyms of the UMLS preferred term are added to the query.</p><p>Another configuration option is the "stem and star" option, in which all the terms in the query are first stemmed. A wildcard (*) is then appended to the word to allow the search of words containing the desired root.</p><p>The last option allows the user to only send unique terms to the search engine. This can be useful when using the UMLS option, as many of the synonyms have a lot of overlap in the preferred terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Interactive mode</head><p>In addition to user-selectable search engine configuration options described above, our system provides users with other interactive features. Once a user has submitted a query using the above-described query parser, they have the option to improve the precision of their results by using an interactive re-ordering system. In this year's system, users select what they feel to be a visually representative image from their search's results. The system then attempts to re-rank the search results according to their degree of visual similarity with the "probe image" that the user selected. If the user is not satisfied with the re-ordering produced by their choice if image, they may repeat the process by selecting different probe images until they arrive at a satisfactory sorting.</p><p>To assess the visual similarity of the images within a result set, the system uses a relatively straightforward approach derived from Latent Semantic Analysis <ref type="bibr" coords="4,270.69,132.48,15.40,9.04" target="#b16">[17]</ref>. In this approach, each image in the result set is abstracted into a feature vector, which thereafter plays the same role that a document's "term vector" would play in classical LSA. We have experimented with sets of features derived from image color, texture, and frequency attributes; in our final system, the user is able to select which combinations of features they wish to use.</p><p>Once the feature vectors have been assembled for the images in a result set, they are combined into an ! n " m matrix. In this matrix, n is equal to the number of images in the result set, and m is equal to the number of features that the user has selected. Depending on the combination of features, this could be in the hundreds or low thousands. We then follow the classical LSA process, beginning by taking the Singular Value Decomposition (SVD) of our large matrix. This transforms our single matrix into three matrices that may be trivially recomposed to approximate the original matrix. The elements of one of these matrices represents the eigenvalues of the original document/term matrix; by varying the number of these elements that we use when recomposing the matrices, we may vary the fidelity of the resulting approximation.</p><p>After carrying out the SVD, we retain the first r eigenvalues of the decomposed matrix, project the probe image's m-dimensional feature vector into the new lower-dimensional space, and, finally, compute the vector distance between the probe image's new representation and that of the images in the result set. In our system, the user is able to experiment with different values for r, and may pick the one that achieves the best performance for a given set of results. The user may also quickly and easily select different images to act as probe images, and can therefore evaluate many possible result sortings.</p><p>Obviously, this system's utility is variable, and depends highly on the contents of the initial result set. In the case of a set where the desired images are simultaneously visually similar to one another and distinct from the rest of the images in the set, this visual re-sorting system works quite well. However, in the case where the desired images are visually different from one another, or where all of the results (including the non-relevant ones) are visually similar, this re-sorting system is not very useful.</p><p>For example, a result set consisting entirely of ultrasound images will not be improved very much by resorting. In fact, in this particular case, resorting the result set may hurt its precision, as any ordering imposed by our textual search engine will be lost. On the other hand, a result set in which most of the relevant images are ultrasounds and most of the non-relevant images are x-rays could benefit from being re-ordered based on visual similarity to a user-selected probe image.</p><p>Our present system requires the user to select a combination of features to use. This is clearly sub-optimal, and our future work could include improved feature selection methods. Similarly, the user is currently able to change the number of eigenvalues used by the algorithm. While this is a powerful tool for tuning the algorithm's performance, it is also something that we would ultimately like to automate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs Submitted</head><p>We submitted a total of 10 runs. The search options for the different runs are provided in table 1. These runs included textual and mixed, automatic and interactive options. Although the ImageCLEF2005-2007 collection with qrels and topics was available, we did not use any external training data.</p><p>Three automatic text-based runs were submitted with different custom parsing options including the use of UMLS term expansion. We also submitted four mixed, automatic runs. The modality classification based on the text parsing of the caption and the classification based on visual features was used to improve the precision of the search.</p><p>While the majority of our runs were automatic in nature, several of ours were interactive. In the first such run (ohsu_int_2), the user chose different combinations of options for each topic and added terms based on the list provided using the UMLS query expansion option. Two runs using the interactive result sorting system were submitted. The first such run, "ohsu_sdb_lsa", used the result sorting system on every topic. The second run, "ohsu_sdb_full_interactive", only used the result sorting system on topics where the user thought that it would be beneficial to the run's precision. This second run also featured much more intervention on the part of the user, who took full advantage of our retrieval system's interactive nature and enabled or disabled options and features as needed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Table <ref type="table" coords="5,108.79,323.28,4.92,9.04" target="#tab_1">2</ref> contains a subset of the official performance metrics for the OHSU runs. We have also included the average of these metrics for all runs, the highest measure in each category as well as data from the best run (based on MAP) in the 2008 campaign. OHSU performed reasonably well, especially among the runs that did not use any external training data. All but two of our runs performed better than the average for all measures. As described in the previous section, our systems have been designed to improve precision, perhaps at the expense of recall. Our custom parsing improved the mean average precision as well as the early precision, as can be seen in the text runs. The use of modality parsing and detection improved the MAP as well as the early precision. All our mixed runs performed better than the corresponding text runs. OHSU-ohsu_mod_pars2_sp.txt had the highest early precision (up to P30) of all official runs. OHSU had submitted four of the top ten mixed runs, as sorted using the precision as 10.</p><p>The use of term expansion with UMLS increased the recall. We had submitted runs after the creation of the pools. This penalizes the runs as potentially fewer of the images are judged. One of these runs had the highest bpref, a measure that is robust in the case of incomplete judgments.</p><p>The performance of the first LSA run (ohsu_sdb_lsa) was unsatisfactory: as described earlier, there are many situations in which the original result sorting provided by our textual search engine was adequate, and changing it by means of our interactive visual re-sorting system damaged a topic's precision. The second LSA run, "ohsu_sdb_full_interactive", performed much better. In fact, its p10 was greater than that of the overall competition winner's (0.46 for "ohsu_sdb_full_interactive" vs 0.43 for "SINAI-sinai_CT_Mesh_Fire20"). The third interactive run, where the parsing mode and UMLS term expansion was performed interactively also performed quite well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>Our image retrieval system built using open-source tools is a flexible platform for evaluating various tools and techniques in image processing as well as natural language processing for medical image retrieval. The use of visual information to automatically extract the imaging modality is a promising approach for the ImageCLEFmed campaign. The use of UMLS term expansion, query parsing and modality detection all add value over the basic Ferret (Lucene) search engine. We will continue to improve our image retrieval system by adding more image tags using automatic visual feature extraction. Our next goal is to annotate the images with the their anatomical location and view attributes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,83.32,418.66,435.12,304.80"><head></head><label></label><figDesc></figDesc><graphic coords="2,83.32,418.66,435.12,304.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,76.84,98.19,432.68,177.43"><head>Table 1 .</head><label>1</label><figDesc>OHSU runs submitted</figDesc><table coords="5,76.84,120.87,432.68,154.76"><row><cell></cell><cell>Text/visual/</cell><cell>automatic/manual/</cell><cell></cell><cell></cell></row><row><cell>Run Name</cell><cell>mixed</cell><cell>interactive</cell><cell>Data used</cell><cell>Parsing options</cell></row><row><cell>OHSU-text_or_1</cell><cell>text</cell><cell>automatic</cell><cell>full caption</cell><cell>none</cell></row><row><cell>ohsu_text_3</cell><cell>text</cell><cell>automatic</cell><cell>full caption, title</cell><cell>custom</cell></row><row><cell>ohsu_text_umls_4</cell><cell>text</cell><cell>automatic</cell><cell>full caption, title</cell><cell>custom, umls, unique</cell></row><row><cell>ohsu_vis_mod_3</cell><cell>mixed</cell><cell>automatic</cell><cell>full caption</cell><cell>custom, modality</cell></row><row><cell>OHSU-ohsu_mod_pars2_sp.txt</cell><cell>mixed</cell><cell>automatic</cell><cell>full caption</cell><cell>custom, modality</cell></row><row><cell>ohsu_vis_mod_5</cell><cell>mixed</cell><cell>automatic</cell><cell>full caption, title</cell><cell>custom, modality</cell></row><row><cell>ohsu_vis_mod_umls_4</cell><cell>mixed</cell><cell>automatic</cell><cell>full caption</cell><cell>custom, modality, umls</cell></row><row><cell>OHSU-ohsu_sdb_lsa.txt</cell><cell>mixed</cell><cell>interactive</cell><cell>full caption</cell><cell>custom, modality</cell></row><row><cell>ohsu_sdb_full_interactive.txt</cell><cell>mixed</cell><cell>interactive</cell><cell>full caption, title</cell><cell>custom, modality</cell></row><row><cell>ohsu_int_2</cell><cell>mixed</cell><cell>interactive</cell><cell>precisise caption, title</cell><cell>custom, modality, umls</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,72.04,369.87,423.18,205.99"><head>Table 2 .</head><label>2</label><figDesc>Metrics of OHSU runs submitted</figDesc><table coords="5,76.84,396.15,418.38,179.72"><row><cell>Run Name</cell><cell>MAP</cell><cell>Bpref</cell><cell>P10</cell><cell>P30</cell><cell>Recall</cell></row><row><cell>OHSU-text_or_1</cell><cell>0.11</cell><cell>0.18</cell><cell>0.26</cell><cell>0.24</cell><cell>0.41</cell></row><row><cell>ohsu_text_3</cell><cell>0.15</cell><cell>0.23</cell><cell>0.31</cell><cell>0.22</cell><cell>0.52</cell></row><row><cell>ohsu_text_umls_4</cell><cell>0.20</cell><cell>0.30</cell><cell>0.29</cell><cell>0.25</cell><cell>0.57</cell></row><row><cell>ohsu_vis_mod_3</cell><cell>0.15</cell><cell>0.25</cell><cell>0.32</cell><cell>0.24</cell><cell>0.53</cell></row><row><cell>OHSU-ohsu_mod_pars2_sp.txt</cell><cell>0.21</cell><cell>0.30</cell><cell>0.55</cell><cell>0.46</cell><cell>0.45</cell></row><row><cell>ohsu_vis_mod_5</cell><cell>0.23</cell><cell>0.33</cell><cell>0.38</cell><cell>0.29</cell><cell>0.58</cell></row><row><cell>ohsu_vis_mod_umls_4</cell><cell>0.23</cell><cell>0.35</cell><cell>0.37</cell><cell>0.28</cell><cell>0.60</cell></row><row><cell>OHSU-ohsu_sdb_lsa.txt</cell><cell>0.10</cell><cell>0.20</cell><cell>0.27</cell><cell>0.27</cell><cell>0.47</cell></row><row><cell>ohsu_sdb_full_interactive.txt</cell><cell>0.18</cell><cell>0.29</cell><cell>0.46</cell><cell>0.33</cell><cell>0.47</cell></row><row><cell>ohsu_int_2</cell><cell>0.22</cell><cell>0.31</cell><cell>0.49</cell><cell>0.39</cell><cell>0.46</cell></row><row><cell>average</cell><cell>0.14</cell><cell>0.20</cell><cell>0.28</cell><cell>0.24</cell><cell>0.40</cell></row><row><cell>best in category</cell><cell>0.29</cell><cell>0.35</cell><cell>0.55</cell><cell>0.46</cell><cell>0.66</cell></row><row><cell>SINAI-sinai_CT_Mesh_Fire20</cell><cell>0.29</cell><cell>0.33</cell><cell>0.43</cell><cell>0.40</cell><cell>0.62</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,77.32,733.12,92.31,8.16"><p>http://www.ruby-lang.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,77.32,743.92,98.38,8.16"><p>http://www.rubyonrails.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,77.32,733.12,100.84,8.16"><p>http://www.mathworks.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,77.32,743.92,106.83,8.16"><p>http://ferret.davebalmain.com</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We acknowledge the support of <rs type="funder">NLM</rs> <rs type="grantName">Training</rs> Grant <rs type="grantNumber">1T15 LM009461</rs> and <rs type="funder">NSF</rs> Grant <rs type="grantNumber">ITR-0325160</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_F6QTwMN">
					<idno type="grant-number">1T15 LM009461</idno>
					<orgName type="grant-name">Training</orgName>
				</org>
				<org type="funding" xml:id="_jvFctg7">
					<idno type="grant-number">ITR-0325160</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,75.43,382.72,447.67,8.16;6,85.48,393.04,154.88,8.16" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,184.63,382.72,308.17,8.16">Advancing biomedical image retrieval: development and analysis of a test collection</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,499.03,382.72,24.07,8.16;6,85.48,393.04,74.52,8.16">J. Am. Med. Inform. Assoc</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="488" to="496" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.43,403.60,447.74,8.16;6,85.48,413.92,251.17,8.16" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,217.00,403.60,220.42,8.16">Content-Based Image Retrieval at the End of the Early Years</title>
		<author>
			<persName coords=""><forename type="first">Awm</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,443.80,403.60,79.37,8.16;6,85.48,413.92,152.76,8.16">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.43,424.24,447.92,8.16;6,85.48,434.56,75.97,8.16" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,180.29,424.24,233.50,8.16">Medical Image Databases: A Content-Based Retrieval Approach</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">D</forename><surname>Tagare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jaffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,420.82,424.24,102.54,8.16">J. Am. Med. Inform. Assoc</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="184" to="198" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.43,445.12,447.61,8.16;6,85.48,455.44,264.81,8.16" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,204.83,445.12,318.21,8.16;6,85.48,455.44,139.99,8.16">Automated storage and retrieval of thin-section CT images to assist diagnosis: System description and preliminary assessment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Aisen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Broderick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,231.43,455.44,35.95,8.16">Radiology</title>
		<imprint>
			<biblScope unit="volume">228</biblScope>
			<biblScope unit="page" from="265" to="270" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.43,465.76,448.03,8.16;6,85.48,476.32,224.26,8.16" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,236.32,465.76,283.71,8.16">Towards a computer-aided diagnosis system for pigmented skin lesions</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schmid-Saugeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guillod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,85.48,476.32,164.86,8.16">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="65" to="78" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.43,486.64,447.69,8.16;6,85.48,496.96,269.86,8.16" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,277.00,486.64,246.12,8.16;6,85.48,496.96,132.59,8.16">A review of content-based image retrieval systems in medicineclinical benefits and future directions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Michoux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bandon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,223.72,496.96,69.36,8.16">Int. J. Med. Inform</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.43,507.52,447.87,8.16;6,85.48,517.84,255.97,8.16" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,222.99,507.52,276.22,8.16">Medical image retrieval and automated annotation: OHSU at ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="6,85.48,517.84,186.08,8.16">Springer Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<biblScope unit="page" from="660" to="669" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.43,528.16,448.05,8.16;6,85.48,538.72,260.00,8.16" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,202.12,528.16,321.36,8.16;6,85.48,538.72,56.29,8.16">Automatic Image Modality Based Classification and Annotation to Improve Medical Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hersh</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="1334" to="1338" />
			<pubPlace>MedInfo; Brisbane, Australia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.43,549.04,447.69,8.16;6,85.48,559.36,24.10,8.16" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,174.31,549.04,247.17,8.16">Cross-language evaluation forum: objectives, results, achievements</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,428.94,549.04,54.88,8.16">Inform Retriev</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="7" to="31" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,79.60,569.68,443.77,8.16;6,85.48,580.24,437.60,8.16;6,85.48,590.56,302.50,8.16" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,308.92,569.68,214.46,8.16;6,85.48,580.24,309.10,8.16">Overview of the ImageCLEFmed 2006 medical retrieval annotation tasks, Evaluation of Multilingual and Multi-modal Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hersh</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,401.39,580.24,121.68,8.16;6,85.48,590.56,147.91,8.16">Seventh Workshop of the Cross-Language Evaluation Forum, CLEF 2006</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="595" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,79.60,600.88,443.61,8.16;6,85.48,611.44,202.16,8.16" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,184.36,600.88,311.56,8.16">Evaluation Axes for Medical Image Retrieval Systems -The ImageCLEF Experience</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,502.60,600.88,20.62,8.16;6,85.48,611.44,88.29,8.16">ACM Int. Conf. on Multimedia</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">November, (2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,79.60,621.76,443.45,8.16;6,85.48,632.08,146.60,8.16" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="6,309.88,621.76,208.34,8.16">Medical image categorization with MedIC and MedGIFT</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Florea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rogozan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geissbühler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Darmoni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIE</publisher>
		</imprint>
		<respStmt>
			<orgName>Medical Informatics Europe</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,79.60,642.64,433.18,8.16" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,211.00,642.64,190.66,8.16">MedPost: a part-of-speech tagger for biomedical text</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rindflesch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wilbur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,403.72,642.64,54.14,8.16">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,79.60,652.96,444.06,8.16;6,85.48,663.28,111.20,8.16" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="6,167.86,652.96,290.44,8.16">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,464.73,652.96,58.93,8.16;6,85.48,663.28,22.49,8.16">Int. J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,79.60,673.60,364.29,8.16" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="6,128.44,673.60,155.08,8.16">Netlab: Algorithms for Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">T</forename><surname>Nabney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>London, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,79.60,684.16,434.35,8.16" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="6,125.56,684.16,205.14,8.16">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,336.76,684.16,93.09,8.16">Int. J. of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,79.60,694.48,443.80,8.16;6,85.48,704.80,438.16,8.16;6,85.48,715.36,311.15,8.16" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="6,416.91,694.48,106.49,8.16;6,85.48,704.80,230.61,8.16">Information retrieval using a singular value decomposition model of latent semantic structure</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Streeter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lochbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,321.80,704.80,201.83,8.16;6,85.48,715.36,284.71,8.16">SIGIR &apos;88: Proceedings of the 11th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
