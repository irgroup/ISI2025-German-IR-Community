<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,120.36,146.21,362.25,18.08">XRCE&apos;s Participation to ImageCLEF 2008</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,155.52,181.27,44.99,10.46"><forename type="first">J</forename><surname>Ah-Pine</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 ch. de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,208.65,181.27,46.94,10.46"><forename type="first">C</forename><surname>Cifarelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 ch. de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.65,181.27,51.98,10.46"><forename type="first">S</forename><surname>Clinchant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 ch. de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.43,181.27,44.17,10.46"><forename type="first">G</forename><surname>Csurka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 ch. de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,389.28,181.27,58.20,10.46"><forename type="first">J</forename><forename type="middle">M</forename><surname>Renders</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 ch. de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,120.36,146.21,362.25,18.08">XRCE&apos;s Participation to ImageCLEF 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5ECF8CBBC594460B1D9D0878E2D661CB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Cross-media information retrieval, Trans-media relevance feedback, diversity-based re-ranking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year, our participation to ImageCLEF 2008 (Photo Retrieval sub-task) was motivated by trying to address three different problems: visual concept detection and its exploitation in a retrieval context, multimedia fusion methods for improved retrieval performance and diversity-based re-ranking methods. From a purely visual perspective, the representation based on Fisher vectors derived from a generative mixture model appeared to be efficient for both visual concept detection and content-based image retrieval. From a multimedia perspective, we used an intermediate fusion approach, based on cross-media relevance feedback that can be seen as a multigraph-based query regularization method with alternating steps. The combination allowed to improve both mono-media systems by more than 50% (relative). Finally, as one of main goals of the organizers was to promote both relevance and diversity in the retrieval outputs, we designed and assessed several re-ranking strategies that turned out to preserve standard retrieval performance (such at precision at 20 or mean average precision) while significantly decreasing the redundancy in the top documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In ImageCLEFphoto 2007, the issue we wanted to address was the semantic gap between text and image in retrieval tasks. Indeed, the challenges proposed by ImageCLEF these last years and particularly in the context of the Photo retrieval task, are well-adapted to investigate that kind of problems as we have at our disposal images that are particularly well-described from a textual point of view. In the photo retrieval task, the multi-media objects are constituted of some text and some image that correspond to two different kinds of information from a semantic point of view. Thus, the scientific challenge was to exploit in an efficient manner these two types of information in order to improve the search results.</p><p>In last year's session, we addressed the question of combining textual and visual information by developing an intermediate fusion approach which allows to go further than late or early fusion methods <ref type="bibr" coords="2,158.61,110.53,9.96,10.46" target="#b3">[4]</ref>. The results we obtained showed that our proposal allows to outperform either monomedia-based retrieval or classical late or early fusion-based retrieval.</p><p>In ImageCLEFphoto 2008, however, the main goal is to promote diversity among the first search results. We address this issue using a two-step approach. The first step is to ignore the question of diversity. In other words, we first try to find the most relevant objects using the material introduced in <ref type="bibr" coords="2,190.74,170.31,9.96,10.46" target="#b3">[4]</ref>. Then, in a second step, we re-rank the first relevant objects by taking into account their mutual similarities in order to avoid redundancy and thus to promote diversity.</p><p>The present working note explains the different approaches that we investigated for designing and developing methods that tackle the challenges proposed in ImageCLEFphoto 2008. This was an opportunity to both enhance the system we developed last year and to implement methods which aims at incorporating diversity into the search results.</p><p>The document is organized as follows.</p><p>Section 2 is devoted to standard retrieval based on the textual part of the photos. In particular, the aim of the experiments reported in this section was to use external resources for enriching the textual description of both the objects and the queries. Two kinds of resources were used: the first one is rather linguistic and consists for example, in adding synonyms of terms; the second one is rather "cross-media oriented" as it aims at adding visual concepts obtained after processing the images. Such visual concepts are typically the categories found by an image categorizer system.</p><p>In section 3, we detail the different experiments we made in the context of image retrieval and classification. First we briefly remember the representation of images with Fisher Vectors (subsection 3.1). In subsection 3.2 we describe our image categorization method used in the ImageClef Visual Concept Detection subtask and in subsection 3.3 our image retrieval system applied to the ImageCLEFPhoto task is detailed. In subsection 3.4 we propose different strategies to incorporate diversity (i.e. to reduce redundancy) in the top retrieved photos, and in the last subsection 3.4.3, we give some further precisions concerning the submitted AUTO-IMAGE runs.</p><p>In section 4, we present the system we developed for multi-media information retrieval. First, we briefly recall in subsection 4.1 the approach we defined in <ref type="bibr" coords="2,364.16,421.37,10.52,10.46" target="#b3">[4]</ref> concerning intermediate fusion between different mono-modal sources. Second, in subsection 4.2, we explain the different methods we implemented for promoting diversity into the multi-media search results, and in subsection 4.2.3 we present the characteristics of the different AUTO-TXTIMG runs we have submitted.</p><p>The last section is, as usual, devoted to a set of conclusions we can draw from the experimental results of ImageCLEF 2008.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Text Retrieval</head><p>One axis of research that we wanted to investigate in this session of ImageCLEF was the use of external resources (thesauri) for enriching the textual side of the objects (image description / queries) of the task. The idea is to exploit such resource to compensate for:</p><p>• the relative sparsity of the textual representation of the photos (even if, this year, the textual part of the photos included a more detailed description of its content);</p><p>• the gap between the lexical fields of these descriptions and the queries : queries are expressed in a more abstract way than the factual description of the photos.</p><p>Another issue that we wanted to address was the use of the visual concepts provided by the organizers as extra "textual words", refining the original textual representation of the photo by higher-level visual information.</p><p>As basic text retrieval engine, we used the LEMUR Toolkit for Information Retrieval <ref type="bibr" coords="2,494.74,679.37,14.61,10.46" target="#b10">[11]</ref>, adopting the Language Modeling framework <ref type="bibr" coords="2,283.31,691.33,15.50,10.46" target="#b15">[16]</ref> to retrieve and rank relevant objects with respect to the textual part of the query. For each photo, the title, location and description fields were simply concatenated to form a single document; these documents were then linguistically preprocessed (lemmatization and stop-word removal), before being indexed by the LEMUR tool. Dirichlet smoothing <ref type="bibr" coords="2,181.30,739.15,15.50,10.46" target="#b19">[20]</ref> was applied in the retrieval phase, with a parameter which is directly related to the average document length (this parameter choice strategy was also adopted when using enriched documents). Documents are ranked by decreasing similarity with respect to the query, the similarity being defined as the cross-entropy between the query model and the document (smoothed) language model.</p><p>The first variant we developed consisted in exploiting the English Open Office thesaurus<ref type="foot" coords="3,495.92,157.28,3.97,7.32" target="#foot_0">1</ref> to enrich the textual description of the photos and/or the queries. Several strategies can be chosen. We chose the following ones:</p><p>• Document enrichment: we added all the synonyms and broader terms to the terms of the original description, when they are covered by some thesaurus entry; as we wanted to give more weight to the original terms, the latter ones are artificially replicated 15 times.</p><p>• Query enrichment: we added all the synonyms and narrower terms to the terms of the original description, when available; as we wanted to give more weight to the original terms, the latter ones are artificially replicated 5 times.</p><p>Note that we also simultaneously enriched both the queries and the documents, but this resulted in performance deterioration (too much noise introduced).</p><p>As pseudo-relevance feedback (PRF) is another way to do query expansion, we systematically ran experiments with and without pseudo-relevance feedback for each setting (baseline, document enrichment, query enrichment). The ten top terms of the ten top documents were used to expand the initial query language model by convex linear combination (coefficient =0.6 for the feedback model); query model updating was based on the mixture model method <ref type="bibr" coords="3,413.75,361.60,14.61,10.46" target="#b18">[19]</ref>. The performance (Mean Average Precision and Precision@20) is given in Table <ref type="table" coords="3,361.46,373.55,3.87,10.46">1</ref> It clearly appears that combining document enrichment by thesaurus and query expansion by PRF (using the thesaurus-enriched documents in the first feedback phase) gives the best results. Doing query semantic enrichment followed by PRF (using the thesaurus-enriched query in the first feedback phase) gives slightly worse results. In any case, the use of this external resource is beneficial with respect to a standard PRF query expansion.</p><p>The second variant we developed aimed at assessing the benefits of introducing the Visual Concepts, as produced by two generic Visual Concept Detectors (we refer to the subsection 3.2 of this note and to the Visual Concept Detection sub-task of ImageCLEF 2008 for further information). These two detectors are designated by XRCE's and RWTH's detectors respectively. The approach we adopted was quite simple: we added to the original textual description the terms corresponding to the visual classes (e.g. indoor, outdoor, building, sky, night, animal, etc.) of the query images, if the associated confidence score was above some threshold (0.65 in our case; see subsection 3.2 for more details). We did the same for the photo annotations: both the queries and the documents of the collection were enriched by the concepts of the associated image(s). This could be considered as a very simplistic way of doing multi-media retrieval.</p><p>The performances (Mean Average Precision and Precision@20) are given in Table <ref type="table" coords="3,464.49,673.82,3.87,10.46" target="#tab_1">2</ref>.</p><p>Clearly, the use of the visual concepts increases the retrieval performance, even if the inclusion of visual concepts is done in a rather simplistic way. But, to anticipate a little bit the results of the multi-media retrieval section, it will turn out that this advantage is lost when we use other, more complex multi-media fusion mechanisms, based on lower-level features than the visual concepts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Image Retrieval</head><p>In this section, we first describe briefly our image representation based on Fisher Vectors (for more details see <ref type="bibr" coords="4,138.30,257.60,10.52,10.46" target="#b3">[4,</ref><ref type="bibr" coords="4,152.43,257.60,11.62,10.46" target="#b14">15]</ref>). The same representation was used in the Visual Concept Detection sub-task (section 3.2) and ImageCLEFPhoto (section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fisher Vectors for Images</head><p>As image representation, we use the Fisher Vector as proposed in <ref type="bibr" coords="4,386.66,315.57,14.61,10.46" target="#b14">[15]</ref>. This is an extension of the bag-of-visual-words (BOV) representation; the main idea is to characterize the image with the gradient vector derived from the generative probability model (a visual vocabulary modeled by a GMM in our case). This representation can then be subsequently fed to a discriminative classifier for categorization, or used to compute similarities between images for retrieval.</p><p>The generative probability model in our case is the Gaussian Mixture Model (GMM) which approximates the distribution of the low-level features in images, where each Gaussian component can be seen as a visual word.</p><p>If we denote the set of parameters of the GMM by Φ = {w i , µ i , Σ i , i = 1...N } (w i , being the mixture's weight), we can compute the gradient vector of the likelihood ∇ Φ log p(I|Φ) that the image was generated by the model Φ. This gradient of the log-likelihood describes the direction in which parameters should be modified to best fit the data (image features). One of its advantages is that it transforms a variable length sample (number of local patches in the image) into a fixed length representation (which we will call Fisher Vector) whose size is only dependent on the number of parameters in the model (|Φ|).</p><p>Before feeding these vectors to a classifier or computing similarities between images, each vector is first normalized using the Fisher Information matrix F Φ (see <ref type="bibr" coords="4,363.06,506.86,15.50,10.46" target="#b14">[15]</ref> for the computational details):</p><formula xml:id="formula_0" coords="4,248.27,526.60,264.73,15.42">f I = F -1/2 Φ ∇ Φ log p(I|Φ)<label>(1)</label></formula><p>with</p><formula xml:id="formula_1" coords="4,194.47,568.53,189.70,12.93">F Φ = E X P ∇ Φ log p(I|Φ)∇ Φ log p(I|Φ) T .</formula><p>and then re-normalized to have an L1-norm equal to 1.</p><p>Note that we used in our experiments two types of low-level local features extracted both on regular grids at different scales (see <ref type="bibr" coords="4,248.02,614.46,10.52,10.46" target="#b3">[4]</ref> for further details). The first feature set is based on local histograms of orientations (referred as texture) and the second one is based on local RGB statistics (referred as color). This led to two visual vocabularies, so that we have two Fisher Vectors per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual Concept Detection</head><p>The Visual Concept Detection task of ImageClef had the objective to identify visual concepts in users' photos <ref type="bibr" coords="4,151.52,708.29,14.61,10.46" target="#b17">[18]</ref>. The training images were labeled with 17 visual concepts: indoor, outdoor, person, day, night, water, road or pathway, vegetation, tree, mountains, beach, buildings, sky, sunny, partly cloudy, overcast and animal. The goal was to indicate the presence or the absence of these concepts. In spite of the fact that the concepts were presented in a hierarchy, we have not used it (neither in training nor in testing), but considered the task as a multi-class multi-label image categorization problem. Using the Fisher Vector image representation, we trained linear and non-linear (kernel based) classifiers, where each classifier was designed to detect the presence of one of the 17 concepts.</p><p>The binary one-against-all linear classifiers were trained directly on labeled normalized Fisher Vectors. As linear classifier, we used our own implementation of Sparse Logistic Regression (SLR) <ref type="bibr" coords="5,90.01,437.96,14.61,10.46" target="#b9">[10]</ref>, i.e. logistic regression with a Laplacian prior.</p><p>In the case of non-linear (kernel-based) classifiers, we use the following kernel:</p><formula xml:id="formula_2" coords="5,219.04,470.65,293.96,31.41">K(f I , f J ) = T t=1 || fI -ft || 1 • || fJ -ft || 1<label>(2)</label></formula><p>where Note that, as we used two types of low-level local feature vectors (color and texture), we trained two classifiers for each concepts. We transformed the classifier scores s in probabilities with the sigmoid mapping (1 + exp(s)) -1 . To decide about the presence or the absence of a concept, the mean of the two probabilities (color and texture) was compared to a threshold (0.65 in our case).</p><p>Both systems, the linear one and the kernel-based one, performed well compared to other systems (see figure <ref type="figure" coords="5,174.65,644.04,3.87,10.46" target="#fig_0">1</ref>), the kernel-based system slightly over-performing the linear system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Content Based Image Retrieval</head><p>In the case of image retrieval, we use also the Fisher Vector representation of the images. To obtain the similarity between two images I and J, we first concatenate the color and the texture Fisher vectors before computing the L1-norm of the difference between the concatenated vectors:</p><formula xml:id="formula_3" coords="5,165.72,734.82,347.28,23.18">sim(I, J) = norm max -||f I -f J || 1 = norm max - i |f i I -f i J | (3)</formula><p>where f i are the elements of the concatenated vector f and norm max = 2. Given a query image, we can simply rank the images of the database according to this similarity measure. Nevertheless, in the ImageClef Photo Retrieval Task, we have not one but three query images. Therefore, the question is how to combine them for a better retrieval. We investigated three different strategies:</p><p>• I1 : We considered the mean of the three Fisher Vectors (this can be seen as the concatenation of the three image in a single one) and used this mean Fisher Vector to query the database.</p><p>• I2 : The database images were ranked according to each of the three images independently and the three ranked list was combined using round-robin type selection (i.e. intermixing the three lists), of course eliminating the repetitions of the same images.</p><p>• I3 : We combine the three similarity scores (with respect to each image of the query) by averaging the scores after normalisation (by studentization, a.k.a. z-scores). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Incorporating Diversity</head><p>One of the aims of ImageCLEFPhoto 2008 is to promote both relevance and diversity into the first retrieved elements. Therefore, in order to introduce diversity at the top level (e.g top k=20 images), we try to find images that are representatives of groups of closely related images (assuming that they contain redundant information). The idea is to bring these representative images to the top of the list and to push down the images which are redundant with respect to images already selected.</p><p>To achieve the selection of such representative images, we tested two different strategies. The first one uses explicit image clustering (section 3.4.1) and selects in each cluster the most relevant image according to the original ranking described in section 3.3. The second approach is based on image density estimation (see sub-section 3.4.2), where a number of representative images are automatically detected by seeking for local maximum of the density function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Explicit Image Clustering</head><p>In order to cluster the top N ranked images using the Fisher Vector representation, we proceed as follows. First, we build the K-Nearest Neighbors (K-NN) graph: nodes i and j are connected if the image I j is amongst the K-NN neighbour of the image I i based on the similarity measure defined by <ref type="bibr" coords="6,103.66,662.37,11.62,10.46" target="#b2">(3)</ref>. Edges are weighted by the corresponding similarity measure. As the K-NN relationship is not symmetric, the weighted adjacency matrix is asymmetric and the resulting graph is directed.</p><p>Then, Probabilistic Latent Semantic Analysis <ref type="bibr" coords="6,305.02,686.29,10.52,10.46" target="#b7">[8]</ref> (PLSA) -some form of discrete Probabilistic Principal Component Analysis -is performed on the weighted adjacency matrix. Note that PLSA is known to be an efficient clustering method to handle sparse matrices (as in our case for K much smaller than N ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Density Based Representative Image Detection</head><p>As an alternative to explicit clustering, we experimented a second approach that relies on direct identification of peaks in some density function estimated from the top N images. Among different possible density estimation methods, we used a modified version of the method proposed in <ref type="bibr" coords="7,499.72,152.84,9.96,10.46" target="#b5">[6]</ref>. The main idea is to define the density value at each point (image in our case) I i as the number of elements (images) that fall in the hypersphere with center in I i and radius r. Once again, Fisher Vector representation and the L1-norm were used to define the distance between images.</p><p>Usually, the density in one point is estimated considering only a fixed radius. We propose here a variant that allows us to obtain a smoother density estimation that better fits our scope. This can be obtained, by considering not only one possible radius of the hypersphere but a predefined set of them {r 1 , . . . , r R } and compute the density by:</p><formula xml:id="formula_4" coords="7,243.77,255.91,269.23,31.76">d(I i ) = R k=1 n(I i , r k )/V (r k )<label>(4)</label></formula><p>where n(I i , r k ) is the number of images that fall in the hypersphere of radius r k with volume</p><formula xml:id="formula_5" coords="7,90.00,306.82,27.95,11.35">V (r k ).</formula><p>The local maxima of the density function are considered "modes" (i.e. local agglomeration of images that generate a local maximum in the density function) and used as representative images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Description of AUTO-IMAGE runs</head><p>As mentioned above, to introduce diversity, we first rank the images according to one of the methods described in section 3.3 and then re-rank the list using either clustering or density estimation on the top N elements of this list.</p><p>As the original list is obtained as an answer to a query, the hope is that selected images represent groups of relevant images. Nevertheless this is not necessarily verified. Furthermore, if N gets larger, the probability to get groups of similar but non relevant images<ref type="foot" coords="7,458.86,433.31,3.97,7.32" target="#foot_1">2</ref> in the top list increases. Therefore the N parameter was selected relatively small (N=100) trying to reduce maximally these cases.</p><p>• N being small, we had to use a small number of clusters (we used C=10) in the first diversification strategy. In each cluster we considered as representative image the one that had the best rank in the list I3 (see section 3.3). The selected ten images were placed at the top of the list keeping the order of the other images unchanged. The name of this run was xrce img cl10 of.</p><p>• Using a small C allowed to diversify only the top C=10 elements. Of course we can increase this number by increasing C and N, but as we have not submitted these runs, we have no result for them. As an alternative, we experimented another strategy which is based on the combination of l ranked lists (l is typically 2 or 3 in our particular setting). The main idea is to consecutively cluster the top N elements of each of the lists and concatenate the C &lt;= l • C representative images after eliminating images that occurred more than one time. For example we cluster the top N images in I3 and I2 and concatenate the 20 images selected as above in the 2x20 clusters. If the same image was selected twice, we consider it only once. We ranked them according to their fused similarity score (used to obtain list I3) and place them at the top. The rest of the images in the list I3 is placed after them. This run was submitted under the name xrce img cl10 of rg. Similarly, the runs xrce img cl10 ff of and xrce img cl10 of ff rg were obtained by this strategy.</p><p>• In the case of density-based representative image detection, we again used only the top N=100 images which led to relatively few (4-5) local maxima in the density function. Therefore, we consecutively used the three lists Ii and concatenated the modes (representative images) as above into a single set. These images were reordered according to their ranks in I3 and placed at the top, while the remaining images in I3 put just after them. Two such runs were submitted: xrce img knn10 mRm, with R=4 and R=5 corresponding to two different sets of radii (see section 3.4.2).</p><p>• Finally xrce img knn10 m5 is an alternative to xrce img knn10 m5m, where we simply replaced the image corresponding to the local maxima by one of its 5 nearest neighbour, if this latter had a better combined relevance score (rank in I3).</p><p>Table <ref type="table" coords="8,132.40,210.16,4.98,10.46" target="#tab_5">4</ref> shows the results obtained with these strategies. Compared to table <ref type="table" coords="8,441.69,210.16,4.98,10.46" target="#tab_3">3</ref> we can deduce that the CR20 was not always improved, or even where it was improved the improvement was not too big. The reason is probably that the clustering criteria are purely visual and we cannot ensure that the desired cluster diversity source (such as different cities, states, sports, etc) was taken into account. 4 Multimedia Retrieval</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Standard Retrieval, without Diversity Seeking Goal</head><p>This section deals with the standard retrieval problem, where we don't care about the object redundancy/diversity in the returned ranked list. For the standard problem, the method we adopted this year is quite similar to the one we used last year (see <ref type="bibr" coords="8,400.84,503.09,10.30,10.46" target="#b3">[4]</ref>). It will be the basis for diversity-based extensions, as explained in the next section. For completeness and ease of understanding, we describe hereafter the method we used last year. This method can be seen as a graph-based query regularization method (see <ref type="bibr" coords="8,450.58,538.96,10.30,10.46" target="#b4">[5]</ref>), where we simultaneously use two graphs: the one representing visual similarities between objects, and the one representing textual similarities between objects (objects include the query and the annotated photos of the collection). As explained in the previous sections, the cross-entropy between the language models of two objects (possibly expanded by different mechanisms such as thesaurus enrichment and/or pseudo-relevance feedback) is used as textual similarity measure, while a cosinelike norm between the "Fisher Vector" representation of two images is employed as visual similarity measure.</p><p>Let us introduce some notations. Let us call w t (q t ) the similarity vector of a given textual query q t with respect to the documents of the collection. This similarity vector is typically obtained by normalizing the retrieval scores obtained by the methods described in the section devoted to the purely textual retrieval (we used z-score normalization). Let us call W t the textual similarity matrix between all documents of the collection, obtained in the same way. Similarly, let us define w i (q i ) and W i , respectively the normalized similarity vector of an image query q i with respect to the images of the collection, and the pair-wise similarity matrix between the images of the collection. They are obtained by the methods explained in the previous section. Let us denote by κ(w, k) the thresholding function that puts to zero all values of the vector w that are lower than the top k values and keeps all other components to their initial value. The cross-media similarity measures that we developed last year could be expressed as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><formula xml:id="formula_6" coords="9,104.94,273.94,169.74,31.15">• Sim IM G-T XT (q i ) = κ(w i (q i ), k i ).W t • Sim T XT -IM G (q t ) = κ(w t (q t ), k t ).W i</formula><p>where the '.' symbol designates the standard matrix product. Note that Sim IM G-T XT does not rely on the textual part of the query, while Sim T XT -IM G does not rely on the visual part of the query. Conversely, Sim IM G-T XT does not rely on the visual part of the documents (photos) of the collection, while Sim T XT -IM G does not rely on the textual part of the images of the collection. This can be understood as query score regularization through a two-step diffusion process, the first step being performed in one mode and the second step being performed in the other one.</p><p>Note that we could design in the same way mono-media similarity measures that amount to do some pseudo-relevance feedback:</p><formula xml:id="formula_7" coords="9,104.94,418.68,169.95,31.15">• Sim T XT -T XT (q t ) = κ(w t (q t ), k t ).W t • Sim IM G-IM G (q i ) = κ(w i (q i ), k i ).W i</formula><p>We could combine all these new similarity measures, for instance by linear combination, with the basic mono-media ones. We tried several combinations, namely:</p><formula xml:id="formula_8" coords="9,104.94,491.69,300.25,52.44">• C1 : λ 1 w t (q t ) + u w i (u) (what is classically called "late fusion") • C2 : λ 2 w t (q t ) + u Sim IM G-T XT (u) • C3 : λ 3 Sim T XT -IM G (q t ) + u w i (u)</formula><p>where u iterates over the three query images associated to q t . Note that we used the simple sum to aggregate the normalized scores (z-scores) over the different (3) images associated with the query q t . Table <ref type="table" coords="9,132.60,588.75,4.98,10.46" target="#tab_6">5</ref> reports the performance (Mean Average Precision and Precision@20) of the different multimedia combinations (including the mono-media baselines). The parameters are set to:</p><formula xml:id="formula_9" coords="9,90.00,600.70,423.00,23.31">k i =2; k t =25; λ 1 =2; λ 2 =1; λ 3 =1.</formula><p>The textual baseline (that corresponds to w t ) is the one obtained by thesaurus-based document enrichment and pseudo-relevance feedback. For completeness, we also included the simple late fusion variant where the textual part of the query is augmented by the visual concepts detected by the XRCE detector.</p><p>The order in the cross-media similarity appears to be important: identifying the k-visualnearest neighbors of the queries and considering their associated texts as a proxy for the textual part of the query is much more efficient than the converse (identifying the k-textual-nearest neighbors of the queries and considering their associated images as a proxy for the visual part of the query). The linear combination of the purely textual relevance score with the cross-media similarity measure (IMG-TXT) allows the system to still achieve significant improvement: this is not counter-intuitive, as the IMG-TXT cross-media similarity measure does not use the textual part of the query.</p><p>As already announced, the introduction of visual concepts as extra features in the textual representation of the query and the documents (photo annotations) appears to loose its efficiency if we take into account the medium-level visual features through the channel of the purely visual similarity, at least in a late fusion context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Incorporating Diversity</head><p>As mentioned in section 3, one of the aims of ImageCLEFPhoto 2008 compared to previous sessions, is to promote diversity in the search results so that the first retrieved elements are not redundant. We investigated two main families of methods: implicit and explicit clustering-based approaches. The first method is commonly known as "Maximal Margin Relevance" (henceforth MMR). It amounts to re-rank the search results so that the element chosen at rank j has to be dissimilar to elements that were already selected at ranks j &lt; j. We give more details on this approach in paragraph 4.2.1. The second method is based on an explicit clustering of the first k elements, followed by a strategy designed to re-rank the elements so that many different clusters are represented among the first elements of the re-ranked list. Within that framework, different clustering algorithms and strategies can be combined. For image retrieval, some combinations have already been proposed in paragraph 3.4. In paragraph 4.2.2, we explain another particular approach that we assessed in the multi-media case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Implicit clustering based approach</head><p>MMR is a re-ranking algorithm which aims at avoiding redundancy among the first elements to be re-ranked <ref type="bibr" coords="10,150.32,380.43,9.96,10.46" target="#b2">[3]</ref>. It has been successfully applied in different fields such as active learning in information retrieval <ref type="bibr" coords="10,183.07,392.38,14.61,10.46" target="#b16">[17]</ref>, <ref type="bibr" coords="10,204.65,392.38,10.52,10.46" target="#b8">[9]</ref> or in document summarizing <ref type="bibr" coords="10,347.01,392.38,14.61,10.46" target="#b11">[12]</ref>, <ref type="bibr" coords="10,368.59,392.38,9.96,10.46" target="#b1">[2]</ref>.</p><p>We suppose that we are given a relevance score vector s 1 (for a given query q) as well as an inter-object similarity matrix S 2 (for each pair of annotated photos of the collection). The MMR framework supposes that the elements i should be ranked according to s 1 and S 2 . It is a greedy algorithm: at each step (rank) j we choose the element i that maximizes the following re-ranking criterion:</p><formula xml:id="formula_10" coords="10,189.74,474.08,323.25,16.89">M M R(i) = β(j)s 1 (i) -(1 -β(j)) max i ∈P j S 2 (i, i )<label>(5)</label></formula><p>where β(j) is a mixture parameter (between 0 and 1) depending on the rank and P j is the set of objects already selected (rank lower than j).</p><p>Traditionally, β is kept constant, but we propose here a more efficient variant, where β(j) linearly increases between β(1) = α (&lt; 1) and β(k)=1 for some k (typically k=100), before saturating at value β = 1.</p><p>Regarding the choice of s 1 , we adopted the (best) combination of mono-media and cross-media similarity measures (based on the C2 measure introduced in section 4.1). For S 2 , we tested different possibilities. We will give some details on this part in paragraph 4.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Explicit clustering based approach</head><p>In this method, for a topic q, we start by selecting the first k elements according to s 1 . Let us denote by Q k the subset constituted of these selected elements. Next, we cluster objects within Q k in order to find different themes. The similarity measures used in the clustering process are provided by a similarity matrix between multi-media objects, S 2 . From this global matrix, we extract the sub-matrix corresponding to objects in Q k , and we row-normalize it. This row-normalized sub-matrix is denoted S 2 . Then, we use the Relational Analysis (RA henceforth) approach for the clustering stage <ref type="bibr" coords="10,160.98,713.55,14.61,10.46" target="#b13">[14]</ref>, <ref type="bibr" coords="10,182.57,713.55,14.61,10.46" target="#b12">[13]</ref>.</p><p>From a theoretical point of view the RA method allows us to model the clustering problem through an integer linear program. The extended form of the partitioning criterion used in this approach is the following one:</p><formula xml:id="formula_11" coords="11,210.25,155.25,302.75,38.62">C(S 2 , X) = k i,i (S 2 (i, i ) -m) contribution X(i, i )<label>(6)</label></formula><p>m representing a threshold to which the similarity between two objects is compared. X is called a relational matrix. It is a binary square matrix of dimension k, which represents the partition we are looking for.</p><p>In our case, we avoid fixing arbitrarily this parameter by taking a central tendency measure of the similarities <ref type="bibr" coords="11,169.99,249.63,9.96,10.46" target="#b0">[1]</ref>. For our experiments we took the arithmetic means of the strictly positive similarities. We first define</p><formula xml:id="formula_12" coords="11,210.88,259.87,171.19,14.23">S + 2 = {(i, i ) ∈ Q k × Q k : S 2 (i, i ) &gt; 0}.</formula><p>Then we have:</p><formula xml:id="formula_13" coords="11,234.23,281.15,278.77,32.97">m = 1 |S + 2 | (i,i )∈S + 2 S 2 (i, i )<label>(7)</label></formula><p>The more the similarity between two objects exceeds m, the higher the chance for them to be in the same cluster.</p><p>The general term X(i, i ) equals 1 if element i and i are in the same cluster; 0 otherwise. Using this formalism, we can express the relational properties that a partition must respect by linear equations <ref type="bibr" coords="11,162.31,370.89,14.61,10.46" target="#b13">[14]</ref>:</p><formula xml:id="formula_14" coords="11,104.94,390.43,278.50,69.67">• binarity: X(i, i ) ∈ {0, 1}, ∀i, i = 1, . . . , k • reflexivity: X(i, i) = 1, ∀i = 1, . . . , k • symmetry: X(i, i ) -X(i , i) = 0, ∀i = 1, . . . , k • transitivity: X(i, i ) + X(i , i ) -X(i, i ) ≤ 1, ∀i, i , i = 1, . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , k</head><p>As the partitioning criterion and the constraints are linear with respect to X, we can use integer linear programming in order to solve the clustering problem. Furthermore, we can see that this model doesn't require to fix the number of clusters. However, the integer linear program is NP-hard and in practice, we need to apply an heuristic for dealing with large data-sets.</p><p>The heuristic we used in our experiments is quite similar to the leader algorithm introduced in <ref type="bibr" coords="11,90.00,540.93,9.96,10.46" target="#b6">[7]</ref>. This approach is based upon transfer operations <ref type="bibr" coords="11,318.96,540.93,14.61,10.46" target="#b12">[13]</ref>, <ref type="bibr" coords="11,340.24,540.93,9.96,10.46" target="#b0">[1]</ref>: at each step, it reassigns an element to the cluster with which its aggregated contribution is maximal. If this last contribution score is negative then it creates a new cluster based on the current element.</p><p>We give a sketch of this heuristic in algorithm 1. Using this algorithm, we don't need to fix the number of clusters which can evolve at anytime. It is a property that is important as it allows to obtain pure clusters (as long as the parameter m is strict enough and this is the case when taking the value given by equation ( <ref type="formula" coords="11,249.19,612.66,3.87,10.46" target="#formula_13">7</ref>)). However, we need to provide a number of iterations (or a stopping criterion) for having an approximated solution in a reasonable amount of time; the algorithm typically converges in less than 5 iterations for a number of elements (k) smaller than 500. This kind of algorithm is highly dependent on the objects' scanning order. In our context, this order can be given by the relevance score provided by s 1 . In other words, the scanning order of objects Q k used in the RA heuristic is the decreasing order based on s 1 : we first take the most relevant objects then the less relevant ones.</p><p>After having clustered Q k , the k most relevant multi-media elements of a topic, we have to define a re-ranking strategy which takes into account the diversity provided by the clustering results. The main idea of our approach is to represent, among the first re-ranked results, elements</p><p>• xrce cm qpert 70: explicit clustering based approach where S 2 is constituted of the global similarity measures on textual an visual data. The relevance threshold to stop appending new clusters is given by the 30 th percentile of S 1 .</p><p>Our baseline is the basic result we obtained without any diversity goal seeking (such as in paragraph 4.1):</p><p>• xrce cm best basic: it is the basic search result based on the global similarity measures C2 using textual and visual data, which doesn't involve any methods for promoting diversity.</p><p>Finally, we give in table 6 the results obtained for the best runs we submitted in the EN-EN-AUTO-TXTIMG category. The runs are sorted with respect to the mean between the ranks obtained according to P20 and CR20 among XRCE runs.  <ref type="table" coords="14,118.22,419.50,3.87,10.46">6</ref>: Precision at 20 (P20) and Cluster Recall at 20 (CR20) measures for the best runs in EN-EN-AUTO-TXTIMG category Our best runs allow us to improve the CR20 measure compared to the baseline xrce cm best basic. Therefore, both the implicit and the explicit clustering based methods allow us to incorporate diversity into the first re-ranked elements. However, we are less precise in terms of P20. Concerning the different kinds of measures we chose to use for representing thematic similarity between annotated photos, we can see that either the pure textual data ("tilo ", constituted of title and location fileds) using cross-entropy measures or the combination of textual and visual data ("cm") using cross-media similarities perform better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This year, the main lessons we learnt from our participation to ImageCLEF-Photo were:</p><p>• in the case of pure text-based retrieval, both document enrichment by thesaurus and query enrichment improve the results, and combining the former with query expansion using PRF improves further the results (Table <ref type="table" coords="14,270.44,625.94,3.87,10.46">1</ref>);</p><p>• Fisher Vectors are rich image signatures and have state-of-the-art performance both in visual concept detection (see Figure <ref type="figure" coords="14,246.02,657.38,4.43,10.46" target="#fig_0">1</ref>) and content based image retrieval (Table <ref type="table" coords="14,439.85,657.38,3.87,10.46" target="#tab_3">3</ref>);</p><p>• the use of the visual concepts increases the retrieval performance when combined with pure text, but this advantage is lost when we use other, more complex multi-media fusion mechanisms, based on lower-level features than the visual concepts (Table <ref type="table" coords="14,418.88,700.77,3.87,10.46" target="#tab_1">2</ref>);</p><p>• combining the two mono-media information sources (image and text) using cross-media similarities based on transmedia pseudo-relevance feedback improves significantly (by more than 50% relative) the retrieval results (xrce cm best basic with MAP=0.39 and P@20=0.56).</p><p>• concerning the diversity, most strategies we proposed succeeded in reducing the redundancy in the top documents. As none of the techniques used explicitly the provided clustering criterion (e.g. diversifying according to cities or states or sports, etc.), the CR20 score was not always significantly increased (or in a few cases it was even decreased). This is not surprising, as we were seeking and improving the diversity in a blind (unsupervised) way, and therefore the "new" documents introduced at top level in many cases were not necessarily different following the criterion the organizers decided. For example, a pure visual content-based system will surely not group pictures of "persons on a beach" into different clusters each corresponding to a different "country" (topic 34). There is more chances that this happens for text or multi-modal system. Obviously, one possible improvement is the explicit integration of the provided "clustering criterion" into the diversification strategy when possible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,90.01,323.26,423.00,10.46;5,90.00,335.22,224.82,10.46"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: ImageClef Visual Concept Detection Task results. The red circle represents our linear system and the green square our non-linear system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,140.38,373.55,322.24,107.50"><head></head><label></label><figDesc>.</figDesc><table coords="3,140.38,396.76,322.24,84.28"><row><cell></cell><cell cols="2">Without PRF</cell><cell cols="2">With PRF</cell></row><row><cell>Method</cell><cell>MAP</cell><cell cols="2">P20 MAP</cell><cell>P20</cell></row><row><cell>Baseline</cell><cell cols="4">0.215 0.259 0.239 0.293</cell></row><row><cell cols="5">Document Enrichment 0.231 0.268 0.260 0.308</cell></row><row><cell>Query Enrichment</cell><cell cols="4">0.218 0.264 0.257 0.282</cell></row><row><cell cols="5">Table 1: Performance (MAP and P@20) of different enrichment strategies</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,109.34,423.01,96.23"><head>Table 2 :</head><label>2</label><figDesc>Performance (MAP and P@20) of the combination with different Visual Concept Detectors</figDesc><table coords="4,177.59,109.34,247.83,61.86"><row><cell></cell><cell cols="2">Without PRF</cell><cell cols="2">With PRF</cell></row><row><cell>Method</cell><cell>MAP</cell><cell cols="2">P20 MAP</cell><cell>P20</cell></row><row><cell>Baseline</cell><cell cols="4">0.215 0.259 0.239 0.293</cell></row><row><cell>XRCE Visual Concepts</cell><cell cols="4">0.241 0.297 0.269 0.334</cell></row><row><cell cols="5">RWTH Visual Concepts 0.232 0.271 0.258 0.308</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,90.00,297.84,423.02,94.08"><head>Table 3</head><label>3</label><figDesc>compares these three strategies. We can see that the early fusion (mean Fisher Vector) performs worse than late score level fusions. The best performance was obtained by the score averaging strategy I3.</figDesc><table coords="6,212.65,344.40,177.71,47.52"><row><cell>Method</cell><cell>MAP</cell><cell>P20 CR20</cell></row><row><cell>I1 (xrce img ff)</cell><cell cols="2">0.119 0.255 0.330</cell></row><row><cell cols="3">I2 (xrce img rg) 0.130 0.301 0.351</cell></row><row><cell cols="3">I3 (xrce img of) 0.151 0.328 0.352</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,135.32,403.88,332.37,10.46"><head>Table 3 :</head><label>3</label><figDesc>Performances (MAP and P@20 and CR@20) of different strategies.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,158.35,278.70,286.29,121.35"><head>Table 4 :</head><label>4</label><figDesc>Performances (P@20 and CR@20) of different strategies.</figDesc><table coords="8,219.60,278.70,163.80,98.94"><row><cell></cell><cell>P20 CR20</cell></row><row><cell>xrce img cl10 of</cell><cell>0.312 0.353</cell></row><row><cell>xrce img cl10 of rg</cell><cell>0.290 0.360</cell></row><row><cell>xrce img cl10 ff of</cell><cell>0.270 0.349</cell></row><row><cell cols="2">xrce img cl10 of ff rg 0.254 0.365</cell></row><row><cell cols="2">xrce img knn10 m5m 0.276 0.334</cell></row><row><cell cols="2">xrce img knn10 m4m 0.290 0.360</cell></row><row><cell>xrce img knn10 m5</cell><cell>0.300 0.357</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,115.18,109.34,372.64,121.34"><head>Table 5 :</head><label>5</label><figDesc>Performance (MAP and P@20) of the different multi-media fusion strategies</figDesc><table coords="9,360.75,109.34,58.04,10.46"><row><cell>MAP</cell><cell>P20</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,740.90,243.24,8.37"><p>Available on http://wiki.services.openoffice.org/wiki/Dictionaries</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,105.24,736.39,407.87,8.37;7,90.00,745.85,302.88,8.37"><p>Indeed, even images visually similar to query images can be non relevant according to the complete query, such as churches with one tower for query 2 or straight roads outside USA for query 6.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 1 RA heuristic Require: nbitr = number of iterations; S 2 the similarity matrix i = 1 Take the first element i as the first element of the first cluster clu 1 κ = 1 where κ is the current number of cluster for l = 1 to nbitr do for i = 1 to k do for j = 1 to κ do Compute the contribution of i with cluster clu j : cont j = i ∈clu j (S 2 (i, i ) -m) end for j * is the cluster id which has the highest contribution with i and cont j * is the corresponding contribution value</p><p>Create a new cluster where i is the first element κ ← κ + 1 else Assign i to cluster clu j * if the cluster where was taken i before its new assignment, is empty then κ ← κ -1 end if end if end for end for Return the partition found which belong to different clusters until a stopping criterion is fulfilled. The strategy employed is described in algorithm 2. In this approach the scanning order of objects in Q k is the same one as previously used, namely we take the most relevant objects first. By doing this, each cluster is represented by its most relevant object according to s 1 .</p><p>Algorithm 2 Re-ranking strategy for a topic Require: A topic q; s 1 the relevance score vector between objects in Q k and q; R the clustering results of objects in Q k L1, L2 and CL are empty lists i = 1 Take the first element i as the first element of the re-ranked list L1 Add R(i), the cluster id of element i in the cluster list</p><p>We tested different stopping criteria in algorithm 2:</p><p>1. the number of different clusters represented among the first results should not exceed a predefined number nbdiv ∈ 1, . . . , κ where κ is the number of clusters found 2. we append different clusters' representatives as long as their relevance scores are not lower than a predefined threshold qpert ∈</p><p>Concerning the first stopping criterion, let us assume that nbdiv = 10. Then, this implies that the first 10 elements of the re-ranked list have to belong to 10 different clusters (assuming that κ ≥ 10). Once 10 different clusters are appended, the complementary list (from the 11 th rank to the k th rank), is constituted of the remaining multi-media objects sorted in the decreasing order with respect to s 1 and without taking into account the cluster membership information.</p><p>For illustrating the second stopping criterion, let us take for example k = 100. Q k is sorted according to the decreasing order with respect to s 1 (such as for the scanning order). Then if we take furthermore qpert = s 1 (70), this means that the threshold is the 30 th percentile of the distribution given by s 1 . Accordingly, in the re-ranked list, we append the most relevant element of a cluster that it is not already represented as long as the relevance measure of this chosen element is greater or equal to the 30 th percentile of s 1 . This is an alternative which, unlike the first stopping criterion, avoid fixing a number of clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Description of AUTO-TXTIMG runs</head><p>In this paragraph, we give more details about the runs we submitted in the EN-EN-AUTO-TXTIMG category. Our different approaches concerning this sub-task have been described in the previous paragraph. For s 1 , we obviously chose the best multimedia combination as explained in section 4.1. Let us come back to the choice of the pairwise similarity matrix S 2 between the annotated photos of the collection. The kind of similarity coefficient computed in S 2 differs in general from s 1 . The idea is to investigate whether some weighting schemes are more adequate, or whether the use of a single mode or the use of specific fields give better results in diversity-based re-ranking. Let us be more precise in the three alternatives we adopted for S 2 :</p><p>1. the cross-entropy measure between texts given by the concatenation of the title and the location fields associated to each multi-media object. This similarity matrix is referred as "tilo" in the title of our runs.</p><p>2. the tf idf 3 measure between texts given by the concatenation of the title, the location and the narrative fields associated to each multi-media object. This similarity matrix is referred as "tfidf" in the title of our runs.</p><p>3. the (best) cross-media similarity measure which combines textual and visual similarity measures such as described in paragraph 4.1. In other words, it is the extension of s 1 to pairs of elements in the dataset. This similarity matrix is referred as "cm" in the title of our runs.</p><p>Clustering -or re-ranking -is limited to the k most relevant elements with respect to a topic q. In all our runs, we took k = 100.</p><p>The title of our runs is constituted as follows: "xrce (S2 matrix) (Stopping criterion) (Parameter value)". We give below some examples of the meaning of the titles of our runs:</p><p>• xrce tilo nbdiv 15: explicit clustering-based approach where S 2 is constituted of the crossentropy measure using textual data (title and location fields). The number of different clusters is nbdiv = 15.</p><p>• xrce cm mmr 07: implicit clustering based approach where S 2 is constituted of the global similarity measures using textual and visual data. The basic mixture parameter in the MMR re-ranking function (for rank 1) is set to α = 0.7 (β is linearly increasing from 0.7 to 1, value achieved for j=k=100).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="15,110.47,296.81,402.53,10.46;15,110.48,308.77,149.03,10.46" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="15,168.56,296.81,313.00,10.46">Sur des aspects algébriques et combinatoires de l&apos;Analyse Relationnelle</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ah-Pine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">6, 2007</date>
		</imprint>
		<respStmt>
			<orgName>University of Paris</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="15,110.47,328.70,402.54,10.46;15,110.48,340.65,307.48,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,339.15,328.70,173.86,10.46;15,110.48,340.65,217.08,10.46">A scalable MMR approach to sentence scoring for multi-document update summarization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Boudin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>El-Bèze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Torres-Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,349.22,340.65,36.62,10.46">COLING</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,360.57,402.53,10.46;15,110.48,372.53,238.79,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,253.30,360.57,259.71,10.46;15,110.48,372.53,159.99,10.46">The use of MMR, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,292.23,372.53,25.85,10.46">SIGIR</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,392.45,402.53,10.46;15,110.48,404.41,402.52,10.46;15,110.48,416.36,22.69,10.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,396.66,392.45,116.35,10.46;15,110.48,404.41,50.02,10.46">Xrce&apos;s participation to imageclef 2007</title>
		<author>
			<persName coords=""><forename type="first">Stephane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Michel</forename><surname>Renders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,180.58,404.41,189.09,10.46">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,436.29,363.77,10.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,150.04,436.29,176.55,10.46">Regularizing query-based retrieval scores</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,335.06,436.29,92.98,10.46">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,456.21,402.53,10.46;15,110.48,468.16,307.20,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,278.94,456.21,234.06,10.46;15,110.48,468.16,112.90,10.46">A density-based algorithm for discovering clusters in large databases with noise</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,257.33,468.16,128.25,10.46">Proceedings of ACM SIGKDD</title>
		<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,488.09,290.96,10.46" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="15,177.61,488.09,93.46,10.46">Clustering Algorithms</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>John Wiley and Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,508.02,402.52,10.46;15,110.48,519.97,82.78,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,170.70,508.02,273.41,10.46">Unsupervised learning by probabilistic latent semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,452.57,508.02,60.43,10.46;15,110.48,519.97,36.19,10.46">J. of Machine Learning</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,539.89,402.55,10.46;15,110.48,551.86,320.53,10.46" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="15,136.67,551.86,220.74,10.46">Active learning for interactive multimedia retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">K</forename><surname>Dagli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rajaram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Poliner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,571.78,402.54,10.46;15,110.48,583.74,297.61,10.46" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,394.33,571.78,118.68,10.46;15,110.48,583.74,233.38,10.46">Sparse multinomial logistic regression: Fast algorithms and generalization bounds</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hartemink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,353.14,583.74,23.16,10.46">PAMI</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,603.66,170.72,10.46" xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Lemur</surname></persName>
		</author>
		<ptr target="http://www.lemurproject.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,623.59,402.53,10.46;15,110.48,635.54,330.13,10.46" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,393.41,623.59,119.60,10.46;15,110.48,635.54,118.36,10.46">NUS at DUC 2007: Using evolutionary models of text</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,249.75,635.54,160.24,10.46">Document Understanding Conference</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,655.47,402.53,10.46;15,110.48,667.42,211.95,10.46" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,268.44,655.47,239.79,10.46">Heuristic approach of the similarity aggregation problem</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Marcotorchino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,110.48,667.42,128.07,10.46">Methods of operation research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="395" to="404" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,687.35,402.54,10.46;15,110.48,699.30,285.23,10.46" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,276.51,687.35,217.75,10.46">Optimisation en analyse de données relationnelles</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Marcotorchino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,110.48,699.30,131.62,10.46">Data Analysis and informatics</title>
		<meeting><address><addrLine>North Holland Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,719.23,402.53,10.46;15,110.48,731.18,69.79,10.46" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,239.35,719.23,269.41,10.46">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,122.93,731.18,25.06,10.46">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.47,110.53,402.53,10.46;16,110.48,122.49,402.52,10.46;16,110.48,134.45,233.10,10.46" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,221.37,110.53,237.06,10.46">A language modelling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,477.09,110.53,35.92,10.46;16,110.48,122.49,402.52,10.46;16,110.48,134.45,105.20,10.46">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.47,154.37,389.75,10.46" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,217.85,154.37,204.63,10.46">Active feedback in ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">X</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,443.18,154.37,25.85,10.46">SIGIR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.47,174.29,353.96,10.46" xml:id="b17">
	<monogr>
		<ptr target="http://www.imageclef.org/2008/vcdt" />
		<title level="m" coord="16,110.47,174.29,181.01,10.46">ImageClef Visual Concept Detection Task</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.47,194.22,402.53,10.46;16,110.48,206.18,402.52,10.46;16,110.48,218.13,132.62,10.46" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="16,215.41,194.22,254.48,10.46">Model-based feedback in the kl-divergence retrieval model</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,493.90,194.22,19.11,10.46;16,110.48,206.18,402.52,10.46;16,110.48,218.13,32.98,10.46">Proceedings of the 10th International Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 10th International Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.47,238.06,402.53,10.46;16,110.48,250.01,354.52,10.46" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="16,210.34,238.06,302.66,10.46;16,110.48,250.01,100.84,10.46">A study of smoothing methods for language models applied to ad hoc to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,232.04,250.01,104.28,10.46">Proceedings of SIGIR&apos;01</title>
		<meeting>SIGIR&apos;01</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
