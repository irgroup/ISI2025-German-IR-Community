<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,91.32,99.02,419.98,15.49;1,250.32,120.98,102.36,15.49">Getting geographical answers from Wikipedia: the GikiP pilot at CLEF</title>
				<funder ref="#_k2SYfhw">
					<orgName type="full">EU</orgName>
				</funder>
				<funder>
					<orgName type="full">European Union (FEDER and FSE)</orgName>
				</funder>
				<funder ref="#_2ksHqrf">
					<orgName type="full">FCT (Portugal)</orgName>
				</funder>
				<funder>
					<orgName type="full">Portuguese Government</orgName>
				</funder>
				<funder ref="#_7PANwns">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,131.52,154.74,52.86,8.97"><forename type="first">Diana</forename><surname>Santos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Linguateca</orgName>
								<address>
									<settlement>Oslo</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,193.55,154.74,57.88,8.97"><forename type="first">Nuno</forename><surname>Cardoso</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University of Lisbon</orgName>
								<orgName type="institution" key="instit2">LasiGE</orgName>
								<address>
									<region>DI, XLDB, Linguateca (</region>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,260.51,154.74,60.89,8.97"><forename type="first">Paula</forename><surname>Carvalho</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University of Lisbon</orgName>
								<orgName type="institution" key="instit2">LasiGE</orgName>
								<address>
									<region>DI, XLDB, Linguateca (</region>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.47,154.74,63.52,8.97"><forename type="first">Iustin</forename><surname>Dornescu</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Research Group in Computational Linguistics (CLG)</orgName>
								<orgName type="institution">University of Wolverhampton</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,402.95,154.74,64.61,8.97"><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Intelligent Information and Communication Systems (IICS)</orgName>
								<orgName type="institution">University of Hagen (FernUniversität in Hagen)</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.88,168.78,74.18,8.97"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Intelligent Information and Communication Systems (IICS)</orgName>
								<orgName type="institution">University of Hagen (FernUniversität in Hagen)</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.03,168.78,66.03,8.97"><forename type="first">Yvonne</forename><surname>Skalban</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Research Group in Computational Linguistics (CLG)</orgName>
								<orgName type="institution">University of Wolverhampton</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">SINTEF ICT (Norway)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,91.32,99.02,419.98,15.49;1,250.32,120.98,102.36,15.49">Getting geographical answers from Wikipedia: the GikiP pilot at CLEF</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9955AADDF82A13F47D9B5ED3D80137D2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Question answering, Questions beyond factoids, Geographical information retrieval, Cross-lingual information retrieval, Wikipedia, German, Portuguese, English, Evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports on the GikiP pilot that took place in 2008 in GeoCLEF. After providing motivation from both organizers and participants, it presents the task description, detailing topic choice and evaluation measures. Results are reported together with assessment difficulties and issues. Each participant system is described in detail, and the paper concludes with remarks on the current venue as well as ideas for improvements for future editions of GikiP or similar evaluation contests.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Motivation</head><p>This paper introduces GikiP<ref type="foot" coords="1,203.28,625.22,3.72,6.64" target="#foot_0">1</ref> , an evaluation contest on retrieving information from Wikipedia <ref type="bibr" coords="1,470.33,627.18,16.66,8.97" target="#b31">[30]</ref> in the form of a list of answers (corresponding to articles) that have some geographical component. We start by reporting different kinds of motivation that led us to propose, organize or participate in GikiP: the first is the often voiced dissatisfaction of GeoCLEF participants (and people involved in GIR in general) with continuing querying a collection of old newspapers -this has been a constant in Geo-CLEF <ref type="bibr" coords="2,141.95,85.98,10.78,8.97" target="#b5">[5,</ref><ref type="bibr" coords="2,155.49,85.98,7.42,8.97" target="#b6">6,</ref><ref type="bibr" coords="2,165.67,85.98,13.30,8.97" target="#b20">19]</ref> breakout sessions. Although we believe that the main GeoCLEF task is interesting enough, and that there are valid user models for it, there is a wealth of other sources as well as other kinds of applications where geographical information can be brought to bear. Furthermore, variation in an evaluation campaign, which after all has as one of its goals to foster innovation, is certainly beneficial for the field.</p><p>the second is the emergence of Wikipedia as an unavoidable resource for IR and NLP (becoming soon even more used than WordNet, as could for example be appreciated in this year's LREC <ref type="bibr" coords="2,490.11,165.78,15.02,8.97" target="#b19">[18]</ref>). In fact, Wikipedia's growth does not appear to slow down at present, with considerable content (more than 50%) in languages other than English <ref type="bibr" coords="2,289.72,189.66,15.33,8.97" target="#b30">[29]</ref>, and it is on the top 10 of the most visited sites on the Web, according to <ref type="bibr" coords="2,204.25,201.66,11.75,8.97" target="#b0">[1]</ref> (for all this it appeals to the IR community as much as to the NLP one -if it still makes sense to separate the two).</p><p>the third is our own interest in finding more realistic models for evaluation tasks, also in a multilingual context. Wikipedia is a truly multilingual resource, and it is not, as most other multilingual resources are, based on machine translation. Still, it has interesting alignment properties and metadata, which for example newspaper collections do not have.</p><p>Our view is that traditional evaluation tasks tend to create artificial divides where ultimately what is at stake is the satisfaction of user needs concerning access to information and knowledge through automatic means (or automatic helpers). It is not the form of the question or of the answer that should ultimately define the limits of what information access can offer.</p><p>Clearly, CLEF <ref type="bibr" coords="2,167.95,337.14,15.24,8.97" target="#b21">[20]</ref>, NTCIR <ref type="bibr" coords="2,224.29,337.14,15.33,8.97" target="#b17">[16]</ref>, and TREC <ref type="bibr" coords="2,294.30,337.14,16.66,8.97" target="#b29">[28]</ref> are undeniably extremely important for bringing progress and respectability to the IR and information access communities. Key ingredients are: (i) Separation of the teams who create the evaluation data from those who develop the systems, (ii) the proposal of challenging tasks to advance the state of the art, (iii) and attempting to measure progress from edition to edition.</p><p>However, the need to capitalize on the evaluation setup already created, as well as the upsurge of scientific communities around a particular task, may, in the long run, cause separation of otherwise similar concerns and interests, as is, in our opinion, what happens with QA@CLEF and GeoCLEF (or, for the sake of the argument, also WebCLEF and Ad-Hoc CLEF).</p><p>GikiP is an attempt to bring back together the two communities/tasks, by merging -albeit in a very specific context -the two forms of information request (questions or topics) and the two kinds of expected answers (factoids or documents), which are typically the hallmark of QA and IR, respectively.</p><p>Another concern of GikiP is to encourage multilingual and cross-lingual processing. In fact, in CLEF this has -understandably -not been a priority for participants. Even though the organizers take pains to treat all languages equally well, one might call CLEF (or some of CLEF tracks, at least) a set of monolingual evaluation campaigns in disguise. This is not a criticism. Rather, it reflects an important reality: In most research groups, most if not all resources are devoted to the processing of one's language (as it should be). Nevertheless, GikiP provides a task where it might be comparatively easy to satisfy other languages' needs (and/or make use of other languages), given that it has as target one of the most genuinely multilingual resources available: <ref type="bibr" coords="2,222.57,564.30,43.74,8.97">Wikipedia.</ref> Let us acknowledge that CLEF has already witnessed an interesting pilot with Wikipedia: WiQA 2006 <ref type="bibr" coords="2,113.72,588.18,15.69,8.97" target="#b16">[15,</ref><ref type="bibr" coords="2,133.25,588.18,12.45,8.97" target="#b32">31,</ref><ref type="bibr" coords="2,149.43,588.18,11.86,8.97" target="#b15">14]</ref>. WiQA was unfortunately not continued, after a promising start with seven participants in three languages (Dutch, English and Spanish). Our guess is that this happened because it was too ambitious and had a too specific user model, namely to help create new Wikipedia pages.</p><p>On the contrary, GikiP has a very straightforward and understood task (answer open list questions), and a very broad intended user community: any user/reader of Wikipedia might be interested in asking questions to it and get a list of articles. Systems that can perform successfully in this task may help harnessing the wealth of information that is included in Wikipedia. (And in fact, GikiP could be just the beginning of a contest where one would also look for images <ref type="bibr" coords="2,333.90,671.82,16.66,8.97" target="#b24">[23]</ref> -cf. the concurrent pilot at ImageCLEF with its WikipediaMM task <ref type="bibr" coords="2,201.86,683.82,15.02,8.97" target="#b13">[13]</ref>.) But let us go back to the 2008 pilot in the remainder of this paper. We first describe the task in Section 2, with Section 3 devoted to topic choice, translation and assessment. Section 4 describes actual participation and results, while Sections 5 to 7 present each participant's approach in detail. Section 8 sums up and concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The GikiP task</head><p>We defined the particular specific task to be solved by this year's participants: Find Wikipedia entries (i.e. articles) that answer a particular information need which requires geographical reasoning of some sort.</p><p>In order to guarantee a common evaluation ground, participants were required to use the Wikipedia collection(s) already used in the QA@CLEF main track<ref type="foot" coords="3,288.84,215.78,3.72,6.64" target="#foot_1">2</ref> , henceforth called the GikiP 2008 collections.</p><p>Fifteen topics were made available on the 2th June 2008 from the GikiP site, where eight example topics in the three languages had already been published. Below is topic GP4 in the English version:</p><formula xml:id="formula_0" coords="3,95.40,255.50,399.60,71.30">¤B 4 1£ ¦¥ H ¢¡ ¤£ ¦¥ ¥£ §¦ ¥ @ ¤I ¤¦ © ¥ @ ¦I ¦ )¡ §4 " ¢¦ "F ¡ "( QF ¦S ¡ " ( ¤£ ¦¥ ¦¥ ¢ 5 § 10 2 ¦0 10 ¤I )£ ¤¥ f )¡ Q §4 " ¦ 2 ( G0 )¡ B )¡ ¤ ¦¥ !¦ ¤)¡ ¥ 2 S F ¡ 1( QF ( ¦£ ¦¥ ¦¥ ) £ 10 ¦¥ "F § 5 § ¦0 2 10 "! ¦S ¡ ¢" 10 §4 1£ ¦¥ 2 S )¡ Q "F # 10 ¤I ¢£ ¦¥ f e¨ 2 § ( G0 )¡ B )¡ ¤ ¦¥ ¦ ¦B !¦</formula><p>Participants had 10 days to return the results, in the form of a list of Wikipedia articles, by providing their results as a list including the title of Wikipedia web pages. A (reduced) example of results for the GP4 topic is:</p><formula xml:id="formula_1" coords="3,90.10,384.90,192.80,143.60">2 V £ ¥ %$ £ ¦¥ ¦¥ &amp; £ 10 "H £ ¦@ 68F ¤I 4 2 V £ ¥ %$ £ ¦¥ ¦¥ ' §£ § ¤ 4 b 1d £ ¤¥ 2 9 ( QF £ "! " e$F GI 4 2 V £ ¥ %$ £ ¦¥ ¦¥ ' §£ § ¤ 4 b " 1 £ "2 " e8F ¤I 4 2 V £ ¥ %$ £ ¦¥ ¦¥ ( ) @ 0 ¢¡ "( QF 68F ¤I 4 ¦¥ £ £ 0 &amp; £ 10 H £ ¦@ 6¨F ¤I )4 ¦¥ 5 £ ' §£ § ¦ 4 b 1d £ ¤¥ 2 ©8F " ¤I )4 ¦¥ ( £ ¥ ©0 £ ¦¥ ¦¥ "! ( ¦@ 0 )¡ "( QF 6¨F ¤I 4 ¦¥ F @ %1 F @ 0 "H £ ¦@ e¨F GI 4 B £ 0 H &amp; 0 H 32 ¦E )¡ ¤£ 4 ( ¤£ ¦¥ 5 £ 6 78F ¤I )4 B 5 £ ' §£ § ¡ ¤4 72 9¡ G£ b 0 £ QI B § C¨F GI 4 B 5 £ ' §£ § ¡ ¤4 72 9¡ G£ b 0 ¡ G2 "£ "2 C8F ¤I ¢4 B ( £ ¥ ©0 £ ¦¥ 2 ( ¦@ 0 ¢¡ GD 1@ § ©¨F ¤I 4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task definition</head><p>Only answers / documents of the correct type were requested (and therefore assessed as correct). In other words, if a topic concerned painters or scientists, results should be (lists of) names of people (painters and scientists), and not names of boats or countries. Conversely, if the question was about countries, the type of results should be places of the country type, and not wars or kings. The maximum number of documents returned per topic was set to 100, but systems were strongly encouraged to try to return only the right ones (which, as the organization was at pains to emphasize, were typically much less than that number). Initially, we had decided on a maximum of two runs per system, but Composers of Renaissance music born in Germany GP10 Polynesian islands with more than 5,000 inhabitants GP11 Which plays of Shakespeare take place in an Italian setting? GP12 Places where Goethe lived GP13 Which navigable rivers in Afghanistan are longer than 1000 km? GP14 Brazilian architects who designed buildings in Europe GP15 French bridges which were in construction between 1980 and 1990 later on we accepted up to six runs from the one group who suggested this alternative. And we believe that this, coupled with a restriction like "as long as the total number of different topic-document pairs does not exceed 1500 (i.e. the number of assessments needed for 1 dumb run that delivers 100 documents for each of the 15 topics)", would be a better task definition for further editions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation score</head><p>Evaluation was devised in order to emphasize diversity and multilinguality, ensuring that the systems which were able to retrieve most cases and in most languages would be considered best by the GikiP scores.</p><p>We delivered (and assessed) topics in English, German, and Portuguese, and therefore, an additional bonus was computed for multilinguality, mult, which is 1, 2 or 3 depending on the number of languages tried out by the systems.</p><p>System results are thus evaluated according to the following formula mult N N ¡ total where mult rewards multilinguality, N is the number of correct hits, and N ¡ total is precision. The system's final score is given by the average of the individual scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Topics: selection, translation, and assessment</head><p>Table <ref type="table" coords="4,114.57,529.02,5.03,8.97" target="#tab_0">1</ref> presents the titles of all topics used in this first GikiP pilot. The topic description was generally a less condensed and more verbose version of the topic meaning, but would not add crucial information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topic choice</head><p>Considerable care was put in obtaining a balance between topics more naturally covered in each of the three GikiP languages, as well as providing also other topics equally alien, in principle, to any of the languages or cultures (such as those on French bridges, Afghan rivers or Polynesian islands). This is not necessarily a true description of what is or can be found in the particular Wikipedia language versions available, but this was a guideline for topic choice.</p><p>Although, by definition, a geographic flavour had to be associated, we also strived to create topics quite different from those used in the GeoCLEF main task -in order to cover higher levels of the typology described in <ref type="bibr" coords="4,143.58,671.82,10.60,8.97" target="#b5">[5]</ref>. In a way, GikiP is -as already emphasized -a way to explore further the goals of GeoCLEF by changing the document collections and also the kind of geographical reasoning required. It was therefore important not to repeat the same kind of topics with just another collection. In our view, this was one weakness of the QA task in 2007 <ref type="bibr" coords="5,262.72,306.78,10.69,8.97" target="#b7">[7]</ref>, which continued with the same kind of questions and just enlarged the collections (by adding Wikipedia).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topic translation</head><p>All topics were initially devised in Portuguese, and then roughly translated into English and from there to German. Then a thorough revision of the three versions took place. Although it was not intended, it appeared that some topics in Portuguese were hard to translate into the two Germanic languages, so we note this here, as a counterpart to the usual difficulty to translate some English topics into Portuguese (reported for example in <ref type="bibr" coords="5,189.36,413.82,15.02,8.97" target="#b22">[21]</ref>). One issue was the navigability and length, which we meant as independent features of a river, but which strongly conveyed the implication in English<ref type="foot" coords="5,393.12,423.86,3.72,6.64" target="#foot_2">3</ref> that it was the length of the navigable stretch one was measuring.</p><p>Another issue was the difficulty of expressing something that overlapped temporally with a particular decade in English -bringing the awkward formulation "whose construction started, continued or ended in or between 1980 and 1990". Likewise, the expression of "the place of the plot" in German was not direct, either, and in fact "Welche Stücke Shakespeares spielen in Italien?" can also indicate that a theater company is doing a Shakespeare play in some part of Italy.</p><p>Of course we are not saying that either of these questions is impossible or even too difficult to express in German or English. We are just pointing out that they are probably more naturally come upon and formulated by Portuguese speakers. This is the sort of information that is interesting to amass in a crosslingual context: some questions are easier to answer (and more natural to pose) in different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Topic assessment</head><p>In order to provide an idea of the assessment work, we describe briefly the pool obtained, by listing, for each topic, the number of different answers from all participants, as well as number of the correct answers, in Table <ref type="table" coords="5,124.52,616.38,3.77,8.97" target="#tab_1">2</ref>. <ref type="foot" coords="5,132.12,614.54,3.72,6.64" target="#foot_3">4</ref>As far as assessment matters are concerned, as usual some decisions had to be made: As a general rule, we considered as wrong answers all cases where the human assessor was not able, to the best of her ability, to verify the truth of the answer. For example, in topic GP14 (Brazilian architects who designed buildings in Europe), several Brazilian architects where no mention to works in Europe could be found were deemed incorrect, because we expect that non-verifiable information is not useful for any user. If the user wanted to assess himself based on the information that only he had, then he would have asked for Brazilian architects only.</p><p>Of course, other issues were more difficult to assess. For example, does the mention that a particular river was strategic for military operations imply that it is navigable? -we assumed a negative answer in topic GP13 (Which navigable rivers in Afghanistan are longer than 1000 km?).</p><p>We acknowledge that our role as human assessors may not guarantee perfect knowledge, but this is in no way different from any other evaluation contests which involve human judgements.</p><p>Rather more interesting was the problem of different language versions having different answers (for example as far as population of the Tahaa island, for GP10 (Polynesian islands . . . ) is concerned). Here, we assumed a very liberal procedure. If a positive answer could be found in any version, all of them were deemed correct.</p><p>Also, some answers to GP7 (African capitals . . . ) referred to places that no longer exist or have changed name or status (such as Salisbúria or Abidjan). We considered them correct since no temporal restriction was explicitly mentioned in the topic, but this is obviously an issue that has to be better dealt with in real-life information access.</p><p>In fact, the temporal interaction is even more complex: if we take a closer look at the GP7 topic, three ways to interpret it arise: (i) a user might be interested in African capitals with a given population at any time (this is the broadest possible interpretation) or (ii) only with such population when they were capitals (even if they are no longer capitals), or even (iii) only cities with such population, even though no longer capitals (provided they had been once).</p><p>Yet another interesting issue also raised in connection with the GP7 topic is the status of capital itself: in some countries, it is distributed among different cities, as is the case of South Africa, with three capitals (administrative: Pretoria, legislative: Cape Town and judicial: Bloemfontein). Any of these cities was considered a correct hit if it also satisfied the other topic requirement(s).</p><p>During assessment, we also found interesting language differences concerning cognate proper names: While Salisbúria in Portuguese refers unambiguously to a previous (temporal phase of a current) African capital, now named Harare, the "corresponding" Salisbury entry in the English Wikipedia points to a place in the United Kingdom, more specifically in the English county of Wiltshire. (This reminds us of the issues with last year's GeoCLEF topics on St. Paul's Cathedral and St. Andrews, whose "translation" into S ão Paulo and Santo André brought up different and more prominent places in Brazil and Portugal as well <ref type="bibr" coords="6,490.19,420.78,15.02,8.97" target="#b20">[19]</ref>). This shows clearly the need not to completely trust translation equivalences, which can be misleading in one of the directions.</p><p>At another level, it was not always possible to maintain the level of detail as far as specific objects or concepts were concerned: for example, r ápidos in Portuguese is a subtype of waterfall (namely one which is navigable), which apparently has to be rendered in English as waterfall, which is undeniably much more generic.</p><p>Finally, some features of Wikipedia itself caused unexpected problems for assessment: While a question for a city population seems to be natural from a user's point of view, we found that for some cities there are up to three different numbers (city, urban, metropolitan)! This implies that interaction with the user would be needed to identify which of these concepts s/he had in mind. For the time being, we accepted answers as correct provided one of the numbers satisfied the restriction stated in the topic (two topics concerned city populations: GP3 and GP7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Overview of participation and results</head><p>As usual, a pilot task gets more expressions of interest than actual participations, and this happened in GikiP, with 13 groups reporting interest but only three participants in the end. One probably not irrelevant consideration is that GikiP is a hybrid task between QA and GIR, but was fully deployed -and therefore only conveniently publicized -under the GeoCLEF umbrella. <ref type="foot" coords="6,336.48,654.14,3.72,6.64" target="#foot_4">5</ref> These are the participating systems: Curiously, we had one participant per country where one of the three languages is spoken, and participation was divided equally between GeoCLEFers and QA@CLEFers (given that the IICS group is known to participate in both). We have also kept non-official submission to GikiP open until later (30 June) in order for people busy with the GeoCLEF main task to be able to try GikiP, although non-officially, but we received no further submissions. The participating systems are shortly described in Table <ref type="table" coords="7,332.98,409.14,3.77,8.97" target="#tab_2">3</ref>, together with a fully manual run based on the current Wikipedia, which was provided by Paulo Rocha. The point of requesting this run was to be able to, later on, compare human performance to automatic answers. Also, we wanted to assess how much Wikipedia information had changed regarding the particular topics, from the official collections to the June 2008 date. Each system will be fully described in the corresponding section in the paper.</p><p>The results obtained by the systems can be found in Table <ref type="table" coords="7,340.04,468.90,3.77,8.97">4</ref>. For the record, the maximum number of documents returned per topic was 23 (for GP7), and the minimum was zero. It is also interesting to note that the human participant was not able to find any results for topics GP2 (Which Vienna circle members . . . ) and GP15 (French bridges . . . ), contrarily the automatic systems, which together managed to find 7 and 5 correct hits, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GIRSA-WP participation</head><p>GIRSA-WP (GIRSA for Wikipedia) is a fully-automatic, hybrid system combining methods from question answering (QA) and geographic information retrieval (GIR). In particular, it merges results from InSicht, an open-domain QA system <ref type="bibr" coords="7,203.86,596.58,10.60,8.97" target="#b9">[9]</ref>, and GIRSA, a system for textual GIR <ref type="bibr" coords="7,372.28,596.58,15.33,8.97" target="#b18">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">System description</head><p>In comparison with the two underlying basic systems, GIRSA-WP applies a semantic filter on the article titles (which are encoded in the answers in GikiP) to increase precision. This semantic filter ensures that the expected answer type (EAT) of the topic and the title of a Wikipedia article are compatible. This technique is widely known from QA for typical answer types such as PERSON, ORGANIZATION, or LOCATION. In our system, a concept (a disambiguated word) corresponding to the EAT is extracted from the topic title or description. Then, this concept and the title of a candidate article are parsed by WOCADI <ref type="bibr" coords="8,490.73,62.10,10.60,8.97" target="#b8">[8]</ref>, a syntactico-semantic parser for German text. The semantic representations (more specifically, the ontological sort and the semantic features, see <ref type="bibr" coords="8,242.58,85.98,16.66,8.97" target="#b12">[12]</ref> for details) of the semantic heads are unified. If this unification succeeds, the candidate article is kept; otherwise it is discarded. For example, from topic GP4 (Which Swiss cantons border Germany?), the extracted concept is canton, which is an artificial geographical entity denoting a kind of regional institution.</p><p>The major differences to InSicht and GIRSA are that GIRSA-WP does not merge streams of answers and does not include a logical answer validation. In contrast to GIRSA, the retrieval is based on documents indexed on a per-sentence basis of Wikipedia articles. In addition, the documents from Wikipedia had not been geographically annotated at all.</p><p>For the GikiP experiments, the topic title and description were analyzed and sent to GIRSA and InSicht. In GIRSA, the top 1000 results were retrieved and scores were normalized in the interval from 0 to 1. For results returned by both GIRSA and InSicht, the maximum score was chosen. Results whose score was below a given threshold were discarded and the semantic filter was applied to the remaining results. To obtain multilingual results, the German article names were translated to English and Portuguese using the Wikipedia linking between languages. Note that this linking was the only non-textual information we used from Wikipedia; for example, categories and inter-article links were completely ignored.</p><p>In InSicht, the semantic representation of the query and the semantic representations of document sentences are compared. To go beyond perfect matches, InSicht uses many techniques, for example intratextual coreference resolution, query expansion by inference rules and lexicosemantic relations, and splitting the query semantic network at certain semantic relations. InSicht employed a special technique called query decomposition (first tried in GeoCLEF-2007, <ref type="bibr" coords="8,274.72,313.14,15.92,8.97" target="#b18">[17]</ref>) or question decomposition in the context of QA <ref type="bibr" coords="8,493.46,313.14,15.33,8.97" target="#b10">[10]</ref>. Among the different decomposition classes described in the latter paper, only meronymy decomposition and description decomposition are promising for current queries in GikiP. They led to successful decompositions, e.g. topic GP4 (Which Swiss cantons border Germany?) is decomposed into subquestions like Name a canton in Switzerland. (with subanswers Aargau, Basel, . . . ) and revised questions like Does Aargau border Germany? (examples translated from German). We tried a strategy to answer subquestions also in the CLEF-News corpus used in QA@CLEF but not in GikiP. But, the overall results were equal to the ones obtained using only Wikipedia for subquestions. InSicht achieved a higher precision than GIRSA-WP as a whole (0.144 compared to 0.107), but recall is still problematic as already seen in similar evaluations, e.g. GeoCLEF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments</head><p>Due to time constraints, the Wikipedia articles had not been fully processed and some methods have been applied to the topics only although they should have been applied to the documents, too. We performed six runs with the following experiment settings: run 1: topics and documents (Wikipedia sentences) are processed with full word forms (no stemming and no stopword removal); results are filtered by applying a threshold score of 0.01 run 2: same setting as in run 1; location names are identified and normalized (for topics only) run 3: same setting as in run 2; German noun compounds are identified and split into their constituents (for topics only) run 4-6: same settings as in run 1-3; results are filtered by applying a threshold score of 0.03</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation and discussion</head><p>Several reasons account for the relatively low performance of GIRSA-WP:</p><p>In comparison with topics from the QA@CLEF task at CLEF, GikiP topics are more difficult to answer and aim at a wider range of EATs.</p><p>In comparison with topics from the GeoCLEF task, topics are at least as difficult. They include complex geographic relations (GP2: outside, GP4: on the border), restrictions on measurable properties (GP3: more than, GP13: longer than), and temporal constraints (GP9: Renaissance, GP15: between 1980 and 1990).</p><p>Indexing sentences instead of complete Wikipedia articles was meant to ensure a high precision. However, the fallback strategy (GIRSA) does not work well when applied on document sentences. Geographic entities were not annotated at all in the documents for GIRSA. Thus, the high precision and recall observed with GIRSA for the news collection could not be observed with the Wikipedia articles.</p><p>For InSicht, the main problems were (1) that important information is given in tables (like inhabitant numbers), but the syntactico-semantic parser ignores these parts of articles and (2) that the semantic matching approach forming the basis of QA is still too strict for the IR oriented parts of GikiP queries (similar problems occurred for GeoCLEF experiments).</p><p>The system's multilingual approach is too simple because it relies only on the Wikipedia of one language (German) and adds results by following title translation links to other languages. Therefore for questions that have no or few articles in German, relevant articles from English or Portuguese cannot be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Future work</head><p>Future work will include tackling some of the problems discussed in Section 5.3, enabling the annotation of geographic entities and geo-inferences, and preferring special regions of Wikipedia articles (for example, the introductory sentences).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RENOIR participation</head><p>The goal of RENOIR participation in GikiP is to explore new ways of doing GIR, specially for those kinds of geographic queries that cannot be correctly handled by just naïvely expanding the query terms, and hoping that the IR system with some sort of geographic reasoning capabilities captures the full meaning of the topic at stake, as we do for GeoCLEF.</p><p>As such, we chose to participate with one semi-automatic run using query procedures as retrieval input, instead of query terms. We define query procedures as a group of pipelined actions that express each GikiP topic. The generation of query procedures was entirely manual, and the execution of each action varies from automatic, semi-automatic and manual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">What is RENOIR</head><p>RENOIR is an interactive tool where query procedures are executed, generating partial and final results for each GikiP topic. RENOIR makes extensive use of REMBRANDT <ref type="bibr" coords="9,356.02,555.18,10.60,8.97" target="#b4">[4]</ref>, a named entity recognition module which explores the Wikipedia document structure, links and categories, to identify and classify named entities (NEs) in texts written in Portuguese and English.</p><p>REMBRANDT classifies NEs according to the following 9 main categories:</p><formula xml:id="formula_2" coords="9,90.30,583.50,422.98,28.40">¡ £¢ ¥¤ §¦ , ¤ §¢ ¢©&amp; ¡¦ ©( ©&amp; 1 §¤ §¦ , d ¤ 0 &amp; d , 1 ¡ 1 &amp; ¡ , &amp; d ¤a , &amp; ' § 1 ¢ &amp; 0 1 ¤ §¦ , 1 X ©¦ ¢¨, &amp; 1 ¡¢ 0 and ¡¦ 1 . REMBRANDT participated</formula><p>on the second HAREM <ref type="bibr" coords="9,188.66,614.94,15.33,8.97" target="#b25">[24]</ref>, a named entity recognition evaluation contest for the Portuguese language <ref type="bibr" coords="9,90.00,626.82,15.81,8.97" target="#b26">[25,</ref><ref type="bibr" coords="9,108.21,626.82,11.86,8.97" target="#b23">22]</ref>, obtaining F-measure values of 0.57 for the full NER task. We indexed the GikiP 2008 collection with MG4J <ref type="bibr" coords="9,303.98,638.82,10.69,8.97" target="#b3">[3]</ref>, and it was used for basic document retrieval. For retrievals involving Wikipedia categories and links, we preferred to use different snapshots of Wikipedia (namely the Portuguese and English static SQL dumps dated from April 2008, onwards referred to as the Wikipedia dumps to avoid confusion), because the information regarding Wikipedia categories, redirections and page links was already available in SQL databases, and thus we did not need to preprocess the GikiP 2008 collection. As such, RENOIR also allowed us to perform basic actions to match the documents from the Wikipedia dumps to its corresponding GikiP 2008 documents, to cope with GikiP's submission format.</p><p>The RENOIR actions used for the query procedures are described as follows. The actions are labelled as automatic (RENOIR performs the action alone), semi-automatic (the action is supervised), or manual (the action is made manually).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Retrieval actions</head><p>&amp; ¡¢ 0 X 1 £¢ ¡ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Generating the query procedures</head><p>The query procedures were formulated in a simple modular and pipelined approach. This allowed us to "divide and conquer" the complex task of translating the GikiP topics into a machine-understandable way. So, the actions that could be made automatically were therefore implemented, while the more complex actions performed in GikiP with human intervention (so far) were also kept simple in order to be possible to extend RENOIR to perform them automatically in the future. Anyway, while RENOIR can not perform (yet) all actions automatically, the methodology devised to handle the GikiP task can already be tested, and we can get acquainted with the difficulties that will face us during further development of RENOIR. Table <ref type="table" coords="10,129.70,573.42,5.03,8.97" target="#tab_4">5</ref> presents the query procedures used for the submitted runs for English. The query procedures for Portuguese had the same actions, with small exceptions discussed later. The query procedures are best explained by following the example topic GP7 (Capitals of Africa . . . ), which has a query procedure pattern similar to other topics.</p><formula xml:id="formula_3" coords="10,102.48,622.30,353.52,64.40">&amp; ¡¢ 0 X 0 &amp; 1 ¢¤ §¢ ¢¡ "Capitals of Africa" ¤ Docs 1 ; ¢ ¡ ¡ ' ¡¢ &amp; ¡¦ 1 Docs 1 ¤ Docs ¥ 1 , NE 1 ; ©Qd 1 ¨¦ ¡ ' £¡ 1 ¡ ¡ NE 1 ¦ &amp; d ¤a ¨ § a ¢&amp; ¡¦ 1 ¨1 ¡ ¤ NE ¥ 1 ; ©Qd 1 ¨ ¤ 0 ' £¡ &amp; d NE ¥ 1 © 2,000,000 ¤ Docs 2 ; &amp; ¤ 0 Docs 2 ¤ Docs ¥ 2 . 1. &amp; ¡¢ 0 X 0 &amp; 1 ¢¥¤ §¢ ¨¡ ¨ 0 £ ¦B ¡ Q £ 4 1! &amp; ! 10 )¡ "( ¦£ ¢ ¤ Docs 1 -</formula><p>The expected answers for this topic is a filtered list of Wikipedia pages that are likely to belong to a same category, "Capitals of Africa". </p><formula xml:id="formula_4" coords="11,148.20,154.58,302.64,21.44">; ¨! ¢" $# % ¡ &amp; ' ( ¦' ¦) ¡ NE 1 0 " 21 § £ " 23 54 £ ¦ ¦¡ ¥ ¨ § 1 76 ¥ 2 5¡ NE 1 ; ¤£ ¦) 8 ¦¡ NE 1 Docs 2 .</formula><p>2</p><formula xml:id="formula_5" coords="11,148.10,170.85,325.06,49.32">¢¡ ¤£ ¦¥ ¨ § © ¡ ¥ "Vienna Circle" Docs 1 ; ¥ ¡ ¦¥ ¤£ ¦ ¤ Docs 1 Docs 1 , NE 1 ; ¨! ¢" $# % ¡ &amp; ' ( ¦' ¦) ¡ NE 1 0 ) ¡ ¥ ¨ 1 3 ¤! ! ¢9 @! 6 £ " NE 1 ; ¤£ ¦) 8 ¦¡ NE 1 Docs 2 ; ¥ ¦¡ ¥ ¤£ ¤ Docs 2 Docs 2 , NE 2 ; 2! ¢" A# B ¡ 8 ' C ¦' ) ¡ NE 2 0 " 21 § ¦£ " ¨3 § 1 ¢6 ¦¥ ' NE 2 ; ¨! ¢" $# % 1 § &amp; ' C¡ 9 £ " NE 2 D 0 BE 'Austria', 'Hungary', 'Germany'F G Docs 2 .</formula><p>3</p><formula xml:id="formula_6" coords="11,148.10,215.05,329.86,50.00">¢¡ ¤£ ¦¥ ¨ § © H § £ ¦¡ ¨I 1 ¥ ' "Rivers of Portugal" Docs 1 ; ¥ ¡ ¦ ¥ ¤£ ¦ Docs 1 Docs 1 , NE 1 ; ¨! ¢" $# % ¡ &amp; ' ( ¦' ¦) ¡ NE 1 0 " 21 § £ " 23 ! 79 @! ! ¦1 NE 1 ; ¤£ ) 8 ¡ NE 1 Docs 2 ; ¥ ¦¡ ¥ ¤£ ¤ Docs 2 Docs 2 , NE 2 ; 2! ¢" A# B ¡ 8 ' C ¦' ) ¡ NE 2 0 9 £ " ¦6 ¡ 3 P ¢6 £ ¦ ! ¦' NE 2 ; ¨! ¢" $# % 1 § &amp; ' C¡ 9 £ " NE 2 Q 150,000 Docs 2 ; ¤£ ) 8 1 § Docs 2 Docs 2 .</formula><p>4</p><formula xml:id="formula_7" coords="11,148.20,259.35,331.20,38.34">¢¡ ¤£ ¦¥ ¨ § © H § £ ¦¡ ¨I 1 ¥ ' "Cantons of Switzerland" Docs 1 ; ¥ ¡ ¦¥ ¤£ ¦ ¤ Docs 1 Docs 1 , NE 1 ; ¨! ¢" $# % ¡ &amp; ' ( ¦' ¦) ¡ NE 1 0 " 21 § £ " 23 § 1 76 ¤ ¦¥ ' NE 1 ; ¨! ¢" $# % 1 § &amp; ' C¡ 9 £ " NE 1 0 BE 'Germany'F R Docs 2 ; ¤£ ¦) 8 1 § Docs 2 Docs 2 .</formula><p>5</p><formula xml:id="formula_8" coords="11,148.10,292.55,288.82,38.50">¢¡ ¤£ ¦¥ ¨ § © ¡ ¥ "Greece" Docs 1 ; 5¡ ¤£ ¥ ¨ § © ! " @! S 2 Docs 1 Docs 2 ; ¥ ¦¡ T# % 1 § C 1 ¦¡ Docs 2 NE 2 ; 2! ¢" $# U ¡ 8 ' ( ' ) ¡ NE 2 0 ¡ 9 ¡ ¤ NE 2 ; ¨! ¢" $# % 1 § &amp; ' C¡ 9 £ " NE 2 contains 'war' NE 2 ; £ ¦) C ¡ NE 2 Docs 3 .</formula><p>6</p><formula xml:id="formula_9" coords="11,148.20,325.95,330.24,38.90">¢¡ ¤£ ¦¥ ¨ § © H § £ ¦¡ ¨I 1 ¥ ' "Mountains of Australia" Docs 1 ; ¥ ¦¡ ¥ ¤£ ¤ Docs 1 Docs 1 , NE 1 ; ¨! ¢" $# % ¡ &amp; ' ( ¦' ¦) ¡ NE 1 0 9 £ " 6 ¡ 3 P ¢6 £ ¦ ¤ ! ' NE 1 ; ¨! ¢" $# % 1 § &amp; ' C¡ 9 £ " NE 1 Q 2000 m Docs 2 ; ¤£ ¦) C 1 § Docs 2 Docs 2 .</formula><p>7</p><formula xml:id="formula_10" coords="11,123.60,359.15,335.88,60.70">¢¡ ¤£ ¦¥ ¨ § © H § £ ¦¡ ¨I 1 ¥ ' "Capitals of Africa" Docs 1 ; ¥ ¡ ¦ ¥ ¤£ ¦ Docs 1 Docs 1 , NE 1 ; ¨! ¢" $# % ¡ &amp; ' ( ¦' ¦) ¡ NE 1 0 9 £ " 6 ¡ 3 P ¢6 £ ¦ ¤ ! ' NE 1 ; ¨! ¢" $# % 1 § &amp; ' C¡ 9 £ " NE 1 Q 2,000,000 Docs 2 ; £ ¦) 8 1 § Docs 2 Docs 2 . 8 ¢¡ ¤£ ¦¥ ¨ § © H § £ ¦¡ ¨I 1 ¥ ' "Bridges of Brazil" Docs 1 ; ¨! ¢" $# % 1 § &amp; ' 8 ¡ ¥ "suspen*" Docs 1 ; ¤£ ) 8 1 § Docs 1 Docs 1 .</formula><p>9</p><formula xml:id="formula_11" coords="11,148.20,414.75,319.56,39.00">¢¡ ¤£ ¦¥ ¨ § © H § £ ¦¡ ¨I 1 ¥ ' "German composers" Docs 1 ; ¥ ¡ ¦ ¥ ¤£ ¦ ¦ ¤ Docs 1 Docs 1 , NE 1 ; ¨! ¢" $# % ¡ &amp; ' ( ¦' ¦) ¡ NE 1 0 ¤£ ¡ NE 1 ; ¨! ¢" $# % 1 § &amp; ' C¡ 9 £ " NE 1 V 1380 W NE 1 X 1640 Docs 2 ; ¤£ ¦) C 1 § Docs 2 Docs 2 .</formula><p>10</p><formula xml:id="formula_12" coords="11,123.60,447.95,354.36,104.98">¢¡ ¤£ ¦¥ ¨ § © ¡ ¥ "Polynesia" Docs 1 ; ¥ ¡ ¦ ¥ ¤£ ¦ ¦ ¤ Docs 1 Docs 1 , NE 1 ; ¨! ¢" $# % ¡ &amp; ' ( ¦' ¦) ¡ NE 1 0 " 21 § £ " 23 ¤! " £ ¦ NE 1 ; ¤£ ¦) C ¡ NE 1 Docs 2 ; ¥ ¦¡ ¥ ¤£ ¤ Docs 2 Docs 2 , NE 2 ; 2! ¢" A# B ¡ 8 ' C ¦' ) ¡ NE 2 0 9 £ " ¦6 ¡ 3 P ¢6 £ ¦ ! ¦' NE 2 ; ¨! ¢" $# % 1 § &amp; ' C¡ 9 £ " NE 2 Q 5,000 Docs 3 . 11 ¢¡ ¤£ ¦¥ ¨ § © H § £ ¦¡ ¨I 1 ¥ ' "Shakespearean plays" Docs 1 ; ¥ ¡ ¦ ¥ ¤£ ¦ Docs 1 Docs 1 , NE 1 ; ¨! ¢" $# % ¡ &amp; ' ( ¦' ¦) ¡ NE 1 0 " 21 § £ " 23 ! 79 @! ! ¦1 $Y ` § 1 ¢6 ¤ ¦¥ ' NE 1 ; ¨! ¢" $# % 1 § &amp; ' C¡ 9 £ " NE 1 0 Italy Docs 2 ; ¤£ ) 8 1 § Docs 2 Docs 2 . 12 ¢¡ ¤£ ¦¥ ¨ § © ¡ ¥ 2 "Goethe" Docs 1 ; ¥ ¦¡ ¥ ¤£ ¤ Docs 1 Docs 1 , NE 1 ; ¨! ¢" $# % ¡ &amp; ' ( ¦' ¦) ¡ NE 1 0 " 21 § £ " 23 ! 79 @! ! ¦1 NE 1 ; ¤£ ) 8 ¡ NE 1 Docs 1 .</formula><p>13</p><formula xml:id="formula_13" coords="11,148.20,547.75,325.92,39.10">¢¡ ¤£ ¦¥ ¨ § © H § £ ¦¡ ¨I 1 ¥ ' "Rivers of Afghanistan" Docs 1 ; ¥ ¡ ¥ £ ¦ ¤ Docs 1 Docs 1 , NE 1 ; ¨! ¢" $# % ¡ &amp; ' ( ¦' ¦) ¡ NE 1 0 9 £ " 6 ¡ 3 P ¢6 £ ¦ ¤ ! ' NE 1 ; ¨! ¢" $# % 1 § &amp; ' C¡ 9 £ " NE 1 Q 1,000 km Docs 2 . £ ¦) 8 1 § Docs 2 Docs 2 .</formula><p>14</p><formula xml:id="formula_14" coords="11,148.20,581.15,316.56,38.38">¢¡ ¤£ ¦¥ ¨ § © H § £ ¦¡ ¨I 1 ¥ ' "Brazilian architects" Docs 1 ; ¥ ¡ ¦ ¥ ¤£ ¦ ¦ ¤ Docs 1 Docs 1 , NE 1 ; ¨! ¢" $# % ¡ &amp; ' ( ¦' ¦) ¡ NE 1 0 " 21 § £ " 23 ! 79 @! ! ¦1 NE 1 ; ¨! ¢" $# % 1 § &amp; ' C¡ 9 £ " NE 1 0 Europe Docs 2 ; £ ¦) 8 1 § Docs 2 Docs 2 .</formula><p>15</p><formula xml:id="formula_15" coords="11,148.20,614.35,319.56,39.10">¢¡ ¤£ ¦¥ ¨ § © H § £ ¦¡ ¨I 1 ¥ ' "Bridges in France" Docs 1 ; ¥ ¡ ¦¥ ¤£ ¦ ¤ Docs 1 Docs 1 , NE 1 ; ¨! ¢" $# % ¡ &amp; ' ( ¦' ¦) ¡ NE 1 0 ¤£ ¡ NE 1 ; ¨! ¢" $# % 1 § &amp; ' C¡ 9 £ " NE 1 V 1980 W NE 1 X 1990 Docs 2 ; ¤£ ¦) C 1 § Docs 2 Docs 2 .</formula><p>So, we first search for all Wikipedia pages that contain that category, and obtain the list of Wikipedia documents Docs 1 . This step is done automatically.</p><p>2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¢ ¡ ¡ ' ¥¢ &amp; ¡¦ 1 Docs</head><formula xml:id="formula_16" coords="12,187.68,85.30,65.81,20.00">1 ¤ Docs ¥ 1</formula><p>% NE 1 -Afterwards, REMBRANDT annotates the documents in Docs 1 , generating a tagged version of these same documents, Docs ¥ 1 , and their corresponding named entities, NE 1 . This step is done automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>.</p><formula xml:id="formula_17" coords="12,115.10,129.10,244.51,20.00">©Gd 1 ¨¦ £ ' £¡ 1 ¡ £ NE 1 ¦ &amp; d ¦a ¨ § a &amp; ¡¦ 1 ¨1 ¡ ¤ NE ¥ 1 -</formula><p>We narrow the NE list to the entities that were classified by REMBRANDT as VALUE/QUANTITY, generating the subset NE ¥ 1 . This step is done automatically. <ref type="bibr" coords="12,102.48,181.62,3.77,8.97" target="#b4">4</ref>.</p><formula xml:id="formula_18" coords="12,115.10,172.90,225.58,21.10">©Gd 1 Ẅ ¤ 0 ' £¡ &amp; d NE ¥ 1 © R A% ¡ ¢ ¢ &amp;% ¡ ¢ ¢ ¤ Docs 2 -</formula><p>We now evaluate the NE subset NE ¥ 1 , searching for values greater than 2,000,000. When found, the respective document is therefore added to Docs 2 . This step is done manually. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results and discussion</head><p>Our approach for GikiP has some obvious faults and over-simplifications that are clearly compensated by human supervision. For instance, knowing which NE of type &amp; d ¦a ¥ ¨ § a &amp; £¦ 1 ¨1 ¡ corresponds to a popu- lation count is easy for a human, but it is not an easy task for an automated system. Yet, one of the best features of Wikipedia (from an IE system developer's point of view) is the clustering of such important features into infoboxes, which are easily machine-interpretable and thus this problem can be mitigated by developing a robust Wikipedia infobox parsing module.</p><p>Other over-simplifications involve, for instance, topic GP14 (Brazilian architects . . . ), where we assume that a Wikipedia page that contains at least one NE referring to a place in Europe is enough to consider that the targeted Brazilian architect page is relevant, or in topic GP11 (Shakespearean plays . . . ) where reference to a place in Italy is enough to consider that a given Shakespearean play actually happens in Italy. Last but not least, knowing whether the place names were Italian or European was manually done. Later on, we will try to access geographic ontologies in order to implement such restrictions, to fully verify the feasibility of the approach.</p><p>Also worth mentioning is the bypass we made to the problem of selecting the correct categories for the first pool of Wikipedia documents, as done in the query procedures that started with the &amp; £¢ 0 X 0 &amp; 1 ¢¤ §¢ ¨¡ action, by doing it manually. For instance, in topic GP9 (German composers . . . ), it turned out that the best category to start with was "German composers", while in topic GP15 (French bridges. . . ) the best category is "Bridges in France". We tentatively concluded that the first form is typically used to cluster persons by country and occupation, and the second is used for non-person entities.</p><p>Nonetheless, one of the requirements for an automated query procedure builder module is to figure out that "Composers of Germany" and "French bridges" are not Wikipedia categories, and thus a domain identification step as performed by WikipediaListQA@wlv (see Section 7) is quite important. Another problem arises for topics that could be started with a category, but there is no such category, as in topic GP5 (Name all wars . . . ). We expected a category such as "Wars of Greece" to exist, but it did not, so we had to choose a different query procedure that involved heavy processing: we selected Wikipedia pages with outlinks to Greece, resulting in an initial pool of over 20,000 documents linking to the Wikipedia page of Greece. Then all titles were classified by REMBRANDT, and those who had an 1E ¦¥ plus the term "War" in the title were finally selected.</p><p>Lastly, we report the differences between the Portuguese and English query procedures, which were related to the Wikipedia categories used in both snapshots. Take for instance the topic GP6 (Which Australian mountains . . . ). It is well bootstrapped in Portuguese by the category "Montanhas da Austrália", but, in the English Wikipedia snapshot, the category "Mountains of Australia" has a bundle of Wikipedia pages pointing to mountains and also subcategories that group mountains in Australian regions, such as New South Wales, South Australia, Northern Territory or Tasmania. Clearly, the two topic formulations, although apparently similar, may refer in English to a whole continent while in Portuguese only to a country (the continent being named Oce ânia).</p><p>On the other hand, in Wikipedia there are some categories which have subcategories before we arrive to the nodes that are real answers. To handle this we included new queries for the subcategories. For example, for topic GP11 (Shakespearean plays . . . ), the English Wikipedia category "Shakespearean plays" is divided into several subcategories that group the adaptations, comedies, tragedies, histories and apocrypha. So all these subcategories had to be visited. In this case, there was a difference between the English and the Portuguese Wikipedia since the latter had the subcategories but these were not yet filled, so the procedure had to be different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Conclusions and future work</head><p>The GikiP experiments allowed us to take a first glance at the difficulties that expect us when dealing with more complex queries with specific geographic criteria. As our goal is to research retrieval approaches that can profit from a comprehensive semantic layer over queries and documents, we found the GikiP exercise to be interesting and innovative.</p><p>The next obvious step is to implement the automatic generator of query procedures, dealing with the problems that were mitigated by using human reasoning. At the same time, future work includes the improvement of the Wikipedia mining approaches, namely extracting information from infoboxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">WikipediaListQA@wlv participation</head><p>The participation in this pilot task was motivated by our interest in using Wikipedia as a backbone in QA. In addition, the way the task is setup required us to rely on the information inherent in the Wikipedia article link graph and the relation between entities, rather than developing accurate textual answer extractors.</p><p>For example, if we try to find out information about the cities that the Douro river flows through, we can extract all the links present in the article describing the Douro river, creating a list of entities that are related to the river. If we only select the articles from this list that describe a settlement, a town or a city, the filtered list would likely contain localities that the river is passing through. By examining the infobox of each candidate, we can determine the population of each locality. We may apply a selection function to this list. We can compute the average population size, number of towns with population larger than 150,000 inhabitants, etc. discovering new facts.</p><p>For example, in solving the GP3 topic, the system identifies the Wikipedia category Category:Rivers of Portugal. All the articles directly linked to this category are likely to be rivers, thus the list of candidate answers is created. As previously described, for each river the system extracts the list of links to articles and attempts to extract the population size from each one. The maximum value encountered is computed. If this value is smaller than the threshold, the river is discarded.</p><p>In order to navigate the Wikipedia link graph we had to transform the Wikipedia SQL dump and index it with Lucene <ref type="bibr" coords="13,149.70,535.86,16.66,8.97" target="#b11">[11]</ref> because loading the data in MySQL would have taken too long (more than 200 million links in English Wikipedia alone). The index already contained a cleaned version of <ref type="bibr" coords="13,443.47,547.74,69.17,8.97;13,90.00,559.74,21.66,8.97">Wikipedia (April 2008)</ref>. Thus our results had to be mapped to the November 2006 version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Overview of the system</head><p>We propose a simple model for topic interpretation. It exploits relationships between entities that may not be expressed in the article text, but are implied by the links between the articles.</p><p>Our system starts by identifying a domain category that comprises candidate articles, and then filters out the ones that do not correspond to the topic filter. Thus two parts are identified in each topic: a) the domain of the candidate answers, and b) the filters to apply in order to select the correct ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain identification</head><p>We had to identify a Wikipedia category that would describe the candidate articles. We used the Connexor FDG parser <ref type="bibr" coords="13,258.17,693.18,16.66,8.97" target="#b28">[27]</ref> in order to extract noun phrases from the topics. The first noun phrase was matched with a category, by querying the Lucene <ref type="bibr" coords="14,357.25,62.10,16.54,8.97" target="#b11">[11]</ref> index. We used lexical rules in order to achieve a good accuracy (e.g. £ B § 10 " "@ H "@ § ¦ 0 )¡ E 10 ¢ £ ¡ 4 £ 0 )¡ E 10 ¢ ¡ ¥ B § ¦0 "@ H £ 4 £ £ 0 )¡ E 10 ¢ "! B § 10 1@ H £ 4 £ £ B § 10 1@ H £ 4 0 )¡ QE 10 9 £ £¢ 2 6 5 4 £ B 10 "@ H 1@ § § ¦ 0 )¡ QE 10 9 £ 5 £ B § ¦0 "@ H "@ § ¦ 0 ¢¡ QE 10 ¢ £ £¢ 2 6 5 B § 10 "@ H £ 4 0 )¡ E 10 ¢ B § ¦0 "@ H "@ § ¦ ). This simple method succeeded in identifying good categories in most cases. However at times it only matched a very general category (waterfalls in GP1, wars in GP5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidate filtering</head><p>For the purpose of this pilot only very simple filters were implemented. There are two main filter categories: entity filters and factoid filters.</p><p>The entity filters match documents that mention or have a link to a given entity. 8 of the 15 topics had such a constraint. In 2 cases there was a list of entities that should have matched. Most notably the temporal restriction identified in topic GP15 between 1980 and 1990 was expanded to the list of 10 years in the range. This behaviour was adapted from our QA system.</p><p>The factoid filters match documents in which the identified fact can be extracted, and the value corresponds to the selection criterion. The facts were extracted using components from our question answering system (infobox look-up, regex patterns). The facts were: population (3 topics), nationality (2 topics), height (1 topic) and length (1 topic). Articles from which the fact could not be extracted were dismissed. The selection criterion was applied to the extracted fact: greaterThan (5 topics -numeric facts), inList (1 topic), not inList (1 topic).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilinguality</head><p>The method proposed has the advantage that it can easily be adapted to a cross-lingual task. Only the English topics were analyzed. The results of the analysis can be directly mapped to any language version of Wikipedia. Firstly the corresponding category describing the domain of the candidate articles has to be identified by using inter-wiki links. Secondly the filter has to be "translated". This either means translating the entities, or having the necessary language dependent fact extractors. Cross-wiki answers can be combined and re-ranked, aggregating the results from all the languages by using the inter-wiki links. This allows an English-speaking user, for example, to exploit the fact that the Portuguese Wikipedia has much richer content regarding Brazil and Portugal since most Portuguese Wikipedia contributors live in these two countries. This year we only searched the English Wikipedia and the results were mapped to all the three languages by using the inter-wiki links because we did not have time to create the fact extractors for German and Portuguese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Results and error analysis</head><p>The accuracy of the system is limited due to the ambiguity of links. Not all articles that pertain to Category:Abidjan refer to an African capital. Category relations were not classified: hypernymy vs. meronymy vs. similarity. When searching all the articles that have a certain hyper-category, due to link type ambiguity, very large article sets might be extracted (the system did not return any results for three topics, because the list of domain articles was too large). This can be avoided by using resources that map Wikipedia articles to WordNet (e.g. Yago <ref type="bibr" coords="14,186.60,520.98,16.66,8.97" target="#b27">[26]</ref> and DBpedia <ref type="bibr" coords="14,262.35,520.98,11.24,8.97" target="#b2">[2]</ref>) and disambiguate the type of the entity described in each article. Also, filters can be implemented using tools of the Semantic Web (e.g. SPARQL queries).</p><p>The current system is a simple model which has proven to have a good precision of finding the answer. Its main advantage is that -using a small set of filters -very complex data can be queried from Wikipedia. Its greatest disadvantage lies in the complexity of correctly identifying (combined) filters in natural language questions. Given an appropriate user interface, this method can become an alternative to Wikipedia search, allowing users to access information that is not textually present in the encyclopedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Concluding remarks</head><p>We think that the results presented in this pilot are encouraging, both for the possibility of automating the particular task, and for its general interest as another way of reaching the information in Wikipedia. GikiP has shown that there are interesting kinds of non-trivial questions that have a retrievable answer in Wikipedia, and which can be quickly assessed by users, often without even having to visit the page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Summing up the approaches</head><p>Interestingly, participating systems took a wide range of different approaches, as well as different main collections (GIRSA-WP used German and WikipediaListQA2wlv English as main answering sources; while RENOIR did a parallel process for English and Portuguese). We intend, in a following publication, to study results per language and per topic to see whether results might have been heavily influenced by this choice of main collection to investigate. While one participant took a preliminary semi-automatic approach, although making use of several automated procedures, the two others (including the winner) used fully automatic systems from start to end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Remaining work</head><p>Much work remains to be done for issues of redundancy removal, choice of which language / answer to present first (or only), as well as how to present a compound set of pages to justify a particular answer.</p><p>Also, the task needs a more reflected and precise definition.</p><p>For example, the fact that natural language is fundamentally vague may lead to a set of answers at different levels, which, in order to satisfy a human user, should be presented in a more appropriate way: Consider the case of topic GP12 (Places where Goethe lived), where also names of countries were considered correct. Obviously, a more adequate answer would structure or make use of the hierarchical relations between the answers and not present all of them alphabetically as a flat list.</p><p>Most information needs require a lot of conceptualization. This is illustrated by topic GP5 (Name all wars that occurred on Greek soil). After starting assessment, we found that the relation of battles or sieges (which occur in particular places) as belonging to wars may require considerable theorizing and historical knowledge. Wars, especially perhaps when one is considering ancient wars, are more difficult to pin down to particular places. So, assessment turned out to be quite tricky, and we fear that the topic did not illustrate a realistic user need.</p><p>Finally, the mult factor, which was assigned 2 to RENOIR and 3 to the other two systems, was a fairly gross attempt to incentivize results in the three languages. However, different topics, as discussed in Section 3.1, would be easier or more productive in different languages and in fact answers which could only be found in one vs. two or three languages might be considered more difficult and therefore rewarded with another kind of multilinguality-related factor. This is something that must be investigated in the future. Namely, how to increase the score of particularly difficult topics.</p><p>We hope that further editions of GikiP and similar tasks will help research and development along those lines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="12,102.48,225.54,7.54,8.97;12,115.10,218.10,5.30,10.80;12,141.60,218.25,10.40,11.25;12,156.70,225.54,24.25,9.96;12,187.10,224.50,35.23,10.40;12,222.60,216.80,20.49,10.40;12,222.36,230.18,3.72,6.64;12,228.84,225.54,283.96,8.97;12,114.96,237.42,109.19,8.97;12,232.50,229.40,68.30,12.00;12,308.39,237.42,204.60,9.96;12,114.96,249.42,244.13,8.97;12,359.60,240.70,20.49,10.40;12,359.40,254.06,3.72,6.64;12,363.48,249.42,149.37,8.97;12,114.96,261.42,31.16,8.97;12,146.40,252.70,20.49,10.40;12,146.16,266.06,3.72,6.64;12,152.88,261.42,334.15,8.97"><head>5 .&amp; ¤ 0</head><label>50</label><figDesc>Docs 2 ¤ Docs ¥ 2 -Finally, since we started with a pool of Wikipedia documents from the SQL dumps because of the &amp; ¡¢ 0 X 0 &amp; 1 ¢¥¤ §¢ ¨¡ action, we need to map the documents in Docs 2 to their corresponding GikiP 2008 documents, generating Docs ¥ 2 , which is the final result. The URLs of Docs ¥ 2 are then added to the submission file. The mapping step is done semi-automatically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,96.00,68.94,410.68,140.85"><head>Table 1 :</head><label>1</label><figDesc>Topic titles in GikiP 2008</figDesc><table coords="4,96.00,80.82,410.68,128.97"><row><cell>ID</cell><cell>English topic title</cell></row><row><cell>GP1</cell><cell>Which waterfalls are used in the film "The Last of the Mohicans"?</cell></row><row><cell>GP2</cell><cell>Which Vienna circle members or visitors were born outside the Austria-Hungarian empire or</cell></row><row><cell></cell><cell>Germany?</cell></row><row><cell>GP3</cell><cell>Portuguese rivers that flow through cities with more than 150,000 inhabitants</cell></row><row><cell>GP4</cell><cell>Which Swiss cantons border Germany?</cell></row><row><cell>GP5</cell><cell>Name all wars that occurred on Greek soil.</cell></row><row><cell>GP6</cell><cell>Which Australian mountains are higher than 2000 m?</cell></row><row><cell>GP7</cell><cell>African capitals with a population of two million inhabitants or more</cell></row><row><cell>GP8</cell><cell>Suspension bridges in Brazil</cell></row><row><cell>GP9</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,192.12,68.94,218.63,212.97"><head>Table 2 :</head><label>2</label><figDesc>Topic size of GikiP 2008, only automatic runs</figDesc><table coords="5,237.72,80.82,111.51,201.09"><row><cell cols="3">Topic Results Correct</cell></row><row><cell>GP1</cell><cell>5</cell><cell>1</cell></row><row><cell>GP2</cell><cell>31</cell><cell>7</cell></row><row><cell>GP3</cell><cell>28</cell><cell>8</cell></row><row><cell>GP4</cell><cell>79</cell><cell>21</cell></row><row><cell>GP5</cell><cell>69</cell><cell>21</cell></row><row><cell>GP6</cell><cell>36</cell><cell>7</cell></row><row><cell>GP7</cell><cell>90</cell><cell>33</cell></row><row><cell>GP8</cell><cell>49</cell><cell>2</cell></row><row><cell>GP9</cell><cell>49</cell><cell>17</cell></row><row><cell>GP10</cell><cell>53</cell><cell>2</cell></row><row><cell>GP11</cell><cell>35</cell><cell>23</cell></row><row><cell>GP12</cell><cell>51</cell><cell>25</cell></row><row><cell>GP13</cell><cell>9</cell><cell>4</cell></row><row><cell>GP14</cell><cell>60</cell><cell>6</cell></row><row><cell>GP15</cell><cell>18</cell><cell>2</cell></row><row><cell>Total</cell><cell>662</cell><cell>179</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,114.96,68.94,397.74,269.49"><head>Table 3 :</head><label>3</label><figDesc>GikiP participation in 2008</figDesc><table coords="7,114.96,80.82,397.69,193.89"><row><cell>System</cell><cell cols="3">Runs Type of Run</cell><cell>Size</cell><cell>Languages</cell></row><row><cell>GIRSA-WP</cell><cell>6</cell><cell cols="2">automatic</cell><cell>798 (372)</cell><cell>de, en, pt</cell></row><row><cell>RENOIR</cell><cell>1</cell><cell cols="2">semi-automatic</cell><cell>218</cell><cell>en, pt</cell></row><row><cell>WikipediaListQA@wlv</cell><cell>1</cell><cell cols="2">automatic</cell><cell>123</cell><cell>de, en, pt</cell></row><row><cell>Human</cell><cell>1</cell><cell>manual</cell><cell></cell><cell>235</cell><cell>de, en, pt</cell></row><row><cell></cell><cell cols="4">Table 4: GikiP results in 2008</cell></row><row><cell>Run</cell><cell cols="5">Answers Correct Avg. Prec Score</cell></row><row><cell>GIRSA-WP (best)</cell><cell></cell><cell>79</cell><cell>9</cell><cell>0.107</cell><cell>0.704</cell></row><row><cell cols="2">GIRSA-WP (all runs merged)</cell><cell>372</cell><cell>11</cell><cell>0.038</cell><cell>0.286</cell></row><row><cell>RENOIR</cell><cell></cell><cell>218</cell><cell>122</cell><cell>0.554</cell><cell>10.946</cell></row><row><cell>WikipediaListQA@wlv</cell><cell></cell><cell>123</cell><cell>93</cell><cell>0.632</cell><cell>15.815</cell></row><row><cell cols="6">GIRSA-WP, represented by Sven Hartrumpf and Johannes Leveling, Intelligent Information and</cell></row><row><cell cols="6">Communication Systems (IICS) at the FernUniversität in Hagen (Germany)</cell></row></table><note coords="7,114.96,285.66,397.74,8.97;7,114.96,297.54,340.78,8.97;7,114.96,317.46,397.73,8.97;7,114.96,329.46,228.36,8.97"><p>RENOIR (acronym for REMBRANDT's Extended NER On Interactive Retrievals), represented by Nuno Cardoso, University of Lisbon, Faculty of Sciences, LaSIGE, XLDB (Portugal) WikipediaListQA@wlv, represented by Iustin Dornescu, Research Group in Computational Linguistics (CLG) at the University of Wolverhampton (UK)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,110.88,143.58,411.10,309.09"><head></head><label></label><figDesc>Semi-automatic Maps a document from the Wikipedia dump to its counterpart in the GikiP 2008 collection.</figDesc><table coords="10,110.88,143.58,411.10,309.09"><row><cell></cell><cell></cell><cell>Automatic</cell><cell>Performs a simple term query search in the GikiP 2008 col-</cell></row><row><cell>&amp; ¡¢ 0 X 0 &amp; 1 ¢¤  §¢ ¢¡</cell><cell></cell><cell>Automatic</cell><cell>lection, and returns a list of Wikipedia documents. Searches the Wikipedia dumps for documents with the</cell></row><row><cell></cell><cell></cell><cell></cell><cell>given Wikipedia category, and returns a list of Wikipedia</cell></row><row><cell>&amp; ¡¢ 0 X ¦ d ¦ $</cell><cell></cell><cell>Automatic</cell><cell>documents. Searches the Wikipedia dumps for documents that link to a</cell></row><row><cell></cell><cell></cell><cell></cell><cell>given Wikipedia document.</cell></row><row><cell cols="2">2. Mapping actions &amp; ¤ 0</cell><cell></cell></row><row><cell>&amp; ¦ ¡</cell><cell></cell><cell cols="2">Semi-automatic Maps a NE to its corresponding document in the GikiP</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2008 collection.</cell></row><row><cell cols="2">3. Annotation actions ¢ ¡ £ ' ¥¢ &amp; ¡¦ " 1</cell><cell>Automatic</cell><cell>Annotates selected Wikipedia document(s) with REM-</cell></row><row><cell>¢ ¡ £ ' 6¨ ¤ 0 1 ¤ ¦ ¡</cell><cell></cell><cell>Automatic</cell><cell>BRANDT, generating lists of NEs for each document. Invokes REMBRANDT to classify the title of a given</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Wikipedia document, generating the respective NE.</cell></row><row><cell cols="2">4. Filtering actions ©Qd 1 ¨¦ ¡ ' £¡ 1 ¡ ¡</cell><cell>Automatic</cell><cell>Filters a list of NEs of a given classification category, gen-</cell></row><row><cell cols="3">©Qd 1 ¨ ¤ 0 ' £¡ 1 ¡¢ ¡ Automatic</cell><cell>erating a subset of NEs. Filters a list of Wikipedia documents by having (or not) a</cell></row><row><cell>©Qd 1 ¨ ¤ 0 ' £¡</cell><cell cols="2">&amp; d Manual</cell><cell>given term/pattern Filters a list of Wikipedia document by evaluating a condi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>tion for a given subset of NEs. For instance, if the document</cell></row><row><cell></cell><cell></cell><cell></cell><cell>has a number NE greater than 1000, or if it has a place name</cell></row><row><cell></cell><cell></cell><cell></cell><cell>NE within Europe.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,90.00,119.34,423.17,48.71"><head>Table 5 :</head><label>5</label><figDesc>RENOIR's query procedures for the GikiP topics. Docs n represents lists of Wikipedia documents, NE n represents lists of NEs. Docs 1 Docs 1 , NE 1</figDesc><table coords="11,123.60,143.18,230.40,24.87"><row><cell>GP Query Procedure 1 ¢¡ ¤£ ¦¥ ¨ § © ¡ ¥ "last mohicans film" Docs 1 ;</cell><cell>¥ ¡ ¦¥ ¤£ ¦ ¤</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,104.40,670.54,100.81,7.17"><p>http://www.linguateca.pt/GikiP/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,104.40,674.86,407.87,7.17;3,90.00,684.34,422.53,7.17;3,90.00,693.70,287.66,7.17"><p>This was also a way to diminish the threshold for QA participants, who already had to process those collections anyway. However, in the end it apparently scared away other participants who had other Wikipedia versions at their disposal, and produced some problems for the organisers as well since some runs did provide out-of-collection answers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,104.40,683.62,171.26,7.17"><p>And possibly also in German. Here, opinions diverge.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,104.40,693.46,404.78,7.17"><p>Note that the answers themselves may not necessarily be different, they just need to correspond to a different Wikipedia article.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,104.40,675.46,407.91,7.17;6,90.00,684.94,422.55,7.17;6,90.00,694.42,151.29,7.17"><p>This should by no means be read as a critic of QA@CLEF organizers. On the contrary, they gave us at once access to their data and considered the task interesting. It just happened that the task was located at GeoCLEF because we were involved in GeoCLEF organization and not in QA@CLEF at the time.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We are grateful to <rs type="person">Ross Purves</rs> for checking the English rendering, to <rs type="person">Sven Hartrumpf</rs> for analysing and debugging the German and English versions with the WOCADI parser, to <rs type="person">Anselmo Peñas</rs> for making the Wikipedia collections available to GikiP participants, and most particularly to <rs type="person">Paulo Rocha</rs> for providing a fully manual run in two days.</p><p>The organization work was done in the scope of the Linguateca project, jointly funded by the <rs type="funder">Portuguese Government</rs> and the <rs type="funder">European Union (FEDER and FSE)</rs> under contract ref.</p><p><rs type="grantNumber">POSC/339/1.3/C/NAC</rs>.</p><p>The development of the WikipediaListQA@wlv system was partly supported by the <rs type="funder">EU</rs> funded project <rs type="projectName">QALL-ME</rs> (<rs type="grantNumber">FP6 IST-033860</rs>).</p><p>The development of <rs type="projectName">RENOIR</rs> was supported by grant <rs type="grantNumber">SFRH/BD/29817/2006</rs> from <rs type="funder">FCT (Portugal)</rs>.</p></div>
			</div>
			
			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>


			<listOrg type="funding">
				<org type="funding" xml:id="_7PANwns">
					<idno type="grant-number">POSC/339/1.3/C/NAC</idno>
				</org>
				<org type="funded-project" xml:id="_k2SYfhw">
					<idno type="grant-number">FP6 IST-033860</idno>
					<orgName type="project" subtype="full">QALL-ME</orgName>
				</org>
				<org type="funded-project" xml:id="_2ksHqrf">
					<idno type="grant-number">SFRH/BD/29817/2006</idno>
					<orgName type="project" subtype="full">RENOIR</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="15,111.58,672.66,78.86,8.97" xml:id="b0">
	<monogr>
		<title level="m" coord="15,111.58,672.66,75.50,8.97">Alexa top 500 sites</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,194.10,662.75,106.10,14.20;15,320.60,664.70,5.70,11.60;15,341.60,664.70,4.90,11.60;15,352.20,669.05,4.90,14.20;15,357.60,664.70,36.60,11.60;15,399.30,669.05,5.70,14.20;15,403.70,664.70,53.00,11.60;15,461.53,672.66,2.51,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="15,231.10,664.70,69.10,11.60;15,320.60,664.70,5.70,11.60;15,341.60,664.70,4.90,11.60;15,352.20,669.05,4.90,14.20;15,357.60,664.70,36.60,11.60;15,399.30,669.05,5.70,14.20;15,403.70,664.70,47.80,11.60">6T£ 4 &quot; 13 £ ©P( ¤ QI ¡ 2 9 ¤B ¢ ¡ § ¦ 9 ¢ I ) &quot;2 &quot; ¡ 1H §4 1 ¤5 §£</title>
		<author>
			<persName coords=""><surname>F "b ¡ S S S</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.59,62.10,401.01,8.97;16,111.60,74.10,401.20,8.97;16,111.60,85.98,401.04,8.97;16,111.60,97.98,278.57,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,111.60,74.10,187.65,8.97">DBpedia: A Nucleus for a Web of Open Data</title>
		<author>
			<persName coords=""><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,321.26,74.10,191.54,8.97;16,111.60,85.98,335.40,8.97;16,209.61,97.98,47.67,8.97">The Semantic Web: 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007 + ASWC 2007</title>
		<meeting><address><addrLine>Busan, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">November 11-15, 2007. 2008</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct coords="16,111.59,116.94,401.08,8.97;16,111.60,128.82,128.95,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,254.58,116.94,62.83,8.97">MG4J at TREC</title>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastiano</forename><surname>Vigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,357.44,116.94,155.23,8.97;16,111.60,128.82,99.82,8.97">Proceedings of the 14th Text REtrieval Conference (TREC 2005)</title>
		<meeting>the 14th Text REtrieval Conference (TREC 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.59,147.78,400.93,8.97;16,111.60,159.78,401.50,8.97;16,111.60,171.66,401.00,8.97;16,111.60,183.66,67.71,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,175.70,147.78,336.81,8.97;16,111.60,159.78,119.76,8.97">REMBRANDT -Reconhecimento de Entidades Mencionadas Baseado em Relac ¸ões e Análise Detalhada do Texto</title>
		<author>
			<persName coords=""><forename type="first">Nuno</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,422.54,159.78,90.57,8.97;16,111.60,171.66,382.48,8.97">Desafios na avaliac ¸ão conjunta do reconhecimento de entidades mencionadas: Actas do Encontro do Segundo HAREM</title>
		<editor>
			<persName><forename type="first">Cristina</forename><surname>Mota</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008-09-11">11 September 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.59,202.62,400.96,8.97;16,111.60,214.50,401.04,8.97;16,111.60,226.50,400.91,8.97;16,111.60,238.50,401.18,8.97;16,111.60,250.38,401.37,8.97;16,111.60,262.38,91.31,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,425.48,202.62,87.06,8.97;16,111.60,214.50,250.37,8.97">GeoCLEF: the CLEF 2005 Cross-Language Geographic Information Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">Frederic</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ray</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hideo</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,162.53,238.50,350.25,8.97;16,111.60,250.38,104.17,8.97">Acessing Multilingual information Repositories: 6th Workshop of the Cross-Language Evaluation Forum, CLEF</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Frederic</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><surname>De Rijke</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005. 2006</date>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="908" to="919" />
		</imprint>
	</monogr>
	<note>Revised Selected papers</note>
</biblStruct>

<biblStruct coords="16,111.58,281.34,401.16,8.97;16,111.60,293.22,401.19,8.97;16,111.60,305.22,401.18,8.97;16,111.60,317.22,401.05,8.97;16,111.60,329.10,401.08,8.97;16,111.60,341.10,401.15,8.97;16,111.60,352.98,218.21,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,380.92,293.22,131.87,8.97;16,111.60,305.22,270.99,8.97">GeoCLEF 2006: the CLEF 2006 Cross-Language Geographic Information Retrieval Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ray</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kerstin</forename><surname>Bishoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christa</forename><surname>Womser-Hacker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giorgio</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,227.86,329.10,284.82,8.97;16,111.60,341.10,240.69,8.97">Evaluation of Multilingual and Multi-modal Information Retrieval: 7th Workshop of the Cross-Language Evaluation Forum, CLEF</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maximilian</forename><surname>Stempfhuber</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006. 2007</date>
			<biblScope unit="volume">4730</biblScope>
			<biblScope unit="page" from="852" to="876" />
		</imprint>
	</monogr>
	<note>Revised selected papers</note>
</biblStruct>

<biblStruct coords="16,111.58,371.94,400.94,8.97;16,111.60,383.94,400.91,8.97;16,111.60,395.82,401.05,8.97;16,111.60,407.82,242.58,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,421.28,383.94,91.23,8.97;16,111.60,395.82,178.59,8.97">Overview of the CLEF 2007 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christelle</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Cristea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,479.20,395.82,33.44,8.97;16,111.60,407.82,140.48,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">Alessandro</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
			<biblScope unit="page" from="19" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.59,426.78,400.93,8.97;16,111.60,438.66,100.87,8.97" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="16,185.54,426.78,219.37,8.97">Hybrid Disambiguation in Natural Language Analysis</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Der Andere Verlag</publisher>
			<pubPlace>Osnabrück, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.59,457.62,401.04,8.97;16,111.60,469.62,401.15,8.97;16,111.60,481.50,400.82,8.97;16,111.60,493.50,401.00,8.97;16,111.60,505.50,22.64,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,186.02,457.62,307.31,8.97">Question answering using sentence parsing and semantic network matching</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,144.56,481.50,367.87,8.97;16,111.60,493.50,144.79,8.97">Multilingual Information Access for Text, Speech and Images: 5th Workshop of the Cross-Language Evaluation Forum, CLEF</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004. 2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.58,524.34,400.92,8.97;16,111.60,536.34,401.01,8.97;16,111.60,548.34,333.65,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,181.81,524.34,189.91,8.97">Semantic decomposition for question answering</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,351.77,536.34,160.84,8.97;16,111.60,548.34,157.94,8.97">Proceedings of the 18th European Conference on Artificial Intelligence (ECAI)</title>
		<editor>
			<persName><forename type="first">Malik</forename><surname>Ghallab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Constantine</forename><forename type="middle">D</forename><surname>Spyropoulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nikos</forename><surname>Fakotakis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nikos</forename><surname>Avouris</surname></persName>
		</editor>
		<meeting>the 18th European Conference on Artificial Intelligence (ECAI)<address><addrLine>Patras, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-07">July 2008</date>
			<biblScope unit="page" from="313" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.58,567.18,400.94,8.97;16,111.60,579.18,47.45,8.97" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Hatcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Otis</forename><surname>Gospodnetic</surname></persName>
		</author>
		<title level="m" coord="16,259.85,567.18,139.50,8.97">Lucene in Action (In Action series)</title>
		<meeting><address><addrLine>Greenwich, CT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Manning</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.58,598.14,401.27,8.97;16,111.60,610.02,52.48,8.97" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="16,189.61,598.14,274.54,8.97">Knowledge Representation and the Semantics of Natural Language</title>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Helbig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.58,628.98,163.62,8.97" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="16,111.58,628.98,134.87,8.97">ImageCLEF&apos;s WikipediaMM task</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,278.70,619.05,178.80,14.20;16,462.28,628.98,2.51,8.97" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="16,320.60,621.00,136.90,11.60">I )£ 1H §( 14 &quot; ¦! #T ¦0 &quot;H R ¢ ¡ S )¡ QV )¡ B § 12 9¡ ¤£</title>
		<author>
			<persName coords=""><forename type="first">F "b ¡ ¡</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.58,647.94,400.97,8.97;16,111.60,659.94,400.94,8.97;16,111.60,671.82,401.18,8.97;16,111.60,683.82,401.07,8.97;16,111.60,695.70,282.20,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,270.75,647.94,171.81,8.97">Overview of the WiQA Task at CLEF 2006</title>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,284.67,671.82,228.11,8.97;16,111.60,683.82,304.83,8.97">Evaluation of Multilingual and Multi-modal Information Retrieval: 7th Workshop of the Cross-Language Evaluation Forum, CLEF</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maximilian</forename><surname>Stempfhuber</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006. 2007</date>
			<biblScope unit="volume">4730</biblScope>
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
	<note>Revised selected papers</note>
</biblStruct>

<biblStruct coords="17,111.58,62.10,401.00,8.97;17,111.60,74.10,401.19,8.97;17,111.60,85.98,83.39,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="17,292.83,62.10,219.75,8.97;17,111.60,74.10,39.69,8.97">WiQA: Evaluating Multi-lingual Focused Access to Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,172.49,74.10,305.32,8.97">The First International Workshop on Evaluating Information Access (EVIA)</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-05-15">May 15 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.58,106.02,401.14,8.97;17,111.60,117.90,401.01,8.97;17,111.60,129.90,400.98,8.97;17,111.60,141.78,53.80,8.97" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="17,150.31,117.90,214.11,8.97">Overview of IR Tasks at the first NTCIR Workshop</title>
		<author>
			<persName coords=""><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kazuko</forename><surname>Kuriyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Toshihiko</forename><surname>Nozue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Koji</forename><surname>Eguchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiroyuki</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Souichiro</forename><surname>Hidaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,388.65,117.90,123.96,8.97;17,111.60,129.90,285.81,8.97">Proceedings of the 1st NTCIR Workshop on Research in Japanese Text Retrieval and Term Recognition</title>
		<meeting>the 1st NTCIR Workshop on Research in Japanese Text Retrieval and Term Recognition<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
			<biblScope unit="page" from="11" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.58,161.70,401.04,8.97;17,111.60,173.70,401.07,8.97;17,111.60,185.70,401.18,8.97;17,111.60,197.58,401.16,8.97;17,111.60,209.58,217.97,8.97" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="17,283.69,161.70,228.93,8.97;17,111.60,173.70,24.12,8.97">Inferring location names for geographic information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,344.72,185.70,168.06,8.97;17,111.60,197.58,344.01,8.97">Advances in Multilingual and Multimodal Information Retrieval: 8th Workshop of the Cross-Language Evaluation Forum, CLEF</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vivien</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007. 2008</date>
			<biblScope unit="volume">5152</biblScope>
			<biblScope unit="page" from="773" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.58,229.50,401.30,8.97;17,111.60,241.50,247.25,8.97" xml:id="b19">
	<monogr>
		<title level="m" coord="17,111.58,229.50,401.30,8.97;17,111.60,241.50,42.29,8.97">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2008-05-30">28-30th May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.58,261.42,401.05,8.97;17,111.60,273.30,401.32,8.97;17,111.60,285.30,401.06,8.97;17,111.60,297.30,288.01,8.97" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="17,309.85,273.30,203.06,8.97;17,111.60,285.30,202.56,8.97">GeoCLEF 2007: the CLEF 2007 Cross-Language Geographic Information Retrieval Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giorgio</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ray</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christa</forename><surname>Womser-Hacker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,111.60,297.30,178.14,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">Alessandro</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-21">19-21th September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.58,317.22,401.08,8.97;17,111.60,329.10,378.80,8.97" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="17,253.03,317.22,230.52,8.97">Cross-Language System Evaluation: the CLEF campaigns</title>
		<author>
			<persName coords=""><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,491.45,317.22,21.21,8.97;17,111.60,329.10,270.57,8.97">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1067" to="1072" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.58,349.02,401.17,8.97;17,111.60,361.02,401.02,8.97;17,111.60,372.90,401.31,8.97;17,111.60,384.90,401.09,8.97;17,111.60,396.90,36.43,8.97" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="17,247.13,349.02,77.82,8.97">Portuguese at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nuno</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,111.60,372.90,401.31,8.97;17,111.60,384.90,56.11,8.97">Acessing Multilingual Information Repositories: 6th Workshop of the Cross-Language Evaluation Forum, CLEF</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Frederic</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><surname>De Rijke</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005. 2006</date>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="1007" to="1010" />
		</imprint>
	</monogr>
	<note>Revised selected papers</note>
</biblStruct>

<biblStruct coords="17,111.58,416.82,401.65,9.09;17,111.60,428.70,376.41,9.09" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nuno</forename><surname>Cardoso</surname></persName>
		</author>
		<title level="m" coord="17,279.48,416.82,233.74,9.09;17,111.60,428.70,297.23,9.09">Reconhecimento de entidades mencionadas em portugu ês: Documentac ¸ão e actas do HAREM, a primeira avaliac ¸ão conjunta na área</title>
		<imprint>
			<publisher>Linguateca</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.58,448.62,401.06,8.97;17,111.60,460.62,241.11,8.97" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="17,250.13,448.62,225.61,8.97">GikiP: Evaluating geographical answers from Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nuno</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,495.97,448.62,16.67,8.97;17,111.60,460.62,18.11,8.97">GIR 2008</title>
		<meeting><address><addrLine>Napa Valley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-11-01">1 November 2008</date>
		</imprint>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct coords="17,111.58,480.54,401.06,8.97;17,111.60,492.54,401.03,8.97;17,111.60,504.42,193.75,8.97" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="17,422.25,480.54,90.39,8.97;17,111.60,492.54,105.61,8.97">Second HAREM: new challenges and old wisdom</title>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cláudia</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo Gonc ¸alo</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paula</forename><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,358.01,492.54,38.49,8.97;17,488.77,492.54,19.09,8.97">PROPOR</title>
		<editor>
			<persName><forename type="first">António</forename><surname>Teixeira</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">5190</biblScope>
			<biblScope unit="page" from="8" to="10" />
			<date type="published" when="2008-09">2008. September 2008</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>LNAI</note>
</biblStruct>

<biblStruct coords="17,111.58,524.34,401.18,8.97;17,111.60,536.34,401.15,8.97;17,111.60,548.34,95.61,8.97" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="17,347.69,524.34,165.07,8.97;17,111.60,536.34,89.46,8.97">HAREM: An Advanced NER Evaluation Contest for Portuguese</title>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nuno</forename><surname>Seco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nuno</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rui</forename><surname>Vilela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,348.45,536.34,107.03,8.97">Proceedings of LREC&apos;2006</title>
		<editor>
			<persName><forename type="first">Nicoletta</forename><surname>Calzolari</surname></persName>
		</editor>
		<meeting>LREC&apos;2006</meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="page" from="22" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.58,568.26,401.27,8.97;17,111.60,580.14,401.40,8.97;17,111.60,592.14,161.69,8.97" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="17,361.72,568.26,146.50,8.97">Yago: a core of semantic knowledge</title>
		<author>
			<persName coords=""><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,122.39,580.14,320.38,8.97">WWW &apos;07: Proceedings of the 16th international conference on World Wide Web</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.58,612.06,401.01,8.97;17,111.60,624.06,401.11,8.97;17,111.60,635.94,22.06,8.97" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="17,260.56,612.06,142.91,8.97">A non-projective dependency parser</title>
		<author>
			<persName coords=""><forename type="first">Pasi</forename><surname>Tapanainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timo</forename><surname>Järvinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,422.46,612.06,90.13,8.97;17,111.60,624.06,213.19,8.97">Proceedings of the 5th Conference of Applied Natural Language Processing</title>
		<meeting>the 5th Conference of Applied Natural Language Processing<address><addrLine>Washington D.C., USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.58,655.86,401.16,8.97;17,111.60,667.86,248.23,8.97" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="17,314.00,655.86,198.74,8.97;17,111.60,667.86,35.06,8.97">TREC: Experiment and Evaluation in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Massachusetts, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,111.58,62.10,401.04,8.97;18,111.60,74.10,401.00,8.97;18,111.60,85.98,22.64,8.97" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="18,164.55,62.10,85.36,8.97">Measuring Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,271.11,62.10,241.51,8.97;18,111.60,74.10,264.90,8.97">Proceedings of the 10th International Conference of the International Society for Scientometrics and Informetrics, ISSI&apos;2005</title>
		<meeting>the 10th International Conference of the International Society for Scientometrics and Informetrics, ISSI&apos;2005<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-07-28">24-28 July 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,111.58,106.02,43.74,8.97;18,158.80,96.05,121.00,14.20;18,290.20,106.02,2.51,8.97" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="18,172.00,96.05,107.80,14.20">B ¡ S S S 68S )¡ QV )¡ B &quot;2 ¢¡ G£ C 10 1H</title>
		<author>
			<persName coords=""><forename type="middle">F</forename><surname>Wikipedia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="18,111.58,125.94,209.03,8.97" xml:id="b32">
	<monogr>
		<title level="m" type="main" coord="18,111.58,125.94,179.70,8.97">WiQA: Question Answering using Wikipedia</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,324.10,115.95,163.60,14.20;18,492.09,125.94,2.51,8.97" xml:id="b33">
	<monogr>
		<title level="m" type="main" coord="18,337.30,115.95,150.40,14.20">B ¡ ¡ ¦4 ¤B ) 7¨ ( ¡ ¤ G¥ )( ¦ A8@ E £ A8¥ 94 )¡ § &amp;</title>
		<author>
			<persName coords=""><forename type="first">F</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
