<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,109.32,98.73,384.63,15.51;1,215.64,120.69,171.77,15.51">Cheshire at GeoCLEF 2008: Text and Fusion Approaches for GIR</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,269.76,154.07,63.74,9.96"><forename type="first">Ray</forename><forename type="middle">R</forename><surname>Larson</surname></persName>
							<email>ray@sims.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,109.32,98.73,384.63,15.51;1,215.64,120.69,171.77,15.51">Cheshire at GeoCLEF 2008: Text and Fusion Approaches for GIR</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5676464FA3272745C911BFD7A283E849</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval Algorithms</term>
					<term>Performance</term>
					<term>Measurement Cheshire II</term>
					<term>Logistic Regression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we will briefly describe the approaches taken by Berkeley for the main GeoCLEF 2008 tasks (Mono and Bilingual retrieval). The approach this year used probabilistic text retrieval based on logistic regression and incorporating blind relevance feedback for all of the runs and in addition we ran a number of tests combining this type of search with OKAPI BM25 searches using a fusion approach. All translation for bilingual tasks was performed using the LEC Power Translator PC-based MT system.</p><p>Our results were good overall with Cheshire systems runs appearing in the top 5 participants for each task (German, English and Portuguese both Monolingual and Bilingual) with the highest ranked runs for Monolingual Portuguese and for Bilingual German, English and Portuguese. All of these top-ranked runs used the fusion approach.</p><p>However, once again this year we did not attempt to do any specialized geographic processing, because it appears that purely textual approaches to GIR are more effective when only textual topics, lacking explicit geographic coordinate constraints, are used.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Geographic Information Retrieval (GIR) as it was originally defined was concerned with providing access to georeferenced information resources using a combination of Information Retrieval (IR) and Data Retrieval (DR) (or database) methods <ref type="bibr" coords="1,301.35,645.71,13.68,9.96" target="#b5">[6]</ref>. In GeoCLEF the nature of the topics has tended to emphasize the IR aspects of GIR, largely because no explicit georeferencing of documents or topics has been supplied to the experimenter.</p><p>Without the explicit georeferencing of documents and/or topics the experimenter (or searcher) is faced with attempting to provide such georeferencing and therefore solving all of the attendent problems of ambiguity and multiplicity of toponyms and the issues of name polysemy that explicit georeferencing is intended to alleviate. An alternative approach is to, in effect, ignore geographic clues by treating them like any other term in a normal IR search process. Our approach this year has been to take this latter approach, and use only text retrieval methods on the provided topics with no explicit identification or treatment of toponyms. This paper describes the retrieval algorithms and evaluation results for Berkeley's official submissions for the GeoCLEF 2008 track. All of the submitted runs were automatic without manual intervention in the queries (or translations). We submitted nine Monolingual runs (three German, three English, and three Portuguese) and eighteen Bilingual runs (three runs for each of the three languages to each each other language). The runs varied in the topic elements used, and whether or not a fusion approach (described below) was used.</p><p>This paper first describes the retrieval algorithms and fusion operations used for our submissions, followed by a discussion of the processing used for the runs. We then examine the results obtained for our officially submitted runs, and finally present conclusions and future directions for GeoCLEF participation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Retrieval Algorithms and Fusion Operators</head><p>Note that this section is virtually identical to one that appears in our Adhoc-TEL and Domain Specific papers, with the addition of the Okapi BM-25 description and fusion operations subsections. The basic form and variables of the Logistic Regression (LR) algorithm used for all of our submissions was originally developed by Cooper, et al. <ref type="bibr" coords="2,326.78,319.43,10.00,9.96" target="#b4">[5]</ref>. As originally formulated, the LR model of probabilistic IR attempts to estimate the probability of relevance for each document based on a set of statistics about a document collection and a set of queries in combination with a set of weighting coefficients for those statistics. The statistics to be used and the values of the coefficients are obtained from regression analysis of a sample of a collection (or similar test collection) for some set of queries where relevance and non-relevance has been determined. More formally, given a particular query and a particular document in a collection P (R | Q, D) is calculated and the documents or components are presented to the user ranked in order of decreasing values of that probability. To avoid invalid probability values, the usual calculation of P (R | Q, D) uses the "log odds" of relevance given a set of S statistics, s i , derived from the query and database, such that:</p><formula xml:id="formula_0" coords="2,235.08,448.36,277.96,30.57">log O(R | Q, D) = b 0 + S i=1 b i s i (1)</formula><p>where b 0 is the intercept term and the b i are the coefficients obtained from the regression analysis of the sample collection and relevance judgements. The final ranking is determined by the conversion of the log odds form to probabilities:</p><formula xml:id="formula_1" coords="2,232.44,529.15,280.59,25.12">P (R | Q, D) = e log O(R|Q,D) 1 + e log O(R|Q,D)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TREC2 Logistic Regression Algorithm</head><p>For GeoCLEF we used a version the Logistic Regression (LR) algorithm that has been used very successfully in Cross-Language IR by Berkeley researchers for a number of years <ref type="bibr" coords="2,442.52,598.67,11.84,9.96" target="#b2">[3]</ref>. The formal definition of the TREC2 Logistic Regression algorithm used is:</p><formula xml:id="formula_2" coords="2,184.08,641.03,233.68,61.09">log O(R|C, Q) = log p(R|C, Q) 1 -p(R|C, Q) = log p(R|C, Q) p(R|C, Q) = c 0 + c 1 * 1 |Q c | + 1 |Qc| i=1 qtf i ql + 35 + c 2 * 1 |Q c | + 1 |Qc| i=1 log tf i cl + 80 (3) -c 3 * 1 |Q c | + 1 |Qc| i=1 log ctf i N t + c 4 * |Q c |</formula><p>where C denotes a document component (i.e., an indexed part of a document which may be the entire document) and Q a query, R is a relevance variable, c k are the k coefficients obtained though the regression analysis.</p><formula xml:id="formula_3" coords="3,90.00,185.27,422.96,62.17">p(R|C, Q) is the probability that document component C is relevant to query Q, p(R|C, Q) the probability that document component C is not relevant to query Q, which is 1.0 - p(R|C, Q) |Q c |</formula><p>If stopwords are removed from indexing, then ql, cl, and N t are the query length, document length, and collection length, respectively. If the query terms are re-weighted (in feedback, for example), then qtf i is no longer the original term frequency, but the new weight, and ql is the sum of the new weight values for the query terms. Note that, unlike the document and collection lengths, query length is the "optimized" relative frequency without first taking the log over the matching terms.</p><p>The coefficients were determined by fitting the logistic regression model specified in log O(R|C, Q) to TREC training data using a statistical software package. The coefficients, c k , used for our official runs are the same as those described by Chen <ref type="bibr" coords="3,320.65,492.11,13.51,9.96" target="#b0">[1]</ref>. These were: c 0 = -3.51, c 1 = 37.4, c 2 = 0.330, c 3 = 0.1937 and c 4 = 0.0929. Further details on the TREC2 version of the Logistic Regression algorithm may be found in Cooper et al. <ref type="bibr" coords="3,321.15,516.11,10.00,9.96" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Blind Relevance Feedback</head><p>In addition to the direct retrieval of documents using the TREC2 logistic regression algorithm described above, we have implemented a form of "blind relevance feedback" as a supplement to the basic algorithm. The algorithm used for blind feedback was originally developed and described by Chen <ref type="bibr" coords="3,115.44,598.19,10.00,9.96" target="#b1">[2]</ref>. Blind relevance feedback has become established in the information retrieval community due to its consistent improvement of initial search results as seen in TREC, CLEF and other retrieval evaluations <ref type="bibr" coords="3,179.78,622.07,10.00,9.96" target="#b6">[7]</ref>. The blind feedback algorithm is based on the probabilistic term relevance weighting formula developed by Robertson and Sparck Jones <ref type="bibr" coords="3,358.97,634.07,14.56,9.96" target="#b9">[10]</ref>.</p><p>Blind relevance feedback is typically performed in two stages. First, an initial search using the original topic statement is performed, after which a number of terms are selected from some number of the top-ranked documents (which are presumed to be relevant). The selected terms are then weighted and then merged with the initial query to formulate a new query. Finally the reweighted and expanded query is submitted against the same collection to produce a final ranked list of documents. Obviously there are important choices to be made regarding the number of top-ranked documents to consider, and the number of terms to extract from those documents. For ImageCLEF this year, having no prior data to guide us, we chose to use the top 10 terms from 10 top-ranked documents. The terms were chosen by extracting the document vectors for each of the 10 and computing the Robertson and Sparck Jones term relevance weight for each document. This weight is based on a contingency table where the counts of 4 different conditions for combinations of (assumed) relevance and whether or not the term is, or is not in a document. Table <ref type="table" coords="4,478.93,133.19,4.98,9.96" target="#tab_1">1</ref> shows this contingency table. </p><formula xml:id="formula_4" coords="4,187.08,199.55,228.37,34.68">R t N t -R t N t Not in doc R -R t N -N t -R + R t N -N t R N -R N</formula><p>The relevance weight is calculated using the assumption that the first 10 documents are relevant and all others are not. For each term in these documents the following weight is calculated:</p><formula xml:id="formula_5" coords="4,255.24,292.00,257.80,30.29">w t = log Rt R-Rt Nt-Rt N -Nt-R+Rt (4)</formula><p>The 10 terms (including those that appeared in the original query) with the highest w t are selected and added to the original query terms. For the terms not in the original query, the new "term frequency" (qtf i in main LR equation above) is set to 0.5. Terms that were in the original query, but are not in the top 10 terms are left with their original qtf i . For terms in the top 10 and in the original query the new qtf i is set to 1.5 times the original qtf i for the query. The new query is then processed using the same LR algorithm as shown in Equation <ref type="formula" coords="4,404.05,388.31,4.98,9.96">4</ref>and the ranked results returned as the response for that topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Okapi BM-25 Algorithm</head><p>The version of the Okapi BM-25 algorithm used in these experiments is based on the description of the algorithm in Robertson <ref type="bibr" coords="4,228.51,458.51,14.60,9.96" target="#b11">[12]</ref>, and in TREC notebook proceedings <ref type="bibr" coords="4,416.40,458.51,14.60,9.96" target="#b10">[11]</ref>. As with the LR algorithm, we have adapted the Okapi BM-25 algorithm to deal with document components :</p><formula xml:id="formula_6" coords="4,232.20,491.83,280.83,31.73">|Qc| j=1 w (1) (k 1 + 1)tf j K + tf j (k 3 + 1)qtf j k 3 + qtf j<label>(5)</label></formula><p>Where (in addition to the variables already defined): w (1) is the Robertson-Sparck Jones weight:</p><formula xml:id="formula_7" coords="4,90.00,555.35,125.79,10.65">K is k 1 ((1 -b) + b • dl/avcl)</formula><formula xml:id="formula_8" coords="4,252.72,639.04,121.24,34.00">w (1) = log ( r+0.5 R-r+0.5 ) ( nt j -r+0.5 N -nt j -R-r+0.5 )</formula><p>r is the number of relevant components of a given type that contain a given term, R is the total number of relevant components of a given type for the query.</p><p>Our current implementation uses only the a priori version (i.e., without relevance information) of the Robertson-Sparck Jones weights, and therefore the w (1) value is effectively just an IDF weighting. The results of searches using our implementation of Okapi BM-25 and the LR algorithm seemed sufficiently different to offer the kind of conditions where data fusion has been shown to be be most effective <ref type="bibr" coords="5,181.90,129.23,9.91,9.96" target="#b8">[9]</ref>, and our overlap analysis of results for each algorithm (described in the evaluation and discussion section) has confirmed this difference and the fit to the conditions for effective fusion of results.</p><p>The system used supports searches combining probabilistic and (strict) Boolean elements, as well as operators to support various merging operations for both types of intermediate result sets. However, in GeoCLEF we did not use this capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Fusion Operators</head><p>The Cheshire II system used in this evaluation provides a number of operators to combine the intermediate results of a search from different components or indexes. With these operators we have available an entire spectrum of combination methods ranging from strict Boolean operations to fuzzy Boolean and normalized score combinations for probabilistic and Boolean results. These operators are the means available for performing fusion operations between the results for different retrieval algorithms and the search results from different different components of a document. We will only describe one of these operators here, because it was the only type used in the GEOCLEF runs reported in this paper.</p><p>The MERGE PIVOT operator is used primarily to adjust the probability of relevance for one search result based on matching elements in another search result. It was developed primarily to adjust the probabilities of a search result consisting of sub-elements of a document (such as titles or paragraphs) based on the probability obtained for the same search over the entire document. It is basically a weighted combination of the probabilities based on a "DocPivot" fraction, such that:</p><formula xml:id="formula_9" coords="5,207.60,414.59,305.44,10.33">P n = DocP ivot * P d + (1 -DocP ivot) * P s<label>(6)</label></formula><p>where P d represents the document-level probability of relevance, P s represents the subelement probability, and P n represents the resulting new probability estimate.</p><p>For all of our fusion experiments this year, the P s was the estimated probability of relevance for a document obtained by a TREC2 with blind feedback search using the topic title and description (as described above) normalized using MINMAX normalization, and P d was an OKAPI BM-25 search using the topic title, description, and narrative, also normalized to 0-1 range using MINMAX normalization. The "DocP ivot" value used for all of the runs submitted was 0.29. Note that this is not the first time we have used some fusion approaches in GeoCLEF. In the first GeoCLEF (2005) we also employed fusion approaches in some of our runs, but these did not use the TREC2 with blind feedback algorithm <ref type="bibr" coords="5,286.38,540.11,12.77,9.96" target="#b7">[8]</ref>, which appears to make an important difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approaches for GeoCLEF</head><p>In this section we describe the specific approaches taken for our submitted runs for the GeoCLEF tasks. First we describe the indexing and term extraction methods used, and then the search features we used for the submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Indexing and Term Extraction</head><p>The Cheshire II system uses the XML structure of the documents to extract selected portions for indexing and retrieval. Any combination of tags can be used to define the index contents.  <ref type="table" coords="6,132.00,258.71,4.98,9.96" target="#tab_2">2</ref> lists the indexes created by the Cheshire II system for the GeoCLEF database and the document elements from which the contents of those indexes were extracted. The "Used" column in Table <ref type="table" coords="6,128.77,282.59,4.98,9.96" target="#tab_2">2</ref> indicates whether or not a particular index was used in the submitted GeoCLEF runs.</p><p>The georeferencing indexing subsystem of Cheshire II was used for the geotext, geopoint, and geobox indexes. This subsystem is intended to extract proper nouns from the text being indexed and then attempts to match them in a digital gazetteer. For GeoCLEF we used a gazetteer derived from the World Gazetteer (http://www.world-gazetteer.com) with 224698 entries in both English and German. The indexing subsystem provides three different index types: verified place names (an index of names which matched the gazetteer), point coordinates (latitude and longitude coordinates of the verified place name) and bounding box coordinates (bounding boxes for the matched places from the gazetteer). All three types were created, but we ended up not using any of the geographic indexes in this year's submissions. Because we do not use complete NLP parsing techniques, the system is unable to distinguish between proper nouns for places from those for individuals. This leads to errors in geographic assignment where, for example, articles about Irving Berlin might be tagged as refering to the city.</p><p>Because there was no explicit tagging of location-related terms in the collections used for GeoCLEF, we applied the above approach to the "TEXT", "LD", and "TX" elements of the records of the various collections. The part of news articles normally called the "dateline" indicating the location of the news story was not separately tagged in any of the GeoCLEF collections, but often appeared as the first part of the text for the story.</p><p>Geographic indexes were not created for the Portuguese sub-collection due to the lack of a suitable gazetteer. We plan for later work to substitute the "GeoNames" database which is much more detailed and provides a more complete geographical hierarchy in its records, along with alternate names in multiple languages.</p><p>For all indexing we used language-specific stoplists to exclude function words and very common words from the indexing and searching. The German language runs did not use decompounding in the indexing and querying processes to generate simple word forms from compounds. Although we tried again this year to make this work within the Cheshire system, we again lacked the time needed to implement it correctly.</p><p>The Snowball stemmer was used by Cheshire for language-specific stemming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Search Processing</head><p>Searching the GeoCLEF collection using the Cheshire II system involved using TCL scripts to parse the topics and submit the title and description or the title, description, and narrative from the topics. For monolingual search tasks we used the topics in the appropriate language (English, German, and Portuguese), for bilingual tasks the topics were translated from the source language to the target language using the LEC Power Translator PC-based machine translation system. Table <ref type="table" coords="7,117.24,481.43,4.98,9.96" target="#tab_3">3</ref> shows the runs submitted and the characteristics of each run, including which task it was submitted for, and the topic elements used in searching (this are indicated by using the "T" for the "title" element, "D" for the "description" element and "N" for the "narrative" element. The topic elements used were combined into a single probabilistic query. For those runs including the term "fusion" in the "Type" column, both TREC2 with blind feedback algorithm results (using only topic title and description) and Okapi BM-25 results (using title description and narrative) were combined using the MERGE PIVOT fusion operator described above with a pivot value of 0.29.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results for Submitted Runs</head><p>The summary results (as Mean Average Precision) for the submitted monolingual and bilingual runs for both English, German and Portuguese are also shown in Table <ref type="table" coords="7,413.52,631.79,3.90,9.96" target="#tab_3">3</ref>, the Recall-Precision curves for these runs are also shown in Figures 1 (for monolingual) and 2 (for bilingual). In Figures <ref type="figure" coords="7,126.24,655.79,4.98,9.96" target="#fig_1">1</ref> and<ref type="figure" coords="7,155.53,655.79,4.98,9.96" target="#fig_2">2</ref> the names for the monolingual runs represent the search algorithms and topic elements used for that particular target language, which can easily be compared with full names and descriptions in Table <ref type="table" coords="7,199.94,679.67,4.98,9.96" target="#tab_3">3</ref> (since each combination has only a single run). The names for Bilingual runs indicate the languages and topic elements used (except for PIV which is the fusion runs that Table <ref type="table" coords="8,133.32,453.35,4.98,9.96" target="#tab_3">3</ref> indicates the runs that had the highest overall MAP for the task and language by asterisks next to the run name.</p><p>Once again we found some rather interesting results among the official runs. For example, it seem clear that using topic title and description alone is a much better approach with our algorithms than using title description and narrative. However, in most cases the fusion approach either exceeds, or is very close to the performance of the TREC2 with blind feedback search with title and description due to "supporting evidence" for relevance from the Okapi BM-25 algorithm.</p><p>Last year the "weak man" in our runs was German, both monolingual and bilingual. At the time we were not sure if that might just be due to decompounding issues. However, it turned out that the real cause of last year's poor performance was that in indexing the database we had completely forgotten all of the SDA German collection, and thus the database we used in 2007 had only about half of the German documents. Needless to say, this was remedied in our submitted runs for this year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis and Conclusions</head><p>Because we used a similar processing approach (except for fusion operations) this year as we used for some of our runs submitted for GeoCLEF 2006 and 2007, we build Table <ref type="table" coords="8,451.69,663.59,4.98,9.96" target="#tab_5">4</ref>   Based on the summary data across participants for GeoCLEF available on the DIRECT system, it is apparent that the text-based and fusion approaches that we used this year are quite effective relative to some other approaches. In all of the six main GeoCLEF tasks there was a cheshire run in the top five participants listed. In Monolingual Portuguese and each Bilingual task (German, English, and Portuguese) one of the cheshire runs was ranked highest in MAP over all participants.</p><p>We need to do further testing with the fusion approaches, since the results are sensitive to the "pivot" value used. In addition, since the data show the relative importance of using only the title and description element instead of title, description, and narrative, we need to see if using using only title and description only in both parts of the fusion query further improves, or degrades performance.</p><p>The challenge for next year is to reintroduce actual geographic elements to the mix to see if, for example, automatic expansion of toponyms in the topic texts will enhance or degrade performance over the purely textual approach. Since this was done explicitly in many of the topic narratives we, and use of narrative proved counter-productive we suspect that such expansion may be more a source of noise instead of fostering improved results. In previous years it appeared that implicit or explicit toponym inclusion in queries led to better performance when compared to using titles and descriptions alone in retrieval. But given the results this time, some doubt has been cast over that assumption, at least for the algorithms that we have been using.</p><p>Although we did not do any explicit geographic processing other than in indexing for this year, we plan to do so in the future, because we still believe that use of geographical knowledge and evidence in topics and documents should improve performance over purely text-based methods. However, given the results reported above, this belief is still unsupported in our experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,90.00,575.27,319.21,10.65;4,90.00,595.19,245.98,9.96"><head>k 1 ,</head><label>1</label><figDesc>b and k 3 are parameters (1.5, 0.45 and 500, respectively, were used), avcl is the average component length measured in bytes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,97.44,399.11,408.30,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Monolingual Runs -German (top left), English (top right), and Portuguese (lower)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,90.00,399.11,423.19,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Bilingual Runs -To German (top left), To English (top right) and to Portuguese (lower)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,237.11,385.95,129.85"><head></head><label></label><figDesc>is the number of matching terms between a document component and a query, qtf i is the within-query frequency of the ith matching term, tf i is the within-document frequency of the ith matching term, ctf i is the occurrence frequency in a collection of the ith matching term,</figDesc><table coords="3,90.00,316.79,385.95,50.17"><row><cell>ql is query length (i.e., number of terms in a query like |Q| for non-feedback situations),</cell></row><row><cell>cl is component length (i.e., number of terms in a component), and</cell></row><row><cell>N t is collection length (i.e., number of terms in a test collection).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,178.68,175.91,245.72,33.60"><head>Table 1 :</head><label>1</label><figDesc>Contingency table for term relevance weighting Relevant Not Relevant In doc</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,105.00,68.39,301.63,200.28"><head>Table 2 :</head><label>2</label><figDesc>Cheshire II Indexes for GeoCLEF 2006Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,100.20,68.39,402.53,364.52"><head>Table 3 :</head><label>3</label><figDesc>Submitted GeoCLEF Runs</figDesc><table coords="7,100.20,78.86,402.53,354.05"><row><cell>Run Name</cell><cell>Description</cell><cell>Type</cell><cell>MAP</cell></row><row><cell>BERKGCMODETD</cell><cell>Monolingual German</cell><cell>TD auto</cell><cell>0.2295 *</cell></row><row><cell>BERKGCMODETDN</cell><cell>Monolingual German</cell><cell>TDN auto</cell><cell>0.2050</cell></row><row><cell>BERKMODETDNPIV</cell><cell>Monolingual German</cell><cell>TDN auto fusion</cell><cell>0.2292</cell></row><row><cell>BERKGCMOENTD</cell><cell>Monolingual English</cell><cell>TD auto</cell><cell>0.2652</cell></row><row><cell>BERKGCMOENTDN</cell><cell>Monolingual English</cell><cell>TDN auto</cell><cell>0.2001</cell></row><row><cell>BERKMOENTDNPIV</cell><cell>Monolingual English</cell><cell cols="2">TDN auto fusion 0.2685 *</cell></row><row><cell>BERKGCMOPTTD</cell><cell>Monolingual Portuguese</cell><cell>TD auto</cell><cell>0.2170</cell></row><row><cell>BERKGCMOPTTDN</cell><cell>Monolingual Portuguese</cell><cell>TDN auto</cell><cell>0.1741</cell></row><row><cell>BERKMOPTTDNPIV</cell><cell>Monolingual Portuguese</cell><cell cols="2">TDN auto fusion 0.2310 *</cell></row><row><cell>BERKGCBIENDETD</cell><cell>Bilingual EnglishRightarrowGerman</cell><cell>TD auto</cell><cell>0.2150</cell></row><row><cell>BERKGCBIENDETDN</cell><cell>Bilingual EnglishRightarrowGerman</cell><cell>TDN auto</cell><cell>0.1682</cell></row><row><cell cols="2">BERKBIENDETDNPIV Bilingual EnglishRightarrowGerman</cell><cell cols="2">TDN auto fusion 0.2251 *</cell></row><row><cell>BERKGCBIPTDETD</cell><cell cols="2">Bilingual PortugueseRightarrowGerman TD auto</cell><cell>0.1950</cell></row><row><cell>BERKGCBIPTDETDN</cell><cell cols="2">Bilingual PortugueseRightarrowGerman TDN auto</cell><cell>0.1108</cell></row><row><cell cols="3">BERKBIPTDETDNPIV Bilingual PortugueseRightarrowGerman TDN auto fusion</cell><cell>0.1912</cell></row><row><cell>BERKGCBIDEENTD</cell><cell>Bilingual GermanRightarrowEnglish</cell><cell>TD auto</cell><cell>0.2274</cell></row><row><cell>BERKGCBIDEENTDN</cell><cell>Bilingual GermanRightarrowEnglish</cell><cell>TDN auto</cell><cell>0.1894</cell></row><row><cell cols="2">BERKBIDEENTDNPIV Bilingual GermanRightarrowEnglish</cell><cell cols="2">TDN auto fusion 0.2304 *</cell></row><row><cell>BERKGCBIPTENTD</cell><cell>Bilingual PortugueseRightarrowEnglish</cell><cell>TD auto</cell><cell>0.1886</cell></row><row><cell>BERKGCBIPTENTDN</cell><cell>Bilingual PortugueseRightarrowEnglish</cell><cell>TDN auto</cell><cell>0.1540</cell></row><row><cell cols="2">BERKBIPTENTDNPIV Bilingual PortugueseRightarrowEnglish</cell><cell>TDN auto fusion</cell><cell>0.2101</cell></row><row><cell>BERKGCBIDEPTTD</cell><cell cols="2">Bilingual GermanRightarrowPortuguese TD auto</cell><cell>0.1346</cell></row><row><cell>BERKGCBIDEPTTDN</cell><cell cols="2">Bilingual GermanRightarrowPortuguese TDN auto</cell><cell>0.1260</cell></row><row><cell cols="3">BERKBIDEPTTDNPIV Bilingual GermanRightarrowPortuguese TDN auto fusion</cell><cell>0.1488</cell></row><row><cell>BERKGCBIENPTTD</cell><cell>Bilingual EnglishRightarrowPortuguese</cell><cell>TD auto</cell><cell>0.1913</cell></row><row><cell>BERKGCBIENPTTDN</cell><cell>Bilingual EnglishRightarrowPortuguese</cell><cell>TDN auto</cell><cell>0.1762</cell></row><row><cell cols="2">BERKBIENPTTDNPIV Bilingual EnglishRightarrowPortuguese</cell><cell cols="2">TDN auto fusion 0.2074 *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,90.00,663.59,423.19,33.84"><head></head><label></label><figDesc>examine the differences. Overall, we did see some distinct improvements in the text-based approaches over those used in 2006 and 2007. In Table4the MAP for our best runs from 2006, 2007, and 2008 for</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,90.00,68.39,423.24,110.72"><head>Table 4 :</head><label>4</label><figDesc>Comparison of Berkeley's best 2006, 2007 and 2008 runs for English, German and Portuguese</figDesc><table coords="10,130.80,92.78,341.21,86.33"><row><cell>TASK</cell><cell cols="3">MAP MAP MAP</cell><cell>Diff.</cell><cell>Diff.</cell><cell>Diff.</cell></row><row><cell></cell><cell>2006</cell><cell>2007</cell><cell>2008</cell><cell cols="2">'06-'07 '07-'08 '06-'08</cell></row><row><cell>Monolingual English</cell><cell cols="3">0.250 0.264 0.268</cell><cell>5.303</cell><cell>1.493</cell><cell>6.716</cell></row><row><cell>Monolingual German</cell><cell cols="5">0.215 0.139 0.230 -54.676 39.565</cell><cell>6.522</cell></row><row><cell>Monolingual Portuguese</cell><cell cols="3">0.162 0.174 0.231</cell><cell>6.897</cell><cell>24.675 29.870</cell></row><row><cell>Bilingual English⇒German</cell><cell cols="4">0.156 0.090 0.225 -73.333</cell><cell>60.00</cell><cell>30.667</cell></row><row><cell cols="4">Bilingual English⇒Portuguese 0.126 0.201 0.207</cell><cell>37.313</cell><cell>2.899</cell><cell>39.130</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,110.52,268.31,402.62,9.96;10,110.52,280.31,402.46,9.96;10,110.52,292.19,402.33,9.96;10,110.52,304.19,402.44,9.96;10,110.52,316.19,141.84,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,170.17,268.31,295.75,9.96">Multilingual information retrieval using english and chinese queries</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,422.04,280.31,90.94,9.96;10,110.52,292.19,402.33,9.96;10,110.52,304.19,82.58,9.96">Evaluation of Cross-Language Information Retrieval Systems: Second Workshop of the Cross-Language Evaluation Forum, CLEF-2001</title>
		<title level="s" coord="10,429.96,304.19,83.00,9.96;10,110.52,316.19,89.53,9.96">Springer Computer Scinece Series LNCS</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001. 2002</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,336.11,402.56,9.96;10,110.52,347.99,94.69,9.96" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,170.52,336.11,212.20,9.96">Cross-Language Retrieval Experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002. 2003</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2785</biblScope>
			<biblScope unit="page" from="28" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,367.91,402.63,9.96;10,110.52,379.91,350.54,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,250.83,367.91,262.33,9.96;10,110.52,379.91,169.07,9.96">Multilingual information retrieval using machine translation, relevance feedback and decompounding</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,288.96,379.91,92.95,9.96">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="149" to="182" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,399.83,402.60,9.96;10,110.52,411.83,402.37,9.96;10,110.52,423.71,81.01,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,285.16,399.83,227.96,9.96;10,110.52,411.83,205.17,9.96">Full Text Retrieval based on Probabilistic Equations with Coefficients fitted by Logistic Regression</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,342.36,411.83,165.22,9.96">Text REtrieval Conference (TREC-2)</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,443.63,402.51,9.96;10,110.52,455.63,402.54,9.96;10,110.52,467.63,402.66,9.96;10,110.52,479.51,122.54,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,376.02,443.63,137.02,9.96;10,110.52,455.63,105.63,9.96">Probabilistic retrieval based on staged logistic regression</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Dabney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,235.80,455.63,277.26,9.96;10,110.52,467.63,185.50,9.96">15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Copenhagen, Denmark; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">June 21-24. 1992</date>
			<biblScope unit="page" from="198" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,499.43,402.41,9.96;10,110.52,511.43,402.57,9.96;10,110.52,523.43,380.91,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,188.53,499.43,241.52,9.96">Geographic information retrieval and spatial browsing</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,224.52,511.43,256.45,9.96">GIS and Libraries: Patrons, Maps and Spatial Information</title>
		<editor>
			<persName><forename type="first">Linda</forename><surname>Smith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Myke</forename><surname>Gluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Urbana-Champaign</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="81" to="124" />
		</imprint>
		<respStmt>
			<orgName>University of Illinois at Urbana-Champaign, GSLIS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,543.35,402.60,9.96;10,110.52,555.23,402.50,9.96;10,110.52,567.23,52.81,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,185.77,543.35,327.35,9.96;10,110.52,555.23,26.47,9.96">Probabilistic retrieval, component fusion and blind feedback for XML retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,157.92,555.23,47.38,9.96">INEX 2005</title>
		<title level="s" coord="10,326.33,555.23,186.69,9.96">Lecture Notes in Computer Science, LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3977</biblScope>
			<biblScope unit="page" from="225" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,587.15,402.73,9.96;10,110.52,599.15,402.41,9.96;10,110.52,611.03,378.61,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,331.96,587.15,181.30,9.96;10,110.52,599.15,199.84,9.96">Berkeley at GeoCLEF: Logistic regression and fusion for geographic information retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivien</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Petras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,329.52,599.15,183.41,9.96;10,110.52,611.03,18.34,9.96">Cross-Language Evaluation Forum: CLEF 2005</title>
		<title level="s" coord="10,249.53,611.03,183.45,9.96">Lecture Notes in Computer Science LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="963" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,630.95,402.42,9.96;10,110.52,642.95,402.44,9.96;10,110.52,654.95,359.89,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,179.04,630.95,188.77,9.96">Analyses of multiple evidence combination</title>
		<author>
			<persName coords=""><forename type="first">Joon</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,394.32,630.95,118.63,9.96;10,110.52,642.95,402.44,9.96;10,110.52,654.95,93.10,9.96">SIGIR &apos;97: Proceedings of the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">July 27-31, 1997. 1997</date>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,674.87,402.50,9.96;10,110.52,686.75,328.34,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,283.72,674.87,157.06,9.96">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,450.84,674.87,62.19,9.96;10,110.52,686.75,181.54,9.96">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976-06">May-June 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.52,61.43,402.51,9.96;11,110.52,73.43,402.46,9.96;11,110.52,85.31,224.65,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,466.07,61.43,46.96,9.96;11,110.52,73.43,227.10,9.96">OKAPI at TREC-7: ad hoc, filtering, vlc and interactive track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Micheline</forename><forename type="middle">M</forename><surname>Hancock-Beauliee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,359.88,73.43,153.10,9.96;11,110.52,85.31,8.12,9.96">Text Retrieval Conference (TREC-7)</title>
		<imprint>
			<date type="published" when="1998">Nov. 9-1 1998. 1998</date>
			<biblScope unit="page" from="152" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.52,105.35,402.57,9.96;11,110.52,117.23,402.53,9.96;11,110.52,129.23,321.85,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,302.32,105.35,210.77,9.96;11,110.52,117.23,28.50,9.96">On relevance weights with little relevance information</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,159.84,117.23,353.21,9.96;11,110.52,129.23,177.62,9.96">Proceedings of the 20th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 20th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="16" to="24" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
