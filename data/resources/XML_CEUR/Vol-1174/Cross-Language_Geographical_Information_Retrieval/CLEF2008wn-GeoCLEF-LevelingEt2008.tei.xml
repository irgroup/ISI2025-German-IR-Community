<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.55,148.79,289.91,15.48;1,204.63,170.71,193.74,15.48;1,175.70,192.62,251.60,15.48">University of Hagen at GeoCLEF 2008: Combining IR and QA for Geographic Information Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,222.64,226.91,74.03,8.64"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Information and Communication Systems (IICS</orgName>
								<orgName type="institution">University of Hagen (FernUniversität in Hagen</orgName>
								<address>
									<postCode>58084</postCode>
									<settlement>Hagen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.04,226.91,64.32,8.64"><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Information and Communication Systems (IICS</orgName>
								<orgName type="institution">University of Hagen (FernUniversität in Hagen</orgName>
								<address>
									<postCode>58084</postCode>
									<settlement>Hagen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.55,148.79,289.91,15.48;1,204.63,170.71,193.74,15.48;1,175.70,192.62,251.60,15.48">University of Hagen at GeoCLEF 2008: Combining IR and QA for Geographic Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A8B5B22F7B3AF3C15B89C76E8C142B74</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing-Indexing methods</term>
					<term>Linguistic processing</term>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Query formulation</term>
					<term>Search process</term>
					<term>H.3.4 [Information Storage and Retrieval]: Systems and Software-Performance evaluation (efficiency and effectiveness) Experimentation, Measurement, Performance Geographic Information Retrieval, Question Answering, Cross-language Information Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of GIRSA at GeoCLEF 2008, the geographic information retrieval task at CLEF. GIRSA is a modified and improved variant of the system which participated at GeoCLEF 2007. It combines results retrieved with methods from information retrieval (IR) on geographically annotated data and question answering (QA) employing query decomposition.</p><p>For the monolingual German experiments, several parameter settings were varied: using a single index or a separate index for content and geographic annotation, using complex term weighting, adding location names from the narrative part of the topics, and merging results from IR and QA. The best mean average precision (MAP) was obtained by combining IR and QA results (0.2608 MAP).</p><p>For bilingual (English-German and Portuguese-German) experiments, topics were translated via various machine translation web services: Applied Language Solutions, Google Translate, and Promt Online Translator. Performance for these experiments is generally lower than for monolingual experiments. For both source languages, Google Translate seems to return the best translations. For English topics, 60% (0.1571 MAP) of the maximum MAP for monolingual German experiments is achieved. For bilingual Portuguese-German experiments, 80% (0.2085 MAP) of the maximum MAP for monolingual German experiments is achieved.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>GeoCLEF is the geographic information retrieval (GIR) task at CLEF, the cross-language evaluation campaign. In recent years, we have developed GIRSA (Geographic Information Retrieval by Semantic Annotation), a system for exploring novel approaches at GIR. GIRSA supports methods to improve precision (e.g. annotation of metonymic location names <ref type="bibr" coords="2,257.49,170.47,11.20,8.64" target="#b4">[5]</ref>) and methods to improve recall (e.g. normalization of location name synsets <ref type="bibr" coords="2,162.68,182.43,10.45,8.64" target="#b3">[4]</ref>). For GeoCLEF 2008, the major improvement lies in the combination of results from information retrieval (IR) on geographically annotated documents with methods from question answering (QA).</p><p>2 System Description GIRSA is a system for the evaluation of novel indexing and retrieval methods for GIR. Basically, the GIRSA setup introduced at GeoCLEF 2007 is used for the GIR experiments. This setup involves the identification and normalization of location indicators, i.e. text segments from which a geographic scope can be inferred. Location adjectives, names for inhabitants of a place, geographic codes, orthographic variants, acronyms, and abbreviations are mapped to location names. For its participation in GeoCLEF 2008, selected aspects of the IR subsystem have been improved:</p><p>• The IR indexing methods utilize an improved version of the German stemmer (in the old version, adjectives were often stemmed incorrectly due to an incorrectly implemented stemming rule).</p><p>• The resources for the identification of location indicators have been expanded. Additional lists of synonymous location names were extracted from Wikipedia articles and added to the geographic annotation data. For the normalization of multi-word names, missing inflectional variants of names were automatically generated and added. Furthermore, an automatic consistency check to find circular normalizations and other data inconsistencies was integrated and inconsistencies in the annotation data were removed (e.g. if the data contains entries to normalize "Geneva" to "Genf" and vice versa, this will be detected).</p><p>• The retrieval was modified to include a weighting scheme already used in our QA system <ref type="bibr" coords="2,479.12,465.46,10.58,8.64" target="#b2">[3]</ref>. The term weighting is meant to achieve a higher initial MAP by assigning weights according to the semantic contribution of words from the topic. Terms receive weights corresponding to their importance as follows (in order of increasing weights): lower case words (e.g. adjectives and adverbs), numeric expressions (e.g. temporal expressions), the answer subtype (similar to the expected answer type known from QA, typically the first noun from a question), nouns, and proper nouns.</p><p>The QA subsystem of GIRSA is InSicht, which also participates in QA@CLEF (see for example <ref type="bibr" coords="2,495.58,545.17,10.45,8.64" target="#b1">[2]</ref>). For the specific requirements in an IR setting, the QA system has been modified in the following ways:</p><p>• The normal processing of queries or questions stops after matching semantic representations of the query with semantic representations of documents. Answer generation is skipped because typical IR queries are not asking for answers, but for relevant documents.</p><p>• Semantic decomposition of queries, which was pioneered in the previous GeoCLEF <ref type="bibr" coords="2,463.55,620.88,10.58,8.64" target="#b3">[4]</ref>, was extended by developing 6 decomposition methods aiming at improving recall for QA and/or IR (see <ref type="bibr" coords="2,501.39,632.84,11.62,8.64" target="#b0">[1]</ref> for details on the application of this approach to QA). For this year's experiments, only two decomposition methods were activated in order to reduce runtime and to avoid finding irrelevant documents.</p><p>For the title of topic 91-GC ("Waldbrände auf spanischen Inseln"/'Forest fires on Spanish islands'), description decomposition produces the subquestion "Nenne spanische Inseln."/'Name Spanish islands.' The 14 subanswers found (e.g. "Gran Canaria") are substituted on the level of semantic representations in the original question, leading to 14 revised queries, e.g. "Waldbrände auf Gran Canaria". For the title of topic 96-GC ("Wirtschaftsaufschwung in Südostasien"/'Economic boom in Southeast Asia'), meronymy decomposition leads to subquestions like "Welche Region/Welcher Staat/Welche Stadt liegt in Südostasien?"/'Which region/country/city is located in Southeast Asia?'.</p><p>As these examples indicate, subquestions produce background knowledge (often of a geographic type) on the fly. Some pieces of knowledge are to be found in gazetteers, but there are many cases ("Mittelmeeranrainerstaaten"/'Mediterranean countries' in topic 81-GC, "Nordafrika"/'Northern Africa' in topic 83-GC, "Südpazifik"/'South Pacific' in topic 85-GC, etc.) where it is unlikely to find the relevant information in static, general-purpose gazetteers. To improve the answers for subquestions, these subquestions (in contrast to the original GeoCLEF queries) are answered also over the Wikipedia corpus used in QA@CLEF. With decomposition, 1238 documents (232 assessed as relevant) were retrieved; only 125 documents (77 assessed as relevant) without decomposition.</p><p>• The semantic network for a query can be split into two semantic networks at certain relations, e.g. splitting off temporal or local restrictions. In GeoCLEF 2007, these two parts had to be matched in the same document; this year, a NEAR operator (with 2000 characters) instead of the AND operator was applied in order to improve precision for these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We formulate our expectations regarding the MAP for different parameter settings in our experiments as hypotheses:</p><p>H1 Experiments using additional location names from the narrative part of the topics will achieve a higher MAP than experiments that do not (to confirm results from GeoCLEF 2007).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H2</head><p>The MAP for experiments adding results from the QA subsystem will be somewhat higher than for experiments with pure GIR.</p><p>H3 Topic translations with the Promt Online Translator web service will be better (e.g. containing less untranslated words) than those from the other web services tested. The corresponding results will therefore have a higher MAP.</p><p>H4 Applying the weighting from QA (for all experiments), merging results from IR and QA, and combining indexes for location names and content words will result in a higher initial MAP.</p><p>GIRSA was employed to produce results for a number of monolingual and bilingual experiments. The following parameter settings were varied in different retrieval experiments (see Table <ref type="table" coords="3,431.15,491.02,3.69,8.64" target="#tab_0">1</ref>):</p><p>• language (lang.): German (DE), English (EN), or Portuguese (PT) serves as topic source language.</p><p>• translation (transl.): Applied Language Solutions<ref type="foot" coords="3,228.55,552.85,3.69,6.39" target="#foot_0">1</ref> (A), Google Translate<ref type="foot" coords="3,322.02,552.85,3.69,6.39" target="#foot_1">2</ref> (G), or Promt Online Translator<ref type="foot" coords="3,454.42,552.85,3.69,6.39" target="#foot_2">3</ref> (O) was used to translate topics.</p><p>• fields: Content keywords and location indicators are extracted from the topic title and description: with location names from the topic narrative (TDN) or without (TD).</p><p>• index:</p><p>-All words are stemmed; a single index is produced (A).</p><p>-Content words are decompounded (if possible) and stemmed; location names are identified; both are indexed separately (B).</p><p>-Content words are decompounded (if possible) and stemmed; location indicators are normalized; both are indexed separately (C). • combination (comb.): Results from IR and QA are combined (Y) or not (N). <ref type="foot" coords="4,329.89,390.75,3.69,6.39" target="#foot_3">4</ref>Three metrics are employed to measure retrieval performance (see Table <ref type="table" coords="4,380.74,410.10,3.69,8.64" target="#tab_0">1</ref>):</p><p>• MAP: mean average precision,</p><p>• rel ret: the number of relevant and retrieved documents (a total of 1417 documents was assessed as relevant for the GeoCLEF 2008 topics), and</p><p>• P@N: precision at N documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Let us revisit the hypotheses from Section 3.</p><p>H1 Experiments using additional location names from the narrative part of the topics will achieve a higher MAP than experiments that do not (to confirm results from GeoCLEF 2007). This turned out to be false. The MAP for experiments with additional location names from the topic narrative is lower than for the experiments using title and description only (e.g. FUHtd20 vs. FUHtdn20). Maybe additional location names from the topic narrative do not match the names in documents as exactly as in old topics; maybe too many additional location names are added, causing a topic shift.</p><p>A solution would require a more elaborate weighting algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H2</head><p>The MAP for experiments adding results from the QA subsystem will be somewhat higher than for experiments with pure GIR. This is also not true: performance is considerably higher due to the improvements in the QA subsystem (query decomposition, less strict matching). The MAP for merged runs is higher in all cases. FUHtd01m shows a relative improvement of 7.8% in MAP compared to FUHtd01, FUHtd20m shows an improvement of 28.6% compared to FUHtd20; also, more relevant documents are retrieved in both cases. InSicht found documents for 13 (of the 25) topics, which is much better than last year. These results alone are not sufficient for GIR, but due to their high complementarity merging these results improves GIRSA significantly.</p><p>H3 Topic translations with the Promt Online Translator web service will be better (e.g. containing less untranslated words) than those from the other web services tested. The corresponding results will therefore have a higher MAP. The MAP for the best bilingual English-German experiment is 0.1571 (about 60% of the best MAP for monolingual German); the MAP for the best bilingual Portuguese-German experiment is 0.2085 (about 80% compared to monolingual German). The highest MAP was achieved with Google Translate. The experiments with topics translated by Google Translate returned the best results (FUHENGtdn20 vs. FUHENOtdn20 vs. FUHENAtdn20). Promt offers a web service (in beta status) different from previous years, which may be a reason why topics could not be translated well enough.</p><p>H4 Applying the weighting from QA (for all experiments), merging results from IR and QA, and combining indexes for location names and content words will result in a higher initial MAP. In comparison with results from the Berkeley group, the initial MAP was considerably higher: GIRSA returned 69% MAP at 0% recall for monolingual German experiments (experiment FUHtd01m), other participants achieved 43% and 16%, respectively (cf. the GeoCLEF overview paper in this volume); GIRSA achieved 63% MAP at 0% recall for bilingual experiments (experiment FUHPTGtd01), other participants achieved 47% and 16%, respectively.</p><p>To test GIRSA, experiments with the same parameter settings were conducted for the GeoCLEF 2007 topics before the 2008 campaign. The test experiments for topics from 2007 showed different results, e.g. the hypothesis H1 is true for the GeoCLEF 2007 topics, but not for the GeoCLEF 2008 topics (see also results for official experiments described in <ref type="bibr" coords="5,272.29,355.43,10.45,8.64" target="#b3">[4]</ref>). Future work will include a more thorough, per-topic analysis of errors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,94.67,119.13,417.09,236.75"><head>Table 1 :</head><label>1</label><figDesc>Results for monolingual and bilingual retrieval experiments on German GeoCLEF documents.</figDesc><table coords="4,95.98,136.16,415.78,219.73"><row><cell>Run</cell><cell></cell><cell></cell><cell>Parameters</cell><cell></cell><cell></cell><cell></cell><cell>Results</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">lang. transl. fields index comb.</cell><cell cols="4">MAP rel ret P@5 P@10 P@20</cell></row><row><cell>FUHtd01</cell><cell>DE</cell><cell>-</cell><cell>TD</cell><cell>A</cell><cell>N</cell><cell>0.2420</cell><cell>977 0.39</cell><cell>0.37</cell><cell>0.31</cell></row><row><cell>FUHtd01m</cell><cell>DE</cell><cell>-</cell><cell>TD</cell><cell>A</cell><cell>Y</cell><cell>0.2608</cell><cell>1028 0.38</cell><cell>0.37</cell><cell>0.35</cell></row><row><cell>FUHtd20</cell><cell>DE</cell><cell>-</cell><cell>TD</cell><cell>B</cell><cell>N</cell><cell>0.1719</cell><cell>914 0.20</cell><cell>0.29</cell><cell>0.27</cell></row><row><cell>FUHtd20m</cell><cell>DE</cell><cell>-</cell><cell>TD</cell><cell>B</cell><cell>Y</cell><cell>0.2211</cell><cell>998 0.36</cell><cell>0.35</cell><cell>0.34</cell></row><row><cell>FUHtdn20</cell><cell>DE</cell><cell>-</cell><cell>TDN</cell><cell>B</cell><cell>N</cell><cell>0.1478</cell><cell>834 0.17</cell><cell>0.24</cell><cell>0.20</cell></row><row><cell>FUHENAtd20</cell><cell>EN</cell><cell>A</cell><cell>TD</cell><cell>B</cell><cell>N</cell><cell>0.1076</cell><cell>644 0.18</cell><cell>0.17</cell><cell>0.17</cell></row><row><cell>FUHENAtdn20</cell><cell>EN</cell><cell>A</cell><cell>TDN</cell><cell>B</cell><cell>N</cell><cell>0.0962</cell><cell>610 0.14</cell><cell>0.15</cell><cell>0.13</cell></row><row><cell>FUHENGtdn20</cell><cell>EN</cell><cell>G</cell><cell>TDN</cell><cell>B</cell><cell>N</cell><cell>0.1571</cell><cell>800 0.21</cell><cell>0.21</cell><cell>0.21</cell></row><row><cell>FUHENOtd20</cell><cell>EN</cell><cell>O</cell><cell>TD</cell><cell>B</cell><cell>N</cell><cell>0.1179</cell><cell>703 0.23</cell><cell>0.23</cell><cell>0.21</cell></row><row><cell>FUHENOtdn20</cell><cell>EN</cell><cell>O</cell><cell>TDN</cell><cell>B</cell><cell>N</cell><cell>0.1146</cell><cell>699 0.21</cell><cell>0.21</cell><cell>0.19</cell></row><row><cell>FUHPTGtd01</cell><cell>PT</cell><cell>G</cell><cell>TD</cell><cell>A</cell><cell>N</cell><cell>0.2085</cell><cell>903 0.41</cell><cell>0.38</cell><cell>0.33</cell></row><row><cell>FUHPTGtd20</cell><cell>PT</cell><cell>G</cell><cell>TD</cell><cell>B</cell><cell>N</cell><cell>0.1776</cell><cell>907 0.29</cell><cell>0.30</cell><cell>0.27</cell></row><row><cell>FUHPTGtdn20</cell><cell>PT</cell><cell>G</cell><cell>TDN</cell><cell>B</cell><cell>N</cell><cell>0.1571</cell><cell>800 0.21</cell><cell>0.21</cell><cell>0.21</cell></row><row><cell>FUHPTGtd21</cell><cell>PT</cell><cell>G</cell><cell>TD</cell><cell>C</cell><cell>N</cell><cell>0.2002</cell><cell>913 0.34</cell><cell>0.34</cell><cell>0.31</cell></row><row><cell>FUHPTGtdn21</cell><cell>PT</cell><cell>G</cell><cell>TDN</cell><cell>C</cell><cell>N</cell><cell>0.1567</cell><cell>793 0.22</cell><cell>0.21</cell><cell>0.22</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,104.35,725.92,224.41,6.64"><p>http://www.appliedlanguage.com/free_translation.shtml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,104.35,735.73,118.56,6.64"><p>http://translate.google.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,104.35,745.54,140.23,6.64"><p>http://www.online-translator.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,104.35,747.26,367.10,6.91"><p>To merge, the maximum score of results is chosen (for duplicate results), and the top-1000 documents are returned.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,106.60,423.27,406.40,8.64;5,106.60,435.05,406.40,8.82;5,106.60,447.00,323.19,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,178.21,423.27,191.33,8.64">Semantic decomposition for question answering</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,343.50,435.05,169.50,8.59;5,106.60,447.00,147.33,8.59">Proceedings of the 18th European Conference on Artificial Intelligence (ECAI)</title>
		<editor>
			<persName><forename type="first">Malik</forename><surname>Ghallab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Constantine</forename><forename type="middle">D</forename><surname>Spyropoulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nikos</forename><surname>Fakotakis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nikos</forename><surname>Avouris</surname></persName>
		</editor>
		<meeting>the 18th European Conference on Artificial Intelligence (ECAI)<address><addrLine>Patras, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-07">July 2008</date>
			<biblScope unit="page" from="313" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,106.60,467.11,406.40,8.64;5,106.60,478.88,406.40,8.82;5,106.60,490.84,406.40,8.59;5,106.60,502.80,187.78,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,342.87,467.11,170.13,8.64;5,106.60,479.06,354.94,8.64">University of Hagen at QA@CLEF 2008: Efficient question answering with question decomposition and multiple answer streams</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,484.22,478.88,28.78,8.59;5,106.60,490.84,406.40,8.59;5,106.60,502.80,37.31,8.59">Results of the CLEF 2008 Cross-Language System Evaluation Campaign, Working Notes for the CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09">September 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,106.60,522.90,406.40,8.64;5,106.60,534.68,406.40,8.82;5,106.60,546.63,318.25,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,190.85,522.90,316.53,8.64">On the role of information retrieval in the question answering system IRSAW</title>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,118.77,534.68,394.23,8.59;5,106.60,546.63,34.98,8.59">Proceedings of the LWA 2006 (Learning, Knowledge, and Adaptability), Workshop Information Retrieval</title>
		<meeting>the LWA 2006 (Learning, Knowledge, and Adaptability), Workshop Information Retrieval<address><addrLine>Hildesheim, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="119" to="125" />
		</imprint>
		<respStmt>
			<orgName>Universität Hildesheim</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="5,106.60,566.73,406.40,8.64;5,106.60,578.69,406.40,8.64;5,106.60,590.47,406.40,8.82;5,106.60,602.42,406.40,8.82;5,106.60,614.38,302.18,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,268.67,566.73,240.81,8.64">Inferring location names for geographic information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,275.70,590.47,237.30,8.59;5,106.60,602.42,252.27,8.59">Advances in Multilingual and Multimodal Information Retrieval: 8th Workshop of the Cross-Language Evaluation Forum</title>
		<title level="s" coord="5,482.93,602.42,30.07,8.59;5,106.60,614.38,134.21,8.59">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vivien</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007. 2008</date>
			<biblScope unit="volume">5152</biblScope>
			<biblScope unit="page" from="773" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,106.60,634.48,406.40,8.64;5,106.60,646.26,359.28,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,277.84,634.48,235.16,8.64;5,106.60,646.44,24.02,8.64">On metonymy recognition for geographic information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,137.63,646.26,235.03,8.59">International Journal of Geographical Information Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="289" to="299" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
