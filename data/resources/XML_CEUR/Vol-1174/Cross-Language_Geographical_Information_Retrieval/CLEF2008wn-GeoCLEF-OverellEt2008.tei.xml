<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,191.80,148.86,219.41,15.15;1,215.17,170.78,172.65,15.15">MMIS at GeoCLEF 2008: Experiments in GIR</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,194.88,204.67,61.47,8.74"><forename type="first">Simon</forename><surname>Overell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia &amp; Information Systems Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.90,204.67,46.35,8.74"><forename type="first">Adam</forename><surname>Rae</surname></persName>
							<email>a.rae@open.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Knowledge Media Institute The Open University</orgName>
								<address>
									<addrLine>Milton Keynes</addrLine>
									<postCode>MK7 6AA</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.41,204.67,56.91,8.74"><forename type="first">Stefan</forename><surname>Rüger</surname></persName>
							<email>s.rueger@open.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Multimedia &amp; Information Systems Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Knowledge Media Institute The Open University</orgName>
								<address>
									<addrLine>Milton Keynes</addrLine>
									<postCode>MK7 6AA</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,191.80,148.86,219.41,15.15;1,215.17,170.78,172.65,15.15">MMIS at GeoCLEF 2008: Experiments in GIR</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CB1A206FF43AAE70CA3180F8EC381DB3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation Geographic Information Retrieval, Placename Disambiguation, Geographic Indexing, Geographic Relevance Ranking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present our Geographic Information Retrieval System, Forostar, and the results of three experiments. We compare two data fusion methods, and show that a simple geographic filter outperforms a penalty based system. We compare context based disambiguation to a default gazetteer and show no significant difference. Finally we compare a unique geographic index to an ambiguous geographic index. The ambiguous index outperformed all other methods and was statistically significantly better than the baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents the work from the MultiMedia Information Systems group at GeoCLEF 2008 with our GIR application Forostar. We submitted nine runs performing three experiments.</p><p>Our first experiment compares two data fusion methods looking at how best to combine text and geographic relevance data. Our text results take the form of a rank returned from a standard IR system, while our geographic results take the form of a filter -an unranked list of all documents matching the geographic part of the query. We compare penalising documents not appearing in the filter with a learned penalisation value to simply applying the filter to the text rank.</p><p>Our second experiment compares context based placename disambiguation to a default gazetteer. We have a unique geographic index mapping each placename reference in a document to a single location on the Earth's surface. This mapping is not a trivial matter for ambiguous locations, such as "Cambridge" or "London". We compare three ways of generating this index, the simplest matches each placename to the most common location with that name. We compare this to two methods that use other placenames occurring in the document to provide a context to Our final experiment compares our unique index to an ambiguous index, in an ambiguous index each ambiguous placename is indexed multiple times, once for each possible location. An ambiguous index will clearly provide significantly greater recall but at a potential cost to precision (dependent on the accuracy of the unique index).</p><p>In Section 2 we outline our GIR system and the different methods used in our experiments. Section 3 will reiterate our research questions and describe our submitted runs. Section 4 contains our results, significance testing and comparisons to the other GeoCLEF submissions. We conclude with Section 5 containing our observations and planned future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System</head><p>Forostar is our ad-hoc Geographic Information Retrieval system (Figure <ref type="figure" coords="2,415.92,478.97,3.88,8.74" target="#fig_0">1</ref>). At indexing time, documents are analysed and named entities extracted. Named entities tagged as locations are then disambiguated using our co-occurrence model extracted from Wikipedia <ref type="bibr" coords="2,435.74,502.88,9.96,8.74" target="#b7">[8]</ref>. The free-text fields are then indexed by Lucene. In the querying stage we query the Geographic and Text indexes separately combining them in the data-fusion module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexing Stage</head><p>Forostar's Indexer is based on Apache Lucene <ref type="bibr" coords="2,298.48,573.07,9.96,8.74" target="#b8">[9]</ref>. Text fields are pre-processed by a customised analyser similar to Lucene's default analyser: Text is split at white space into tokens, the tokens are then converted to lower case, stop words discarded and stemmed with the "Snowball Stemmer". The processed tokens are held in Lucene's inverted index.</p><p>The Named Entity Reconiser used to process the text fields is Sheffield University's General Architecture for Text Engineering (GATE) <ref type="bibr" coords="2,279.16,632.84,9.97,8.74" target="#b0">[1]</ref>. The bundled Information Extraction Engine, AN-NIE, performs named entity recognition, extracting named entities and tagging them as locations. Our disambiguation system matches these placenames to unique locations in the Getty Thesaurus of Geographical Names (TGN) <ref type="bibr" coords="2,227.76,668.71,9.96,8.74" target="#b2">[3]</ref>.</p><p>PediaCrawler, the application that builds our geographic co-occurrence model, is not discussed in detail in this paper; instead we refer the reader to <ref type="bibr" coords="2,356.72,692.62,9.97,8.74" target="#b7">[8]</ref>. Suffice to say the co-occurrence model produced acts as training data for our placename disambiguation algorithms described below. The co-occurrence model used is available by contacting the lead author.</p><p>We compare four alternative modules for the Disambiguator described below. All map the placenames that occur in a document to locations in the TGN and store them in a geographic index.</p><p>• Most Referred to (MR). The most referred to method matches each each placename to the most referred to location with that placename in the co-occurrence model. Essentially we build a default gazetteer providing a many-to-one mapping of placenames to locations.</p><p>• Support Vector Machine (SVM). We construct a multi-dimensional vector space, which is partitioned with an SVM <ref type="bibr" coords="3,234.71,266.13,9.96,8.74" target="#b4">[5]</ref>: ambiguous placenames are the classification objects, possible locations the classification classes, co-occurring locations are the features, and the features have tf •idf weights. We train a separately classifier for each possible location-placename mapping and classify as the location whose classifier outputs the greatest decision value. The co-occurrence model forms our training data.</p><p>• Neighbourhoods (Neigh). For each possible location of an ambiguous placename we build a collection of trigger words. Trigger words are other placenames, which if they appear in the same document as the ambiguous placename identify the location. If none of the trigger words appear the placename is classified using the MR method.</p><p>Trigger words are found based on a relatedness score. <ref type="bibr" coords="3,356.63,385.68,10.52,8.74" target="#b1">[2]</ref> describe the relatedness score as the ratio of co-occurrences between words divided by the disjunction of occurrences. This can be defined as:</p><formula xml:id="formula_0" coords="3,261.74,417.51,251.26,23.23">r(x, y) = f xy f x + f y -f xy ,<label>(1)</label></formula><p>where f x denotes the total number of times word x appears. The top 5 trigger words for three different locations for the placename 'Cambridge' are shown in Table <ref type="table" coords="3,444.75,459.78,3.88,8.74" target="#tab_0">1</ref>.</p><p>• No Disambiguation (NoDis). Although not strictly speaking a method of disambiguation, the NoDis method provides an alternative to a disambiguation algorithm. NoDis builds an ambiguous index, indexing each ambiguous placename once for each possible location. For example the placename 'Cambridge' would be indexed 3 times: Cambridge, UK, Cambridge MA, and Cambridge NZ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Querying Stage</head><p>The text and geographic indexes are queried separately by two different query engines. Placenames are manually extracted from queries and passed to the Geographic Query Engine<ref type="foot" coords="3,449.90,594.15,3.97,6.12" target="#foot_0">1</ref> . Queries are passed to the text query engine with no pre-processing. We use the standard Lucene Text Query Engine. This performs a comparison between the documents and the query in a tf •idf weighted vector space. The cosine distance is taken between the query vector and the document vectors. The text rank is produced in standard TREC format.</p><p>We also use Lucene to store the Geographic Index. A unique string is formed for each location, this is the TGN id of this location, preceded with the TGN id of all the parent locations, separated with slashes. Thus the unique string for the location "London, UK" is the TGN id for London (7011781), preceded by its parent, Greater London (7008136), preceded by its parent, Britain (7002445). . . until the root location, the World (1000000) is reached. Giving the unique string for London as 1000000\1000003\7008591\7002445\7008136\7011781. The Geographic Query Engine disambiguates each placename passed to it using the MR method of disambiguation<ref type="foot" coords="4,205.12,231.65,3.97,6.12" target="#foot_1">2</ref> . The locations are then converted into unique strings and Lucene's ConstantScoreRangeQuery is used to search the index fast and build a geographic filter (also in TREC format).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disambig. Method Penalisation</head><p>Storing and searching locations in this fashion means all locations contained within a larger location are also included in a query. So a query for "United States" will produce a filter including documents mentioning all the states, counties and towns within the United States as well as references to the country itself.</p><p>The Data Fusion module combines the text rank and the geographic filter to produce a single result rank. Two different Data Fusion methods were used described below:</p><p>• Penalisation. The penalisation method multiplies the rank, r, of each element in the text rank that is not in the geographic filter by a penalisation value p, to give an intermediate rank r . This can be described by the filter function f (r)</p><formula xml:id="formula_1" coords="4,228.02,397.20,284.98,23.08">f (r) = r doc r present in filter rp doc r not present in filter (2)</formula><p>The intermediate rank is sorted by r to give the final returned rank.</p><p>The penalisation value p is found using a brute force search using the 75 queries and relevance judgements from GeoCLEF 2005 -2007 as training data. The search finds the value of p that maximises mean average precision (MAP). The p values found for each disambiguation method are shown in Table <ref type="table" coords="4,237.03,489.01,3.88,8.74" target="#tab_1">2</ref>.</p><p>• Filtering. The filtering method reorders the text rank in a more aggressive way than the penalisation method. All the results of the text rank that are also contained in the geographic filter are returned first, followed by the text results that are not in the geographic filter. This is equivalent to the penalisation method with a p value of ∞.</p><p>An example of these two methods are shown in Figure <ref type="figure" coords="4,351.25,565.61,3.88,8.74" target="#fig_1">2</ref>. It shows a hypothetical text rank containing two entries also in the geographic filter. The penalisation method calculates r , shown in brackets, to re-order the results, while the filter method simply promotes all the documents also in the geographic filter to the top of the rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Our experiments are all monolingual. We use the 25 English Language GeoCLEF queries and the English Language corpus consisting of ≈ 170, 000 documents from the Glasgow Herald and Los Angeles Times. GeoCLEF topics contain title, description and narrative fields. We only use the title field as this field bears most similarity to the types of geographic query submitted to search engines.</p><p>As explained in Section 1 we are performing three experiments at this year's GeoCLEF. This involved us submitting nine runs: a simple text baseline (the text rank before data fusion) and • Can the more sophisticated Penalisation data fusion method outperform the more aggressive Filtering method?</p><p>• Can the more sophisticated context based disambiguation methods Neigh and SVM outperform the default gazetteer MR method?</p><p>• Are the unique geographic index methods accurate enough to outperform the NoDis method?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The results of our nine runs are displayed in Table <ref type="table" coords="5,325.09,602.18,3.88,8.74">3</ref>. Mean Average Precision (MAP) is the metric primarily used in GeoCLEF, however Geometric Average Precision (Geo AP) is also shown to give an indication of how consistent the methods are. Notice that combining the geographic information using the penalisation filter actually gives us worse results than the text baseline. Our assumption here is that the penalisation training on the past GeoCLEF data is over fitting. On the other hand, the filter method outperforms the baseline in every case showing it to be more robust.</p><p>There is minimal difference between which disambiguation method is used regardless of the fusion method.</p><p>We performed pairwise statistical significance testing of each method with the baseline using the Wilcoxon signed rank test rejecting the null hypothesis only when p &lt; 5% <ref type="bibr" coords="5,432.17,721.73,9.96,8.74" target="#b3">[4]</ref>. We found that all the Penalisation results were statistically significantly worse than the baseline, and only the NoDis-Filter method was statistically significantly better. We also performed pairwise significance testing between the Penalisation method and Filter method runs with the same disambiguation method and found that in every case the Filter method was statistically significantly better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with other participants</head><p>For comparison with the rest of the participants at GeoCLEF 2008, Table <ref type="table" coords="6,412.44,452.63,4.98,8.74" target="#tab_2">4</ref> shows the best, worst and quartile ranges of all the submitted runs. Our best result, NoDis-Filter, occurs in the top quartile. The other filtered results and the baseline occur between the Median and Q3. The Penalisation results occur in the lower quartile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In past years at GeoCLEF many submitted methods augmented with Geographic Information (including our past attempts) have not provided a statistically significant improvement over a baseline <ref type="bibr" coords="6,127.44,567.17,10.51,8.74" target="#b6">[7,</ref><ref type="bibr" coords="6,141.02,567.17,7.01,8.74" target="#b5">6]</ref>. We were pleased to buck this trend this year with our NoDis-Filter method, which re-orders the text result using an ambiguous geographic filter. The NoDis-Filter method was statistically significantly better than our text only baseline, it also came in the top quartile of methods submitted.</p><p>In response to our research questions it is still unclear whether an ambiguous or unique text index provides the best results, also whether context based disambiguation can improve over noncontext based methods. We have, however, shown that using a penalisation value to combine text and geographic data is highly sensitive to over fitting and a simple filter much more robust. In fact using a brute force search to optimise the penalisation value resulted in an MAP on the test data statistically significantly worse than the baseline or filter methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Future work</head><p>In future work we would like to expand our context based disambiguation algorithms to take into account not only the content of the documents but also the associated meta-data, i.e. where the documents were published. We believe treating placenames in the Los Angeles Times differently from the Glasgow Herald could produce further improvements in disambiguation and retrieval performance.</p><p>Also we would like to further develop the Penalisation data fusion method as it is potentially more powerful than the Filter method. Alternative methods of training the system will need to be explored, specifically how to avoid over fitting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,237.06,284.78,128.88,8.74;2,107.53,108.86,387.94,150.84"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Forostar framework</figDesc><graphic coords="2,107.53,108.86,387.94,150.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,247.45,419.35,108.09,8.74;5,198.86,108.86,205.27,285.41"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Rank Example</figDesc><graphic coords="5,198.86,108.86,205.27,285.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,119.52,110.82,363.97,54.51"><head>Table 1 :</head><label>1</label><figDesc>Example trigger words for the placename 'Cambridge'</figDesc><table coords="3,119.52,110.82,363.97,32.65"><row><cell cols="2">Cambridge, UK Preston</cell><cell>Derby</cell><cell>Sheffield</cell><cell>Lincoln</cell><cell>King's Lynn</cell></row><row><cell cols="3">Cambridge, MA Barnstaple Bristol</cell><cell>Dukes</cell><cell>Essex</cell><cell>Middlesex</cell></row><row><cell>Cambridge, NZ</cell><cell>Wuxi</cell><cell cols="4">Alexandra Ashburton Carterton Coromandel</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.10,110.43,422.79,78.42"><head>Table 2 :</head><label>2</label><figDesc>Penalisation values found by using results from GeoCLEF 2005 -2007 as training data.</figDesc><table coords="4,362.30,110.43,28.28,8.74"><row><cell>Values</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,214.64,110.43,173.73,250.92"><head>Table 4 :</head><label>4</label><figDesc>GeoCLEF 2008 quartiles</figDesc><table coords="6,214.64,110.43,173.73,229.45"><row><cell cols="2">Disambig. Fusion</cell><cell cols="2">MAP Geo AP</cell></row><row><cell cols="2">Text Baseline</cell><cell>24.1%</cell><cell>6.52%</cell></row><row><cell>MR</cell><cell cols="2">Penalis. 18.9%</cell><cell>5.12%</cell></row><row><cell>SVN</cell><cell cols="2">Penalis. 18.9%</cell><cell>5.17%</cell></row><row><cell>Neigh.</cell><cell cols="2">Penalis. 19.0%</cell><cell>5.24%</cell></row><row><cell>NoDis.</cell><cell cols="2">Penalis. 18.9%</cell><cell>5.17%</cell></row><row><cell>MR</cell><cell>Filter</cell><cell>24.5%</cell><cell>11.22%</cell></row><row><cell>SVN</cell><cell>Filter</cell><cell>24.4%</cell><cell>7.85%</cell></row><row><cell>Neigh.</cell><cell>Filter</cell><cell>24.2%</cell><cell>7.88%</cell></row><row><cell>NoDis.</cell><cell>Filter</cell><cell>26.4%</cell><cell>10.98%</cell></row><row><cell cols="4">Table 3: GeoCLEF 2008 results</cell></row><row><cell></cell><cell cols="2">Quartile MAP</cell><cell></cell></row><row><cell></cell><cell>Best</cell><cell>30.4%</cell><cell></cell></row><row><cell></cell><cell>Q3</cell><cell>26.1%</cell><cell></cell></row><row><cell></cell><cell>Median</cell><cell>23.7%</cell><cell></cell></row><row><cell></cell><cell>Q1</cell><cell>21.4%</cell><cell></cell></row><row><cell></cell><cell>Worst</cell><cell>16.1%</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,734.76,407.76,6.99;3,90.00,744.22,24.46,6.99"><p>The only reason this was not handled by the Named Entity Reconiser was time constraints in the implementation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,105.24,747.00,266.01,6.99"><p>This method is chosen due to the minimal context contained in queries.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.50,214.61,407.49,8.74;7,105.50,226.56,363.06,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,422.00,214.61,90.99,8.74;7,105.50,226.56,150.52,8.74">Developing language processing components with GATE</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tablan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ursu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bontcheva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>University of Sheffield</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="7,105.50,246.49,407.50,8.74;7,105.50,258.44,407.50,8.74;7,105.50,270.40,43.73,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,327.84,246.49,185.16,8.74;7,105.50,258.44,89.31,8.74">Subject-dependent co-occurrence and word sense disambiguation</title>
		<author>
			<persName coords=""><forename type="first">J A</forename><surname>Guthrie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Guthrie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wilks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Aidinejad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,214.61,258.44,244.96,8.74">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="146" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,290.32,407.49,8.74;7,105.50,302.28,58.67,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,161.49,290.32,178.37,8.74">User&apos;s Guide to the TGN Data Releases</title>
		<author>
			<persName coords=""><surname>Harping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,351.19,290.32,139.57,8.74">The Getty Vocabulary Program</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>0 edition</note>
</biblStruct>

<biblStruct coords="7,105.50,322.20,407.49,8.74;7,105.50,334.16,331.54,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,140.07,322.20,277.51,8.74">Using statistical testing in the evaluation of retrieval experiments</title>
		<author>
			<persName coords=""><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,436.97,322.20,76.02,8.74;7,105.50,334.16,233.55,8.74">SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="329" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,354.08,407.50,8.74;7,105.50,366.04,213.69,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,164.50,354.08,348.50,8.74;7,105.50,366.04,130.11,8.74">Advances in Kernel Methods -Support Vector Learning, chapter Making large-Scale SVM Learning Practical</title>
		<author>
			<persName coords=""><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT-Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,385.96,407.50,8.74;7,105.50,397.92,203.09,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,273.01,385.96,118.50,8.74">Forostar: A system for GIR</title>
		<author>
			<persName coords=""><surname>Overell</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Magalhães</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,413.66,385.96,99.35,8.74;7,105.50,397.92,150.38,8.74">Lecture Notes from the Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2006">2006. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,417.85,407.50,8.74;7,105.50,429.80,382.61,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,276.39,417.85,195.75,8.74">GIR experiments with Forostar at geoCLEF</title>
		<author>
			<persName coords=""><surname>Overell</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Magalhães</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,242.82,429.80,95.24,8.74">CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09">2007. September 2007</date>
		</imprint>
	</monogr>
	<note>Working notes</note>
</biblStruct>

<biblStruct coords="7,105.50,449.73,407.49,8.74;7,105.50,461.68,174.26,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,212.06,449.73,188.65,8.74">Geographic co-occurrence as a tool for GIR</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Overell</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,423.97,449.73,89.03,8.74;7,105.50,461.68,144.10,8.74">CIKM Workshop on Geographic Information Retrieval</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,481.61,407.50,8.74" xml:id="b8">
	<monogr>
		<ptr target="http://lucene.apache.org/java/docs/" />
		<title level="m" coord="7,105.50,481.61,99.59,8.74">Apache Lucene Project</title>
		<imprint>
			<date type="published" when="2007-08-01">1 August 2007, 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
