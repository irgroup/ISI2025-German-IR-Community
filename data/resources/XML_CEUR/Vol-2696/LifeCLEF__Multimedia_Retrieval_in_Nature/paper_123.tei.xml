<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.79,115.96,303.78,12.62;1,166.62,133.89,282.11,12.62;1,273.08,151.82,69.20,12.62">Adversarial Consistent Learning on Partial Domain Adaptation of PlantCLEF 2020 Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,221.20,189.66,67.58,8.74"><forename type="first">Youshan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Lehigh University</orgName>
								<address>
									<settlement>Bethlehem</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.11,189.66,76.05,8.74"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Lehigh University</orgName>
								<address>
									<settlement>Bethlehem</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.79,115.96,303.78,12.62;1,166.62,133.89,282.11,12.62;1,273.08,151.82,69.20,12.62">Adversarial Consistent Learning on Partial Domain Adaptation of PlantCLEF 2020 Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1031CB8A39EE227510203F788EA415E9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adversarial learning</term>
					<term>Partial domain adaptation</term>
					<term>Plant identification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptation is one of the most crucial techniques to mitigate the domain shift problem, which exists when transferring knowledge from an abundant labeled sourced domain to a target domain with few or no labels. Partial domain adaptation addresses the scenario when target categories are only a subset of source categories. In this paper, to enable the efficient representation of cross-domain plant images, we first extract deep features from pre-trained models and then develop adversarial consistent learning (ACL) in a unified deep architecture for partial domain adaptation. It consists of source domain classification loss, adversarial learning loss, and feature consistency loss. Adversarial learning loss can maintain domain-invariant features between the source and target domains. Moreover, feature consistency loss can preserve the fine-grained feature transition between two domains. We also find the shared categories of two domains via down-weighting the irrelevant categories in the source domain. Experimental results demonstrate that training features from NASNetLarge model with proposed ACL architecture yields promising results on the PlantCLEF 2020 Challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated plant identification is important in recognizing plant species. The availability of massive labeled training data is a prerequisite of machine learning models. Unfortunately, such a requirement cannot be met in the plant identification problem since we have sparse labels for real-world plant images. Therefore, we propose to transfer knowledge from an existing auxiliary labeled herbarium domain to the field photo domain with limited or no labels. However, due to the phenomenon of data bias or domain shift <ref type="bibr" coords="1,337.58,603.00,14.61,8.74" target="#b10">[11]</ref>, classification models do not generalize well from an existing herbarium domain to a novel field photo domain. Domain adaptation (DA) has been proposed to leverage knowledge from an abundant labeled source domain to learn an effective predictor for the target domain with few or no labels, while mitigating the domain shift problem <ref type="bibr" coords="2,451.39,142.90,16.69,8.74" target="#b15">[16,</ref><ref type="bibr" coords="2,468.07,142.90,12.52,8.74" target="#b16">17,</ref><ref type="bibr" coords="2,134.77,154.86,12.30,8.74" target="#b18">19,</ref><ref type="bibr" coords="2,147.06,154.86,12.30,8.74" target="#b19">20]</ref>. In this paper, we focus on unsupervised domain adaptation (UDA), where the target domain has no labels. Since we have fewer classes in the field photo domain, and the classes of the field photo domain is a subset of the classes of the source herbarium domain, we investigate partial domain adaptation (PDA) for the PlantCLEF 2020 Challenge.</p><p>Recently, deep neural network methods have been widely used in the domain adaptation problem. Notably, adversarial learning shows its power in embedding in deep neural networks to learn feature representations to minimize the discrepancy between the source and target domains <ref type="bibr" coords="2,334.69,250.81,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="2,346.68,250.81,11.62,8.74" target="#b13">14]</ref>. Inspired by the generative adversarial network (GAN) <ref type="bibr" coords="2,258.33,262.77,9.96,8.74" target="#b5">[6]</ref>, adversarial learning also contains a feature extractor and a domain discriminator. The domain discriminator can distinguish the source domain from the target domain, while the feature extractor can learn domain-invariant representations to fool the domain discriminator <ref type="bibr" coords="2,420.90,298.63,11.99,8.74" target="#b8">[9,</ref><ref type="bibr" coords="2,432.89,298.63,11.99,8.74" target="#b9">10,</ref><ref type="bibr" coords="2,444.88,298.63,11.99,8.74" target="#b17">18]</ref>. The target domain risk (the error of the target domain) is expected to be minimized via minimax optimization. Cao et al. presented adversarial learning for PDA, which alleviates negative transfer by reducing the outlier of source classes for training the source classifier and domain labels, while positive transfer is improved via matching the feature distributions in the shared label space <ref type="bibr" coords="2,441.77,358.41,9.96,8.74" target="#b1">[2]</ref>. Similarly, the example transfer network is proposed to jointly learn domain-invariant representations and a progressive weighting method to examine the transferability of source examples. The model can improve positive transfer by relevant examples and mitigate negative transfer by identifying irrelevant examples <ref type="bibr" coords="2,463.25,406.23,9.96,8.74" target="#b2">[3]</ref>.</p><p>Although many methods are proposed for partial domain adaptation, they still suffer from two challenges: (1) the models are evaluated on small datasets, while it has lower transferability to the large-scale dataset, and (2) the feature consistency of two domains is inappropriately ignored.</p><p>To address the aforementioned challenges, we aggregate three different loss functions in one framework: source domain classification loss, adversarial learning loss, and feature consistency loss to reduce the discrepancy of the two domains. Moreover, our model is evaluated on a large-scale plant identification dataset to improve the estimate of the generalization ability of our model.</p><p>Our contributions are three-fold:</p><p>1. We propose a novel adversarial consistent learning network (ACL) for PDA, to adversarially minimize the domain discrepancy of the source and target domains and maintain domain-invariant features; 2. The proposed adversarial learning loss and feature consistency loss can distinguish the target domain from the source domain, and preserve the finegrained feature transition between the two domains; 3. We impose shared category selection to filter out the irrelevant categories in the source domain. By down-weighting the irrelevant categories in the source domain, we can reduce negative transfer from the source domain to the target domain. Experimental results show that ACL achieves higher classification accuracy than several baseline methods and yields promising results on the PlantCLEF 2020 Challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>PlantCLEF 2020 is a large-scale dataset of the PlantCLEF 2020 task <ref type="bibr" coords="3,451.64,316.86,9.96,8.74" target="#b4">[5]</ref>, organized in the context of the LifeCLEF 2020 challenge <ref type="bibr" coords="3,382.32,328.81,9.96,8.74" target="#b7">[8]</ref>. Fig. <ref type="figure" coords="3,420.65,328.81,4.98,8.74" target="#fig_0">1</ref> shows some challenging images in this dataset. The herbarium domain contains 320,750 images in 997 species, and the number of images in different species are unbalanced. This dataset consists of herbarium sheets whereas the test set will be composed of field pictures. The validation set consists of two domains herbarium photo associations and photos. Herbarium photo associations domain includes 1,816 images from 244 species. This domain contains both herbarium sheets and field pictures for a subset of species, which enables learning a mapping between the herbarium sheets domain and the field pictures domain. Another photo domain has 4,482 images from 375 species and images are from plant pictures in the field, which is similar to the test dataset. The test dataset contains 3,186 unlabeled images. Due to the significant difference between herbarium and real photos, it is extremely difficult to identify the correct class.</p><p>We exclude the classId of "108335" in the photo domain since the major classes are from the herbarium domain. In addition, herbarium domain does not contain the "108335" category. Therefore, eight images are excluded in the photo domain. The statistics of the PlantCLEF 2020 dataset are listed in Tab. 1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>Previous partial domain adaptation methods <ref type="bibr" coords="4,332.56,472.09,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="4,344.15,472.09,7.75,8.74" target="#b2">3]</ref> evaluated their models based on a small dataset (e.g., Office 31), while their models have lower generalizability to large-scale datasets. In addition, feature consistency of both source and target domains is not well addressed in the PDA.</p><p>In this paper, we present our approach: adversarial consistent learning (ACL) on partial domain adaptation. It can align the feature distribution of the source and target domains in the shared categories and guarantee feature consistency across the two domains. Importantly, ACL identifies irrelevant source categories via down-weighting class importance automatically. Evaluation on the large-scale PlantCLEF 2020 challenge dataset shows a high generalizability of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem and notation</head><p>For unsupervised domain adaptation, given a source domain</p><formula xml:id="formula_0" coords="4,393.74,629.82,86.36,13.33">D S = {X S i , Y S i } N S i=1</formula><p>of N S labeled samples across the set of categories C S and a target domain</p><formula xml:id="formula_1" coords="4,134.77,644.16,345.83,22.10">D T = {X T j } N T</formula><p>j=1 of N T samples without any labels (Y T is unknown) across the set of categories C T . For partial domain adaptation, the number of categories in C T is less than the number of categories in C S , and C T C S . The samples X S and X T obey the marginal distributions of P S and P T . The conditional distributions of two domains are denoted as Q S and Q T . Due to the discrepancy of two domains, the distributions are assumed to be different, i.e., P S = P T and Q S = Q T . Our ultimate goal is to learn a classifier f under a feature extractor G, which selects shared categories between two domains, and ensures lower generalization error in the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deep features extraction</head><p>To circumvent the large computation resource requirement of training largescale PlantCLEF 2020 challenge datasets, we instead focus on deep features from pre-trained models. Based on Zhang and Davison <ref type="bibr" coords="5,382.49,274.04,14.61,8.74" target="#b16">[17]</ref>, the deep features are extracted from the last fully connected layer of the pretrained model via Φ. One represented feature vector has the size of 1 × 1000 and corresponds to one plant image. Therefore, the source domain and the target domain can be represented by Φ(X S ) ∈ R N S ×1000 and Φ(X T ) ∈ R N T ×1000 , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Source classifier</head><p>The task in the source domain is trained using the typical cross-entropy loss in following equation:</p><formula xml:id="formula_2" coords="5,173.63,400.48,306.97,30.44">L S (f (G(Φ(X S ))), Y S ) = - 1 N S N S i=1 C S c=1 Y S ic log(f (G(Φ(X S i )))),<label>(1)</label></formula><p>where Y S ic ∈ [0, 1] C S is the probability of each class of ground truth for the ith element of S, f is the classifier in Fig. <ref type="figure" coords="5,319.19,453.57,3.87,8.74" target="#fig_1">2</ref>, and f (G(Φ(X S i ))) is the predicted probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Adversarial domain loss</head><p>In general adversarial learning, the system learns a mapping from the source domain to the target domain. Given the feature representation of feature extractor G, we can learn a discriminator D, which can distinguish the two domains using the following loss function:</p><formula xml:id="formula_3" coords="5,145.59,568.06,311.45,65.51">L A (G X S X T , G(Φ(X S )), G(Φ(X T ))) = - 1 N S N S i=1 log(1 -D(G(Φ(X S i )))) - 1 N T N T j=1 log(D(G(Φ(X T j )))).</formula><p>(2) However, Eq. 2 only guarantees source domain data will be close to the target data (G X S X T ), and it does not ensure that the target data will be close to the source data. We hence introduce another mapping from the target domain to the source domain G X T X S in Eq. 3 and train it with the same adversarial loss as in G X S X T as shown in Eq. 2.</p><formula xml:id="formula_4" coords="6,230.49,164.86,250.11,10.27">L A (G X T X S , G(Φ(X S )), G(Φ(X T )))<label>(3)</label></formula><p>For G X S X T , the source domain has the label of 0 and the target domain has the label of 1, which is corresponding to Domain Label 1 in Fig. <ref type="figure" coords="6,404.49,198.97,3.87,8.74" target="#fig_1">2</ref>. Meanwhile, for G X T X S , 1 is the new label for the source domain and and 0 is the new label for target domain, which is corresponding to Domain Label 2 in Fig. <ref type="figure" coords="6,424.70,222.88,3.87,8.74" target="#fig_1">2</ref>. Therefore, we define the adversarial learning loss as:</p><formula xml:id="formula_5" coords="6,166.59,256.23,309.76,25.21">L A (G(Φ(X S )), G(Φ(X T ))) = L A (G X S X T , G(Φ(X S )), G(Φ(X T ))) + L A (G X T X S , G(Φ(X S )), G(Φ(X T ))). (<label>4</label></formula><formula xml:id="formula_6" coords="6,476.35,263.80,4.24,8.74">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Feature consistency loss</head><p>To encourage the source domain and target domain information to be preserved during adversarial learning, we propose a feature consistency loss in our model. Details of the feature reconstruction layers are shown in Fig. <ref type="figure" coords="6,395.11,345.12,3.87,8.74" target="#fig_1">2</ref>; the reconstructed layers are right behind the feature extractor G in the shared layers, and they aim to reconstruct the extracted features and maintain the invariant features during the conversion process. The feature consistency loss is defined as:</p><formula xml:id="formula_7" coords="6,191.69,402.58,288.91,40.46">L Con (G X S X T , G X T X S , G(Φ(X S )), G(Φ(X T ))) = E xs∼G(Φ(X S )) [ (G X T X S (G X S X T (x s )) -x s )] + E xt∼G(Φ(X T )) [ (G X S X T (G X T X S (x t )) -x t )],<label>(5)</label></formula><p>where is the mean squared error loss function, which calculates the difference between true features and the reconstructed features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Shared categories selection</head><p>In PDA, the set of target domain labels is a subset of the source domain labels, i.e., C T C S . In the PlantCLEF challenge, the size of irrelevant label set (C S -C T ) is far larger than the size of</p><formula xml:id="formula_8" coords="6,278.40,536.50,105.67,9.65">C T (|C S -C T | &gt;&gt; |C T |).</formula><p>If we use all elements of the source domain distribution to match the target domain distribution, it will cause negative transfer since the target domain will also be forced to match the irrelevant labels (C S -C T ). Therefore, it is important to identify the shared categories between source and target domains.</p><p>To address the aforementioned challenge, we re-weight the source domain label set via reducing the irrelevant label set. During the training, we can get the predicted probability of the target domain: Ŷ T j = f (G(Φ(X T j ))), which gives a probability of each source label in C S . As we know, the set of irrelevant source labels and target label set are disjoint, and the target data are significantly dissimilar to the source data in the irrelevant label set. Therefore, the probability of irrelevant categories should be sufficiently small and can be ignored. We then defined the weight vector as:</p><formula xml:id="formula_9" coords="7,267.91,147.04,212.69,30.44">W = 1 N T N T j=1 Ŷ T j ,<label>(6)</label></formula><p>where W is a |C S |-dimensional weight vector. The irrelevant categories (C S -C T ) will have a much smaller weight than the shared categories. We then assign the weight as 0 if its element W c is less than a sufficiently small number (e.g., 10e -9). By reducing the weight of irrelevant categories, the shared categories can be emphasized and negative transfer will be mitigated. The weight vector W is applied in both the source classifier and domain discriminator over the source domain data as shown in the following objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Overall objective</head><p>We combine the three aforementioned loss functions to formalize our objective function:</p><formula xml:id="formula_10" coords="7,170.17,326.33,310.42,40.15">L(X S , X T , Y S , G X S X T , G X T X S ) = L S (f (W(G(Φ(X S )))), Y S ) + γL A (W(G(Φ(X S ))), G(Φ(X T ))) + βL Con (G X S X T , G X T X S , W(G(Φ(X S ))), G(Φ(X T ))),<label>(7)</label></formula><p>where γ and β are tradeoff parameters between different loss functions. Our model ultimately minimizes the difference during the transition from the source domain to target domain and from the target domain to the source domain. Meanwhile, it maximizes the ability to distinguish the two domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Gradients of shared layers</head><p>The shared layers consist of the feature extractor G and the feature reconstruction layers. In G, there are two dense layers, a "Relu" activation layer, and a dropout layer. The numbers of units of the dense layer are 1000 and 997, respectively. The rate of the Dropout layer is 0.5. The feature reconstruction layers have a "Relu" activation layer, a dropout layer and a dense layer with the number of units of 1000. The shared layers are jointly optimized by both the source classification loss, adversarial domain loss and feature consistency loss.</p><p>Let F E (•, θ E ) be the output of the shared encoder with parameters of θ E . In addition, let F S (•, θ S ) be the output of class label classifier with parameters of θ S , F A (•, θ A ) be the output of domain label predictor with parameters of θ A , and F Con (•, θ Con ) be the output of feature consistency regressor with parameters of θ Con . Therefore, the shared layers are optimized by these three gradients. The parameters in the shared layers are updated in the following equation:</p><formula xml:id="formula_11" coords="7,166.87,620.06,309.48,49.20">θ S θ S -η ∂L S ∂θ S , θ A θ A -ητ θ A ∂L A ∂θ A , θ Con θ Con -η ∂L Con ∂θ Con θ E θ E -η( ∂L S ∂θ S + τ θ A ∂L A ∂θ A + ∂L Con ∂θ Con ), (<label>8</label></formula><formula xml:id="formula_12" coords="7,476.35,639.61,4.24,8.74">)</formula><p>where η is the learning rate and τ is the adaptation factor from gradient reversal layer (GRL) in <ref type="bibr" coords="8,202.94,130.95,9.96,8.74" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10">Theoretical Analysis</head><p>We now formalize the error bound of our model. ACL model is trained with both the labeled source domain and the unlabeled target domain. The error bound of the source domain and the target domain ( (h)) in our model is then formally written as:</p><formula xml:id="formula_13" coords="8,237.10,235.54,239.25,12.78">(h) = X S (h, Y S ) + X T (h, ŶT ), (<label>9</label></formula><formula xml:id="formula_14" coords="8,476.35,237.63,4.24,8.74">)</formula><p>where ŶT is the predicted label of target domain. The term</p><formula xml:id="formula_15" coords="8,134.77,260.14,345.83,22.22">X S (h, Y S ) = E x∼X S [|h(x) -Y S |] and X T (h, ŶT ) = E x∼X T [|h(x) -ŶT |]</formula><p>denote the expected risk over the source domain and the target domain with respect to the ground truth labels and predicted labels, respectively (where</p><formula xml:id="formula_16" coords="8,369.72,296.01,87.12,8.74">| • | is the L1 norm).</formula><p>During the training, we expect the error X T (h, ŶT )) to be close to X T (h, Y T ), which evaluates the classifier f with true target domain labels. The smaller the difference between these two errors, the better the model performs and more discrepancies of the two domains are reduced. Existing domain adaptation theory shows that the risk in the target domain can be minimized by bounding the source risk and discrepancy between source and target domains <ref type="bibr" coords="8,415.45,368.44,10.30,8.74" target="#b0">[1]</ref>). Therefore, the generalization error bound of our model is shown in the following Lemma.</p><p>Lemma 1 Let h be a hypothesis in a class H. Then</p><formula xml:id="formula_17" coords="8,235.77,415.69,244.82,27.73">(h) = X S (h, Y S ) + X T (h, ŶT ) ≤ 2 X S (h) + d H (D S , D T ) + C,<label>(10)</label></formula><p>where</p><formula xml:id="formula_18" coords="8,134.77,453.29,345.83,22.22">d H (D S , D T ) = 2 sup h,h ∈H | X S (h, h ) -X T (h, h )| is the H-divergence of training and test data in the hypothesis space H. C = X S (h * , Y S )+ X T (h * , Y T )</formula><p>is the adaptability to quantify the error in ideal hypothesis h * space of training and test data, which should be small and is the optimal hypothesis via minimizing the joint error in Eq. 11.</p><formula xml:id="formula_19" coords="8,226.06,525.73,254.53,12.34">h * = arg min X S (h, Y S ) + X T (h, Y T )<label>(11)</label></formula><p>In Lemma 1, the generalization boundary of our model consists of three terms: training data error, data discrepancy d H (D S , D T ), which is estimated by the disagreement of hypothesis in the space H, and the adaptability C of the ideal joint hypothesis. In ACL model, the first term is measured by Eq. 1. The domain discrepancy is assessed by adversarial learning loss and feature consistency loss. Furthermore, ACL finds the ideal hypothesis and reduces the training error in each iteration. Hence, our model can find a minimal boundary for two domains. In other words, ACL can implicitly minimize the target domain risk, domain discrepancy, and the adaptability of true hypothesis h in terms of the hypothesis space H. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>As aforementioned, the deep features are extracted from the last fully connected layer <ref type="bibr" coords="9,192.72,404.68,15.50,8.74" target="#b14">[15,</ref><ref type="bibr" coords="9,210.21,404.68,11.62,8.74" target="#b16">17]</ref>. One represented feature vector has the size of 1 × 1000 and is corresponding to one plant image. Therefore, the feature representation of domain herbarium (H) has the size of 320, 750 × 1000, domain herbarium photo associations (A) has the size of 1, 816 × 1000, domain photo (P) has the size of 4, 482 × 1000, and domain test (T) has the size of 3, 186 × 1000. In the experiment, our task is to reduce the error in the target domain (real-world plant images), i.e., photo domain or test domain. Our tasks will focus more on the evaluation of the domain P and domain T. Since the herbarium photo associations (A) is important to bridge the map between two domains, we hence include the domain A in the training procedure to form a new source domain, which consists of domain herbarium (H) and domain A. Domain H + A has the size of 322, 566 × 1000. We then train the model based on these extracted feature vectors. In Tab. 2, H P represents learning knowledge from domain H, which is applied to domain P.</p><p>The parameters of ACL are first tuned based on the performance of the domain P, while the model is trained with H + A domain. We then apply these parameters to domain T and submit it to the challenge for the evaluation. Our implementation is based on Keras. The parameters settings are β = γ = 0.5, τ = 0.31, learning rate: η = 0.0001, batch size = 128, the number of iterations is 1000 and the optimizer is Adam. The details of the layers are shown in Fig. <ref type="figure" coords="9,472.84,632.02,3.87,8.74" target="#fig_2">3</ref>.</p><p>We also compare our results with two domain adaptation methods: DANN <ref type="bibr" coords="9,470.07,644.16,10.52,8.74" target="#b3">[4]</ref> and ADDA <ref type="bibr" coords="9,185.87,656.12,14.61,8.74" target="#b13">[14]</ref>. In addition, we extracted features from four well-trained models (ResNet50 <ref type="bibr" coords="10,183.69,248.63,9.96,8.74" target="#b6">[7]</ref>, InceptionV3 <ref type="bibr" coords="10,257.35,248.63,14.61,8.74" target="#b12">[13]</ref>, Inception-Resnet-V2 <ref type="bibr" coords="10,372.16,248.63,14.61,8.74" target="#b11">[12]</ref>, NASNetLarge <ref type="bibr" coords="10,458.45,248.63,14.76,8.74" target="#b20">[21]</ref>), which is trained based on large-scale ImageNet datasets. We then feed these different extracted features into the shared layers and optimize the objective function in Eq. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The performance of the photo domain is shown in Tab. 2. We report the accuracy of the whole photo domain (Acc =</p><formula xml:id="formula_20" coords="10,342.49,347.43,133.79,14.11">N T j=1 ( Ŷ T j == Y T j )/N T × 100)</formula><p>, where ŶT is the predicted label for the target domain. We can observe that the extracted features from NASNetLarge with our ACL architecture achieves the highest performance across all three tasks. We observe that two domain adaptation methods have relatively lower performance in all three tasks. One reason is that these two methods have weak feature extractors, and they do not exclude the irrelevant categories in the source domain, which might cause the negative transfer. Moreover, with the increasing of the ImageNet model, we can extract better features from plant images, which lead to the high performance of the NASNetLarge-ACL model. In addition, we conduct an ablation study in which we train the best NASNetLarge-ACL model without the shared categories selection (NASNetLarge-ACL -W). The results from all three tasks are lower than NASNetLarge-ACL model, which indicates the shared categories selection is useful in our model. These experiments demonstrate the efficiency of the ACL model in finding the invariant-features of two domains.</p><p>In the final stage of the PlantCLEF 2020 Challenge, our solutions are evaluated by the organizers using the test domain data. As shown in Tab. 3, our method achieved mean reciprocal rank (MRR) of 0.032 in the whole test domain, and MRR of 0.016 in the subset of the test domain, and our method places 4th in the contest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>There are two compelling advantages of the ACL model. First, we consider the adversarial consistent learning paradigm, which maintains the domain-invariant features from the source domain to the target domain and vice versa. Secondly, Table <ref type="table" coords="11,199.13,127.36,3.87,8.74">3</ref>: MRR on PlantCLEF 2020 dataset for test domain <ref type="bibr" coords="11,433.11,127.36,10.52,8.74" target="#b4">[5]</ref> Team we reduce the weight of irrelevant categories in the source domain, which eliminates the negative transfer during the training. Although the performance of our model is better than several baseline methods, the highest accuracy of the photo domain is less than 10%, which illustrates that the transfer learning ability in the real world image is lower. One underlying reason is that PlantCLEF 2020 Challenge has difficult datasets-that there are significant differences between herbarium domain and photo domain, as shown in Fig. <ref type="figure" coords="11,389.17,316.65,3.87,8.74" target="#fig_0">1</ref>. Another reason is caused by the weakness of our model since we only train deep features instead of raw images to reduce the computational requirements; some features might be ignored during the training. The performance of the ACL model could be improved if we train the architecture with raw images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose an adversarial consistent learning network on partial domain adaptation termed (ACL) to overcome limitations in finding proper shared categories and guaranteeing the feature consistency of two domains. Our model is optimized via minimizing a three-component loss function. As each component of our ACL model, explicit domain-invariant features are maintained through such a cross-domain training scheme. Experimental results demonstrate our proposed ACL model yields promising results on the PlantCLEF 2020 Challenge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,195.21,345.83,8.74;3,134.77,207.17,276.05,8.74;3,146.11,115.83,323.15,67.85"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Examples images of the PlantCLEF 2020 dataset. The large discrepancy between training and test data cause the difficulty in the PDA.</figDesc><graphic coords="3,146.11,115.83,323.15,67.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,267.11,345.83,8.74;4,134.77,279.07,345.82,8.74;4,134.77,291.02,345.82,8.74;4,134.77,302.98,302.16,8.74;4,438.13,300.93,14.53,6.73;4,438.99,308.45,12.80,6.73;4,454.69,302.98,2.77,8.74;4,463.15,300.93,15.75,6.73;4,464.01,308.45,14.02,6.73;4,134.77,317.34,16.05,8.74;4,154.90,315.30,23.39,6.73;4,155.77,322.82,21.67,6.73;4,179.99,317.34,300.60,8.74;4,134.77,329.30,345.82,8.74;4,134.77,341.25,345.83,9.65;4,134.77,353.21,344.75,9.65;4,134.77,365.16,345.82,8.74;4,134.77,377.12,345.83,8.74;4,134.77,389.08,79.95,8.74;4,140.68,115.84,334.00,139.75"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The architecture of our proposed ACL model. We first extract deep features from a pre-trained model for both source and target domains via Φ. The shared layers are jointly trained with source and target features. Also, the parameters in shared layers are updated by the backward gradients ( ∂L S ∂θ S , ∂L A ∂θ A and ∂L Con ∂θ Con ) from class label classifier, domain label predictor and feature consistency regressor. The ACL model consists of three different loss functions (source classification loss L S , adversarial domain loss L A , and feature consistency loss L Con ). The feature extractor G in the shared layers is used for both classifier f and domain discriminator D (The blue dash lines are the backward gradients, and GRL stands for gradient reversal layer). Layers visualization of architecture is shown in Fig. 3.</figDesc><graphic coords="4,140.68,115.84,334.00,139.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,134.77,250.62,345.83,8.74;9,134.77,262.58,345.83,8.74;9,134.77,274.53,345.82,8.74;9,134.77,286.49,345.83,8.74;9,134.77,298.44,345.83,8.74;9,134.77,310.40,281.77,8.74;9,134.77,115.84,345.83,123.26"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Layers visualization of our proposed ACL model. Two input layers are from source domain data and target domain data, respectively. The intermediate model is the shared layers in Fig. 2. The source classification layer refers to the classifier f , and two reconstruction layers guarantee the feature consistency of two domains. Two subtract layers are used for the domain discriminator. In addition, the gradient reversal layer is used for backpropagation.</figDesc><graphic coords="9,134.77,115.84,345.83,123.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,143.84,585.58,327.77,65.43"><head>Table 1 :</head><label>1</label><figDesc>Statistics on PlantCLEF 2020 dataset</figDesc><table coords="3,143.84,598.91,327.77,52.10"><row><cell>Domain</cell><cell>Number of Samples</cell><cell>Number of Classes</cell></row><row><cell>Herbarium (H)</cell><cell>320,750</cell><cell>997</cell></row><row><cell>Herbarium photo associations (A)</cell><cell>1,816</cell><cell>244</cell></row><row><cell>Photo (P)</cell><cell>4,482</cell><cell>375</cell></row><row><cell>Test (T)</cell><cell>3,186</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,156.25,127.36,302.86,99.70"><head>Table 2 :</head><label>2</label><figDesc>Accuracy (%) on PlantCLEF 2020 dataset for photo domain</figDesc><table coords="10,182.91,139.70,249.53,87.37"><row><cell>Task</cell><cell>A P</cell><cell>H P</cell><cell>H+A P</cell></row><row><cell>DANN [4]</cell><cell>1.07</cell><cell>1.85</cell><cell>2.01</cell></row><row><cell>ADDA [14]</cell><cell>2.95</cell><cell>3.05</cell><cell>3.43</cell></row><row><cell>ResNet50-ACL</cell><cell>2.96</cell><cell>4.83</cell><cell>6.97</cell></row><row><cell>InceptionV3-ACL</cell><cell>3.02</cell><cell>5.93</cell><cell>7.95</cell></row><row><cell>Inception-Resnet-V2-ACL</cell><cell>3.73</cell><cell>7.07</cell><cell>8.43</cell></row><row><cell>NASNetLarge-ACL -W</cell><cell>3.84</cell><cell>7.92</cell><cell>8.18</cell></row><row><cell>NASNetLarge-ACL</cell><cell>5.98</cell><cell>8.64</cell><cell>9.67</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,549.04,337.63,7.86;11,151.52,559.97,329.07,7.89;11,151.52,570.96,25.60,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,151.52,560.00,176.54,7.86">A theory of learning from different domains</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,336.00,560.00,72.71,7.86">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,581.31,337.63,7.86;11,151.52,592.27,329.07,7.86;11,151.52,603.22,42.49,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,308.07,581.31,153.68,7.86">Partial adversarial domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,592.27,285.74,7.86">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,613.57,337.63,7.86;11,151.52,624.53,284.78,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,307.25,613.57,173.33,7.86;11,151.52,624.53,118.46,7.86">Domain adversarial reinforcement learning for partial domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04094</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.96,634.88,337.64,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,100.39,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,305.05,634.88,175.55,7.86;11,151.52,645.84,43.20,7.86">Domain adaptive neural networks for object recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,216.54,645.84,244.16,7.86">Pacific Rim international conference on artificial intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="898" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,119.67,337.63,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,190.04,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,281.81,119.67,174.93,7.86">Overview of lifeclef plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,167.11,130.63,313.48,7.86;12,151.52,141.59,24.01,7.86">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">2020. Sep. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>CLEF working notes 2020</note>
</biblStruct>

<biblStruct coords="12,142.96,151.56,337.63,7.86;12,151.52,162.52,329.07,7.86;12,151.52,173.48,217.12,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,272.16,162.52,109.25,7.86">Generative adversarial nets</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,403.03,162.52,77.56,7.86;12,151.52,173.48,123.83,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,183.45,337.63,7.86;12,151.52,194.41,329.07,7.86;12,151.52,205.37,116.21,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,290.95,183.45,172.55,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,194.41,329.07,7.86;12,151.52,205.37,31.16,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,215.33,337.64,7.86;12,151.52,226.29,329.07,7.86;12,151.52,237.25,329.07,7.86;12,151.52,248.21,329.07,7.86;12,151.52,259.17,329.07,7.86;12,151.52,270.13,236.49,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,340.94,237.25,139.65,7.86;12,151.52,248.21,329.07,7.86;12,151.52,259.17,39.12,7.86">Overview of lifeclef 2020: a systemoriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,214.30,259.17,266.30,7.86;12,151.52,270.13,70.48,7.86">Proceedings of CLEF 2020, CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting>CLEF 2020, CLEF: Conference and Labs of the Evaluation Forum<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">Sep. 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,280.10,337.64,7.86;12,151.52,291.06,329.07,7.86;12,151.52,302.02,127.12,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,317.77,280.10,162.83,7.86;12,151.52,291.06,162.43,7.86">Transferable adversarial training: A general approach to adapting deep classifiers</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,333.19,291.06,147.40,7.86;12,151.52,302.02,33.81,7.86">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4013" to="4022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,311.99,337.98,7.86;12,151.52,322.94,329.07,7.86;12,151.52,333.90,25.60,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,327.98,311.99,152.61,7.86;12,151.52,322.94,22.82,7.86">Conditional adversarial domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,197.89,322.94,215.29,7.86">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1647" to="1657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,343.87,337.97,7.86;12,151.52,354.81,235.02,7.89" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,264.13,343.87,118.92,7.86">A survey on transfer learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,390.51,343.87,90.08,7.86;12,151.52,354.83,129.05,7.86">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,364.80,337.97,7.86;12,151.52,375.76,329.07,7.86;12,151.52,386.72,146.20,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,360.37,364.80,120.22,7.86;12,151.52,375.76,200.58,7.86">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,372.47,375.76,108.13,7.86;12,151.52,386.72,117.53,7.86">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,396.69,337.97,7.86;12,151.52,407.65,329.07,7.86;12,151.52,418.61,254.31,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,393.24,396.69,87.35,7.86;12,151.52,407.65,148.57,7.86">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,321.62,407.65,158.98,7.86;12,151.52,418.61,161.31,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,428.57,337.97,7.86;12,151.52,439.53,329.07,7.86;12,151.52,450.49,158.87,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,343.31,428.57,137.28,7.86;12,151.52,439.53,41.89,7.86">Adversarial discriminative domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,213.97,439.53,266.62,7.86;12,151.52,450.49,65.59,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,460.46,337.98,7.86;12,151.52,471.42,329.07,7.86;12,151.52,482.38,329.07,7.86;12,151.52,493.31,197.20,7.89" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,365.14,460.46,115.45,7.86;12,151.52,471.42,329.07,7.86;12,151.52,482.38,279.43,7.86">Automated identification of hookahs (waterpipes) on instagram: an application in feature extraction using convolutional neural network and support vector machine classification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Allem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,438.66,482.38,41.93,7.86;12,151.52,493.34,105.70,7.86">Journal of Medical Internet Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">10513</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,503.31,337.98,7.86;12,151.52,514.27,297.41,7.86" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02322</idno>
		<title level="m" coord="12,259.18,503.31,221.42,7.86;12,151.52,514.27,131.47,7.86">Modified distribution alignment for domain adaptation with pre-trained inception resnet</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,524.24,337.98,7.86;12,151.52,535.19,329.07,7.86;12,151.52,546.15,154.78,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,258.96,524.24,221.63,7.86;12,151.52,535.19,14.75,7.86">Impact of imagenet model selection on domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,186.16,535.19,294.44,7.86;12,151.52,546.15,70.27,7.86">Proceedings of the IEEE Winter Conference on Applications of Computer Vision Workshops</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,556.12,337.98,7.86;12,151.52,567.08,329.07,7.86;12,151.52,578.04,190.99,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,305.81,556.12,174.78,7.86;12,151.52,567.08,74.78,7.86">Domain-symmetric networks for adversarial domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,247.89,567.08,232.71,7.86;12,151.52,578.04,97.71,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5031" to="5040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,588.01,337.97,7.86;12,151.52,598.97,323.08,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,297.83,588.01,182.76,7.86;12,151.52,598.97,34.18,7.86">Transductive learning via improved geodesic sampling</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,207.12,598.97,238.82,7.86">Proceedings of the 30th British Machine Vision Conference</title>
		<meeting>the 30th British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,608.94,337.98,7.86;12,151.52,619.90,245.96,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,255.73,608.94,224.86,7.86;12,151.52,619.90,67.09,7.86">Domain adaptation for object recognition using subspace sampling demons</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,226.37,619.90,142.43,7.86">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,629.87,337.98,7.86;12,151.52,640.82,329.07,7.86;12,151.52,651.78,213.33,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,340.06,629.87,140.54,7.86;12,151.52,640.82,115.95,7.86">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,286.99,640.82,193.61,7.86;12,151.52,651.78,120.32,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
