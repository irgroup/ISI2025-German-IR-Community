<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.77,115.96,341.83,12.62;1,271.22,133.89,72.91,12.62;1,208.91,154.10,197.55,10.52">Domain Adaptation in the context of herbarium collections A submission to PlantCLEF 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,148.14,190.50,56.37,8.74"><forename type="first">Juan</forename><surname>Villacis</surname></persName>
							<email>jvillacis@ic-itcr.ac.cr</email>
							<affiliation key="aff0">
								<orgName type="institution">Costa Rica Institute of Technology</orgName>
								<address>
									<settlement>Cartago</settlement>
									<country key="CR">Costa Rica</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,215.07,190.50,56.56,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<email>herve.goeau@cirad.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">AMAP</orgName>
								<orgName type="laboratory" key="lab2">CIRAD</orgName>
								<orgName type="institution" key="instit1">Univ Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">INRAE</orgName>
								<orgName type="institution" key="instit4">IRD</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.52,190.50,60.95,8.74"><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<email>pierre.bonnet@cirad.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">AMAP</orgName>
								<orgName type="laboratory" key="lab2">CIRAD</orgName>
								<orgName type="institution" key="instit1">Univ Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">INRAE</orgName>
								<orgName type="institution" key="instit4">IRD</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,366.35,190.50,48.08,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.fr</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Zenith Team</orgName>
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">UMR LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,444.36,190.50,22.86,8.74;1,274.02,202.46,62.85,8.74"><forename type="first">Erick</forename><surname>Mata-Montero</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Costa Rica Institute of Technology</orgName>
								<address>
									<settlement>Cartago</settlement>
									<country key="CR">Costa Rica</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.77,115.96,341.83,12.62;1,271.22,133.89,72.91,12.62;1,208.91,154.10,197.55,10.52">Domain Adaptation in the context of herbarium collections A submission to PlantCLEF 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3C34D927E7EC2D164A556DDAC34AD98C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a submission to the PlantCLEF 2020 challenge, whose topic was the classification of plant images in the field, based on a dataset composed mainly of herbaria.. This work proposes the usage of domain adaptation techniques to tackle the problem. In particular, it makes use of the Few-Shot Adversarial Domain Adaptation method proposed by Motiian et al. ( <ref type="formula" coords="1,326.85,366.43,3.58,7.86">9</ref>). Additionally, a modification of this architecture is proposed to take advantage of upper taxa relations between species in the dataset. Experiments performed show that domain adaptation can provide very significant increases in accuracy when compared with traditional CNN-based approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent approaches to automated plant identification have relied on deep learningbased techniques <ref type="bibr" coords="1,211.70,482.63,11.62,8.74" target="#b0">(1)</ref>. These techniques can be very effective and compete with human experts if a large amount of labeled data is available, even if it is partially noisy (2; 6). However, in the path towards achieving the goal of universal plant species identification, a significant obstacle is posed by the large number of species for which there are none or very few samples of their appearance in their natural state, thus making it very difficult to use this kind of methods. Carrying out missions to collect more data, typically in tropical regions, is not a feasible solution due to the elevated cost, difficulty to access the areas where the species are located and vast amount of data still not labeled. Nonetheless, vast amounts of data about these species exist in the form of herbarium sheets, collected over centuries by botanists and which has been recently massively digitized and published online. This is the topic of the PlantCLEF 2020 challenge <ref type="foot" coords="2,134.77,117.42,3.97,6.12" target="#foot_0">5</ref> . Given a large dataset of digitized herbarium sheets and very few photos in the field, the objective is to develop a classifier that can perform well on a test set consisting only of field photos after being trained primarily on herbariums. This article describes in detail the methods used for our submissions to the challenge (identified by the acronym aabab on the challenge web page <ref type="foot" coords="2,395.41,165.24,3.97,6.12" target="#foot_1">6</ref> ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>Dataset The main dataset to be used is PlantCLEF 2020 (3), <ref type="bibr" coords="2,408.28,435.43,11.62,8.74" target="#b7">(8)</ref>. This dataset has 320,752 herbarium images from 997 species, 4,482 field images from 375 species and 1,816 images from 244 species where for each specimen there are images both in its natural state and in herbarium sheets. In addition to this dataset, some experiments will include additional data from sources like GBIF <ref type="foot" coords="2,476.12,481.68,3.97,6.12" target="#foot_2">7</ref>and PlantCLEF 2019 <ref type="bibr" coords="2,233.82,495.21,11.62,8.74" target="#b6">(7)</ref>. These images come from the dataset used by <ref type="bibr" coords="2,460.11,495.21,16.38,8.74" target="#b10">(11)</ref>. Figure <ref type="figure" coords="2,166.50,507.16,4.98,8.74">2</ref> shows examples of images in the PlantCLEF 2020 dataset. As can be observed in these examples, the herbarium and field pictures differ greatly, even if they come from the same species (and even the same specimen). This aspect makes the task at hand a particularly challenging one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Architecture and Models</head><p>Convolutional Neural Networks A common way to perform classification is to take a pretrained CNN and to re-train it on the new target classes. As mentioned earlier, this approach usually requires vast amounts of data. Given Fig. <ref type="figure" coords="3,153.45,329.31,3.87,8.74">2</ref>: Herbarium and Field images from the Asystasia gangetica (L.) T.Anderson specie. Significant visual variations between them in color, background and shape can be observed even though they are from the same species that the training set is comprised mostly of herbariums and the test set of field photos, we expect the performance of such an approach to be low. This reason motivates us to look for alternate solutions (which will be described in the next sections), but it is still necessary to measure the performance of the baseline CNN approach. Therefore, the submitted runs also include experiments with a CNN-based approach. The architecture chosen is Resnet50 (4) to maintain the same conditions as those used in the other experiments. Experiments will be performed using the PlantCLEF2020 dataset solely, or the union of all the datasets described in section 2.1. To take advantage of the data available, training will be performed in three stages. First the model will be trained on the ImageNet dataset, in a second stage only the herbariums will be used and in the last stage only the photos will be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Adaptation Architectures</head><p>To tackle the problem of having very few photos in the field domain we base our solution on the architecture presented in <ref type="bibr" coords="3,147.23,569.27,11.62,8.74" target="#b8">(9)</ref>. This architecture which was devised to tackle the problem of few-shot domain adaptation has the following elements (see Figure <ref type="figure" coords="3,389.70,581.22,3.87,8.74" target="#fig_1">3</ref>):</p><p>a CNN-based feature extractor E that maps from the source dataset (herbaria) and target dataset (field images) into a common space, in which it is expected for the features represented to be independent from the original domain. a classifier F that performs species classification on the common space a discriminator D that determines to which of the following categories a pair of samples from the common space belongs to 1. Samples from different domains and different classes 2. Samples from different domains but the same class 3. Samples from the same domain but different classes 4. Samples from the same domain and the same class The division into these four categories instead of just into two categories determined by the domain is done to take advantage of label information in the target domain <ref type="bibr" coords="4,233.64,189.87,11.62,8.74" target="#b8">(9)</ref>.</p><p>The feature extractor and the classifier are trained in an adversarial approach with the discriminator in order to guarantee a domain agnostic common space and a robust classifier. In addition to this strategy, data augmentation is done in the target domain in order to complete the feature space with more training samples from this domain. The training is completed in 3 stages. During the first stage the encoder E and the classifier F are trained in a standard way with samples from only the source domain. In the second stage the discriminator D is trained to distinguish between samples from the 4 categories mentioned before. The objective of the first two stages is to initialize the weights of E F and D. Finally, during the third stage they are all trained together with the objective of performing domain adaptation. It can be said that domain adaptation has been achieved once the discriminator is not able to distinguish samples from categories 1 and 2 and categories 3 and 4. This means that once the samples have been encoded into the common domain, it is difficult for the discriminator to tell which was the original domain of such sample.</p><p>The architecture used has a ResNet50 (4) based encoder, which provides a good compromise between performance, memory use and training time. This is done by removing the last fully connected layer from the ResNet50 architecture. After applying these changes, the dimensionality of the common domain becomes 2048 features. This decision affects the architecture of the classifier and the discriminator. The first one is composed of a single fully-connected layer with 2048 inputs and 997 outputs. The discriminator is a multilayer perceptron, the input is composed of two feature vectors of 2048 features stacked together and it has 6 fully-connected layers that reduce the input size from 4096 features to just 4 outputs. Figure <ref type="figure" coords="5,233.56,272.87,4.98,8.74" target="#fig_2">4</ref> portrays these components. In the herbariums, a special tilling around the center is added. This operation creates a crop of the original herbarium that is centered around the center and can has a zoom level randomly set between 0.9 and 1.3. This is done because in herbariums the plant samples are commonly placed around the center of the sheet. Examples of this crops can be observed in figure <ref type="figure" coords="6,381.73,235.54,3.87,8.74">5</ref>. In the field domain, the transformation used is a center crop as large as the original picture permits it to be.</p><p>Self Supervision Self supervision is a technique derived from unsupervised learning that tries to address situations found in supervised learning in which there might not be enough labeled data in order to train an efficient model. The objective of these tasks is to extract robust visual information from the pictures which can be useful either as initial weights or to help the main model during training. In our experiments self supervision is used following the ideas presented in <ref type="bibr" coords="6,190.73,362.04,16.38,8.74" target="#b11">(12)</ref>, where it is used in a multi-task learning approach to help the main classifier. Figure <ref type="figure" coords="6,232.97,374.00,4.98,8.74" target="#fig_3">6</ref> depicts how it is performed in the context of the FADA architecture. Self-supervision is only used during the third stage in the training of the encoder and the classifier. The self-supervision task is applied to the image after it has undergone the data augmentation process. This modification is also used in the traditional CNN approach. In this case, the main model is joined by an additional classifier in a multi-task learning approach. The new classifier tries to predict the correct self-supervision label for the data. In this case, the extra classifier loss is combined with the loss from the main classifier From the several self supervision tasks in existence (5), we used the jigsaw puzzle solving <ref type="bibr" coords="6,199.49,481.79,16.38,8.74" target="#b9">(10)</ref>. This decision was taken based on the findings of ( <ref type="formula" coords="6,445.35,481.79,8.86,8.74">12</ref>) that when incorporating self-supervision into domain adaptation it is important to choose tasks that do not reinforce domain-dependent features and the fact that the spatial information learned from this showed to be useful when compared to other tasks like recolorization. This task consists in dividing the original image into tiles, rearranging them randomly into one of the 64 possible orderings with the largest distance between them and then having the network try to determine which of the rearrangements is used. Figure <ref type="figure" coords="6,328.90,565.48,4.98,8.74" target="#fig_3">6</ref> shows this process.</p><p>Upper taxa Given the nature of the dataset, it is possible to obtain taxonomical information from each species, like the genus or family name (= upper taxa). Because of the lack of data, we try to incorporate this information to the architecture on a multi-task learning approach, so that the features from the common domain are not only used to predict the species name, but also the genus or family of the specimen. This is done in two different approaches. For Fig. <ref type="figure" coords="7,153.45,473.17,3.87,8.74">5</ref>: Special tilling used for herbariums to take maximum advantage of the information they have the FADA architecture, both the classifier and the discriminator are extended with two additional sub-tasks, one for the genus level and one for the family level. These components have the same function as the original species classifier and discriminator, but they have to perform the discrimination and classification tasks with the genus or family instead. It is expected that specimens from the same group share partially similar visual content, and as such this training taking into account upper taxa can be indirectly used to increase performance on specimens that are poorly represented in the dataset but which might have related species in the dataset. In the flowers in figure <ref type="figure" coords="7,370.05,632.21,4.98,8.74" target="#fig_4">7</ref> it is possible to observe the visual similarities between plants from a different specie and the same genus. This is the kind of information we hope to take of.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Results from the runs submitted to the challenge can be seen in table <ref type="table" coords="9,455.23,385.74,4.98,8.74">2</ref> and figure <ref type="figure" coords="9,162.46,397.69,3.87,8.74" target="#fig_5">8</ref>.</p><p>The metric used to present the results of the challenge is the Mean Reciprocal Rank (MRR), which measures the average rank of the correct answer in a series of predictions. It is described by the following formula</p><formula xml:id="formula_0" coords="9,257.81,456.25,98.05,31.18">M RR = 1 |Q| |Q| i=1 1 rank i</formula><p>Two distinct MRRs are computed. A first MRR is computed on the full test set. Then, a second one is computed on a subset of the whole test set whose classes have particularly few field (or none) images in the training set.</p><p>As can be expected from the inherent difficulty of the challenge, the overall results obtained are low compared to previous editions of PlantCLEF. The method described here obtains the best result on the whole test set, and the second best result on the difficult subset of the test set.</p><p>In the runs submitted to the challenge, domain adaptation had a very significant impact on the results. Between the runs with a CNN and those that used this technique there is a 2600% and 1850% increase in the MRR All and the MRR Few. These results can be observed in figure <ref type="figure" coords="9,357.99,619.74,3.87,8.74">9</ref>.</p><p>Additional training data also seems to be a significant factor in obtaining higher values for the evaluation metric. In the MRR All there is a 5500% and a 165% increase in the values when comparing the results from a CNN and FADA approaches with the same techniques but adding the complementary dataset into the training process. This high increase can be observed in figure <ref type="figure" coords="10,442.09,130.95,9.96,8.74" target="#fig_0">10</ref> The last improvement that this work highlights is the benefit of the proposed extensions of FADA. The usage of self supervision and upper taxa information actually leads to slight but consistent increases of performance. Among this, the most useful to improve the MRR All turns out to be the combination of self supervision and upper taxa, with a 12% increase in the metric. Looking at the MRR on the difficult species, the use of upper taxa alone leads to an increase of 59% on the obtained value. This can be observed in figure <ref type="figure" coords="10,450.08,214.64,9.96,8.74" target="#fig_0">11</ref> and<ref type="figure" coords=""></ref> shows that the visual similarities between species of the same genus or family are particularly useful for species with very few training samples. The main conclusions from the experiments are the following -Domain Adaptation can have a very significant impact in obtaining better results in scenarios where there is a very limited availability of data in one domain but a large dataset on the other -The addition of extra data showed to be a very significant factor in achieving higher MRRs on the complete test set. On the subset of the most difficult species, however, this conclusion is not as clear-cut. The additional data provided an consistent gain when using the CNN approach, but on the other hand, the gain when using FADA was very small. This is expected to occur due to the fact that FADA is very sensitive to data (even one additional -As well upper taxa information, self supervision was an important factor in obtaining increases in performance. Although the increases were smaller, they were nonetheless consistent in particular when combined with multitask at upper taxa and on the few shot classes.</p><p>As future work, some other paths that can be explored are -Test different botanical or morphological information in addition to taxonomy. For instance, whether a species is Woody/Non-Woody may be an additional task to be solved. The usage of this information is expected to help even more general features that can boost even more the results.</p><p>-Exploit additional metadata contained in the dataset like geolocation information or individual pairs. This might require modifications to the architectures used.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,234.29,337.52,146.78,8.74;2,134.77,198.56,145.50,110.50"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Description of the problem</figDesc><graphic coords="2,134.77,198.56,145.50,110.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,524.54,345.82,8.74;4,134.77,536.49,219.09,8.74;4,177.18,288.01,261.00,225.00"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Approach by (9) to tackle the problem of few-shot domain adaptation. It has an encoder E, classifier F and discriminator D</figDesc><graphic coords="4,177.18,288.01,261.00,225.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,166.69,619.44,281.97,8.74;5,134.77,325.11,345.83,282.81"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Details of the FADA architecture used in the experiments</figDesc><graphic coords="5,134.77,325.11,345.83,282.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,134.77,272.97,345.82,8.74;8,134.77,284.93,54.57,8.74;8,134.77,115.83,345.83,145.61"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Addition of jigsaw self-supervision complementary task (10) to the FADA architecture.</figDesc><graphic coords="8,134.77,115.83,345.83,145.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,209.42,563.79,196.52,8.74;8,134.77,314.19,345.84,238.07"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Multi-task learning with upper taxons</figDesc><graphic coords="8,134.77,314.19,345.84,238.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,134.77,452.59,345.83,8.74;10,134.77,464.55,255.53,8.74;10,134.77,269.45,345.82,171.62"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Results of the PlantCLEF 2020 Challenge, the submissions by the ITCR Pl@ntNet team (AIcrowd username aabab) are highlighted</figDesc><graphic coords="10,134.77,269.45,345.82,171.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,197.54,326.38,220.29,8.74;11,160.70,115.83,293.95,199.02"><head>Fig. 9 :Fig. 10 :Table 2 :Fig. 11 :</head><label>910211</label><figDesc>Fig. 9: Impact of domain adaptation on the results</figDesc><graphic coords="11,160.70,115.83,293.95,199.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,134.77,115.83,345.83,201.95"><head></head><label></label><figDesc></figDesc><graphic coords="3,134.77,115.83,345.83,201.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,202.54,117.78,206.54,203.03"><head>Table 1 :</head><label>1</label><figDesc>Hyperparameters used in the training</figDesc><table coords="9,202.54,117.78,173.00,190.78"><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Framework</cell><cell>Pytorch</cell></row><row><cell cols="2">Learning rate E and F in FADA 0.001</cell></row><row><cell>Learning rate D in FADA</cell><cell>0.001</cell></row><row><cell>Learning rate in CNN</cell><cell>0.03</cell></row><row><cell>Batch size in FADA</cell><cell>15</cell></row><row><cell>Batch size in CNN</cell><cell>64</cell></row><row><cell cols="2">Number of epochs, stage 1 FADA 60</cell></row><row><cell cols="2">Number of epochs, stage 2 FADA 60</cell></row><row><cell cols="2">Number of epochs, stage 3 FADA 30</cell></row><row><cell>Number of epochs CNN</cell><cell>60</cell></row><row><cell cols="2">Learning rate scheduler criterion Step</cell></row><row><cell>Learning rate scheduler gamma</cell><cell>0.1</cell></row><row><cell>Learning rate scheduler step in</cell><cell>15</cell></row><row><cell>FADA stage 3</cell><cell></cell></row><row><cell>Learning rate scheduler step in</cell><cell>50</cell></row><row><cell>CNN and FADA stage 1</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0" coords="2,144.73,634.88,177.21,7.86"><p>https://www.imageclef.org/PlantCLEF2020</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1" coords="2,144.73,645.84,277.31,7.86"><p>https://www.aicrowd.com/challenges/lifeclef-2020-plant/submissions</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2" coords="2,144.73,656.80,85.57,7.86"><p>https://www.gbif.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="14,155.24,170.80,325.35,8.74;14,155.24,182.75,325.35,8.74;14,155.24,194.68,171.58,8.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,155.24,182.75,294.80,8.74">Going deeper in the automated identification of herbarium specimens</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carranza-Rojas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mata-Montero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,457.21,182.75,23.38,8.74;14,155.24,194.71,88.58,8.74">BMC evolutionary biology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">181</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,155.24,207.38,325.34,8.74;14,155.24,219.33,325.35,8.74;14,155.24,231.29,325.34,8.74;14,155.24,243.24,147.95,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,303.90,207.38,176.68,8.74;14,155.24,219.33,274.29,8.74">Plant identification based on noisy web data: the amazing performance of deep learning (lifeclef 2017)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,453.89,219.33,26.71,8.74;14,155.24,231.29,320.15,8.74">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">Sep. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>CLEF task overview 2017</note>
</biblStruct>

<biblStruct coords="14,155.24,255.91,325.35,8.74;14,155.24,267.86,325.35,8.74;14,155.24,279.82,255.67,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,298.77,255.91,181.82,8.74;14,155.24,267.86,52.33,8.74">Overview of the lifeclef 2020 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,230.80,267.86,249.79,8.74;14,155.24,279.82,76.19,8.74">CLEF task overview, CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">Sep. 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,155.24,292.48,325.35,8.74;14,155.24,304.44,325.35,8.74;14,155.24,316.40,172.74,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,311.35,292.48,169.24,8.74;14,155.24,304.44,24.20,8.74">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,205.28,304.44,275.31,8.74;14,155.24,316.40,82.15,8.74">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,155.24,329.06,325.35,8.74;14,155.24,341.02,325.35,8.74;14,155.24,352.97,80.26,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,238.34,329.06,242.25,8.74;14,155.24,341.02,82.17,8.74">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,245.54,341.02,235.06,8.74;14,155.24,352.97,49.26,8.74">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,155.24,365.64,325.35,8.74;14,155.24,377.59,325.35,8.74;14,155.24,389.55,325.35,8.74;14,155.24,401.50,325.35,8.74;14,155.24,413.46,222.64,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,265.10,377.59,215.49,8.74;14,155.24,389.55,321.84,8.74">Overview of lifeclef 2018: a large-scale evaluation of species identification and recommendation algorithms in the era of ai</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,171.03,401.50,309.56,8.74;14,155.24,413.46,88.79,8.74">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="247" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,155.24,426.13,325.35,8.74;14,155.24,438.08,325.35,8.74;14,155.24,450.04,325.35,8.74;14,155.24,461.99,325.35,8.74;14,155.24,473.95,319.83,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,399.14,438.08,81.46,8.74;14,155.24,450.04,325.35,8.74;14,155.24,461.99,89.09,8.74">Overview of lifeclef 2019: Identification of amazonian plants, south &amp; north american birds, and niche prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Robert-Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,268.78,461.99,211.81,8.74;14,155.24,473.95,185.98,8.74">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="387" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,155.24,486.61,325.35,8.74;14,155.24,498.57,325.35,8.74;14,155.24,510.52,325.35,8.74;14,155.24,522.48,325.35,8.74;14,155.24,534.43,325.34,8.74;14,155.24,546.39,325.34,8.74;14,155.24,558.34,149.37,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,172.80,522.48,307.79,8.74;14,155.24,534.43,240.33,8.74">Overview of lifeclef 2020: a system-oriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,417.63,534.43,62.96,8.74;14,155.24,546.39,297.52,8.74">Proceedings of CLEF 2020, CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting>CLEF 2020, CLEF: Conference and Labs of the Evaluation Forum<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">Sep. 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,155.24,571.01,325.35,8.74;14,155.24,582.96,325.35,8.74;14,155.24,594.92,92.99,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,376.02,571.01,104.56,8.74;14,155.24,582.96,70.40,8.74">Few-shot adversarial domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Iranmanesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,249.11,582.96,226.72,8.74">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6670" to="6680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,155.24,607.59,325.35,8.74;14,155.24,619.54,325.35,8.74;14,155.24,631.50,98.58,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,262.10,607.59,218.49,8.74;14,155.24,619.54,94.12,8.74">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,273.18,619.54,185.41,8.74">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,155.24,644.16,325.34,8.74;14,155.24,656.12,325.35,8.74;15,155.24,118.99,325.35,8.74;15,155.24,130.95,149.05,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,294.27,644.16,186.31,8.74;14,155.24,656.12,236.42,8.74">Recognition of the amazonian flora by inception networks with test-time class prior estimation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,415.67,656.12,64.92,8.74;15,155.24,118.99,297.13,8.74">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">Sep. 2019. 2019</date>
		</imprint>
	</monogr>
	<note>CLEF working notes 2019</note>
</biblStruct>

<biblStruct coords="15,155.24,142.90,325.35,8.74;15,155.24,154.86,302.54,8.74" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11825</idno>
		<title level="m" coord="15,350.15,142.90,130.44,8.74;15,155.24,154.86,123.18,8.74">Unsupervised domain adaptation through self-supervision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
