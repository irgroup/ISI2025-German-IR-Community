<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,179.26,115.96,256.84,12.62;1,237.28,133.89,140.81,12.62">Bird Species Recognition via Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,149.09,171.81,70.20,8.74"><forename type="first">Markus</forename><surname>Mühling</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Marburg</orgName>
								<address>
									<addrLine>Hans-Meerwein-Straße 6</addrLine>
									<postCode>D-35032</postCode>
									<settlement>Marburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.49,171.81,51.66,8.74"><forename type="first">Jakob</forename><surname>Franz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Marburg</orgName>
								<address>
									<addrLine>Hans-Meerwein-Straße 6</addrLine>
									<postCode>D-35032</postCode>
									<settlement>Marburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.02,171.81,78.39,8.74"><forename type="first">Nikolaus</forename><surname>Korfhage</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Marburg</orgName>
								<address>
									<addrLine>Hans-Meerwein-Straße 6</addrLine>
									<postCode>D-35032</postCode>
									<settlement>Marburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,392.82,171.81,73.45,8.74"><forename type="first">Bernd</forename><surname>Freisleben</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Marburg</orgName>
								<address>
									<addrLine>Hans-Meerwein-Straße 6</addrLine>
									<postCode>D-35032</postCode>
									<settlement>Marburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,179.26,115.96,256.84,12.62;1,237.28,133.89,140.81,12.62">Bird Species Recognition via Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EAA1FC2F0F358C2CF285AF951C519025</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bird Species Recognition</term>
					<term>BirdClef 2020</term>
					<term>Neural Architecture Search</term>
					<term>Gabor Wavelet Layer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the winning approach of the BirdCLEF 2020 challenge. The challenge is to automatically recognize bird sounds in continuous soundscapes. In our approach, a deep convolutional neural network model is used that directly operates on the audio data. This neural network architecture is based on a neural architecture search and contains multiple auxiliary heads and recurrent layers. During the training process, scheduled drop path is used as a regularization method and extensive data augmentation is applied to the audio input. Furthermore, species location lists are used in the post-processing step to reject unlikely classes. Our best run on the test set obtains a classification mean average precision score (cmap) of 13.1% and a retrieval mean average precision score (rmap) of 19.2%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatically identifying bird species in continuous sound recordings is an important task for monitoring the populations of different bird species in forest ecosystems. In the BirdCLEF 2020 challenge <ref type="bibr" coords="1,337.41,495.06,9.96,8.74" target="#b3">[4]</ref>, which is part of the LifeClef challenge <ref type="bibr" coords="1,178.52,507.01,9.96,8.74" target="#b2">[3]</ref>, participants have to identify 960 bird species in 5 seconds snippets of continuous audio recordings. The test data contains 153 soundscapes of a length of 10 minutes recorded at four different locations in Peru (Concesion de Ecoturismo Inka Terra), the USA (Sierra Nevada/High Sierra in California and Sapsucker Woods/Ithaca in New York), and Germany (Laubach). Each soundscape contains high quantities of (overlapping) bird vocalizations. In contrast to the BirdCLEF 2019 challenge <ref type="bibr" coords="1,270.13,578.75,9.96,8.74" target="#b4">[5]</ref>, no other than the provided training data is allowed to build the recognition system. This prohibits fine-tuning convolutional neural networks (CNNs) pretrained on other datasets, as applied in the best approaches of previous BirdClef challenges. This is reasonable, since fine-tuning a CNN pretrained for the task of image classification on the ILSVRC dataset <ref type="bibr" coords="2,134.77,130.95,15.50,8.74" target="#b10">[11]</ref> has proven to yield excellent results for many tasks, even for the task of bird recognition using spectogram images.</p><p>This restriction makes it even more important to find an optimal neural network architecture for the BirdClef 2020 challenge. For this purpose, we applied a network architecture search (NAS) approach. NAS is a current field of research. It allows to automatically learn an optimal neural network architecture for a specific problem and offers an alternative to the time-consuming task of manual architecture optimization.</p><p>The designed architecture is directly applied to the audio input using a Gabor wavelet transformation, similar to Zeghidour et al. <ref type="bibr" coords="2,369.98,239.13,15.50,8.74" target="#b12">[13]</ref> in speech recognition. This transformation is integrated into the neural network architecture as a complex 1-D convolutional layer.</p><p>The contributions of the paper are as follows:</p><p>-A novel neural architecture search approach based on a memetic algorithm is used to find an optimal neural network architecture for the task of bird sound recognition. -The proposed neural network architecture directly operates on the audio input.</p><p>The paper is organized as follows. Section 2 describes the bird recognition approach including data pre-processing, data augmentation, the construction of the neural network architecture, and details of the training process. Experimental results are presented in Section 3. Section 4 concludes the paper and outlines areas for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this section, the proposed system for bird sound recognition is presented. Section 2.1 describes the preprocessing steps. The data augmentation methods used during the training process are specified in Section 2.2. The design of the neural network architecture is explained in Section 2.3, including the Gabor wavelet layer, the NAS approach, and the composition of the neural network using multiple auxiliary heads and recurrent layers. Section 2.4 provides information about the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Pre-processing</head><p>First, the audio recordings are split into 5 seconds segments with an overlap of 0.25 seconds. Then, these snippets are classified as either bird sound or noise (i.e., no audible bird sounds) using the heuristic of the BirdCLEF 2018 baseline system <ref type="bibr" coords="2,167.37,632.21,9.96,8.74" target="#b5">[6]</ref>. The sets are called T signal (bird sounds) and T noise (noise segments). Finally, the recordings are normalized to -3 db and resampled to 22,050 Hz. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Augmentation</head><p>To avoid overfitting and to improve the generalization capabilities of the neural network model to various recording conditions, the following data augmentation methods are applied to the audio segments (see Figure <ref type="figure" coords="3,376.77,564.75,3.87,8.74" target="#fig_0">1</ref>):</p><p>-Pitch augmentation: Shifting pitch by one to three semitones.</p><p>-Mask augmentation: A randomly chosen segment of 0.5 seconds duration is masked with zeros. -Noise augmentation: White noise is added to the training snippet.</p><p>-Loudness augmentation: The volume of a training snippet is increased by a randomly chosen factor within range [0.25, 4] (0.25 leads to a decrease by factor 4, 4 leads to an increase by factor 4).</p><p>-Noise snippets augmentation: Each training snippet is a randomly weighted sum of one augmented bird snippet from T signal and four noise snippets from T noise .</p><p>First, pitch, mask, noise, and loudness augmentation are applied to the training segments of T signal . For this purpose, between one and four augmentation methods are randomly selected and applied in random order. Second, noise snippet augmentation is used. Figure <ref type="figure" coords="4,285.58,202.46,4.98,8.74" target="#fig_0">1</ref> visualizes the impact of the different data augmentation methods on the resulting audio spectograms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Neural Network Architecture</head><p>A NAS approach based on a memetic algorithm is used to find an optimal convolutional neural network architecture for the task of bird sound recognition. The overall network architecture is based on a Gabor wavelet layer directly operating on the audio input, similar to Zeghidour et al. <ref type="bibr" coords="4,342.32,304.54,15.50,8.74" target="#b12">[13]</ref> for speech recognition. The optimal cell structure found by NAS and multiple architecture heads including recurrent layers are used to take further advantage of temporal information.</p><p>Gabor Wavelet Layer. The extraction of the audio spectograms is integrated into the neural network architecture using a Gabor wavelet layer. For this purpose, a complex 1-D convolution with n filters is applied to the audio input where n is the number of frequencies. The weights of the complex kernels are created using Gabor wavelets. The complex 1-D convolution is followed by applying the logarithm and a zero centering normalization.</p><p>Furthermore, we experimented with audio spectograms generated by applying the FFT, as provided by the baseline system of the BirdClef challenge 2018 <ref type="bibr" coords="4,134.77,456.64,9.96,8.74" target="#b6">[7]</ref>. While our experiments on this dataset showed that using the Gabor wavelet layer did not lead to quality improvements compared to using audio spectograms generated by the FFT, the implemented Gabor wavelet layer has some advantages. First, data augmentation methods can be also flexibly applied to the audio input. Second, arbitrary frequencies can be extracted in contrast to the FFT with equidistant spacing. This allows us to directly use mel-scaled frequencies.</p><p>Neural Architecture Search. NAS is a recent field of research. The aim of NAS is to automatically find an optimal neural network architecture on a specific task. The NASNet architecture <ref type="bibr" coords="4,298.50,572.43,15.50,8.74" target="#b13">[14]</ref> is the first design that outperformed handcrafted, manually optimized architectures for the task of image classification. The main idea is to break down the search space and search for cells that form the building blocks for the overall network architecture. Other approaches primarily exhibit runtime improvements <ref type="bibr" coords="4,311.87,620.25,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="4,329.03,620.25,7.75,8.74" target="#b8">9,</ref><ref type="bibr" coords="4,338.43,620.25,7.75,8.74" target="#b0">1,</ref><ref type="bibr" coords="4,347.85,620.25,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="4,362.24,620.25,7.01,8.74" target="#b1">2]</ref>. While the approaches of Chen et al. <ref type="bibr" coords="4,186.58,632.21,10.52,8.74" target="#b0">[1]</ref> and Real et al. <ref type="bibr" coords="4,268.63,632.21,15.50,8.74" target="#b9">[10]</ref> use an evolutionary search method, Dong et al. <ref type="bibr" coords="4,148.82,644.16,10.52,8.74" target="#b1">[2]</ref> start with a huge, over-parameterized network which is then shrunk and locally optimized step by step. In the following, we describe an NAS approach to find an optimized architecture for bird sound recognition. The approach uses a memetic search algorithm that combines local optimization with an evolutionary algorithm. Like Zoph et al. <ref type="bibr" coords="5,148.31,243.95,14.61,8.74" target="#b13">[14]</ref>, we do not search for full network architectures but instead for relatively small structures called cells that are later stacked in a predefined manner to build the full network. NAS approaches are typically categorized according to the used search space, the search strategy, and the performance estimation/evaluation strategy, which are described in the following paragraphs.</p><p>Search Space. As previously described, we search for cell structures that are later upscaled to the full network architecture. Like the NASNet search space <ref type="bibr" coords="5,134.77,341.61,14.61,8.74" target="#b13">[14]</ref>, a cell consists of blocks and operations. While a cell in the NASNet search space consists of a fixed number of blocks and operations, our cell structure is more flexible. A cell can be composed of a variable number of blocks, and each block can contain a variable number of operations. The set of allowed operations to perform is as follows:</p><p>identity depth-wise separable convolution normal convolution max pooling average pooling Kernel sizes for the convolution and pooling operations are restricted to the following sizes: {3 × 3, 5 × 5, 7 × 7}.</p><p>The outputs of the operations within a block are merged with an add layer that forms the output of the corresponding block. Therefore, each operation contains some extra layers to satisfy shape constraints. The output of blocks that have not been used as input to any other operation within the cell are merged using either an add or a concatenate layer to form the output of the cell. The input of an operation can be either the output of one of the previous two cells or the output of a preceding block of the current or previous cell.</p><p>Performance Estimation. For performance estimation, a network architecture is constructed from the cell. First, a normal and a reduction cell are derived from the cell structure. While the normal cell retains the spatial size of its predecessor cell (i.e., every convolution or pooling operation has stride 1 × 1 and appropriate padding), the reduction cell reduces the spatial size by a factor of 2 (i.e., convolutions and pooling operations have stride 2 × 2). Second, the overall network is stacked, as shown in Figure <ref type="figure" coords="6,306.20,436.19,3.87,8.74" target="#fig_1">2</ref>.</p><p>The output head is a global average pooling layer, followed by a densely connected layer with softmax activation. The convolutions of the first reduction cell have F filters. The number of filters is then doubled after every reduction cell.</p><p>To determine the quality (or fitness) of a cell, a full network is derived from each cell with F = 8 initial filters. Then, the model is trained and evaluated on a subset of the BirdCLEF 2020 training data set. For this purpose, 300 training samples are randomly selected for each of 50 randomly chosen classes. The resulting data set with 15, 000 audio samples is split into training and validation set with a validation split of 0.1. Samples of the same audio file are either all in the training set or in the validation set. The model is trained for 20 epochs using the ADAM optimizer and a cosine learning rate scheduler. Finally, the accuracy of the model is calculated on the validation set.</p><p>Search Strategy. The idea of the search algorithm is to use an evolutionary algorithm and local optimization. This memetic search algorithm starts with a population of cells randomly drawn from the search space. The tournament selection method is used to select two cells of the current population based on their fitness values. Cells with high fitness values have a higher chance to be chosen. The two selected cells are used to create a new cell by merging all blocks. Considering the order of the blocks, all operations are joined, so that block i of the new cell contains all operations of the i-th blocks of the parents whereby the input connections of the operations are preserved. In a local optimization step, the most important operations per block are identified similar to the approach of Dong et al. <ref type="bibr" coords="7,300.67,436.38,9.96,8.74" target="#b1">[2]</ref>. For this purpose, a network is derived from the child cell, trained for two epochs and shrunk to the most important operations.</p><p>Afterwards, mutation operations are applied to the new cell, for example, changing operation types, input connections or adding and removing blocks. Finally, the cell is added to the population, and the fitness values are recomputed. This procedure is repeated for a certain number of rounds, and the cell with the best fitness value is returned. The search algorithm is described in Algorithm 1.</p><p>Multi-Head Model. In contrast to natural images, audio spectrograms contain temporal information. It seems reasonable to use this information, since the individual sounds of a bird's song may follow a certain chronological order. For this reason, we added output heads with recurrent layers to the network architecture. Altogether, the final network architecture contains two output heads, each with and without recurrent layers: one pair at the end of the network and one pair at an intermediate stage. Output heads without a recurrent layer contain a global average pooling layer followed by a densely connected layer with softmax activation, recurrent output heads contain two consecutive GRU layers with 128 units each, followed by a global average pooling layer and a densely connected layer with softmax activation. Each output head has its own loss function in the training phase, whereas for inference the output heads are merged using a concat layer followed by global average pooling. Figure <ref type="figure" coords="8,376.31,142.90,4.98,8.74" target="#fig_3">3</ref> shows the structure of the network including all output heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training Methodology</head><p>We trained our models using a weighted loss function consisting of a softmax log loss for each output head. The loss terms of the intermediate output heads are weighted by a factor of 0.6, whereas the loss terms of the output heads at the end of the network are weighted by a factor of 1. The ADAM optimizer <ref type="bibr" coords="8,459.43,240.88,10.52,8.74" target="#b7">[8]</ref> is used for the training process with a cosine learning rate scheduler. Furthermore, scheduled drop path <ref type="bibr" coords="8,230.77,264.79,15.50,8.74" target="#b13">[14]</ref> where the drop rate d r is linearly increased to 0.4 throughout the training process is used as a regularization method. Within the scheduled drop path, each operation of each cell in the network gets dropped (i.e., its output is set to zero) by the probability d r • c n if the network has n cells and if we consider an operation of the c-th cell of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we present the results of the two official and two post challenge submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Metrics</head><p>The task of the challenge is to identify all audible bird species in 5 second snippets of continuous soundscape recordings from four different locations. With the BirdClef challenge 2020 <ref type="bibr" coords="8,260.03,454.78,9.96,8.74" target="#b3">[4]</ref>, the following two metrics are used to measure the performance of a submitted run:</p><p>-Classification Mean Average Precision (cmap):</p><formula xml:id="formula_0" coords="8,264.91,508.68,101.28,25.41">cmap = C c=1 AveP (c) C</formula><p>where C is the total number of classes and AveP (c) is the average precision of class c. -Retrieval Mean Average Precision (rmap):</p><formula xml:id="formula_1" coords="8,263.51,592.33,104.08,25.41">rmap = X x=1 AveP (x) X</formula><p>where X is the total number of audio snippets of all test files and AveP (x) is the average precision of snippet x. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Sets</head><p>The training set consists of more than 70,000 recordings across 960 bird species classes contributed by the Xeno-canto community. Exactly one foreground bird species is assigned to each audio recording file. Additionally, metadata such as recording location, recording date, elevation, and recording quality is provided. The validation set contains 12 soundscape files recorded at two different locations (one in Peru, one in the USA). Each soundscape file has a duration of 10 minutes and is divided into 5 second snippets. For our evaluation, a list of audible bird species is assigned to each 5 seconds snippet of the validation data.</p><p>The test set consists of 153 soundscape files of 10 minutes duration recorded at four different locations (one in Peru, two in the USA, and one in Germany).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Two network architectures found by the NAS method described in Section 2.3 are evaluated. Altogether, we submitted five runs: two official runs with the first architecture and three post challenge runs with the second architecture.</p><p>The first network architecture was found by running the search algorithm described in Section 2.3 for 100 rounds. Figure <ref type="figure" coords="9,344.19,560.41,4.98,8.74" target="#fig_4">4</ref> shows the cell architecture of this network. The overall network was constructed as described in Section 2.3 with the number of initial filters set to F = 16 and trained for 30 epochs, as described in Section 2.4. We used 128 mel-scaled frequencies in the range of 170 Hz to 10,000 Hz. The width of the kernel of the Gabor wavelet layer was set to w = 1024, resulting in spectograms with a resolution of 256x128. The training took 65 hours on four Nvidia TITAN XP GPUs.</p><p>The trained network takes about 43 seconds to analyze a 10 minutes soundscape file. However, most of the time (≈ 40 s) is consumed by the non-optimized The two official runs differ only in the post-processing step.</p><p>Official Run 1. In the first run, the species location lists provided by the challenge organizers are used to reject impossible species from the model's predictions. This run obtains a cmap of 12.8% and a rmap of 19.3%, which is 8.6% better than the cmap of the second best submission (4.2%) and thus wins the challenge.</p><p>Official Run 2. For the second run, we used species lists per location and season, derived from the training data to remove unlikely species from the model's predictions. This led to a slight decrease in cmap and a slight increase in rmap, resulting in a cmap value of 12.7% and a rmap value of 19.8%.</p><p>After the challenge, we ran the NAS algorithm for another 50 rounds and found a new promising cell architecture. The network corresponding to the newly found cell with initial filters set to F = 16 was used to submit three further runs. Figure <ref type="figure" coords="10,165.73,494.03,4.98,8.74" target="#fig_5">5</ref> shows how two consecutive cells of the network are connected. While in the Post-Submission Runs 1 and 2 the species lists per location and season are used, Post-Submission Run 3 only applies the species lists per location in the post-processing step.</p><p>Post-Submission Run 1. The network used in this run was trained in the same way as the network of Run 1 but only for 10 instead of 30 epochs, since this seemed to be sufficient. This run obtains a cmap of 12.6% and a rmap of 20.3%.</p><p>Post-Submission Run 2. For this run, the width of the kernel of the Gabor wavelet layer was reduced from w = 1024 to w = 512, resulting in higher resolution spectrograms with 428x128 pixels. The network was initialized with the weights obtained by Post-Submission Run 1 and fine-tuned for another 5 epochs. This resulted in a small improvement of the cmap and rmap values (cmap = 12.7%, rmap = 20.6%). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>Table <ref type="table" coords="11,161.30,413.24,4.98,8.74" target="#tab_0">1</ref> shows a comparison between all the runs on the validation and test set. It is evident that even though the network used for the post-submission runs scored significantly better on the validation set, this is only partially transferred to the test set. Post-Submission Run </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The presented approach won the BirdCLEF 2020 challenge with a cmap of 12.8% and a rmap of 19.3%. In the post-challenge phase, the scores have been further improved to 13.1% cmap and 20.6% rmap. However, the task of recognizing all bird species in soundscapes is far from being solved. One possible reason could be the discrepancy between training and test data. The training data consists of sound files of various lengths. Only the most audible bird is labeled in each sound file. This means that there could be samples in the training data where multiple birds or even a bird other than the labeled one is audible. This can be misleading in the training process. However, the task for validation and testing is to identify all audible birds in a snippet. This is a much harder task than just recognizing the most audible bird in a recording. A more fine-grained multi-label annotation of the training data could significantly improve the quality of the bird sound recognition models.</p><p>There are several possibilities to improve our approach with the existing training data. There is room for improvement in the data augmentation step that has proven to be very important in previous challenges. For example, we could apply further data augmentation methods on the audio data or even apply some augmentation on the spectrograms produced by the Gabor wavelet layer. Another option is to learn the weights of the complex 1-D convolution kernel of the Gabor wavelet layer to produce better spectrograms. or to use a selfsupervised pre-training approach to learn a more suitable audio representation <ref type="bibr" coords="12,134.77,346.14,15.50,8.74" target="#b11">[12]</ref> and adapt it for birdcall identification. Furthermore, it may be beneficial to run the neural network search algorithm for a longer period of time to find a network that works even better on this task. Finally, the performance could be improved at the cost of longer training and inference runtimes by scaling up the network's capacity and using stronger regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgement</head><p>This work is funded by the Hessian State Ministry for Higher Education, Research and the Arts (HMWK) (LOEWE Natur 4.0).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,474.76,345.82,8.74;3,134.77,486.72,114.48,8.74;3,152.06,377.13,138.33,69.17"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Visualization of the impact of different audio augmentation methods on the resulting spectrogram.</figDesc><graphic coords="3,152.06,377.13,138.33,69.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,155.51,171.05,304.33,8.74"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Stacked network with reduction cells (R) and normal cells (N).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,142.24,121.70,182.08,8.77;6,149.71,135.33,217.35,7.89;6,149.71,146.29,66.09,7.89;6,165.05,157.92,155.34,7.47;6,165.05,168.23,156.81,7.86;6,165.05,179.19,185.21,7.86;6,165.05,190.15,170.40,7.86;6,149.71,202.08,16.66,7.89;6,149.71,213.71,221.25,7.47;6,149.71,224.02,79.02,7.86;6,149.71,234.95,90.44,7.89;6,165.05,245.94,118.16,7.86;6,165.05,257.54,160.05,7.47;6,165.05,267.86,139.55,7.86;6,165.05,279.46,141.22,7.47;6,165.05,289.77,148.70,7.86;6,165.05,301.38,145.93,7.47;6,165.05,311.69,85.10,7.86;6,165.05,322.65,219.59,7.86;6,165.05,333.61,247.99,7.86;6,165.05,344.57,233.18,7.86;6,165.05,355.53,79.02,7.86;6,149.71,367.46,16.66,7.89;6,149.71,378.42,194.53,7.89"><head>Algorithm 1 :</head><label>1</label><figDesc>Neural architecture search Input: initial population P , number of cells to visit r for cell in P do // transform cell to full network cell.network := cell-to-cnn(cell, F =8); cell.accuracy := train-and-eval(cell.network); cell.f lops := compute-flops(cell.network); end // compute fitness values of population members compute-fitness(P ); for round = 1 to r do parents := select-parents(P ); // perform the crossover operation child-cell := create-child(parents); // perform mutation operations child-cell := mutate-child(child-cell ); // add child cell to population P .append(child-cell ); child -cell.network := cell-to-cnn(child -cell, F =8); child -cell.accuracy := train-and-eval(child -cell.network); child -cell.f lops := compute-flops(child -cell.network); compute-fitness(P ); end return max(P , key=lambda cell : cell.f itness);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,212.57,222.38,190.22,8.74"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: General structure of the full network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,201.19,297.75,212.98,8.74"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Structure of the cell used in Runs 1 and 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,156.90,238.58,301.57,8.74"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Structure of the cell used in Post-Submission Runs 1, 2, and 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,134.77,119.68,345.83,256.67"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the submitted runs in terms of cmap and rmap on validation and test set.Post-Submission Run 3. In contrast to Post-Submission Run 1, this run uses a different sampling strategy. Instead of sampling from the preclassified bird sound snippets (T signal ), the batches are sampled from the audio files extracting a random five second snippet per file. In this case, the number of training samples per epoch equals the number of mono-species audio files. Due to the smaller number of samples per epoch, the network was trained for 100 epochs. This run obtains a cmap of 13.1% and a rmap of 19.2%, which is the best result of the challenge in terms of cmap.</figDesc><table coords="11,177.60,119.68,249.65,93.74"><row><cell></cell><cell cols="2">validation</cell><cell></cell><cell>test</cell></row><row><cell></cell><cell>cmap</cell><cell>rmap</cell><cell>cmap</cell><cell>rmap</cell></row><row><cell>Run 1</cell><cell cols="4">14.8% 21.8% 12.8% 19.3%</cell></row><row><cell>Run 2</cell><cell cols="4">14.8% 22.2% 12.7% 19.8%</cell></row><row><cell>Post-Submission Run 1</cell><cell cols="4">15.1% 24.4% 12.6% 20.3%</cell></row><row><cell>Post-Submission Run 2</cell><cell cols="4">16.2% 24.0% 12.7% 20.6%</cell></row><row><cell>Post-Submission Run 3</cell><cell cols="4">17.8% 23.4% 13.1% 19.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,134.77,449.10,345.83,152.20"><head></head><label></label><figDesc>2, for example, obtains a validation cmap of 16.2% and a validation rmap of 24.0%, but yields a test cmap of only 12.7%, which is 0.1% worse than the test cmap of Run 1, although Run 1 obtains a significantly lower validation cmap of only 14.8%. The 2.2% better validation rmap score is, on the other hand, reflected on the test set with a test rmap of 20.6% in comparison to only 19.3% of Run 1. Interestingly, Post-Submission Run 3 yielded the best cmap score both on the validation and test set without distinguishing between bird sound and noise snippets during the training phase. The extraction of snippets at random positions and the different weighting of classes in the training phase seems to be beneficial for the cmap score with 13.1% on the test data, while the rmap score decreases to 19.2%. Both network architectures used for the submissions have a similar inference time of approximately 40 snippets per second.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,505.44,337.63,7.86;12,151.52,516.40,185.76,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="12,437.69,505.44,42.90,7.86;12,151.52,516.40,157.09,7.86">Reinforced evolutionary neural architecture search</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,526.67,337.63,7.86;12,151.52,537.63,329.07,7.86;12,151.52,548.59,86.01,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,231.32,526.67,231.97,7.86">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,537.63,325.15,7.86">Proceedings of the IEEE Conference on computer vision and pattern recognition</title>
		<meeting>the IEEE Conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,558.86,337.64,7.86;12,151.52,569.82,329.07,7.86;12,151.52,580.78,329.07,7.86;12,151.52,591.74,329.07,7.86;12,151.52,602.69,329.07,7.86;12,151.52,613.65,285.06,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,404.73,580.78,75.87,7.86;12,151.52,591.74,329.07,7.86;12,151.52,602.69,90.77,7.86">Overview of lifeclef 2020: a system-oriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,266.15,602.69,214.45,7.86;12,151.52,613.65,119.03,7.86">Proceedings of CLEF 2020, CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting>CLEF 2020, CLEF: Conference and Labs of the Evaluation Forum<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">Sep. 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,623.92,337.64,7.86;12,151.52,634.88,329.07,7.86;12,151.52,645.84,329.07,7.86;12,151.52,656.80,252.37,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,211.75,634.88,268.84,7.86;12,151.52,645.84,65.83,7.86">Overview of birdclef 2020: Bird sound recognition in complex acoustic environments</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Clapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hopping</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,240.22,645.84,240.37,7.86;12,151.52,656.80,86.35,7.86">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">Sep. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>CLEF task overview 2020</note>
</biblStruct>

<biblStruct coords="13,142.96,119.67,337.64,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,329.07,7.86;13,151.52,152.55,252.16,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,167.54,130.63,308.63,7.86">Overview of BirdCLEF 2019: Large-Scale Bird Recognition in Soundscapes</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,165.36,141.59,310.42,7.86">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="13,168.93,152.55,119.44,7.86">CEUR Workshop Proceedings</title>
		<imprint>
			<publisher>CEUR</publisher>
			<date type="published" when="2019-09">Sep 2019</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,163.51,337.64,7.86;13,151.52,174.47,329.07,7.86;13,151.52,185.43,25.60,7.86" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07177</idno>
		<title level="m" coord="13,409.23,163.51,71.36,7.86;13,151.52,174.47,189.63,7.86">Recognizing birds from sound -the 2018 birdclef baseline system</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.96,196.39,337.64,7.86;13,151.52,207.34,329.07,7.86;13,151.52,218.30,25.60,7.86" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07177</idno>
		<title level="m" coord="13,409.23,196.39,71.36,7.86;13,151.52,207.34,189.63,7.86">Recognizing birds from sound -the 2018 birdclef baseline system</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.96,229.26,337.63,7.86;13,151.52,240.22,247.52,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,234.35,229.26,191.55,7.86">Adam: A method for stochastic optimization in</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,432.52,229.26,48.07,7.86;13,151.52,240.22,218.84,7.86">Proceedings of international conference on learning representations</title>
		<meeting>international conference on learning representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,251.18,337.64,7.86;13,151.52,262.14,329.07,7.86;13,151.52,273.10,307.31,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,262.06,262.14,150.08,7.86">Progressive neural architecture search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,432.52,262.14,48.07,7.86;13,151.52,273.10,231.48,7.86">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,284.06,337.98,7.86;13,151.52,294.99,187.44,7.89" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="13,332.99,284.06,147.60,7.86;13,151.52,295.02,97.99,7.86">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,305.98,337.97,7.86;13,151.52,316.93,329.07,7.86;13,151.52,327.89,329.07,7.86;13,151.52,338.83,126.11,7.89" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,415.77,316.93,64.82,7.86;13,151.52,327.89,145.50,7.86">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,305.92,327.89,174.68,7.86;13,151.52,338.85,28.80,7.86">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,349.81,337.98,7.86;13,151.52,360.77,287.42,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="13,366.80,349.81,113.79,7.86;13,151.52,360.77,121.37,7.86">wav2vec: Unsupervised pretraining for speech recognition</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05862</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.62,371.73,337.98,7.86;13,151.52,382.69,329.07,7.86;13,151.52,393.65,329.07,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,151.52,382.69,234.83,7.86">Learning filterbanks from raw speech for phone recognition</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,406.79,382.69,73.80,7.86;13,151.52,393.65,300.73,7.86">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,404.61,337.98,7.86;13,151.52,415.56,329.07,7.86;13,151.52,426.52,213.33,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,340.06,404.61,140.54,7.86;13,151.52,415.56,115.95,7.86">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,286.99,415.56,193.61,7.86;13,151.52,426.52,120.32,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
