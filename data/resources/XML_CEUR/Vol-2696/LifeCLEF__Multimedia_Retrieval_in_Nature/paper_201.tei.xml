<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.87,115.96,335.62,12.62;1,154.66,133.89,306.03,12.62;1,211.70,151.82,191.96,12.62">Combination of image and location information for snake species identification using object detection and EfficientNets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,146.56,212.67,55.95,8.74"><forename type="first">Louise</forename><surname>Bloch</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Str. 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<settlement>Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.64,212.67,67.70,8.74"><forename type="first">Adrian</forename><surname>Boketta</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Str. 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,145.80,224.63,82.86,8.74"><forename type="first">Christopher</forename><surname>Keibel</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Str. 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.44,224.63,48.66,8.74"><forename type="first">Eric</forename><surname>Mense</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Str. 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,150.66,236.58,98.30,8.74"><forename type="first">Alex</forename><surname>Michailutschenko</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Str. 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.51,236.58,61.02,8.74"><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Str. 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Diagnostic and Interventional Radiology and Neuroradiology</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<settlement>Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,424.65,236.58,40.04,8.74;1,164.06,248.54,34.20,8.74"><forename type="first">Johannes</forename><surname>Rückert</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Str. 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,296.05,248.54,66.84,8.74"><forename type="first">Leon</forename><surname>Willemeit</surname></persName>
							<email>leon.willemeit002@stud.fh-dortmund.de</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Str. 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,392.82,248.54,52.52,8.74;1,239.33,260.49,38.66,8.74"><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
							<email>christoph.friedrich@fh-dortmund.de</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Str. 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<settlement>Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">FHDO Biomedical Computer Science Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.87,115.96,335.62,12.62;1,154.66,133.89,306.03,12.62;1,211.70,151.82,191.96,12.62">Combination of image and location information for snake species identification using object detection and EfficientNets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D92E008CDD1E9670084BD1DDBA2A9293</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>snake species identification</term>
					<term>object detection</term>
					<term>EfficientNets</term>
					<term>image classification</term>
					<term>metadata inclusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Snake species identification based on images is important to quickly treat patients suffering from snake bites using the correct antivenom. The SnakeCLEF 2020 challenge, which is part of the LifeCLEF research platform, is focused on this task and provides snake images and associated location information. This paper describes the participation of the FHDO Biomedical Computer Science Group (BCSG) in this challenge. The implemented machine learning workflow uses Mask Region-based Convolutional Neural Network (Mask R-CNN) for object detection, various image pre-processing steps, EfficientNets for classification as well as different methods to fuse image and location information. The best model submitted before the challenge deadline achieved a macro-averaging F1-score of 0.404. After the expiration of this deadline, the results could be improved up to a macro-averaging F1-score of 0.594.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper explains the participation of University of Applied Sciences and Arts Dortmund (FHDO) Biomedical Computer Science Group (BCSG) at the Conference and Labs of the Evaluation Forum (CLEF) 2020 <ref type="foot" coords="2,374.58,167.42,3.97,6.12" target="#foot_0">4</ref> SnakeCLEF challenge<ref type="foot" coords="2,476.12,167.42,3.97,6.12" target="#foot_1">5</ref> for snake species identification <ref type="bibr" coords="2,269.12,180.95,14.61,8.74" target="#b19">[20]</ref>. This challenge is part of the LifeCLEF 2020 research platform which focuses on the automated identification of species <ref type="bibr" coords="2,465.09,192.90,15.50,8.74" target="#b13">[14]</ref> and consists of four challenges. The implemented approach in this paper is inspired by an article <ref type="bibr" coords="2,219.59,216.81,10.52,8.74" target="#b8">[9]</ref> about the winning entry of round 2 of the AICrowd Snake Species Identification Challenge <ref type="foot" coords="2,272.91,227.20,3.97,6.12" target="#foot_2">6</ref> .</p><p>The identification of snake species is important as there are approximately between 81,410 and 137,880 victims of snakebites dying every year <ref type="bibr" coords="2,433.05,253.12,14.61,8.74" target="#b28">[29]</ref>. These deaths result from inaccurate knowledge about the species and consequently about the antivenom needed <ref type="bibr" coords="2,262.06,277.03,9.96,8.74">[5]</ref>.</p><p>The high diversity of snake species <ref type="bibr" coords="2,298.03,289.42,15.50,8.74" target="#b26">[27]</ref> and their partially similar appearances lead to confusion <ref type="bibr" coords="2,209.39,301.37,10.52,8.74">[5]</ref> and make this choice more complicated. It is also mentioned, that an increasing amount of people who were bitten by a snake bring pictures of the snake, for example, taken with a smartphone, or the killed snake itself to the physician <ref type="bibr" coords="2,195.71,337.24,9.96,8.74">[5]</ref>.</p><p>Therefore, the target of the SnakeCLEF challenge is the improved and robust identification of snake species based on photographs <ref type="bibr" coords="2,365.04,361.58,14.61,8.74" target="#b19">[20]</ref>.</p><p>In this article, the experiments and results of FHDO BCSG are presented. For this reason, Section 2 describes previous work in this field of research. Afterwards, the general machine learning workflow is illustrated in Section 4, followed by a description of the achieved results in Section 5. Finally, the results are summarized in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Automated identification of snake species using machine learning is rarely studied, resulting from small datasets of annotated images.</p><p>James et al. <ref type="bibr" coords="2,208.69,506.29,15.50,8.74" target="#b12">[13]</ref> described a semiautomatic approach, where taxonomical features have been extracted from images to discriminate six different species. The dataset contained 1,299 images and the least frequent class included 88 images. Using different feature selection approaches, it has been concluded that the bottom-view taxonomical features are less important for the species identification than the front-and side-view features.</p><p>As manual extraction of features describing the appearance of a snake is tedious, recent articles used automated feature extraction, for example, texture features <ref type="bibr" coords="2,172.20,602.37,10.52,8.74" target="#b3">[4]</ref> or deep learning <ref type="bibr" coords="2,259.72,602.37,11.32,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,271.04,602.37,7.55,8.74" target="#b2">3,</ref><ref type="bibr" coords="2,278.58,602.37,7.55,8.74" target="#b8">9,</ref><ref type="bibr" coords="2,286.13,602.37,11.32,8.74" target="#b17">18]</ref>.</p><p>Texture features were used in Amir et al. <ref type="bibr" coords="3,335.83,118.99,10.52,8.74" target="#b3">[4]</ref> to distinguish between 22 different species. Their dataset contained 349 images and the least frequent snake species included three images. Using classical machine learning methods, the approach achieved a classification accuracy of 87 %.</p><p>Patel et al. <ref type="bibr" coords="3,203.78,166.81,15.50,8.74" target="#b17">[18]</ref> used deep learning methods to develop an application for smartphones which distinguishes images of nine different snake species, occurring on the Galápagos Islands in Ecuador. To this end, object detection, as well as classification algorithms, have been used. The training dataset for their implementation has been a bundle of three data sources, two internet searches of the platforms Google and Flickr were combined with an image dataset provided by the Ecuadorian institution Tropical Herping <ref type="foot" coords="3,324.02,236.97,3.97,6.12" target="#foot_3">7</ref> . In total, 250 images were collected and the least frequent class contained seven images. Different model architectures have been tested for object detection and image classification. The model which was based on Faster Region-based Convolutional Neural Network (Faster R-CNN) <ref type="bibr" coords="3,174.68,286.37,15.50,8.74" target="#b22">[23]</ref> ResNet <ref type="bibr" coords="3,228.14,286.37,15.50,8.74" target="#b10">[11]</ref> achieved the best classification accuracy of 75 %. The authors state that a larger amount of training samples would be important for further investigations in this field.</p><p>Abdurrazaq et al. <ref type="bibr" coords="3,235.63,322.23,10.52,8.74" target="#b1">[2]</ref> used three different Convolutional Neural Network (CNN) architectures to distinguish five different snake species. They used a dataset containing 415 images. For the least frequent snake species, 72 images were available. The best results were achieved using a medium-sized classification network.</p><p>Abeysinghe et al. <ref type="bibr" coords="3,228.47,382.01,10.52,8.74" target="#b2">[3]</ref> used a deep Siamese network <ref type="bibr" coords="3,371.91,382.01,10.52,8.74" target="#b5">[6]</ref> to classify a relatively small dataset containing 200 images of 84 species based on World Health Organization (WHO) venomous snake database<ref type="foot" coords="3,330.67,404.34,3.97,6.12" target="#foot_4">8</ref> . The approach described in their article concentrated on single-shot learning as the dataset included 3 to 16 images per species. The achieved results of the automated classification model performed worse than human classification accuracy. Pairwise classification results exceed class prediction accuracy.</p><p>As already mentioned, Gokula Krishnan <ref type="bibr" coords="3,328.70,465.69,10.52,8.74" target="#b8">[9]</ref> described the results of round 2 of the AICrowd Snake Species Identification Challenge. The solution which achieved the best results has used object detection as a pre-processing step to focus on the image parts containing the snake. On this basis, EfficientNets were applied afterwards for image classification. In round 2 the dataset included 187,720 images assigned to 85 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>The training dataset used in the actual SnakeCLEF and AICrowd Snake Species Identification Challenge round 4 consists of 245,185 red-green-blue-(RGB-) color-space-images (models trained on the training dataset were referred to as T1) assigned to 783 different snake species. Additionally, a validation dataset is available, which includes another 14,029 images (models trained on the training and validation dataset were referred to as T2). The class distribution of the snake species is highly unbalanced as can be seen in the bar plot of the absolute class frequencies depicted in Figure <ref type="figure" coords="4,292.04,154.86,3.87,8.74" target="#fig_0">1</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Dataset</head><p>An analysis of the dataset with AntiDupl 9 revealed 1,713 duplicate images in the training set. Some of these duplicates are associated with different species like "Image not found" images, that are the result of download problems. Other duplicates are correctly associated with several species as they depict distinct snakes. When the mean squared difference between images in AntiDupl is relaxed to 2 %, another 2,114 duplicates can be found. These are the result of different jpeg compression rates for the same image, resize operations or deletion of copyright information. Another problem that has been found are out-of-class images, that have been injected by the organizers. These images contain no snakes but for example, ice-hockey players, churches, other animals, persons, and mangas.</p><p>To identify them for exclusion from the training set, a standard ImageNet <ref type="bibr" coords="5,470.08,190.72,10.52,8.74" target="#b7">[8]</ref> classifier with 1,000 classes and based on a ResNet50 <ref type="bibr" coords="5,368.80,202.68,15.50,8.74" target="#b10">[11]</ref> architecture has been used and a positive list of snake and reptile classes, that are part of the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSRVC2012) <ref type="bibr" coords="5,411.86,226.59,15.50,8.74" target="#b24">[25]</ref> dataset has been used. With this classifier, about 4,000 out-of-class images have been identified and the effects of the reduced dataset (abbreviated as D1 hereafter) has been tested and compared to the unfiltered dataset. The results of this comparison are summarized in Table <ref type="table" coords="5,245.85,274.41,3.87,8.74" target="#tab_7">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Metadata</head><p>The images are associated with metadata that provides information about the continent and country of the place where the image has been taken. For some snake depictions, the information is not given and only "UNKNOWN" is provided in the metadata. This information could be used for better classification.</p><p>It should be noted, that the number of snake species in the dataset does not match the natural occurrence of a snake in a location. For example, the most frequent species with German country information in the dataset is pantherophis guttatus, the corn snake which is not endemic in Germany but is the pet snake number one in Germany. Accordingly, the data set takes into account that pet snakes can also attack humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>This section describes the workflow used to learn a discrimination between the different snake species. The generalized workflow is depicted in Figure <ref type="figure" coords="5,451.58,512.03,3.87,8.74" target="#fig_2">2</ref>. The workflow is modular and in the course of the challenge, it was examined how different implementations of the individual modules affect the classification performance on the test dataset. In this section, the components are described more precisely and different implementations of them are demonstrated. The workflow has been implemented using the programming language Python 3.6.9 <ref type="bibr" coords="5,445.18,571.81,15.50,8.74" target="#b27">[28]</ref> and was based on Keras 2.2.4-tf <ref type="bibr" coords="5,254.66,583.76,10.52,8.74" target="#b6">[7]</ref> with a Tensorflow 2.1.0 <ref type="bibr" coords="5,370.02,583.76,10.52,8.74" target="#b0">[1]</ref> backend. For the inference on the AICrowd submission platform, Tensorflow 2.0.0 was used for reasons of compatibility. Image pre-processing included an optional object detection stage and a mandatory stage, where rectangular images were transferred to a square shape afterwards. Additionally, the images were augmented, optionally branded using locational information and fed into the deep learning training network. Finally, an optional multiplication of the prediction probabilities and the a priori probability distribution of the snake species occuring at the corresponding location has been implemented.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Object Detection</head><p>The idea of using an object detection stage before executing an image classification was inspired by the winning team <ref type="bibr" coords="6,318.23,464.84,10.52,8.74" target="#b8">[9]</ref> of round 2 of the AICrowd Snake Species Identification Challenge. Object detection has been implemented using the Mask R-CNN procedure, first described by He et al. <ref type="bibr" coords="6,379.74,488.75,14.61,8.74" target="#b9">[10]</ref>. Mask R-CNN performs instance segmentation as it extracts a bounding box, a class label and a pixel-wise segmentation mask for each object detected in an image. The Mask R-CNN algorithm is organized using two different stages. In the first stage, a backbone CNN extracts a feature map from the original image. In this paper, Resnet-50 has been used as a backbone. Afterwards but also in the first stage, a Region Proposal Network (RPN) is used to identify candidate object regions. So-called anchor boxes are used in this step to predefine bounding boxes. The second stage consists of a Region of Interest (ROI) align network which extracts multiple possible ROI sections. Based on these sections, a fully connected layer network is trained to perform a parallel softmax classification for class identification (snake vs. background in this case) and a regression task to specify bounding boxes. Additionally, a CNN-based mask classifier is employed for pixel-wise segmentation. In this article, the backbone model weights were initialized by the model weights trained on the ImageNet <ref type="bibr" coords="6,313.32,656.12,10.52,8.74" target="#b7">[8]</ref> dataset. The training on the snake dataset has been implemented in two different phases. First all layers except the layers which are included in the backbone were trained using 20 epochs to warm up the model and afterwards 30 epochs were performed to train the entire model. The implementation of the Mask R-CNN used in this article is an adaption<ref type="foot" coords="7,472.15,153.28,7.94,6.12" target="#foot_6">10</ref> of the implementation of Abdulla<ref type="foot" coords="7,279.31,165.24,7.94,6.12" target="#foot_7">11</ref> transferred to use Tensorflow 2.1.0. No data augmentation has been used for object detection. The threshold of minimum detection confidence has been set to 0.3. Stochastic gradient descent (SDG) was used as an optimizer to train the model, momentum was set to 0.9. Further parameters include a weight decay, which was set to 0.0001 and a batch size of 8 was used.</p><p>In order to train the object detection model, the annotated snake images available from the winning solution of round 2 of the AICrowd Snake Species Identification Challenge <ref type="bibr" coords="7,240.28,262.46,10.52,8.74" target="#b8">[9]</ref> (O1 in Section 5) were used initially. Later, 400 additionally annotations were added to this dataset (O2 in Section 5) to investigate whether the object detection and thus the classification performance can be improved. The object detection results can be found in Table <ref type="table" coords="7,386.43,298.32,3.87,8.74" target="#tab_3">3</ref>. Since Mask R-CNN is used in this approach only for object detection and not for instance segmentation, it may be an adequate solution to use Faster R-CNN instead of the Mask R-CNN. However, the results of the TensorFlow Object Detection application programming interface (API) <ref type="bibr" coords="7,267.57,346.14,14.61,8.74" target="#b11">[12]</ref>, which represents a guide to choose an adequate object detection architecture shows an increased mean average precision (mAP) of 39.0 for the Microsoft Common Objects in Context (COCO) dataset <ref type="bibr" coords="7,134.77,382.01,15.50,8.74" target="#b16">[17]</ref> for Mask R-CNN object detection in comparison to Faster RCNN, which achieves a mAP of 38.7 <ref type="foot" coords="7,236.72,392.39,7.94,6.12" target="#foot_8">12</ref> . The use of the Mask R-CNN object detection makes it easier to supplement segmentation data prospectively, which was not used during this challenge due to a lack of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image Pre-processing</head><p>As most of the deep learning classification models expect input images of square shape and predefined dimensions, it has been important to transform the mostly rectangular images, or image parts detected by the object detection into square shape and adjust the image for the expected image dimension of the classification model. There are different possibilities for the extraction of quadratic from rectangular images. The methods used in this paper are described in this section and the implemented combinations of this methods are summarized in Table <ref type="table" coords="7,134.77,544.25,3.87,8.74" target="#tab_1">1</ref>. The results of the experiments achieved using different image pre-processing methods are summarized in Table <ref type="table" coords="7,286.03,556.21,3.87,8.74" target="#tab_4">4</ref>.</p><p>Resize The least complex possibility has been to rescale images without consideration of aspect ratio. This resulted in highly distorted images so the texture and the shape of the snake have been disturbed especially for images with strongly different image dimensions. In this paper, two rescaling procedures, one considering and one retaining the aspect ratio were compared to each other. In the latter case, images had to be padded with further information to transfer them to a square shape.</p><p>Scaling Another problem which occurs during pre-processing is the problem of upscaling. Upscaling small images lead to poor image quality. It has been suspected that this could bring difficulties in texture recognition. In this paper, approaches which did and did not use upscaling for image pre-processing were compared to each other. If upscaling has been avoided, approaches were needed to pad pixel information for the remaining image sections.</p><p>Fill boundaries As previously mentioned, there were some different cases, where padding was required to get input images with preset image dimensions. One strategy to solve this issue has been to pad the image by a monochrome color. Koitka and Friedrich <ref type="bibr" coords="8,254.61,320.15,15.50,8.74" target="#b15">[16]</ref> recommended padding with a color matching to the image instead of using a predefined color (usually black or white). Since black is usually the most frequently occurring color in shady images, this approach used the average color of the original image or rather of the cropped areas as an alternative to pad the image.</p><p>In combination with object detection, it has been possible to increase the ROI and thus pad the image using background information instead of monochrome color. In this case, the image section predefined by the object detection workflow has been expanded as long as a quadratic section is found or one of the dimensions of the original image were smaller than the expected dimension of the square. If this happened, the average color of the image has been used to pad protruding boundaries. It has been attempted to include background evenly on all sides to center the snake. Sometimes this was not possible, for example, if the snake was located in a corner of the original image. In this case, the ROI has been moved to include background information of the remaining directions, thus the snake has not been centered in the image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data Augmentation</head><p>Data augmentation has been used to expand the training images and avoid overfitting. In each epoch of the training process, the images were randomly transformed. These transformations included random cropping of approximately 10 % of the image pixels per dimension, a rotation in the range of ±40 • , a widthshift, height-shift, random shearing, zooming each with a factor of 0.2, as well as the possibility of horizontal flipping. If pixel positions were generated during this procedure, for which no image information has been available, those were filled using the value of the nearest available image position. During the challenge, the workflow has been adapted to speed up the image classification procedure. In the later version of the workflow those pixels used black as a monochrome color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Image Classification</head><p>EfficientNets As also used by Gokula Krishnan <ref type="bibr" coords="9,364.07,297.39,9.96,8.74" target="#b8">[9]</ref>, EfficientNets, first described in Tan and Le <ref type="bibr" coords="9,238.61,309.35,14.61,8.74" target="#b25">[26]</ref>, were used for classification in this approach. The baseline EfficientNet-B0 architecture is generated using an architecture search that parallely optimizes accuracy on a predefined classification task and Floating Point Operations Per Second (FLOPS) <ref type="bibr" coords="9,305.31,345.21,14.61,8.74" target="#b25">[26]</ref>. Based on this baseline model, larger models of the same family are created by scaling the depth, height and resolution of the baseline model uniformly. The different models of this family achieve state-of-the-art classification accuracy on ImageNet <ref type="bibr" coords="9,362.60,381.08,9.96,8.74" target="#b7">[8]</ref>. Additionally, the architecture is smaller and faster on inference compared to other existing CNNs <ref type="bibr" coords="9,462.33,393.03,14.61,8.74" target="#b25">[26]</ref>.</p><p>EfficientNets were successfully adapted to different machine learning problems using transfer learning <ref type="bibr" coords="9,235.97,416.94,14.61,8.74" target="#b25">[26]</ref>.</p><p>Various models of the EfficientNets family were used in this competition from EfficientNet-B0 up to EfficientNet-B4 networks (B0 -B4 in Section 5). The results of using different models of the EfficientNets family can be found in Table <ref type="table" coords="9,176.32,464.84,3.87,8.74" target="#tab_6">6</ref>. The model weights were initialized by a model pre-trained using noisy student <ref type="bibr" coords="9,197.19,476.79,14.61,8.74" target="#b29">[30]</ref>. The EfficientNets were extended by a flatten layer, a dense layer with 1000 neurons and Swish <ref type="bibr" coords="9,293.93,488.75,15.49,8.74" target="#b21">[22]</ref> as an activation function and a dense layer with 783 neurons, which corresponds to the number of snake species and softmax activation were added to the previous architecture. The described model was trained for a few epochs on the snake classification task to warm-up the network. In this phase only the newly added layers and the batch normalization layers have been trained. Afterwards all layers were trained for a larger number of epochs (N10+50 denotes a warm-up phase including ten epochs and 50 epochs are used to train the entire model). Different batch sizes were used as further parameters to train the model (32 is encoded as BS32, 64 as BS64 etc., BS64/32 means that a batch size of 64 has been used during the warm-up phase and a batch size of 32 has been used afterwards). The chosen batch size depended on the image size (e.g., an image size of 128×128 is encoded as S128 in Section 5) the classification model and the available graphics processing unit (GPU) memory. The results of models using different image sizes can be found in Table <ref type="table" coords="9,452.03,644.16,3.87,8.74" target="#tab_5">5</ref>. The learning rate (α) was likewise adjusted depending on the batch size (LR1 encodes a learning rate of 10 -4 during the warm-up phase and 10 -5 during fine-tuning and LR2 encodes a learning rate of 10 -5 during the warm-up phase and 10 -6 during fine-tuning in Section 5). All submissions described in this paper used the Adam optimizer (β 1 = 0.9, β 2 = 0.999, = 10 -7 ) <ref type="bibr" coords="10,359.07,154.86,15.50,8.74" target="#b14">[15]</ref> to minimize categorical cross entropy. The implementation of the classification model workflow used an EfficientNets 1.1.0 implementation of Tensorflow Keras 2.2.4 <ref type="bibr" coords="10,401.91,178.77,9.96,8.74" target="#b6">[7]</ref>.</p><p>Since the dataset of the challenge had very unbalanced class frequencies, different class weight functions were used in order to implement an oversampling. Equation 1 describes a linear class weight function (W1 in Section 5) and Equation 2 describes a function where very low frequencies were less oversampled (W2 in Section 5). For both equations, F (c) denotes the frequency of class c.</p><p>For comparability reasons, one model has been trained without class weights.</p><formula xml:id="formula_0" coords="10,267.98,270.92,212.61,22.31">w 1 (c) = max F (c) F (c)<label>(1)</label></formula><formula xml:id="formula_1" coords="10,244.81,302.36,235.78,29.49">w 2 (c) = 1 - 1 max F (c) F (c) + 0.5<label>(2)</label></formula><p>Polyak Averaging Polyak averaging, based on the approach of Polyak <ref type="bibr" coords="10,446.49,351.85,15.50,8.74" target="#b20">[21]</ref> and Ruppert <ref type="bibr" coords="10,175.45,363.81,14.61,8.74" target="#b23">[24]</ref>, is a method to combine the learned weights of different epochs during the model training in order to obtain a final model with more robust weights. In this paper, it has been tested if Polyak averaging leads to improved classification results (P1 denotes the described Polyak averaging in Section 5). Therefore the learned weights of the last five epochs were averaged using an exponential function described in Equation <ref type="formula" coords="10,328.61,423.59,3.87,8.74">3</ref>, where i has a value of 1 for the last epoch, 2 for the penultimate epoch and 5 for the fifth last epoch.</p><formula xml:id="formula_2" coords="10,258.34,455.06,222.25,22.31">W polyak (i) = exp -i 2 (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Addition Of Location Information</head><p>Optionally, location information was added to some models by multiplying the prediction probabilities of the classification model by the a priori probability of the snake class for the specified location (M1 denotes the multiplication of the locational distribution). The a priori probabilities were estimated by the relative frequency distribution of the snake species at the location in the training and validation dataset. Usually, the country information was used in this step, only if this information was missing, the distribution of the continent has been used instead. For some images, both country and continent information were missing. In those cases, the frequency distribution of the entire dataset has been used. The softmax function was applied after this multiplication, to normalize the results.</p><p>Another variant has been implemented based on the previously described procedure. The sole exception has been that the raw prediction probabilities of images with missing country and continent information were not multiplied (abbreviated as M2 in Section 5). As a second variation of this method, all prediction probabilities were multiplied by a binary variant of the frequency distribution, which thus denotes if a snake was or was not present at a location (M3 in Section 5). The results achieved using the different metadata integration strategies are summarizes in Table <ref type="table" coords="11,288.66,178.77,3.87,8.74" target="#tab_8">8</ref>.</p><p>During the experiments of the FHDO BCSG a few alternatives have been investigated. These methods were only tested in small experiments and are not described in this paper for reasons of clarity.</p><p>Image Branding As an alternative to the simple multiplication of the location distributions, an approach has been implemented, which directly adds the location information into the classification network. This has been done using a binary image branding technique introduced in Pelka et al. <ref type="bibr" coords="11,390.32,281.00,14.61,8.74" target="#b18">[19]</ref>, which adds grey (RGB = [102,102,102]) boxes encoding the location information directly to the images. The height of the boxes was set to 8 pixels while the width (b w ) depends on the image dimensions d and is described in Equation <ref type="formula" coords="11,382.21,316.87,3.87,8.74" target="#formula_3">4</ref>.</p><formula xml:id="formula_3" coords="11,277.50,337.67,203.10,22.31">b w = d 8 -4<label>(4)</label></formula><p>The first box starts directly at the left border of the image and after every box, space was left for 4 pixels. The continent information has been added as binary boxes at the top border of the image, while the country information has been added at the bottom border of the image. Since a distinction has been made between seven continents as well as the "unknown"-class, every box at the top of the image represents a continent (abbreviated as M4). A similar approach to encode the country information would result in small boxes because 189 countries had to be distinguished. The used image branding approach is illustrated in Figure <ref type="figure" coords="11,375.41,467.70,3.87,8.74" target="#fig_3">3</ref>. In this case, a binary encoding of the country index has been chosen, so that eight boxes could be used to represent 2 8 = 256 different countries. Hereafter, the combined branding of continent and country information is abbreviated as M5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In this section, the classification results for the test dataset of the challenge are described. Table <ref type="table" coords="11,210.07,572.33,4.98,8.74" target="#tab_2">2</ref> summarizes the most relevant successful submissions of the FHDO BCSG for the SnakeCLEF challenge.</p><p>This table is mainly used to give an overview about the submitted models. In order to get a better insight into the partial results and the effects of the different methods used, partial aspects are considered in individual tables in the further course of this section. It was possible to submit models to the AICrowd Snake Species Identification Challenge after the submission deadline of Snake-CLEF expired. Therefore Table <ref type="table" coords="11,278.62,656.12,4.98,8.74" target="#tab_2">2</ref> presents a few models which achieve better results than the best submission in the SnakeCLEF challenge. In order to avoid miscommunication, the submissions in Table <ref type="table" coords="12,333.60,328.11,4.98,8.74" target="#tab_2">2</ref> are listed in chronological order and the deadline of the challenge is highlighted.</p><p>The results of the different object detection datasets are summarized in Table <ref type="table" coords="12,134.77,365.02,3.87,8.74" target="#tab_3">3</ref>. This table only presents the parameters, which are necessary for this comparison. It should be noted, that all the other parameters of the compared models are identical, as can be verified in Table <ref type="table" coords="12,309.33,388.93,3.87,8.74" target="#tab_2">2</ref>. This type of presentation is also used in subsequent tables.</p><p>The comparison of submissions 68418 and 68450, as reflected in Table <ref type="table" coords="12,475.61,413.88,4.98,8.74" target="#tab_3">3</ref> shows, that the macro-averaging F 1 -score (abbreviated as F 1 hereafter) increased by 0.010 when additional images were annotated, whereas log loss remains stable. Moreover, the number of images where no snakes were identified decreases from 141 to 123 in the joined training and validation dataset.</p><p>Additionally, submission 68678 is a model, which was trained using no object detection. This model is not completely comparable to any other models, but submission 68632 differs only in the image pre-processing step. Comparing those two models, shows a slightly better performance of the model which used the object detection. As previously mentioned, this comparison is not entirely fair.</p><p>Table <ref type="table" coords="12,178.49,535.52,4.98,8.74" target="#tab_4">4</ref> summarizes the results of models trained based on different preprocessing methods. As can be seen, pre-processing influenced the classification results achieved for the test dataset. Remarkable was the good performance of the submission 68506 which used image resizing without consideration of the aspect ratio. This model achieved the best F 1 of 0.452. It has been expected, that this image pre-processing would achieve bad results as major distortions were possible, so in some cases humans were not able to recognize snakes in those images.</p><p>The second-best result was achieved for submission 67962. In this submission, the ROIs detected during object detection have been expanded and thus were padded using background information. The submission reached an F 1 of 0.403 No aspect ratio, up-scaling, no padding, I2: Aspect ratio, up-scaling, monochrome padding, I3: Aspect ratio, no up-scaling, monochrome padding, I4: Aspect ratio, no up-scaling, background padding, Sx: Image size: x×x pixels, Bx: EfficientNet-Bx, BSx: Batch size of x for image classification, BSx/y: Batch size: warm-up-phase: x, finetuning: y, W1: Linear weights, W2: Nonlinear weights, LR1: Learning rate warm-up phase: 10 -4 , fine-tuning: 10 -5 , LR2: Warm-up phase: 10 -5 , fine-tuning: 10  and thus outperformed the F 1 of submission 68432, which used a monochrome color padding strategy, by 0.034. The comparison between submission 68432 and submission 67727 shows a slightly positive effect of using upscaling, as the F 1 of submission 67727 is 0.020 higher than the F 1 of submission 68432. The previously described comparison is based on small images containing 128×128 pixels, for future investigations, it would be interesting how the pre-processing methods affect larger images. Table <ref type="table" coords="14,177.17,560.48,4.98,8.74" target="#tab_5">5</ref> summarizes the official classification results achieved using different image sizes as model input. The results of the comparison corresponds to other experiments executed during the challenge and shows that models trained on larger image input sizes achieved better classification results. Increasing the image size from 128×128 to 196×196 boosted the F 1 by approximately 0.080. The used image sizes may look striking, because EfficientNet-B0 models are usually trained using images including 224×224 pixels and EfficientNet-B4 models are optimized for an image size of 380×380 pixels. The use of small images in this approach resulted from the fact that some early submissions failed because of memory issues. The problem has been fixed after the deadline of the SnakeCLEF challenge expired. Some of the later submissions used larger image sizes consistent to the original EfficientNets input sizes and thus achieved better results. Next, the influence of different model architectures on the classification results were investigated. In Table <ref type="table" coords="15,273.95,334.83,3.87,8.74" target="#tab_6">6</ref>, a comparison is presented concerning different model architectures. The comparison shows, concurrently to some experiments not listed here for reasons of clarity, increased F 1 for upscaled models. Submission 67727, which was based on an EfficientNet-B2 architecture outperformed submission 67700 by an increase of the macro averaging F 1 -score of 0.037. Submission 68541, which represents an EfficientNet-B4 architecture, achieved an F 1 of 0.426 and thus outperformed the results of submissions 67727 and 67700 by 0.037 and 0.074. It should be noted that all of the submissions summarized in Table <ref type="table" coords="15,161.74,430.47,4.98,8.74" target="#tab_6">6</ref> were trained using an image size of 128×128 pixels which is due to some memory issues already mentioned before. Some additional experiments were performed comparing different top layer architectures, for lack of time those were not completely comparable to each other, especially because the number of epochs used for training differed for most of the models. For this reason these results are not elaborated in this paper. It has been mentioned in Section 4 that different weight functions can be used to overcome unbalanced class distributions. The results of submissions 67727, 67882 and 67901 show, that the function introduced in Equation <ref type="formula" coords="16,425.53,142.90,3.87,8.74" target="#formula_1">2</ref>, which has been used in submission 67727 achieved a macro averaging F 1 -score of 0.389 and thus outperformed submission 67882, which used a linear class weight function and achieved an F 1 of 0.365 and submission 67901, which used no class weights and achieved a macro averaging F 1 -score of 0.377. One possible reason for the poor results of the linear weighting could be the high differences in class frequencies, which lead to larger weights for rare classes.</p><p>The results of the dataset filtering strategies, which are presented in Table 7, have been inconclusive. For the workflow used in submissions 67675 and 67700, the model trained on the reduced dataset performed worse than the model trained on the complete dataset. The opposite behaviour has been observed for the workflow used in submissions 67696 and 67727, which achieved F 1 of 0.392 and 0.389. As the filtering removed images from the training dataset, where no snakes were present and no clear benefit is reached using this filtering, one could assume, that there might be some images in the test dataset where no snakes are present. As can be noted in Table <ref type="table" coords="16,266.17,512.16,3.87,8.74" target="#tab_2">2</ref>, most of the earlier submissions, achieved high log losses of about 6.6, while others had log losses of about 1 with a confusing dependency to the achieved macro averaging F 1 -scores. This problem appeared because of softened prediction results if softmax normalization is used after the multiplication of the location frequencies and has been fixed using maximumnormalization instead (e.g., submissions 68520, 68574, 68575 and 68655).</p><p>The results of adding metadata to improve image classification are described in Table <ref type="table" coords="16,176.13,596.34,3.87,8.74" target="#tab_8">8</ref>. It can be seen, that adding metadata to a model by multiplying the model results using the a priori probability of the snake class for the given location lead to an increased F 1 . Submission 68574, which was trained using the same workflow as submission 68520, except adding metadata, outperforms this model by an increase of F 1 of 0.109. As can be seen looking at submission 68575 those results can be further improved by a value of 0.016, if the multiplication is only used for available country and continent information. Submission 68655 exhibited a similar result of 0.445 using binary information about the availability of a species in a country or continent.</p><p>The results of the submissions 69365, 69750, 69768 and 69849 show that models which used the image branding presented in Section 4, achieved no benefit in comparison to multiplying the raw predictions by the location information. It can be noted, that submission 69750, which combined country and continent branding achieved a poor F 1 of 0.361, whereas the model, which used only continent branding (submission 69768), achieved a better F 1 of 0.437. One possible reason for this might be the use of the complex positional encoding of the country information which is hard to learn for a CNN, which focuses more on local differences. Because of the limited time, it was not possible to investigate this problem more thoroughly, thus additional investigations should be under examination in future work. The combination of continent branding and multiplication of the a priori probability distribution in submission 69849 achieved similar results than the model, which used no branding, but the multiplication. The best model submitted before the SnakeCLEF deadline expired was submission 68023 which was based on an EfficientNet-B4 model architecture and achieved an F 1 of 0.404. Due to the previously mentioned memory issues, this model used an unusual small image size of 128×128 pixels. The newly added layers of the described model were trained with a warm-up phase of ten epochs and another 50 epochs were used to fine-tune the entire model. During the training process a batch size of 64, a learning rate of 0.0001 and Adam optimizer have been used. Location information was added using the described multiplication procedure. Polyak averaging with exponential weights has been used to combine the results of the last five training epochs. The Polyak averaging achieved an improvement of F 1 of 0.001 in comparison to submission 67734 which used no Polyak averaging.</p><p>The best submission after the SnakeCLEF deadline expired was submission 69888 which achieved a macro-averaging F 1 -score of 0.594 and a log loss of 1.064. The main differences in comparison to the best model before the deadline expired were, that the model was trained using the predefined image dimensions of an EfficientNet-B4, which are 380×380 pixels. Due to the increased image size, a smaller batch size of 13 and a decreased learning rate has been used. Furthermore, 109 instead of 50 training epochs have been applied, and the location distribution was multiplied only for known countries and continents. The model included no Polyak averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In conclusion, it can be stated that snake species identification is a challenging task, primarily because of the high diversity of snake species, high intra-class variance, and low inter-class variance.</p><p>The main improvements in snake species classification presented in this paper are based on increasing image size, combining location and image information as well as upscaled model architecture. The results presented in this article show improved classification results using an object detection strategy previously to the image classification. However, a plausibility filtering of the training dataset showed no clear improvement. Some differences were detected in dependence of the pre-processing steps. Nevertheless, no clear insights could be achieved about which steps are particularly promising for good classification results. The implementation and application of the different pre-processing steps turned out to be relatively time-consuming. Besides, it has been previously mentioned, that there were some memory issues which lead to a focus on small image sizes as well as less upscaled model architectures in the early course of the challenge. Thus the time needed to optimize the classification parameters more precisely and to try out different optimizers was reduced. It is expected, that the results may be further improved by adjusting those parameters.</p><p>Due to the use of AICrowd as a submission platform, it has been possible to test a large number of different models. This enables to get direct feedback about the performance on the test dataset, and thus gives a good estimate about which methods gets the most promising results. In addition it facilitates the comparison between teams before the deadline expires. In this article, it has been mentioned before, that there were some memory issues which were related to the architecture of the test environment. In some cases debugging has been complicated because the logs were not accessible. These concerns were compensated by the very prompt and useful help from the organizers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,156.05,531.27,303.26,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Distribution of the snake species in training and validation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,186.15,383.22,243.06,7.89"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Generalized workflow for snake species classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,151.97,280.34,311.41,7.89"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Appropriated branding approach for country and continent branding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,134.77,542.56,345.83,7.89;13,134.77,553.51,345.83,7.89;13,134.77,564.47,345.83,7.89;13,134.77,575.43,345.83,7.89;13,134.77,586.39,345.82,7.89;13,134.77,597.35,345.82,7.89;13,134.77,608.34,37.02,7.86;13,171.78,606.57,9.41,5.24;13,181.69,608.34,63.17,7.86;13,244.86,606.57,9.41,5.24;13,254.77,608.31,108.04,7.89;13,362.81,606.57,9.41,5.24;13,372.72,608.34,63.17,7.86;13,435.89,606.57,9.41,5.24;13,445.80,608.31,34.79,7.89;13,134.77,619.27,345.83,7.89;13,134.77,630.23,345.83,7.89;13,134.77,641.19,345.83,7.89;13,134.77,652.15,257.86,7.89"><head>- 6 ,</head><label>6</label><figDesc>Abbreviations: O1: Object detection dataset from<ref type="bibr" coords="13,353.16,542.58,9.22,7.86" target="#b8">[9]</ref>, O2: Expanded dataset, I1: No aspect ratio, up-scaling, no padding, I2: Aspect ratio, up-scaling, monochrome padding, I3: Aspect ratio, no up-scaling, monochrome padding, I4: Aspect ratio, no up-scaling, background padding, Sx: Image size: x×x pixels, Bx: EfficientNet-Bx, BSx: Batch size of x for image classification, BSx/y: Batch size: warm-up-phase: x, finetuning: y, W1: Linear weights, W2: Nonlinear weights, LR1: Learning rate warm-up phase: 10 -4 , fine-tuning: 10 -5 , LR2: Warm-up phase: 10 -5 , fine-tuning: 10 -6 , Nx+y: Training epochs warm-up phase: x, fine-tuning: y, P1: Polyak averaging, D1: Reduced dataset, T1: Training dataset, T2: Training + test dataset, M1: Multiplication of metadata, M2: Multiplication without unknown cases, M3: Binary multiplication, M4: Continent branding, M5: Continent and country branding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,139.90,533.93,323.65,93.42"><head>Table 1 .</head><label>1</label><figDesc>Methods used for image pre-processing.</figDesc><table coords="8,139.90,564.69,323.65,62.66"><row><cell>Abbre-</cell><cell>Resizing</cell><cell>Scaling</cell><cell>Fill boundaries</cell></row><row><cell>viation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>I1</cell><cell cols="2">No consideration of the aspect ratio Up-scaling</cell><cell>No padding</cell></row><row><cell>I2</cell><cell cols="2">Consideration of the aspect ratio Up-scaling</cell><cell>Monochrome padding</cell></row><row><cell>I3</cell><cell cols="3">Consideration of the aspect ratio No up-scaling Monochrome padding</cell></row><row><cell>I4</cell><cell cols="3">Consideration of the aspect ratio No up-scaling Background padding</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="13,134.77,118.80,347.04,431.65"><head>Table 2 .</head><label>2</label><figDesc>Classification results achieved for the official test dataset, including macroaveraging F1-score (F1) and log loss. The best results in each section are highlighted in bold.</figDesc><table coords="13,134.77,180.69,347.04,369.76"><row><cell>ID</cell><cell>Ob-</cell><cell>Image</cell><cell cols="3">Classification model training</cell><cell>Data-</cell><cell>Meta-</cell><cell>F1 Log</cell></row><row><cell></cell><cell>ject</cell><cell>pre-</cell><cell></cell><cell></cell><cell></cell><cell>set</cell><cell>data</cell><cell>loss</cell></row><row><cell></cell><cell>de-</cell><cell>process-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>tec-</cell><cell>ing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>tion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">67675 O1 I2 S128 B0 BS64</cell><cell cols="4">W2 LR1 N10+50 -D1 T2 M1</cell><cell>0.338 6.652</cell></row><row><cell cols="4">67696 O1 I2 S128 B2 BS64</cell><cell cols="4">W2 LR1 N10+50 -D1 T2 M1</cell><cell>0.392 6.630</cell></row><row><cell cols="4">67700 O1 I2 S128 B0 BS64</cell><cell cols="4">W2 LR1 N10+50 --T2 M1</cell><cell>0.352 6.651</cell></row><row><cell cols="4">67727 O1 I2 S128 B2 BS64</cell><cell cols="4">W2 LR1 N10+50 --T2 M1</cell><cell>0.389 6.650</cell></row><row><cell cols="4">67734 O1 I2 S128 B4 BS64</cell><cell cols="4">W2 LR1 N10+50 -D1 T1 M1</cell><cell>0.403 6.650</cell></row><row><cell cols="4">67882 O1 I2 S128 B2 BS64</cell><cell cols="4">W1 LR1 N10+50 --T2 M1</cell><cell>0.365 6.657</cell></row><row><cell cols="4">67901 O1 I2 S128 B2 BS64</cell><cell>-</cell><cell cols="3">LR1 N10+50 --T2 M1</cell><cell>0.377 6.647</cell></row><row><cell cols="4">67962 O1 I4 S128 B2 BS64</cell><cell cols="4">W2 LR1 N10+50 --T2 M1</cell><cell>0.403 6.650</cell></row><row><cell cols="4">68023 O1 I2 S128 B4 BS64</cell><cell cols="4">W2 LR1 N10+50 P1 D1 T1 M1</cell><cell>0.404 6.650</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">submission deadline</cell><cell></cell></row><row><cell cols="4">68418 O1 I4 S196 B2 BS64</cell><cell cols="4">W2 LR1 N10+50 --T2 M1</cell><cell>0.475 6.645</cell></row><row><cell cols="4">68432 O1 I3 S128 B2 BS64</cell><cell cols="4">W2 LR1 N10+50 --T2 M1</cell><cell>0.369 6.650</cell></row><row><cell cols="4">68450 O2 I4 S196 B2 BS64</cell><cell cols="4">W2 LR1 N10+50 --T2 M1</cell><cell>0.485 6.645</cell></row><row><cell cols="4">68506 O1 I1 S128 B2 BS64</cell><cell cols="4">W2 LR1 N10+50 --T2 M1</cell><cell>0.452 6.648</cell></row><row><cell cols="4">68520 O1 I2 S224 B0 BS64</cell><cell cols="4">W2 LR1 N10+50 --T1 -</cell><cell>0.322 1.877</cell></row><row><cell cols="4">68541 O1 I2 S128 B4 BS64</cell><cell cols="4">W2 LR1 N10+50 --T2 M1</cell><cell>0.426 6.648</cell></row><row><cell cols="4">68574 O1 I2 S224 B0 BS64</cell><cell cols="4">W2 LR1 N10+50 --T1 M1</cell><cell>0.431 1.659</cell></row><row><cell cols="4">68575 O1 I2 S224 B0 BS64</cell><cell cols="4">W2 LR1 N10+50 --T1 M2</cell><cell>0.447 1.583</cell></row><row><cell cols="4">68593 O1 I2 S196 B4 BS64</cell><cell cols="4">W2 LR1 N10+50 -D1 T1 M1</cell><cell>0.483 6.645</cell></row><row><cell cols="8">68632 O1 I3 S196 B4 BS64/32 W2 LR1 N10+50 --T2 M1</cell><cell>0.366 6.646</cell></row><row><cell cols="4">68655 O1 I2 S224 B0 BS64</cell><cell cols="4">W2 LR1 N10+50 --T1 M3</cell><cell>0.445 1.596</cell></row><row><cell cols="2">68678 -</cell><cell cols="6">I1 S196 B4 BS64/32 W2 LR1 N10+50 --T2 M1</cell><cell>0.347 6.647</cell></row><row><cell cols="4">69365 O2 I4 S380 B4 BS13</cell><cell cols="4">W2 LR2 N10+50 --T1 M1</cell><cell>0.460 1.379</cell></row><row><cell cols="4">69750 O2 I4 S380 B4 BS13</cell><cell cols="4">W2 LR2 N10+50 --T1 M5</cell><cell>0.361 1.541</cell></row><row><cell cols="4">69768 O2 I4 S380 B4 BS13</cell><cell cols="4">W2 LR2 N10+50 --T1 M4</cell><cell>0.437 1.363</cell></row><row><cell cols="4">69849 O2 I4 S380 B4 BS13</cell><cell cols="4">W2 LR2 N10+50 --T1 M4+M1 0.459 1.355</cell></row><row><cell cols="4">69888 O1 I3 S380 B4 BS13</cell><cell cols="4">W2 LR2 N10+109 --T1 M2</cell><cell>0.594 1.064</cell></row><row><cell cols="8">Abbreviations: O1: Object detection dataset from [9], O2: Expanded dataset, I1:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="14,134.77,115.91,345.83,161.96"><head>Table 3 .</head><label>3</label><figDesc>Official classification results on the test dataset to compare object detection datasets. The results include F1 and log loss. The best results in each section are highlighted in bold. Presented results represent ablation studies, thus non-mentioned parameters are fixed in each section.</figDesc><table coords="14,150.36,179.55,314.65,98.32"><row><cell>ID</cell><cell>Object detection</cell><cell>Image pre-processing</cell><cell cols="2">F1 Log loss</cell></row><row><cell cols="3">68418 Dataset from [9] (O1) Aspect ratio, no up-scaling,</cell><cell cols="2">0.475 6.645</cell></row><row><cell></cell><cell></cell><cell>background padding (I4)</cell><cell></cell></row><row><cell cols="3">68450 Expanded dataset (O2) Aspect ratio, no up-scaling,</cell><cell cols="2">0.485 6.645</cell></row><row><cell></cell><cell></cell><cell>background padding (I4)</cell><cell></cell></row><row><cell cols="3">68632 Dataset from [9] (O1) Aspect ratio, no up-scaling,</cell><cell cols="2">0.366 6.646</cell></row><row><cell></cell><cell></cell><cell>monochrome padding (I3)</cell><cell></cell></row><row><cell cols="3">68678 No object detection (-) No aspect ratio, up-scaling,</cell><cell>0.347</cell><cell>6.647</cell></row><row><cell></cell><cell></cell><cell>without padding (I1)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,134.77,418.43,345.83,104.38"><head>Table 4 .</head><label>4</label><figDesc>Official classification results on the test dataset to compare pre-processing methods. The results include F1 and log loss. The best results are highlighted in bold. Presented results represent ablation studies, thus non-mentioned parameters are fixed.</figDesc><table coords="14,153.55,471.11,308.27,51.70"><row><cell>ID</cell><cell>Pre-processing pipeline</cell><cell cols="2">F1 Log loss</cell></row><row><cell cols="2">68506 No aspect ratio, up-scaling, without padding (I1)</cell><cell cols="2">0.452 6.648</cell></row><row><cell cols="2">67727 Aspect ratio, up-scaling, monochrome padding (I2)</cell><cell>0.389</cell><cell>6.650</cell></row><row><cell cols="3">68432 Aspect ratio, no up-scaling, monochrome padding (I3) 0.369</cell><cell>6.650</cell></row><row><cell cols="3">67962 Aspect ratio, no up-scaling, background padding (I4) 0.403</cell><cell>6.650</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="15,134.77,183.03,345.83,93.42"><head>Table 5 .</head><label>5</label><figDesc>Official classification results on the test dataset to compare image input sizes. Both models use the EfficientNet-B4 architecture. The results include F1 and log loss. The best results are highlighted in bold. Presented results represent ablation studies, thus non-mentioned parameters are fixed.</figDesc><table coords="15,231.05,246.66,153.25,29.78"><row><cell>ID</cell><cell>Image size</cell><cell cols="2">F1 Log loss</cell></row><row><cell cols="3">67734 128×128 (S128) 0.403</cell><cell>6.650</cell></row><row><cell cols="4">68593 196×196 (S196) 0.483 6.645</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="15,134.77,546.63,345.83,104.38"><head>Table 6 .</head><label>6</label><figDesc>Official classification results on the test dataset to compare different models of the EfficientNets family. The results include F1 and log loss. All models used input images containing 128×128 pixels. The best results are highlighted in bold. Presented results represent ablation studies, thus non-mentioned parameters are fixed.</figDesc><table coords="15,221.39,610.27,172.58,40.74"><row><cell>ID</cell><cell>Model architecture</cell><cell cols="2">F1 Log loss</cell></row><row><cell cols="3">67700 EfficientNet-B0 (B0) 0.352</cell><cell>6.651</cell></row><row><cell cols="3">67727 EfficientNet-B2 (B2) 0.389</cell><cell>6.650</cell></row><row><cell cols="4">68541 EfficientNet-B4 (B4) 0.426 6.648</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="16,134.77,355.23,345.83,118.13"><head>Table 7 .</head><label>7</label><figDesc>Official classification results on the test dataset to compare different dataset filtering strategies. The results include F1 and log loss. The best results in each section are highlighted in bold. Presented results represent ablation studies, thus nonmentioned parameters are fixed in each section.</figDesc><table coords="16,180.94,418.87,253.47,54.49"><row><cell>ID</cell><cell>Strategy for dataset filtering</cell><cell cols="2">F1 Log loss</cell></row><row><cell cols="3">67675 Duplicates and plausibility filtering (D1) 0.338</cell><cell>6.652</cell></row><row><cell cols="2">67700 No filtering (-)</cell><cell cols="2">0.352 6.651</cell></row><row><cell cols="4">67696 Duplicates and plausibility filtering (D1) 0.392 6.630</cell></row><row><cell cols="2">67727 No filtering (-)</cell><cell>0.389</cell><cell>6.650</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="17,134.77,339.09,345.83,162.36"><head>Table 8 .</head><label>8</label><figDesc>Official classification results on the test dataset to compare strategies to fuse image and location data. The results include F1 and log loss. The best results in each section are highlighted in bold. Presented results represent ablation studies, thus non-mentioned parameters are fixed in each section.</figDesc><table coords="17,163.91,402.73,287.54,98.72"><row><cell>ID</cell><cell>Strategy to integrate metadata</cell><cell cols="2">F1 Log loss</cell></row><row><cell cols="2">68520 No metadata (-)</cell><cell>0.322</cell><cell>1.877</cell></row><row><cell cols="2">68574 Multiplication (M1)</cell><cell>0.431</cell><cell>1.659</cell></row><row><cell cols="2">68575 Multiplication without unknown (M2)</cell><cell cols="2">0.447 1.583</cell></row><row><cell cols="2">68655 Binary multiplication (M3)</cell><cell>0.445</cell><cell>1.596</cell></row><row><cell cols="2">69365 Multiplication (M1)</cell><cell>0.460</cell><cell>1.379</cell></row><row><cell cols="2">69750 Branding continent and country (M5)</cell><cell>0.361</cell><cell>1.541</cell></row><row><cell cols="2">69768 Branding continent (M4)</cell><cell>0.437</cell><cell>1.363</cell></row><row><cell cols="4">69849 Branding continent and multiplication (M4+M1) 0.459 1.355</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,623.92,280.19,8.12"><p>https://clef2020.clef-initiative.eu/, [last accessed: 2020-07-17]   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="2,144.73,634.88,293.81,8.12"><p>https://www.imageclef.org/SnakeCLEF2020, [last accessed: 2020-07-17]   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="2,144.73,646.48,302.27,7.47;2,144.73,656.80,152.60,8.12"><p>https://www.aicrowd.com/challenges/snake-species-identificationchallenge, [last accessed: 2020-07-17]   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="3,144.73,634.88,260.86,8.12"><p>https://www.tropicalherping.com/, [last accessed: 2020-07-17]   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4" coords="3,144.73,645.84,335.87,8.12;3,144.73,656.80,75.88,7.86"><p>https://apps.who.int/bloodproducts/snakeantivenoms/database/, [last accessed: 2020-07-17]   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5" coords="4,144.73,656.80,284.40,8.12"><p>https://github.com/ermig1979/AntiDupl, [last accessed: 2020-07-15]   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6" coords="7,144.73,623.92,294.31,8.12"><p>https://github.com/DiffPro-ML/Mask_RCNN, [last accessed: 2020-06-30]   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7" coords="7,144.73,634.88,289.11,8.12"><p>https://github.com/matterport/MaskRCNN, [last accessed: 2020-06-30]   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8" coords="7,144.73,646.48,305.98,7.47;7,144.73,656.80,277.14,8.12"><p>https://github.com/tensorflow/models/blob/master/research/object_ detection/g3doc/tf2_detection_zoo.md [last accessed: 2020-08-03]</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The work of Louise Bloch and Obioma Pelka was partially funded by a PhD grant from University of Applied Sciences and Arts Dortmund, Germany.</p><p>The authors want to thank Raphael Brüngel for the constructive proofreading of the manuscript.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="19,142.96,238.32,337.63,7.86;19,151.52,249.28,329.07,7.86;19,151.52,260.24,329.07,7.86;19,151.52,271.20,329.07,7.86;19,151.52,282.16,329.07,7.86;19,151.52,293.12,329.07,8.12;19,151.52,304.72,108.76,7.47" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="19,228.09,271.20,213.93,7.86">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="19,462.67,271.20,17.92,7.86;19,151.52,282.16,329.07,7.86;19,151.52,293.12,11.52,7.86">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,315.29,337.64,7.86;19,151.52,326.25,329.07,7.86;19,151.52,337.21,329.07,7.86;19,151.52,348.16,329.07,7.86;19,151.52,359.12,197.00,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="19,355.14,315.29,125.45,7.86;19,151.52,326.25,208.33,7.86">Image-Based Classification of Snake Species Using Convolutional Neural Network</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">S</forename><surname>Abdurrazaq</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Suyanto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Utama</surname></persName>
		</author>
		<idno type="DOI">10.1109/isriti48646.2019.9034633</idno>
		<ptr target="https://doi.org/10.1109/isriti48646.2019.9034633" />
	</analytic>
	<monogr>
		<title level="m" coord="19,404.75,326.25,75.84,7.86;19,151.52,337.21,324.82,7.86">International Seminar on Research of Information Technology and Intelligent Systems (ISRITI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
		<respStmt>
			<orgName>Institute of Electrical and Electronics Engineers</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,370.34,337.64,7.86;19,151.52,381.30,329.07,7.86;19,151.52,392.26,329.07,7.86;19,151.52,403.21,329.07,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="19,313.80,370.34,166.79,7.86;19,151.52,381.30,35.56,7.86">Snake Image Classification Using Siamese Networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Abeysinghe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Welivita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Perera</surname></persName>
		</author>
		<idno type="DOI">10.1145/3338472.3338476</idno>
		<ptr target="https://doi.org/10.1145/3338472.3338476" />
	</analytic>
	<monogr>
		<title level="m" coord="19,209.68,381.30,270.91,7.86;19,151.52,392.26,139.73,7.86">Proceedings of the 2019 3rd International Conference on Graphics and Signal Processing (ICGSP &apos;19)</title>
		<meeting>the 2019 3rd International Conference on Graphics and Signal Processing (ICGSP &apos;19)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery (ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,414.43,337.64,7.86;19,151.52,425.39,329.07,7.86;19,151.52,436.35,329.07,7.86;19,151.52,447.30,329.07,7.86;19,151.52,458.26,55.29,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="19,360.39,414.43,120.20,7.86;19,151.52,425.39,176.76,7.86">Image Classification for Snake Species Using Machine Learning Techniques</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A H</forename><surname>Zahri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yaakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Ahmad</surname></persName>
		</author>
		<ptr target="https://doi.org/10.1007%2F978-3-319-48517-1" />
	</analytic>
	<monogr>
		<title level="m" coord="19,220.17,436.35,209.11,7.86">Computational Intelligence in Information Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Phon-Amnuaisuk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Au</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Omar</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,469.48,337.64,7.86;19,151.52,480.44,329.07,7.86;19,151.52,491.40,329.07,7.86;19,151.52,502.33,319.03,7.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="19,213.85,480.44,266.74,7.86;19,151.52,491.40,297.59,7.86">Identifying the snake: First scoping review on practices of communities and healthcare providers confronted with snakebite across the world</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Botero Mesa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Alcoba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chappuis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0229989</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0229989" />
	</analytic>
	<monogr>
		<title level="j" coord="19,456.28,491.40,24.32,7.86;19,151.52,502.35,20.35,7.86">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">229989</biblScope>
			<date type="published" when="2020">03 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,513.57,337.63,7.86;19,151.52,524.53,329.07,7.86;19,151.52,535.46,329.07,7.89;19,151.52,546.44,255.85,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="19,204.13,524.53,271.92,7.86">Signature Verification using a &quot;Siamese&quot; Time Delay Neural Network</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1142/s0218001493000339</idno>
		<ptr target="https://doi.org/10.1142/s0218001493000339" />
	</analytic>
	<monogr>
		<title level="j" coord="19,151.52,535.49,295.13,7.86">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">07</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1993-08">08 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,557.66,297.48,8.12" xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
	</analytic>
	<monogr>
		<title level="j" coord="19,200.55,557.66,23.11,7.86">Keras</title>
		<imprint>
			<date type="published" when="2015">2015. 2020-07-14</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,568.87,337.63,7.86;19,151.52,579.83,329.07,7.86;19,151.52,590.79,329.07,7.86;19,151.52,601.75,236.75,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="19,384.81,568.87,95.78,7.86;19,151.52,579.83,110.63,7.86">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2009.5206848</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2009.5206848" />
	</analytic>
	<monogr>
		<title level="m" coord="19,306.27,579.83,174.33,7.86;19,151.52,590.79,79.20,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
		<respStmt>
			<orgName>Institute of Electrical and Electronics Engineers</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,612.96,337.64,7.86;19,151.52,623.92,329.07,7.86;19,151.52,634.88,28.16,7.86;19,200.87,635.53,279.22,7.47;19,151.52,646.48,320.87,7.47;19,151.52,656.80,162.01,8.12" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Gokula</forename><surname>Krishnan</surname></persName>
		</author>
		<ptr target="https://medium.com/@Stormblessed/diving-into-deep-learning-part-3-a-deep-learning-practitioners-attempt-to-build-state-of-the-2460292bcfb" />
		<title level="m" coord="19,230.34,612.96,250.25,7.86;19,151.52,623.92,329.07,7.86">Diving into Deep Learning -Part 3 -A Deep learning practitioner&apos;s attempt to build state of the art snake-species image classifier</title>
		<imprint>
			<date type="published" when="2019">2019. 2020-06-10</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,119.67,337.97,7.86;20,151.52,130.63,329.07,7.86;20,151.52,141.59,329.07,7.86;20,151.52,152.55,152.64,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="20,373.87,119.67,56.57,7.86">Mask R-CNN</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.322</idno>
		<ptr target="https://doi.org/10.1109/iccv.2017.322" />
	</analytic>
	<monogr>
		<title level="m" coord="20,151.52,130.63,272.06,7.86">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
		<respStmt>
			<orgName>Institute of Electrical and Electronics Engineers</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,162.53,337.98,7.86;20,151.52,173.48,329.07,7.86;20,151.52,184.44,329.07,7.86;20,151.52,195.40,181.33,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="20,306.88,162.53,173.71,7.86;20,151.52,173.48,22.38,7.86">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2016.90" />
	</analytic>
	<monogr>
		<title level="m" coord="20,196.30,173.48,284.29,7.86;20,151.52,184.44,31.16,7.86">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
		<respStmt>
			<orgName>Institute of Electrical and Electronics Engineers</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,205.38,337.98,7.86;20,151.52,216.34,329.07,7.86;20,151.52,227.30,329.07,7.86;20,151.52,238.25,256.31,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="20,361.82,216.34,118.78,7.86;20,151.52,227.30,155.64,7.86">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,329.87,227.30,150.72,7.86;20,151.52,238.25,162.03,7.86">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3296" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,248.23,337.97,7.86;20,151.52,259.19,329.07,7.86;20,151.52,270.12,322.67,7.89" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="20,359.88,248.23,120.71,7.86;20,151.52,259.19,192.61,7.86">Discriminative histogram taxonomy features for snake species identification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sugathan</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13673-014-0003-0</idno>
		<ptr target="https://doi.org/10.1186/s13673-014-0003-0" />
	</analytic>
	<monogr>
		<title level="j" coord="20,352.29,259.19,128.29,7.86;20,151.52,270.15,83.52,7.86">Human-centric Computing and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,280.12,337.98,7.86;20,151.52,291.08,329.07,7.86;20,151.52,302.04,329.07,7.86;20,151.52,313.00,329.07,7.86;20,151.52,323.96,329.07,7.86;20,151.52,334.92,329.07,7.86;20,151.52,345.88,25.60,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="20,442.94,302.04,37.65,7.86;20,151.52,313.00,329.07,7.86;20,151.52,323.96,163.62,7.86">Overview of LifeCLEF 2020: a System-oriented Evaluation of Automated Species Identification and Species Distribution Prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,337.49,323.96,143.11,7.86;20,151.52,334.92,188.77,7.86">Proceedings of CLEF 2020, CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting>CLEF 2020, CLEF: Conference and Labs of the Evaluation Forum<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">Sep. 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,355.85,337.97,7.86;20,151.52,366.81,329.07,8.11;20,151.52,378.41,61.20,7.47" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="20,245.29,355.85,183.57,7.86">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m" coord="20,451.81,355.85,28.78,7.86;20,151.52,366.81,210.42,7.86">3rd International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,387.74,337.97,7.86;20,151.52,398.70,329.07,7.86;20,151.52,409.66,329.07,7.86;20,151.52,420.62,329.07,7.86;20,151.52,431.58,329.07,7.86;20,151.52,442.54,329.07,7.86;20,151.52,453.50,63.22,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="20,266.08,387.74,214.51,7.86;20,151.52,398.70,147.18,7.86">Optimized Convolutional Neural Network Ensembles for Medical Subfigure Classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-65813-1_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-65813-15" />
	</analytic>
	<monogr>
		<title level="m" coord="20,433.85,409.66,46.75,7.86;20,151.52,420.62,329.07,7.86;20,151.52,431.58,224.34,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction: Proceedings of the 8th International Conference of the CLEF Association</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lawless</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Kelly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017-09">2017. 09 2017</date>
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,463.47,337.97,7.86;20,151.52,474.43,329.07,7.86;20,151.52,485.39,329.07,7.86;20,151.52,496.35,329.07,7.86;20,151.52,507.31,60.15,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="20,209.68,474.43,177.29,7.86">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10602-148" />
	</analytic>
	<monogr>
		<title level="m" coord="20,298.43,485.39,124.82,7.86">Computer Vision -ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,517.28,337.98,7.86;20,151.52,528.24,329.07,7.86;20,151.52,539.18,328.69,7.89" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="20,151.52,528.24,329.07,7.86;20,151.52,539.20,57.24,7.86">Revealing the Unknown: Real-Time Recognition of Galápagos Snake Species Using Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Khatod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Matijosaitiene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Gilkey</surname></persName>
		</author>
		<idno type="DOI">10.3390/ani10050806</idno>
		<ptr target="https://doi.org/10.3390/ani10050806" />
	</analytic>
	<monogr>
		<title level="j" coord="20,216.06,539.20,33.07,7.86">Animals</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">806</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,549.18,337.98,7.86;20,151.52,560.14,329.07,7.86;20,151.52,571.09,329.07,7.86;20,151.52,582.05,329.07,7.86;20,151.52,593.01,183.35,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="20,310.02,549.18,170.58,7.86;20,151.52,560.14,183.54,7.86">Variations on Branding with Text Occurrence for Optimized Body Parts Classification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1109/EMBC.2019.8857478</idno>
		<ptr target="https://doi.org/10.1109/EMBC.2019.8857478" />
	</analytic>
	<monogr>
		<title level="m" coord="20,355.60,560.14,124.99,7.86;20,151.52,571.09,323.70,7.86">2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="890" to="894" />
		</imprint>
		<respStmt>
			<orgName>Institute of Electrical and Electronics Engineers</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,602.99,337.97,7.86;20,151.52,613.95,329.07,7.86;20,151.52,624.90,329.07,7.86;20,151.52,635.86,158.15,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="20,442.93,602.99,37.65,7.86;20,151.52,613.95,309.63,7.86">Overview of the SnakeCLEF 2020: Automatic Snake Species Identification Challenge</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M</forename><surname>Sharada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,151.52,624.90,324.27,7.86">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">Sep. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>CLEF task overview 2020</note>
</biblStruct>

<biblStruct coords="20,142.62,645.84,337.98,7.86;20,151.52,656.77,118.40,7.89" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="20,201.95,645.84,192.04,7.86">New method of stochastic approximation type</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,402.51,645.84,78.09,7.86;20,151.52,656.80,30.48,7.86">Automatic Remote Control</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="937" to="946" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.62,119.67,337.97,7.86;21,151.52,130.61,329.07,8.14;21,151.52,142.24,65.90,7.47" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="21,307.45,119.67,131.21,7.86">Searching for activation functions</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>CoRR) abs/1710.05941</idno>
		<ptr target="http://arxiv.org/abs/1710.05941" />
	</analytic>
	<monogr>
		<title level="m" coord="21,444.75,119.67,35.84,7.86;21,151.52,130.63,100.41,7.86">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.62,152.55,337.97,7.86;21,151.52,163.51,329.07,7.86;21,151.52,174.44,329.07,7.89;21,151.52,185.43,179.00,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="21,353.73,152.55,126.86,7.86;21,151.52,163.51,236.27,7.86">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2016.2577031</idno>
		<ptr target="https://doi.org/10.1109/tpami.2016.2577031" />
	</analytic>
	<monogr>
		<title level="j" coord="21,399.72,163.51,80.88,7.86;21,151.52,174.47,209.73,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.62,196.39,337.98,7.86;21,151.52,207.34,329.07,7.86;21,151.52,218.30,131.90,7.86" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="21,205.41,196.39,275.18,7.86;21,151.52,207.34,14.42,7.86">Efficient Estimations from a Slowly Convergent Robbins-Monro Process</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ruppert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988-02">02 1988</date>
			<pubPlace>Ithaca, NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>School of Operations Research and Industrial Engineering, Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct coords="21,142.62,229.26,337.97,7.86;21,151.52,240.22,329.07,7.86;21,151.52,251.15,329.07,7.89;21,151.52,262.14,239.21,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="21,420.31,240.22,60.28,7.86;21,151.52,251.18,131.71,7.86">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j" coord="21,290.32,251.18,168.61,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.62,273.10,337.98,7.86;21,151.52,284.06,329.07,7.86;21,151.52,295.02,277.99,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="21,220.03,273.10,260.56,7.86;21,151.52,284.06,35.56,7.86">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,381.65,284.06,98.94,7.86;21,151.52,295.02,182.36,7.86">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.62,316.93,337.48,8.11;21,151.52,327.89,209.08,8.12" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="21,295.59,316.93,87.44,7.86">The Reptile Database</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Uetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hallermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hosek</surname></persName>
		</author>
		<ptr target="http://reptile-database.reptarium.cz" />
		<imprint>
			<date type="published" when="2019">2019. 2020-06-10</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.62,338.85,337.98,7.86;21,151.52,349.81,102.14,7.86" xml:id="b27">
	<monogr>
		<title level="m" type="main" coord="21,278.85,338.85,112.54,7.86">Python 3 Reference Manual</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Van Rossum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">L</forename><surname>Drake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CreateSpace</publisher>
			<pubPlace>Scotts Valley, CA</pubPlace>
		</imprint>
	</monogr>
	<note>1 edn.</note>
</biblStruct>

<biblStruct coords="21,142.62,360.77,337.97,7.86;21,151.52,371.73,28.16,7.86;21,201.37,372.37,278.73,7.47;21,151.52,382.69,157.30,8.12" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="21,309.40,360.77,171.19,7.86">Snakebite envenoming -Key Facts 2019</title>
		<ptr target="https://www.who.int/news-room/fact-sheets/detail/snakebite-envenoming" />
		<imprint>
			<date type="published" when="2019">2019. 2020-06-10</date>
			<publisher>World Health Organization (WHO)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.62,393.65,337.98,7.86;21,151.52,404.61,329.07,7.86;21,151.52,415.56,321.36,7.86" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="21,330.19,393.65,150.41,7.86;21,151.52,404.61,121.17,7.86">Self-training with Noisy Student improves ImageNet classification</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,294.30,404.61,186.29,7.86;21,151.52,415.56,205.58,7.86">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06">06 2020</date>
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
