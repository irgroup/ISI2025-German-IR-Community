<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,166.26,115.96,282.84,12.62;1,198.19,133.89,218.99,12.62">Xception Based Method for Bird Sound Recognition of BirdCLEF 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,204.43,171.56,50.42,8.74"><forename type="first">Jisheng</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,265.41,171.56,48.71,8.74"><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<email>chenjf@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.05,171.56,62.40,8.74"><forename type="first">Jianfeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,166.26,115.96,282.84,12.62;1,198.19,133.89,218.99,12.62">Xception Based Method for Bird Sound Recognition of BirdCLEF 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">518B4950A4021A70747ED6616BA70069</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bird sound recognition</term>
					<term>Xception</term>
					<term>Data augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present an Xception based method for bird sound recognition of BirdCLEF2020. The goal of BirdCLEF2020 is to detect and classify 960 bird species within the provided soundscape recordings, it is more complex to differentiate such a large number of birds than BirdCLEF2019. In our approach, logmel or loglinear spectrograms are extracted as features, and some data augmentation techniques are utilized to improve the performance of detecting the bird sounds. Finally, we evaluate our system on BirdCLEF2020 test dataset and achieve a classification mean average precision (c-mAP) score of 0.0421.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It is difficult to take clear photos of birds, which requires an open environment, professional equipment, high level of photography, and it takes a lot of time to actively look for the objects to be photographed. Instead, we can identify a bird's specie through a segment of its voice. Hearing the sound and distinguishing birds is important for many environmental and scientific purposes. Some successful techniques will be used in monitoring of ecological environment in the future. Birds are highly sensitive to the environment. If some areas are polluted, some birds will gradually fly away. Therefore, the changes of bird habits and population can reflect the changes of the environment. If some microphones are installed in the forests, the information of bird population and activities can be collected, which can significantly improve the efficiency and accuracy compared with manual observation. The sounds of different birds are indeed specific. A large number of sound data can be collected by installing specific recording equipment. This can be done automatically in an unattended environment, without the need for people to invest extra time and money.</p><p>With the development of deep learning, a large body of research in sound classification is proven to outperform traditional methods in bird sound classification <ref type="bibr" coords="1,164.85,616.00,9.96,8.74" target="#b3">[5]</ref>. Convolutional neural networks(CNNs) show great feature extraction ability and results in many computer vision tasks. The image-based architectures, Inception-v3 for example, can obtain the best performance in sound classification or what ever the targeted domain <ref type="bibr" coords="2,326.96,142.90,9.96,8.74" target="#b4">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>The training data consists of audio recordings for bird species from South and North America and Europe. The Xeno-canto community contributes this data and provides more than 70,000 high-quality recordings across 960 species to this year's challenge. Each recording is accompanied by metadata containing information on recording location, date and other high-level descriptions provided by the recordists.</p><p>The test data consists of 153 soundscapes recorded in Peru, the USA, and Germany. Each soundscape is of ten-minute duration and contains high quantities of (overlapping) bird vocalizations <ref type="bibr" coords="2,305.68,314.27,9.96,8.74">[1]</ref>.</p><p>The number of recordings for each specie are calculated, although the training dataset contains over 70,000 recordings, there are less than 90 recordings in some species, the rest species are between 90 and 100 recordings. For example, the bird specie with a ebird name of "banfru1" only contains 1 recording, but there are 100 recordings in another specie "whcspa". The number distribution of the dataset is shown in Fig. <ref type="figure" coords="2,269.81,387.81,4.13,8.74" target="#fig_0">1</ref>. The imbalance of the dataset can have effect on recognizing bird species. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bird sing separation</head><p>To separate bird sound and background noise from a original recording, we apply dilation, erosion and smooth masking. Similar techniques are presented in <ref type="bibr" coords="3,465.09,179.67,15.50,8.74" target="#b9">[11]</ref> and used in <ref type="bibr" coords="3,186.85,191.63,10.52,8.74" target="#b7">[9]</ref> and <ref type="bibr" coords="3,218.95,191.63,9.96,8.74" target="#b0">[2]</ref>. We then separate all recordings into 960 bird sing species and noise classes. Details are described as following:</p><p>-Every recording is loaded with a sample rate of 22050Hz.</p><p>-Short-time Fourier transform(STFT) is utilized to calculate spectrograms with a window length of 1024 and hop length of 512.</p><p>-We calculate median value for each row and column, then set every element in the spectrogram to 1 if it is 1.5 times bigger than the median of its related row and column, otherwise it's set to 0.</p><p>-Binary erosion and dilation filters are implemented to distinguish noise and signal parts. The filter size is 4 by 4 square.</p><p>-Here we create a one-dimension vector named indicator vector, its i th element is set to 1 if its related column has at least one 1, or it is 0.</p><p>-Finally, we smooth the indicator vector twice by a dilation filter of size 8 by 1 then use it as a mask to separete original bird recordings. Each recording can be divided into lots of signal and noise parts, all signal parts are concatenated as one and the same as noise. We cut all recordings of every species into 5 seconds parts. After these, we can get 960 folders of each specie of 5-second bird sound and noise recordings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data augmentation</head><p>In recent years, some data augmentation techniques are successfully applied in sound recognition tasks. Two data augmentation metheds are utilized in our approach. Some time and frequency augmentation methods have been used in <ref type="bibr" coords="3,450.40,583.93,10.52,8.74" target="#b7">[9]</ref> and <ref type="bibr" coords="3,134.77,595.89,14.61,8.74" target="#b9">[11]</ref>. In our proposed method, we simply use some of the techniques to augment the dataset and they can be described as follows:</p><p>-Load a bird sound file from random position (it starts from the beginning if it reach the end). To handle the imbalanced dataset and prevent overfitting, we apply mix-up during training stage <ref type="bibr" coords="4,229.52,504.66,14.61,8.74" target="#b11">[13]</ref>. It can be expressed as:</p><formula xml:id="formula_0" coords="4,265.02,528.57,215.57,26.31">x = λx i + (1 -λ)x j (1) ỹ = λy i + (1 -λ)y j<label>(2)</label></formula><p>where x i , x j are input features, y i , y j are target labels, λ ∈ [0, 1] is a random number drawn from the Beta (c, a) distribution. We can get more training samples without extra computing resource, new samples are linear interpolated of real samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Denoising</head><p>All the bird sounds come from diverse area around the world with special environmental background noise. In bird sound recordings, loud raining sound, quiet background, sound of wind and sound of other animals may exist and even cover some bird sounds. To eliminate this, we propose a denoising method to do spectral subtraction on a spectrogram:</p><p>-Load a 5-second sound file and generate an amplitude spectrogram.</p><p>-Calculate mean amplitude values over frames, mark the minimum 20 frames and calculate the mean amplitude values of these frames, we can get a primary subtraction vector. Then subtract the primary subtraction vector over frequencies for each frame, we can get the final denoised spectrogram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Features</head><p>During the training, separated bird sing, augmented bird sound and spectral subtraction sound with logmel and loglinear spectrograms of 128 bands are generated as input. All the spectrograms are calculated by logarithm.</p><p>4 Network architecture</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Xception</head><p>Inception-v3 is one of the state of art architectures in image classification challenge <ref type="bibr" coords="5,159.98,394.97,14.61,8.74" target="#b10">[12]</ref>. And it is confirmed that Inception-based CNNs on Mel spectrograms provide the best performance <ref type="bibr" coords="5,262.85,406.92,9.96,8.74" target="#b2">[4]</ref>. The best network for bird song detection seems to be the Inception-v3 architecture and it preforms better than even the more recent architectures <ref type="bibr" coords="5,223.46,430.83,14.61,8.74" target="#b8">[10]</ref>.</p><p>Xception is a improvement of Inception-v3 proposed by Google. In <ref type="bibr" coords="5,449.60,443.46,9.96,8.74" target="#b1">[3]</ref>, the correlation between channels and spatial correlation should be dealt with separately. The convolution operation in the original Inception-v3 is replaced by a separate concept (Extreme Inception), which is the basic module of Xception. The Xception network is composed of a series of separable convolution, residual connection similar to ResNet and some other conventional operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training strategy</head><p>To recognize 960 species and handle such a large amount of recordings, we used Xception architecture instead of other CNN architectures. As for features, we selected logmel and loglinear spectrogram as input. We applied a denoising method and a data augmentation method during the data preprocessing. Pytorch was implemented to train model, and python librosa library was applied to process recordings and generate features.</p><p>During the training, categorical cross entropy was used as loss function and stochastic gradient descent was used as optimizer with weight decay of 1e-4 and a constant learning rate of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation metrics</head><p>The evaluation metric is the classification mean Average Precision (c-mAP), considering each class c of the ground truth as a query. This means that for each class c, all predictions are extracted from the run file with ClassId(c), rank them by decreasing probability and compute the average precision for that class, which can be expressed as</p><formula xml:id="formula_1" coords="6,247.81,232.77,228.54,26.24">c -mAP = C c-1 AveP (c) C (<label>3</label></formula><formula xml:id="formula_2" coords="6,476.35,243.45,4.24,8.74">)</formula><p>where C is the number of species in the ground truth and AveP (c) is the average precision for a given species c computed as:</p><formula xml:id="formula_3" coords="6,238.17,299.41,238.17,26.33">AveP (c) = n k=1 P (k) × rel(k) n rel (c) (<label>4</label></formula><formula xml:id="formula_4" coords="6,476.35,309.25,4.24,8.74">)</formula><p>where k is the rank of an item in the list of the predicted segments containing c, n is the total number of predicted segments containing c, P (k) is the precision at cut-off k in the list, rel(k) is an indicator function equaling 1 if the segment at rank k is a relevant one (i.e. is labeled as containing c in the ground truth) and n rel is the total number of relevant segments for c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance on validation split</head><p>For each validation or test file, we smoothed the prediction vector by applying a moving window size of 5 seconds. Then we only selected first 3 maximum probabilities as the result of a 5-second test fragment.</p><p>To achieve the performance of our methods, we only trained 76 bird species included in the validation dataset, the best c-mAP scores of the experiments are shown in Table <ref type="table" coords="6,201.69,496.49,3.87,8.74">1</ref>, data augmentation with adding sounds randomly is denoted as AR, spectral subtraction is denoted as Spec sub. Experiment 1 and 2 only used the extracted bird sing to generate features, the rest of experiments extracted features Continuously from original recordings.</p><p>From Table <ref type="table" coords="6,206.44,544.51,4.98,8.74">1</ref> we can see, logmel with mix-up method achieves the best c-mAP score, loglinear performs well with separated bird sings and data augmentaion with randomly adding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance on test split</head><p>Four teams submitted 29 submissions in LifeCLEF 2020 Bird Monophone. Finally we got the 2 th rank among the teams and achieved a best c-mAP score of 0.0421 and r-mAP of 0.0671. Details are shown in Table 2 <ref type="bibr" coords="6,386.60,644.16,23.38,8.74">[7] [8]</ref>. Three different methods were tested and results are shown in Table <ref type="table" coords="6,364.27,656.12,3.87,8.74">3</ref>:</p><p>In this paper, we proposed a system for bird recognition based on Xception with some data augmentation and denoising techniques. Finally, we got a c-mAP score of 0.0421 on official test dataset. To handle more than 70,000 recordings, Xception was chosen because of its great feature extraction ability. During training, mix-up and randomly adding data augmentation methods were applied to prevent overfitting and improve generalization performance. We finally submitted 7 submissions of three main methods. Ensemble of networks is banned this year. We will focus on the performance of convolutional recurrent neural networks and other data augmentation methods without more computing resources for bird recognition. Features can also have great impact on performance sometimes and they would be studied as well. There is still a lot of work to improve in bird sound recognition in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,181.48,630.54,252.40,7.89;2,188.34,434.81,238.68,171.00"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The number distribution of training dataset recordings</figDesc><graphic coords="2,188.34,434.81,238.68,171.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,217.50,310.65,180.35,7.89;4,194.29,115.84,226.78,170.08"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An example of bird sound separation</figDesc><graphic coords="4,194.29,115.84,226.78,170.08" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Finally we got a c-mAP score of 0.027 and r-mAP score of 0.0558. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,350.62,337.63,7.86;8,151.52,361.58,228.84,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,333.76,350.62,146.83,7.86;8,151.52,361.58,84.18,7.86">Inception-v3 based method of lifeclef 2019 bird recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,256.77,361.58,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,372.46,337.64,7.86" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<title level="m" coord="8,199.79,372.46,252.52,7.86">Xception: Deep learning with depthwise separable convolutions</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,383.33,337.63,7.86;8,151.52,394.29,329.07,7.86;8,151.52,405.25,25.60,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,431.93,383.33,48.65,7.86;8,151.52,394.29,239.02,7.86">Overview of birdclef 2018: monophone vs. soundscape bird identification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">G</forename><surname>Hervé Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">CLEF working notes</note>
</biblStruct>

<biblStruct coords="8,142.96,416.12,337.64,7.86;8,151.52,427.08,329.07,7.86;8,151.52,438.04,329.07,7.86;8,151.52,449.00,329.07,7.86;8,151.52,459.96,62.50,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,198.76,427.08,281.84,7.86;8,151.52,438.04,209.62,7.86">Overview of lifeclef 2018: a large-scale evaluation of species identification and recommendation algorithms in the era of ai</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,381.15,438.04,99.44,7.86;8,151.52,449.00,269.59,7.86">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="247" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,470.83,337.63,7.86;8,151.52,481.79,329.07,7.86;8,151.52,492.75,329.07,7.86;8,151.52,503.71,329.07,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,348.80,481.79,131.79,7.86;8,151.52,492.75,152.70,7.86">Lifeclef 2017 lab overview: multimedia species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,325.61,492.75,154.98,7.86;8,151.52,503.71,208.98,7.86">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="255" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,514.59,337.64,7.86;8,151.52,525.54,329.07,7.86;8,151.52,536.50,329.07,7.86;8,151.52,547.46,329.07,7.86;8,151.52,558.42,329.07,7.86;8,151.52,569.38,285.06,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,404.73,536.50,75.87,7.86;8,151.52,547.46,329.07,7.86;8,151.52,558.42,90.77,7.86">Overview of lifeclef 2020: a system-oriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,266.15,558.42,214.45,7.86;8,151.52,569.38,119.03,7.86">Proceedings of CLEF 2020, CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting>CLEF 2020, CLEF: Conference and Labs of the Evaluation Forum<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">Sep. 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,580.25,337.64,7.86;8,151.52,591.21,329.07,7.86;8,151.52,602.17,329.07,7.86;8,151.52,613.13,252.37,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,211.75,591.21,268.84,7.86;8,151.52,602.17,65.83,7.86">Overview of birdclef 2020: Bird sound recognition in complex acoustic environments</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Clapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hopping</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,240.22,602.17,240.37,7.86;8,151.52,613.13,86.35,7.86">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">Sep. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>CLEF task overview 2020</note>
</biblStruct>

<biblStruct coords="8,142.96,624.01,337.64,7.86;8,151.52,634.94,192.49,7.89" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,202.79,624.01,277.80,7.86;8,151.52,634.96,33.97,7.86">Audio-based bird species identification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,192.81,634.96,98.24,7.86">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,645.84,337.97,7.86;8,151.52,656.80,329.07,7.86;9,151.52,119.67,329.07,7.86;9,151.52,130.63,229.73,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,246.74,645.84,233.85,7.86;8,151.52,656.80,185.16,7.86">Audio bird classification with inception-v4 extended with time and time-frequency attention mechanisms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sevilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1866/paper177.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="8,357.36,656.80,123.23,7.86;9,151.52,119.67,184.33,7.86">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,141.59,337.97,7.86;9,151.52,152.55,236.19,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,357.14,141.59,123.45,7.86;9,151.52,152.55,160.42,7.86">Audio based bird species identification using deep learning techniques</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sprengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct coords="9,142.62,163.51,337.97,7.86;9,151.52,174.47,329.07,7.86;9,151.52,185.43,254.31,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,393.24,163.51,87.35,7.86;9,151.52,174.47,148.57,7.86">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,321.62,174.47,158.98,7.86;9,151.52,185.43,161.31,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,196.39,337.98,7.86;9,151.52,207.34,217.41,7.86" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m" coord="9,363.43,196.39,117.16,7.86;9,151.52,207.34,51.04,7.86">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
