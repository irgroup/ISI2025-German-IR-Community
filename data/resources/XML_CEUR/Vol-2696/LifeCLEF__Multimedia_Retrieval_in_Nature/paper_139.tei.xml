<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.17,115.96,303.03,12.62;1,209.67,133.89,196.03,12.62">Plant Species Identification Using Transfer Learning -PlantCLEF 2020</title>
				<funder ref="#_zxsTfZC">
					<orgName type="full">TensorFlow Research Cloud (TFRC)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,189.53,171.90,77.01,8.74"><forename type="first">Nanda</forename><forename type="middle">H</forename><surname>Krishna</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering SSN College of Engineering</orgName>
								<address>
									<settlement>Kalavakkam, Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,350.90,171.90,59.69,8.74"><forename type="first">Ram</forename><surname>Kaushik</surname></persName>
							<email>ramkaushik17125@cse.ssn.edu.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering SSN College of Engineering</orgName>
								<address>
									<settlement>Kalavakkam, Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.17,115.96,303.03,12.62;1,209.67,133.89,196.03,12.62">Plant Species Identification Using Transfer Learning -PlantCLEF 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9B292AD87E14F9E901D9676A2F9873EB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Species Identification</term>
					<term>Transfer Learning</term>
					<term>TPU</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated prediction of plant species from images is immensely useful to conservationists, especially in the case of data-scarce regions of biodiversity. The PlantCLEF 2020 Challenge provides a platform for the creation of a classifier to identify plant species from a large collection of labelled images. The aim of the challenge is to identify which methods work best on the same data, and hence accelerate research progress in the field. In this paper, we discuss the submissions made by our team to the challenge, based on transfer learning. For our submissions, we trained our models on Cloud TPUs and TPU Pods available on Google Cloud Platform. All our models, which were initially trained on the ImageNet Dataset, were fine-tuned to the PlantCLEF 2020 Dataset using transfer learning. With our ResNet-50 models, we achieved an overall MRR of 0.008 in the testing phase. For specifically chosen classes with fewer training samples, we achieved an MRR of 0.003.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated plant species identification from images is immensely useful in datascarce regions of biodiversity, to identify and record the flora present. With the advent of Deep Learning and novel model architectures, performance in this task has improved considerably over the years. However, classification with a very large number of classes is still a tough task with considerable scope for improvement.</p><p>It is with the objective of building a reliable plant species identification system that the PlantCLEF 2020 Challenge <ref type="bibr" coords="1,314.21,579.39,10.52,8.74" target="#b2">[3]</ref> was organised, as part of LifeCLEF 2020 <ref type="bibr" coords="1,157.83,591.35,9.96,8.74" target="#b4">[5]</ref>. The challenge provides a platform for the evaluation of different methods on the same dataset, in an effort to identify the best-performing algorithm for the task. A large labelled dataset of plant images was provided by the organisers, wherein the images exhibit great inter-and intra-class diversity, mimicking the real world.</p><p>Evaluation of the submissions to the challenge was based on the Mean Reciprocal Rank (MRR) metric. Additionally, the MRR for the classification of specific species with fewer training samples was considered as a secondary metric.</p><p>In this paper, we will discuss our team's submissions to the PlantCLEF 2020 Challenge in detail. We will first discuss the methodology we employed to solve the task. next, we will outline the resources used to build our models. Finally, we will discuss the results obtained by our submissions, along with an analysis and a note on future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Preprocessing</head><p>First, we normalised the pixel values to the range [0, 1]. All the images were then resized to 224 × 224 × 3, due to limited computational resources. To do so, we first resize the smaller of the two spatial dimensions (H or W ) to 224 pixels, and then extract the center crop of size 224 × 224 (H × W ). The disadvantage of this preprocessing step is the possible removal of salient information present in pixels that were discarded. An alternative approach could be the use of multiple 224 × 224 crops from the image extracted with a certain stride, to ensure all details in the original image are present in a subset of the extracted images. However, this would increase the number of images in the dataset and thus the computation time.</p><p>We applied augmentations to the training images to improve the generalisation performance of our models. Images were randomly zoomed-in or zoomed-out by up to 20% of the image width, rotated by an angle in the range [-45 • , 45 • ] and flipped about their vertical axis. The objective of these augmentations is to make the models learn robust features, invariant to scale or rotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Models Used</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VGG-16 &amp; VGG-19:</head><p>We first experimented with the VGG-16 and VGG-19 architectures <ref type="bibr" coords="2,195.18,596.34,9.96,8.74" target="#b8">[9]</ref>, pre-trained on the ImageNet Dataset <ref type="bibr" coords="2,384.83,596.34,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,395.35,596.34,7.01,8.74" target="#b7">8]</ref>. The pre-trained models were used as non-trainable feature extractors, and their output for every image was passed to a shallow ANN of 2 Dense layers (each having 4096 units), followed by a Fully-Connected layer (with softmax activation). This method did not perform well on our validation set, and we did not make any submissions based on this method.</p><p>ResNet-50: Both our final submissions to the challenge were made using the ResNet-50 <ref type="bibr" coords="3,182.43,130.95,10.52,8.74" target="#b3">[4]</ref> models. We used the ResNet-50 architecture, again pre-trained on the ImageNet Dataset <ref type="bibr" coords="3,234.54,142.90,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="3,245.05,142.90,7.01,8.74" target="#b7">8]</ref>, as a trainable feature extractor network. Following the layers of the ResNet, we added 2 Dense layers of 2048 units each, followed by a Fully-Connected layer with softmax activation.</p><p>All models were trained with Adam as the optimiser and categorical cross-entropy as the loss function. The number of training epochs was set to 8. As there were training examples for only 998 classes, all our models had an output vector of 998 probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Prediction</head><p>A single species could have one or more images associated with the same submission ID. Two techniques were used to make predictions for the individual species. The predictions were made for the individual images, and either the average or the maximum of the probabilities predicted were used to rank the species and generate the final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Source Code and Resources Used</head><p>Our models were built using the Keras <ref type="bibr" coords="3,311.58,376.86,10.52,8.74" target="#b0">[1]</ref> Deep Learning framework, and the code was written in Python 3. Jupyter Notebooks containing our code have been made publicly available on our GitHub Repository<ref type="foot" coords="3,355.37,399.20,3.97,6.12" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All of the work was performed on the Google Cloud Platform (GCP). Tensor</head><p>Processing Units (TPU) are processors developed by Google specifically for the purpose of accelerating tasks that involve computation on tensors. TPU Pods are a collection of TPUs that are connected by a high speed network. GCP provides access to Cloud TPUs and Cloud TPU Pods. Both individual TPUs (version 2 with 8 cores and version 3 with 8 cores) as well as a TPU Pod (version 3 with 256 cores) were made use of in the training phase.</p><p>For all other tasks, including making of predictions, an n1-highmem VM instance was used, with 8 CPU cores and 52GB of RAM. The storage drive used was a 150GB SSD, connected to the instance.</p><p>Despite the power of the computational resources available, we were unable to experiment with more powerful models or approaches due to limited availability of credits. Furthermore, the TPUs allocated to us were present in regions other than that of our VM instance, causing delays and increased compute time due to network operations. The training of our ResNet model, for instance, took around 2 hours per epoch, highlighting the importance of computational resources in data-intensive tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The results obtained by our models can be seen in Table <ref type="table" coords="4,391.29,145.18,3.87,8.74" target="#tab_0">1</ref>. We have included the categorical cross-entropy loss for all our models as well as the MRR score for our submitted models. With both our submissions, we obtained a Testing MRR score of 0.008, with an MRR of 0.003 on the species with fewer training samples. Overall our submissions received the 44th and 45th ranks on the challenge leaderboard. The scores obtained by all submissions to the challenge can be visualised in Fig. <ref type="figure" coords="4,215.88,378.06,3.87,8.74" target="#fig_1">1</ref>  Despite the resources available, training our models was computationally expensive as the large dataset was very large. We were unable to train our models for a larger number of epochs due to the constraints of our cloud computing resources. Increasing the number of training epochs could yield larger gains in classification accuracy or MRR, as indicated by Fig. <ref type="figure" coords="5,365.93,192.38,3.87,8.74">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Validation cross-entropy loss curve</head><p>Additionally, the lower scores obtained by our methods could be attributed to the low similarity between the dataset used for pre-training and the task-specific dataset. In such a setting, using homogeneous domain adaptation techniques such as MMD <ref type="bibr" coords="5,198.52,453.55,10.52,8.74" target="#b5">[6]</ref> or approaches based on an architecture criterion <ref type="bibr" coords="5,425.89,453.55,10.52,8.74" target="#b6">[7]</ref> could improve performance on the target domain by accounting for the difference in data distribution. We consider only the supervised setting as the PlantCLEF dataset contains sufficient number of examples to facilitate the same. An alternative to this would be the use of heterogeneous domain adaptation techniques using generative models as discussed in <ref type="bibr" coords="5,272.11,513.32,14.61,8.74" target="#b9">[10]</ref>, which could help improve performance on a target domain which is very different from the source domain, as is in our case.</p><p>For the future, we would still consider a transfer learning based approach, however, with pre-training on a similar large dataset. This would considerably improve model performance. In addition, we would like to consider using extreme classification methods to handle the large number of output classes better.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,219.76,378.06,260.83,8.74;4,134.77,390.02,345.83,8.74;4,134.77,401.97,345.82,8.74;4,134.77,413.93,232.19,8.74"><head></head><label></label><figDesc>. The top scores were obtained by teams ITCR PlantNet (Overall MRR 0.180) and Neoun AI (Overall MRR 0.121). On the species with limited training examples, the ITCR PlantNet team obtained a poorer score (MRR 0.062) than the Neuon AI team (MRR 0.108).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,192.93,655.03,229.50,7.89;4,134.77,446.82,346.80,193.44"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overall results of the PlantCLEF 2020 Challenge</figDesc><graphic coords="4,134.77,446.82,346.80,193.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,151.01,213.46,313.34,111.95"><head>Table 1 .</head><label>1</label><figDesc>Validation and Testing Loss and MRR obtained by our models</figDesc><table coords="4,151.01,239.21,313.34,86.20"><row><cell cols="4">Model Training Loss Validation Loss Testing MRR</cell><cell>Testing MRR (Few Species)</cell></row><row><cell>VGG-19</cell><cell>2.3316</cell><cell>4.2236</cell><cell>-</cell><cell>-</cell></row><row><cell>VGG-16</cell><cell>2.1365</cell><cell>3.0349</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet-50 Avg</cell><cell>1.7417</cell><cell>2.2261</cell><cell>0.008</cell><cell>0.003</cell></row><row><cell>ResNet-50 Max</cell><cell>1.7417</cell><cell>2.2261</cell><cell>0.008</cell><cell>0.003</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,657.44,216.54,7.47"><p>https://github.com/nandahkrishna/PlantCLEF2020</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>We would like to acknowledge the support received in the form of Cloud TPUs and a <rs type="projectName">Cloud TPU</rs> v3 Pod from <rs type="funder">TensorFlow Research Cloud (TFRC)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_zxsTfZC">
					<orgName type="project" subtype="full">Cloud TPU</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,142.59,296.46,8.12" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<title level="m" coord="6,226.66,142.59,21.39,7.86">Keras</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,153.55,337.64,7.86;6,151.52,164.51,223.70,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,399.39,153.55,81.21,7.86;6,151.52,164.51,137.62,7.86">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,310.72,164.51,35.84,7.86">CVPR09</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,175.46,337.63,7.86;6,151.52,186.42,329.07,7.86;6,151.52,197.38,190.04,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,285.03,175.46,195.56,7.86;6,151.52,186.42,15.40,7.86">Overview of the lifeclef 2020 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,186.97,186.42,293.63,7.86;6,151.52,197.38,24.01,7.86">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">Sep. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>CLEF working notes 2020</note>
</biblStruct>

<biblStruct coords="6,142.96,208.34,337.64,7.86;6,151.52,219.30,329.07,7.86;6,151.52,230.26,329.07,7.86;6,151.52,241.22,329.07,8.11;6,151.52,252.82,56.49,7.47" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,299.05,208.34,177.61,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m" coord="6,166.05,219.30,314.54,7.86;6,151.52,230.26,16.79,7.86">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,263.14,337.64,7.86;6,151.52,274.09,329.07,7.86;6,151.52,285.05,329.07,7.86;6,151.52,296.01,329.07,7.86;6,151.52,306.97,329.07,7.86;6,151.52,317.93,25.60,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,376.51,285.05,104.08,7.86;6,151.52,296.01,159.72,7.86">Lifeclef 2020: Biodiversity identification and prediction challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castaneda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,334.81,296.01,145.78,7.86;6,151.52,306.97,188.77,7.86">Proceedings of CLEF 2020, CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting>CLEF 2020, CLEF: Conference and Labs of the Evaluation Forum<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">Sep. 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,328.89,337.64,7.86;6,151.52,339.85,329.07,7.86;6,151.52,350.81,329.07,7.86;6,151.52,361.77,329.07,7.86;6,151.52,372.72,273.39,8.12" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,324.63,328.89,155.97,7.86;6,151.52,339.85,60.74,7.86">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/long17a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="6,351.26,339.85,129.34,7.86;6,151.52,350.81,203.87,7.86">Proceedings of the 34th International Conference on Machine Learning, ICML 2017</title>
		<title level="s" coord="6,208.10,361.77,172.07,7.86">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning, ICML 2017<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08-11">6-11 August 2017. 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,383.68,337.64,7.86;6,151.52,394.62,329.07,7.89;6,151.52,405.60,329.07,8.12;6,151.52,417.21,108.27,7.47" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,333.41,383.68,147.19,7.86;6,151.52,394.64,78.46,7.86">Beyond sharing weights for deep domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2814042</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2814042" />
	</analytic>
	<monogr>
		<title level="j" coord="6,241.04,394.64,182.43,7.86">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="801" to="814" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,427.52,337.63,7.86;6,151.52,438.48,329.07,7.86;6,151.52,449.44,329.07,7.86;6,151.52,460.37,326.23,8.14" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,415.77,438.48,64.82,7.86;6,151.52,449.44,145.50,7.86">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j" coord="6,305.92,449.44,174.68,7.86;6,151.52,460.40,28.80,7.86">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,471.35,337.63,7.86;6,151.52,482.31,329.07,7.86;6,151.52,493.27,329.07,7.86;6,151.52,504.23,296.86,8.12" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,278.92,471.35,201.67,7.86;6,151.52,482.31,70.15,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
	</analytic>
	<monogr>
		<title level="m" coord="6,363.90,482.31,116.70,7.86;6,151.52,493.27,166.48,7.86">3rd International Conference on Learning Representations, ICLR 2015</title>
		<title level="s" coord="6,151.52,504.23,121.34,7.86">Conference Track Proceedings</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,515.19,337.97,7.86;6,151.52,526.15,329.07,7.86;6,151.52,537.11,329.07,7.86;6,151.52,548.71,193.00,7.47" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,295.71,515.19,180.83,7.86">Unsupervised cross-domain image generation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Sk2Im59ex" />
	</analytic>
	<monogr>
		<title level="m" coord="6,165.02,526.15,278.42,7.86">5th International Conference on Learning Representations, ICLR 2017</title>
		<title level="s" coord="6,257.58,537.11,192.12,7.86">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
