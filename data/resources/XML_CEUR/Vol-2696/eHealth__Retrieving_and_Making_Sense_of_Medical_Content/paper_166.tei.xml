<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.97,115.96,343.41,12.62;1,261.19,133.89,92.98,12.62">Text Augmentation Techniques for Clinical Case Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,136.35,171.73,67.88,8.74"><forename type="first">Ana√Øs</forename><surname>Ollagnier</surname></persName>
							<email>a.ollagnier@exeter.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">University of Exeter</orgName>
								<address>
									<postCode>EX4 4QE</postCode>
									<settlement>Exeter</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.62,171.73,68.69,8.74"><forename type="first">Hywel</forename><surname>Williams</surname></persName>
							<email>h.t.p.williams@exeter.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">University of Exeter</orgName>
								<address>
									<postCode>EX4 4QE</postCode>
									<settlement>Exeter</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.97,115.96,343.41,12.62;1,261.19,133.89,92.98,12.62">Text Augmentation Techniques for Clinical Case Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C4B1664FF872F0C9162BAFAA5D3714B1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical text classification</term>
					<term>Data augmentation</term>
					<term>Text generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Clinical coding consists in the transformation (or classification) of patient record information into a structured or coded format using internationally recognized class codes. Coding accuracy is an ongoing challenge which has led to the organization of challenges and shared tasks to evaluate AI-enhanced, computer-assisted coding systems. In this paper we present our contribution at CodiEsp: Clinical Case Coding Task (CLEF eHealth 2020) on the automatic assignment of clinical coding (diagnosis and procedures) to clinical cases in Spanish. We approach the task as multi-label classification problem and leverage the powerful language model: Multilingual BERT (M-BERT) to represent the clinical cases and design various deep learning architectures based on a Convolutional Neural Network and a Long Short-Term Memory Network (CNN-LSTM) classifier. To handle the class-imbalance problem, we present other models based on data augmentation techniques (i.e. word-level transformations and text generation methods) for synthesizing labeleddata. Models based on data augmentation pipelines obtain the best results, measured by the F1-score, in comparison to the other proposed models for both tasks. The pipeline based on the word-level transformations obtains the best F1-score (0.143) for the CodiEsp-D task, while the data augmentation technique using the text generation method achieves the best F1-score (0.216) for the CodiEsp-P task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The International Classification of Diseases (ICD) is a health care classification system which provides standardized codes for reporting diseases and health conditions 1 . ICD codes are widely used in Electronic Medical Records (EMR) to describe a patient's diagnosis or treatment. In current practice medical coders review a physician's clinical diagnosis (almost always recorded as free text) then manually assign ICD codes according to coding guidelines. While the process of standardizing EMR is important for making clinical and financial decisions manual ICD coding is expensive, time-consuming and prone to error <ref type="bibr" coords="2,430.83,154.86,14.87,8.74" target="#b11">[12,</ref><ref type="bibr" coords="2,445.69,154.86,7.43,8.74" target="#b2">3]</ref>. Considering these constraints, automated ICD coding has become an important line of research in the Artificial Intelligence community. Traditional machine learning and deep learning techniques have been applied successfully in this context and show promising results <ref type="bibr" coords="2,237.82,202.68,15.84,8.74" target="#b12">[13,</ref><ref type="bibr" coords="2,253.66,202.68,7.92,8.74" target="#b7">8,</ref><ref type="bibr" coords="2,261.58,202.68,11.88,8.74" target="#b15">16,</ref><ref type="bibr" coords="2,273.46,202.68,11.88,8.74" target="#b9">10]</ref>. However, developing an accurate computational system to support automated ICD coding is still a challenging task. The idiosyncrasies of medical language, the scarcity of hospitals using EMR and the class-imbalance problem in training datasets are among the persistent challenges <ref type="bibr" coords="2,134.77,250.50,9.96,8.74" target="#b3">[4]</ref>. Issues related to clinical coding have led to the organization of challenges and shared tasks aiming to evaluate automated clinical coding systems such as the CLEF eHealth Evaluation Lab. The CLEF eHealth<ref type="foot" coords="2,394.46,272.84,3.97,6.12" target="#foot_0">2</ref>  <ref type="bibr" coords="2,403.05,274.41,9.96,8.74" target="#b6">[7]</ref>, established in 2012 as part of the Conference and Labs of the Evaluation Forum (CLEF), is a workshop offering evaluation labs (datasets, evaluation frameworks, and events) in the medical and biomedical domain on different tracks such as information extraction, information management and information retrieval in a mono-and multilingual setting. During the CLEF eHealth 2020 the Clinical Case Coding in Spanish Shared Task<ref type="foot" coords="2,226.43,344.57,3.97,6.12" target="#foot_1">3</ref> (CodiEsp) <ref type="bibr" coords="2,283.04,346.14,15.50,8.74" target="#b10">[11]</ref> was introduced with the aim to evaluate systems devoted to the automatic assignment of ICD codes to EMR in Spanish. This task includes three sub-tasks: (1) Codiesp Diagnosis Coding (CodiEsp-D) which consists of automatically assigning ICD10-Clinical Modification codes to clinical cases in Spanish; (2) Codiesp Procedure Coding (CodiEsp-P) which focuses on assigning ICD10-Procedure codes to clinical cases in Spanish; (3) Codiesp Explainable Artificial Intelligence (CodiEsp-X) which evaluates the explainability/interpretability of the proposed systems (i.e. request to return the text spans supporting the ICD10 code assignment). This paper presents our contribution at the CLEF eHealth CodiEsp 2020 CodiEsp-D and CodiEsp-P sub-tasks. In total five models were submitted during the official evaluation, all based on a Convolutional Neural Network and Long Short-Term Memory Network (CNN-LSTM) classifier. Multilingual BERT (M-BERT) achieved the best performances in the CLEF eHealth 2019 Multilingual Information Extraction task <ref type="bibr" coords="2,262.47,524.17,14.61,8.74" target="#b14">[15]</ref>, hence we proposed to leverage the M-BERT pre-trained model as a part of various deep learning architectures. Then, in order to handle the class-imbalance problem, we designed data augmentation pipelines exploring word-level transformation and a text generation method for synthesizing labeled-data. To compare all the proposed systems we carried out empirical comparisons against a standard CNN architecture, used here as a baseline.</p><p>The Codiesp corpus<ref type="foot" coords="3,221.38,144.04,3.97,6.12" target="#foot_2">4</ref> consists of a set of 1000 clinical cases manually annotated by clinical coding professionals <ref type="foot" coords="3,271.62,155.99,3.97,6.12" target="#foot_3">5</ref> . Documents were coded with clinical diagnosis and procedure codes from the Spanish official version of ICD10-Clinical Modification and ICD10-Procedure. The released corpus has around 16,504 sentences and 396,988 words, with an average of 396.2 words per clinical case. The corpus has been randomly sampled into three subsets: the training set (500 clinical cases), the development set and the test sets (250 clinical cases each). Each subset provides clinical cases in plain text format stored as single files (each filename corresponds to an unique clinical case identifier) and a tab-separated file with either ICD10-Diagnostico (equivalent to ICD10-CM) or ICD10-Procedimiento (equivalent to ICD10-PCS) code assignments according to the target task. Table <ref type="table" coords="3,451.79,265.16,4.98,8.74">1</ref> summarises the top-5 most frequent ICD10-Diagnostico and ICD10-Procedimiento codes from the training and development datasets for both tasks.</p><p>Table <ref type="table" coords="3,167.89,321.65,4.13,7.89">1</ref>. Top-5 most frequent ICD10-Diagnostico and ICD10-Procedimiento codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CodiEsp-D</head><p>CodiEsp-P R52 118 (15.73%) bw03zzz 74 (9.87%) R69 106 (14.13%) bw40zzz 61 (8.13%) R50.9 99 (13.2%) bw20 56 (7.47%) i10 81 (10.8%) bw24 36 (4.8%) R60.9 70 (9.33%) 4a02x4z 34 (4.53%)</p><p>As we can observe in Table <ref type="table" coords="3,278.33,436.02,3.87,8.74">1</ref>, the datasets provided are high imbalanced (i.e. there is high disparity between classes), with 15.73% and 9.87% respectively as the highest frequency rates for the Codiesp-D and Codiesp-P tasks. In total, 10,711 codes were assigned for both tasks, of which, 1819 are unique in the Codiesp-D datasets and 608 in the Codiesp-P datasets. The proportion of rare classes (i.e. classes with only one observation) is also high, 1022 classes (i.e. 56.18%) in the codiesp-D datasets and 393 (i.e. 64.64%) in the Codiesp-P datasets. These findings led us to investigate data augmentation techniques which have shown promise in scarce labeled data situations <ref type="bibr" coords="3,396.90,531.67,14.87,8.74" target="#b16">[17,</ref><ref type="bibr" coords="3,411.76,531.67,7.43,8.74" target="#b1">2]</ref>. Moreover, to expand the training and development corpora, the organizers have also released several additional data resources 6 including medical literature abstracts (i.e. abstracts from Lilacs and Ibecs with ICD10 codes), linguistic resources, gazetteers and a machine-translated version from English of Codiesp corpus clinical cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System architectures</head><p>Empirical studies conducted on the development sets for each task<ref type="foot" coords="4,426.53,182.19,3.97,6.12" target="#foot_5">7</ref> found best performance using a CNN-LSTM classifier. Figure <ref type="figure" coords="4,366.90,195.72,4.98,8.74" target="#fig_0">1</ref> details the architecture used and the shared parameters for both tasks.The model takes as input a timeordered sequence of tokens (words) of arbitrary length (truncated to 396 words which corresponds here the averaged number of words per document, and then padded with zero vectors) and outputs a document-level prediction. After the embedding layer, the layer corresponding to the CNN classifier (one-headed) is introduced using a configuration of 100 parallel feature maps and a kernel size of 3. Immediately afterwards, a LSTM layer is added (set to 100 internal units). Then a dense layer of 64 nodes with ReLu is inserted. Finally an output layer is used with one node containing softmax function. The models have been trained using the Adam optimizer, with a learning rate of 0.001 and a batch size fixed to 32 for both tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Methods</head><p>All proposed methods were trained and tested using the Spanish version of the released corpora. Concerning the preprocessing steps, clinical cases were converted to lowercase and stop-words were removed. After the tokenization process, all tokens based only on non-alphanumeric characters and all short tokens (with &lt; 3 characters) were also deleted. In total five models were submitted to the official evaluation, we provide below a detailed description of each of them:</p><p>-CNN-LSTM: this default approach (used here as a baseline) is based on the architecture presented in section 3. -M-BERT: this approach is based on the BERT language model <ref type="bibr" coords="4,433.44,591.46,9.96,8.74" target="#b4">[5]</ref>. Briefly, BERT, which is based on a transformer architecture, is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. Several pre-trained language models (PTM) have been built from this text encoding model which has previously been successfully applied to various biomedical NLP tasks <ref type="bibr" coords="5,407.92,130.95,9.96,8.74" target="#b8">[9]</ref>. In the CLEF eHealth 2019 Multilingual Information Extraction task, models relying on BERT and its variants (BioBERT and M-BERT) obtained the best results <ref type="bibr" coords="5,151.70,166.81,11.25,8.74" target="#b0">[1,</ref><ref type="bibr" coords="5,162.96,166.81,7.50,8.74" target="#b5">6,</ref><ref type="bibr" coords="5,170.46,166.81,11.25,8.74" target="#b14">15]</ref>. Here, we propose to explore a Sequential Transfer Learning-based technique (STL) using M-BERT <ref type="foot" coords="5,292.54,177.20,3.97,6.12" target="#foot_6">8</ref> . In a STL scenario the source and target tasks are different and training is performed in sequence. Typically, STL consists of two stages: a pre-training phase in which general representations are learned on a source task or domain, and an adaptation phase during which the learned knowledge is transferred to the target task or domain <ref type="bibr" coords="5,449.09,226.59,14.61,8.74" target="#b13">[14]</ref>. In the proposed models the pre-trained language representation (i.e. M-BERT) is introduced during the pre-training phase and then the CNN-LSTM architecture (c.f. section 3) is applied during the adaptation phase to fine-tune models to the target task. -WordNet: this approach explores a traditional textual data augmentation technique consisting of a word-level transformation: synonym replacement.</p><p>Introduced in <ref type="bibr" coords="5,213.56,310.82,14.61,8.74" target="#b16">[17]</ref>, the application of this kind of local change was shown to improve performance on text classification tasks, especially for small training datasets. The process of synonym replacement is implemented as a preprocessing step in a data generator pipeline (this pipeline generates batches of tensor data with real-time data augmentation). For each batch, 10% of documents' words (randomly selected) are substituted by WordNet's synonyms 9 (except stopwords). Finally, edited documents are used to feed models relying on a CNN-LSTM architecture. Below is an example of synonym replacement on a clinical case sample.</p><p>-original: Paciente de 50 a√±os con antecedente de litiasis renal de repetici√≥n que consult√≥ por hematuria recidivante y sensaci√≥n de malestar. El estudio citol√≥gico seriado de orinas demostr√≥ la presencia de c√©lulas at√≠picas sospechosas de malignidad.</p><p>-edited: Paciente de 50 a√±os con antecedente de litiasis nefr√≠tico de repetici√≥n que consult√≥ por hematuria recidivante y percepci√≥n de malestar. El estudio citol√≥gico seriado de orinas demostr√≥ la apariencia de c√©lulas at√≠picas sospechosas de malignidad. -WordNet M-BERT: based on the two previous approaches: WordNet and M-BERT, we explore the combination of a word-level data augmentation technique and the M-BERT pre-trained language model representation. Also implemented as a preprocessing step of the data generator pipeline, synonym replacement is based on the same setup as in the original approach i.e. 10% of documents' words are substituted. Then, as introduced in the M-BERT approach, models are trained as a part of a STL scenario. -TEXT GEN: in this approach we propose to explore a novel data augmentation technique based on a text generation method. This strategy was recently introduced for synthesizing labeled data to improve text classification tasks.</p><p>Approaches leveraging text generation have shown promise, outperforming state-of-the-art techniques for data augmentation, specifically for handling scarce data situations <ref type="bibr" coords="6,247.81,154.86,9.96,8.74" target="#b1">[2]</ref>. The proposed data augmentation pipeline consists in two stages: a pre-training phase in which a language model is learned from the given training sets and a generative phase during which the pre-trained language model is used to generate artificial data. In detail, the pre-trained language model is built using an n-gram modeling approach which estimates n-gram distribution probabilities learned from a given corpus. The language models are trained for both tasks using the CNN-LSTM architecture presented in section 3. During the generative phase the appropriate pre-trained language model is introduced to generate artificial data as a preprocessing step in the data generator pipeline. 30% of each document is altered for each mini batch. Formally, each document is split into sentences then 30% of the sentences are replaced by synthesized data. To synthesize new data 30% of the beginning of a given sentence is used as a seed then extended according to the average length of sentences in the corpus (set to 20 words). Below is an example of a synthesized sentence using the pre-trained language model learned from the CodiEsp-D training set.</p><p>-original: Analytical analysis showed hydroxyvitamin lion ponesium sodium.</p><p>-edited: Analytical analysis showed sequence made transoperative urine outpatient image microbiological markers flap immunohistochemical intravenous dorsolumbar remained level signs 1788 partially establishing transplantation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results on the Test sets</head><p>Models were trained on a workstation with a 36-core CPU and an AMD Fire-Pro W2100 GPU. Systems were evaluated according to the following metrics: Mean Average Precision (MAP), MAP@30, precision, recall and F1-score. For experimental purposes two versions of the F1-score metric are computed: the F1-score measure which considers the full code for both tasks and the F1-score CAT which considers only the first three digits of ICD10-Clinical Modification codes (e.g. codes R20.1 and R20.9 are mapped to R20) and the first four digits of ICD10-Procedure codes (e.g. the code bw40zzz is mapped to bw40). Table <ref type="table" coords="6,475.61,544.39,4.98,8.74" target="#tab_0">2</ref> summarizes the results obtained for both the CodiEsp-D task and the CodiEsp-P task on the test sets. For readability purposes only the MAP, the F1-score and the F1-score CAT are reported. Due to lack of time, not all models for each task were proposed at the official evaluation. However we performed the missing evaluations using the evaluation library released by the organizers <ref type="foot" coords="6,421.38,602.59,7.94,6.12" target="#foot_7">11</ref> , results are presented in italic font. As we can observe the results obtained depend on both the tasks and the models used. For the CodiEsp-D task the Wordnet model (WN) achieves the best MAP, followed closely by the model based on the pre-trained language model M-BERT. For the F1-score, the WN model also obtains the best performance against the other proposed approaches (+0.029 from the baseline). For the F1score CAT, the baseline is first-ranked, slightly outperforming the WN model (-0.001 from the baseline). For the CodiEsp-P task the best MAP is obtained using the combination of M-BERT and the data augmentation technique based on synonym replacement (WN M-BERT) while the TEXT GEN model achieves the best performance for both F1-scores (+0.031 for the F1-score and +0.096 for the F1-score CAT, measured relative to the baseline). Concerning the unofficial results (in italic font), the model WN is first-ranked on the CodiEsp-P task while the TEXT GEN model is the less efficient for the CodiEsp-D task.</p><p>In the overall evaluation, the use of a STL-based architecture combined with a pre-trained model has shown its efficiency, outperforming the baseline on both tasks on the majority of evaluation metrics. Concerning data augmentation techniques, despite missing evaluations, the proposed techniques produced strong results in comparison with the other models, outperforming both the baseline and the models based on M-BERT on both tasks on the majority of evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we presented our contribution to the CLEF eHealth CodiEsp 2020 CodiEsp-D and CodiEsp-P sub-tasks. In total we proposed five models during the official evaluation in which we explored both the powerful language model: Multilingual BERT (M-BERT) and two data augmentation techniques, wordlevel transformation and text generation methods, for synthesizing labeled-data. Models based on data augmentation pipelines achieved the best performances in comparison to the other proposed models for both tasks on the majority of evaluation metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,236.32,412.62,142.72,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. System architecture details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.77,116.41,347.65,117.23"><head>Table 2 .</head><label>2</label><figDesc>Official Results of the Clinical Case Coding in Spanish Shared Task (eHealth CLEF 2020).</figDesc><table coords="7,136.56,148.64,345.86,85.00"><row><cell>Model</cell><cell></cell><cell>CodiEsp-D</cell><cell></cell><cell></cell><cell>CodiEsp-P</cell><cell></cell></row><row><cell></cell><cell>MAP</cell><cell>F1-score</cell><cell>F1-score</cell><cell>MAP</cell><cell>F1-score</cell><cell>F1-score</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CAT</cell><cell></cell><cell></cell><cell>CAT</cell></row><row><cell>Baseline</cell><cell>0.0.76</cell><cell>0.114</cell><cell>0.166</cell><cell>0.123</cell><cell>0.114</cell><cell>0.12</cell></row><row><cell>M-BERT</cell><cell>0.081</cell><cell>0.13</cell><cell>0.151</cell><cell>0.123</cell><cell>0.124</cell><cell>0.129</cell></row><row><cell>WN M-BERT</cell><cell>0.078</cell><cell>0.136</cell><cell>0.153</cell><cell>0.125</cell><cell>0.117</cell><cell>0.121</cell></row><row><cell>WN</cell><cell>0.082</cell><cell>0.143</cell><cell>0.165</cell><cell>0.132</cell><cell>-</cell><cell>-</cell></row><row><cell>TEXT GEN</cell><cell>0.071</cell><cell>-</cell><cell>-</cell><cell>0.121</cell><cell>0.145</cell><cell>0.216</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,144.73,645.84,261.70,8.12"><p>https://clefehealth.imag.fr/ Date of access: 18th June 2020.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,144.73,656.80,261.70,8.12"><p>https://temu.bsc.es/codiesp/ Date of access: 18th June 2020.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="3,144.73,602.00,335.87,8.12;3,144.73,612.96,186.39,8.12"><p>Codiesp corpus available online: https://zenodo.org/record/3837305# .XvsEN5bTVhF Date of access: 30th June 2020.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="3,144.73,623.92,335.87,8.12;3,144.73,634.88,178.50,8.12"><p>Information about annotation guidelines: https://zenodo.org/record/3632523# .Xvw2N5bTU5m Date of access: 1st July 2020.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="3,144.73,645.84,335.87,8.12;3,144.73,656.80,85.23,7.86"><p>https://temu.bsc.es/codiesp/index.php/2019/09/19/resources/ Date of access: 30th June 2020.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="4,144.73,645.84,335.86,7.86;4,144.73,656.80,256.68,7.86"><p>Evaluations (not reported here) were conducted on LSTM, BiLSTM, BiGRU, CNN and CNN-LSTM using the same architecture as presented here.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="5,144.73,634.88,335.86,7.86;5,144.73,645.84,219.21,7.86"><p>BioBERT trained from the original BERT pre-trained model and medical resources in English can't be applied to clinical cases in Spanish</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7" coords="6,144.73,634.88,43.39,7.86;6,204.28,634.88,29.48,7.86;6,249.93,635.53,230.66,7.47;6,144.73,645.84,335.86,8.12;6,144.73,656.80,92.13,7.86"><p>Evaluation library:https://temu.bsc.es/codiesp/index.php/2019/09/19/ evaluation-library/. This library only allows to compute the MAP. Date of access: 13th July 2020.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,142.36,337.63,7.86;8,151.52,153.32,329.07,7.86;8,151.52,164.28,329.07,7.86;8,151.52,175.24,149.77,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,177.92,153.32,302.67,7.86;8,151.52,164.28,15.38,7.86">Mlt-dfki at clef ehealth 2019: Multi-label classification of icd-10 codes with bert</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Dunfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vechkaeva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Wixted</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,187.07,164.28,293.52,7.86;8,151.52,175.24,121.10,7.86">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,186.15,337.64,7.86;8,151.52,197.11,329.07,7.86;8,151.52,208.07,277.28,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,260.58,197.11,220.01,7.86">Do not have enough data? deep learning to the rescue!</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Anaby-Tavor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carmeli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Goldbraich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shlomov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Zwerdling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,165.60,208.07,170.42,7.86">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7383" to="7390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,218.98,337.64,7.86;8,151.52,229.94,329.07,7.86;8,151.52,240.88,329.07,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,267.39,218.98,213.20,7.86;8,151.52,229.94,329.07,7.86;8,151.52,240.90,78.73,7.86">Computer-assisted clinical coding: A narrative review of the literature on its benefits, limitations, implementation and impact on clinical coding professionals</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Giadresco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,237.05,240.90,165.57,7.86">Health Information Management Journal</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="18" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,251.82,337.64,7.86;8,151.52,262.75,252.31,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,333.40,251.82,143.07,7.86">Towards automated clinical coding</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Catling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">P</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,151.52,262.77,175.97,7.86">International journal of medical informatics</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="50" to="61" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,273.69,337.63,7.86;8,151.52,284.62,329.07,7.89" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,346.99,273.69,133.60,7.86;8,151.52,284.65,186.51,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">CoRR</note>
</biblStruct>

<biblStruct coords="8,142.96,295.56,337.64,7.86;8,151.52,306.52,257.07,7.86" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>D√∂rendahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Leich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sch√∂nfelder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grune</surname></persName>
		</author>
		<title level="m" coord="8,431.61,295.56,48.99,7.86;8,151.52,306.52,228.39,7.86">Overview of the clef ehealth 2019 multilingual information extraction</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,317.43,337.63,7.86;8,151.52,328.39,329.07,7.86;8,151.52,339.35,329.07,7.86;8,151.52,350.31,329.07,7.86;8,151.52,361.27,329.07,7.86;8,151.52,372.23,329.07,7.86;8,151.52,383.19,148.63,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,362.80,328.39,117.79,7.86;8,151.52,339.35,76.44,7.86">Overview of the clef ehealth evaluation lab 2020</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Gonzales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Viviani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,453.30,350.31,27.29,7.86;8,151.52,361.27,329.07,7.86;8,151.52,372.23,296.38,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction: Proceedings of the Eleventh International Conference of the CLEF Association (CLEF</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>N√©v√©ol</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Ferro</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">number</biblScope>
			<biblScope unit="page">12260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,394.10,337.63,7.86;8,151.52,405.06,329.08,7.86;8,151.52,415.99,185.98,7.89" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,287.23,394.10,193.36,7.86;8,151.52,405.06,285.32,7.86">An empirical evaluation of supervised learning approaches in assigning diagnosis codes to electronic medical records</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kavuluru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,444.99,405.06,35.60,7.86;8,151.52,416.02,93.96,7.86">Artificial intelligence in medicine</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,426.93,337.63,7.86;8,151.52,437.89,329.07,7.86;8,151.52,448.82,159.79,7.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,438.58,426.93,42.01,7.86;8,151.52,437.89,324.76,7.86">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,151.52,448.85,58.56,7.86">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,459.76,337.97,7.86;8,151.52,470.72,329.07,7.86;8,151.52,481.68,47.10,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,216.10,459.76,264.49,7.86;8,151.52,470.72,85.94,7.86">Icd coding from clinical text using multi-filter residual convolutional neural network</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,260.32,470.72,173.20,7.86">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8180" to="8187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,492.59,337.98,7.86;8,151.52,503.55,329.07,7.86;8,151.52,514.51,329.07,7.86;8,151.52,525.47,329.07,7.86;8,151.52,536.43,62.00,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,151.52,503.55,329.07,7.86;8,151.52,514.51,247.47,7.86">Overview of automatic clinical coding: annotations, guidelines, and solutions for non-english clinical cases at codiesp track of ehealth clef 2020</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Armengol-Estap√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,420.53,514.51,60.06,7.86;8,151.52,525.47,230.39,7.86">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum</title>
		<title level="s" coord="8,389.86,525.47,90.73,7.86;8,151.52,536.43,33.34,7.86">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,547.34,337.97,7.86;8,151.52,558.28,205.44,7.89" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,204.22,547.34,271.85,7.86">Coding errors in nhs cause up to¬£ 1bn worth of inaccurate payments</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>O'dowd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,151.52,558.30,157.78,7.86">BMJ: British Medical Journal (Online)</title>
		<imprint>
			<biblScope unit="volume">341</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,569.22,337.97,7.86;8,151.52,580.18,329.07,7.86;8,151.52,591.11,238.35,7.89" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,151.52,580.18,234.09,7.86">Diagnosis code assignment: models and evaluation metrics</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Perotte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pivovarov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,392.72,580.18,87.87,7.86;8,151.52,591.13,146.34,7.86">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="237" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,602.05,337.98,7.86;8,151.52,613.01,248.97,7.86" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<title level="m" coord="8,193.34,602.05,230.13,7.86">Neural Transfer Learning for Natural Language Processing</title>
		<meeting><address><addrLine>GALWAY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>NATIONAL UNIVERSITY OF IRELAND</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="8,142.61,623.92,337.98,7.86;8,151.52,634.88,329.07,7.86;8,151.52,645.84,329.07,7.86;8,151.52,656.80,76.75,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,342.44,623.92,138.16,7.86;8,151.52,634.88,273.43,7.86">Classifying german animal experiment summaries with multi-lingual bert at clef ehealth 2019 task 1</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>S√§nger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kittner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Leser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,446.52,634.88,34.07,7.86;8,151.52,645.84,329.07,7.86;8,151.52,656.80,48.07,7.86">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,119.67,337.97,7.86;9,151.52,130.61,219.83,7.89" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="9,353.74,119.67,126.85,7.86;9,151.52,130.63,76.59,7.86">Towards automated icd coding using deep learning</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04075</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">CoRR</note>
</biblStruct>

<biblStruct coords="9,142.62,141.59,337.97,7.86;9,151.52,152.52,248.12,7.89" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="9,217.89,141.59,262.70,7.86;9,151.52,152.55,104.93,7.86">Eda: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11196</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">CoRR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
