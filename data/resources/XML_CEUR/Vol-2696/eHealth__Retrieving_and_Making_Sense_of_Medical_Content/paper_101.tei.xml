<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,157.35,115.96,300.65,12.62;1,134.77,133.89,345.83,12.62">ICB-UMA at CLEF e-Health 2020 Task 1: Automatic ICD-10 coding in Spanish with BERT</title>
				<funder>
					<orgName type="full">I Plan Propio de Investigación y Transferencia of the Universidad de Málaga</orgName>
				</funder>
				<funder>
					<orgName type="full">Plan Nacional de I+D+I</orgName>
				</funder>
				<funder ref="#_RMZW2dW">
					<orgName type="full">MINECO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.42,171.56,102.75,8.74"><forename type="first">Guillermo</forename><surname>López-García</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Departamento de Lenguajes y Ciencias de la Computación</orgName>
								<orgName type="department" key="dep2">ETSI Informática</orgName>
								<orgName type="institution">Universidad de Málaga</orgName>
								<address>
									<settlement>Málaga (</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,276.17,171.56,57.90,8.74"><forename type="first">José</forename><forename type="middle">M</forename><surname>Jerez</surname></persName>
							<email>jmjerez@uma.es</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Departamento de Lenguajes y Ciencias de la Computación</orgName>
								<orgName type="department" key="dep2">ETSI Informática</orgName>
								<orgName type="institution">Universidad de Málaga</orgName>
								<address>
									<settlement>Málaga (</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.94,171.56,89.00,8.74"><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Veredas</surname></persName>
							<email>franveredas@uma.es</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Departamento de Lenguajes y Ciencias de la Computación</orgName>
								<orgName type="department" key="dep2">ETSI Informática</orgName>
								<orgName type="institution">Universidad de Málaga</orgName>
								<address>
									<settlement>Málaga (</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,157.35,115.96,300.65,12.62;1,134.77,133.89,345.83,12.62">ICB-UMA at CLEF e-Health 2020 Task 1: Automatic ICD-10 coding in Spanish with BERT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">153DBF9809396CC33FE6A74F43574657</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Clinical coding</term>
					<term>Spanish clinical cases</term>
					<term>BERT</term>
					<term>Text classification</term>
					<term>Transfer learning</term>
					<term>Clinical NLP</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This working notes paper presents our contribution to the CLEF eHealth 2020 Task 1. Our team has participated in the CodiEsp-D subtask, the first shared task consisted in the automatic clinical coding of medical cases in Spanish, annotated with ICD-10-CM codes. We tackled the task as a multi-label classification problem using BERT model [4]. With the aim of leveraging all the language modeling capacities of the deep bidirectional encoder architecture of BERT, we developed a tailored approach to annotate short fragments of text extracted from the long clinical cases present in the CodiEsp corpus and use them as input to the model. Two publicly available Spanish versions of BERT, namely BETO [3]  and BERT-SciELO [1], were fine-tuned on the CodiEsp-D corpus extended by a set of abstracts annotated with ICD-10 codes, following our fragment-based classification approach. BERT-SciELO, a BERT-Base model pre-trained from scratch on an unlabeled corpus of biomedical articles in Spanish, achieved the best results among our three submitted systems, obtaining a final Mean Average Precision (MAP) metric score of 0.482 on the evaluation set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The increasingly adoption of electronic health records (EHRs) as a key component in many hospital information systems across the globe has posed a series of questions to the scientific community that remain partially unresolved. One of the main issues is how to effectively leverage the information stored in the system to improve patient care. EHRs store heterogeneous data in a wide variety of formats, including free-text documents like clinical notes <ref type="bibr" coords="1,396.65,579.86,14.61,8.74" target="#b19">[20]</ref>. These medical textual representations contain crucial patient information related to diagnosis, treatments, or procedures. However, their unstructured nature makes it specially challenging to extract the relevant medical concepts from the data. Automatic clinical coding is the task of transforming unstructured clinical text into a structured format, following standard coding terminologies and using computational methods <ref type="bibr" coords="2,239.86,142.90,14.61,8.74" target="#b21">[22]</ref>. The produced structured data can be subsequently used for medical billing, conducting epidemiological studies, exchanging information between medical institutions, performing statistical analysis, and many other purposes, with the additional advantage of not requiring any human intervention throughout the coding process. Given the importance of storing natural language descriptions of medical cases in modern EHR systems, automatic clinical coding constitutes an essential task in the process of extracting valuable information from EHR data, improving many aspects of clinical care.</p><p>Historically, natural language processing (NLP) techniques have been applied to the problem of clinical coding <ref type="bibr" coords="2,284.63,254.20,15.49,8.74" target="#b14">[15,</ref><ref type="bibr" coords="2,301.78,254.20,12.73,8.74" target="#b17">18,</ref><ref type="bibr" coords="2,316.18,254.20,12.73,8.74" target="#b16">17,</ref><ref type="bibr" coords="2,330.57,254.20,7.01,8.74" target="#b7">8]</ref>. However, most of the previous works only focus on English text, as the availability of corpora annotated with clinical coding information and additional linguistic resources in languages other than English is scarce. With the intention of overcoming this issue, over the past four years, the CLEF eHealth Lab has organised a series of clinical coding shared tasks on non-English or multilingual corpora. Concretely, CLEF eHealth 2016 Task 2 focused on the assignment of International Statistical Classification of Diseases and Related Health Problems (ICD-10) codes to French free-text death certificates, using CépiDC corpus <ref type="bibr" coords="2,280.20,349.84,14.61,8.74" target="#b11">[12]</ref>. In 2017, the CLEF eHealth Task 1 continued with automatic coding of death certificates, but turning the challenge into a bilingual task, using both CépiDC (French) and CDC (English) annotated corpora <ref type="bibr" coords="2,157.17,385.71,14.61,8.74" target="#b10">[11]</ref>. One year later, the CLEF eHealth 2018 Task 1 explored a multilingual clinical coding challenge, with the assignment of ICD-10 codes to death reports in French, Hungarian and Italian <ref type="bibr" coords="2,279.45,409.62,14.61,8.74" target="#b12">[13]</ref>. Finally, last year, the CLEF eHealth 2019 Task 1 again focused on the assignment of ICD-10 codes, but instead of using death reports, non-technical summaries of animal experiments in German were employed <ref type="bibr" coords="2,178.77,445.48,14.61,8.74" target="#b13">[14]</ref>. This year, CLEF eHealth 2020 Task 1 corresponds to the CodiEsp track <ref type="bibr" coords="2,462.33,461.14,14.61,8.74" target="#b9">[10]</ref>, the first shared task consisted in the automatic coding of clinical cases in Spanish, using the Spanish version of ICD-10 (CIE-10) and the CodiEsp corpus, a synthetic corpus of 1K clinical cases in Spanish manually curated by the organisers of the task. The CodiEsp track is composed of three different subtasks: CodiEsp Diagnosis (CodiEsp-D) subtask, CodiEsp Procedure (CodiEsp-P) subtask and Explainable AI (CodiEsp-X) subtask. Given a free-text clinical case in Spanish, CodiEsp-D subtask requires assigning a set of diagnosis codes-ICD-10-CM or CIE-10 Diagnóstico in Spanish-to the medical document, whereas in CodiEsp-P subtask procedures codes-ICD-10-PCS or CIE-10 Procedimientoare predicted for each clinical case. On the other hand, systems participating in CodiEsp-X subtask are required to predict both diagnosis and procedures codes, but also to provide references in the text justifying the coding predictions.</p><p>In this work, we present our contribution to the CLEF eHealth 2020 <ref type="bibr" coords="2,467.31,620.25,9.96,8.74" target="#b4">[5]</ref>, where our team has participated in the CodiEsp-D subtask. We have tackled the problem as a multi-label text classification task using BERT <ref type="bibr" coords="2,394.70,644.16,9.96,8.74" target="#b3">[4]</ref>, a contextualized neural language model that achieved state-of-the-art results on eleven distinct NLP tasks, and was employed by the best performing team in the CLEF eHealth 2019 Task 1 <ref type="bibr" coords="3,189.35,130.95,14.61,8.74" target="#b18">[19]</ref>. Since BERT was specially designed to process short fragments of text-in contrast with the long clinical notes present in the CodiEsp corpus-, we have developed a tailored approach to turn the text classification task into a short segments text classification problem, in order to leverage all the predictive capabilities of the BERT model when applied to the CodiEsp corpus. In this way, each medical document from the corpus was split into short fragments of text. Then, using the information available for the CodiEsp-X subtask, we annotated each short fragment with ICD-10-CM codes information, and used the annotated segments as input to the model. Once the probabilities for individual codes were predicted by the model on every fragment of a document, a maximum probability criterion was used to obtain the probability of each ICD-10 diagnosis code at the document level. Finally, for every document, a list of codes ordered by confidence or relevance was produced, which was used by the organisers to evaluate the performance of the participating systems. For reproducibility purposes, all the code generated to implement our approach is publicly available at https:// github.com/guilopgar/CLEF-2020-CodiEsp.</p><p>The rest of the paper is organised as follows. In Section 2, a short description of the CodiEsp corpus is given, as well as the details of our proposed strategy to tackle the CodiEsp-D subtask are described. We analyze the obtained results in Section 3, whereas Section 4 presents some conclusions and perspectives for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Corpora</head><p>According to the task organisers<ref type="foot" coords="3,280.68,436.93,3.97,6.12" target="#foot_0">1</ref> , the CodiEsp corpus comprises 1K clinical case studies covering a broad diversity of medical subjects, such as pneumology, urology, cardiology or oncology. The entire corpus was randomly split into three different subsets, the training (500 documents), development (250 documents) and test (250 documents) sets. Also, jointly with the test set, a background set of 2751 clinical cases was released. The latter had the intention of guaranteeing that teams participating in the CodiEsp track could not do manual corrections, since submissions to the task had to include predictions for the 3001 medical documents present in both the background and test sets, but they were only evaluated on the test set.</p><p>For the CodiEsp-D subtask, annotation tables containing the assignment of ICD-10-CM codes to the medical documents in the corpus were also available. Furthermore, along with the CodiEsp corpus, a set of additional documents was provided<ref type="foot" coords="3,191.77,592.35,3.97,6.12" target="#foot_1">2</ref> . The supplementary documents correspond to Spanish abstracts obtained from LILACS <ref type="foot" coords="3,233.07,604.30,3.97,6.12" target="#foot_2">3</ref> and IBECS 4 biomedical literature databases, which were annotated with ICD-10 codes information. From the whole set of abstracts, we only selected those texts annotated with ICD-10-CM codes present in the list of valid codes for the CodiEsp track supplied by the organisers <ref type="foot" coords="4,397.64,141.33,3.97,6.12" target="#foot_4">5</ref> .</p><p>Table <ref type="table" coords="4,177.37,155.10,4.98,8.74" target="#tab_0">1</ref> contains a basic description of the CodiEsp-D corpus (training, development and test subsets) as well as the additional abstracts annotated with diagnosis codes information. As it is shown in the table, CodiEsp-D is a considerably challenging task, given the limited number of clinical cases (1000) and the large number of unique codes (2557) present in the corpus, including 363 codes that are only present in the test subset. Additionally, the number of codes annotations is also scarce, resulting in a highly imbalanced multi-label classification problem, where for each code, the number of negative samples clearly surpasses the number of positive cases, i.e. number of documents annotated with a certain code. For these reasons, we decided to expand the CodiEsp-D corpus using the additional set of abstracts. Considering that most of the diagnosis codes (2153 out of 2984) included in the abstracts corpus are not present in the CodiEsp-D annotations, we experimented with two distinct ways of expanding the training and development CodiEsp-D corpora: either using all available abstracts, or, alternatively, solely using the abstracts annotated with ICD-10-CM codes contained in the training and development sets, leading to a reduced version of the abstracts corpus comprising 115457 documents and 160652 codes annotations, from which 733 are unique ICD-10 codes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classification system</head><p>We have tackled the CodiEsp-D challenge using BERT model <ref type="bibr" coords="4,415.36,564.86,9.96,8.74" target="#b3">[4]</ref>. BERT is a contextual language representation model, based on the encoder part of the Transformer architecture <ref type="bibr" coords="4,246.38,588.77,14.61,8.74" target="#b23">[24]</ref>, designed to extract deep bidirectional contextual representations both at the token and sentence level. The model can be pretrained on an unlabeled corpus in an unsupervised manner using two language modeling objectives, namely next sentence prediction and masked language modeling. Unlike other contextual language models, BERT can be transferred to be used in a downstream task, by fine-tuning the whole architecture in a supervised way, then adapting all its pretrained weights to solve a specific task. BERT has gained a lot of attention in the NLP community, including in biomedical and clinical domains, where BERT-based approaches have obtained state-of-the-art results in a wide range of tasks <ref type="bibr" coords="5,340.99,167.75,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="5,353.16,167.75,11.62,8.74" target="#b20">21]</ref>. In this work, we have experimented with two Spanish versions of the BERT-Base architecture: BETO <ref type="bibr" coords="5,470.07,179.70,10.52,8.74" target="#b2">[3]</ref> and BERT-SciELO <ref type="bibr" coords="5,221.19,191.66,10.52,8.74" target="#b0">[1]</ref> models. While both models were pretrained from scratch on unlabeled Spanish corpora, BETO was trained on a compilation of general domain texts <ref type="foot" coords="5,190.63,213.99,3.97,6.12" target="#foot_5">6</ref> , whereas BERT-SciELO was pre-trained on a corpus of biomedical articles retrieved from SciELO <ref type="foot" coords="5,267.77,225.95,3.97,6.12" target="#foot_6">7</ref> .</p><p>In contrast to sequential models such as recurrent neural networks (RNNs), for computational feasibility reasons, Transformer architectures cannot deal with long input sequences of variable length, since for the self-attention layers the complexity is quadratic on the length of the input sequence <ref type="bibr" coords="5,392.77,276.28,14.61,8.74" target="#b23">[24]</ref>. For instance, in the original implementation of BERT-which uses WordPiece <ref type="bibr" coords="5,402.80,288.23,10.52,8.74" target="#b5">[6]</ref> tokenization to subdivide each input token into further sub-token units-, the maximum input sub-tokens sequence length is 512. This constitutes an important limitation when dealing with document or long-text classification tasks like CodiEsp-D, where the sub-tokens sequence size of many clinical cases is clearly above the maximum length supported by BERT.</p><p>Over the last year, a few works have already explored different strategies to overcome this limitation. The most straightforward approach is to use a text truncation method, like the one adopted by the CLEF eHealth 2019 Task 1 best performing team <ref type="bibr" coords="5,210.84,396.76,14.61,8.74" target="#b18">[19]</ref>, which simply consisted in using the first 510<ref type="foot" coords="5,426.66,395.19,3.97,6.12" target="#foot_7">8</ref> sub-tokens of each document as input to the model. In <ref type="bibr" coords="5,323.46,408.72,14.61,8.74" target="#b22">[23]</ref>, additional truncation strategies were considered, and the best performing method was to select the first 128 and the last 382 sub-tokens from each input text. Authors hypothesized that the most relevant information in a document appears at the beginning and at the end of it, which may be the case for the texts present in the corpora analyzed in that work, specifically the movie review IMDb corpus and the Chinese Sogou news articles dataset. However, in the CodiEsp-D corpus, clinical information relevant to solve the task may be spread anywhere within the clinical cases, hence eliminating parts of the documents may not be the most appropriate strategy for our needs.</p><p>Another recent work explored a different approach that does not make use of any truncation method to adapt BERT to solve a text classification task. In this way, authors in <ref type="bibr" coords="5,203.82,553.11,15.50,8.74" target="#b15">[16]</ref> proposed to segment the input texts into smaller parts, and then fine-tune the BERT model on a segment-level supervised task, assigning to each segment the labels associated with the entire document where the segment comes from. If we applied the same strategy to solve the CodiEsp-D multi-label classification task, we would annotate all the segments obtained from a single clinical case with the same ICD-10-CM codes present in the complete document. This constitutes a problematic situation, as many fragments would be annotated with diagnosis codes which are not represented within the fragment, but appear in other segments from the same clinical document.</p><p>With the aim of adopting a strategy to adjust BERT to the distinctive features of the CodiEsp-D subtask, we have developed a three-phases custom approach that transforms the multi-label long-text classification task into a multilabel short-fragment classification problem. Unlike <ref type="bibr" coords="6,362.07,202.92,14.61,8.74" target="#b15">[16]</ref>, using the annotations available for CodiEsp-X named-entity recognition (NER) subtask, for each fragment we only assign the labels occurring within the specific fragment, avoiding misleading the model by assigning codes to a fragment that are present in other parts of the document. In the next paragraphs, the three stages of the developed approach are described.</p><p>Splitting each clinical case into fragments. As indicated before, BERT supports input sequence lengths up to a maximum value of N (N = 512 in the original implementation). For this reason, for every clinical case in the CodiEsp-D corpus, after performing WordPiece tokenization, we split the resulting sub-tokens sequence w = (w 1 , w 2 , ..., w k ), of length k, into a sequence of m = k/(N -2) contiguous sub-token fragments f = (f 1 , f 2 , ..., f m ) = ((w 1 , ..., w N -2 ), (w (N -2)+1 , ..., w 2 * (N -2) ), ..., (w (m-1) * (N -2)+1 , ..., w k )), in which the m -1 first fragments f 1 , .., f m-1 have a length of N -2 sub-tokens while the last fragment f m contains the remaining final sub-tokens of the document (considering that the tokens [CLS] and [SEP] have to be later added at the beginning and the end of each fragment, respectively, to get sequences of size N sub-tokens that will constitute the input to BERT).</p><p>Annotating text fragments with ICD-10-CM codes. Annotations available for the CodiEsp-D subtask contain the assignment of a set of diagnosis codes to each clinical document (see Fig. <ref type="figure" coords="6,317.61,480.26,9.55,8.74" target="#fig_0">1A</ref>). On the other hand, in CodiEsp-X subtask-which uses the same CodiEsp corpus as the CodiEsp-D subtask-, codes annotations include an extra field indicating the reference in the text that explains the coding assignment (see Fig. <ref type="figure" coords="6,336.73,516.13,9.34,8.74" target="#fig_0">1B</ref>). In this way, using the information provided for the CodiEsp-X subtask, we managed to annotate each of the fragments-resulting from the splitting procedure-exclusively with those ICD-10-CM codes annotations whose text references were contained within the fragment. Thus, given a certain fragment f l and the set of its associated diagnosis codes C f l , as well as a CodiEsp-X annotation a i consisted in a code c i and a text reference spanning a group of λ continuous or discontinuous sub-tokens 9 t i = (w i1 , w i2 , ..., w iλ ), we annotated f l with c i when any of the sub-tokens of t i was contained inside the limits of f l , i.e.</p><formula xml:id="formula_0" coords="6,134.77,611.77,345.83,21.61">c i ∈ C f l ⇐⇒ ∃w ij ∈ t i | w ijs ≥ f ls ∧ w ije ≤ f le ,</formula><p>where w ijs and w ije stand for the starting and ending character positions of the sub-token w ij , respectively, whereas f ls and f le represent the starting and ending character positions of the fragment f l . To illustrate the annotation process, in Fig. <ref type="figure" coords="7,242.12,130.95,3.87,8.74">2</ref>, we show the annotations generated for the fragments extracted from the S1139-76322012000400011-1 CodiEsp document using the information available for the same document in the CodiEsp-X subtask (see Fig. <ref type="figure" coords="7,155.10,166.81,9.34,8.74" target="#fig_0">1B</ref>).</p><p>Obtaining codes probabilities at document level. Using the sub-tokens fragments from the clinical cases in the CodiEsp corpus annotated with ICD-10-CM codes, we trained the BERT model on a fragment-level classification problem. Since many distinct codes may be assigned to the same medical document, we treated the task as a multi-label classification problem. To fine-tune the whole architecture of BERT on a supervised learning task at text-level, the representation produced by the model for the initial [CLS] token was fed into a final output layer for classification. As we were dealing with a multi-label task-which is equivalent to multiple independent binary classification tasks-, we used the sigmoid activation function and D output units, with D representing the number of unique diagnosis codes occurring in the texts used to train the model. Hence, given a text fragment as input to the model, the produced output vector could be interpreted as the probability of each code to occur within the input fragment. However, CodiEsp-D task was a document classification problem, and the evaluation of the participating systems was performed at document level. Accordingly, using a maximum probability criterion, we post-processed the model predicted probabilities to obtain codes probabilities at document level. Thus, given a sequence f containing all fragments generated from a single document d as input, the model produces the output probability matrix M ∈ R |f |×D . Then, selecting the maximum probability value across every column of M , a final vector p ∈ R D of codes probabilities at document level is generated, which contains the probability of each code to appear in d. Using this method, we were able to produce, for each clinical case, a list of D distinct codes sorted in descending order according to their probability values predicted by the model, which was finally used by the organisers to evaluate the performance of the classification system.</p><p>It should be noted that, in the case of the additional abstracts corpus (see Section 2.1), the available codes annotations did not contain any extra field indicating the text reference that supports the coding assignment. Therefore, our fragment-based custom approach could not be applied to the abstracts corpus, but only to the CodiEsp-D corpus enriched with the information available for the CodiEsp-X subtask. For this reason, exclusively the abstracts that, after performing WordPiece tokenization, contain a maximum number of N -2 subtokens were used to expand the CodiEsp-D corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Experiments</head><p>We experimented with two Spanish versions of the BERT-Base model, namely BETO and BERT-SciELO. Since the vocabulary of the BERT-SciELO model Presentamos el caso de un varón de 1 1 años original de Gambia que consulta por hematuria macroscópica de predominio al final de la micción y disuria de un año de evolución; sin antecedentes de fiebre. Al realizar la anamnesis refieren un viaje reciente a su país de origen y en el transcurso del mismo varios baños en lagos de la región. La exploración física es anodina.</p><p>Ante la sospecha clínica de bilharzhiasis, se contacta con el Servicio de Microbiología del hospital de referencia, donde indican recogida de orina de tres días consecutivos, preferentemente del mediodía y del final de la micción (momento en que es máxima la excreción de huevos) y se solicita estudio mediante ecografía renovesical. El estudio microbiológico demostró huevos de Schistosoma haematobium. La ecografía vesical practicada puso de manifiesto un engrosamiento parietal que llegaba a alcanzar un grosor máximo de 9 mm en un radio de 20 mm, lo que sugiere esquistosomiasis, por lo que se prescribió tratamiento con prazicuantel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R31.9</head><p>R31.0 R30.0 R50.9 B65.9 B65.0 B65.9</p><p>Presentamos el caso de un varón de 1 1 años original de Gambia que consulta por hematuria macroscópica de predominio al final de la micción y disuria de un año de evolución; sin antecedentes de fiebre. Al realizar la anamnesis refieren un viaje reciente a su país de origen y en el transcurso del mismo varios baños en lagos de la región. La exploración física es anodina. Ante la sospecha clínica de bilharzhiasis, se contacta con el Servicio de Microbiología del hospital de referencia, donde indican recogida de orina de tres días consecutivos, preferentemente del mediodía y del final de la micción (momento en que es máxima la excreción de huevos) y se solicita estudio mediante ecografía renovesical. El estudio microbiológico demostró huevos de Schistosoma haematobium. La ecografía vesical practicada puso de manifiesto un engrosamiento parietal que llegaba a alcanzar un grosor máximo de 9 mm en un radio de 20 mm, lo que sugiere esquistosomiasis, por lo que se prescribió tratamiento con prazicuantel.</p><p>R30.0 R31.0 R31.9 R50.9 B65.0 B65.9 Presenta ##mos el caso de un varón de 1 1 años original de Gam ##bia que consulta por hem ##at ##uria macro ##s ##có ##pica de predomin ##io al final de la mic ##ción y disu ##ria de un año de evolución [UNK] sin antecedentes de fiebre . Al realizar la an ##am ##nes ##is refieren un viaje reciente a su país de origen y en el transcurso del mismo varios baños en lagos de la región . La exploración física es ano ##dina . Ante la sospecha clínica de bil ##har ##z ##hi ##asis , se contac ##ta con el Servicio de Micro ##bio ##logía del hospital de referencia , donde indican recogida de orina de tres días consecutivos , preferentemente del mediodía y del final de la mic ##ción ( momento en que es máxima la excre ##ción de huevos ) y se solicita estudio mediante eco ##grafía renov ##es ##ical . El estudio micro ##bio ##lógico demostró huevos de Sch ##istos ##oma ha ##emato ##bi ##um . La eco ##grafía ves ##ical practica ##da puso de manifiesto un en ##gros ##amiento par ##ie ##tal que llegaba a alcanzar un gros ##or máximo de 9 mm en un radio de 20 mm , lo que sugiere esqui ##stos ##omi ##asis , por lo que se prescri ##bió tratamiento con pra ##zi ##cu ##ante ##l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B</head><p>B65.9 R30.0 R31.0 R31.9 R50.9 B65.0 B65.9</p><p>Fig. <ref type="figure" coords="8,154.40,574.69,4.13,7.89">2</ref>. Illustration of the text fragments ICD-10-CM codes annotations obtained after applying the first two phases of our developed approach to the S1139-76322012000400011-1 clinical case from the CodiEsp training corpus. To generate the sub-tokens fragments, we used the WorPiece tokenizer of the BETO model, setting a maximum fragment length of N -2 = 78 sub-tokens.</p><p>did not include any punctuation character, we used a pre-processed version of the expanded CodiEsp-D corpus (together with the additional abstracts) in which punctuation marks were substituted by spaces. In the case of BETO, the raw text from the documents was employed, as punctuation marks were contained in the vocabulary of the model. Regarding the hardware resources employed, all experiments were executed on a single GeForce GTX 1080 Ti 11 GB GPU. Given the limitations imposed by the hardware, we used a maximum input sequence length of N = 230 for the BERT-SciELO and a value of N = 275 for the BETO model, with both models having ∼ 184M trainable weights. Finally, with respect to the hyperparameters of the models, for fine-tuning, we used RAdam <ref type="bibr" coords="9,172.34,238.55,10.52,8.74" target="#b8">[9]</ref> with learning rate of 3 × 10 -5 , a batch size of 16 and the number of epochs were experimentally determined on the CodiEsp-D development set using early-stopping, with an upper limit of 40 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>In this section, we present the results obtained by our team, ICB-UMA, at the CodiEsp-D subtask. Task organisers allowed each participating team to submit up to 5 runs of their classification systems. We submitted three distinct runs. The first two submissions (ICB-UMA-run1 and ICB-UMA-run2) corresponded to the BERT-SciELO model fine-tuned on the CodiEsp-D training and development corpora expanded using additional abstracts annotated with ICD-10-CM codes present in the training and development sets. Thus, the extended corpus contained 2194 distinct diagnosis codes-the number of unique ICD-10-CM codes present in the training and development corpora (see Table <ref type="table" coords="9,419.45,428.97,5.40,8.74" target="#tab_0">1</ref>)-, having a final BERT-SciELO model with an output classification layer of D = 2194 units. Since exclusively the abstracts containing a maximum number of 230 -2 Word-Piece sub-tokens were considered for the BERT-SciELO model (see Section 2.2), a final set of 66620 abstracts with 92239 codes annotations were employed to expand the training and development corpora. Submission ICB-UMA-run2 used a dropout probability of 0.1 on the output layer of BERT-SciELO model, whereas in submission ICB-UMA-run1 no dropout was used on the output layer. For its part, in submission ICB-UMA-run3 the BETO model was fine-tuned on the CodiEsp-D training and development sets extended using all available abstracts with a maximum number of 275 -2 WordPiece sub-tokens, which comprised a total of 87871 documents and 208076 diagnosis codes annotation, from which 2204 were unique ICD-10-CM codes, i.e. codes that were not present in the training and development corpora. Finally, for submission ICB-UMA-run3 the output layer of the BETO model had D = 4398 units, with no dropout on the final layer. Apart from these three runs, we also experimented with the BERT-SciELO model fine-tuned on the CodiEsp-D corpus expanded using all available abstracts, as well as with the BETO model fine-tuned on the CodiEsp-D corpus extended using additional abstracts annotated with training and development diagnosis codes. However, these two approaches obtained worse results on the development set, and we submitted the three strategies that best performed on the development corpus. Table <ref type="table" coords="10,177.49,142.90,4.98,8.74" target="#tab_1">2</ref> and Table <ref type="table" coords="10,233.74,142.90,4.98,8.74" target="#tab_2">3</ref> show the predictive performance of our three different submitted runs on the CodiEsp test corpus, as a result of the evaluation performed by the CodiEsp track organisers. Namely, Table <ref type="table" coords="10,386.04,166.81,4.98,8.74" target="#tab_1">2</ref> presents the results obtained according to the official evaluation metric of the CodiEsp-D subtask, i.e. the Mean Average Precision (MAP). The second column of the table contains the MAP values calculated considering all codes present in the CodiEsp test subset, whereas the results shown in the third column (MAP codes) were computed taking into account only the predictions for the test codes that were also present in the training and development subsets. Finally, the fourth column (MAP30 ) measures MAP-at-k metric (MAP@k), with k equals 30, while the last column (MAP30 codes) measures MAP@30 considering only the test codes that were present in the training and development corpora (as in the third column of the table). According to the results observed in Table <ref type="table" coords="10,420.98,286.37,3.87,8.74" target="#tab_1">2</ref>, the BERT-SciELO-based model outperformed the BETO-based classifier on the CodiEsp-D subtask, since both the ICB-UMA-run1 and ICB-UMA-run2 systems achieved higher values for all analyzed metrics than the ICB-UMA-run3 submitted system. The best performance is obtained by the BERT-SciELO model when no dropout is used on the output layer, as the ICB-UMA-run1 results slightly surpassed the performance of the ICB-UMA-run2 system across the four examined evaluation metrics. To summarize, we can say that, according to our obtained results, the BERT-SciELO model outperformed the BETO on the CodiEsp-D predictive task. Therefore, pre-training the BERT-Base architecture from scratch on an unlabeled corpus of Spanish biomedical articles, rather than using a general domain Spanish corpus, leads to a better performance of the model on a clinical coding task in the context of Spanish medical narrative. The results obtained in this work for the CodiEsp-D subtask support the hypothesis already explored in previous works <ref type="bibr" coords="10,216.20,453.74,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="10,228.37,453.74,11.62,8.74" target="#b20">21]</ref>, claiming that a clinical domain-specific BERT yields superior performance on medical classification problems than a general domain version of the model. On the other hand, to perform a larger analysis of the results, the task organisers evaluated the performance of the systems according to a set of additional metrics, other than the official ones. In this way, in Table <ref type="table" coords="10,394.25,632.21,3.87,8.74" target="#tab_2">3</ref>, the second, third and fourth columns show the calculated values using precision (P ), recall (R) and the F 1 score (F1 ) metrics, respectively, considering all codes present in the CodiEsp-D test set, while the fifth (P codes), sixth (R codes) and seventh (F1 codes) columns contain the results computed using the same three metrics but taking into consideration only the codes present in the training and development subsets. Finally, in the last three columns (P cat, R cat and F1 cat), the previous metrics are used to evaluate the submitted predictions at the hierarchical category level of the ICD-10-CM codes contained in the test set. As it can be observed from Table <ref type="table" coords="11,227.10,190.72,3.87,8.74" target="#tab_2">3</ref>, for the precision and consequently for the F 1 score, our three submitted systems obtained extremely poor values, though for the recall the computed values are abnormally high. The reason for this is that, with the aim of maximizing the score obtained for the official evaluation metric, i.e. MAP, for each test document we submitted all codes considered by the model-2194 codes in the case of ICB-UMA-run1 and ICB-UMA-run2 and 4398 codes for the ICB-UMA-run3 submission-ordered by their predicted probability of occurrence (see Section 2.2). If we had optimized precision, recall and F1 score metrics instead of MAP, in place of submitting all codes, a decision threshold would have been defined to select solely a subset of the codes according to their predicted probabilities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we present our contribution to the CodiEsp-D subtask from the CodiEsp track <ref type="bibr" coords="11,203.08,500.46,15.50,8.74" target="#b9">[10]</ref> of CLEF eHealth 2020 <ref type="bibr" coords="11,329.59,500.46,9.96,8.74" target="#b4">[5]</ref>. The shared task proposes the automatic assignment of ICD-10-CM codes to Spanish clinical cases. The scarce number of medical cases present in the CodiEsp corpus in combination with the large quantity of unique diagnosis codes used to annotate the documents, make CodiEsp-D a considerably challenging task.</p><p>We have tackled the challenge as a multi-label text classification task using BERT model <ref type="bibr" coords="11,198.19,572.43,9.96,8.74" target="#b3">[4]</ref>. A fragment-based classification approach was developed in order to take advantage of all the predictive capacity of BERT when receiving the long-text clinical cases contained in the CodiEsp corpus as input to the model. Our strategy consisted in using the available annotations for the CodiEsp-X NER subtask to turn the CodiEsp-D multi-label document classification task into a multi-label short-fragment classification problem. We experimented with two publicly available Spanish versions of the BERT model, fine-tuned on the CodiEsp-D corpus expanded using a set of available abstracts annotated with ICD-10-CM codes. BERT-SciELO <ref type="bibr" coords="12,285.70,118.99,9.96,8.74" target="#b0">[1]</ref>, a BERT-Base architecture pre-trained on a corpus of biomedical articles in Spanish, yielded the best performance among our three submitted systems, obtaining a MAP score of 0.482 on the evaluation set. The obtained results in this work reinforced the idea that a medical domain version of BERT achieves higher performance on clinical classification tasks than nonspecific domain versions of the model.</p><p>In future works, we will try to enhance the developed fragment-based classification strategy to further improve the obtained results on the CodiEsp-D subtask. For instance, when splitting each clinical case into fragments, we could perform the text segmentation at the sentence level, producing fragments comprising a sequence of sentences with a complete semantic meaning. On the other hand, given the superior performance observed from the BERT-SciELO model, it is worth investigating whether alternative clinical-specific versions of BERT pre-trained on Spanish medical corpora more similar to the CodiEsp corpus could increase the results further. Additionally, due to the widespread adoption of BERT in multi-lingual setups across domains, we could also explore the pre-training of the model on a multi-lingual medical corpus. This would permit the creation of enormous medical corpora comprising clinical documents written in many different languages. Because of the sub-word vocabulary employed by BERT and the common etymology of numerous medical terms across distinct languages, multi-lingual clinical corpora could serve as a valuable source of data to pre-train BERT models. The resulting BERT's deep bidirectional architecture could leverage its language modeling capabilities to produce effective contextual representations that could be used in applications within a vast number of medical NLP information-extraction problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,134.77,309.31,345.83,8.14;8,134.77,320.27,345.83,7.89;8,134.77,331.23,345.82,7.89;8,134.77,342.21,19.25,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of annotation formats using the S1139-76322012000400011-1 clinical case from the CodiEsp training corpus. A Codes annotations available for the CodiEsp-D subtask. B Diagnosis codes annotations available for the CodiEsp-X subtask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,390.30,345.82,106.02"><head>Table 1 .</head><label>1</label><figDesc>Summary of CodiEsp-D and additional abstracts corpora annotated with ICD-10-CM codes.</figDesc><table coords="4,137.38,420.29,334.64,76.03"><row><cell></cell><cell>Training</cell><cell>Development</cell><cell>Test</cell><cell>Abstracts</cell></row><row><cell>Documents</cell><cell>500</cell><cell>250</cell><cell>250</cell><cell>170120</cell></row><row><cell>Total ICD Codes</cell><cell>5639</cell><cell>2677</cell><cell>2842</cell><cell>403856</cell></row><row><cell>Avg. ICD codes per doc.</cell><cell>11.278</cell><cell>10.708</cell><cell>11.368</cell><cell>2.374</cell></row><row><cell>Unique ICD codes</cell><cell>1767</cell><cell>1158</cell><cell>1143</cell><cell>2984</cell></row><row><cell>Avg. docs. per ICD code</cell><cell>3.191</cell><cell>2.312</cell><cell>2.486</cell><cell>135.340</cell></row><row><cell>Unique unseen ICD codes</cell><cell>-</cell><cell>427</cell><cell>363</cell><cell>2153</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,134.77,506.02,345.82,73.69"><head>Table 2 .</head><label>2</label><figDesc>Classification performance of each submitted run assessed using MAP, the official CodiEsp-D subtask evaluation metric.</figDesc><table coords="10,179.59,537.75,256.17,41.96"><row><cell>Submission</cell><cell cols="4">MAP MAP codes MAP30 MAP30 codes</cell></row><row><cell cols="2">ICB-UMA-run1 0.482</cell><cell>0.567</cell><cell>0.460</cell><cell>0.542</cell></row><row><cell cols="2">ICB-UMA-run2 0.471</cell><cell>0.554</cell><cell>0.449</cell><cell>0.529</cell></row><row><cell cols="2">ICB-UMA-run3 0.455</cell><cell>0.536</cell><cell>0.430</cell><cell>0.509</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,134.77,342.25,349.77,71.95"><head>Table 3 .</head><label>3</label><figDesc>Classification performance of each submitted run evaluated according to additional metrics.</figDesc><table coords="11,136.16,372.24,348.38,41.96"><row><cell>Submission</cell><cell>P</cell><cell>R</cell><cell cols="2">F1 P codes R codes F1 codes P cat R cat F1 cat</cell></row><row><cell cols="4">ICB-UMA-run1 0.004 0.858 0.009 0.004</cell><cell>1.0</cell><cell>0.009</cell><cell>0.010 0.968 0.021</cell></row><row><cell cols="4">ICB-UMA-run2 0.004 0.858 0.009 0.004</cell><cell>1.0</cell><cell>0.009</cell><cell>0.010 0.968 0.021</cell></row><row><cell cols="4">ICB-UMA-run3 0.002 0.897 0.005 0.004</cell><cell>1.0</cell><cell>0.009</cell><cell>0.008 0.987 0.016</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,624.57,244.78,7.47"><p>https://temu.bsc.es/codiesp/index.php/category/data/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,635.53,216.54,7.47"><p>https://zenodo.org/record/3606662#.XvxBT59fg8o</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,646.48,141.22,7.47"><p>https://lilacs.bvsalud.org/es/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,144.73,657.44,108.27,7.47"><p>http://ibecs.isciii.es/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,144.73,657.44,216.54,7.47"><p>https://zenodo.org/record/3706838#.XvxD3J9fg8o</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,144.73,624.57,217.03,7.47"><p>https://github.com/josecannete/spanish-corpora</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="5,144.73,635.53,122.39,7.47"><p>https://www.scielo.org/es/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="5,144.73,645.84,335.87,8.12;5,144.73,656.80,178.93,7.86"><p>Since BERT always adds two special tokens ([CLS] and [SEP]) at the first and last positions, respectively, of an input sequence.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was partially supported by the project <rs type="grantNumber">TIN2017-88728-C2-1-R</rs>, <rs type="funder">MINECO</rs>, <rs type="funder">Plan Nacional de I+D+I</rs>, and <rs type="funder">I Plan Propio de Investigación y Transferencia of the Universidad de Málaga</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RMZW2dW">
					<idno type="grant-number">TIN2017-88728-C2-1-R</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,523.70,337.63,7.86;12,151.52,534.66,329.07,7.86;12,151.52,545.61,329.07,7.86;12,151.52,556.57,158.34,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="12,423.19,523.70,57.40,7.86;12,151.52,534.66,329.07,7.86;12,151.52,545.61,40.34,7.86">Testing Contextualized Word Embeddings to Improve NER in Spanish Clinical Case Narratives</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Akhtyamova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cardiff</surname></persName>
		</author>
		<idno type="DOI">10.21203/rs.2.22697/v1</idno>
		<ptr target="https://doi.org/10.21203/rs.2.22697/v1" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Research Square</publisher>
		</imprint>
	</monogr>
	<note>Preprint (Version 1</note>
</biblStruct>

<biblStruct coords="12,142.96,568.06,337.64,7.86;12,151.52,579.02,329.07,7.86;12,151.52,589.98,329.07,7.86;12,151.52,600.94,329.07,7.86;12,151.52,611.90,158.52,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,223.34,579.02,184.70,7.86">Publicly available clinical BERT embeddings</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jindi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcdermott</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-1909</idno>
		<ptr target="https://doi.org/10.18653/v1/W19-1909" />
	</analytic>
	<monogr>
		<title level="m" coord="12,432.51,579.02,48.08,7.86;12,151.52,589.98,255.37,7.86">Proceedings of the 2nd Clinical Natural Language Processing Workshop</title>
		<title level="s" coord="12,466.98,589.98,13.62,7.86;12,151.52,600.94,163.41,7.86">Association for Computational Linguistics</title>
		<meeting>the 2nd Clinical Natural Language Processing Workshop<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,623.39,337.64,7.86;12,151.52,634.35,282.23,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,343.94,623.39,136.65,7.86;12,151.52,634.35,82.30,7.86">Spanish Pre-Trained BERT Model and Evaluation Data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cañete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chaperon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,307.95,634.35,97.14,7.86">PML4DC at ICLR 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>to appear in</note>
</biblStruct>

<biblStruct coords="12,142.96,645.84,337.64,7.86;12,151.52,656.80,329.07,7.86;13,151.52,119.67,329.07,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,329.07,7.86;13,151.52,152.55,155.96,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,364.92,645.84,115.67,7.86;12,151.52,656.80,224.16,7.86">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="12,401.63,656.80,78.96,7.86;13,151.52,119.67,329.07,7.86;13,151.52,130.63,228.09,7.86">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,163.52,337.63,7.86;13,151.52,174.48,329.07,7.86;13,151.52,185.44,329.07,7.86;13,151.52,196.40,329.07,7.86;13,151.52,207.36,329.07,7.86;13,151.52,218.32,329.07,7.86;13,151.52,229.28,217.57,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,357.72,174.48,122.87,7.86;13,151.52,185.44,80.83,7.86">Overview of the CLEF eHealth Evaluation Lab 2020</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Saez Gonzales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Viviani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,466.38,196.40,14.21,7.86;13,151.52,207.36,329.07,7.86;13,151.52,218.32,301.18,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction: Proceedings of the Eleventh International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="13,151.52,229.28,141.41,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Ferro</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,240.25,337.64,7.86;13,151.52,251.21,329.07,7.86;13,151.52,262.16,329.07,7.86;13,151.52,273.10,293.95,7.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,409.11,251.21,71.48,7.86;13,151.52,262.16,287.20,7.86">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,445.96,262.16,34.63,7.86;13,151.52,273.12,219.02,7.86">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,284.09,337.64,7.86;13,151.52,295.05,329.07,7.86;13,151.52,305.99,329.07,7.89;13,151.52,316.97,188.04,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,151.52,295.05,329.07,7.86;13,151.52,306.01,128.44,7.86">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btz682" />
	</analytic>
	<monogr>
		<title level="j" coord="13,295.07,306.01,58.56,7.86">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,327.94,337.63,7.86;13,151.52,338.90,329.07,7.86;13,151.52,349.83,211.12,7.89" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,386.14,327.94,94.45,7.86;13,151.52,338.90,141.06,7.86">Automated ICD-9 Coding via A Deep Learning Approach</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,300.61,338.90,179.98,7.86;13,151.52,349.86,109.89,7.86">IEEE/ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1193" to="1202" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,360.83,337.63,7.86;13,151.52,371.79,192.92,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="13,404.35,360.83,76.23,7.86;13,151.52,371.79,164.24,7.86">On the Variance of the Adaptive Learning Rate and Beyond</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,382.76,337.98,7.86;13,151.52,393.72,329.07,7.86;13,151.52,404.68,329.07,7.86;13,151.52,415.64,329.07,7.86;13,151.52,426.60,76.75,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,151.52,393.72,329.07,7.86;13,151.52,404.68,251.83,7.86">Overview of automatic clinical coding: annotations, guidelines, and solutions for non-English clinical cases at CodiEsp track of CLEF eHealth</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Armengol-Estapé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,446.52,404.68,34.07,7.86;13,151.52,415.64,329.07,7.86;13,151.52,426.60,48.07,7.86">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,437.57,337.98,7.86;13,151.52,448.53,329.07,7.86;13,151.52,459.49,329.07,7.86;13,151.52,470.45,262.94,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,278.61,448.53,201.98,7.86;13,151.52,459.49,324.85,7.86">CLEF eHealth 2017 multilingual information extraction task overview: ICD10 coding of death certificates in English and French</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">N</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grouin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lavergne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zweigenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,165.60,470.45,151.82,7.86">Proc of CLEF eHealth Evaluation lab</title>
		<meeting>of CLEF eHealth Evaluation lab<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,481.42,337.97,7.86;13,151.52,492.38,329.07,7.86;13,151.52,503.34,329.07,7.86;13,151.52,514.29,157.09,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,399.59,492.38,81.00,7.86;13,151.52,503.34,193.02,7.86">Clinical information extraction at the CLEF eHealth evaluation lab</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grouin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lavergne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zweigenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,387.59,503.34,93.00,7.86;13,151.52,514.29,57.59,7.86">Proc of CLEF eHealth Evaluation lab</title>
		<meeting>of CLEF eHealth Evaluation lab<address><addrLine>Evora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,525.27,337.98,7.86;13,151.52,536.22,329.07,7.86;13,151.52,547.18,329.07,7.86;13,151.52,558.14,317.47,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,277.75,536.22,202.84,7.86;13,151.52,547.18,329.07,7.86;13,151.52,558.14,43.22,7.86">CLEF eHealth 2018 Multilingual Information Extraction Task Overview: ICD10 Coding of Death Certificates in French, Hungarian and Italian</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Grippo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Morgand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Orsi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Pelikan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ramadier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zweigenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,215.52,558.14,151.82,7.86">Proc of CLEF eHealth Evaluation lab</title>
		<meeting>of CLEF eHealth Evaluation lab<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,569.11,337.97,7.86;13,151.52,580.07,329.07,7.86;13,151.52,591.03,329.07,7.86;13,151.52,601.99,329.07,7.86;13,151.52,612.95,138.27,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,195.57,580.07,205.59,7.86">Overview of the CLEF eHealth Evaluation Lab 2019</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Butzke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dörendahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Leich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Schönfelder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,321.40,591.03,159.19,7.86;13,151.52,601.99,155.53,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. CLEF</title>
		<title level="s" coord="13,334.53,601.99,142.10,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">11696</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,623.92,337.98,7.86;13,151.52,634.88,329.07,7.86;13,151.52,645.81,329.07,7.89;13,151.52,656.80,199.48,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,341.02,623.92,139.57,7.86;13,151.52,634.88,329.07,7.86;13,151.52,645.84,42.64,7.86">Automating the Assignment of Diagnosis Codes to Patient Encounters Using Example-based and Machine Learning Techniques</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">V</forename><surname>Pakhomov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Buntrock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Chute</surname></persName>
		</author>
		<idno type="DOI">10.1197/jamia.M2077</idno>
		<ptr target="https://doi.org/10.1197/jamia.M2077" />
	</analytic>
	<monogr>
		<title level="j" coord="13,201.34,645.84,230.04,7.86">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="516" to="525" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,119.67,337.98,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,329.07,7.86;14,151.52,152.55,203.96,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,440.62,119.67,39.97,7.86;14,151.52,130.63,212.26,7.86">Hierarchical Transformers for Long Document Classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pappagari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zelasko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Carmiel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASRU46091.2019.9003958</idno>
		<ptr target="https://doi.org/10.1109/ASRU46091.2019.9003958" />
	</analytic>
	<monogr>
		<title level="m" coord="14,387.90,130.63,92.69,7.86;14,151.52,141.59,239.22,7.86">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="838" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,163.51,337.97,7.86;14,151.52,174.47,329.07,7.86;14,151.52,185.40,329.07,7.89;14,151.52,196.39,182.33,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,201.67,174.47,248.31,7.86">Diagnosis code assignment: models and evaluation metrics</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Perotte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pivovarov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
		<idno type="DOI">10.1136/amiajnl-2013-002159</idno>
		<ptr target="https://doi.org/10.1136/amiajnl-2013-002159" />
	</analytic>
	<monogr>
		<title level="j" coord="14,459.45,174.47,21.14,7.86;14,151.52,185.43,227.31,7.86">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="237" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,207.34,337.98,7.86;14,151.52,218.30,329.07,7.86;14,151.52,229.26,329.07,7.86;14,151.52,240.22,329.07,7.86;14,151.52,251.18,159.96,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,223.01,218.30,257.58,7.86;14,151.52,229.26,36.86,7.86">A Shared Task Involving Multi-Label Classification of Clinical Free Text</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Pestian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Brew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Matykiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Hovermale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Duch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,209.61,229.26,270.98,7.86;14,151.52,240.22,163.56,7.86;14,366.74,240.22,46.61,7.86">Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</title>
		<meeting>the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
	<note>BioNLP &apos;07</note>
</biblStruct>

<biblStruct coords="14,142.62,262.14,337.98,7.86;14,151.52,273.10,329.07,7.86;14,151.52,284.06,123.58,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,348.65,262.14,131.94,7.86;14,151.52,273.10,311.41,7.86">Classifying German Animal Experiment Summaries with Multi-lingual BERT at CLEF eHealth 2019 Task 1</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sänger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kittner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Leser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,151.52,284.06,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,295.02,337.97,7.86;14,151.52,305.98,329.07,7.86;14,151.52,316.91,329.07,7.89;14,151.52,327.89,25.60,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,360.63,295.02,119.96,7.86;14,151.52,305.98,329.07,7.86;14,151.52,316.93,32.40,7.86">Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Shickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bihorac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rashidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,191.81,316.93,214.70,7.86">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1589" to="1604" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,338.85,337.97,7.86;14,151.52,349.81,329.07,7.86;14,151.52,360.74,261.80,7.89" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,307.76,338.85,172.83,7.86;14,151.52,349.81,90.28,7.86">Enhancing clinical concept extraction with contextual embeddings</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocz096</idno>
		<ptr target="https://doi.org/10.1093/jamia/ocz096" />
	</analytic>
	<monogr>
		<title level="j" coord="14,249.43,349.81,231.16,7.86">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1297" to="1304" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,371.73,337.97,7.86;14,151.52,382.69,329.07,7.86;14,151.52,393.62,329.07,7.89;14,151.52,404.61,173.12,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="14,454.21,371.73,26.38,7.86;14,151.52,382.69,324.86,7.86">A systematic literature review of automated clinical coding and classification systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Stanfill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Fenton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Jenders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<idno type="DOI">10.1136/jamia.2009.001024</idno>
		<ptr target="https://doi.org/10.1136/jamia.2009.001024" />
	</analytic>
	<monogr>
		<title level="j" coord="14,151.52,393.65,233.32,7.86">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="646" to="651" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,415.56,337.98,7.86;14,151.52,426.52,329.07,7.86;14,151.52,437.48,216.63,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="14,312.34,415.56,168.25,7.86;14,151.52,426.52,40.24,7.86">How to Fine-Tune BERT for Text Classification?</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,211.52,426.52,190.86,7.86">Chinese Computational Linguistics. CCL 2019</title>
		<title level="s" coord="14,410.96,426.52,69.63,7.86;14,151.52,437.48,71.32,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11856</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,448.44,337.97,7.86;14,151.52,459.40,329.07,7.86;14,151.52,470.36,329.07,7.86;14,151.52,481.32,233.46,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,225.93,459.40,96.79,7.86">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="14,430.47,470.36,50.12,7.86;14,151.52,481.32,157.25,7.86">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
