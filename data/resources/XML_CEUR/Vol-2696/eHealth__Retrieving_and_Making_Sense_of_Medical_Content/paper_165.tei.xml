<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.58,115.96,302.18,12.62;1,166.26,135.80,282.83,10.52;1,155.44,149.74,304.48,10.52">IXA-AAA at CLEF eHealth 2020 CodiEsp</title>
				<funder ref="#_bwZtGVw">
					<orgName type="full">Spanish Ministry of Science and Technology</orgName>
				</funder>
				<funder ref="#_V3HzTWe">
					<orgName type="full">Basque Government</orgName>
				</funder>
				<funder ref="#_bUmpBrW #_PUQv8bd">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,187.75,185.77,66.31,8.74"><forename type="first">Alberto</forename><surname>Blanco</surname></persName>
							<email>alberto.blanco@ehu.eus</email>
							<affiliation key="aff0">
								<orgName type="department">HiTZ Center -Ixa</orgName>
								<orgName type="institution">University of the Basque Country UPV/EHU</orgName>
								<address>
									<addrLine>Manuel Lardizabal 1</addrLine>
									<postCode>20080</postCode>
									<settlement>Donostia</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.19,185.77,52.48,8.74"><forename type="first">Alicia</forename><surname>PÃ©rez</surname></persName>
							<email>alicia.perez@ehu.eus</email>
							<affiliation key="aff0">
								<orgName type="department">HiTZ Center -Ixa</orgName>
								<orgName type="institution">University of the Basque Country UPV/EHU</orgName>
								<address>
									<addrLine>Manuel Lardizabal 1</addrLine>
									<postCode>20080</postCode>
									<settlement>Donostia</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.60,185.77,71.53,8.74"><forename type="first">Arantza</forename><surname>Casillas</surname></persName>
							<email>arantza.casillas@ehu.eus</email>
							<affiliation key="aff0">
								<orgName type="department">HiTZ Center -Ixa</orgName>
								<orgName type="institution">University of the Basque Country UPV/EHU</orgName>
								<address>
									<addrLine>Manuel Lardizabal 1</addrLine>
									<postCode>20080</postCode>
									<settlement>Donostia</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.58,115.96,302.18,12.62;1,166.26,135.80,282.83,10.52;1,155.44,149.74,304.48,10.52">IXA-AAA at CLEF eHealth 2020 CodiEsp</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">09B6FDF63084261ED846F0B9B09F459A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CLEF</term>
					<term>CodiEsp</term>
					<term>Clinical records</term>
					<term>Similarity Match</term>
					<term>Multi-label classifier</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>These working notes present the participation of the IXA-AAA team on the CodiEsp Track, as part of the CLEF 2020. The track is about automatic coding of clinical records according to the International Classification of Diseases 10th revision (ICD-10). There are three sub-tasks: CodiEsp-D, CodiEsp-P and CodiEsp-X. The two main tasks, CodiEsp-D and CodiEsp-P, aim to develop systems able to automatically classify clinical texts according to the ICD-10, respectively for diagnostics and procedures. CodiEsp-X, by contrast, is an exploratory sub-task within the framework of Explainable AI in which the goal is to detect the text fragment that motivates the presence of the ICD code. For the IXA-AAA team participation, we have developed several systems to cope with the three sub-tasks, including tree-based multi-label classifiers, similarity match strategies, and ensemble models. For the similarity match, we have explored several approaches and algorithms from string edit distances as Levenshtein to dense representation with Transformers grounded BERT models. Our best results overall are achieved by the combination of models, with a MAP of 69.8% for CodiEsp-D and 48.1% for CodiEsp-P. Regarding the exploratory task, CodiEsp-X, our best coder achieve a micro F1-Score of 30.6%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Here we gather the contribution of IXA-AAA team in the CodiEsp Track from the eHealth CLEF 2020 -Multilingual Information Extraction <ref type="bibr" coords="1,408.51,568.84,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="1,424.01,568.84,11.62,8.74" target="#b14">15]</ref>. The task consists in the automatic classification of clinical notes according to the ICD-10 codes, considering both procedures and diagnosis. The track contains three independent sub-tasks, two of them considered as the main tasks and the other regarded as exploratory. The main tasks require systems able to perform ICD assignments (diagnosis and procedures) to a given clinical note. In the exploratory task, the systems must also submit the text that motivated each code assigned. Therefore, the three sub-tasks are a) Diagnosis Coding main (CodiEsp-D): automatic ICD-10-CM (i.e. diagnosis) code assignment. b) Procedure Coding main (CodiEsp-P): automatic ICD-10-PCS (i.e. procedure) code assignment. c) Explainable AI exploratory (CodiEsp-X): automatic ICD-10-CM and ICD-10-PCS code assignment and text position for reference designation.</p><p>These tasks present several challenges regarding the text, multi-label and ICD classification domain. The documents, written in Spanish, come from a set of clinical case studies showing properties of both, the biomedical and medical literature, as well as clinical records. Moreover, they cover a variety of medical topics, including oncology, urology, cardiology, pneumology or infectious diseases, which increases both the quantity and the diversity of the ICD codes present in the dataset. Each clinical note can have several diagnoses or procedures and, therefore, we face a multi-label classification task. The text multi-label classification alone is an open challenge in the machine learning field but, conjugating this with the large label-set yielded by the ICD-10 codes, with the low frequency and with imbalance of labels, then, the task involves overcoming multiple and varied barriers. Moreover, we are confronted with a zero-shot learning paradigm, where the clinical cases from the different data partitions (train, dev, test) have non-overlapping label-sets. Regarding the exploratory task, the identification of the text position reference for a given code is not trivial since the non-standard medical language in the text can differ heavily from the standard terms in the ICD. Besides, apart from the continuous references, there are also discontinuous references (i.e. references with several parts distributed along the clinical note). In practical terms, the evaluation of the discontinuous references is carried out taking the beginning of the first fragment and the end of the last.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The automatic classification of medical records according to the ICD is an active field of research with a presence on shared tasks competitions <ref type="bibr" coords="2,408.57,512.40,15.50,8.74" target="#b18">[19]</ref> and Natural Language Processing literature <ref type="bibr" coords="2,276.21,524.35,14.61,8.74" target="#b20">[21]</ref>. Through the years, numerous techniques and systems have been developed to solve these tasks, such as Dictionary lookups <ref type="bibr" coords="2,134.77,548.26,9.96,8.74" target="#b3">[4]</ref>, statistical models like Topic Modeling <ref type="bibr" coords="2,325.56,548.26,14.61,8.74" target="#b17">[18]</ref>, machine learning models and, lately, Deep Learning models <ref type="bibr" coords="2,264.64,560.22,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,275.16,560.22,7.01,8.74" target="#b1">2]</ref>. <ref type="bibr" coords="2,149.71,572.43,15.50,8.74" target="#b20">[21]</ref> indicates that it is troublesome to evaluate the advances in the field since neither the models nor the evaluation results are generally comparables across related works. Hence, it is a significant milestone to establish standard datasets along with evaluation systems like in this and in past CLEF eHealth editions since 2012 <ref type="bibr" coords="2,181.77,620.25,9.96,8.74" target="#b7">[8]</ref>. In 2018, the sixth annual edition of the CLEF eHealth evaluation lab <ref type="bibr" coords="2,151.01,632.21,14.61,8.74" target="#b21">[22]</ref>, the organizers bestowed a multilingual information extraction lab, with ICD-10 coding of death certificates as the main task. The dataset contained freetext descriptions in 5 languages of causes of death as reported by practitioners in the standardized causes of death forms, and the teams must extract ICD-10 codes from the raw lines of death certificate text. The best system was provided by the IxaMed team <ref type="bibr" coords="3,196.08,142.90,9.96,8.74" target="#b2">[3]</ref>, which cast the problem following a sequence-to-sequence prediction paradigm. The authors leveraged only the organizers-provided datasets, namely, ICD-10 dictionaries and the different sets of death reports texts with their corresponding ICD codes, which fed to an encoder-decoder model were able to deliver high-quality, while language-independent, results. On the last year edition of the CLEF eHealth evaluation lab <ref type="bibr" coords="3,355.76,202.68,15.50,8.74" target="#b11">[12]</ref> the main task consisted of the classification of non-technical summaries of German animal experiments according to the ICD-10 codes. Although the dataset consisted of veterinary texts, it still comprised a biomedical lexicon, which combined with the use of ICD codes, brought a narrowly related task. The WBI team <ref type="bibr" coords="3,395.94,250.50,15.50,8.74" target="#b19">[20]</ref> approached the task as a multi-label classification problem and leveraged the BERT Multilingual model, extended by an output layer which produced the individual probabilities for each possible ICD-10 code. With this setup, the authors succeeded to get the best results on both Precision and F-Measure metrics. However, the MLT-DFKI team <ref type="bibr" coords="3,160.54,310.28,10.52,8.74" target="#b1">[2]</ref> managed to improve their recall. While the authors also employed a BERT-based model, in this case, they applied its biomedical variant BioBERT <ref type="bibr" coords="3,134.77,334.19,14.61,8.74" target="#b12">[13]</ref>, in conjunction with an automatic translation system from German to English, (as the BioBERT model is trained based on the English BERT model, instead of the multilingual). It is worth noting that the WBI team also made use of extra training data from the German Clinical Trials Register and tried ensemble techniques to improve the overall performance. On this year edition, the clinical notes yield longer texts while preserving the challenges related to the clinical language, the non-standard terms and the ICD-10 large label-set. Besides, the Explainable-AI-related assignment brings a new challenge regarding to the interpretability of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Materials</head><p>For the resolution of the three sub-tasks, the organization has provided both main and additional data, and we have employed extra additional resources. The main data consists of 1,000 clinical studies which are coded manually according to the ICD-10 by practising physicians and clinical documentalists. Table <ref type="table" coords="4,448.01,186.23,4.98,8.74" target="#tab_0">1</ref> shows a brief quantitative description of the main datasets regarding the texts. Note that the row with the test data is in fact 'test + background' and that there is a gap between the number of clinical studies which are coded manually (1,000) and the full number of available documents <ref type="bibr" coords="4,384.37,403.57,12.45,8.74" target="#b2">(3,</ref><ref type="bibr" coords="4,396.82,403.57,16.60,8.74">751)</ref>. The reason is that the test set is intentionally inflated with a so-called 'background' test with â¼2.700 documents, added to the real test documents (250) to prevent manual predictions. The systems will only be evaluated on the 250 test set documents, but since we cannot discern, the statistics shown are for the test and background sets in conjunction. Regarding the 1,000 coded documents, the partitions split proportion is 50/25/25. Including the background set, there is a total of 71,190 sentences, 1,370,047 words from 105,038 unique words leading to 354Â±210 words mean Â± standard deviation length documents. It is relevant to mention that the texts comprise biomedical, medical literature and clinical records, involving a variety of medical topics such as oncology, urology, cardiology, pneumology or infectious diseases. Hence the variety of technical lexicon is increased, which increases the challenge. It is relevant to note that relative to the OOVs, the percentage of OOV words in the dev set is 47.81% while it is 80.47% in the test + background set, meaning that both sets do not follow a similar pattern concerning the lexical distribution (note that since it also includes the background set we cannot claim that this divergence prevails considering only the test set).</p><p>Regarding the labels, namely the ICD-10 codes, there are 10,711 annotated codes with 2,925 unique ones from both the ICD-10-CM (diagnostics) and ICD-10-PCS (procedures). Table <ref type="table" coords="4,259.02,632.21,4.98,8.74" target="#tab_1">2</ref> presents an overview of the statistics of the train and dev partitions (which were the available annotated partitions of the corpus before the submission, and consequently the data used for training the models). One can see that all the codes from train + dev set only represent a small percentage of the full ICD-10 codes (98,287 for ICD-10-CM and 87,169 for ICD-10-PCS), but still portray a large label-set, especially taking into account the low representativeness of some codes (i.e. only 200 CM labels appears on 1% or more of the clinical cases from the train set) and the extreme imbalance. But more important is the question of the disjoint codes among sets, and especially, unseen codes in the test set. In fact, there may be unseen codes in the test set, and in general, there are codes which only appear on one partition, since the partitions were obtained via a random split of training, dev and test (i.e, there are 1,036 CM and 352 PCS labels on dev set not seen on train set). This question leads to a zero-shot learning environment where a standard classifier will make predictions solely among the seen codes on the training phase, and therefore, fail to predict the unseen codes.</p><p>The CodiEsp-X sub-task requires to detect the text reference position, so the available corpus also brings the start and end position noted. Also, keep in mind that there are continuous and discontinuous codes, the formers implies that all the words related to the code appear sequentially in the text, while for the latter there are several fragments of texts related to the code. Nevertheless, in both cases, the way to evaluate the detection as correct is to give the start position of the first (or unique) fragment and end position of the last (or unique) fragment, regardless of the number of fragments. The organization also provides additional resources, and from those, we have used the Spanish abstracts from Lilacs and Ibecs with ICD-10 codes, to expand the dictionary of ICD and nonstandard descriptions. The in-house resources employed by our team consist of additional non-standard term descriptions for some ICDs. Moreover, we have applied a Medical Named Entity Recognition (NER) system to extract medical terms such as diagnostic and procedure terms, and to reduce noisy words. This alternative representation of texts has helped us with the augmentation of the train and dev sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>The systems developed to deal with each subtask are of two different kinds. First, we have applied a tree-based multi-label classifier based on gradient boosting machines <ref type="bibr" coords="5,177.75,656.12,9.96,8.74" target="#b8">[9]</ref>, to cope with CodiEsp-D and CodiEsp-P, presented in section 4.1.</p><p>Furthermore, we have developed a coder based on string similarities, which can cope with CodiEsp-D and CodiEsp-P sub-tasks, but also CodiEsp-X, introduced in section 4.2. Besides, we combined the outputs from the classifiers and the coders to improve the overall results.</p><p>Regarding the text representation, we have applied a Medical Named Entity Recognition (NER) tool to extract medical entities from the raw texts. Particularly, it classifies each word as 'Disease', 'Procedure', 'Drug', 'Part of the body' or 'Others'. Taking that classification, we have extracted three alternate representations of the raw clinical notes following two strategies; i) Medical terms (NER Med): Aims for noise removal, preserving only those words not classified as 'Others' and ii) Diagnostics (NER D) or Procedures (NER P): Preserve only the words marked as 'Disease' or 'Procedure', accordingly. These alternate representations can also be concatenated to the raw texts, as a data augmentation technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tree-based multi-label classifier: Gradient Boosting Machines</head><p>The Gradient Boosting Machine or GBM is an ensemble classifier. Ensemble classifiers rely on the combination of several base-classifiers to make a final prediction. Specifically, the boosting technique consists in training several classifiers sequentially, in a manner that each classifier learns from the errors made by the previous ones. The objective of each individual classifier is to reduce the loss function, in this case, binary cross-entropy (CE), given by expression <ref type="bibr" coords="6,436.55,406.57,11.62,8.74" target="#b0">(1)</ref>, where log is the natural logarithm, y is the binary label and p is the prediction or membership probability to the given class.</p><formula xml:id="formula_0" coords="6,228.25,462.60,252.34,8.74">CE = -[y log p + (1 -y) log (1 -p)]<label>(1)</label></formula><p>The optimization of the function uses a gradient descent algorithm to minimize the loss when adding new classifiers <ref type="bibr" coords="6,331.32,500.70,9.96,8.74" target="#b4">[5]</ref>. To cope with the multi-label paradigm, we applied the one-versus-rest approach, in which as many binary classifiers as present labels are trained. Thus, for the i-th binary classifier, label i is treated as the positive class and all the remaining labels as negative.</p><p>The training procedure is then followed by a post-processing stage where the optimal threshold must be found. However, note that the evaluation metric to be applied in this task is the Mean Average Precision (MAP) well suited for candidate-ranking. For consistency with this metric, the output from our system is a ranking of all the possible labels ordered by probability. That is, the system should provide all the labels (1, 767 for CM and 563 for PCS) even though from the data analysis (in Table <ref type="table" coords="6,278.23,620.25,4.43,8.74" target="#tab_1">2</ref>) one could expect the system to provide just around 11.3 labels in CM and 3.6 in PCS. More about this question is discussed in section 6. We have applied the XGBoost implementation <ref type="bibr" coords="6,398.58,644.16,10.52,8.74" target="#b5">[6]</ref> with the Scikitlearn <ref type="bibr" coords="6,159.70,656.12,15.50,8.74" target="#b16">[17]</ref> wrapper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Similarity Match</head><p>Our Similarity Match algorithms set their foundation on the similarity between two strings. For this work, we have implemented two variations that, although they follow this same approach, differ severely on the core of the algorithm, i.e. the computation of the similarity itself. We have named them, respectively PartialMatch and BERTMatch coders.</p><p>First, let us describe the shared logic behind the two coders using our specific use-case as an example. On the one hand, we get a clinical record with several sentences. Naturally, in each sentence one or more terms associated with a given ICD code can appear. As an example, here we are a sentence from the corpus: Sentence: 'Realizamos frotis sanguÃ­neo para justificar la causa de la anemia y trombocitopenia' On the other hand, there is an ICD code dictionary, which relates each code to one or more arbitrary-length description strings (either standard diagnostic terms from the ICD or gold mentions from the corpus). An entry from the ICD dictionary, as shown below, conveys the ICD code (D69.6) and one or more standard ways to refer to that code (e.g. plaquetopenia, tombocitopenia, trombocitopenia, trombopenia). Dictionary entry: D69.6: plaquetopenia tombocitopenia trombocitopenia trombopenia</p><p>The dictionary can include a variety of terms including standard and nonstandard, single-word descriptions and even phrases frequently associated with the code like 'enfermedad de graves basedow' for the 'E05.00' code, which differs harshly from the standard ICD description ('tirotoxicosis con bocio difuso sin crisis tirotoxica ni tormenta tiroidea').</p><p>Next, a Similarity Match algorithm will cycle through all the associated strings of each ICD code, and through all the texts, computing the similarity between pairs of standard and non-standard terms and text fragments. The texts fragments are extracted with a sliding window in which the length is set to the number of words of the current ICD description. Following the example, the process to find the likelihood of the D96.6 code on the sample text is as follows: compute the similarity between the 'plaquetopenia' term and each word of the target text, and store the maximum value. Then, repeat for the rest of the associated terms (tombocitopenia, trombocitopenia. . . ), and finally get the overall maximum value. As the similarity metric is normalized in the [0, 1] range, it can be interpreted as a membership probability for each code on each clinical record. In the case of CodiEsp-X, which requires identifying the range, it is only necessary to search for the range of the text fragment that leads to the maximum similarity.</p><p>The similarity computation is then what differentiates the two developed coders. Let us describe a similarity function as one that for a given pair of strings as input generates a similarity coefficient, as described in <ref type="bibr" coords="8,383.86,207.18,11.62,8.74" target="#b1">(2)</ref>, where s 1 and s 2 is a pair of strings, sim is the similarity coefficient normalized in a range [0, 1] and Î£ is the vocabulary. Regarding the interpretation, sim(s 1 , s 2 ) = 1 means that s 1 and s 2 are the same string while sim(s 1 , s 2 ) = 0 means that are completely different.</p><formula xml:id="formula_1" coords="8,241.62,283.85,238.96,10.81">sim : Î£ * Ã Î£ * -â [0, 1]<label>(2)</label></formula><formula xml:id="formula_2" coords="8,274.34,300.86,99.39,9.65">(s 1 , s 2 ) sim(s 1 , s 2 )</formula><p>On this basis, the Partial Match coder applies a regular string similarity algorithm, such as the Jaro Winkler <ref type="bibr" coords="8,278.99,343.74,15.50,8.74" target="#b23">[24]</ref> or Levenshtein Distance <ref type="bibr" coords="8,406.51,343.74,15.50,8.74" target="#b13">[14]</ref> (and we also enable an 'Auto' configuration that dynamically choose one or the other based on the length of the given term).</p><p>On the other hand, the BERTMatch coder leverages the BERT Multilingual model <ref type="bibr" coords="8,163.80,396.06,10.52,8.74" target="#b6">[7]</ref> to come out with a similarity between strings, the process is as follows. First, for each string (s i ) a dense representation (v(s i ) â R n with n = 768) is extracted from the representation of the texts generated internally by the BERT model. Then, the similarity between the vectors v(s 1 ) and v(s 2 ) is computed via the Cosine Similarity <ref type="bibr" coords="8,249.06,443.88,14.61,8.74" target="#b10">[11]</ref>: sim BERT M atch (s 1 , s 2 ) = cos(v(s 1 ), v(s 2 )). Note that the BERTMatch algorithm is far more computationally demanding than PartialMatch, hence, it was not applied to the test set predictions (indeed, the test set is, curiously enough, the largest set, as shown in Table <ref type="table" coords="8,411.17,479.74,3.87,8.74" target="#tab_1">2</ref>).</p><p>Following the example, the matching score between the word 'trombocitopenia' from the model sentence and the word 'tombocitopenia' from the dictionary entries gives a similarity value (in the range [0.0, 1.0], being 0.0 completely different words and 1.0 exactly the same word) of 0.98 with the JaroWinkler (as it is almost the same word but with a slight spelling mistake) but only 0.46 with the BERT embeddings. However, the score between 'trombocitopenia' and 'plaquetopenia', is as low as 0.57 with JaroWinkler (although they are synonyms) and 0.78 with the BERT embeddings, a much more appropriate score since both words mean the same thing.</p><p>Finally, it should be noted that we have developed all the classifiers and coders in a way so that their outputs can be combined. Combining is done using simple aggregation functions such as the mean, minimum, or maximum over the similarity scores or probabilities, which is a straightforward but practical way to improve results by ensembling strategies.</p><p>The results from the submissions, on the test set, as reported by the CLEF organizers for the CodiEsp-D/P and X sub-tasks, are presented on this section. Table <ref type="table" coords="9,163.08,165.96,3.87,8.74">3</ref>, 4 and 5 show our team submission results, including the predictions from the test set (250 docs) (and excluding the 2,751 background set docs). For the official results, only predictions for test files and labels from train and dev sets were considered.</p><p>Note that during the development phase of the challenge, it was reported that there were codes present in the test set that were not present in the train and validation sets, but only after the submission phase was reported that these codes would not be taken into account for the evaluation of the results. Therefore, the systems were developed considering that all metrics would be computed taking into account also the predictions for the codes present only in the test set, which could have had significant harmful effects on the results.</p><p>The official metrics for the subtasks are MAP for the CodiEsp-D/P and F-Score for the CodiEsp-X, but other metrics were also computed and reported. Specifically, MAP@30, Precision and Recall for CodiEsp-D/P and Precision and Recall for CodiEsp-X. Finally, in CodiEsp-D, Precision, Recall and F-score were also computed for categories, considering a category the first three digits of an ICD-10-CM code. (I.e. codes P96.5 and P96.89 are mapped to P96). Therefore, systems that predict the code P96.89 for a document whose correct code is P96 would be correct. In CodiEsp-P, Precision, Recall and F-score are also computed for categories. In this case, considering categories the first four digits of the code.</p><p>In relation with the name of the columns, M stands for MAP; M30 stands for MAP@30; P stands for Precision; R stands for Recall; and F1 stands for F1-Score. The T suffix stands for Test and the C suffix stands for Category. Those columns with the Test suffix show the results evaluated only on the labels from the train and dev sets (without the only test set labels), while those with the Category suffix show the results considering the category labels.</p><p>For all sub-tasks, each submitted run is the result of applying different techniques. Table <ref type="table" coords="9,195.36,488.75,4.98,8.74">3</ref> presents the results from the CodiEsp-D sub-task, and each run corresponds to the following setup: 1) XGBoost classifier, trained with documents and diagnostics labels from train and dev sets, augmenting the clinical texts with the outputs of the NER Med and the NER D. 2) Partial Match coder with the Jaro Winkler similarity algorithm and predicting only the diagnostics labels present on the train and dev sets. 3) The combination of the outputs from 1) and 2).</p><p>Regarding the official metric for this task, namely the MAP, or more precisely, the MAP evaluated only on the test set (M-T column), the XGBoost classifier prevails over the Partial Match strategy with 63.8 and 57.1 points respectively. However, the best result comes from the run3, with the combination of both methods, leading to 69.8 MAP points.</p><p>Table <ref type="table" coords="9,176.90,632.21,4.98,8.74">4</ref> presents the results from the CodiEsp-P sub-task, and each run corresponds to the following setup: 1) XGBoost classifier, trained with documents and procedure labels from train and dev sets, augmenting the clinical texts with Run M M-T M30 M30-T P R F1 P-T R-T F1-T P-C R-C F1-C run1 0.543 0.638 0.529 0.622 0.004 0.858 0.009 0.004 1.0 0.009 0.01 0.968 0.021 run2 0.485 0.571 0.469 0.553 0.004 0.858 0.009 0.004 1.0 0.009 0.01 0.968 0.021 run3 0.593 0.698 0.578 0.681 0.004 0.858 0.009 0.004 1.0 0.009 0.01 0.968 0.021 Table <ref type="table" coords="10,164.82,162.74,4.13,7.89">3</ref>. Submission results for the CodiEsp-D sub-task as reported by the CLEF organization.</p><p>the outputs of the NER Med and the NER P. 2) Partial Match coder with the Jaro Winkler similarity algorithm and predicting only the procedure labels present on the train and dev sets. 3) The combination of the outputs from 1) and 2).</p><formula xml:id="formula_3" coords="10,134.37,289.07,346.61,19.25">Run M M-T M30 M30-T P R F1 P-T R-T F1-T P-C R-C F1-C<label>run1</label></formula><p>0.412 0.46 0.395 0.441 0.004 0.825 0.008 0.004 1.0 0.008 0.005 0.857 0.01 run2 0.362 0.414 0.339 0.389 0.004 0.825 0.008 0.004 1.0 0.008 0.005 0.857 0.01 run3 0.425 0.481 0.401 0.455 0.004 0.825 0.008 0.004 1.0 0.008 0.005 0.857 0.01 Table <ref type="table" coords="10,164.89,334.06,4.13,7.89">4</ref>. Submission results for the CodiEsp-P sub-task as reported by the CLEF organization.</p><p>Similarly to the D sub-task, the best M-T result from single models is achieved by the XGBoost classifier, with 46.0 points, while the Partial Match strategy stays about 5 points below, with 41.4 points. Once again, the combination of both methods manages to improve individual performance, with a solid 48.1 MAP points.</p><p>Table <ref type="table" coords="10,178.62,449.43,4.98,8.74">5</ref> presents the results from the CodiEsp-X sub-task, and each run corresponds to the following setup: 1) Partial Match coder with the Jaro Winkler similarity algorithm. 2) Partial Match coder with the Auto configuration for the similarity algorithm. 3) Partial Match coder with the Levenshtein similarity algorithm. For each setup, predicting only the diagnostics and procedure labels present on the train and dev sets.</p><p>Run P R F1 P-T R-T F1-T run1 0.043 0.318 0.075 0.043 0.374 0.076 run2 0.144 0.301 0.195 0.144 0.354 0.205 run3 0.288 0.278 0.283 0.288 0.327 0.306 Table <ref type="table" coords="10,164.82,588.49,4.13,7.89">5</ref>. Submission results for the CodiEsp-X sub-task as reported by the CLEF organization.</p><p>For the CodiEsp-X task, the official metric is the F1-Score, particularly, the F1-Score evaluated only on the test set (F1-T column). We can see that the Jaro Winkler algorithm, which dominated on the D/P tasks, here is, curiously, the worst-performing one with 7.6 points. The 'Auto' configuration, that mixes the Jaro Winkler and Levenshtein algorithms, gets an increased 20.5 points. Finally, the Levenshtein algorithm improves that mark by approximately 10 points, with a solid F1 score of 30.6, which is our best result overall for the CodiEsp-X task. Although we have not been able to apply the Similarity Match algorithm based on BERT embeddings on the test + background set for computational reasons, our experiments in the dev set suggest that the BERTMatch algorithm is able to overcome the Jaro Winkler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>For sub-tasks CodiEsp-D and CodiEsp-P, the code predictions must be ranked, this is, generating a list of possible codes ordered by confidence. The main metric for evaluating these outputs is the Mean Average Precision or MAP. It is computed iteratively; First, precision is computed considering only the first ranked code, then, it is computed considering the first two codes, etc. Finally, precision values are averaged over the number of gold codes. The organizers claim that the MAP is the most standard ranking metric among the TREC community, and it has shown good discrimination and stability <ref type="bibr" coords="11,334.09,356.95,14.61,8.74" target="#b15">[16]</ref>. However, the way to exploit the MAP metric is to output all the considered codes, without discrimination, and ranked by confidence. In other words, ranking all the considered ICD codes and not establishing a threshold for a discrete "Yes/No" decision. Our scripts yield this output because it is the way to maximize the MAP metric and face the competition but we believe that this way of evaluating might not be the most desirable, since the notion of an "automatic classifier" that "decides" whether or not a code belongs in a given document is shaded. We feel that instead of ranking all the labels available within the ICD, the system should just limit the output to a subset of labels that correspond to the document. Nevertheless, MAP metric favours a rank over all the labels above a rank of a sub-set of labels. In brief, the ability to state whether a code is present or not in the in the given medical record is not regarded by the MAP metric. Accordingly, a weakness of this task is the need of a threshold for accepting and discarding codes given the ranked list. By contrast, the CodiEsp-X sub-task does not present this drawback, since the main evaluation metric is the micro F-Score, and therefore each predicted code that does not belong to the ground truth carry a penalty.</p><p>In the CodiEsp-X sub-task, there are some codification errors on which the assigned ICD code and the text which has motivated the assignation of code mismatches involving those errors. We have found slight differences with the main ICD block (the first three digits of the ICD) remain while the modifiers (other digits) vary. However, the evaluation entails the F-Score of the full-code, without considering the relationship between codes according the hierarchy. This type of errors (confounding two closely related diseases) penalize as any other error (i.e. confounding un-related diseases). For example, for the record with ID 'S0211-69952011000500011-3', the label 'K85.10 -Billiary acute pancreatitis without necrosis or infection' is assigned, motivated by the following text fragment: 'acute non-lithiasic pancreatitis'. The mistake is that the record elucidates that it is 'non-lithiasic pancreatitis', but the code corresponds to that of 'lithiasic' or 'biliary' pancreatitis. The label assigned by our system is 'K85.90 -Acute pancreatitis without necrosis or infection, unspecified', and although we cannot claim that the K85.90 is the correct label, it seems that is, at least, more accurate than K85.10, but it is counted as an error.</p><p>On document 'S2254-28842013000300009-1' we have the following text fragment: 'Mujer de 73 aÃ±os de edad con antecedentes personales de [. . .], histerectomÃ­a por prolapso uterino y [. . .] '. Our system gives a confidence of 98.5% to the 'Z90.710 -Acquired absence of both cervix and uterus' code, which describes a hysterectomy (the surgical removal of the uterus, which may also include the cervix and other surrounding structures <ref type="bibr" coords="12,398.62,262.66,14.75,8.74" target="#b22">[23]</ref>). The Z90.710 code is considered as incorrect, and there is no other code that matches the 'histerectomÃ­a' word (though it is coded with 'N81.2 -Incomplete uterovaginal prolapse' due to 'prolapso uterino', which is the cause of the hysterectomy, and seems correctly coded). There are abundant examples of this type of missing codes in the ground truth that, unfairly lead to False Positives. Accordingly, we believe that the evaluation results of these tasks should be regarded with prudence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Concluding remarks and future work</head><p>The CodiEsp Track proposes two different sub-tasks based on the classification of medical texts according to the ICD-10 CM and PCS codes. The CodiEsp-D/P sub-tasks aim to the automatic classification of diagnostic and procedures codes, while the CodiEsp-X sub-task strives to bring explainability to the challenge.</p><p>We have developed several systems to cope with these tasks, two strategies with five different algorithms for the D and P sub-tasks, and one strategy with four algorithms capable of producing explainable results, in conjunction with the ability of ensembling the distinct models, enhanced by techniques that yield alternate representations of the medical texts with tools as Medical NER, while experimenting also with different label-sets.</p><p>Regarding the D and P sub-tasks, the similarity match based algorithms perform better, on average, than the multi-label classifiers. However, we conclude that the NER techniques for enriching the medical text inputs for the classifiers accomplish to improve the performance of the classifiers, resulting in the best overall results being achieved with the combination of both methods.</p><p>It seems that the best similarity algorithm for the diagnostics and procedures individually is the Jaro Winkler, while it is the Levenshtein for the CodiEsp-X sub-task as a whole. We have not delved in this topic, but might be related to divergences among the average length of diagnostic and procedure terms.</p><p>The similarity match algorithm based on the BERT dense representations appears to be weaker than the traditional approaches but shows promising results when applying it to the extraction of diagnostic and procedure terms boundaries.</p><p>The consideration of the full ICD-10 codes instead of those from the train set degrades the performance. It can be observed in every sub-task, and we believe that this is due to the large number of extra codes considered with respect to the actual number of codes that only appear in the dev set. Improving NER and looking for combined match approaches might lead to further improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,235.50,345.83,101.89"><head>Table 1 .</head><label>1</label><figDesc>Quantitative description of the main dataset by partition. Number of documents, sentences per doc, total words, average length in number of words, vocabulary size (unique words) and Out-of-Vocabulary (OOV) words for dev and test sets are given.</figDesc><table coords="4,162.08,235.50,291.20,58.10"><row><cell>Partition</cell><cell>docs</cell><cell cols="3">sent/doc words words/doc vocab OOV</cell></row><row><cell>Train</cell><cell>500</cell><cell cols="2">17.62 172,533</cell><cell>345Â±162 26,298 N/A</cell></row><row><cell>Dev</cell><cell>250</cell><cell>18.45</cell><cell>86,913</cell><cell>347Â±165 16,768 8,016</cell></row><row><cell>Test</cell><cell>250 + 2,751</cell><cell cols="2">19.25 1,110,601</cell><cell>370Â±304 92,900 74,753</cell></row><row><cell>All</cell><cell>3,751</cell><cell cols="2">18.44 1,370,047</cell><cell>354Â±210 105,038 N/A</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,144.60,117.75,326.15,85.75"><head>Table 2 .</head><label>2</label><figDesc>partition label-set label count unique labels cardinality max imb. ratio Statistical description of the labels of the main dataset by partition</figDesc><table coords="5,144.60,129.14,326.15,63.45"><row><cell>Train</cell><cell>CM PCS</cell><cell>5,661 1,550</cell><cell>1,767 563</cell><cell>11.3 3.6</cell><cell>0.009 0.015</cell></row><row><cell>Dev</cell><cell>CM PCS</cell><cell>2,683 817</cell><cell>1,158 375</cell><cell>10.7 3.7</cell><cell>0.02 0.025</cell></row><row><cell>All</cell><cell>CM PCS</cell><cell>7,211 3,500</cell><cell>2,196 729</cell><cell>11.0 3.6</cell><cell>0.014 0.02</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgments</head><p>This work was partially supported by the <rs type="funder">Spanish Ministry of Science and Technology</rs> (<rs type="grantNumber">PAT-MED PID2019-106942RB-C31</rs>) and by the <rs type="funder">Basque Government</rs> (<rs type="grantNumber">Elkartek KK-2019/00045</rs>, <rs type="grantNumber">IXA IT-1343-19</rs>, Predoctoral Grant <rs type="grantNumber">PRE-2019-1-0158</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bwZtGVw">
					<idno type="grant-number">PAT-MED PID2019-106942RB-C31</idno>
				</org>
				<org type="funding" xml:id="_V3HzTWe">
					<idno type="grant-number">Elkartek KK-2019/00045</idno>
				</org>
				<org type="funding" xml:id="_bUmpBrW">
					<idno type="grant-number">IXA IT-1343-19</idno>
				</org>
				<org type="funding" xml:id="_PUQv8bd">
					<idno type="grant-number">PRE-2019-1-0158</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.96,323.66,337.63,7.86;13,151.52,334.62,329.07,7.86;13,151.52,345.55,99.51,7.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,380.67,323.66,99.92,7.86;13,151.52,334.62,268.68,7.86">Icd-10 coding of spanish electronic discharge summaries: An extreme classification problem</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Almagro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Unanue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Fresno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Montalvo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,428.30,334.62,52.30,7.86">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="100073" to="100083" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,357.02,337.63,7.86;13,151.52,367.98,329.07,7.86;13,151.52,378.94,145.88,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="13,177.92,367.98,302.67,7.86;13,151.52,378.94,15.38,7.86">Mlt-dfki at clef ehealth 2019: Multi-label classification of icd-10 codes with bert</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Dunfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vechkaeva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Wixted</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
	<note>Working Notes</note>
</biblStruct>

<biblStruct coords="13,142.96,390.38,337.63,7.86;13,151.52,401.34,329.07,7.86;13,151.52,412.30,329.07,7.86;13,151.52,423.26,91.44,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,389.28,401.34,91.31,7.86;13,151.52,412.30,252.88,7.86">Ixamed at clef ehealth 2018 task 1: Icd10 coding with a sequence-to-sequence approach</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Atutxa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Casillas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ezeiza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Fresno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gojenola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>MartÃ­nez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">O</forename><surname>Anchordoqui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Perez-De ViÃ±aspre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,424.89,412.30,55.70,7.86;13,151.52,423.26,40.20,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,434.71,337.64,7.86;13,151.52,445.66,329.07,7.86;13,151.52,456.62,70.45,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,312.16,434.71,168.43,7.86;13,151.52,445.66,251.03,7.86">Tlemcen university at celf ehealth 2018 team techno: Multilingual information extraction-icd10 coding</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bounaama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E A</forename><surname>Abderrahim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,424.20,445.66,56.39,7.86;13,151.52,456.62,41.77,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,468.07,337.64,7.86;13,151.52,479.00,230.89,7.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,201.98,468.07,278.61,7.86;13,151.52,479.03,23.75,7.86">MÃ©thode gÃ©nÃ©rale pour la rÃ©solution des systemes d&apos;Ã©quations simultanÃ©es</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cauchy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,182.30,479.03,70.78,7.86">Comp. Rend. Sci</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1847</biblScope>
			<biblScope unit="page" from="536" to="538" />
			<date type="published" when="1847">1847</date>
			<pubPlace>Paris</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,490.47,337.63,7.86;13,151.52,501.43,329.07,7.86;13,151.52,512.39,110.07,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,247.50,490.47,164.04,7.86">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,432.52,490.47,48.07,7.86;13,151.52,501.43,329.07,7.86;13,151.52,512.39,25.89,7.86">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,523.83,337.63,7.86;13,151.52,534.79,329.07,7.86;13,151.52,545.75,25.60,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="13,346.99,523.83,133.60,7.86;13,151.52,534.79,189.89,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.96,557.20,301.95,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="13,193.42,557.20,90.48,7.86">What happened in clef</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<editor>Crestani et al.</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">94</biblScope>
		</imprint>
	</monogr>
	<note>for a while</note>
</biblStruct>

<biblStruct coords="13,142.96,568.64,337.63,7.86;13,151.52,579.60,154.05,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,216.65,568.64,241.78,7.86">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,465.49,568.64,15.10,7.86;13,151.52,579.60,64.97,7.86">Annals of statistics</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1189" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,591.04,337.98,7.86;13,151.52,602.00,329.07,7.86;13,151.52,612.96,329.07,7.86;13,151.52,623.92,329.07,7.86;13,151.52,634.88,329.07,7.86;13,151.52,645.84,329.07,7.86;13,151.52,656.80,148.63,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,357.72,602.00,122.87,7.86;13,151.52,612.96,76.44,7.86">Overview of the CLEF eHealth evaluation lab 2020</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Saez Gonzales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Viviani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,466.38,623.92,14.21,7.86;13,151.52,634.88,329.07,7.86;13,151.52,645.84,301.18,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction: Proceedings of the Eleventh International Conference of the CLEF Association (CLEF</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>NÃ©vÃ©ol</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Ferro</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">number</biblScope>
			<biblScope unit="page">12260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,119.67,337.98,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,329.07,7.86;14,151.52,152.55,329.07,7.86;14,151.52,164.16,324.81,7.47" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,272.13,119.67,117.42,7.86">2 -getting to know your data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<idno type="DOI">10.1016/B978-0-12-381479-1.00002-2</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/B9780123814791000022" />
	</analytic>
	<monogr>
		<title level="m" coord="14,219.35,130.63,113.72,7.86;14,386.91,130.63,93.68,7.86;14,151.52,141.59,149.79,7.86">The Morgan Kaufmann Series in Data Management Systems</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="39" to="82" />
		</imprint>
	</monogr>
	<note>Data Mining (Third Edition). third edition edn.</note>
</biblStruct>

<biblStruct coords="14,142.62,174.47,337.98,7.86;14,151.52,185.43,329.07,7.86;14,151.52,196.39,329.07,7.86;14,151.52,207.34,205.93,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,326.69,185.43,153.90,7.86;14,151.52,196.39,12.29,7.86">Overview of the clef ehealth evaluation lab</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Spijker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Scells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,202.88,196.39,277.71,7.86;14,151.52,207.34,82.13,7.86">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="322" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,218.30,337.97,7.86;14,151.52,229.26,329.07,7.86;14,151.52,240.20,159.79,7.89" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,438.58,218.30,42.01,7.86;14,151.52,229.26,324.76,7.86">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,151.52,240.22,58.56,7.86">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,251.18,337.97,7.86;14,151.52,262.14,259.31,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,228.53,251.18,252.05,7.86;14,151.52,262.14,33.31,7.86">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">I</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,205.69,262.14,88.83,7.86">Soviet physics doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="707" to="710" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,273.10,337.98,7.86;14,151.52,284.06,329.07,7.86;14,151.52,295.02,329.07,7.86;14,151.52,305.98,329.07,7.86;14,151.52,316.93,76.75,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,151.52,284.06,329.07,7.86;14,151.52,295.02,249.95,7.86">Overview of automatic clinical coding: annotations, guidelines, and solutions for non-english clinical cases at codiesp track of CLEF eHealth</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Armengol-EstapÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,446.52,295.02,34.07,7.86;14,151.52,305.98,329.07,7.86;14,151.52,316.93,48.07,7.86">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,327.89,337.98,7.86;14,151.52,338.85,176.46,7.86" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mogotsi</surname></persName>
		</author>
		<title level="m" coord="14,203.19,327.89,277.41,7.86;14,151.52,338.85,147.79,7.86">Christopher d. manning, prabhakar raghavan, and hinrich schÃ¼tze: Introduction to information retrieval</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,349.81,337.98,7.86;14,151.52,360.77,329.07,7.86;14,151.52,371.73,329.07,7.86;14,151.52,382.66,325.87,7.89" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,394.26,371.73,86.33,7.86;14,151.52,382.69,73.64,7.86">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,232.82,382.69,155.11,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,393.65,337.98,7.86;14,151.52,404.61,329.07,7.86;14,151.52,415.54,133.16,7.89" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,341.75,393.65,138.85,7.86;14,151.52,404.61,168.76,7.86">Cardiology record multi-label classification using latent dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>PÃ©rez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>PÃ©rez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Casillas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gojenola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,328.47,404.61,152.12,7.86;14,151.52,415.56,47.62,7.86">Computer methods and programs in biomedicine</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="111" to="119" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,426.52,337.97,7.86;14,151.52,437.48,329.07,7.86;14,151.52,448.44,322.10,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,197.33,437.48,279.53,7.86">A shared task involving multi-label classification of clinical free text</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pestian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Brew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Matykiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Hovermale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Duch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,165.60,448.44,228.76,7.86">Biological, translational, and clinical language processing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,459.40,337.97,7.86;14,151.52,470.36,329.07,7.86;14,151.52,481.32,55.09,7.86" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="14,342.44,459.40,138.15,7.86;14,151.52,470.36,257.56,7.86">Classifying german animal experiment summaries with multi-lingual bert at clef ehealth 2019 task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>SÃ¤nger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kittner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Leser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">CLEF</note>
	<note>Working Notes</note>
</biblStruct>

<biblStruct coords="14,142.62,492.28,337.97,7.86;14,151.52,503.24,329.07,7.86;14,151.52,514.17,323.00,7.89" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,454.21,492.28,26.38,7.86;14,151.52,503.24,324.86,7.86">A systematic literature review of automated clinical coding and classification systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Stanfill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Fenton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Jenders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,151.52,514.19,230.99,7.86">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="646" to="651" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,525.15,337.98,7.86;14,151.52,536.11,329.07,7.86;14,151.52,547.07,329.07,7.86;14,151.52,558.03,249.36,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="14,369.23,536.11,111.36,7.86;14,151.52,547.07,55.68,7.86">Overview of the clef ehealth evaluation lab</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>NÃ©vÃ©ol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ramadier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Spijker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,245.76,547.07,234.84,7.86;14,151.52,558.03,125.55,7.86">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,568.99,337.98,7.86;14,151.52,579.95,25.60,7.86" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Thomson</surname></persName>
		</author>
		<title level="m" coord="14,218.20,568.99,221.33,7.86">Handbook of Consult and Inpatient Gynecology 1st ed</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,590.91,337.97,7.86;14,151.52,601.87,263.16,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,218.61,590.91,242.23,7.86">The state of record linkage and current research problems</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">E</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,151.52,601.87,113.98,7.86">Statistical Research Division</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
