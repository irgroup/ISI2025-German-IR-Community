<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.21,115.96,286.94,12.62;1,182.43,133.89,250.51,12.62;1,249.32,151.82,116.71,12.62">A study of Machine Learning models for Clinical Coding of Medical Reports at CodiEsp 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.54,190.54,71.96,8.74"><forename type="first">Marco</forename><surname>Polignano</surname></persName>
							<email>marco.polignano@uniba.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. Computer Science</orgName>
								<orgName type="institution">University of Bari Aldo Moro</orgName>
								<address>
									<settlement>Bari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,221.41,190.54,74.40,8.74"><forename type="first">Vincenzo</forename><surname>Suriano</surname></persName>
							<email>v.suriano10@studenti.uniba.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. Computer Science</orgName>
								<orgName type="institution">University of Bari Aldo Moro</orgName>
								<address>
									<settlement>Bari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.63,190.54,60.46,8.74"><forename type="first">Pasquale</forename><surname>Lops</surname></persName>
							<email>pasquale.lops@uniba.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. Computer Science</orgName>
								<orgName type="institution">University of Bari Aldo Moro</orgName>
								<address>
									<settlement>Bari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,372.10,190.54,76.87,8.74"><forename type="first">Marco</forename><surname>De Gemmis</surname></persName>
							<email>marco.degemmis@uniba.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. Computer Science</orgName>
								<orgName type="institution">University of Bari Aldo Moro</orgName>
								<address>
									<settlement>Bari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.38,202.50,82.59,8.74"><forename type="first">Giovanni</forename><surname>Semeraro</surname></persName>
							<email>giovanni.semeraro@uniba.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. Computer Science</orgName>
								<orgName type="institution">University of Bari Aldo Moro</orgName>
								<address>
									<settlement>Bari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,164.21,115.96,286.94,12.62;1,182.43,133.89,250.51,12.62;1,249.32,151.82,116.71,12.62">A study of Machine Learning models for Clinical Coding of Medical Reports at CodiEsp 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">49D3FBC51418EB850AD721607780CA9E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>BERT</term>
					<term>CNN</term>
					<term>BiLSTM</term>
					<term>Self Attention</term>
					<term>Machine Learning</term>
					<term>ICD-10</term>
					<term>Medical Diagnosis</term>
					<term>Deep Learning</term>
					<term>Clinical Coding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of identifying one or more diseases associated with a patient's clinical condition is often very complex, even for doctors and specialists. This process is usually time-consuming and has to take into account different aspects of what has occurred, including symptoms elicited and previous healthcare situations. The medical diagnosis is often provided to patients in the form of written paper without any correlation with a national or international standard. Even if the WHO (World Health Organization) released the ICD10 international glossary of diseases, almost no doctor has enough time to manually associate the patient's clinical history with international codes. The CodiEsp task at CLEF 2020 addressed this issue by proposing the development of an automatic system to deal with this task. Our solution investigated different machine learning strategies in order to identify an approach to face that challenge. The main outcomes of the experiments showed that a strategy based on BERT for pre-filtering and one based on BiLSTM-CNN-SelfAttention for classification provide valuable results. We carried out several experiments on a subset of the training set for tuning the final model submitted to the challenge. In particular, we analyzed the impact of the algorithm, the input encoding strategy, and the thresholds for multi-label classification. A set of experiments has been carried out also during a post hoc analysis. The experiments confirmed that the strategy submitted to the CodiEsp task is the best performing one among those evaluated, and it allowed us to obtain a final mean average error value on the test set equal to 0.202. To support future developments of the proposed approach and the replicability of the experiments we decided to make the source code publicly accessible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Clinical coding <ref type="bibr" coords="2,204.81,143.95,15.50,8.74" target="#b29">[30]</ref> is the task of associating unique identification codes with a clinical diagnosis, or sometimes with a portion of it. Doctors and specialists associate the diagnosis given to the patients with the corresponding international classification only in rare cases. One of the most widely adopted standards is ICD10 <ref type="bibr" coords="2,178.07,191.77,14.61,8.74" target="#b21">[22]</ref>, the tenth version of the international medical glossary released by <ref type="bibr" coords="2,148.55,203.72,153.40,8.74">WHO (World Health Organization)</ref>. Although this annotation task may not seem very useful for medical purposes, it is extremely relevant for statistical purposes, automatic diagnosis analysis of clinical records, and data interoperability across healthcare systems in different countries. Indeed, if each diagnosis is fully digitized with a worldwide standard, every doctor in the world who is visiting us could uniquely interpret our medical records and provide us with the appropriate treatment. In addition, diagnostic patterns used by clinicians could be identified to improve automatic disease prediction strategies and provide automatic specialist support for decision making. These observations strongly support the need for automated systems to support clinicians to perform this task quickly and without human intervention. From the technical point of view, this task is very challenging because it requires the development of an artificial intelligence system able to not only assign more than one class label to the medical response choosing from a very high number of choices, but also to identify the fragment of text associated to that choice. The CodiEsp task at CLEF 2020 <ref type="bibr" coords="2,414.31,371.09,15.50,8.74" target="#b29">[30,</ref><ref type="bibr" coords="2,431.46,371.09,12.73,8.74" target="#b12">13,</ref><ref type="bibr" coords="2,445.86,371.09,12.73,8.74" target="#b20">21]</ref> tries to face this problem by releasing a corpus of 1,000 clinical case studies manually selected by practicing physicians and clinical documentalists. Using that dataset, we carried out an experimental study by testing several machine learning approaches, and we submitted the best performing one to the competition. In the following, we first analyze the state of the art concerning the reference topic (Sect. 2), then we provide the details of the proposed models (Sect. 3). In Sect. <ref type="bibr" coords="2,134.77,454.78,3.87,8.74" target="#b3">4</ref>, we thoroughly present the performed experiments, and we finally present the main outcomes and possible future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The research community has addressed clinical coding tasks for a long time, and numerous scientific contributions have been proposed about the topic. Indeed, CLEF (Conference and Labs of the Evaluation Forum) conference has been working on eHealth and Information Extraction since 2013, but the oldest corpus on the subject dates back to 1973 <ref type="bibr" coords="2,279.07,572.43,14.61,8.74" target="#b29">[30]</ref>. Chapman et al. <ref type="bibr" coords="2,368.03,572.43,9.96,8.74" target="#b8">[9]</ref>, already in 1999, stated that a computer algorithm could solve the clinical coding task better than a human being. This assertion is intuitive because even for an expert in the field, it can be very complex to assign a specific code to the result of a medical diagnosis choosing it from more than 70,000 currently available ICD-10 codes. However, in 2006, Kukafka et al. <ref type="bibr" coords="2,237.68,632.21,14.61,8.74" target="#b17">[18]</ref>, confirmed that when the identification of the right code is not obvious, also Natural Language Processing (NLP) tools could easily lack accuracy. Today, NLP and machine learning techniques are widespread and are receiving substantial attention from research communities. Among the best performing systems, the one proposed by Miftahutdinov <ref type="bibr" coords="3,385.08,130.95,15.50,8.74" target="#b18">[19]</ref> at CLEF eHealth 2017 uses an LSTM on a TF-IDF representation of the text to identify the most suitable ICD-10 code for the input sequence. This allows to obtain an F1 score equal to 0.85, considering a classification on 1,256 distinct classes. During the same competition, Cabot et al. <ref type="bibr" coords="3,318.00,178.77,10.52,8.74" target="#b6">[7]</ref> used an NLP pipeline to obtain the highest F1 score of the competition on the data provided in French (i.e., 0.764). Several preprocessing steps were performed, including stop words filtering, then a method based on the Double Metaphone phonetic encoding algorithm was used to operate a first approximate term search. Finally, a Weighted Distance Score algorithm has been developed to rank the list of candidate terms. The most likely term having the highest score is retained as the matching ICD-10 code for the phrase. In 2018, Atutxa et al. <ref type="bibr" coords="3,271.03,262.46,9.96,8.74" target="#b2">[3]</ref>, proposed a three-level sequence-to-sequence neural network-based approach. The first neural network tries to assign one set of ICD-10 codes to the whole document, then they are refined to assign one set of codes to the line, and finally one specific code. This strategy allowed the model to obtain an F1 score between 0.7086 and 0.9610, depending on the language of the dataset on which the system has been evaluated. Almagro et al. <ref type="bibr" coords="3,418.99,322.23,10.52,8.74" target="#b0">[1]</ref> proposed a supervised learning system based on a multilayer perceptron, SVMs, and a Onevs-Rest strategy. The approach allows to train a binary model for each of the target ICD-10 codes, indicating the presence or absence of the code. The model was able to obtain an F1 score of 0.910. At CLEF eHealth 2019, the best system was proposed by Sänger et al. <ref type="bibr" coords="3,267.48,382.01,14.61,8.74" target="#b27">[28]</ref>, obtaining an F1 score of 0.80. The proposed model utilized a multilingual BERT <ref type="bibr" coords="3,301.93,393.96,15.50,8.74" target="#b9">[10]</ref> text encoding model, fine-tuned on additional training data of German clinical trials also annotated with ICD-10 codes. The model is extended by a single output layer to produce probabilities for specific ICD-10 codes. Amin et al. <ref type="bibr" coords="3,290.85,429.83,10.52,8.74" target="#b1">[2]</ref> participated in the same task obtaining the second place. They evaluated various approaches, such as Convolutional Neural Networks (CNN) and Attention models, among others. They obtained the best results when relying on Bidirectional Encoder Representations from Transformers (BERT) and, more specifically, on BioBERT, which was trained on biomedical documents. Considering the successful models presented as state of the art, we decided to use a machine learning approach that combines CNNs, Bidirectional LSTMs, Attention Layers, and BERT. Details of the proposed architecture are provided in Sect. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Resources and Model Architectures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CodiEsp 2020 corpora</head><p>The CodiEsp track at CLEF 2020 <ref type="bibr" coords="3,286.73,592.14,15.50,8.74" target="#b29">[30,</ref><ref type="bibr" coords="3,303.89,592.14,12.73,8.74" target="#b12">13,</ref><ref type="bibr" coords="3,318.29,592.14,12.73,8.74" target="#b20">21]</ref> contains three sub-tracks (2 main and 1 exploratory) about analysis of clinical reports:</p><p>-CodiEsp Diagnosis Coding main sub-task (CodiEsp-D): it requires automatic ICD10-CM [CIE10 Diagnóstico] code assignment. This sub-track evaluates systems that predict ICD10-CM codes (in the Spanish translation, CIE10-Diagnóstico codes). -ArticleID: it contains the identifier of the clinical text that corresponds to the name of the file. -Label: it contains the diagnostic or procedimiento code.</p><p>-ICD10-code: it contains the ICD10 code.</p><p>-Text-reference: it contains the word or phrase in the clinical text.</p><p>In Fig. <ref type="figure" coords="4,182.74,572.43,4.98,8.74" target="#fig_0">1</ref> and<ref type="figure" coords="4,210.64,572.43,3.87,8.74" target="#fig_1">2</ref>, it is possible to observe how annotations provided with the training data of the two tasks are differently distributed between the different sections of the two ICD10 vocabularies. It is immediately clear that the distribution is not uniform, and some classes are more represented than others. For example, for the training set of task CodiEspD, class 18 is the most represented one, with 2,214 annotations, while class 16 is the least represented with only 23 examples. In Table <ref type="table" coords="4,235.92,644.16,3.87,8.74" target="#tab_0">1</ref>, it is possible to observe that considering all the possible codes of ICD10, the training dataset covers only 1,788 unique codes for  CodiEspD and 546 codes for CodiEspP. Based on this observation, we decided to use models that only provide codes with at least one example in the training set.</p><p>The organizers of the CodiEsp task also released an additional resource that extends the previous datasets. It contains the description of the codes in the ICD10 vocabulary, both "Diagnosis" and "Procedures" (Fig. <ref type="figure" coords="6,406.32,118.99,3.87,8.74" target="#fig_2">3</ref>). This resource contains two files:</p><p>-"codiesp-D codes.tsv": it contains all the 98,288 ICD10-CM codes, along with their description in Spanish and English.</p><p>-"codiesp-P codes.tsv": it contains all the 87,170 ICD10-PCS codes, along with the their description in Spanish and English. It contains the codes up to the fourth nesting lever its of hierarchy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification model proposed to the CodiEsp task</head><p>The Clinical Coding task has been approached using different machine learning strategies that are commonly used to deal with the classification task in the field of Natural Language Processing (NLP). In particular we focused on the use of deep learning techniques such as LSTM <ref type="bibr" coords="6,315.88,428.92,14.61,8.74" target="#b14">[15]</ref>, CNN <ref type="bibr" coords="6,364.52,428.92,14.61,8.74" target="#b16">[17]</ref>, Attention Layers <ref type="bibr" coords="6,465.10,428.92,15.50,8.74" target="#b31">[32]</ref> and Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" coords="6,447.61,440.87,14.61,8.74" target="#b9">[10]</ref>.</p><p>Long-short term memory model. The neural network model based on long-short term memory (LSTM) was proposed in 1997 <ref type="bibr" coords="6,382.02,476.79,14.61,8.74" target="#b14">[15]</ref>. Since then, it has been widely used with data that have an inherent sequential structure, such as text. LSTMs are part of the family of sequential models based on recurring neurons <ref type="bibr" coords="6,157.23,512.66,14.61,8.74" target="#b11">[12]</ref>. In particular, an architecture of this type is based on the idea that the state of the specific neuron depends on that of the previous t -1 state. The natural evolution of a model based on recurring neurons introduces a memory. This structure allows the recurring neuron to depend not only on the state of the single one at step t-1, but also on the state of the different neurons at step t-n. This idea is the base of architectures such as RNN, LSTM, and GRU. Among them, LSTM has the peculiarity of having also a forget gate able to manage the amount of information to be kept in memory. At each step a portion of the memory is deleted and another one is added. These features allow the model to be state-of-the-art for many NLP applications, such as machine translation <ref type="bibr" coords="6,134.77,632.21,14.61,8.74" target="#b34">[35]</ref>, automatic summarization <ref type="bibr" coords="6,273.62,632.21,14.61,8.74" target="#b28">[29]</ref>, parsing, and sentiment analysis <ref type="bibr" coords="6,439.59,632.21,15.50,8.74" target="#b32">[33,</ref><ref type="bibr" coords="6,456.75,632.21,7.01,8.74" target="#b4">5]</ref>. In our architecture we used LSTM in its bidirectional variant.</p><p>Convolutional neural network. Convolutional neural networks (CNN) are born from an accurate study of how the portion of the brain cortex works for vision <ref type="bibr" coords="7,180.28,142.90,14.61,8.74" target="#b15">[16]</ref>. This has promoted their wide use in the computer vision task for image recognition since 1980 <ref type="bibr" coords="7,276.38,154.86,14.61,8.74" target="#b10">[11]</ref>. Recently, they are also widely used in text analysis tasks, thanks to the fast increase in computational power available to everyone. A convolutional neuron is able to concentrate only on a portion of the input data <ref type="bibr" coords="7,185.30,190.72,14.61,8.74" target="#b11">[12]</ref>, e.g., a set of pixels in an image. A layer full of neurons using the same filter (or convolution kernels) gives a feature map, which highlights the areas in an input that are most similar to the filter. During the training, a CNN finds the most useful filters for its task, and learns to combine them into more complex patterns. A CNN is usually followed by a Pooling Layer. Its goal is to subsample the input image in order to reduce the computational load, the memory usage, and the number of parameters. A pooling layer typically works on every input channel independently, so the output depth is the same as the input depth. A neural model using CNN usually alternating CNN layers with Pooling layers with a dense final layer aimed at prediction (classification or regression).</p><p>Self attention. Similarly to the attention strategy proposed in <ref type="bibr" coords="7,444.97,322.23,9.96,8.74" target="#b3">[4]</ref>, selfattention, also known as intra-attention, provides the model ability to weigh the vectors of single words of the sentence differently, according to the similarity of the neighboring tokens. It is possible to say that the level of attention can provide us an idea of what features the network is looking at most during learning and subsequent classification. In particular, we consider an additive context-aware self-attention equal to the whole set of words in input (Eq. 1) <ref type="bibr" coords="7,406.85,393.96,14.61,8.74" target="#b33">[34]</ref>.</p><formula xml:id="formula_0" coords="7,235.91,413.72,244.68,76.04">h t,t = tanh(x T t W t + x T t W t + b t ) e t,t = σ(W e h t,t + b e ) a t,t = sof tmax(e t,t ) l t = n t =1 a t,t x t (1)</formula><p>where, σ is the element-wise sigmoid function, W t and W t are the weight matrices corresponding to the hidden states h t and h t '; W e is the weight matrix corresponding to their non-linear combination; b t and b e are the bias vectors. The attention-focused hidden state representation l t of a token at timestamp t is given by the weighted summation of the hidden state representation h t of all other tokens at timesteps t. We use the last self-attention implementation for Keras<ref type="foot" coords="7,163.08,565.43,3.97,6.12" target="#foot_0">1</ref> .</p><p>BERT. The Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" coords="7,134.77,602.87,15.50,8.74" target="#b9">[10]</ref> is a deep learning model based on the Transformer concept <ref type="bibr" coords="7,412.61,602.87,14.61,8.74" target="#b31">[32]</ref>. In particular, a Transformer architecture can be considered as a stack of N input encoding modules and M decoding modules to obtain an output using multi-head attention and feed-forward layers. The encoder and decoder blocks are identical in numbers, and they are stacked on top of each other. The idea behind a Transformer architecture is to formalize the dependencies between input and output without the use of recurring neural networks. BERT uses a modified version of this architecture in which only encoding layers are present. In particular, the basic version of BERT uses 12 layers, the full version 24. BERT uses two different training strategies "masking" and "next sentence prediction". The first strategy trains the model to recognize certain words in the input sentence that have been appropriately hidden. Usually, the amount of hidden elements is 20% of the words in the sentence. The second mode is to guess the sentence that follows the input sentence. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. This training strategy makes BERT an extremely reliable model that can be very accurate in formalizing semantics among words considering their context. As a result, the pre-trained BERT model can be refined with only one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without significant changes to the task-specific architecture. There are currently several versions of BERT, also trained on data in languages other than English, such as BETO <ref type="bibr" coords="8,417.61,334.19,10.52,8.74" target="#b7">[8]</ref> for Spanish and AlBERTo <ref type="bibr" coords="8,199.38,346.14,15.50,8.74" target="#b25">[26]</ref> for Italian.</p><p>The machine learning model we proposed for the CodiEsp challenge uses all the previously described architectures. Specifically, we decided to use a BERTbased classifier to perform a pre-filtering operation in order to select a subset of sentences possibly referring to a clinical state. Later, the candidate sentences are submitted to a classifier based on BiLSTM, CNN, and self-attention to assign them one or more clinical codes. The architecture of the final model proposed for the clinical coding task is shown in Fig. <ref type="figure" coords="8,325.29,447.33,3.87,8.74" target="#fig_3">4</ref>. It is worth to note that using the proposed strategy it was not possible to participate in task 3 of the competition (CodiEsp-X), which requires the identification of the fragment of text referring to the code. Indeed, we do not focus on the portion of text that specifically refers to a disease, while we used a classification model working with the whole sentence.</p><p>Focusing more on the proposed model in Fig. <ref type="figure" coords="8,354.94,524.61,3.87,8.74" target="#fig_3">4</ref>, we observe that the input text is provided to a BERT model. The goal is the data pre-filtering, in order to select generally speaking sentences, from those talking about a symptom, disease, or treatment. This is a mandatory step because BERT accepts as input only pieces of text not exceeding 128 characters. For this reason, we work on the task at the sentence level, splitting the original clinical report into many sentences that could be or not associated with one or more codes. As pre-trained BERT model, we decided to use BETO <ref type="bibr" coords="8,285.40,608.30,9.96,8.74" target="#b7">[8]</ref>, a Spanish pre-trained version of BERT. The authors trained BETO using 12 self-attention layers with 16 attentionheads each and 1,024 as hidden size. They used all the data from Wikipedia and all of the sources of the OPUS Project <ref type="bibr" coords="8,335.36,644.16,14.61,8.74" target="#b30">[31]</ref>, having the text in Spanish. This source includes the United Nations and Government journals, TED Talks, Subtitles, News Stories, and more. The total size of the corpora gathered was comparable with the corpora used in the original BERT. We decided not to use the multilingual version of BERT because it has been shown that a version trained on the native language performs much better in many NLP tasks <ref type="bibr" coords="9,462.33,632.21,14.61,8.74" target="#b26">[27]</ref>. The sentences classified as possible references of clinical codes are consequently passed to the second part of our model. First of all, the sentences are encoded into word embeddings. In this step, we decided to use a FastText embedding strategy <ref type="bibr" coords="10,134.77,130.95,9.96,8.74" target="#b5">[6]</ref>, which proved to be more effective than GLoVE <ref type="bibr" coords="10,355.73,130.95,15.50,8.74" target="#b23">[24]</ref> and Word2Vec <ref type="bibr" coords="10,439.70,130.95,15.50,8.74" target="#b19">[20]</ref> when many domain-specific words occur in the dataset <ref type="bibr" coords="10,346.72,142.90,14.61,8.74" target="#b24">[25]</ref>. For our final configuration of the model, we chose the one released by José Cañete<ref type="foot" coords="10,368.92,153.28,3.97,6.12" target="#foot_1">2</ref> made of 300 dimensions, trained on the Spanish Unannotated Corpora<ref type="foot" coords="10,336.91,165.24,3.97,6.12" target="#foot_2">3</ref> containing more than 3 billion words. We have configured the LSTM network to work with its bidirectional version. We set the value of hidden units to 64 and the internal dropout value to 0.3. This choice was motivated by the need to reduce the dimensionality of the network output, in order to make the operations to be carried out by the following layers not computationally expensive. Moreover, the dropout value was used to reduce, during the learning, the effect of the overfitting on the training data. We have also decided to vary the function of activation used by the net, setting it to the hyperbolic tangent function (tanh). This activation function has an S-Shape and produces values in the -1 and 1 range, making the layer output more centered to the 0. Moreover, it produces a gradient larger than the sigmoid function, helping to speed up the convergence. A level of self-attention is added following the LSTM. We applied the CNN layer on the result of the attention algorithm. Such hidden level has a matrix form due to the vectorial representation supplied by the word embeddings on the tokens in the input. In detail, it has the form of 128x64, which allows us to apply a 1D Convolutional network with 64 filters and 5x5 kernel. We used ReLu as activation function, that unlike the hyperbolic tangent is faster to calculate. On the top of the CNN layer, we added a Max Pooling function for subsampling the values obtained, reducing the computational load and, the number of parameters of the model. In particular, we used a small 2x2 kernel. On the output of the last max-pooling layer, we applied a dropout function. The hidden model obtained until this step has been merged with the output of the previous Bi-LSTM. We apply this operation for letting the model conceptualize both local and long-term features better. After that, we used a max-pooling layer for 'flattening' the results and reduce the model parameters. An analog function of dimensionality reduction is performed by the consequent dense layer and the following dropping function. Finally, another dense layer with a soft-max activation function has been applied for estimating the probability distribution of each clinical code available in the dataset. The source code of the model is publicly available on GitHub<ref type="foot" coords="10,440.27,511.94,3.97,6.12" target="#foot_3">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental session</head><p>The final architecture of the model, proposed in Section 3.2, was obtained after conducting several experiments on 20% of the training dataset released for the Codiesp-D subtask. In order to always select the same portion of the dataset, we randomly selected sentences using the value 42 as seed for our random func-tion. During our experimental session, we raised five different experimental questions:</p><p>-RQ1: Do recent deep learning models, such as LSTMs, outperform classical machine learning approaches? -RQ2: Which is the best strategy for encoding text in the form of word embedding? -RQ3: Which is the best model among those we proposed that allows us to achieve the best performance? -RQ4: Can the use of a sentence pre-filtering classifier help to improve the performance of the model? -RQ5: Is there a class probability threshold that allows us to choose more than one code as a result of the classification?</p><p>In order to validate our claims we repeated the experiments also on the annotated test set released by the task after the challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Metrics and Settings</head><p>We trained different classification models in order to understand the best approach to solve the CodiEsp challenge. In particular we developed the following models:</p><p>- For the selection of the word embedding strategy to use for encoding the textual sentences, we have evaluated the following resources:</p><p>-FastText Spanish official release <ref type="bibr" coords="11,294.28,534.85,15.50,8.74" target="#b13">[14]</ref> -Fastext Spanish Unannotated Corpora (SUC) by José Cañete<ref type="foot" coords="11,423.96,545.17,3.97,6.12" target="#foot_4">5</ref> -GloVe Spanish Billion Word Corpus (SBWC) by George Pérez 5 -Word2Vec Spanish Billion Word Corpus (SBWC) by Cristian Cardellino 5   Finally, as baselines we chose classic machine learning approaches used for dealing with a classification problem. In particular we implemented them in Python using the scikit learn library <ref type="bibr" coords="11,296.99,616.08,14.61,8.74" target="#b22">[23]</ref>:</p><p>-Logistic Regression, C=0.1 The model performance has been evaluated using the standard metrics of precision, recall, F1 in their macro-average version and on test set also the Mean Average Precision (MAP) <ref type="bibr" coords="12,251.43,483.19,14.61,8.74" target="#b11">[12]</ref>. We trained all the models for 30 epochs with a fixed random value of 42 a batch size of 256 when needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discussion of results</head><p>Looking at results in Tab. 2, it is worth to note that the strategy based on deep learning, i.e. the LSTM model, outperforms those based on classical machine learning approaches. Specifically, the logistic regression is the best model among the "classic" ones, with a F1 score equal to 0.03071. The LSTM strategy here proposed, based on a layer of word embeddings trained only data provided as input, i.e. no pretrained weights are used, achieves a F1 score performance which is higher than twice that of logistic regression, i.e. 0.07923. This result allows us to provide a positive answer to RQ1.</p><p>Tab. 3 reports the results obtained by evaluating different pre-trained word embedding weights. As expected, the differences in the final F1 outcome are not significant, but the best score is obtained using the FastText approach pretrained on the Spanish Unannotated Corpora (SUC). We decided not to evaluate word embeddings weights released for English, because we would like to focus on the portion of data released in Spanish rather than its translated version. In this new experiment, we increased the F1 score previously claimed, reaching a value of 0.10410. These results allow us to answer properly to RQ2. In Tab. 4, we reported results obtained by varying the architecture of our model, by holding the pre-trained word embeddings choice at the previous experimental step. An unexpected result is obtained by observing the strategy based Table <ref type="table" coords="14,163.78,115.91,4.13,7.89">6</ref>. Results obtained keeping fixed the FastText SUC word embedding, the two models (BERT, BiLSTM + CNN + SelfAttention) and varying the threshold on the probability returned by the architecture for each class. We use it for selecting multiple labels for the specific sentence. Also in this case the evaluation has been performed on the 20% of the training set. on BERT as a simple classifier of codes. It performs quite worst than the one that uses BiLSTM, CNN, and self-attention layers. The difference between the two approaches is tiny, and, from our point of view, it is not very relevant. For this reason, we decided to go over using both the approaches. In this step, we obtained the best F1 score of 0.10410, i.e., around 0.02 points greater than the previous result. The results allow us to provide a valid answer to RQ3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT (BETO</head><p>The following evaluation step is about splitting the clinical coding task into two independent steps: pre-filtering and classification. We reported the results obtained by this evaluation step in Tab. 5. It is possible to note that, among the different combinations of classifiers used for the two steps, the best configuration is that using BERT as a pre-filtering strategy and BiLSTM + CNN + Self Attention as a classification approach. We were able to increase F1 score by around 0.03 points from the previous step, reaching the value of 0.13632. The behavior we observed in results allows us to answer at RQ4 positively.</p><p>The last evaluation concerns the strategy for selecting many labels for a single sentence. We decided to use a threshold on the result of the softmax function that allowed us to extract the label on which the model is more certain. We decided to vary the thresholds using both fixed and dynamic ones. The results are reported in Tab. 6. It is possible to observe that both the values 0.10 and 0.25 achieve good results. In particular, using a threshold of 0.10, we are maximizing the recall, on the contrary, we observe high values of precision. Due to the consideration that in a real scenario, a tool like this can be a decision support system for the doctors, we decided to use the configuration that uses 0.10 as the final model for the CodiEsp task. The results support our positive answer also for RQ5.</p><p>The model we implemented, has been used for participating at both CodiEsp subtasks, i.e., CodiEsp-D and CodiEsp-P. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Post Hoc Analysis</head><p>We performed a further investigation of the performance of our model on the gold annotated test set. In particular, in this phase, we take into account the score obtained for the MAP metric, because it is used by the organizers for calculating the final leaderboard. As we can observe in Fig. <ref type="figure" coords="16,395.58,178.20,3.87,8.74">7</ref>, the configuration of the model using a threshold equal to 0.10 is the best performing one, followed by those obtained using a the threshold 0.05 and 0.25, respectively. The results successfully supported our choice of using a threshold able to maximize the recall more than the precision. The final results obtained by the CodiEsp task are those reported in Tab. 8. Looking at the final scores of precision and recall, it is worth to note that our model is more feasible for real use as a decision support system. Indeed, it is able to obtain a higher recall than precision and, as previously stated, this easily allows to select candidate codes for clinical reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we faced the problem of clinical coding by applying several machine learning methods. We compared traditional classification approaches such as logistic regression, random forests, and SVM, with deep learning models, including LSTM, CNN, and BERT. The experimental analyses allowed us to propose a classification model based on two steps of execution: pre-filtering and classification. In the pre-filtering phase, we use a BERT based classification model to select a set of medical report sentences that we believe can be associated with one or more ICD10 codes. Later, we use a BiLSTM, CNN, and Self-Attention-based classifier to select the specific set of possible codes for the candidate sentence. The results obtained in the post hoc evaluation phase have shown that the approach proposed for the challenge is the best possible among those considered in this study. The results obtained for the challenge showed a MAP score of 0.202 for the CodiEsp-D task and 0.221 for the CodiEsp-P task. These are encouraging results given the difficulty of the task, and there is also the possibility of further improving the figures by better balancing the training data available among the various categories of codes. As future work, we expect to be able to associate the ICD10 code with the corresponding portion of text in order to implement a first strategy for explaining results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,310.85,345.83,7.89;5,134.77,321.84,22.07,7.86;5,157.68,115.83,300.00,180.25"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Distribution of annotations in the training dataset among the ICD10-CM sections.</figDesc><graphic coords="5,157.68,115.83,300.00,180.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,542.32,345.83,7.89;5,134.77,553.30,33.90,7.86;5,157.68,347.30,300.00,180.25"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Distribution of annotations in the training dataset among the ICD10-PCS sections.</figDesc><graphic coords="5,157.68,347.30,300.00,180.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,163.22,320.84,288.91,7.89;6,134.77,238.55,350.01,67.52"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example of code description provided as addition task resource.</figDesc><graphic coords="6,134.77,238.55,350.01,67.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,137.08,553.77,341.20,7.89;9,134.77,115.83,350.01,423.16"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Architecture of the model used as final submission at the CodiEsp challenge.</figDesc><graphic coords="9,134.77,115.83,350.01,423.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,151.70,386.38,72.09,8.74;11,140.99,398.25,136.35,8.77;11,140.99,410.15,38.25,8.77;11,140.99,422.05,240.96,8.77;11,140.99,433.94,329.96,8.77;11,140.99,445.84,329.96,8.77;11,140.99,457.74,207.78,8.77;11,140.99,469.64,339.60,8.77;11,151.70,481.62,108.49,8.74"><head></head><label></label><figDesc>LSTM, BiLSTM -CNN, CNN + Self Attention -BERT -BiLSTM + CNN, BiLSTM + CNN + Self Attention -(Pre-filtering) BERT -(Classification) BiLSTM + CNN + Self Attention -(Pre-filtering) BiLSTM + CNN + Self Attention -(Classification) BERT -(Pre-filtering) BERT -(Classification) BERT -(Pre-filtering) BiLSTM + CNN + Self Attention -(Classification) BiLSTM + CNN + Self Attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,115.91,345.83,378.74"><head>Table 1 .</head><label>1</label><figDesc>Statistics on the training datasets.</figDesc><table coords="4,134.77,136.71,345.83,357.94"><row><cell></cell><cell cols="2">ICD10-CM ICD10-PCS</cell></row><row><cell>Number of code sections</cell><cell>21</cell><cell>16</cell></row><row><cell cols="2">Total number of possible codes 71,486</cell><cell>72,081</cell></row><row><cell>Unique codes used in the training set</cell><cell>1,788</cell><cell>546</cell></row><row><cell>Number of total annotations in the training set</cell><cell>8,494</cell><cell>2,587</cell></row><row><cell cols="3">-CodiEsp Procedure Coding main sub-task (CodiEsp-P): it requires auto-</cell></row><row><cell cols="3">matic ICD10-PCS [CIE10 Procedimiento] code assignment. This sub-track</cell></row><row><cell cols="3">evaluates systems that predict ICD10-PCS codes (in the Spanish translation,</cell></row><row><cell>CIE10-Procedimiento codes).</cell><cell></cell><cell></cell></row><row><cell cols="3">-CodiEsp Explainable AI exploratory sub-task (CodiEsp-X). Systems are re-</cell></row><row><cell cols="3">quired to submit the reference to the predicted codes (both ICD10-CM and</cell></row><row><cell cols="3">ICD10-PCS). The correctness of the provided reference is assessed in this</cell></row><row><cell cols="2">sub-track, in addition to the code prediction.</cell><cell></cell></row><row><cell cols="3">For each task a dataset for training, development and test has been released.</cell></row><row><cell cols="3">Generally speaking, the CodiEsp corpora contain manually annotated clinical</cell></row><row><cell cols="3">reports with corresponding clinical codes. The clinical reports are written in</cell></row><row><cell cols="3">Spanish, and they are annotated with the CIE10 glossary (the Spanish version</cell></row><row><cell cols="3">of ICD10-CM and ICD10-PCS). The training set contains 500 clinical cases,</cell></row><row><cell cols="3">while the development and the test set provide 250 clinical cases each. The</cell></row><row><cell cols="3">CodiEsp corpus format is plain text with UTF8 encoding, where each clinical</cell></row><row><cell cols="3">case is stored in a single file whose name is the clinical unique case identifier. The</cell></row><row><cell cols="3">final collection of the 1,000 clinical cases of the corpus contains 16,504 sentences,</cell></row><row><cell cols="3">with 16.5 sentences per clinical case on average. It contains 396,988 words, with</cell></row><row><cell cols="3">396.2 words per clinical report on average. For sub-task 1 and 2 of the CodiEsp</cell></row><row><cell cols="3">task, (CodiEspD and CodiEsp-P), the training files contain the following fields:</cell></row><row><cell cols="2">[articleID, label, ICD10-code, text-reference].</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,146.17,116.41,323.03,107.45"><head>Table 2 .</head><label>2</label><figDesc>Results obtained running the baselines on the 20% of the training set.</figDesc><table coords="12,187.13,137.17,241.11,86.68"><row><cell></cell><cell cols="2">Marco-P Macro-R Macro-F1</cell></row><row><cell>Logistic Regression</cell><cell>0.03367 0.03412</cell><cell>0.03071</cell></row><row><cell>SVC -rbf</cell><cell>0.00106 0.00128</cell><cell>0.00116</cell></row><row><cell>Decision Tree Classifier</cell><cell>0.00729 0.00868</cell><cell>0.00771</cell></row><row><cell cols="2">Random Forest Classifier 0.00531 0.00272</cell><cell>0.00275</cell></row><row><cell>Ada Boost Classifier</cell><cell>0.00106 0.00127</cell><cell>0.00116</cell></row><row><cell>LSTM -no pre-trained word embeddings</cell><cell cols="2">0.08473 0.09060 0.07923</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,134.77,238.97,345.83,205.83"><head>Table 3 .</head><label>3</label><figDesc>Results obtained varying the pre-trained word embeddings on the 20% of the training set.</figDesc><table coords="12,140.99,270.76,287.09,174.04"><row><cell></cell><cell cols="2">Macro-P Macro-R Macro-F1</cell></row><row><cell>LSTM -FastText Spanish official</cell><cell>0.09299 0.09629</cell><cell>0.08589</cell></row><row><cell>LSTM -FastText SUC</cell><cell cols="2">0.09357 0.09903 0.08845</cell></row><row><cell>LSTM -GloVe SBWC</cell><cell>0.09252 0.09722</cell><cell>0.08715</cell></row><row><cell>LSTM -Word2Vec SBWC</cell><cell>0.09202 0.09672</cell><cell>0.08685</cell></row><row><cell cols="2">-SVC (SVM Classifier) with RBF kernel, C=0.1</cell><cell></cell></row><row><cell>-Decision Tree Classifier</cell><cell></cell><cell></cell></row><row><cell cols="2">-Random Forests Classifier, n estimators= 500</cell><cell></cell></row><row><cell cols="2">-ADA Boost, n estimators= 100, learning rate= 0.01</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="13,134.77,115.91,345.83,141.89"><head>Table 4 .</head><label>4</label><figDesc>Results obtained holding the FastText SUC word embedding, varying the model, using an evaluation on the 20% of the training set.</figDesc><table coords="13,187.67,147.87,240.02,109.93"><row><cell></cell><cell cols="2">Macro-P Macro-R Macro-F1</cell></row><row><cell>LSTM</cell><cell>0.09357 0.09903</cell><cell>0.08845</cell></row><row><cell>BiLSTM</cell><cell>0.10354 0.10909</cell><cell>0.09789</cell></row><row><cell>BiLSTM + CNN</cell><cell>0.09995 0.11552</cell><cell>0.09831</cell></row><row><cell>BiLSTM + CNN + SelfAtt.</cell><cell cols="2">0.10629 0.11887 0.10410</cell></row><row><cell>CNN</cell><cell>0.09511 0.10095</cell><cell>0.09100</cell></row><row><cell>CNN + SelfAtt.</cell><cell>0.09279 0.09484</cell><cell>0.08706</cell></row><row><cell>BERT (BETO)</cell><cell>0.10381 0.10821</cell><cell>0.10294</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,134.77,276.15,345.83,250.50"><head>Table 5 .</head><label>5</label><figDesc>Results obtained holding the FastText SUC word embedding, the two models (BERT, BiLSTM + CNN + SelfAttention) and varying their combinations for prefiltering and classification, using an evaluation on the 20% of the training set.</figDesc><table coords="13,186.74,318.67,241.88,207.98"><row><cell></cell><cell cols="2">Macro-P Macro-R Macro-F1</cell></row><row><cell>(Pre-filtering)</cell><cell></cell><cell></cell></row><row><cell>BiLSTM + CNN + SelfAtt.</cell><cell></cell><cell></cell></row><row><cell>-</cell><cell>0.13241 0.10934</cell><cell>0.11534</cell></row><row><cell>(Classification)</cell><cell></cell><cell></cell></row><row><cell>BiLSTM + CNN + SelfAtt.</cell><cell></cell><cell></cell></row><row><cell>(Pre-filtering)</cell><cell></cell><cell></cell></row><row><cell>BiLSTM + CNN + SelfAtt.</cell><cell></cell><cell></cell></row><row><cell>-</cell><cell>0.09180 0.08871</cell><cell>0.10022</cell></row><row><cell>(Classification)</cell><cell></cell><cell></cell></row><row><cell>BERT (BETO multi-class)</cell><cell></cell><cell></cell></row><row><cell>(Pre-filtering)</cell><cell></cell><cell></cell></row><row><cell>BERT (BETO)</cell><cell></cell><cell></cell></row><row><cell>-</cell><cell>0.09704 0.10092</cell><cell>0.11734</cell></row><row><cell>(Classification)</cell><cell></cell><cell></cell></row><row><cell>BERT (BETO multi-class)</cell><cell></cell><cell></cell></row><row><cell>(Pre-filtering)</cell><cell></cell><cell></cell></row><row><cell>BERT (BETO)</cell><cell></cell><cell></cell></row><row><cell>-</cell><cell cols="2">0.13823 0.12053 0.13632</cell></row><row><cell>(Classification)</cell><cell></cell><cell></cell></row><row><cell>BiLSTM + CNN + SelfAtt.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="15,137.20,115.91,340.95,313.29"><head>Table 7 .</head><label>7</label><figDesc>Results obtained on the annotated test set of the CodiEsp-D subtask.</figDesc><table coords="15,137.20,136.68,340.95,292.52"><row><cell cols="4">BERT -BiLSTM + CNN + SelfAtt. MAP Macro-P Macro-R Macro-F1</cell></row><row><cell>Threshold 0.05</cell><cell cols="2">0.194 0.12913 0.25016</cell><cell>0.16804</cell></row><row><cell>Threshold 0.10</cell><cell cols="3">0.202 0.19238 0.21361 0.19931</cell></row><row><cell>Threshold 0.25</cell><cell cols="3">0.181 0.28702 0.19935 0.2201</cell></row><row><cell>Threshold 0.50</cell><cell cols="2">0.15 0.38739 0.11682</cell><cell>0.1765</cell></row><row><cell>Threshold 0.75</cell><cell>0.118 0.425</cell><cell>0.08365</cell><cell>0.13742</cell></row><row><cell>Threshold maxProb-0.10</cell><cell cols="2">0.156 0.00599 0.31974</cell><cell>0.01172</cell></row><row><cell>Threshold maxPrb-0.25</cell><cell cols="2">0.142 0.00394 0.50282</cell><cell>0.07813</cell></row><row><cell>Threshold maxProb -(maxProb*0.10)</cell><cell cols="2">0.165 0.21341 0.15787</cell><cell>0.17897</cell></row><row><cell>Threshold maxProb -(maxProb*0.25)</cell><cell cols="2">0.168 0.19305 0.16649</cell><cell>0.17636</cell></row><row><cell>Threshold: MeanValue</cell><cell cols="2">0.096 0.00836 0.48201</cell><cell>0.0134</cell></row><row><cell>Threshold: MeanValue -MeanValue*0.25</cell><cell cols="2">0.093 0.00735 0.49042</cell><cell>0.01187</cell></row><row><cell>Threshold: MeanValue -MeanValue*0.35</cell><cell cols="2">0.0901 0.00698 0.49313</cell><cell>0.01118</cell></row><row><cell>Threshold: MeanValue -MeanValue*0.50</cell><cell cols="2">0.089 0.00643 0.49681</cell><cell>0.01022</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="15,136.16,446.67,349.20,185.67"><head>Table 8 .</head><label>8</label><figDesc>Official CodiEsp task results.</figDesc><table coords="15,136.16,467.44,349.20,164.90"><row><cell></cell><cell cols="3">CodiEsp-D</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell>MAP</cell><cell>MAP codes</cell><cell>MAP30</cell><cell>MAP30 codes</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>BERT (BETO)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-</cell><cell cols="3">0.202 0.236 0.202</cell><cell>0.236</cell><cell cols="3">0.295 0.323 0.308</cell></row><row><cell>BiLSTM + CNN + SelfAtt.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT (BETO)</cell><cell cols="3">0.117 0.136 0.117</cell><cell>0.136</cell><cell cols="3">0.272 0.218 0.242</cell></row><row><cell>BiLSTM+CNN+SelfAtt.</cell><cell cols="3">0.169 0.195 0.16</cell><cell>0.185</cell><cell cols="3">0.135 0.442 0.207</cell></row><row><cell></cell><cell cols="3">CosiEsp-P</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-</cell><cell cols="3">0.221 0.25 0.219</cell><cell>0.247</cell><cell cols="3">0.186 0.38 0.25</cell></row><row><cell>BiLSTM + CNN + SelfAtt.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BiLSTM</cell><cell cols="3">0.137 0.152 0.127</cell><cell>0.141</cell><cell cols="3">0.122 0.399 0.187</cell></row><row><cell>BERT</cell><cell cols="3">0.141 0.154 0.14</cell><cell>0.153</cell><cell cols="3">0.155 0.323 0.209</cell></row><row><cell>BiLSTM+CNN+SelfAtt.</cell><cell cols="3">0.17 0.191 0.15</cell><cell>0.168</cell><cell cols="3">0.097 0.513 0.164</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="7,144.73,656.80,207.36,7.86"><p>https://github.com/CyberZHG/keras-self-attention</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="10,144.73,634.88,226.01,7.86"><p>https://github.com/dccuchile/spanish-word-embeddings</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="10,144.73,645.84,160.34,7.86"><p>http://crscardellino.github.io/SBWCE/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="10,144.73,656.80,180.41,7.86"><p>https://github.com/marcopoli/CODIESP-10</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="11,144.73,656.80,226.01,7.86"><p>https://github.com/dccuchile/spanish-word-embeddings</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="16,142.96,602.00,337.63,7.86;16,151.52,612.96,329.07,7.86;16,151.52,623.92,329.07,7.86;16,151.52,634.88,329.07,7.86;16,151.52,645.84,329.07,7.86;16,151.52,656.80,329.07,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,374.24,602.00,106.35,7.86;16,151.52,612.96,329.07,7.86;16,151.52,623.92,177.46,7.86">MAMTRA-MED at CLEF ehealth 2018: A combination of information retrieval techniques and neural networks for ICD-10 coding of death certificates</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Almagro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Montalvo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>De Ilarraza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pérez</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2125/paper110.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="16,219.71,634.88,260.88,7.86;16,151.52,645.84,47.21,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="16,373.07,645.84,107.52,7.86;16,151.52,656.80,14.79,7.86">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Soulier</surname></persName>
		</editor>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
			<biblScope unit="volume">2125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,119.67,337.63,7.86;17,151.52,130.63,329.07,7.86;17,151.52,141.59,159.95,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="17,177.92,130.63,302.67,7.86;17,151.52,141.59,15.38,7.86">Mlt-dfki at clef ehealth 2019: Multi-label classification of icd-10 codes with bert</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Dunfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vechkaeva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Wixted</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,187.90,141.59,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,151.87,337.63,7.86;17,151.52,162.83,329.07,7.86;17,151.52,173.79,329.07,7.86;17,151.52,184.75,91.44,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="17,389.28,162.83,91.31,7.86;17,151.52,173.79,252.88,7.86">Ixamed at clef ehealth 2018 task 1: Icd10 coding with a sequence-to-sequence approach</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Atutxa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Casillas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ezeiza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Fresno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gojenola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">O</forename><surname>Anchordoqui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Perez-De Viñaspre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,424.89,173.79,55.70,7.86;17,151.52,184.75,40.20,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,195.03,337.63,7.86;17,151.52,205.99,247.18,7.86" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m" coord="17,296.12,195.03,184.47,7.86;17,151.52,205.99,85.88,7.86">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,142.96,216.27,337.64,7.86;17,151.52,227.23,329.07,7.86;17,151.52,238.17,156.09,7.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="17,356.11,216.27,124.48,7.86;17,151.52,227.23,182.33,7.86">O verview of the evalita 2018 aspect-based sentiment analysis task (absita)</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Polignano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,341.10,227.23,139.49,7.86;17,151.52,238.19,95.81,7.86">EVALITA Evaluation of NLP and Speech Tools for Italian</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,248.48,337.63,7.86;17,151.52,259.43,329.07,7.86;17,151.52,270.37,88.81,7.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="17,364.59,248.48,116.00,7.86;17,151.52,259.43,81.78,7.86">Enriching word vectors with subword information</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,240.54,259.43,240.05,7.86;17,151.52,270.39,13.87,7.86">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,280.68,337.63,7.86;17,151.52,291.63,329.07,7.86;17,151.52,302.59,25.60,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,332.57,280.68,148.02,7.86;17,151.52,291.63,216.03,7.86">Sibm at clef ehealth evaluation lab 2017: Multilingual information extraction with cim-ind</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cabot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">F</forename><surname>Soualmia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Darmoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,387.24,291.63,93.35,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,312.88,337.64,7.86;17,151.52,323.83,278.14,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="17,353.42,312.88,127.18,7.86;17,151.52,323.83,78.59,7.86">Spanish pre-trained bert model and evaluation data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cañete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chaperon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,251.36,323.83,149.64,7.86">to appear in PML4DC at ICLR 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,334.12,337.63,7.86;17,151.52,345.08,329.07,7.86;17,151.52,356.04,199.86,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="17,270.58,334.12,210.01,7.86;17,151.52,345.08,125.98,7.86">Comparing expert systems for identifying chest x-ray reports that support pneumonia</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Haug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,298.23,345.08,148.31,7.86">Proceedings of the AMIA Symposium</title>
		<meeting>the AMIA Symposium</meeting>
		<imprint>
			<publisher>American Medical Informatics Association</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page">216</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,366.32,337.97,7.86;17,151.52,377.28,319.24,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="17,354.85,366.32,125.73,7.86;17,151.52,377.28,199.77,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,372.72,377.28,54.52,7.86">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,387.56,337.98,7.86;17,151.52,398.52,329.07,7.86;17,151.52,409.48,166.51,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,263.68,387.56,216.91,7.86;17,151.52,398.52,180.24,7.86">Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miyake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,352.22,398.52,128.38,7.86;17,151.52,409.48,43.38,7.86">Competition and cooperation in neural nets</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="267" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,419.76,337.98,7.86;17,151.52,430.72,329.07,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="17,196.81,419.76,283.78,7.86;17,151.52,430.72,233.77,7.86">Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Géron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>O&apos;Reilly Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,441.00,337.98,7.86;17,151.52,451.96,329.07,7.86;17,151.52,462.92,329.07,7.86;17,151.52,473.88,329.07,7.86;17,151.52,484.84,329.07,7.86;17,151.52,495.80,329.07,7.86;17,151.52,506.75,148.63,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="17,357.72,451.96,122.87,7.86;17,151.52,462.92,76.44,7.86">Overview of the CLEF eHealth evaluation lab 2020</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Saez Gonzales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Viviani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,453.30,473.88,27.29,7.86;17,151.52,484.84,329.07,7.86;17,151.52,495.80,296.38,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction: Proceedings of the Eleventh International Conference of the CLEF Association (CLEF</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Ferro</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">number</biblScope>
			<biblScope unit="page">12260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,517.04,337.97,7.86;17,151.52,528.00,329.07,7.86;17,151.52,538.96,189.94,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="17,403.47,517.04,77.12,7.86;17,151.52,528.00,83.97,7.86">Learning word vectors for 157 languages</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,254.85,528.00,225.74,7.86;17,151.52,538.96,161.27,7.86">Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC 2018)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,549.24,337.98,7.86;17,151.52,560.17,92.85,7.89" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="17,288.76,549.24,100.73,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,398.69,549.24,81.90,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,570.48,337.98,7.86;17,151.52,581.41,217.73,7.89" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="17,263.66,570.48,216.93,7.86;17,151.52,581.44,23.50,7.86">Receptive fields of single neurones in the cat&apos;s striate cortex</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,182.01,581.44,105.15,7.86">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">574</biblScope>
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,591.72,337.98,7.86;17,151.52,602.68,329.07,7.86;17,151.52,613.64,86.01,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="17,328.09,591.72,152.50,7.86;17,151.52,602.68,103.94,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,275.64,602.68,200.74,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,623.92,337.97,7.86;17,151.52,634.88,329.07,7.86;17,151.52,645.84,329.07,7.86;17,151.52,656.77,186.90,7.89" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="17,385.54,623.92,95.05,7.86;17,151.52,634.88,329.07,7.86;17,151.52,645.84,186.41,7.86">Human and automated coding of rehabilitation discharge summaries according to the international classification of functioning, disability, and health</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kukafka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Bales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,345.47,645.84,135.12,7.86;17,151.52,656.80,94.89,7.86">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="508" to="515" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,119.67,337.97,7.86;18,151.52,130.63,329.07,7.86;18,151.52,141.59,55.09,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="18,294.13,119.67,186.46,7.86;18,151.52,130.63,240.35,7.86">Kfu at clef ehealth 2017 task 1: Icd-10 coding of english death certificates with recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Miftahutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tutubalina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,414.54,130.63,66.06,7.86;18,151.52,141.59,26.42,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,151.56,337.98,7.86;18,151.52,162.52,263.42,7.86" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m" coord="18,341.33,151.56,139.26,7.86;18,151.52,162.52,101.88,7.86">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="18,142.62,172.49,337.98,7.86;18,151.52,183.45,329.07,7.86;18,151.52,194.41,329.07,7.86;18,151.52,205.37,329.07,7.86;18,151.52,216.32,76.75,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="18,151.52,183.45,329.07,7.86;18,151.52,194.41,249.95,7.86">Overview of automatic clinical coding: annotations, guidelines, and solutions for non-english clinical cases at codiesp track of CLEF eHealth</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Armengol-Estapé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,446.52,194.41,34.07,7.86;18,151.52,205.37,329.07,7.86;18,151.52,216.32,48.07,7.86">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,226.29,337.98,7.86;18,151.52,237.25,329.07,7.86" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="18,265.45,226.29,215.14,7.86;18,151.52,237.25,159.76,7.86">The ICD-10 classification of mental and behavioural disorders: diagnostic criteria for research</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H</forename><surname>Organization</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>World Health Organization</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,247.22,337.98,7.86;18,151.52,258.18,329.07,7.86;18,151.52,269.11,329.07,7.89" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="18,395.74,258.18,84.86,7.86;18,151.52,269.14,71.10,7.86">Scikit-learn: Machine learning in python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,229.41,269.14,163.72,7.86">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,279.11,337.97,7.86;18,151.52,290.07,329.07,7.86;18,151.52,301.03,215.28,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="18,329.82,279.11,150.77,7.86;18,151.52,290.07,35.30,7.86">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,207.71,290.07,272.88,7.86;18,151.52,301.03,120.77,7.86">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,311.00,337.98,7.86;18,151.52,321.95,329.07,7.86;18,151.52,332.91,329.07,7.86;18,151.52,343.87,134.60,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="18,387.16,311.00,93.43,7.86;18,151.52,321.95,325.38,7.86">A comparison of wordembeddings in emotion detection from text using bilstm, cnn and self-attention</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Polignano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Gemmis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Semeraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,165.07,332.91,315.52,7.86;18,151.52,343.87,59.94,7.86">Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="63" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,353.84,337.98,7.86;18,151.52,364.80,329.07,7.86;18,151.52,375.76,59.51,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="18,418.67,353.84,61.93,7.86;18,151.52,364.80,310.89,7.86">Alberto: Italian bert language understanding model for nlp challenging tasks based on tweets</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Polignano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Gemmis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Semeraro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Basile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,151.52,375.76,30.85,7.86">CLiC-it</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,385.73,337.98,7.86;18,151.52,396.69,329.07,7.86;18,151.52,407.62,167.42,7.89" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="18,439.25,385.73,41.34,7.86;18,151.52,396.69,207.83,7.86">AlBERTo: Modeling Italian Social Media Language with BERT</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Polignano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Gemmis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Semeraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,367.72,396.69,112.88,7.86;18,151.52,407.65,106.20,7.86">Italian Journal of Computational Linguistics -IJCOL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,417.62,337.98,7.86;18,151.52,428.57,329.07,7.86;18,151.52,439.53,95.81,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="18,344.30,417.62,136.29,7.86;18,151.52,428.57,281.66,7.86">Classifying german animal experiment summaries with multi-lingual bert at clef ehealth 2019 task 1</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sänger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kittner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Leser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,455.89,428.57,24.70,7.86;18,151.52,439.53,67.14,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,449.50,337.97,7.86;18,151.52,460.44,319.52,7.89" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="18,285.89,449.50,194.70,7.86;18,151.52,460.46,78.13,7.86">Abstractive text summarization using lstm-cnn based deep learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,236.60,460.46,142.43,7.86">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="857" to="875" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,470.43,337.97,7.86;18,151.52,481.39,329.07,7.86;18,151.52,492.32,323.00,7.89" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="18,454.21,470.43,26.38,7.86;18,151.52,481.39,324.86,7.86">A systematic literature review of automated clinical coding and classification systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Stanfill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Fenton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Jenders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,151.52,492.35,230.99,7.86">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="646" to="651" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,502.32,337.97,7.86;18,151.52,513.28,70.14,7.86" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="18,215.23,502.32,166.34,7.86">Parallel data, tools and interfaces in opus</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,403.01,502.32,20.12,7.86">Lrec</title>
		<imprint>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="2214" to="2218" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,523.25,337.97,7.86;18,151.52,534.20,329.07,7.86;18,151.52,545.16,167.19,7.86" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="18,228.73,534.20,100.53,7.86">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,351.88,534.20,128.71,7.86;18,151.52,545.16,73.89,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,555.13,337.97,7.86;18,151.52,566.09,329.07,7.86;18,151.52,577.02,124.29,7.89" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="18,334.58,555.13,146.01,7.86;18,151.52,566.09,325.27,7.86">Emotion-semantic-enhanced bidirectional lstm with multi-head attention mechanism for microblog sentiment analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,151.52,577.05,47.63,7.86">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">280</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,587.02,337.97,7.86;18,151.52,597.98,329.07,7.86;18,151.52,608.94,329.07,7.86;18,151.52,619.90,25.60,7.86" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="18,339.67,587.02,140.92,7.86;18,151.52,597.98,117.33,7.86">Opentag: Open attribute value extraction from product profiles</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,288.36,597.98,192.23,7.86;18,151.52,608.94,236.22,7.86">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,629.87,337.98,7.86;18,151.52,640.82,329.07,7.86;18,151.52,651.76,195.75,7.89" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="18,343.75,629.87,136.84,7.86;18,151.52,640.82,199.37,7.86">Deep recurrent models with fastforward connections for neural machine translation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,356.83,640.82,123.76,7.86;18,151.52,651.78,120.82,7.86">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="371" to="383" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
