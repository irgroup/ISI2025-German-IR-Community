<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.61,115.96,332.14,12.62;1,138.16,133.89,339.03,12.62">Classifying clinical case studies with ICD-10 at Codiesp CLEF eHealth 2020 Task 1-Diagnostics</title>
				<funder ref="#_xyMERue">
					<orgName type="full">of the Ministry of Economy and Competitiveness -Government of Spain</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,190.22,175.14,95.36,11.26"><forename type="first">Paula</forename><surname>Queipo-Álvarez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer science Department</orgName>
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.75,177.66,107.92,8.74"><forename type="first">Israel</forename><surname>González-Carrasco</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer science Department</orgName>
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.61,115.96,332.14,12.62;1,138.16,133.89,339.03,12.62">Classifying clinical case studies with ICD-10 at Codiesp CLEF eHealth 2020 Task 1-Diagnostics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">33916EE57243FD81F5289BD1F435D87D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ICD-10 Classification</term>
					<term>Clinical case studies</term>
					<term>Named-Entity Recognition</term>
					<term>Dictionary based</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, the authors describe the approach and results for the participation in Task 1 (multilingual information extraction) of CLEF eHealth 2020. This work addresses the task of automatically assign ICD-10 codes for Diagnostics clinical case studies in Spanish and English. A dictionary-based approach has been used relying on the terminological resource provided by the organization. The system achieved a mean average precision of 0.115 (precision: 0.866, recall: 0.066).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, the authors describe the participation of Human Language and Accessibility Technologies Group (HULAT) at CodiEsp CLEF eHealth 2020, in particular to Task 1: Multilingual Information Extraction. The focus is the ICD-10 coding of Diagnostics, belonging to the sub-task D. CodiEsp is the Clinical Cases Coding in Spanish language Track and is devoted to the automatic coding of clinical cases in Spanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">State-of-work</head><p>CLEF eHealth has been running these annual evaluation campaigns since 2013 in the Information retrieval, Information Management and Information Extraction. For example, in task of multilingual Information Extraction, named entity recognition (NER), text classification and acronym normalization. In 2018 <ref type="bibr" coords="1,437.48,575.82,9.96,8.74" target="#b0">[1]</ref>, CLEF organization shared a task to promote automatic clinical coding systems over death reports in the French language (in 2018) and over german non-technical summaries (NPTs) of animal experiments (in 2019).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Task description</head><p>This task utilizes the International Classification of Diseases, 10th revision (ICD-10), which is a terminology resource. The sub-task CodiEsp Diagnosis aims to assign ICD10-CM codes (CIE10 Diagnóstico, in Spanish) to clinical case documents <ref type="bibr" coords="2,134.77,173.42,9.96,8.74" target="#b6">[7]</ref>. This terminology is tree-shaped. Annotated codes are minimum 3-character long, and the codes with a greater number of characters are more granular. The organization provided a list of valid codes for this sub-task with their English and Spanish description, and an annotated corpus as well. The evaluation of the automatic coding is against manually generated ICD10 codifications. The motivation of the task is to determine the most competitive approaches for coding this type of documents and generating new clinical coding tools for other languages and data collections.</p><p>Task 1: Multilingual Information Extraction, was built upon information extraction tasks from previous years <ref type="bibr" coords="2,296.46,281.01,31.27,8.74">[10] [9]</ref>. Information Extraction could be treated as a cascaded named entity recognition with normalization, or a text classification. This task was proposed to explore multilingual approaches, even though the two languages (English and Spanish) were addressed individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In the following subsections, we describe the corpora, the terminology, the dictionary based approach, and SpaCy Language Processing Pipelines.</p><p>The system must predict the codes in both languages, English and Spanish. The dictionary matches the codes with the terms in English and Spanish, and the translation of the Spanish clinical case studies into English was offered. In this case, we recognised entities in each language using the dictionary in both languages.</p><p>The automatic classification with ICD-10 codes it is a multi-class and multilabel problem. This can also be considered as a Named-Entitiy Recognition (NER) task.</p><p>The code is stored in a Github repositoriy <ref type="bibr" coords="2,331.10,489.49,11.98,8.74" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Corpora and Terminologies</head><p>The CodiEsp corpus is available in several versions. In this work, the third version is considered <ref type="bibr" coords="2,229.74,548.52,9.96,8.74" target="#b5">[6]</ref>. The corpus is composed of 1000 clinical case studies annotated by clinical coding professionals meeting strict quality criteria. The clinical case studies have been randomly sampled into three subsets: the train, the development, and the test set. The train set is composed of 500 clinical cases, the development set has of 250 clinical cases each, the test set includes 250 clinical cases, and the background set is composed of 2,751 clinical cases.</p><p>Apart from the text files with all the clinical case studies, the annotations for train, development and test sets are provided. The annotation has the following fields: articleID, that refers to the clinical case study, and every ICD10-code found in the clinical case study. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dictionary based approach</head><p>The organization have provided terminological resources, such as the valid codes for the task <ref type="bibr" coords="3,181.64,275.89,11.75,8.74" target="#b7">[8]</ref>. One of the files contains a list of 71486 CIE10-Diagnósticos terms (2018 version) with their description in Spanish and English. Diagnostic codes have the following fields: code, es-description and en-description. To create the dictionary in Python, it was necessary to map the codes to the references and find the frequency of the tags for each code. It is observed that the frequencies differ and the classes are not balanced.</p><p>Data preprocessing First of all, it is necessary to join all the clinical case studies in different files for train, development and test to work with them easily. In addition, alphanumeric ICD-10 terms were mapped into different numeric values to avoid errors in the Recognizer. Finally, the clinical case studies were tokenized with SpaCy.</p><p>SpaCy Language Processing Pipelines <ref type="bibr" coords="3,330.32,450.13,10.52,8.74" target="#b3">[4]</ref> They have been used in several steps. First, to segment text into tokens and produce doc objects that were processed through the pipeline. Secondly, to assign part-of-speech tags using the tagger. It also uses a parser to assign dependency labels. Furthermore, it includes an Entity Recognizer to detect and label named entities.</p><p>In the Figure <ref type="figure" coords="3,213.19,510.60,4.98,8.74" target="#fig_0">1</ref> it is shown the structure of the SpaCy Language Processing Pipeline.</p><p>In order to use the Entity Recognizer, it is necessary to add Named Entities metadata to doc objects in SpaCy. First, we loaded the English (or Spanish) model. To avoid overlapping of entities, we replaced the default NER module with the dictionary in English (or Spanish). We did this to prevent overlapping of entities.</p><p>After that, we detected Named Entities over the test and background set, processing the text and showing its entities.</p><p>This procedure has been used with two different models: <ref type="bibr" coords="3,396.36,620.25,13.05,8.74" target="#b4">[5]</ref> en core web sm (2.2.5) and es-core-news-sm (2.2.5) in English and Spanish, respectively. These models include convolutional layers, residual connections, layer normalization and maxout non-linearity.</p><p>Fuzzy matching With the library fuzzy-wuzzy, it is possible to adjust the maximum (-1) and minimum (50) scores allowed to match between terms. Finally, the predicted terms are saved to evaluate them with the organization script. <ref type="bibr" coords="4,466.91,142.90,11.31,8.74" target="#b2">[3]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Other approaches</head><p>There are other approximations, such as Semantic rules, Transfer learning, RNNbased or BERT-based. Multi-lingual information retrieval (MLIR) is the retrieval of documents in several languages from a query. So it would be interesting to combine English and Spanish in a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results &amp; Discussion</head><p>In this section, we present the results of one official run. This allows to compare the effectiveness of the classifiers and study the difference in failure analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental setup</head><p>This team submitted predictions for 3001 documents, which includes test files and background files. However, the submission was processed to include only predictions for test files (250 documents) which were considered for computing the metrics.</p><p>The evaluation results came from the organization, that used the scripts distributed as part of the Clinical Cases Coding in Spanish language Track (CodiEsp) <ref type="bibr" coords="4,174.94,408.85,13.39,8.74" target="#b2">[3]</ref>. All the experiments were performed without cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Test results</head><p>The official metric for the subtasks CodiEsp Diagnostic is the mean average precision (MAP). Other metrics computed over a maximum of 1 are: precision, recall and mean average precision for a query of 30 elements (MAP@30). These metrics are computed taking into account only the predictions for the codes presented in the train and development sets. This is because there were codes in the test set that had not been used in the train and validation sets.</p><p>In the code search on a set of clinical case studies, precision is the number of correct codes divided by the number of all returned codes, while recall is the number of correct codes divided by the number of codes that should have been returned.</p><p>Precision value reaches an 86.6%, whereas in recall only a 6.6%. This means that 86.6% of the codes retrieved were correct, which is impresive due to the difficulty of the taks. Recall measures the fraction of true positives among the predicted results. A low result means a high number of false negatives or codes not detected because of the lexical variability. Our dictionary-based approach was not able to detect the majority of the codes because of a lack of flexibility in the recognition. Also, there are three metrics computed for correct categories. These categories are first three digits of a CIE10-Diagnostic code. For example, codes P96.5 and P96.89 pertains to the category P96.</p><p>Table <ref type="table" coords="5,163.62,464.27,4.13,7.89">3</ref>. System performance for ICD-10 coding on the test corpus in terms of precision, recall and F-measure. Computed for categories (first 3 digits).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head><p>Value Precision categories 0.889 Recall categories 0.074 F-measure categories 0.137 These results are slightly better than the official ones, due to the relaxation of the codes into categories.</p><p>We can observe a good result in precision, showing that most of the entities detected where correct, whereas recall results are weak. Also, F-Measure is a good measure when the class distribution is uneven, which is our case. This is an essential problem for this dictionary matching approach. In order to increase the recall, a state-of-the-art approach must be used. Our approach needs to be improved due to the difficulty of the task. This working note presents our contribution to Task 1 of CLEF eHealth competition 2020 <ref type="bibr" coords="6,188.51,154.42,12.45,8.74" target="#b6">[7]</ref>. The task challenges the automatic assignment of ICD-10 codes for Diagnostics to clinical case studies.</p><p>At the previous stages, multi-label classification was tried, but the model did not succeed due to the high number of codes, classes unbalance and a few examples of each class. Then, NER was intended among the clinical case studies to detect Diagnostics, but the model could not predict the code. Due to the lack of results of other approaches, a more traditional dictionary-approach was built. It was able to automatically assign codes to the test set with SpaCy's help.</p><p>Evaluation results highlight that our approach was not good enough to detect all the entities presents in the clinical case studies due to the variability of the terms. Lexical variability has impacted recall, that fails to detect terms. Some improvements would be semantic rules, reduce the number of irrelevant codes, a post-processing filtering phase or including more features. Other upgrades are the treatment of abbreviations and the detection of typos. Also, the future enhancement could be obtained with new terminological and Linguistic resources.</p><p>Also, to include deep learning models to infer the categories that have not been seen. This can use and additional corpus to train the model. Finally, it is interesting to implement a system that combines English and Spanish into a multilingual model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,216.30,205.47,182.76,7.89;3,134.77,115.84,345.83,74.87"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. SpaCy Language Processing Pipeline.</figDesc><graphic coords="3,134.77,115.84,345.83,74.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,510.86,345.83,94.81"><head>Table 1 .</head><label>1</label><figDesc>System performance for ICD-10 coding on the test corpus in terms of mean average precision, precision, recall and F-measure. Official evaluation setup.</figDesc><table coords="4,268.93,542.59,77.51,63.08"><row><cell cols="2">Parameter Value</cell></row><row><cell>MAP</cell><cell>0.115</cell></row><row><cell cols="2">MAP30 0.115</cell></row><row><cell cols="2">Precision 0.866</cell></row><row><cell>Recall</cell><cell>0.066</cell></row><row><cell cols="2">F-measure 0.123</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,276.97,345.83,94.81"><head>Table 2 .</head><label>2</label><figDesc>System performance for ICD-10 coding on the test corpus in terms of mean average precision, precision, recall and F-measure. The test documents without gold labels are ignored for evaluation.</figDesc><table coords="5,254.69,319.66,105.97,52.12"><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>MAP codes</cell><cell>0.138</cell></row><row><cell cols="2">Precision codes 0.935</cell></row><row><cell>Recall codes</cell><cell>0.071</cell></row><row><cell cols="2">F-measure codes 0.132</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="programName">Research Program</rs> <rs type="funder">of the Ministry of Economy and Competitiveness -Government of Spain</rs>, (<rs type="projectName">DeepEMR</rs> project <rs type="grantNumber">TIN2017-87548-C2-1-R</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_xyMERue">
					<idno type="grant-number">TIN2017-87548-C2-1-R</idno>
					<orgName type="project" subtype="full">DeepEMR</orgName>
					<orgName type="program" subtype="full">Research Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,514.88,180.23,8.12" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Codiesp</surname></persName>
		</author>
		<ptr target="https://temu.bsc.es/codiesp/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,525.73,337.64,7.86;6,151.52,536.69,329.07,7.86;6,151.52,547.65,329.07,7.86;6,151.52,558.61,329.08,7.86;6,151.52,570.21,275.02,7.47" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,151.53,525.73,329.07,7.86;6,151.52,536.69,329.07,7.86;6,151.52,547.65,173.51,7.86">GitHub -pqueipo/Codiesp-CLEF-2020-eHealth-Task1: From Overview of automatic clinical coding: annotations, guidelines, and solutions for non-english clinical cases at codiesp track of CLEF eHealth</title>
		<ptr target="https://github.com/pqueipo/Codiesp-CLEF-2020-eHealth-Task1" />
	</analytic>
	<monogr>
		<title level="m" coord="6,363.00,547.65,117.60,7.86;6,151.52,558.61,297.84,7.86">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum.CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,580.41,337.64,7.86;6,151.52,591.37,275.60,8.12" xml:id="b2">
	<monogr>
		<ptr target="https://github.com/TeMU-BSC/CodiEsp-Evaluation-Script" />
		<title level="m" coord="6,232.77,580.41,247.83,7.86;6,151.52,591.37,17.24,7.86">CodiEsp-Evaluation-Script: Evaluation library for CodiEsp Task</title>
		<imprint/>
	</monogr>
	<note type="report_type">GitHub -TeMU-BSC</note>
</biblStruct>

<biblStruct coords="6,142.96,602.22,337.64,8.12;6,151.52,613.83,122.88,7.47" xml:id="b3">
	<monogr>
		<ptr target="https://spacy.io/usage/processing-pipelines" />
		<title level="m" coord="6,151.53,602.22,242.15,7.86">Language Processing Pipelines • spaCy Usage Documentation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,624.03,281.46,8.12" xml:id="b4">
	<monogr>
		<ptr target="https://spacy.io/models" />
		<title level="m" coord="6,151.53,624.03,156.95,7.86">Models • spaCy Models Documentation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,634.88,337.64,7.86;6,151.52,645.84,329.07,7.86;6,151.52,656.80,164.92,7.86;6,334.67,657.44,145.93,7.47;7,151.52,119.67,329.07,8.11;7,151.52,130.63,18.56,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,380.00,634.88,100.59,7.86;6,151.52,645.84,274.60,7.86">CodiEsp corpus: Spanish clinical cases coded in ICD10 (CIE10) -eHealth CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3758054</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3758054" />
		<imprint>
			<date type="published" when="2020-04">2020. Apr 2020</date>
		</imprint>
	</monogr>
	<note>Funded by the Plan de Impulso de las Tecnologías del Lenguaje (Plan TL</note>
</biblStruct>

<biblStruct coords="7,142.96,141.59,337.64,7.86;7,151.52,152.55,329.07,7.86;7,151.52,163.51,329.07,7.86;7,151.52,174.47,329.07,7.86;7,151.52,185.43,76.75,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,151.52,152.55,329.07,7.86;7,151.52,163.51,249.95,7.86">Overview of automatic clinical coding: annotations, guidelines, and solutions for non-english clinical cases at codiesp track of CLEF eHealth</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Armengol-Estapé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,446.52,163.51,34.07,7.86;7,151.52,174.47,329.07,7.86;7,151.52,185.43,48.07,7.86">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,196.39,337.63,7.86;7,151.52,207.34,329.07,8.12;7,151.52,218.30,329.07,8.12;7,151.52,229.26,133.81,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,311.52,196.39,169.07,7.86;7,151.52,207.34,82.69,7.86">CodiEsp codes: list of valid CIE10 codes for the CodiEsp task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3706838</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3706838" />
		<imprint>
			<date type="published" when="2020-01">Jan 2020</date>
		</imprint>
	</monogr>
	<note>Funded by the Plan de Impulso de las Tecnologías del Lenguaje (Plan TL</note>
</biblStruct>

<biblStruct coords="7,142.96,240.22,337.64,7.86;7,151.52,251.18,329.07,7.86;7,151.52,262.14,329.07,7.86;7,151.52,273.10,190.32,8.12" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">N</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grouin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lavergne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rondet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zweigenbaum</surname></persName>
		</author>
		<ptr target="https://www.cdc.gov/" />
		<title level="m" coord="7,329.65,251.18,150.94,7.86;7,151.52,262.14,329.07,7.86;7,151.52,273.10,43.28,7.86">CLEF eHealth 2017 Multilingual Information Extraction task overview: ICD10 coding of death certificates in English and French</title>
		<imprint/>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct coords="7,142.62,284.06,337.97,7.86;7,151.52,295.02,329.07,7.86;7,151.52,305.98,329.07,8.11;7,151.52,317.58,127.10,7.47" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,410.53,295.02,70.06,7.86;7,151.52,305.98,225.46,7.86">Clinical Information Extraction at the CLEF eHealth Evaluation lab</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grouin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lavergne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zweigenbaum</surname></persName>
		</author>
		<ptr target="http://quaerofrenchmed.limsi.fr/" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
