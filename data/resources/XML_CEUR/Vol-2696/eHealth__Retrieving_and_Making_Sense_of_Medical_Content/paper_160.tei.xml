<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.47,115.96,312.42,12.62;1,221.39,133.89,172.58,12.62">SandiDoc at CLEF 2020 -Consumer Health Search : AdHoc IR Task</title>
				<funder>
					<orgName type="full">Australian National University</orgName>
				</funder>
				<funder>
					<orgName type="full">Our Health in Our Hands (OHIOH)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,154.23,171.56,89.99,8.74"><forename type="first">Sandaru</forename><surname>Seneviratne</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research School of Computer Science</orgName>
								<orgName type="department" key="dep2">College of Engineering and Computer Science</orgName>
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.78,171.56,67.58,8.74"><forename type="first">Eleni</forename><surname>Daskalaki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research School of Computer Science</orgName>
								<orgName type="department" key="dep2">College of Engineering and Computer Science</orgName>
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,438.14,171.56,23.00,8.74;1,152.55,183.51,33.60,8.74"><forename type="first">Zakir</forename><surname>Hossain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research School of Computer Science</orgName>
								<orgName type="department" key="dep2">College of Engineering and Computer Science</orgName>
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.47,115.96,312.42,12.62;1,221.39,133.89,172.58,12.62">SandiDoc at CLEF 2020 -Consumer Health Search : AdHoc IR Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D3F40BBE7297EADDAA986E49CAE3E06D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Retrieval</term>
					<term>TF-IDF Score</term>
					<term>Word Vector Representations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Information retrieval (IR) processes deal with the retrieval of ranked documents based on the similarity among documents and the key words specified in user's query. CLEF 2020 eHealth task on consumer health search adhoc IR addresses the need of improved information retrieval techniques in the health domain to provide relevant information to users. This paper presents our work in CLEF eHealth 2020 consumer health search, on term frequency-inverse document frequency and word vector representation-based techniques adopted in the adhoc IR task. The goal of our work is to experiment on different techniques for information retrieval and look into how different word vector representations of text can affect the final results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the increasing expansion of the online content, there has been a growth in online health information retrieval efforts in order to obtain medical knowledge. These efforts, pursued not only by medical specialists but also from the general public, have led to improved mechanisms of health information retrieval. Given the enormous amount of available information, it is vital to provide users with documents fitting to their requests. Information retrieval (IR) can be described as the automatic retrieval of a list of ranked documents that are relevant to a given user query based on similarity measures between the query and the documents. Different theoretical models like boolean, probabilistic, and vector models are used in IR which utilise distinct matching and ranking algorithms to retrieve the documents relevant to a certain query <ref type="bibr" coords="1,356.90,568.00,9.96,8.74" target="#b6">[7]</ref>.</p><p>Most of the early IR systems were based on boolean models <ref type="bibr" coords="1,418.63,579.96,15.50,8.74" target="#b10">[11]</ref> which use boolean logic and set theory to represent the presence or the absence of a term in a document respectively. Another major approach for IR is probabilistic retrieval models <ref type="bibr" coords="1,199.59,615.83,15.50,8.74" target="#b10">[11]</ref> which make use of the probability of relevance of queries to documents by calculating the term weights in queries and documents. In vector space models <ref type="bibr" coords="2,194.84,130.95,14.61,8.74" target="#b10">[11]</ref>, all queries and documents are represented by vectors of an n dimensional vector space where n refers to the number of distinct terms in the collection.</p><p>CLEF eHealth 2020 <ref type="bibr" coords="2,240.03,166.81,10.52,8.74" target="#b3">[4]</ref> task 2 on consumer health search <ref type="bibr" coords="2,402.98,166.81,10.52,8.74" target="#b2">[3]</ref> consists of two sub tasks; adhoc IR and spoken query retrieval. Adhoc IR is a traditional IR task to produce relevant documents to the written queries whereas the spoken query retrieval task utilises spoken queries for the document retrieval. We participated in the sub task 1 of the consumer health search to experiment on adhoc IR.</p><p>This paper is organized as follows. Section 2 introduces the data set, queries and other additional resources used in the task. Section 3 describes the methodology used in the experimental setup. In section 4, we present the results and sections 5 and 6 include the discussion and future work respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Resources</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset</head><p>The document collection used in the document retrieval task was acquired by the common crawl dump of 2018-19. This included web pages of the formats such as HTML, XHTML, XML. The data set used for the task is clefehealth2018 B which is a subset of the initial dataset of 1903 domains. clefehealth B dataset contains web pages from 1653 website domains and was created by removing a number of websites that were not strictly related to health. The size of this corpus was 294 GB out of which a subset of size 30 GB was used in the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Queries</head><p>For the Sub task 1, adhoc IR, 50 topics/queries were provided. These queries were chosen from a set of sample queries collected over 6 months by domain experts. These 50 queries were raw queries with no preprocessing performed beforehand. Fig. <ref type="figure" coords="2,210.04,494.66,4.98,8.74" target="#fig_0">1</ref> provides an example query input which contains the id and the query. The first 3 digits in the id refer to the topic whereas the last 3 digits are used to identify the creator of the query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Word Embedding Model</head><p>As additional resources, Medical Continuous Bag of Words (CBOW) and Skipgram word embeddings created using TREC (The Text REtrieval Conference) Medical Records collection were provided. These models use neural network architecture to develop the word representations. In the CBOW architecture, the model takes the context words into account when predicting the target word whereas the Skip-gram model architecture uses the target word to predict the context words <ref type="bibr" coords="3,198.77,207.18,9.96,8.74" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we describe the different techniques we used in the document retrieval task. Fig. <ref type="figure" coords="3,219.81,268.00,4.98,8.74">2</ref> gives an overview of the complete process which includes preprocessing, representation of queries and documents, and, finally, a matching and ranking algorithm to get the most relevant document list for the queries. Fig. <ref type="figure" coords="3,221.37,578.69,3.87,8.74">2</ref>: Elements of a document retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>Preprocessing is an important initial step in natural language processing (NLP) <ref type="bibr" coords="3,134.77,656.12,10.52,8.74" target="#b7">[8]</ref> tasks to convert text into a more simplified and an understandable format so that the NLP and Machine Learning (ML) techniques can perform better. Both the clefehealth B dataset and queries are raw data with no preprocessing performed on them. In order to obtain clean text from the data set and the queries, we follow different preprocessing steps <ref type="bibr" coords="4,340.77,154.86,9.96,8.74" target="#b8">[9]</ref>.</p><p>Clefehealth B dataset contains web pages of different formats crawled from the web. These files include the content of the web page along with HTML tags, scripting, and styling. In order to obtain the important content from the web pages, a proper parsing of the web pages is performed using the beautifulsoup library <ref type="bibr" coords="4,191.37,214.85,14.60,8.74" target="#b11">[12]</ref>. Converting text to lower case is one of the simplest forms of preprocessing which is useful in entity normalisation. If ignored, this can lead to identifying the same entity as distinct entities which can eventually affect the final result of a system. Both the queries and the text obtained from HTML parsing were converted to lower-case. Next, the digits or the numbers in the text were converted to text in order to facilitate entity normalization. Stop words carry little to no important information in text. Hence, as a next step, stop words were removed in both the queries and the documents using the stop word list provided by nltk library. The punctuation and other unnecessary characters were removed in order to obtain clean text. To ensure that queries and documents are free of spelling errors, spell correction was done using the edit distance of the words and the words in the given word embedding model. Once that was completed, stemming was performed using Porter Stemmer's algorithm to bring each word to its stem word to ensure that different forms of words are identified as one word <ref type="bibr" coords="4,189.92,382.23,14.61,8.74" target="#b9">[10]</ref>. Fig. <ref type="figure" coords="4,231.85,382.23,4.98,8.74" target="#fig_1">3</ref> gives an overview of the preprocessing function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Document Retrieval</head><p>This section describes TF-IDF and word embedding based techniques used for the IR task.</p><p>TF-IDF score based: TF-IDF (Eq. 1) is a popular IR technique used in many applications <ref type="bibr" coords="4,190.25,596.34,9.96,8.74" target="#b1">[2]</ref>. It is a weight (statistical) measure used to evaluate the importance of a word in a document with respect to the whole collection of documents <ref type="bibr" coords="4,134.77,620.25,9.96,8.74" target="#b5">[6]</ref>. Number of occurrences of a term in a document (term frequency -TF) and inverse document frequency (IDF) are used to calculate TF-IDF weight. There are different variants of TF and IDF scores used in calculating the relevance of a document to a user query. In our work, we use the variation in equation 1.</p><p>Using the TF alone to calculate the scores may give more weight to nonrelevant terms. In order to dampen the effect of TF, IDF score is incorporated. However, a linear IDF function may boost the document scores with high IDF terms. To address this issue and dampen the effect of a linear IDF function, log value (sub-linear function) of IDF is considered.</p><formula xml:id="formula_0" coords="5,258.04,207.79,222.56,9.65">w i,d = tf i,d • log(n/df i )<label>(1)</label></formula><p>Word Embedding based: Word embeddings can capture semantic meanings among words which is a huge advantage in IR tasks. Word embeddings use distributional hypothesis which focuses on the context of words to derive the word representation <ref type="bibr" coords="5,149.99,288.90,11.42,8.74" target="#b0">[1]</ref>. The word embedding model architecture used in the task is Skip-gram which predicts the context words in a window given the target word. Out of the different Skip-gram models, the model with 500 dimensional embedding space and 5 dimensional context window are used in this task.</p><p>To represent the documents in the collection using word embedding, we use average, minimum and maximum vector representations using the 100 most frequent terms in the document. Along with the average vector representation obtained (Eq. 2), we average the minimum and maximum vectors to obtain another vector representation (Eq. 3) for the document. Similarly, we obtain two vector representations (average vector and the average of minimum and maximum vectors) for each given query.</p><formula xml:id="formula_1" coords="5,228.97,449.27,247.38,25.41">vector representation1 i = 100 n=1 x i 100 (<label>2</label></formula><formula xml:id="formula_2" coords="5,476.35,459.11,4.24,8.74">)</formula><formula xml:id="formula_3" coords="5,201.26,514.47,279.33,22.31">vector representation2 i = min vec i + max vec i 2<label>(3)</label></formula><p>We calculate the similarity for TF-IDF technique by summing the TF-IDF scores of query tokens in each document and ranking the scores of documents in descending order. For the two vector representations (average vectors, average of the minimum and maximum vectors) we calculate the similarity using cosine similarity to rank the documents with the highest scores in descending order. For each query, we retrieve the 1000 most similar documents as results. Table <ref type="table" coords="5,475.61,644.16,4.98,8.74" target="#tab_0">1</ref> gives the fields in each row of the results file. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The experiment was performed on a subset of size 30 GB of the Clefehealth B dataset and only the results from the TF-IDF document retrieval algorithm were submitted for evaluation due to time constraints and computational limitations. Using a subset of the dataset has a huge impact on the accuracy of the results since only part of the relevant documents are retrieved, missing a significant number of other relevant documents in the dataset. Table <ref type="table" coords="6,390.95,327.66,4.98,8.74" target="#tab_1">2</ref> provides the result scores for the IR task using TF-IDF technique for the dataset of 30/294 GB. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this paper, we present our methodology for the adhoc IR subtask of CLEF2020 using TF-IDF score and word vector representations. TF-IDF is considered a simple yet effective algorithm which provides an ideal baseline for IR tasks on which we can develop and expand to more complicated IR algorithms. Despite these advantages, TF-IDF lacks the use of context information compared to other models like word embedding models which take context information into account in developing the embeddings for words. If a user query contains "diabetes" as a key word, TF-IDF algorithm would not consider documents which contain the variation "diabetic" in the IR task. Similarly, the algorithm would not consider documents which contain "diabetec" (misspelled terms) despite how relevant they are to the user query. In order to produce the most relevant documents using TF-IDF algorithm, it is vital to preprocess the data prior to applying the algorithm which can have a significant effect on the results. Word embedding models have been successfully used in many NLP and ML tasks since they consider contextual information in developing the representations for words. However, one of the major limitations in word embedding models is that they are unable to identify words similar in text but with different meanings (homonyms) creating a single vector representation for those words. This limitation can be avoided by using approaches which produce multi sense embeddings for words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Work</head><p>In future, we will further improve and expand algorithms for IR building on the baseline models TF-IDF and word embedding. Moreover, we will expand our current work to incorporate query expansion which can be used to obtain different forms of the original query to improve the results of the IR task. One of the popular query expansion techniques is synonym identification and substitution which is done mostly using existing vocabularies. In the medical domain, vocabularies like UMLS (Unified Medical Language System), SNOMED CT (SNOMED Clinical Terms), OAC-CHV (open-access and collaborative consumer health vocabulary) can be used for query expansion along with word embedding techniques. In addition, we will explore techniques for multi sense embeddings to improve on the word embedding based model for IR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,249.13,629.95,117.09,8.74;2,134.77,549.89,345.82,68.53"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Example of a query.</figDesc><graphic coords="2,134.77,549.89,345.82,68.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,206.70,471.45,201.96,8.74"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Elements in the preprocessing function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,134.77,321.59,345.84,245.57"><head></head><label></label><figDesc></figDesc><graphic coords="3,134.77,321.59,345.84,245.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,136.16,117.40,265.46,96.31"><head>Table 1 :</head><label>1</label><figDesc>Fields in the results file.</figDesc><table coords="6,136.16,139.70,265.46,74.02"><row><cell cols="2">Field Details</cell></row><row><cell>qid</cell><cell>query id</cell></row><row><cell>Q0</cell><cell>literal Q0</cell></row><row><cell cols="2">docno document id</cell></row><row><cell cols="2">rank rank of the document</cell></row><row><cell cols="2">score similarity score for the document with respect to the query</cell></row><row><cell>tag</cell><cell>system identifier</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,136.16,372.65,366.56,107.27"><head>Table 2 :</head><label>2</label><figDesc>Results of the adhoc IR task.</figDesc><table coords="6,136.16,394.94,72.85,7.86"><row><cell>Evaluation Metric</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was funded by and has been delivered in partnership with <rs type="funder">Our Health in Our Hands (OHIOH)</rs>, a strategic initiative of the <rs type="funder">Australian National University</rs>, which aims to transform health care by developing new personalized health technologies and solutions in collaboration with patients, clinicians and health-care providers.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.96,558.38,337.64,7.86;7,151.52,569.34,329.07,7.86;7,151.52,580.30,149.67,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,255.23,558.38,136.64,7.86">Relevance-based Word Embedding</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,399.54,558.38,81.05,7.86;7,151.52,569.34,329.07,7.86;7,151.52,580.30,121.00,7.86">SIGIR &apos;17: Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,591.15,337.63,7.86;7,151.52,602.11,329.07,7.86;7,151.52,613.07,240.40,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,243.19,591.15,237.40,7.86;7,151.52,602.11,87.06,7.86">Adapting the tf idf Vector-Space Model to Domain Specific Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fautsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<ptr target="http://www.lucene.apache.org/" />
	</analytic>
	<monogr>
		<title level="m" coord="7,246.97,602.11,233.62,7.86;7,151.52,613.07,79.61,7.86">SAC &apos;10: Proceedings of the 2010 ACM Symposium on Applied Computing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,623.92,337.63,7.86;7,151.52,634.88,329.07,7.86;7,151.52,645.84,329.07,7.86;7,151.52,656.80,263.06,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,201.32,634.88,279.26,7.86;7,151.52,645.84,125.26,7.86">Overview of the CLEF eHealth 2020 task 2: Consumer health search with ad hoc and spoken queries</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Gonzales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Viviani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,297.42,645.84,183.17,7.86;7,151.52,656.80,234.39,7.86">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,119.67,337.63,7.86;8,151.52,130.63,329.07,7.86;8,151.52,141.59,329.07,7.86;8,151.52,152.55,329.07,7.86;8,151.52,163.51,329.07,7.86;8,151.52,174.47,329.07,7.86;8,151.52,185.43,279.34,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,357.72,130.63,122.87,7.86;8,151.52,141.59,76.44,7.86">Overview of the CLEF eHealth evaluation lab 2020</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Saez Gonzales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Viviani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,453.30,152.55,27.29,7.86;8,151.52,163.51,329.07,7.86;8,151.52,174.47,296.38,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction: Proceedings of the Eleventh International Conference of the CLEF Association (CLEF</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">number</biblScope>
			<biblScope unit="page">12260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,196.39,337.63,7.86;8,151.52,207.34,329.07,7.86;8,151.52,218.30,329.07,7.86;8,151.52,229.26,140.38,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,379.59,196.39,101.00,7.86;8,151.52,207.34,166.48,7.86">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://ronan.collobert.com/senna/" />
	</analytic>
	<monogr>
		<title level="m" coord="8,328.60,207.34,152.00,7.86;8,151.52,218.30,293.02,7.86">Conference: Proceedings of the International Conference on Learning Representations (ICLR 2013)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,240.22,337.64,7.86;8,151.52,251.18,68.63,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,199.37,240.22,277.12,7.86">Using TF-IDF to Determine Word Relevance in Document Queries</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ramos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct coords="8,142.96,262.14,337.63,7.86;8,151.52,273.10,109.38,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,234.43,262.14,197.71,7.86">Modern Information Retrieval: A Brief Overview</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal Google</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct coords="8,142.96,284.06,337.64,7.86;8,151.52,295.02,329.07,7.86;8,151.52,305.98,329.07,7.86;8,151.52,316.93,329.07,7.86;8,151.52,327.89,175.63,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,315.41,284.06,165.18,7.86;8,151.52,295.02,194.12,7.86">Empirical Studies on the NLP Techniques for Source Code Data Preprocessing</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2627508.2627514</idno>
		<ptr target="http://dx.doi.org/10.1145/2627508.2627514" />
	</analytic>
	<monogr>
		<title level="m" coord="8,417.75,295.02,62.84,7.86;8,151.52,305.98,329.07,7.86;8,151.52,316.93,75.13,7.86">Proceedings of the 2014 3rd International Workshop on Evidential Assessment of Software Technologies</title>
		<meeting>the 2014 3rd International Workshop on Evidential Assessment of Software Technologies<address><addrLine>EAST</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,338.85,33.71,7.86;8,192.90,338.85,21.76,7.86;8,230.89,338.85,27.19,7.86;8,274.32,338.85,10.24,7.86;8,300.80,338.85,15.88,7.86;8,332.91,338.85,27.65,7.86;8,376.80,338.85,7.42,7.86;8,400.46,338.85,54.17,7.86;8,470.87,338.85,9.73,7.86;8,151.52,349.81,16.13,7.86;8,188.23,349.81,53.35,7.86;8,262.16,349.81,47.63,7.86;8,330.38,349.81,42.49,7.86;8,393.45,349.81,14.85,7.86;8,428.88,349.81,51.72,7.86;8,151.52,360.74,24.95,7.89;8,200.34,360.77,32.25,7.86;8,256.47,360.77,28.16,7.86;8,308.49,360.77,172.09,7.86;8,151.52,371.73,178.44,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,300.80,338.85,15.88,7.86;8,332.91,338.85,27.65,7.86;8,376.80,338.85,7.42,7.86;8,400.46,338.85,54.17,7.86;8,470.87,338.85,9.73,7.86;8,151.52,349.81,16.13,7.86;8,188.23,349.81,49.79,7.86">The impact of preprocessing on text classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gunal</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2013.08.006</idno>
		<ptr target="http://dx.doi.org/10.1016/j.ipm.2013.08.006" />
	</analytic>
	<monogr>
		<title level="j" coord="8,262.16,349.81,47.63,7.86;8,330.38,349.81,42.49,7.86;8,393.45,349.81,14.85,7.86;8,428.88,349.81,51.72,7.86">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="104" to="112" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,382.69,337.97,7.86;8,151.52,393.62,329.07,7.89;8,151.52,404.61,177.98,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,213.34,382.69,232.37,7.86">The Porter stemming algorithm: Then and now</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Willett</surname></persName>
		</author>
		<idno type="DOI">10.1108/00330330610681295</idno>
		<ptr target="https://doi.org/10.1108/00330330610681295" />
	</analytic>
	<monogr>
		<title level="j" coord="8,460.49,382.69,20.10,7.86;8,151.52,393.65,200.69,7.86">Electronic library and information systems</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="219" to="223" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,415.56,337.97,7.86;8,151.52,426.52,329.07,7.86;8,151.52,437.48,329.07,7.86;8,151.52,448.44,25.60,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,195.97,415.56,280.61,7.86">Research on information retrieval model based on ontology</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13638-019-1354-z</idno>
		<ptr target="https://doi.org/10.1186/s13638-019-1354-z" />
	</analytic>
	<monogr>
		<title level="j" coord="8,151.52,426.52,292.73,7.86">EURASIP Journal on Wireless Communications and Networking</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,459.40,337.97,7.86;8,151.52,470.33,329.07,7.89;8,151.52,481.32,168.51,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,280.75,459.40,199.84,7.86;8,151.52,470.36,131.99,7.86">A Study of Web Information Extraction Technology Based on Beautiful Soup</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.17706/jcp.10.6.381-387</idno>
		<ptr target="https://doi.org/10.17706/jcp.10.6.381-387" />
	</analytic>
	<monogr>
		<title level="j" coord="8,292.24,470.36,90.32,7.86">Journal of Computers</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="387" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
