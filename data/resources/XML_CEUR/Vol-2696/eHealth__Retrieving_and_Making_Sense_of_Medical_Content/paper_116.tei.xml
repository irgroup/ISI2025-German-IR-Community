<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.00,115.96,325.36,12.62;1,286.24,133.89,42.87,12.62">Transformers in Semantic Indexing of Clinical Codes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,344.06,171.56,66.22,8.74"><forename type="first">Sachin</forename><surname>Krishan</surname></persName>
							<email>sachinkrishnan18128@cse.ssn.edu.in</email>
							<affiliation key="aff0">
								<orgName type="institution">SSN College Of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,426.89,171.56,45.69,8.74;1,279.27,183.51,56.82,8.74"><forename type="first">Aravindan</forename><surname>Chandrabose</surname></persName>
							<email>aravindanc@ssn.edu.in</email>
							<affiliation key="aff0">
								<orgName type="institution">SSN College Of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.00,115.96,325.36,12.62;1,286.24,133.89,42.87,12.62">Transformers in Semantic Indexing of Clinical Codes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2D952C27D9A8EE15493495E69C8A566E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>BERT</term>
					<term>Electra</term>
					<term>RoBERTa</term>
					<term>Transformers</term>
					<term>XLNet</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>International Classification of Diseases (ICD) codes is used to represent the diseases and health statuses which is useful for better communication and to facilitate research. Automatic assignment of these codes to the clinical records is essential nowadays whereas multilingual extraction task of eHealth@CLEF-2020 concentrates on predicting ICD10-CM and ICD10-PCS codes in Spanish language. Automatic prediction of these codes have been approached by using transformers such as BERT, RoBERTa, Electra and XLNet. Among the models, Electra predicts ICD10-CM codes better and RoBERTa predicts ICD10-PCS codes better than the other transformer models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The International Classification of Diseases (ICD) is a healthcare classification system maintained by the World Health Organization. Diseases and health statuses are classified according to certain rules and uniquely identified by character codes. The efficiency of ICD coding has begun to receive more attention because of how crucial it is for making clinical and financial decisions. Hospitals with better coding quality see many benefits, including more accurate classification and retrieval of medical records, and better communication with other hospitals to jointly promote healthcare quality and facilitate research.</p><p>Human classification of diagnoses is a labor intensive process that consumes significant resources. Most medical practitioners use specially trained medical coders to categorize diagnoses for billing and research purposes. The coding process requires a comprehensive consideration of each patient's health condition. However, very few medical practitioners are capable of taking over the process since they lack training in professional coding.</p><p>By the Multilingual Extraction Task [7] from eHealth Lab <ref type="bibr" coords="2,382.70,141.93,9.96,8.74" target="#b2">[3]</ref>, the goal is to assign ICD10 codes to clinical records. The task comprises of three subtasks wherein we have participated in two of them, namely, ICD10-CM (Clinical Modification) and ICD10-PCS (Procedure Classification System) codes assignment. The ICD10-CM sub task deals with automation of code assignment for classifying and reporting diseases in all healthcare settings while the ICD10-PCS sub task deals with code assignment for hospital reporting of inpatient services. The task is pursued as a multi-label text classification from our viewpoint while Named Entity Recognition and Normalization paths are also plausible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Description</head><p>The CodiEsp corpus of 1,000 clinical case studies, selected manually by a practicing physician was used for training the models. The CodiEsp corpus is distributed in plain text in UTF8 encoding, where each clinical case is stored as a single file whose name is the clinical case identifier. Each clinical case or its identifier is associated with one or multiple medical codes depending on the sub task's golden truth file.</p><p>The corpus consists of two variants of the clinical files. The first one being the original set filled with Spanish text and the other one being the English machine-translated version of the former. For our system we had used only the English version of the clinical case files to train the models.</p><p>The 1000 clinical case files were distributed as to constitute 500 train files, 250 development files and 250 test files. Additionally 2000 clinical files were released along with test set release to promote that these systems would potentially be able to scale to larger data collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>The task was approached as a multi-label text classification task with the use of transformers. The data is preprocessed and then data is made suitable for training by creating a data frame for all the instances of the data. Model training is done by fine tuning the parameters of four transformers that include BERT, RoBERTa, XLNet and Electra. After training, a file with the probability scores of all possible codes is generated which is post-processed to filter the suitable codes for the instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data preparation</head><p>Initially, the data is preprocessed and then data is made suitable for training. Preprocessing steps include removal of punctuation and stop words followed by tokenization and lemmatization. Then, the text files are prepared by mapping the textual data codes to a data frame that has ones and zeros.</p><p>For example : If there are 5 unique codes in a scenario whereas text data has code1, code3 as its code, Then the text file is trained with a data frame that has rows with ones only at the columns with that codes with remaining zeros. Data frame for that text -1 0 1 0 0 Here, there are 2194 unique labels in which each text file will have labels in the range 1 to 56 for each text file.Thus, the data frames are created for each text file and then given to the model for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample Input data:</head><p>text labels 0 describe case 37yearold man previous active.-[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...] 1 present head neck auscultation cardiac ....</p><p>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training the models</head><p>The transformer models namely BERT, RoBERTa, Electra and XLNet were used whose implementation is explained below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT</head><p>BERT <ref type="bibr" coords="3,166.09,464.84,10.52,8.74" target="#b1">[2]</ref> makes use of transformers, an attention mechanism that learns contextual relations between words in a text with two strategies namely masked LM and next sentence prediction. BERT was fine tuned over "Bio-Clinical-BERT" <ref type="bibr" coords="3,134.77,500.70,9.96,8.74" target="#b4">[5]</ref>, a model trained on all notes from MIMIC III, a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston for predicting the clinical codes.</p><p>In our implementation, the Bio-Clinical-BERT model was made use with ***.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RoBERTa</head><p>RoBERTa <ref type="bibr" coords="3,181.40,596.34,10.52,8.74" target="#b5">[6]</ref> makes use of a robustly optimized method that improves on BERT by modifying key hyperparameters in BERT.</p><p>For our implementation, Roberta was fine tuned over "BioMed-RoBERTa-base" <ref type="bibr" coords="3,134.77,644.16,9.96,8.74" target="#b3">[4]</ref>, a language model based on the RoBERTa-base architecture is trained over 100 epochs with train batch size of 20 and learning rate of 4e-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Electra</head><p>In general, electra <ref type="bibr" coords="4,219.81,154.86,9.96,8.74" target="#b0">[1]</ref>, a modification of BERT corrupts the input by replacing some tokens and trains a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not so that the contextual representations are learned in a better way.</p><p>For our work, electra was fined tuned over "Electra-base" model of 12 Transformer blocks, 768 hidden dimensions and 110 M parameters, trained over 100 epochs with train batch size of 20 and learning rate of 4e-5 to predict the probability of codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XLNet</head><p>XLNet <ref type="bibr" coords="4,159.76,298.32,15.00,8.74" target="#b6">[8]</ref> is a generalized AR pretraining method that uses a permutation language modeling objective to combine the advantages of AR (autoregression) and AE (autoencoding) methods that integrate Transformer-XL and the careful design of the two-stream attention mechanism.</p><p>For predicting the codes, XLNet was fine tuned over "xlnet-base-cased" model of parameters, 12 Transformer blocks, 12 self-attention heads and 768 hidden dimensions which is trained over 100 epochs with train batch size of 20 and learning rate of 4e-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Post-Processing</head><p>After training, a prediction file with a probability score of all 2194 codes is generated which has to be post processed. Post processing includes filtering the suitable codes. Here, we filtered with a threshold value of '0.002'. which seems to be effective. The probability of the codes greater than 0.002 are filtered and considered as codes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>File Model MAP Precision Recall F1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Semantic indexing of clinical codes is very desirable since term mapping of clinical codes are crucial now. We approached the semantic indexing by utilizing the transformers to predict the diagonstic (ICD10-CM ) and procedural (ICD10-PCS) codes for the data. BERT, RoBERTa, Electra and XLNet are the transformers that have been fine tuned to predict the codes. All the four transformer models have been used in ICD10-CM codes which Electra performs better and RoBERTa performs better in predicting ICD10-PCS codes among RoBERTa, Electra and XLNet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,205.96,591.89,203.43,62.81"><head>Table 1 .</head><label>1</label><figDesc>subtask 1 results</figDesc><table coords="4,205.96,591.89,203.43,41.94"><row><cell>Run-1 BERT</cell><cell cols="2">0.001 0.009</cell><cell>0.014 0.011</cell></row><row><cell cols="3">Run-2 Electra 0.007 0.025</cell><cell>0.049 0.033</cell></row><row><cell cols="3">Run-3 RoBERTa 0.004 0.014</cell><cell>0.019 0.016</cell></row><row><cell>Run-4 XLNet</cell><cell>0</cell><cell>0</cell><cell>0.001 0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,118.99,345.83,131.62"><head>Table 1 and</head><label>1</label><figDesc>Table2shows the results of our submission in subtask1 and subtask2 respectively. From Table1, Electra model has attained a better Precision score 0f 0.007, MAP score of 0.025, F1 score of 0.033 and recall of 0.049 than the other models for subtask 1 whereas Roberta attains the better scores in subtask2. The Roberta model of subtask 2 has the best recall 0.075 among all our submissions.</figDesc><table coords="5,201.87,208.65,208.25,41.96"><row><cell>File</cell><cell cols="4">Model MAP Precision Recall F1</cell></row><row><cell cols="2">Run-1 Electra</cell><cell>0</cell><cell>0.001</cell><cell>0.002 0.001</cell></row><row><cell cols="3">Run-2 RoBERTa 0.028</cell><cell>0.016</cell><cell>0.075 0.027</cell></row><row><cell cols="2">Run-3 XLNet</cell><cell>0.002</cell><cell>0.002</cell><cell>0.007 0.003</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,253.79,263.60,107.78,7.89"><head>Table 2 .</head><label>2</label><figDesc>subtask 2 results</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We would like to thank <rs type="institution">DST-SERB</rs> and <rs type="institution">HPC laboratory</rs> for providing resources needed for this work.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="5,138.35,537.77,342.24,7.86;5,146.91,548.73,333.68,7.86;5,146.91,559.69,185.37,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,377.73,537.77,102.85,7.86;5,146.91,548.73,245.04,7.86">ELECTRA: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=r1xMH1BtvB" />
	</analytic>
	<monogr>
		<title level="m" coord="5,422.27,548.73,22.52,7.86">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,569.89,342.24,7.86;5,146.91,580.85,230.16,7.86" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<title level="m" coord="5,353.32,569.89,127.27,7.86;5,146.91,580.85,201.50,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,591.04,342.24,7.86;5,146.91,602.00,333.68,7.86;5,146.91,612.96,333.68,7.86;5,146.91,623.92,333.68,7.86;5,146.91,634.88,333.68,7.86;5,146.91,645.84,333.68,7.86;5,146.91,656.80,121.10,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,359.51,602.00,121.08,7.86;5,146.91,612.96,77.27,7.86">overview of the CLEF eHealth evaluation lab 2020</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Saez Gonzales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Viviani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,450.75,623.92,29.84,7.86;5,146.91,634.88,333.68,7.86;5,146.91,645.84,277.21,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction: Proceedings of the Eleventh International Conference of the CLEF Association (CLEF</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Ferro</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">number</biblScope>
			<biblScope unit="page">12260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,119.67,342.24,7.86;6,146.91,130.63,333.68,7.86;6,146.91,141.59,123.46,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,200.61,130.63,276.17,7.86">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,160.99,141.59,80.71,7.86">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,152.55,342.24,7.86;6,146.91,163.51,333.68,7.86;6,146.91,174.47,333.68,7.86;6,146.91,185.43,196.94,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,438.27,152.55,42.32,7.86;6,146.91,163.51,333.68,7.86;6,146.91,174.47,11.14,7.86">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
		<ptr target="http://dx.doi.org/10.1093/bioinformatics/btz682" />
	</analytic>
	<monogr>
		<title level="j" coord="6,168.56,174.47,58.56,7.86">Bioinformatics</title>
		<imprint>
			<date type="published" when="2019-09">Sep 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,196.39,342.25,7.86;6,146.91,207.34,333.68,7.86;6,146.91,218.30,25.60,7.86;6,134.77,229.26,345.83,7.86;6,146.91,240.22,333.68,7.86;6,146.91,251.18,333.68,7.86;6,146.91,262.14,333.68,7.86;6,146.91,273.10,62.00,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,248.76,207.34,231.83,7.86;6,146.91,240.22,333.68,7.86;6,146.91,251.18,234.90,7.86">Overview of automatic clinical coding: annotations, guidelines, and solutions for non-english clinical cases at codiesp track of CLEF eHealth</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Armengol-Estapé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,421.22,251.18,59.37,7.86;6,146.91,262.14,233.74,7.86">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum</title>
		<title level="s" coord="6,389.02,262.14,91.57,7.86;6,146.91,273.10,33.34,7.86">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2019">2019. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized bert pretraining approach</note>
</biblStruct>

<biblStruct coords="6,138.35,284.06,342.25,7.86;6,146.91,295.02,296.05,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,455.76,284.06,24.83,7.86;6,146.91,295.02,267.39,7.86">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
