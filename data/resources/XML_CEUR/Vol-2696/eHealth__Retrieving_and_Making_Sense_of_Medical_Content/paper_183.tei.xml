<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,175.45,115.96,264.45,12.62;1,135.08,133.89,345.20,12.62;1,261.39,151.82,92.58,12.62">Convolutional Attention Models with Hierarchical Post-Processing Heuristics at CLEF eHealth 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,220.95,189.49,53.38,8.74"><forename type="first">Elias</forename><surname>Moons</surname></persName>
							<email>elias.moons@cs.kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.03,189.49,97.38,8.74"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,175.45,115.96,264.45,12.62;1,135.08,133.89,345.20,12.62;1,261.39,151.82,92.58,12.62">Convolutional Attention Models with Hierarchical Post-Processing Heuristics at CLEF eHealth 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2C73EA1D56295C9EEAB1A241AEF2726F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we compare state-of-the-art neural network approaches to the 2020 CLEF eHealth task 1. The presented models use the neural principles of convolution and attention to obtain their results. Furthermore, a hierarchical component is introduced as well as hierarchical post-processing heuristics. These additions successfully leverage the information that is inherently present in the ICD taxonomy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we compare different neural network approaches in the context of the CLEF eHealth 2020 task 1 <ref type="bibr" coords="1,286.07,377.44,10.79,8.74" target="#b1">[2]</ref> <ref type="bibr" coords="1,296.86,377.44,10.79,8.74" target="#b4">[5]</ref>. More specifically, we have submitted predictions for subtasks 1 and 2 which evaluate systems that predict diagnostic and procedural ICD-codes, respectively. Diagnostic codes represent all the different diagnoses and their variants themselves. Procedural codes identify what was done to or given to a patient (medication, surgeries, etc.). Our strategy combines the principles of convolutional neural networks and attention mechanisms. Furthermore, these models are extended with a hierarchical objective, corresponding to the underlying ICD taxonomy. Lastly, hierarchical heuristics are used for post-processing the results.</p><p>The dataset consists of 1,000 clinical cases, tagged with various ICD-10 codes by health specialists. The original text fragments are in Spanish but an automatically translated version in English is also provided by the organisers. This version was used in this research as the described models are optimized for English texts. Assessing the influence of using this translated version instead of the original Spanish texts would be an interesting addition in future works. The dataset contains a split of 500 training samples, 250 development samples and 250 test samples. In total the 1,000 documents comprise of 16,504 sentences and 396,988 words, with an average of 396.2 words per clinical case. For the first subtask, these documents are trained with corresponding diagnostic ICD-code tags. For the second subtask, these same documents were trained with their procedural ICD-codes instead. The biggest hurdle while training with this dataset is the size and consequently the small number of training samples for each category present. For the diagnostic ICD-codes for example, there are in total 1,767 different categories spread out over only 500 training documents. Every document is labeled with on average 11.  In this paper we hypothesize that exploiting the knowledge of the hierarchical label taxonomy of ICD-10 helps the performance of automated coding when limited training examples that are manually coded are available.</p><p>The remainder of this paper is organized as follows. In section 2 related work relevant for the conducted research will be discussed. The evaluated deep learning methods are described in section 3. These methods are evaluated on the benchmark CodiEsp ICD-10 dataset and all findings are reported in section 4. The most important findings will be recapped in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The most prominent and more recent advancements in categorizing medical reports with standard codes will shortly be described in this section.</p><p>In <ref type="bibr" coords="2,161.90,632.21,10.52,8.74" target="#b6">[7]</ref> an hierarchical support vector machine (SVM) is shown to outperform that of a flat SVM. Results were reported based of F-measure scores on the Mimic-II dataset. <ref type="bibr" coords="2,213.71,656.12,10.52,8.74" target="#b2">[3]</ref> show that datasets of different sizes and different numbers of distinct codes demand different training mechanisms. For small datasets, feature and data selection methods serve better. The authors have evaluated ICD coding performance on a dataset consisting of more than 70,000 textual EMRs (Electronic Medical Records) from the University of Kentucky (UKY) Medical Center tagged with ICD-9 codes.</p><p>A deep learning model that encompasses an attention mechanism is tested by <ref type="bibr" coords="3,149.10,190.99,10.52,8.74" target="#b8">[9]</ref> on the Mimic-III dataset. LSTMs are used for both character and word level representations. A soft attention layer here helps in making predictions for the top 50 most frequent ICD-9 codes in the dataset.</p><p>More recently, <ref type="bibr" coords="3,216.80,227.11,10.52,8.74" target="#b0">[1]</ref> have introduced the Hierarchical Attention bidirectional Gated Recurrent Unit model (HA-GRU). By identifying relevant sentences for each label, documents are tagged with corresponding ICD-9 codes. Results are reported both on the Mimic-II and Mimic-III datasets. <ref type="bibr" coords="3,379.47,262.98,10.52,8.74" target="#b5">[6]</ref> presents the Convolutional Attention for Multi-Label classification (CAML) model that combines the strengths of convolutional networks and attention mechanisms. They propose adding regularization on the long descriptions of the target ICD codes, especially to improve classification results on less represented categories in the dataset. This approach is further extended with the idea of multiple convolutional channels by <ref type="bibr" coords="3,219.59,334.71,10.52,8.74" target="#b7">[8]</ref> with max pooling across all channels. The authors also shift the attention from the last prediction layer, as in <ref type="bibr" coords="3,370.05,346.66,9.96,8.74" target="#b5">[6]</ref>, to the attention layer. <ref type="bibr" coords="3,134.77,358.62,10.52,8.74" target="#b5">[6]</ref> and <ref type="bibr" coords="3,169.32,358.62,10.52,8.74" target="#b7">[8]</ref> achieve state-of-the art results for ICD-9 coding on the MIMIC-III dataset. As an addition to these models, in this paper a hierarchical variant of each of them is constructed and evaluated. Furthermore, if the target output space of categories follows a hierarchy of labels -as is also the case in ICD coding -the trained models can efficiently use this hierarchy for category assignment <ref type="bibr" coords="3,134.77,418.39,10.72,8.74" target="#b6">[7]</ref>[10] <ref type="bibr" coords="3,159.77,418.39,10.72,8.74" target="#b3">[4]</ref>. During categorization the models apply a top-down or a bottom-up approach at the classification stage. In a top-down approach parent categories are assigned first and only children of assigned parents are considered as category candidates. In a bottom-up approach only leaf nodes in the hierarchy are assigned which entail that parent nodes are assigned. The hierarchical structure of a tree leads to various parent-child relations between its categories. For the models discussed in this paper, an hierarchical variant will also be tested which exploits the information of the tree structure and shows that it can enhance the classification performance. Recent research shows the value of these hierarchical dependencies using hierarchical attention mechanisms <ref type="bibr" coords="3,398.04,525.99,10.52,8.74" target="#b0">[1]</ref> and hierarchical penalties <ref type="bibr" coords="3,176.61,537.95,15.50,8.74" target="#b10">[11]</ref> which are also integrated in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we explain the used models for ICD code prediction. First, the preprocessing step is shortly discussed. Then, two recent state-of-the-art models in the field of ICD coding are explained in detail. These models are implemented by the authors following the original papers and are called DR-CAML <ref type="bibr" coords="3,450.85,632.21,10.52,8.74" target="#b5">[6]</ref> and MVC-(R)LDA <ref type="bibr" coords="3,210.37,644.16,9.96,8.74" target="#b7">[8]</ref>, respectively. We discuss in detail the attention mechanisms and loss functions of these models. Afterwards, as a way of handling the hierar-chical dependencies of the ICD-codes, we propose various ways of their integration in all models. This is based on advancements in hierarchical classification as inspired by <ref type="bibr" coords="4,202.01,142.90,14.61,8.74" target="#b10">[11]</ref>. Lastly, heuristics are described for post-processing of the predictions given by the models. This leads in section 4 to a clear comparison between all tested models among themselves as well as with their hierarchical novel variants and the introduced post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>The preprocessing follows as standard procedure described in <ref type="bibr" coords="4,398.52,230.81,9.96,8.74" target="#b5">[6]</ref>, i.e., tokens that contain no alphabetic characters are removed and all tokens are put to lowercase. Furthermore tokens that appear in fewer than three training documents are replaced with the 'UNK' token. All documents are then truncated to a maximum length of 2500 tokens.</p><p>All discussed models have for each document i as input, a sequence of word vectors x i as their representation and as output, a set of ICD-codes y i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional models</head><p>This subsection describes the details of recent state-of-the-art models presented in <ref type="bibr" coords="4,146.39,366.75,10.52,8.74" target="#b5">[6]</ref> and <ref type="bibr" coords="4,179.60,366.75,10.52,8.74" target="#b7">[8]</ref> in the way they are used for the experiments in section 4.</p><p>DR-CAML DR-CAML is a CNN based model adopted for ICD coding <ref type="bibr" coords="4,467.31,397.75,9.96,8.74" target="#b5">[6]</ref>. When an ICD code is defined by the WHO, it is accompanied by a label definition expressed in natural language to guide the model towards learning the appropriate parameter values of the model. For this purpose the model employs a per-label attention mechanism enabling it to learn distinct document representations for each label. It has been shown that for labels for which there are very few training instances available, this approach is advantageous. The idea is that the description of a target code is itself a very good training example for the corresponding code. Similarity between the representation of a given test sample and the representation of the description of a target code gives extra confidence in assigning this label.</p><p>In general, after the convolutional layer, DR-CAML employs a per-label attention mechanism to attend to the relevant parts of text for each predicted label. An additional advantage is that the per-label attention mechanism provides the model with the ability of explaining why it decided to assign each code by showing the spans of text relevant for the ICD code.</p><p>MVC-(R)LDA Both MVC-LDA and MVC-RLDA, can be seen as extensions of DR-CAML. Similar to that model, they are based on a CNN architecture with a label attention mechanism that considers ICD coding as a multi-task binary classification problem. The added functionality lies in the use of parallel CNNs with different kernel sizes to capture information of different granularity.</p><p>In general, these multi-view CNNs are constructed with four CNNs that have the same number of filters but with different kernel sizes. This convolutional layer is followed by a max-pooling function across all channels to select the most relevant span of text for each filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss function</head><p>The loss functions used to train DR-CAML and the multi-view models MVD-(R)LDA are calculated in the same way. The general loss function is the binary cross entropy loss loss BCE . This loss is extended by regularization on the long description vectors of the target categories Given N different training examples x i . The values of ŷl and max-pooled vector z l can be calculated by getting the description of code l out of all L target codes. In this figure and the following formulas β l is a vector of prediction weights and v l the vector representation for code l. Assuming n y is the number of true labels in the training data, the final loss is computed by adding regularization to the base loss function as:</p><formula xml:id="formula_0" coords="5,188.62,312.51,291.98,48.00">ŷl = σ(β t l v l + b l ) (1) loss BCE (X) = - N i=1 L l=1 y l log (ŷ l ) + (1 -y l ) log (1 -ŷl )<label>(2)</label></formula><formula xml:id="formula_1" coords="5,184.65,365.14,295.95,30.55">loss M odel (X) = loss BCE + λ 1 n y N i=1 L l=1 z l -β l 2<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modelling hierarchical dependencies</head><p>In this section we investigate the modelling of hierarchical dependencies as extensions of the models described above. A first part integrates the hierarchical dependencies directly into the structure of the model. This leads to Hierarchical models, which are layered variants of the already discussed approaches. The second way hierarchical dependencies are explicitly introduced into the model is via the use of a hierarchical loss function to penalize hierarchical inconsistencies across the model's prediction layer.</p><p>Hierarchical models Hierarchical relationships can be shaped directly into the architecture of any of the described models above. The ICD-10 taxonomy can be modeled as a tree with a general ICD root and 4 levels of depth. On the highest level, codes have 1 character, the next 2 levels represent categories with respectively 3 and 4 characters. The rest of the codes are combined in the last layer. This leads to a hierarchical variant of any of the models. In this variant, not 1 but 4 identical models will be trained, one for each of the different layers in the ICD hierarchy (corresponding to the length of the codes). An overview of the approach is given in figure <ref type="figure" coords="5,353.93,632.21,3.87,8.74">2</ref>. The input for each layer is partially dependent on an intermediary representation from the previous layer as well as the original input through concatenation of both. Layers are stacked Fig. <ref type="figure" coords="6,189.51,307.17,4.13,7.89">2</ref>. Overview of hierarchical variant of a model, inspired by <ref type="bibr" coords="6,428.58,307.19,13.52,7.86" target="#b10">[11]</ref>.</p><p>from most to least specific or from leaf to root node in the taxonomy. Models corresponding to different layers will then rely on different features, or characteristics, to classify the input vectors. This way the deepest, most advanced representations, can be used for classifying the most abstract and broad categories. On the other hand, for the most specific categories, word level features can directly be used to make detailed decisions between classes that are very similar.</p><p>Hierarchical loss function To capture the hierarchical relationships in a given model, the loss function of the above models can be extended with an additional term. This leads to the definition of a Hierarchical loss function (loss H ). This loss function penalizes classifications that contradict the inherent ICD hierarchy. More specifically, when a parent category is not predicted to be true, none of its child categories should be predicted to be true. The hierarchical loss between a child and its parent in the tree is then defined as the difference between their computed probability scores, with 0 as a lower bound. More formally, for the entire loss function loss H M odel for a category of layer X, combining the regular training loss loss M odel described above and the hierarchical loss loss H , is calculated as follows:</p><formula xml:id="formula_2" coords="6,235.13,592.76,245.47,8.74">P (X) = P robability(X == T rue)<label>(4)</label></formula><p>P ar(X) = P robability(P arent(X) == T rue)</p><p>L(X) = T rue label of X(0 or 1) <ref type="bibr" coords="6,467.86,622.64,12.73,8.74" target="#b5">(6)</ref> loss H (X) = Clip(P (X) -P ar(X), 0, 1) ( <ref type="formula" coords="6,472.10,637.59,4.24,8.74">7</ref>)</p><formula xml:id="formula_4" coords="6,192.39,652.53,288.20,9.65">loss H M odel (X) = (1 -λ)loss M odel (X) + λloss H (X)<label>(8)</label></formula><p>which leaves a parameter λ to optimize the loss function. 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hierarchical post-processing</head><p>As a final step in the classification process, a heuristical post-processing will be applied to some of the submitted models. All considered heuristics are explained below. They are all reliant on the distance of any pair of target categories in the ICD-10 taxonomy and reweigh the prediction values accordingly. The heuristics are numbered from H1 until H7 for efficient referencing in the result section.</p><p>Node distance (H1) Given all L predictions y i made for document i by any given model, the new prediction values y post1 i can be calculated as follows:</p><formula xml:id="formula_5" coords="7,246.07,276.20,230.28,30.32">y post1 i = L j=1 ( y j (1 + dist(i, j)) . (<label>9</label></formula><formula xml:id="formula_6" coords="7,476.35,286.61,4.24,8.74">)</formula><p>The newly calculated prediction values are the result of a weighted sum of all previously calculated prediction values, taking into account the relative distances of all target categories in the ICD taxonomy. In general, dist(i, j) gives the distance between categories i and j in the ICD tree, e.g., the distance between a parent and its child is 1, the distance between two siblings is 2 and the distance of an element to itself is 0.</p><p>Node distance from child to ancestor (H2) This heuristic functions the same way as the heuristic described above but differs in behavior if the lowest common ancestor (LCA) of categories i and j which is not j itself. y j will only be added to the total new score of category i if j is an ancestor of i. This can be formally described as follows:</p><formula xml:id="formula_7" coords="7,246.21,473.64,229.96,30.32">y post2 i = L j=1 dist a,c (i, j) * y j ; (<label>10</label></formula><formula xml:id="formula_8" coords="7,476.16,484.05,4.43,8.74">)</formula><formula xml:id="formula_9" coords="7,176.82,514.41,299.34,37.16">dist a,c (i, j) =    1 (1 + dist(i, j) , if ancestor(i, j) == True 0, if ancestor(i, j) == False. (<label>11</label></formula><formula xml:id="formula_10" coords="7,476.16,529.70,4.43,8.74">)</formula><p>Node distance from ancestor to child (H3) This heuristic functions analogous to heuristic H2 but in the opposite direction. y j will only be added to the total new score of category i if i is an ancestor of j. This gives:</p><formula xml:id="formula_11" coords="7,137.50,615.41,343.09,49.25">y post3 i = L j=1 dist c,a (i, j) * y j ; (12) 1 Parameter λ is optimized over the training set. dist c,a (i, j) =    1 (1 + dist(i, j) , if ancestor(j, i) == True 0, if ancestor(j, i) == False.<label>(13)</label></formula><p>Node distance between ancestors and children (H4) Heuristic H4 combines the ideas presented in the previous two heuristics, only adding y i when either i is an ancestor of j or j is an ancestor of i. Using equations 11 and 13, this evaluates to:</p><formula xml:id="formula_12" coords="8,213.12,226.26,267.47,30.32">y post1 i = L j=1 (dist a,c (i, j) + dist c,a (i, j)) * y j .<label>(14)</label></formula><p>Squared node distance (H5) This heuristic functions as heuristic H1 but squares the value of its distance function. As a result, it gives relatively more weight to predictions made for categories that are closer the observed category in comparison to H1. This leads to the following relationship:</p><formula xml:id="formula_13" coords="8,243.83,331.52,236.76,30.32">y post5 i = L j=1 ( y j (1 + dist(i, j) 2 ) .<label>(15)</label></formula><p>Squared node prediction values (H6) Heuristic H6 differs from the first heuristic in that it rescales the starting prediction values y i . Instead of using the calculated values it will use the squares of these values, making discrepancies in prediction values relatively more prominent. The resulting values can be calculated via:</p><formula xml:id="formula_14" coords="8,246.07,438.08,234.52,30.32">y post6 i = L j=1 ( y 2 j (1 + dist(i, j)) .<label>(16)</label></formula><p>Squared node distances and prediction values (H7) This heuristic combines the ideas that comprise heuristics H5 and H6, leading to the following relationship:</p><formula xml:id="formula_15" coords="8,243.83,522.66,232.33,30.32">y post7 i = L j=1 ( y 2 j (1 + dist(i, j) 2 ) . (<label>17</label></formula><formula xml:id="formula_16" coords="8,476.16,533.07,4.43,8.74">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>For both the subtasks of predicting diagnostic and procedural codes, 5 different models were trained, this was the maximum amount allowed in the competition. Since the size of the dataset was a problem during training, the authors chose to only train models for the top-50 most represented categories in the training dataset. During training of the hierarchical models, ancestors of the top-50 categories were added as well, but only the performance on the original 50 categories was taken into account for calculating the result metrics. A selection of models was chosen aiming for much variety to be able to assess the influence of both proposed models (CAML and MVC-RLDA), the hierarchical objective and postprocessing using a heuristic. The chosen models are summarized below and are the same for both subtasks:</p><p>1. CAML 2. CAML + hierarchical objective 3. MVC-RLDA + hierarchical objective 4. CAML + hierarchical post-processing H1 5. MVC-RLDA + hierarchical objective + hierarchical post-processing H1</p><p>First, one baseline without use of the hierarchy and heuristics was chosen. Since CAML got slightly better results than MVC-RLDA on the development set, this model was selected. Second, to assess the influence the hierarchy can have on the classification results, both CAML and MVC-RLDA models were trained with a hierarchical objective. The last 2 models were chosen with the post-processing heuristic in mind. Only heuristic H1 was chosen for this (based on higher performance on the development set), once in a setting without hierarchical objective (with CAMl) and once with the hierarchical objective (and MVC-RLDA). Since the models used in this paper had a lot of difficulties with the small number of training examples, the prediction probabilities of all categories were rather close together (often in the range of 0.3 to 0.5 instead of from 0.0 until 1.0). For this reason, the prediction files were generated using the top-5 highest predicted categories instead of using a fixed cut-off point. This is not optimal for obtaining a high MAP, where it is better to submit more categories leading to lower performance values. The results obtained by these prediction files are visible in tables 1 and 2 for diagnostic and procedural subtasks respectively. For the case of diagnostic codes, visible in table 1, the best performance is achieved by the CAML model in combination with heuristical post-processing H1. Adding the heuristic to CAML leads to a clear improvement in classification quality. Comparing CAML with CAML+Hier. leads to the conclusion that the hierarchy can as well lead to an improvement, but it is less prominent than using the post-processing heuristic. Furthermore, it is clear that the MVC-RLDA model gets outperformed by CAML. This is most likely due to the fact that the For the case of procedural codes, visible in table 2, the best results are now obtained by a combination of CAML with a hierarchical objective. This is closely followed by CAML with a post-processing heuristic. Both techniques improve the classification scores significantly but the overall scores are lower than for the task of classifying diagnostic codes. Lastly, both MVC-RLDA models predicted invalid codes for all documents in the test set, not being able to learn significant relations present in the data.</p><p>As an extra experiment to assess the performance of the described heuristics, a CAML model got post-processed with 7 different heuristics. In this case, not only the top-5 categories were retained but the top-50 categories were all sorted by confidence. These resulting files were then evaluated by the evaluation file provided by the competition and results are reported in table <ref type="table" coords="10,406.22,389.67,3.87,8.74" target="#tab_3">3</ref>. For both the subtasks of classifying diagnostic and procedural codes, the use of heuristic H1 is the clear winner. It is worth noting that in no case, the results of the baseline got worse because of the use of a post-processing heuristic. Furthermore, in most cases this has led to an improvement of the results strengthening the claim that post-processing heuristics based on the ICD-10 taxonomy can be a valuable tool. Next to H1, the best performing heuristic is H5 which squares the distances between nodes in the classification tree. Since all heuristics that try to give more weight to nodes closer to the observed node underperform with respect to H1, it might be interesting to see whether the opposite can further improve the classification process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we trained 5 models for participation in 2 subtasks of the 2020 CLEF eHealth task 1. For both subtasks, experiments were conducted, yielding interesting results. The hierarchical component as well as the use of postprocessing heuristics proved their value in this setting. The use of a multi-view neural network led to an abundance of trainable parameters which ultimately made the model unable to efficiently generalize over the training samples. An extra experiment was conducted to asses the influence of the presented postprocessing heuristics. This led to the conclusion that these heuristics can be a powerful tool for the classification of ICD codes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,420.34,345.83,7.89;2,134.77,431.32,86.31,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Category frequencies of CodiEsp training dataset (diagnostic on the left, procedural on the right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,134.77,115.83,345.82,176.56"><head></head><label></label><figDesc></figDesc><graphic coords="6,134.77,115.83,345.82,176.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,134.77,154.86,345.83,92.42"><head></head><label></label><figDesc>3 different categories and each category is on average represented by 3.2 training examples. Only seven categories have more than 50 training examples. For the case of procedural ICD-codes, these numbers are slightly lower with 563 different categories, 3.1 categories per example and only 2.7 training examples for each category, leading to a very similar distribution. Figure 1 gives a sorted view of all categories present in the diagnostic training dataset (left) as well as the procedural training dataset (right) and the amount of examples tagged with that specific category.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,164.13,471.71,284.03,83.86"><head>Table 1 .</head><label>1</label><figDesc>Results on the diagnostic codes subtask.</figDesc><table coords="9,334.64,492.51,108.33,7.86"><row><cell>MAP Precision Recall F1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,134.77,115.91,345.82,135.68"><head>Table 2 .</head><label>2</label><figDesc>Results on the procedural codes subtask.</figDesc><table coords="10,349.02,136.71,108.33,7.86"><row><cell>MAP Precision Recall F1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,197.66,424.51,220.05,116.73"><head>Table 3 .</head><label>3</label><figDesc>Comparison of all post-processing heuristics.</figDesc><table coords="10,200.87,445.31,210.54,95.93"><row><cell></cell><cell cols="2">Diagnostic(MAP) Procedural(MAP)</cell></row><row><cell>CAML</cell><cell>0.042</cell><cell>0.052</cell></row><row><cell>CAML + H1</cell><cell>0.075</cell><cell>0.060</cell></row><row><cell>CAML + H2</cell><cell>0.042</cell><cell>0.052</cell></row><row><cell>CAML + H3</cell><cell>0.050</cell><cell>0.052</cell></row><row><cell>CAML + H4</cell><cell>0.050</cell><cell>0.052</cell></row><row><cell>CAML + H5</cell><cell>0.053</cell><cell>0.058</cell></row><row><cell>CAML + H6</cell><cell>0.054</cell><cell>0.052</cell></row><row><cell>CAML + H7</cell><cell>0.047</cell><cell>0.052</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,338.73,337.63,7.86;11,151.52,349.69,240.64,7.86" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Baumel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nassour-Kassis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
		<title level="m" coord="11,383.21,338.73,97.38,7.86;11,151.52,349.69,211.97,7.86">Multi-label classification of patient notes a case study on icd code assignment</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,360.69,337.63,7.86;11,151.52,371.65,329.07,7.86;11,151.52,382.61,329.07,7.86;11,151.52,393.57,329.07,7.86;11,151.52,404.53,329.07,7.86;11,151.52,415.48,329.07,7.86;11,151.52,426.44,148.63,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,357.72,371.65,122.87,7.86;11,151.52,382.61,76.44,7.86">Overview of the CLEF eHealth evaluation lab 2020</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Saez Gonzales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Viviani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,453.30,393.57,27.29,7.86;11,151.52,404.53,329.07,7.86;11,151.52,415.48,296.38,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction: Proceedings of the Eleventh International Conference of the CLEF Association (CLEF</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Ferro</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">number</biblScope>
			<biblScope unit="page">12260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,437.45,337.63,7.86;11,151.52,448.40,329.08,7.86;11,151.52,459.34,185.98,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,287.23,437.45,193.36,7.86;11,151.52,448.40,285.32,7.86">An empirical evaluation of supervised learning approaches in assigning diagnosis codes to electronic medical records</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kavuluru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,444.99,448.40,35.60,7.86;11,151.52,459.36,93.96,7.86">Artificial intelligence in medicine</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,470.37,337.63,7.86;11,151.52,481.32,329.07,7.86;11,151.52,492.28,306.29,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,174.03,481.32,222.32,7.86">Hdltex: Hierarchical deep learning for text classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heidarysafa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Meimandi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,416.51,481.32,64.09,7.86;11,151.52,492.28,259.31,7.86">2017 16th IEEE International Conference on Machine Learning and Applications</title>
		<imprint>
			<date type="published" when="2017-12">Dec 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,503.29,337.64,7.86;11,151.52,514.24,329.07,7.86;11,151.52,525.20,329.07,7.86;11,151.52,536.16,329.07,7.86;11,151.52,547.12,76.75,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,151.52,514.24,329.07,7.86;11,151.52,525.20,249.95,7.86">Overview of automatic clinical coding: annotations, guidelines, and solutions for non-english clinical cases at codiesp track of CLEF eHealth</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Armengol-Estapé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,446.52,525.20,34.07,7.86;11,151.52,536.16,329.07,7.86;11,151.52,547.12,48.07,7.86">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,558.12,337.63,7.86;11,151.52,569.08,329.07,7.86;11,151.52,580.04,329.07,7.86;11,151.52,591.00,329.07,7.86;11,151.52,601.96,329.07,7.86;11,151.52,612.92,329.07,8.12;11,151.52,624.52,85.23,7.47" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,433.10,558.12,47.49,7.86;11,151.52,569.08,197.89,7.86">Explainable prediction of medical codes from clinical text</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mullenbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1100</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-1100" />
	</analytic>
	<monogr>
		<title level="m" coord="11,376.04,569.08,104.55,7.86;11,151.52,580.04,329.07,7.86;11,151.52,591.00,201.20,7.86">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06">Jun 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1101" to="1111" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="11,142.96,634.88,337.63,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.77,202.97,7.89" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,151.52,645.84,242.72,7.86">Diagnosis code assignment: models and evaluation metrics</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Perotte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pivovarov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,402.77,645.84,77.82,7.86;11,151.52,656.80,23.14,7.86">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">24296907</biblScope>
			<date type="published" when="2014-03">Mar 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,119.67,337.64,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,329.07,7.86;12,151.52,152.55,159.05,7.86" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sadoughi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">P</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Korenevski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Baryshnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Axtmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Suendermann-Oeft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01468</idno>
		<title level="m" coord="12,380.23,130.63,100.35,7.86;12,151.52,141.59,325.16,7.86">Medical code prediction with multi-view convolution and description-regularized label-dependent attention</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.96,163.51,337.63,7.86;12,151.52,174.47,242.58,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="12,353.74,163.51,126.85,7.86;12,151.52,174.47,76.59,7.86">Towards automated icd coding using deep learning</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04075</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,185.43,337.98,7.86;12,151.52,196.36,329.07,7.89;12,151.52,207.34,22.02,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,260.75,185.43,219.85,7.86;12,151.52,196.39,80.03,7.86">A survey of hierarchical classification across different application domains</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">N</forename><surname>Silla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,240.12,196.39,162.06,7.86">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="72" />
			<date type="published" when="2011-01">Jan 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,218.30,337.98,7.86;12,151.52,229.26,329.07,7.86;12,151.52,240.22,329.07,7.86;12,151.52,251.18,49.79,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,309.50,218.30,171.09,7.86;12,151.52,229.26,21.39,7.86">Hierarchical multi-label classification networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,298.24,229.26,182.36,7.86;12,151.52,240.22,23.54,7.86">ICML. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting><address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-15">10-15 Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5075" to="5084" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
