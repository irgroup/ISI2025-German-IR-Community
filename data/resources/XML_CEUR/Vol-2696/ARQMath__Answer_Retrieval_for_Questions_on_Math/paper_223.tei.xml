<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,232.15,115.90,151.07,12.68;1,213.98,133.83,187.39,12.68">DPRL Systems in the CLEF 2020 ARQMath Lab</title>
				<funder ref="#_C2qMSEX">
					<orgName type="full">Alfred P. Sloan Foundation</orgName>
				</funder>
				<funder ref="#_FZg8Jgk">
					<orgName type="full">National Science Foundation (USA)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,171.73,171.50,77.58,8.80"><forename type="first">Behrooz</forename><surname>Mansouri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology)</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,261.94,171.50,74.36,8.80"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.44,171.50,70.72,8.80"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology)</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,232.15,115.90,151.07,12.68;1,213.98,133.83,187.39,12.68">DPRL Systems in the CLEF 2020 ARQMath Lab</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">81B8EC87281131730744BC81783483BD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Community Question Answering (CQA)</term>
					<term>Mathematical Information Retrieval</term>
					<term>Math-aware search</term>
					<term>Math formula search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the Document and Pattern Recognition Lab from the Rochester Institute of Technology in the CLEF 2020 ARQMath lab. There are two tasks defined for ARQ-Math: (1) Question Answering, and (2) Formula Retrieval. Four runs were submitted for Task 1 using systems that take advantage of text and formula embeddings. For Task 2, three runs were submitted: one uses only formula embedding, another uses formula and text embeddings, and the final one uses formula embedding followed by re-ranking results by tree-edit distance. The Task 2 runs yielded strong results, the Task 1 results were less competitive.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ARQMath lab at CLEF 2020 has two tasks <ref type="bibr" coords="1,358.89,459.96,9.96,8.80" target="#b2">[3]</ref>. In the first task, called Answer Retrieval, given the mathematical questions, the participants were asked to return a set of relevant answers to each question. Formula Retrieval is the second task, where given a mathematical formula as the query, the participants were asked to retrieve a set of relevant formulas.</p><p>The Document and Pattern Recognition Lab (DPRL) from the Rochester Institute of Technology (RIT, USA) participated in both tasks. For Task 1, we used a text and formula embedding approach, where each question and answer are represented by a vector of float numbers. We submitted 4 runs for this task.</p><p>For the second task, we submitted 3 runs, all based on a mathematical formula embedding model, Tangent-CFT <ref type="bibr" coords="1,311.68,579.51,9.96,8.80" target="#b3">[4]</ref>. This embedding model uses both Symbol Layout Tree (SLT) and Operator Tree (OPT) formula representations. The SLT representation encodes the appearance of the formula, while the OPT representation encodes its operator syntax (i.e., the operator tree). shows the SLT and OPT representations for the formula x 2 = 4. In the SLT representation, the edge labels show the spatial relationship between the formula elements. For instance, the edge label 'a' shows that the number '2' is located above the variable 'x', while the edge label 'n' shows operator '=' is located next after 'x'. In the OPT representation, the edge labels for non-commutative operators indicate argument position. For more details, refer to Mansouri et al. <ref type="bibr" coords="2,148.60,325.26,9.96,8.80" target="#b3">[4]</ref>. While our runs results for Task 1 were not as good as for some other participating teams, our Task 2 results were noteworthy for including a system that was statistically indistinguishable from the baseline (which was a strong baseline relative to submitted systems). In this paper, we first introduce our runs for Task 2 because some techniques from Task 2 were used in our Task 1 systems.</p><p>2 Formula Retrieval (Task 2)</p><p>For Task 2, we built three systems and submitted one run from each. All three systems use formula embeddings produced from strings generated after parsing formulas with Tangent-S <ref type="bibr" coords="2,242.59,468.05,9.96,8.80" target="#b1">[2]</ref>, which produces both SLT and OPT representations. Tangent-S and Tangent-CFT are publicly available. <ref type="foot" coords="2,359.59,478.45,3.97,6.16" target="#foot_0">3</ref>In Tangent-S, candidate formula SLTs and OPTs are first retrieved independently, using tuples representing the relative paths between pairs of symbols. The top-2000 results from the separate OPT and SLT result lists are reranked after aligning candidates to the query formula, using an ordered list based on three features (the second and third features are used to break ties when ranking): 1) a unified structural matching score (Maximum Subtree Similarity (MSS)), 2) precision for candidate symbols matched only by unification (treated as a penalty), and 3) symbol recall based on exact matching of candidate and query symbols. The reranked candidates are then rescored again using a linear combination of the three-element score vectors associated with each retrieved formula's SLT and OPT. This combines the SLT and OPT score vectors for each candidate formula into a single rank score. The baseline uses a weight vector learned on the NTCIR-12 Wikipedia Formula Browsing task test collection <ref type="bibr" coords="2,416.77,635.72,9.96,8.80" target="#b1">[2]</ref>.</p><p>In the following, we describe our formula retrieval models, all of which use the Tangent-S SLT and OPT tree encodings. Note that our systems do not use any of the retrieval models of Tangent-S: only the SLT and OPT string encodings, which represent depth-first traversals on OPTs and SLTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tangent-CFT</head><p>Tangent-CFT <ref type="bibr" coords="3,197.23,207.17,10.51,8.80" target="#b3">[4]</ref> is an embedding model for mathematical formulas. This model uses both SLT and OPT representations of formulas to consider both the appearance and the syntax of formulas. Summarizing the indexing steps of Tangent-CFT (for the complete description, please see the paper):</p><p>-Tuple Extraction: Using Tangent-S, L A T E X formulas are converted to Presentation and Content MathML, from which the internal SLT and OPT formula representations are produced. Depth-first traversals of these OPT and SLT trees are used to produce strings consisting of a sequence of tuples. Each tuple has three elements (N i , N j , E). The N i shows the node value which is in the form of Type!Value, where type can take values such as Variable (V) or Number(N) and value shows the variable name or the numeric value. E shows a sequence of edge labels connecting the two nodes. -Tuple Encoding: Each tuple is then tokenized and enumerated. The tokenization is based on type, value. For example the tuple (V!x, N!2, a), which represents x 2 , will be tokenized to {'V', 'x', 'N', '2', 'a' }.</p><p>-Training Embedding Models with fastText: After tokenizing each tuple, each token is considered as a character and the whole tuple as a word. The resulting sequence of tuples is considered as a sentence in which the words (tuples) appear. As shown in Mansouri et al. <ref type="bibr" coords="3,384.95,431.87,9.96,8.80" target="#b4">[5]</ref>, mathematical formulas can be rather infrequent, so the fastText <ref type="bibr" coords="3,357.09,443.83,10.51,8.80" target="#b0">[1]</ref> n-gram embedding model is used to get vector representations for each tuple. The formula vector representation is then obtained by averaging the vector representations for the individual tuples. -Combining the Vector Representations: Tangent-CFT uses three representations of the formulas: SLT, OPT, and SLT-Type. In SLT-Type, each node represents only the type, and not the value. This can help with the partial matching of formulas especially for the cases where the variable names are not critical. For each representation, a separate embedding model is trained and a vector representation is obtained. The final formula vector is obtained by adding the 3 vectors together.</p><p>We trained Tangent-CFT on Math Stack Exchange formulas, using the default parameters which were obtained by training the model on the NTCIR-12 Wikipedia Formula Browsing task. SLT and OPT representations are sometimes not available for specific formulas, and Tangent-CFT will only retrieve formulas for which at least one of those representations is available. For this task, if one of the formula representations is not available, a vector zeros is used for that embedding.</p><p>Formula Query SLT Tuples Tangent-S OPT Tuples SLT-Type Tuples</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tangent-CFT Encoder</head><p>Tangent-CFT Model Retrieval Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formula Vector Representations</head><p>Query Vector Representation Similar Formulas based on cosine similarity Fig. <ref type="figure" coords="4,154.39,207.23,4.13,7.93">2</ref>. Overview of Tangent-CFT. The set of SLT, OPT and SLT-type tuples are extracted from the formula query using Tangent-S. These tuples are then encoded using the Tangent-CFT encoder, and compared against vector representations for formulas in the collection using cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tangent-CFT-Plus</head><p>As described in <ref type="bibr" coords="4,206.47,292.53,9.96,8.80" target="#b2">[3]</ref>, to decide the relevance, the annotators might consider the question (context) in which the formula appeared in. Tangent-CFT only considers the structural similarity of formulas when embedding them. Tangent-CFT-Plus is an extension to Tangent-CFT embedding model in which each formula has two vector representations:</p><p>-Formula Vector : Vector representation obtained by Tangent-CFT. The vector size is 300. -Text Vector : Vector representation obtained by considering formula as a word</p><p>and training the fastText model on the surrounding words of the formula. We used a vector size of 100 which is the fastText default value.</p><p>Each formula is represented as the concatenation of Formula and Text vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tangent-CFT with Tree Edit Distance</head><p>Experiment results on the NTCIR-12 Wikipedia Formula Browsing task test collection <ref type="bibr" coords="4,179.37,499.03,10.51,8.80" target="#b6">[7]</ref> showed Tangent-CFT to do better on partial matching compared to full matching. To address that issue, we extended Tangent-CFT by re-ranking the retrieval results based on tree-edit distance.</p><p>Tree-edit distance (TED) is the minimum cost of converting one tree to another. Three edit operations were considered: insertion, deletion and substitution. Each of these operations can have different weights when calculating the tree-edit distance cost. The ranking score for each retrieved formula is calculated as:</p><formula xml:id="formula_0" coords="4,236.87,590.20,243.73,23.22">sim(T 1 , T 2 ) = 1 T ED(T 1 , T 2 ) + 1 .<label>(1)</label></formula><p>We tuned the weights for each edit operations, over the range [0, 1] with step size 0.05, on the NTCIR-12 test collection using leave-one-out cross-validation. In the NTCIR-12 Formula Browsing Task test collection <ref type="bibr" coords="4,348.97,644.10,10.51,8.80" target="#b6">[7]</ref> there are 20 formula queries without wildcards. We learn operation weights for each query independently, and report results as averages over the 20 queries using weights learned for each query.</p><p>Our goal was to maximize the harmonic mean bpref score, which balances full and partial bpref scores. As there are two tree representations for formulas (OPT and SLT), the re-ranking is done on each representation separately, and then the results were linearly combined. The linear combination weight is calculated in a similar way as the edit operation weights using leave-one-out cross-validation.</p><p>To use the tree edit distance for formula retrieval, the following steps are taken:</p><p>1. Retrieve the top-1000 results with Tangent-CFT model. 2. Re-rank the results with tree-edit distance using SLT and OPT representations. For SLTs, the edit operations cost were set 0.85, 0.15, and 0.54 for deletion, substitution and insertion operations respectively and for OPTs they were set to 0.28, 0.185, 0.225. 3. The re-ranked results with OPT and SLT representations are linearly combined as:</p><formula xml:id="formula_1" coords="5,187.27,475.65,293.32,9.71">Score q (f ) = α • SLT -Score q (f ) + (1 -α) • OP T -Score q (f )<label>(2)</label></formula><p>with α = 0.95. The tree-edit distance values are calculated using a publiclyavailable Python package for the APTED algorithm <ref type="bibr" coords="5,381.95,509.69,9.96,8.80" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Results</head><p>Table <ref type="table" coords="5,162.65,560.33,4.98,8.80" target="#tab_0">1</ref> shows the results of our runs and for the baseline (Tangent-S) (Taken from the lab overview paper <ref type="bibr" coords="5,258.06,572.29,10.29,8.80" target="#b7">[8]</ref>). For all three evaluation measures, the apparent differences between Tangent-CFTED and Tangent-S (which, for P@10, are 5% relatively higher for Tangent-CFTED) are not statistically significant by a twotailed paired t-test (at p &lt; 0.05).</p><p>We analyzed our runs against the baseline and against each other. Our first analysis was comparing the Tangent-S system against Tangent-CFTED to investigate how many relevant formulas are retrieved by both systems, and how many are retrieved by one system and not the other. We considered different relevance degrees, first treating only high as relevant, then high and medium as relevant, and finally all the three levels as relevant. Figure <ref type="figure" coords="6,353.27,130.89,4.98,8.80" target="#fig_1">3</ref> shows the Venn diagram of retrieval results between Tangent-S (the right circles with dashed blue outline) and Tangent-CFTED (the left circles with grey background).</p><p>The largest difference in P@10 values between Tangent-CFTED and Tangent-S was 0.5 for the formula</p><formula xml:id="formula_2" coords="6,266.70,188.65,213.89,10.81">(x + y) k ≥ x k + y k<label>(3)</label></formula><p>(Topic B.48). Half of the top-10 formulas retrieved by Tangent-CFTED are annotated as highly relevant and the other half are annotated as medium. Tangent-S retrieved 4 formulas annotated as high/medium, and 5 as low. Formulas such as (x + y) p ≡ x p + y p (mod p) with a low relevance rating get a higher rank in Tangent-S, while formulas such as (x + y) a ≥ x a + y a and (x + y) s ≥ x s + y s that are highly relevant get lower ranks, and are not in the top-10 unique formulas. However, these last two formulas appear at ranks 6 and 3 in the Tangent-CFTED search results, respectively. Another topic where Tangent-CFTED did better than the baseline is the formula</p><formula xml:id="formula_3" coords="6,286.53,322.07,194.06,30.55">n k=0 n k k<label>(4)</label></formula><p>(Topic B.4) where P@10 for the baseline was 0, but 0.4 for Tangent-CFTED. The top-5 unique formulas retrieved by Tangent-S and Tangent-CFTED for this query are shown in Table <ref type="table" coords="6,251.64,382.54,3.87,8.80" target="#tab_1">2</ref>. For 8 Task 2 topics, the P@10 for Tangent-CFTED was 0 (i.e., no formulas with medium or high relevance ratings were found), and for 4 of these topics, no relevant formulas (at any relevance level) were retrieved in top-10 results. The highest P@10 achieved by Tangent-S for these 8 topics was 0.3. Some of these topics were more complex than other queries, and had fewer relevant hits. Providing an example, for formula</p><formula xml:id="formula_4" coords="6,232.01,474.15,248.59,8.80">∃p p is prime → ∀x (x is prime)<label>(5)</label></formula><p>(Topic B.56) there are only 5 formulas with a relevance rating of medium or high in the collection. For this formula, the Tangent-CFT model fails to retrieve  </p><formula xml:id="formula_5" coords="7,147.45,162.94,329.00,111.23">n k=0 n k k2 n-k Non-Relevant n k=0 n k k High 2. n k=0 n k k2 n-k Non-Relevant n k=0 n k M k Non-Relevant 3. n k=0 n k k! D n-k Non-Relevant n k=0 n k x k Medium 4. n k=0 n k k p Low n k=0 n k x k Medium 5. n k=0 n k x k (1 -x) n-k k Non-Relevant n k=0 n k k p Low</formula><p>any relevant formulas in its top-10 results, and Tangent-CFTED has only one relevant formula, ∃p(p is prime and p 2 | n), which has a low relevance rating at rank 5. It is possible that the larger depth used for re-ranking by Tangent-S (i.e., top-2000, vs. top-1000 for Tangent-CFTED) may be contributing to the difference in performance for these topics.</p><p>Comparing Tangent-CFT-Plus with our other two runs, there were four queries for which Tangent-CFT-Plus had higher P@10. One of these cases was the query</p><formula xml:id="formula_6" coords="7,277.32,408.61,203.27,22.31">df dx = f (x + 1)<label>(6)</label></formula><p>where the P@10 for our other two runs was 0, but for Tangent-CFT-Plus was 0.4. Tangent-CFT only retrieves 6 relevant formulas for this query, and Tangent-CFTED fails to re-rank them better than Tangent-CFT. The Tangent-CFT-Plus model takes advantage of surrounding text. Terms such "differential equations" appear frequently around the related formulas. Therefore, Tangent-CFT-Plus gives higher ranks to formulas such as df (x) dx = f (x), which was the first formula retrieved by this system for the query df dx = f (x + 1). Performance. Table <ref type="table" coords="7,246.60,524.13,4.98,8.80" target="#tab_2">3</ref> shows the minimum, maximum, mean and standard deviation of retrieval times for our three runs in task 2. Our runs were done on a machine with 4 NVIDIA GeForce GPU cards with 11GB memory, a 528 GB Ram, with Intel(R) Xeon(R) CPU E5-2667 v4 @ 3.20GHz (32 cores). For calculating the cosine similarity of vectors we used Pytorch framework. As can be seen, the retrieval time for Tangent-CFT is lower than Tangent-CFT-PLUS due to smaller size of vectors. As Tangent-CFTED has an additional re-ranking step, its retrieval time is longer than Tangent-CFT.</p><p>Post-Task Experiments. After the submission of our runs, we found an error in our run, Tangent-CFT-Plus which used partial data for retrieval: results were only returned from answers written between 2010 and 2015. Therefore, we ran our model again, after assessment was complete (i.e., after pooling). With correct use of the dataset, Tangent-CFT-Plus could achieved the nDCG value of 0.305, mAP of 0.155 and P@10 of 0.342.</p><p>In another experiment, we looked at the Tangent-CFTED model, to check the effect of α value used to combine SLT and OPT results. This value α was set to 0.95, which was learnt on NTCIR-12 dataset and leaned highly toward SLT representation. We conducted another experiment by setting the α value to 0.5, weighting SLTs and OPTs equally. With this setting, the effectiveness measures were nDCG of 0.424, mAP of value of 0.253 and P@10 of 0.488. This produces small changes in nDCG and mAP scores (≤ 0.5%), and a decrease in P @10 of about two percent compared to using Tangent-CFTED using α = 0.95, and so giving greater weight to OPTs did not improve the performance of the model.</p><p>Note that the formula relevance definition in NTCIR-12 was different than ARQMath, and therefore in future work, we will do parameter tuning using the ARQMath dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Answer Retrieval (Task 1)</head><p>In this section we present our system design, runs, and results for Task 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Design</head><p>For Task 1, our approach was to use text and formula embeddings to find relevant answers for each topic. Our runs differ in how the embedding models are used for retrieval. Each post (question or answer) in our models is represented as a vector with dimension 400: the first 100 elements being the text embedding of the post and the next 300 being the formula embedding.</p><p>We trained fastText <ref type="bibr" coords="8,242.38,524.55,10.51,8.80" target="#b0">[1]</ref> to get text embeddings for each post. To tune the fastText parameters, we used 1,003 questions published in the year 2018 that had duplicate or related questions as our training questions. The duplicate and related questions are determined by the Math Stack Exchange moderators are data that was provided by the ARQMath organizers. The answers given to the duplicate or related questions, were considered as the relevant answers for the training questions. For each training question, all the answers from duplicate and related questions were collected and sorted based on scores. Then, the first quarter of the answers was considered as high, the second quarter medium, and the third quarter as an answer with low relevance level.</p><p>Using the training questions, we tuned the fastText parameters by considering our first proposed retrieval system. Our goal is to train a model that maximizes the precision at 10 for the training questions. After hyper-parameter tuning, our final setting for the fastText model was to use a skip-gram model with negative sampling (15 negative samples), with a context window size of 5, n-grams of lengths between 2 and 5 characters, and a vector size of 100. The fastText model was trained on all the questions, answers, and comments in the collection with 10 epochs. As fastText provides embeddings for the words, we averaged all the vector representations of the words inside a post to get a post embeddings. For the questions, the vector representation is the average of vector representations of the title and the body of the questions.</p><p>The second part of a post vector represents the formula embedding. We trained the formula embedding model, Tangent-CFT <ref type="bibr" coords="9,369.96,238.49,10.51,8.80" target="#b3">[4]</ref> with the same parameters that this model used for the NTCIR-12 <ref type="bibr" coords="9,342.64,250.44,10.51,8.80" target="#b6">[7]</ref> formula browsing task. The model is described in previous section. As each post can have multiple formulas, the formula vector is the unweighted average of all the formulas vectors in three of our runs. In one run (DPRL3), we used the weighted average of formulas, using the length of formula as the weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Submitted Runs</head><p>Here we describe our four runs for Task 1, which are summarized in Table <ref type="table" coords="9,461.96,345.22,3.87,8.80" target="#tab_3">4</ref>.</p><p>DPRL1. This was our primary run, in which the answers in the collection are represented as the summation of two vector representations: one for the answer and one for the question to which the answer was given. Both the answer and question vector representations are obtained as described before, by concatenating the text and formula vectors. For each topic, the cosine similarities of the topic's vector and all the answers' vectors in the collection are computed and used to rank the answers.</p><p>DPRL2. This alternate run is similar to our primary run, with the difference being that the answer vector is just the vector representation of the answer. Compared to our primary run, the question to which the answer was given is ignored. For each topic, the cosine similarity of the topic vector and answer vector is the similarity score.</p><p>DPRL3. This alternate run is again similar to our primary run, with the difference being that the formula part of the vector is calculated differently. As mentioned earlier, there could be multiple formulas in a post. In this run, instead of using the unweighted average of vector representations of formulas, we used the weighted average of vector representations by considering the length of each formula as the weight. Our assumption was that the bigger formulas are more important. We used the length of the L A T E X string representation of formulas for this. As with the primary run, the similarity of topic and answer is the cosine similarity of their vector representations.</p><p>DPRL4. This alternate run first finds questions that are similar to the topic and then ranks the answers to those similar questions. To perform this ranking, after finding similar questions to the topic, we compute the cosine similarity score of the topic vector and the vector for the similar question with which each answer is associated, and we then multiply that by the score assigned to that answer by Math Stack Exchange users. We then sort the answers in decreasing order of that product. We then perform a reranking step in which we make use of the Tags assigned to the Topic question and to the question posts that had been found, which had been provided by the people asking the questions. Answers associated with found questions that have at least one Tag that exactly matches a Tag in the query question are moved ahead of all other answer posts (with their relative order maintained).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>For the answer retrieval task, three evaluation measures were considered: nDCG , MAP and P@10. Our results for this task were not promising. Table <ref type="table" coords="10,446.41,608.24,4.98,8.80" target="#tab_4">5</ref> shows the baselines and our runs results for this task (Taken from the lab overview paper <ref type="bibr" coords="10,162.53,632.15,10.29,8.80" target="#b7">[8]</ref>). Our runs mostly did better for topics in the category "Computation" than for topic in categories "Concept" or "Proof". For instance, all the topics for which our DPRL2 run (which had our highest P@10), was able to find relevant (note that I am assuming this isn't taken as a "trick" question, so that is meant, rather than )</p><p>So everything boils down to the probability of "one of the children is a girl" given that the family has a boy and a girl.</p><p>Now, what does the phrase "one of the children is a girl" mean? There are two typical options This phrase simply means that this is not an all-boy family. In this case, , and .</p><p>The phrase means that a particular child was chosen and it could just has easily have been the boy, so that , and .</p><p>In my opinion, an English speaker would almost always interpret the phrase the first way rather than the second way.</p><p>However, there is a nasty linguistic trap: even meaning it the first way, we may often pick a girl to be the referent of "one" (e.g. so that we can speak of the "other" child). And since we've picked a girl, we get stuck in the second interpretation. This is exacerbated by the fact the second interpretation has a very appealing shortcut for reasoning out the solution.  answers were in the category "Computation". For two topics, the DPRL2 run had P@10 higher than 0.5 (note that P@10 was calculated by considering the relevance degrees of high and medium as relevant). Figure <ref type="figure" coords="11,389.33,484.81,4.98,8.80">4</ref> shows topic A.5 for which DPRL2 achieved P@10 of 0.9, along with the first answer retrieved for this topic, which is highly relevant. Another observation we had was that the DPRL4 run, which has the highest nDCG value among our runs, is able to find relevant answers for categories of Proof and Concept. For instance, for topic A.38 (in the Concept category, with difficulty Hard), this run achieved the P@10 of 0.4.</p><p>As mentioned earlier, we did the hyper-parameter tuning for our proposed models based on MSE scores. Comparing these scores, with the annotation scores, there is no correlation; that is to say, not all the relevant answers have high score and not all the not relevant ones have low score. Therefore, in our future work, we will do the hyper-parameter tuning using the annotated data.</p><p>Performance. Table <ref type="table" coords="11,245.74,632.15,4.98,8.80">6</ref> shows the minimum, maximum and average retrieval time for each of proposed models in task 1. We used the same machine as for task 2. As can be seen, all our three first runs have similar retrieval times as they just use cosine similarity of vector representations of documents. However, for our last run, we have an extra re-ranking step which results in longer retrieval times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we described the DPRL runs for ARQMath lab at CLEF 2020. For Formula Retrieval (Task 2), the DPRL submitted 3 runs. Our run that uses formula embedding and then re-ranks by tree-edit distance achieved the highest P@10 value and the second-best nDCG after the baseline system. For the answer retrieval task (Task 1), we used fastText to embed text, and Tangent-CFT to emebed formulas, concatenating text and formula vectors to represent each post (answer or question). Our runs differed from each other in terms of how the vector embeddings were used for answer retrieval. Our best run found similar questions to the topics and used the answers given to those similar questions. In future work, we plan to conduct further analysis of our results and to further enhance our systems for both tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,218.18,71.75,8.97;2,206.52,217.02,3.65,5.24;2,213.23,218.18,267.36,8.97"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Formula x 2 = 4 represented as (a) Symbol Layout Tree and (b) Operator Tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,134.77,610.12,345.83,8.97;6,134.77,621.08,345.82,8.97;6,134.77,632.04,345.83,8.97;6,134.77,643.00,345.82,8.97;6,134.77,653.96,332.06,8.97"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Venn Diagrams for all Visually Distinct Relevant formulas in Task 2 (45 Topics). Each diagram indicates how many relevant formulas were retrieved by Tangent-S (blue circles) and Tangent-CFTED (grey circles) within their top-1000 results. Relevant formulas missed by both systems are at top-left in each diagram. In total, there are 1,397 relevant formulas in the collection (509 high, 387 medium, and 501 low).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,134.77,322.02,345.83,8.97;11,134.77,332.97,111.15,8.97;11,209.60,355.94,196.15,8.97;11,281.28,379.43,86.97,5.10;11,215.74,385.95,183.88,6.55;11,215.74,398.92,183.88,6.16;11,215.74,406.89,183.88,6.16;11,215.74,414.86,183.89,6.16;11,215.74,422.83,183.88,6.16"><head>Fig. 4 .Table 6 .</head><label>46</label><figDesc>Fig. 4. Topic A.5 and the top answer retrieved by DPRL2 run. The highest P@10 was achieved for this topic (0.9) Table 6. Retrieval Times in Seconds for Task 1. Run Times (in seconds) System Min (Topic) Max (Topic) (Avg, StDev) DPRL 1 0.055 (A.84) 0.061 (A.41) (0.056, 0.0001) DPRL 2 0.056 (A.54) 0.057 (A.49) (0.056, 0.0006) DPRL 3 0.056 (A.1) 0.057 (A.18) (0.056, 0.0001) DPRL 4 0.901 (A.26) 1.143 (A.30) (0.985, 0.0564)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,115.30,345.83,132.85"><head>Table 1 .</head><label>1</label><figDesc>DPRL runs for Task 2, results averaged over 45 topics and computed over deduplicated ranked lists of visually distinct formulas. For MAP and P@10, relevance was thresholded H+M (High or Medium) binarization.</figDesc><table coords="5,340.81,164.01,98.81,6.63"><row><cell>Evaluation Measures</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,138.90,115.11,331.02,62.67"><head>Table 2 .</head><label>2</label><figDesc>Top-5 unique formulas for query n</figDesc><table coords="7,382.65,115.34,33.20,12.32"><row><cell>k=0</cell><cell>n k k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,196.75,115.30,221.86,65.08"><head>Table 3 .</head><label>3</label><figDesc>Retrieval Times in Seconds for Task 2.</figDesc><table coords="8,196.75,138.78,221.86,41.60"><row><cell></cell><cell>Run Time (Seconds)</cell></row><row><cell>System</cell><cell>Min (Topic) Max (Topic) (Avg, StDev)</cell></row><row><cell>Tangent-CFT</cell><cell>0.653 (B.4) 0.669 (B.41) (0.658, 0.004)</cell></row><row><cell cols="2">Tangent-CFT-PLUS 0.848 (B.69) 0.857 (B.54) (0.851, 0.002)</cell></row><row><cell>Tangent-CFTED</cell><cell>0.672 (B.3) 7.052 (B.26) (1.752, 1.622)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,134.77,115.30,345.83,110.04"><head>Table 4 .</head><label>4</label><figDesc>Summary of DPRL Submissions for ARQMath Answer Retrieval Task. Each Answer has a vector representation of size 400, where the first 100 elements belong to the text embedding, and the final 300 elements are the formula embedding.</figDesc><table coords="10,166.03,163.51,283.29,61.82"><row><cell></cell><cell cols="2">Embedding Vector (400 elements)</cell></row><row><cell>Run</cell><cell>Text (100)</cell><cell>Formula (300)</cell></row><row><cell cols="2">DPRL1 Answer + Question</cell><cell>Tangent-CFT (unweighted average)</cell></row><row><cell cols="2">DPRL2 Answer</cell><cell>Tangent-CFT (unweighted average)</cell></row><row><cell cols="2">DPRL3 Answer + Question</cell><cell>Tangent-CFT (weighted average)</cell></row><row><cell cols="2">DPRL4 Question</cell><cell>Tangent-CFT (unweighted average)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,134.77,241.77,345.83,198.10"><head>Table 5 .</head><label>5</label><figDesc>Task 1 (CQA) results, averaged over 77 topics. P indicates a primary run, M indicates a manual run, and ( ) indicates a baseline pooled at the primary run depth. For Precision@10 and MAP, H+M (High or Medium) binarization was used. The best baseline results are in parentheses. * indicates that one baseline did not contribute to judgment pools.</figDesc><table coords="10,160.41,311.90,294.20,127.97"><row><cell></cell><cell cols="5">Run Type Evaluation Measures</cell></row><row><cell>Run</cell><cell>Data P</cell><cell>M</cell><cell cols="3">nDCG MAP P@10</cell></row><row><cell>Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Linked MSE posts</cell><cell>n/a ( )</cell><cell></cell><cell cols="3">(0.303) (0.210) (0.417)</cell></row><row><cell>Approach0 *</cell><cell>Both</cell><cell></cell><cell>0.250</cell><cell>0.100</cell><cell>0.062</cell></row><row><cell>TF-IDF + Tangent-S</cell><cell>Both ( )</cell><cell></cell><cell>0.248</cell><cell>0.047</cell><cell>0.073</cell></row><row><cell>TF-IDF</cell><cell>Text ( )</cell><cell></cell><cell>0.204</cell><cell>0.049</cell><cell>0.073</cell></row><row><cell>Tangent-S</cell><cell>Math ( )</cell><cell></cell><cell>0.158</cell><cell>0.033</cell><cell>0.051</cell></row><row><cell>DPRL4</cell><cell>Both</cell><cell></cell><cell>0.060</cell><cell>0.015</cell><cell>0.020</cell></row><row><cell>DPRL2</cell><cell>Both</cell><cell></cell><cell>0.054</cell><cell>0.015</cell><cell>0.029</cell></row><row><cell>DPRL1</cell><cell>Both</cell><cell></cell><cell>0.051</cell><cell>0.015</cell><cell>0.026</cell></row><row><cell>DPRL3</cell><cell>Both</cell><cell></cell><cell>0.036</cell><cell>0.007</cell><cell>0.016</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,173.86,220.87,339.63,222.52"><head></head><label></label><figDesc>You have to read carefully. Here, the probability of a girl means the probability of at least one girl. You have Now you are calculating the conditional probabilty</figDesc><table coords="11,173.86,220.87,339.63,222.52"><row><cell></cell><cell cols="2">edited Oct 17 '15 at 3:51</cell><cell cols="2">answered Oct 17 '15 at 3:37</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">user14972</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">answered May 27 '16 at 1:27</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ncmathsadist</cell></row><row><cell></cell><cell></cell><cell></cell><cell>45k</cell><cell>2</cell><cell>67</cell><cell>113</cell></row><row><cell>By using our site, you acknowledge that you have read and understand our</cell><cell>, Cookie Policy Privacy Policy , and our</cell><cell cols="2">. Terms of Service</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,657.44,197.66,7.47"><p>https://www.cs.rit.edu/~dprl/software.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This material is based upon work supported by the <rs type="funder">National Science Foundation (USA)</rs> under Grant No. <rs type="grantNumber">IIS-1717997</rs> and the <rs type="funder">Alfred P. Sloan Foundation</rs> under Grant No. <rs type="grantNumber">G-2017-9827</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FZg8Jgk">
					<idno type="grant-number">IIS-1717997</idno>
				</org>
				<org type="funding" xml:id="_C2qMSEX">
					<idno type="grant-number">G-2017-9827</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>I was doing this question using conditional probability formula. Suppose, (1) is the event, that the first child is a boy, and (2) is the event that the second child is a boy.</p><p>Then the probability of the second child to be boy given that first child is a boys by formula, ...since second child to be boy doesn't depend on first child and vice versa. Please provide the detailed solution and correct me if I am wrong.</p><p>probability proof-verification conditional-probability 2 Answers 0 Let's write the sample space: BB, BG, GB, GG</p><p>But we know that one child is a boy, so that means that GG isn't a possibility. Thus, the sample space is reduced to: BB, BG, GB.</p><p>Therefore, the probability of both children being boys given that one is a boy is 1/3.</p><p>1 The flaw in your solution is to write What would be the logic behind this? Think again, the event means "there are boys and there is at least boy". Therefore, is equal to the event "there are two boys".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can you fix your solution yourself now?</head><p>DPRL2 Run: First Answer Retrieved -Fair point, but I would argue that the term "one of them" does not imply the exclusion of "both of them" as in the famous and idiotic puzzle "I have two coins that add to 30 cents and one is not a nickel". The question was clearly not intended to be taken that way. The Count Feb 22 '17 at 19:08 1 -@TheCount, the point I'm trying to make is that the confusion between 1/2 and 1/3 comes about because we don't exactly know what we've been told. There's a third interpretation; not necessarily more correct, but also not invalid. It's a more "it couldn't mean that!" case which is meant to highlight that the question is not well possibly framed. sh1 Feb 22 '17 at 19:13 -I see. Well, you make a fair point, but I suppose my confusion comes about from the fact that your answer seemingly declares your interpretation to be correct one, the instead of explaining it to me as you just did. I think including that info in the answer itself would be helpful. The Count Feb 22 '17 at 19:14</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,138.35,409.98,342.25,8.97;12,146.91,420.94,333.68,8.97;12,146.91,431.90,71.85,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,363.34,409.98,117.25,8.97;12,146.91,420.94,80.99,8.97">Enriching word vectors with subword information</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,234.37,420.94,246.22,8.97">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,442.86,342.24,8.97;12,146.91,453.82,333.68,8.97;12,146.91,464.78,333.68,8.97;12,146.91,475.74,25.59,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,242.45,442.86,238.14,8.97;12,146.91,453.82,97.00,8.97">Layout and semantics: Combining representations for mathematical formula search</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,267.79,453.82,212.80,8.97;12,146.91,464.78,269.25,8.97">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1165" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,486.70,342.24,8.97;12,146.91,497.65,333.68,8.97;12,146.91,508.61,64.90,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,347.99,486.70,132.59,8.97;12,146.91,497.65,148.55,8.97">Finding old answers to new math questions:the ARQMath lab at CLEF</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,333.80,497.65,146.79,8.97;12,146.91,508.61,36.24,8.97">European Conference on Information Retrieval</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,519.57,342.24,8.97;12,146.91,530.53,333.68,8.97;12,146.91,541.49,333.68,8.97;12,146.91,552.45,67.56,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,444.76,519.57,35.83,8.97;12,146.91,530.53,217.54,8.97">Tangent-CFT: An embedding model for mathematical formulas</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,385.28,530.53,95.31,8.97;12,146.91,541.49,329.13,8.97">Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval (ICTIR)</title>
		<meeting>the 2019 ACM SIGIR International Conference on Theory of Information Retrieval (ICTIR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,563.41,342.24,8.97;12,146.91,574.37,232.43,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,312.58,563.41,168.01,8.97;12,146.91,574.37,32.81,8.97">Characterizing searches for mathematical concepts</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,200.96,574.37,149.71,8.97">Joint Conference on Digital Libraries</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,585.33,342.25,8.97;12,146.91,596.28,131.77,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,248.42,585.33,190.59,8.97">Tree edit distance: Robust and memory-efficient</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pawlik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Augsten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,445.77,585.33,34.83,8.97;12,146.91,596.28,51.54,8.97">Information Systems</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,607.24,342.24,8.97;12,146.91,618.20,168.21,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,437.98,607.24,42.61,8.97;12,146.91,618.20,87.93,8.97">NTCIR-12 MathIR task overview</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Topic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,256.14,618.20,30.32,8.97">NTCIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,629.16,342.24,8.97;12,146.91,640.12,230.84,8.97" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,369.19,629.16,111.40,8.97;12,146.91,640.12,202.17,8.97">Overview of arqmath 2020: Clef lab on answer retrieval for questions on math</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
