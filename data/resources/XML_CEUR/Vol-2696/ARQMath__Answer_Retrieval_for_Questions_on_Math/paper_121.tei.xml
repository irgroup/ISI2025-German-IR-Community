<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,174.62,115.96,266.13,12.62;1,153.85,133.89,307.65,12.62">PSU at CLEF-2020 ARQMath Track: Unsupervised Re-ranking using Pretraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,200.95,171.53,84.12,8.77"><forename type="first">Shaurya</forename><surname>Rohatgi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.63,171.53,42.12,8.77"><forename type="first">Jian</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Old</orgName>
								<address>
									<addrLine>Dominion University</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,348.30,171.53,61.63,8.77"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,174.62,115.96,266.13,12.62;1,153.85,133.89,307.65,12.62">PSU at CLEF-2020 ARQMath Track: Unsupervised Re-ranking using Pretraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B118DDCA5AC39943FA7796A9ADE24792</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Math Information Retrieval</term>
					<term>Math Embeddings</term>
					<term>Mathaware search</term>
					<term>Math formula search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper elaborates on our submission to the ARQMath track at CLEF 2020. Our primary run for the main Task-1: Question Answering uses a two-stage retrieval technique in which the first stage is a fusion of traditional BM25 scoring and tf-idf with cosine similarity-based retrieval while the second stage is a finer re-ranking technique using contextualized embeddings. For the re-ranking we use a pre-trained robertabase model (110 million parameters) to make the language model more math-aware. Our approach achieves a higher NDCG score than the baseline, while our MAP and P@10 scores are competitive, performing better than the best submission (MathDowsers) for text and text+formula dependent topics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Intelligent Information Systems Research Laboratory at Pennsylvania State University participated in the CLEF-2020 ARQMath track to contribute and develop new techniques for math-aware information retrieval. One of the main goals <ref type="bibr" coords="1,161.09,461.26,10.52,8.74" target="#b8">[9]</ref> of this track was to push mathematical question answering into an informal language scenario, specifically using data from Math Stack Exchange (MSE). We participated in Task-1: Answer Retrieval track, the goal of which was to return ranked lists of past answers given an actual recent post containing math formulas and text.</p><p>Question answering has elements of both relevance matching and semantic matching but remains a different task from document retrieval <ref type="bibr" coords="1,424.91,532.99,14.61,8.74" target="#b15">[16]</ref>. We use a two phase retrieval and re-ranking approach. In the first phase, retrieval is based on two runs: tf-idf representation using BM25 scoring and cosine similarity. Once we obtain the top 1000 candidates from the first phase, we re-rank them using contextualized BERT-based embeddings from the math-aware language model trained on the MSE dataset. This language model makes these embeddings semantically rich. Finally, we fuse the results from BM25, cosine, and BERTbased re-ranking runs. Our contributions are:</p><p>-Achieve a competitive score that beats the best baseline in terms of the NDCG score. -Achieve better results than the best submission for Text and Text+Math dependent posts. -Propose a Masked Language Model<ref type="foot" coords="2,307.13,164.28,3.97,6.12" target="#foot_0">1</ref> used for re-ranking candidate answers containing both text and math formulas.</p><p>Our paper has the following Sections. Section 2 discusses our two-stage retrieval approach which includes indexing and re-ranking phases. Section 3 describes our experimental setup, system configuration, and a detailed comparison of our results with the best submission. Conclusions are in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>Here we describe our two-stage cascade candidate retrieval and ranking approach. tf-idf with cosine similarity and an off-the-shelf BM25-based search platform <ref type="bibr" coords="2,158.27,307.53,15.50,8.74" target="#b14">[15]</ref> are used for the first stage. This is relatively computationally cheaper than the second more expensive re-ranking stage. For the second stage, we use a pre-trained language model to obtain semantically rich embeddings for the candidates obtained from the first stage. We then use these embeddings to rank the candidates using a cosine distance to the topic/query embedding. This ensures that the ranking captures semantics between the query and the documents. We only rely on the content of the post, which contain raw text and math formulas, not using external information such as votes/scores, user id, user score, post tags, and linked duplicate posts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">First-Phase: Retrieval</head><p>The aim of this phase is to get a sufficient number of relevant candidates to be re-ranked in the next stage. Past work has used BM25 scoring for this but we add cosine similarity ranking as well.</p><p>Indexing: We convert the MSE dataset from XML to JSON format so that it can be ingested by the search platforms we use. Because indexing all answers will potentially lose information in the questions, we generate a document by concatenating the question (Q) post text (title and body) with their corresponding answer (A) posts text (body). As such, the question body and title concatenated with the answer body become a document -one unit of retrieval. This allows us to remove questions without corresponding answers and represent the relevant posts as one large document because the relevant information the user is seeking could be in either the answer or the question post. This results in a total of 1,435,643 Q+A posts to be indexed.</p><p>We index the data using two off-the-shelf libraries -Elasticsearch (ES) and Anserini. We use two different libraries because each has its strengths and weaknesses. Anserini seems to perform better than ES in terms of relevance ranking and recall, which we observed when searching and evaluating the three training queries for the Question Answering Task provided by the organizers. Elasticsearch has a more scalable implementation of tf-idf with cosine similarity which is slow for a large dataset when using Anserini. For the formulas, we use the raw L A T E X strings. We re-rank answers based on contextualized text and formulas embeddings using RoBERTa.</p><p>Retrieval: Once all posts in Elasticsearch and Anserini have been indexed, we query the topics/queries for Task-1 to get the top-1000 candidates. For each task, each index is queried independently and later fused using Reciprocal Rank Fusion (RRF) <ref type="bibr" coords="3,197.52,239.13,10.52,8.74" target="#b1">[2]</ref> which then ranks the documents using a naive scoring formula. Given a set of posts P to be ranked and a set of rankings R from different scoring schemes, for each permutation on 1</p><formula xml:id="formula_0" coords="3,232.86,263.04,247.74,46.64">• • • |P |, we compute RRF score(p ∈ P ) = r∈R 1 k + r(p) ,<label>(1)</label></formula><p>where the original work <ref type="bibr" coords="3,242.63,317.85,10.52,8.74" target="#b1">[2]</ref> suggests k = 60, a hyper-parameter which we keep constant. The fusion of results from two different search platforms above significantly increases the performance numbers as demonstrated in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Second-Phase: Re-Ranking</head><p>Here we describe how we pretrain our language model and re-rank candidates obtained in the last phase. BERT <ref type="bibr" coords="3,180.03,416.78,10.52,8.74" target="#b4">[5]</ref> is a self-supervised approach for fine-tuning a deep transformer encoder <ref type="bibr" coords="3,160.69,428.73,14.61,8.74" target="#b13">[14]</ref>. Given a sequence, BERT learns a contextualized vector representation for each token. The input representations are fed into a stack of multi-layer bidirectional transformer blocks, which uses self-attention to compute semantically rich text representations by considering the whole input sequence.</p><p>BERT-based systems <ref type="bibr" coords="3,243.24,476.67,11.40,8.74" target="#b3">[4]</ref>[17] <ref type="bibr" coords="3,269.84,476.67,15.20,8.74" target="#b10">[11]</ref>[10] have shown significant performance in the recent tasks of TREC-2019 deep learning track <ref type="bibr" coords="3,343.62,488.63,10.52,8.74" target="#b2">[3]</ref> with the Microsoft MARCO dataset <ref type="bibr" coords="3,171.08,500.58,9.96,8.74" target="#b0">[1]</ref>. Participants for these tasks used query-document pairs from the training data to train transformer-based models to predict relevance. However, such models rely on a massive amount of data, which is not available in our task. Therefore, instead of training for relevance, we leveraged an unsupervised model by calculating the semantic similarities of queries and documents and later using them for re-ranking.</p><p>We choose the RoBERTa model <ref type="bibr" coords="3,294.10,572.43,10.52,8.74" target="#b7">[8]</ref> over BERT because in our experiments Vanilla RoBERTa achieves better NDCG scores than Vanilla BERT for the three preliminary training posts provided by the task organizers. Also, RoBERTa converges in fewer training steps than BERT as it gets rid of the computationally expensive next sentence prediction task. RoBERTa attains better performance on various NLP tasks than BERT <ref type="bibr" coords="3,289.24,632.21,9.96,8.74" target="#b6">[7]</ref>. We start with the initial weights of the roberta-base model and further pretrain the model using MSE data. The language model is trained for a mask prediction task. Once we are done training we use the Masked Language Model (MLM) to get contextualized token embeddings averaged over the sequence length to represent the candidates from Phase-1 into a 768 dimension vector. Similarly, we obtain topic/query embeddings and rank the candidates using their cosine distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Our experiments were conducted on a 24 core machine with Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz with 256GB of RAM with 4 RTX 2080 Ti GPUs. Default configurations were adopted in ES (cosine similarity; tf-idf) and Anserini (BM25). Anserini runs on a single thread while ES uses a multi-threaded function. In Figure <ref type="figure" coords="4,199.76,444.12,4.98,8.74" target="#fig_0">1</ref> we compare runtimes of BM25 ranking using Anserini and tf-idf based cosine similarity ranking using ES. The size of ES Q+A index is 3.6GB whereas for Anserini the size is 2.2GB.</p><p>Pretraining RoBERTa: We use the transformers<ref type="foot" coords="4,351.13,491.94,3.97,6.12" target="#foot_1">2</ref> library's implementation of RoBERTa to train the MLM. We start with the original weights released by the authors for roberta-base and then further pretrain the model on the MSE dataset. Fortunately, BASE-vocabulary used in the original was able to cover the whole MSE dataset so did not train from scratch. Further, we reduce the batch size to 4 per GPU and increase step size to facilitate gradient accumulation. We kept the maximum sequence length of 512 to accommodate longer posts. 3  Q+A posts are usually longer than 512 tokens so we had to break the posts into chunks before feeding them to our system. Once we are done with pretraining our MLM, we use it to extract the embeddings for each token in the candidate posts from the first-phase. For longer Q+A posts, we had to find a way of truncating them so that the sequence can fit in the 512 token window. We experimented with the head -tail approach <ref type="bibr" coords="5,462.33,118.99,14.61,8.74" target="#b12">[13]</ref>. The head -tail approach gave a lower NDCG score for the three preliminary train topics. Therefore, we use the head approach, in which we keep the first 510 tokens from the Q+A post and leave two token spaces for [CLS] and [SEP ]. This gives better results for the trained topics. This is likely because our Q+A posts include question text, so if you can find a similar question to a topic, then the answer to the similar question has a higher chance of being an answer to the topic. We show in Table <ref type="table" coords="5,232.94,476.79,4.98,8.74" target="#tab_0">1</ref> how our system compares with other submissions and the baselines. We only include the best submissions for baselines and other systems. Our system BM25+tf+tf.BERT clearly beats the best baseline in terms of NDCG . Before the challenge, we had only three train topics provided by the organizers on which we could test our approach. For these three topics tf-idf with cosine similarity was running better than BM25 scoring, so we did not include BM25 in our final submission. Later we added BM25 scoring which showed a greater increase in the number of relevant retrieved documents. It can be seen that tf.BERT that selects candidates using tf-idf similarity and re-ranking using our pretrained RoBERTa model does not achieve the best performance. But when the rankings of tf.BERT and tf-idf are fused, NDCG <ref type="bibr" coords="5,405.51,596.34,15.50,8.74" target="#b11">[12]</ref> score is substantially boosted. This is because the BERT ranking solely relies on semantic similarity whereas the tf-idf replies only on word frequency. Fusing these two runs seems reasonable since the posts which have a higher rank in both the runs are boosted even higher in the final ranking list. Note that BM25+tf+tf.BERT achieves the maximum number of relevant retrieved posts. It is important to note that the tf runs are not as consistent since ES does sharding and roundrobins between different shard searching. Thus, we did multiple runs to achieve the above scores.</p><p>Comparison with Other submissions: Our best unsubmitted run, BM25+tf+tf.BERT was compared with other submissions, among which Math-Dowsers is the system that achieved one of the best results in Task 1. Submissions by Approach0 and MathDowsers leveraged Tangent-S <ref type="bibr" coords="6,397.28,213.21,15.50,8.74" target="#b17">[18]</ref> and Tangent-L <ref type="bibr" coords="6,134.77,225.17,9.96,8.74" target="#b5">[6]</ref>, which are Symbol Layout and Operator Tree-based systems giving more attention to the math formula in the text. Our post-hoc system does not use a different representation for formula and text, therefore seems to suffer for formula dependent topics. We believe representing math content as trees and separately from the text content could have benefited our scores. Fig. <ref type="figure" coords="6,153.45,472.16,3.87,8.74">2</ref>: Comparison of runs for the 77 topics in Task-1 based on dependence class of the topics. We clearly see that for the topics that depend on Text and Text+Formula our system performs better.</p><p>In Figure <ref type="figure" coords="6,194.66,536.57,4.98,8.74">2</ref> we see that our system is better for the topics that depend on text and text+formula. For Text dependent topics MathDowsers had 4 topics for which there NDCG score was higher than our best run, while our system performed better in 8 topics. For the 31 text+formula dependent topics our system had better NDCG score for 17 topics. MathDowsers achieves a higher (≈ 96.4% higher) NDCG score for the topics which are only formula dependent. In contrast, our system is better (≈ 22.42%) at ranking posts containing both formula and text. This is attributed to the contextualized embeddings which our pretrained MLM can produce. It models equations with surrounding text and hence has better performance in text+formula topics. The difference (≈ 77.2%) is even more when we compare text dependant topics.</p><p>Overall, our participation in the ARQMath Track helped in our understanding of how to improve multi-modal search (text+formula) by exploiting state-ofthe-art text embedding and information retrieval models. In terms of effectiveness, our most effective post-hoc run BM25+tf+tf.BERT was able to outperform the baselines and all submissions except Mathdowsers in terms of NDCG . Our post-hoc system achieved the best results for queries that depend on text and text+formula.</p><p>Future work would include the use of the MSE dataset to get question-answer post pairs to train a better ranking model. It would also be helpful to investigate how important a formula is in a question post to retrieve relevant answers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,240.75,345.82,8.74;4,134.77,252.70,345.83,8.74;4,134.77,264.66,248.23,8.74"><head>Fig. 1 :</head><label>1</label><figDesc>Fig.1: Comparison of run-times for all the topics averaged over 10 runs. Fastest topics to retrieve top-1000 for ES and Anserini were A.39 and A.88 respectively while the slowest topics were A.87 and A.47 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,169.73,412.39,8.07,6.91;6,214.82,412.39,14.97,6.91;6,256.62,412.39,28.43,6.91;6,210.76,416.49,23.08,6.91;6,146.92,410.02,2.32,6.91;6,146.92,390.46,2.32,6.91;6,144.59,370.90,4.65,6.91;6,144.59,351.34,4.65,6.91;6,144.59,331.78,4.65,6.91;6,137.76,371.15,6.91,14.68;6,137.76,366.46,6.91,3.52;6,137.76,354.38,6.91,10.92;6,137.76,347.98,6.91,5.24;6,164.11,376.15,2.32,6.91;6,212.64,376.15,2.32,6.91;6,260.02,340.94,4.65,6.91;6,181.10,391.80,2.32,6.91;6,228.47,317.47,4.65,6.91;6,277.00,352.68,4.65,6.91;6,258.47,320.22,32.62,6.91;6,258.47,325.58,24.74,6.91;6,139.49,429.30,158.97,7.86;6,139.49,440.26,158.97,7.86;6,139.49,451.22,30.06,7.86;6,348.26,412.00,8.39,7.18;6,392.69,412.00,15.56,7.18;6,433.72,412.00,29.55,7.18;6,388.47,416.24,24.00,7.18;6,322.14,409.58,6.04,7.18;6,322.14,391.01,6.04,7.18;6,322.14,372.45,6.04,7.18;6,322.14,353.89,6.04,7.18;6,322.14,335.32,6.04,7.18;6,322.14,316.76,6.04,7.18;6,315.06,366.43,7.18,12.40;6,315.06,354.94,7.18,10.29;6,338.62,317.45,10.87,7.18;6,386.63,364.78,10.87,7.18;6,434.65,345.11,10.87,7.18;6,355.42,356.43,10.87,7.18;6,403.44,324.13,10.87,7.18;6,451.46,356.43,10.87,7.18;6,434.46,320.49,33.91,7.18;6,434.46,326.06,25.71,7.18;6,316.89,429.30,158.97,7.86;6,316.89,440.26,154.50,7.86"><head></head><label></label><figDesc>Number of topics for which each system had higher NCDG than the other system. Average NCDG over total topics depending on different types of topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,247.23,345.83,200.12"><head>Table 1 :</head><label>1</label><figDesc>Results for Task-1 compared with other submissions and best baselines. tf is tf-idf representation and cosine similarity-based ranking and tf.BERT signifies re-ranking using RoBERTa embeddings of the candidates selected by tf.Reciprocal rank fusion is used to fuse the runs separated by + sign. (* represents our best run)</figDesc><table coords="5,143.92,318.86,324.45,128.49"><row><cell></cell><cell>Run Tag</cell><cell>Evaluation Measures NDCG MAP P@10 Rel Ret</cell></row><row><cell>Baselines</cell><cell>Linked MSE Posts Approach-0</cell><cell>0.303 0.210 0.417 395 0.250 0.100 0.062 441</cell></row><row><cell></cell><cell>PSU1 (tf+tf.BERT)</cell><cell>0.263 0.082 0.116 761</cell></row><row><cell>Official Runs</cell><cell>PSU2 (tf)</cell><cell>0.228 0.054 0.055 761</cell></row><row><cell></cell><cell>PSU3 (tf.BERT)</cell><cell>0.221 0.046 0.026 761</cell></row><row><cell>Other Comparable</cell><cell>MIRMU</cell><cell>0.238 0.064 0.139 708</cell></row><row><cell>Systems</cell><cell cols="2">MathDowsers (Task-1 Best) 0.345 0.139 0.161 804</cell></row><row><cell>Unsubmitted Runs</cell><cell>BM25+tf+tf.BERT* BM25+tf BM25</cell><cell>0.314 0.097 0.149 902 0.304 0.098 0.151 875 0.246 0.078 0.139 660</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,657.44,265.10,7.47"><p>https://huggingface.co/shauryr/arqmath-roberta-base-1.5M</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,646.48,202.42,7.47"><p>https://github.com/huggingface/transformers</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank members of the <rs type="institution">ARQMath lab at the Department of Computer Science in Rochester Institute of Technology</rs> for organizing this track. Special thanks to <rs type="person">Behrooz Mansouri</rs> for providing the dataset, initial analysis of topics, and starter code to all the participants of the task; it made it easier for us to pre-process the data and jump directly to the experiments which have been presented in this work.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,142.96,434.61,337.64,7.86;7,151.52,445.56,329.07,7.86;7,151.52,456.52,289.24,7.86" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mc-Namara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<title level="m" coord="7,321.56,445.56,159.03,7.86;7,151.52,456.52,123.15,7.86">Ms marco: A human generated machine reading comprehension dataset</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,142.96,467.98,337.63,7.86;7,151.52,478.94,329.07,7.86;7,151.52,489.90,329.07,7.86;7,151.52,500.86,118.67,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,335.92,467.98,144.67,7.86;7,151.52,478.94,193.65,7.86">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buettcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,367.62,478.94,112.97,7.86;7,151.52,489.90,329.07,7.86;7,151.52,500.86,34.93,7.86">Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,512.32,337.64,7.86;7,151.52,523.28,282.00,7.86" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07820</idno>
		<title level="m" coord="7,416.93,512.32,63.66,7.86;7,151.52,523.28,116.04,7.86">Overview of the trec 2019 deep learning track</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,142.96,534.74,337.63,7.86;7,151.52,545.70,329.07,7.86;7,151.52,556.66,305.14,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,232.09,534.74,248.49,7.86;7,151.52,545.70,60.26,7.86">Deeper text understanding for ir with contextual neural language modeling</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,231.94,545.70,248.65,7.86;7,151.52,556.66,221.39,7.86">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,568.12,337.63,7.86;7,151.52,579.08,329.07,7.86;7,151.52,590.04,25.60,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,346.99,568.12,133.60,7.86;7,151.52,579.08,189.89,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,142.96,601.50,337.63,7.86;7,151.52,612.46,329.07,7.86;7,151.52,623.42,87.04,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,304.75,601.50,175.84,7.86;7,151.52,612.46,55.31,7.86">Choosing math features for BM25 ranking with tangent-l</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">W</forename><surname>Tompa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,226.89,612.46,253.70,7.86">Proceedings of the ACM Symposium on Document Engineering</title>
		<meeting>the ACM Symposium on Document Engineering</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,634.88,337.64,7.86;7,151.52,645.84,329.07,7.86;7,151.52,656.80,159.05,7.86" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10964</idno>
		<title level="m" coord="7,204.38,645.84,272.40,7.86">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.96,119.67,337.64,7.86;8,151.52,130.63,329.07,7.86;8,151.52,141.59,201.31,7.86" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="8,282.11,130.63,198.48,7.86;8,151.52,141.59,34.83,7.86">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.96,152.55,337.63,7.86;8,151.52,163.51,329.07,7.86;8,151.52,174.47,329.07,7.86;8,151.52,185.43,288.90,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,349.77,152.55,130.82,7.86;8,151.52,163.51,163.36,7.86">Finding old answers to new math questions: The arqmath lab at clef 2020</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,381.04,174.47,99.55,7.86;8,151.52,185.43,34.93,7.86">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Castells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Martins</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,196.39,337.97,7.86;8,151.52,207.34,97.80,7.86" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="8,270.02,196.39,133.28,7.86">Passage re-ranking with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.62,218.30,337.97,7.86;8,151.52,229.26,159.05,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="8,317.38,218.30,159.36,7.86">Multi-stage document ranking with bert</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14424</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.62,240.22,337.98,7.86;8,151.52,251.15,318.01,7.89" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,237.87,240.22,242.72,7.86;8,151.52,251.18,131.83,7.86">On information retrieval metrics designed for evaluation with incomplete relevance assessments</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,290.57,251.18,86.95,7.86">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="447" to="470" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,262.14,337.98,7.86;8,151.52,273.10,329.07,7.86;8,151.52,284.06,62.50,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,302.20,262.14,178.39,7.86">How to fine-tune bert for text classification?</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,164.78,273.10,259.88,7.86">China National Conference on Chinese Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="194" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,295.02,337.97,7.86;8,151.52,305.98,329.07,7.86;8,151.52,316.93,167.19,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,228.54,305.98,100.27,7.86">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,351.31,305.98,129.29,7.86;8,151.52,316.93,73.89,7.86">Advances in neural Information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,327.89,337.98,7.86;8,151.52,338.85,329.07,7.86;8,151.52,349.81,314.36,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,260.81,327.89,219.78,7.86;8,151.52,338.85,59.35,7.86">Anserini: Enabling the use of lucene for information retrieval research</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,231.23,338.85,249.36,7.86;8,151.52,349.81,221.39,7.86">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1253" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,360.77,337.98,7.86;8,151.52,371.73,197.75,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="8,278.79,360.77,201.80,7.86;8,151.52,371.73,32.07,7.86">Simple applications of bert for ad hoc document retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10972</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.62,382.69,337.97,7.86;8,151.52,393.65,329.07,7.86;8,151.52,404.61,329.07,7.86;8,151.52,415.56,329.07,7.86;8,151.52,426.52,37.88,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,370.74,382.69,109.84,7.86;8,151.52,393.65,75.37,7.86">Applying bert to document retrieval with birch</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,247.07,393.65,233.53,7.86;8,151.52,404.61,329.07,7.86;8,151.52,415.56,293.68,7.86">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,437.48,337.98,7.86;8,151.52,448.44,329.07,7.86;8,151.52,459.40,329.07,7.86;8,151.52,470.36,288.90,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,377.80,437.48,102.79,7.86;8,151.52,448.44,151.33,7.86">Accelerating substructure similarity search for formula retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,378.68,459.40,101.92,7.86;8,151.52,470.36,34.93,7.86">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Castells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Martins</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="714" to="727" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
