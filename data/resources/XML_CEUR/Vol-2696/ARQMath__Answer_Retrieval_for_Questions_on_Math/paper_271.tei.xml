<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,208.96,115.90,197.43,12.68;1,186.53,133.83,242.31,12.68;1,260.46,151.77,94.44,12.68;1,167.00,169.70,281.36,12.68">Overview of ARQMath 2020 (Updated Working Notes Version): CLEF Lab on Answer Retrieval for Questions on Math</title>
				<funder ref="#_Ap3PsuM">
					<orgName type="full">Alfred P. Sloan Foundation</orgName>
				</funder>
				<funder ref="#_XdHtuaU">
					<orgName type="full">National Science Foundation (USA)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,225.13,207.36,68.99,8.80"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology)</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,306.41,207.36,74.37,8.80"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.94,219.32,69.36,8.80"><forename type="first">Anurag</forename><surname>Agarwal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology)</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.29,219.32,79.65,8.80"><forename type="first">Behrooz</forename><surname>Mansouri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology)</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,208.96,115.90,197.43,12.68;1,186.53,133.83,242.31,12.68;1,260.46,151.77,94.44,12.68;1,167.00,169.70,281.36,12.68">Overview of ARQMath 2020 (Updated Working Notes Version): CLEF Lab on Answer Retrieval for Questions on Math</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">73F5AEF2E172C0B115C6166E7D762A40</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Community Question Answering (CQA)</term>
					<term>Mathematical Information Retrieval</term>
					<term>Math-aware search</term>
					<term>Math formula search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ARQMath Lab at CLEF considers finding answers to new mathematical questions among posted answers on a community question answering site (Math Stack Exchange). Queries are question posts held out from the searched collection, each containing both text and at least one formula. This is a challenging task, as both math and text may be needed to find relevant answer posts. ARQMath also includes a formula retrieval sub-task: individual formulas from question posts are used to locate formulae in earlier question and answer posts, with relevance determined considering the context of the post from which a query formula is taken, and the posts in which retrieved formulae appear.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="792.0" lry="612.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In a recent study, <ref type="bibr" coords="1,214.93,508.70,66.41,8.80">Mansouri et al.</ref> found that 20% of mathematical queries in a general-purpose search engine were expressed as well-formed questions, a rate ten times higher than that for all queries submitted <ref type="bibr" coords="1,347.16,532.61,14.61,8.80" target="#b13">[14]</ref>. Results such as these and the presence of Community Question Answering (CQA) sites such as Math Stack Exchange 3 suggest there is interest in finding answers to mathematical questions posed in natural language, using both text and mathematical notation. Related to this, there has also been increasing work on math-aware information retrieval and math question answering in both the Information Retrieval (IR) and Natural Language Processing (NLP) communities.</p><p>Table <ref type="table" coords="2,163.18,116.37,4.13,7.93">1</ref>. Examples of relevant and not-relevant results for tasks 1 and 2 <ref type="bibr" coords="2,425.94,115.83,13.51,8.97" target="#b11">[12]</ref>. For Task 2, formulas are associated with posts, indicated with ellipses at right (see Figure <ref type="figure" coords="2,461.93,126.79,4.61,8.97" target="#fig_0">1</ref> for more details). Query formulae are from question posts (here, the question at left), and retrieved formulae are from either an answer or a question post. . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relevant</head><p>You can use AM ≥ GM.</p><formula xml:id="formula_0" coords="2,184.50,249.69,273.01,63.81">1 + 1 + • • • + 1 + √ n + √ n n ≥ n 1/n ≥ 1 1 - 2 n + 2 √ n ≥ n 1/n ≥ 1 Relevant . . . lim n→∞ n √ n . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Not Relevant</head><p>If you just want to show it converges, then the partial sums are increasing but the whole series is bounded above by</p><formula xml:id="formula_1" coords="2,220.19,323.47,243.99,47.19">1 + ∞ 1 1 x 2 dx = 2 Not Relevant . . . k=1 1 k 2 = π 2 6 . . .</formula><p>In light of this growing interest, we organized this new lab at the Conference and Labs of the Evaluation Forum (CLEF) on Answer Retrieval for Questions about Math (ARQMath). <ref type="foot" coords="2,249.24,419.53,3.97,6.16" target="#foot_0">4</ref> Using the formulae and text in posts from Math Stack Exchange, participating systems are given a question, and asked to return a ranked list of potential answers. Relevance is determined by how well each returned post answers the provided question. Through this task we explore leveraging math notation together with text to improve the quality of retrieval results. This is one case of what we generically call math retrieval, in which the focus is on leveraging the ability to process mathematical notation to enhance, rather than to replace, other information retrieval techniques. We also included a formula retrieval task, in which relevance is determined by how useful a retrieved formula is for the searcher's intended purpose, as best could be determined from the query formula's associated question post. Table <ref type="table" coords="2,358.91,540.63,4.98,8.80">1</ref> illustrates these two tasks, and Figure <ref type="figure" coords="2,185.57,552.59,4.98,8.80" target="#fig_0">1</ref> shows the topic format for each task.</p><p>For the CQA task, 70,342 questions from 2019 that contained some text and at least one formula were considered as search topics, from which 77 were selected as test topics. Participants had the option to run queries using only the text or math portions of each question, or to use both math and text. One challenge inherent in this design is that the expressive power of text and formulae are sometimes complementary; so although all topics will include both text and formula(s), some may be better suited to text-based or math-based retrieval.</p><p>Task 1: Question Answering &lt;Topics&gt; . . . &lt;Topic number="A . 9 "&gt; &lt;T i t l e&gt; S i m p l i f y i n g t h i s s e r i e s&lt;/ T i t l e&gt; &lt;Question&gt; I n e e d t o w r i t e t h e s e r i e s &lt;span c l a s s = math-c o n t a i n e r ' ' i d = q_52 ' '&gt; $ $ \sum_{ n=0}^N nx^n $$ &lt;/ span&gt; i n a f o r m t h a t d o e s n o t i n v o l v e t h e summation n o t a t i o n , f o r e x a m p l e &lt;span c l a s s = math-c o n t a i n e r ' ' i d = q_53 ' '&gt; $\sum_{ i =0}^n i ^2 = \ f r a c { ( n^2+n ) ( 2 n +1)}{6}$ &lt;/ span&gt; Does a n y o n e h a v e any i d e a how t o do t h i s ? I h a v e a t t e m p t e d m u l t i p l e ways i n c l u d i n g u s i n g g e n e r a t i n g f u n c t i o n s h o w e v e r no l u c k . &lt;/ Question&gt; &lt;Tags&gt;s e q u e n c e s -and-s e r i e s&lt;/Tags&gt; &lt;/ Topic&gt; . . . &lt;/ Topics&gt; Task 2: Formula Retrieval &lt;Topics&gt; . . . &lt;Topic number="B . 9 "&gt; &lt;Formula_Id&gt;q_52&lt;/Formula_Id&gt; &lt;Latex&gt;\sum_{ n=0}^N nx^n&lt;/ Latex&gt; &lt;T i t l e&gt; S i m p l i f y i n g t h i s s e r i e s&lt;/ T i t l e&gt; &lt;Question&gt; . . . &lt;/ Question&gt; &lt;Tags&gt;s e q u e n c e s -and-s e r i e s&lt;/Tags&gt; &lt;/ Topic&gt; . . . &lt;/ Topics&gt; Fig. <ref type="figure" coords="3,154.39,437.50,4.13,7.93" target="#fig_0">1</ref>. XML Topic File Formats for Tasks 1 and 2. Formula queries in Task 2 are taken from questions in Task 1. Here, formula topic B.9 is a copy of question topic A.9 with two additional tags for the query formula identifier and L A T E X before the question post.</p><p>For the formula search task, an individual formula is used as the query, and systems return a ranked list of other potentially useful instances of formulae found in the collection. Each of the 45 queries is a single formula extracted from a question used in the CQA task.</p><p>Mathematical problem solving was amongst the earliest applications of Artificial Intelligence, such as Newell and Simon's work on automatic theorem proving <ref type="bibr" coords="3,134.77,570.54,14.61,8.80" target="#b14">[15]</ref>. More recent work in math problem solving includes systems that solve algebraic word problems while providing a description of the solution method <ref type="bibr" coords="3,462.33,582.50,14.61,8.80" target="#b10">[11]</ref>, and that solve algebra word problems expressed in text and math <ref type="bibr" coords="3,417.64,594.45,14.61,8.80" target="#b9">[10]</ref>. The focus of ARQMath is different; rather than prove or solve concrete mathematical problems, we instead look to find answers to informal, and potentially open-ended and incomplete questions posted naturally in a CQA setting.</p><p>The ARQMath lab provides an opportunity to push mathematical question answering in a new direction, where answers provided by a community are se-lected and ranked rather than generated. We aim to produce test collections, drive innovation in evaluation methods, and drive innovation in the development of math-aware information retrieval systems. An additional goal is welcoming new researchers to work together on these challenging problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The Mathematical Knowledge Management (MKM) research community is concerned with the representation, application, and search of mathematical information. Among other accomplishments, their activities informed the development of MathML<ref type="foot" coords="4,184.19,242.90,3.97,6.16" target="#foot_1">5</ref> for math on the Web, and novel techniques for math representation, search, and applications such as theorem proving. This community continues to meet annually at the CICM conferences <ref type="bibr" coords="4,311.64,268.37,9.96,8.80" target="#b7">[8]</ref>.</p><p>Math-aware search (sometimes called Mathematical Information Retrieval ) has seen growing interest over the past decade. Math formula search has been studied since the mid-1990's for use in solving integrals, and publicly available math+text search engines have been around since the DLMF<ref type="foot" coords="4,411.06,314.63,3.97,6.16" target="#foot_2">6</ref> system in the early 2000's <ref type="bibr" coords="4,190.26,328.14,10.51,8.80" target="#b5">[6,</ref><ref type="bibr" coords="4,202.43,328.14,11.62,8.80" target="#b20">21]</ref>. The most widely used evaluation resources for math-aware information retrieval were initially developed over a five-year period at the National Institute of Informatics (NII) Testbeds and Community for Information access Research (at NTCIR-10 [1], NTCIR-11 <ref type="bibr" coords="4,336.91,364.01,10.51,8.80" target="#b1">[2]</ref> and NTCIR-12 <ref type="bibr" coords="4,419.15,364.01,14.76,8.80" target="#b19">[20]</ref>). NTCIR-12 used two collections, one a set of arXiv papers from physics that is split into paragraph-sized documents, and the other a set of articles from English Wikipedia. The NTCIR Mathematical Information Retrieval (MathIR) tasks developed evaluation methods and allowed participating teams to establish baselines for both "text + math" queries (i.e., keywords and formulas) and isolated formula queries.</p><p>A recent math question answering task was held for SemEval 2019 <ref type="bibr" coords="4,439.40,447.70,9.96,8.80" target="#b6">[7]</ref>. Question sets from MathSAT (Scholastic Achievement Test) practice exams in three categories were used: Closed Algebra, Open Algebra and Geometry. A majority of the questions were multiple choice, with some having numeric answers. This is a valuable parallel development; the questions considered in the CQA task of ARQMath are more informal and open-ended, and selected from actual MSE user posts (a larger and less constrained set).</p><p>At NTCIR-11 and NTCIR-12, formula retrieval was considered in a variety of settings, including the use of wildcards and constraints on symbols or subexpressions (e.g., requiring matched argument symbols to be variables or constants). Our Task 2, Formula Retrieval, has similarities in design to the NTCIR-12 Wikipedia Formula Browsing task, but differs in how queries are defined and how evaluation is performed. In particular, for evaluation ARQMath uses the visually distinct formulas in a run, rather than all (possibly identical) formula instances, as had been done in NTCIR-12. The NTCIR-12 formula retrieval test collection also had a smaller number of queries, with 20 fully specified formula queries (plus 20 variants of those same queries with subexpressions replaced by wildcard characters). NTCIR-11 also had a formula retrieval task, with 100 queries, but in that case systems searched only for exact matches <ref type="bibr" coords="5,422.70,142.84,14.61,8.80" target="#b18">[19]</ref>.</p><p>Over the years, the size of the NTCIR-12 formula browsing task topic set has limited the diversity of examples that can be studied, and made it difficult to measure statistically significant differences in formula retrieval effectiveness. To support research that is specifically focused on formula similarity measures, we have create a formula search test collection that is considerably larger, and in which the definition of relevance derives from the specific task for which retrieval is being performed, rather than isolated formula queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The ARQMath 2020 Math Stack Exchange Collection</head><p>In this section we describe the raw data from which we started, collection processing, and the resulting test that was used in both tasks. Topic development for each task is described in the two subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MSE Internet Archive Snapshot</head><p>We chose Math Stack Exchange (MSE), a popular community question answering site as the collection to be searched. The Internet Archive provides free public access to MSE snapshots. <ref type="foot" coords="5,245.82,377.54,3.97,6.16" target="#foot_3">7</ref> We processed the 01-March-2020 snapshot, which in its original form contained the following in separate XML files:</p><p>-Posts: Each MSE post has a unique identifier, and can be a question or an answer, identified by 'post type id' of 1 and 2 respectively. Each question has a title and a body (content of the question) while answers only have a body. Each answer has a 'parent id' that associates it with the question it is an answer is for. There is other information available for each post, including its score, the post owner id and creation date. -Comments: MSE users can comment on posts. Each comment has a unique identifier and a 'post id' indicating which post the comment is written for. -Post links: Moderators sometimes identify duplicate or related questions that have been previously asked. A 'post link type id' of value 1 indicates related posts, while value 3 indicates duplicates. -Tags: Questions can have one or more tags describing the subject matter of the question. -Votes: While the post score shows the difference between up and down votes, there are other vote types such as 'offensive' or 'spam.' Each vote has a 'vote type id' for the vote type and a 'post id' for the associated post. -Users: Registered MSE users have a unique id, and they can provide additional information such as their website. Each user has a reputation score, which may be increased through activities such as posting a high quality answer, or posting a question that receives up votes.</p><p>-Badges: Registered MSE users can also receive three badge types: bronze, silver and gold. The 'class' attribute shows the type of the badge, value 3 indicating bronze, 2 silver and 1 gold.</p><p>The edit history for posts and comments is also available, but for this edition of the ARQMath lab, edit history information has not been used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The ARQMath 2020 Test Collection</head><p>Because search topics are built from questions asked in 2019, all training and retrieval is performed on content from 2018 and earlier. We removed any data from the collection generated after the year 2018, using the 'creation date' available for each item. The final collection contains roughly 1 million questions and 28 million formulae. Formulae. While MSE provides a &lt;math-container&gt; HTML tag for some mathematical formulae, many are only present as a L A T E X string located between single or double '$' signs. Using the math-container tags and dollar sign delimiters we identified formulae in question posts, answer posts, and comments. Every identified instance of a formula was assigned a unique identifier, and then placed in a &lt;math-container&gt; HTML tag using the form: &lt;span id=FID class="math-container"&gt;... &lt;/span&gt; where FID is the formula id. Overall, 28,320,920 formulae were detected and annotated in this way.</p><p>Additional Formula Representations. Rather than use raw L A T E X, it is common for math-aware information retrieval systems to represent formulas as one or both of two types of rooted trees. Appearance is represented by the spatial arrangement of symbols on writing lines (in Symbol Layout Trees (SLTs)), and mathematical syntax (sometimes referred to as (shallow) semantics) is represented using a hierarchy of operators and arguments (in Operator Trees (OPTs)) <ref type="bibr" coords="6,201.98,481.98,10.51,8.80" target="#b4">[5,</ref><ref type="bibr" coords="6,214.16,481.98,12.73,8.80" target="#b12">13,</ref><ref type="bibr" coords="6,228.54,481.98,11.62,8.80" target="#b22">23]</ref>. The standard representations for these are Presentation MathML (SLT) and Content MathML (OPT). To simplify the processing required of participants, and to maximize comparability across submitted runs, we used LaTeXML<ref type="foot" coords="6,218.57,516.29,3.97,6.16" target="#foot_4">8</ref> to generate Presentation MathML and Content MathML from L A T E X for each formula in the ARQMath collection. Some L A T E X formulas were malformed and LaTeXML has some processing limitations, resulting in conversion failures for 8% of SLTs, and and 10% of OPTs. Participants could elect to do their own formula extraction and conversions, although the formulae that could be submitted in system runs for Task 2 were limited to those with identifiers in the L A T E X TSV file.</p><p>ARQMath formulae are provided in L A T E X, SLT, and OPT representations, as Tab Separated Value (TSV) index files. Each line of a TSV file represents a single instance of a formula, containing the formula id, the id of the post in which the formula instance appeared, the id of the thread in which the post is located, a post type (title, question, answer or comment), and the formula representation in either L A T E X, SLT (Presentation MathML), or OPT (Content MathML). There are two sets of formula index files: one set is for the collection (i.e., the posts from 2018 and before), and the second set is for the search topics (see below), which are from 2019.</p><p>HTML Question Threads. HTML views of threads, similar to those on the MSE web site (a question, along with answers and other related information) are also included in the ARQMath test collection. The threads are constructed automatically from the MSE snapshot XML files described above. The threads are intended for use by teams who performed manual runs, or who wished to examine search results (on queries other than evaluation queries) for formative evaluation purposes. These threads were also used by assessors during evaluation. The HTML thread files were intended only for viewing threads; participants were asked to use the provided XML and formula index files (described above) to train their models.</p><p>Distribution. The MSE test collection was distributed to participants as XML files on Google drive. <ref type="foot" coords="7,248.29,308.66,3.97,6.16" target="#foot_5">9</ref> To facilitate local processing, the organizers provided python code on GitHub<ref type="foot" coords="7,241.79,320.62,7.94,6.16" target="#foot_6">10</ref> for reading and iterating over the XML data, and generating the HTML question threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Task 1: Answer Retrieval</head><p>The primary task for ARQMath 2020 was the answer retrieval task, in which participants were presented with a question that had actually been asked on MSE in 2019, and were asked to return a ranked list of up to 1,000 answers from prior years (2010-2018). System results ('runs') were evaluated using rank quality measures (e.g., nDCG ), so this is a ranking task rather than a set retrieval task, and participating teams were not asked to say where the searcher should stop reading. This section describes for Task 1 the search topics (i.e., the questions), the submissions and baseline systems, the process used for creating relevance judgments, the evaluation measures, and the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Topics</head><p>In Task 1 participants were given 101 questions as search topics, of which 3 were training examples. These questions are selected from questions asked on MSE in 2019. Because we wished to support experimentation with retrieval systems that use text, math, or both, we chose from only the 2019 questions that contain some text and at least one formula. Because ranking quality measures can distinguish between systems only on topics for which relevant documents exist, we calculated the number of duplicate and related posts for each question and chose only from those that had at least one duplicate or related post. <ref type="foot" coords="7,394.69,614.40,7.94,6.16" target="#foot_7">11</ref> Because we were interested in a diverse range of search tasks, we also calculated the number of formulae and Flesch's Reading Ease score <ref type="bibr" coords="8,315.80,130.89,10.51,8.80" target="#b8">[9]</ref> for each question. Finally, we noted the asker's reputation and the tags assigned for each question. We then manually drew a sample of 101 questions that was stratified along those dimensions. In the end, 77 of these questions were evaluated and included in the test collection.</p><p>The topics were selected from various domains (real analysis, calculus, linear algebra, discrete mathematics, set theory, number theory, etc.) that represent a broad spectrum of areas in mathematics that might be of interest to expert or non-expert users. The difficulty level of the topics spanned from easy problems that a beginning undergraduate student might be interested in to difficult problems that would be of interest to more advanced users. The bulk of the topics were aimed at the level of undergraduate math majors (in their 3rd or 4th year) or engineering majors fulfilling their math requirements.</p><p>Some topics had simple formulae; others had fairly complicated formulae with subscripts, superscripts, and special symbols like the double integral V f (x, y)dx dy or binomial coefficients such as n r . Some topics were primarily based on computational steps, and some asked about proof techniques (making extensive use of text). Some topics had named theorems or concepts (e.g. Cesàro-Stolz theorem, Axiom of choice).</p><p>As organizers, we labeled each question with one of three broad categories, computation, concept or proof. Out the 77 assessed questions, 26 were categorized as computation, 10 as concept, and 41 as proof. We also categorized the questions based on their perceived difficulty level, with 32 categorized as easy, 21 as medium, and 24 as hard.</p><p>The topics were published as an XML file with the format shown in Figure <ref type="figure" coords="8,152.50,418.86,3.87,8.80" target="#fig_0">1</ref>, where the topic number is an attribute of the Topic tag, and the Title, Question and asker-provided Tags are from the MSE question post. To facilitate system development, we provided python code that participants could use to load the topics. As in the collection, the formulae in the topic file are placed in 'math-container' tags, with each formula instance being represented by a unique identifier and its L A T E X representation. And, as with the collection, we provided three TSV files, one each for the L A T E X, OPT and SLT representations of the formulae, in the same format as the collection's TSV files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Runs Submitted by Participating Teams</head><p>Participating teams submitted runs using Google Drive. A total of 18 runs were were received from a total of 5 teams. Of these, 17 runs were declared as automatic, meaning that queries were automatically processed from the topic file, that no changes to the system had been made after seeing the queries, and that ranked lists for each query were produced with no human intervention. One run was declared as manual, meaning that there was some type of human involvement in generating the ranked list for each query. Manual runs can contribute diversity to the pool of documents that are judged for relevance, since their error characteristics typically differ from those of automatic runs. All submitted runs used both text and formulae. The teams and submissions are shown in Table <ref type="table" coords="9,134.77,116.37,34.67,7.93">Table 2</ref>. Submitted Runs for Task 1 (18 runs) and Task 2 (11 runs). Additional baselines for Task 1 (5 runs) and Task 2 (1 run) were also generated by the organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic Runs</head><p>Manual Runs Primary Alternate</p><p>Primary Alternate Task 1: Question Answering</p><formula xml:id="formula_2" coords="9,199.67,175.39,198.85,98.83">Baselines 4 1 DPRL 1 3 MathDowsers 1 3 1 MIRMU 3 2 PSU 1 2 ZBMath 1 Task 2: Formula Retrieval Baseline 1 DPRL 1 3 MIRMU 2 3 NLP-NIST 1 ZBMath 1</formula><p>2. Please see the participant papers in the working notes for descriptions of the systems that generated these runs.</p><p>Of the 17 runs declared as automatic, two were in fact manual runs (for ZBMath, see Table <ref type="table" coords="9,221.01,334.41,3.87,8.80">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline Runs</head><p>As organizers, we ran five baseline systems for Task 1. The first baseline is a TF-IDF (term frequency-inverse document frequency) model using the Terrier system <ref type="bibr" coords="9,187.51,415.48,14.61,8.80" target="#b16">[17]</ref>. In the TF-IDF baseline, formulae are represented using their L A T E X string. The second baseline is Tangent-S, a formula search engine using SLT and OPT formula representations <ref type="bibr" coords="9,306.80,439.39,9.96,8.80" target="#b4">[5]</ref>. One formula was selected from each Task 1 question title if possible; if there was no formula in the title, then one formula was instead chosen from the question's body. If there were multiple formulae in the selected field, the formula with the largest number of nodes in its SLT representation was chosen. Finally, if there were multiple formulae with the highest number of nodes, one of these was chosen randomly. The third baseline is a linear combination of TF-IDF and Tangent-S results. To create this combination, first the relevance scores from both systems were normalized between 0 and 1 using min-max normalization, and then the two normalized scores were combined using an unweighted average.</p><p>The TF-IDF baseline used default parameters in Terrier. The second baseline (Tangent-S) retrieves formulae independently for each representation, and then linearly combines SLT and OPT scoring vectors for retrieved formulae <ref type="bibr" coords="9,467.31,583.59,9.96,8.80" target="#b4">[5]</ref>. For ARQMath, we used the average weight vector from cross validation results obtained on the NTCIR-12 formula retrieval task.</p><p>The fourth baseline was the ECIR 2020 version of the Approach0 text + math search engine <ref type="bibr" coords="9,221.30,632.15,14.61,8.80" target="#b21">[22]</ref>, using queries manually created by the third and fourth authors. This baseline was not available in time to contribute to the judgment pools and thus was scored post hoc. The final baseline was built from duplicate post links from 2019 in the MSE collection (which were not available to participants). This baseline returns all answer posts from 2018 or earlier that were in threads from 2019 or earlier that MSE moderators had marked as duplicating the question post in a topic. The posts are sorted in descending order by their vote scores.</p><p>Performance. Table <ref type="table" coords="10,247.65,299.17,4.98,8.80" target="#tab_0">3</ref> shows the minimum, maximum, average, and standard deviation of retrieval times for each of the baseline systems. For running all the baselines, we used a system of 528 GB Ram, with Intel(R) Xeon(R) CPU E5-2667 v4 @ 3.20GHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Assessment</head><p>Pooling. Participants were asked to rank 1,000 (or fewer) answer posts for each Task 1 topic. Top-k pooling was then performed to create pools of answer posts to be judged for relevance to each topic. The top 50 results were combined from all 7 primary runs, 4 baselines, and 1 manual run. To this, we added the top 20 results from each of the 10 automatic alternate runs. Duplicates were then deleted, and the resulting pool was sorted in random order for display to assessors. The pooling process is illustrated in Figure <ref type="figure" coords="10,392.75,463.41,3.87,8.80">2</ref>. This process was designed to identify as many relevant answer posts as possible given the available assessment resources. On average, pools contained about 500 answers per topic.</p><p>Relevance definition. Some questions might offer clues as to the level of mathematical knowledge on the part of the person posing the question; others might not. To avoid the need for the assessor to guess about the level of mathematical knowledge available to the person interpreting the answer, we asked assessors to base their judgments on degree of usefulness for an expert (modeled in this case as a math professor) who might then try to use that answer to help the person who had asked the original question. We defined four levels of relevance, as shown in Table <ref type="table" coords="10,261.90,583.64,3.87,8.80" target="#tab_1">4</ref>.</p><p>Assessors were allowed to consult external sources on their own in order to familiarize themselves with the topic of a question, but the relevance judgments for each answer post were performed using only information available within the collection. For example, if an answer contained an MSE link such as https: //math.stackexchange.com/questions/163309/pythagorean-theorem, they could follow that link to better understand the intent of the person writing the answer, but an external link to the Wikipedia page https://en.wikipedia. org/wiki/Pythagorean_theorem would not be followed.</p><p>Training Set. The fourth author created a small set of relevance judgment files for three topics. We used duplicate question links to find possibly relevant answers, and then performed relevance judgments on the same 0, 1, 2 and 3 scale that was later used by the assessors. We referred to this as a 'training set,' although in practice such a small collection is at best a sanity check to see if</p><formula xml:id="formula_3" coords="11,213.82,496.34,4.80,67.63">… …</formula><p>Top-20 answers selected from alternate runs for a given query.</p><p>Top-50 answers selected from baselines, primary and manual runs, for a given query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>… …</head><p>Top-10 visually distinct formulae selected from each alternate run for a given formula query.</p><p>Top-25 visually distinct formulae selected from baseline and each primary run, for a given formula query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling</head><p>Task 2: FORMULA RETRIEVAL Task 1: QUESTION ANSWERING Fig. <ref type="figure" coords="11,154.39,632.58,4.13,7.93">2</ref>. Pooling Procedures. For Task 1, the pool depth for baselines, primary, and manual runs is 50, and for alternate runs 20. For Task 2 pool depth is the rank at which k visually distinct formulae are observed (25 for primary/baseline, 10 for alternate).</p><p>systems were producing reasonable results. Moreover, these relevance judgments were performed before assessor training had been conducted, and thus the definition of relevance used by the fourth author may have differed in subtle ways from the definitions on which the assessors later settled.</p><p>Assessment System. Assessments were performed using Turkle 12 , a locally installed system with functionality similar to Amazon Mechanical Turk. Turkle uses an HTML task template file, plus a Comma Separate Value (CSV) file to fill HTML templates for each topic. Each row in the CSV file contains the question title, body, and the retrieved answer to be judged. Judgments are exported as CSV files.</p><p>As Figure <ref type="figure" coords="12,195.94,239.76,4.98,8.80" target="#fig_6">6</ref> (at the end of this document) illustrates, there were two panels in the Turkle user interface. The question was shown on the left panel, with the Title on top (in a grey bar); below that was the question body. There was also a Thread link, on which assessors could click to look at the MSE post in context, with the question and all of the answers that were actually given for this question (in 2019). This could help the assessor to better understand the question. In the right panel, the answer to be judged was shown at the top. As with the question, there was a thread link where the assessors could click to see the original thread in which the answer post being judged had been present in MSE. This could be handy when the assessors wanted to see details such as the question that had been answered at the time. Finally, the bottom of the right panel (below the answer) was where assessors selected relevance ratings. In addition to four levels of relevance, two additional choices were available. 'System failure' indicated system issues such as unintelligible rendering of formulae, or the thread link not working (when it was essential for interpretation). If after viewing the threads, the assessors were still not able to decide the relevance degree, they were asked to choose 'Do not know'. The organizers asked the assessors to leave a comment in the event of a system failure or a 'Do no know' selection.</p><p>Assessor Training. Eight paid undergraduate mathematics students (or, in three cases, recent graduates with an undergraduate mathematics degree) were paid to perform relevance judgments. Four rounds of training were performed before submissions from participating teams had been received. In the first round, assessors met online using Zoom with the organizers, one of whom (the third author) is an expert MSE user and a Professor of mathematics. The task was explained, making reference to specific examples from the small training set. For each subsequent round, a small additional additional training set was created using a similar approach (pooling only answers to duplicate questions) with 8 actual Task 1 topics (for which the actual relevance judgments were not then known). The same 8 topics were assigned to every assessor and the assessors worked independently, thus permitting inter-annotator agreement measures to be computed. Each training round was followed by an online meeting with the organizers using Zoom at which assessors were shown cases in which one or more assessor pairs disagreed. They discussed the reasoning for their choices, with the third author offering reactions and their own assessment. These training judg- ments were not used in the final collection, but the same topic could later be reassigned to one of the assessors to perform judgments on a full pool. Some of the question topics would not be typically covered in regular undergraduate courses, so that was a challenge that required the assessors to get a basic understanding of those topics before they could do the assessment. The assessors found the questions threads made available in the Turkle interface helpful in this regard (see Figure <ref type="figure" coords="13,248.25,398.32,3.87,8.80" target="#fig_6">6</ref>).</p><p>Through this process the formal definition of each relevance level in Table <ref type="table" coords="13,134.77,423.24,4.98,8.80" target="#tab_1">4</ref> was sharpened, and we sought to help assessors internalize a repeatable way of making self-consistent judgments that were reasonable in the view of the organizers. Judging relevance is a task that calls for interpretation and formation of a personal opinion, so it was not our goal to achieve identical decisions. We did, however, compute Fleiss' Kappa for the three independently conducted rounds of training to check whether reasonable levels of agreement were being achieved. As Figure <ref type="figure" coords="13,182.63,494.97,4.98,8.80" target="#fig_1">3</ref> shows, kappa of 0.34 was achieved by the end of training on the four-way assessment task. Collapsing relevance to be binary by considering high and medium as relevant and low and not-relevant as a not-relevant (henceforth "H+M binarization") yielded similar results. 13  Assessment. A total of 80 questions were assessed for Task 1. Three judgment pools (for topics A2, A22, and A70) had zero or one posts with relevance ratings of high or medium; these 3 topics were removed from the collection be-  cause topics with no relevant posts cannot be used to distinguish between ranked retrieval systems, and topics with only a single relevant post result in coarsely quantized values for H+M binarized evaluation measures, and that degree of quantization can adversely affect the ability to measure statistically significant differences. For the remaining 77 questions, an average of 508.5 answers were assessed for each question, with an average assessment time of 63.1 seconds per answer post. The average number of answers labeled with any degree of relevance (high, medium, or low; henceforth "H+M+L binarization") was 52.9 per question, with the highest number of relevant answers being 188 (for topic A.38) and the lowest being 2 (for topic A.96).</p><p>Post Assessment. After the official assessments were complete for Task 1, each assessor was assigned two tasks completed by two other assessors to calculate their agreement. As shown in Figure <ref type="figure" coords="14,316.62,476.86,3.87,8.80" target="#fig_2">4</ref>, across all five assessors ('Total') an average Cohen's kappa of 0.29 was achieved on the four-way assessment task, and using H+M binarization the average kappa value was 0.39. The individual assessors are reasonably similar (particularly in terms of 4-way agreement) except for Assessor 4. Comparing Figures <ref type="figure" coords="14,285.66,524.68,4.98,8.80" target="#fig_1">3</ref> and<ref type="figure" coords="14,312.49,524.68,3.87,8.80" target="#fig_2">4</ref>, we see that agreement was relatively stable between the end of training and after assessments were completed. After assessment was complete, a slightly lower 4-way agreement but higher H+M binarized agreement was obtained relative to the end of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation Measures</head><p>One risk when performing a new task for which rich training data is not yet available is that a larger than typical number of relevant answers may be missed. Measures which treat unjudged documents as not relevant can be used when directly comparing systems that contributed to the judgment pools, but subsequent use of such a first-year test collection (e.g., to train new systems for the second year of the lab) can be disadvantaged by treating unjudged documents (which as systems improve might actually be relevant) as not relevant. We therefore chose the nDCG measure (read as "nDCG-prime") introduced by Sakai and Kando <ref type="bibr" coords="15,166.86,154.80,15.49,8.80" target="#b17">[18]</ref> as the primary measure for the task.</p><p>The nDCG measure on which nDCG is based is a widely used measure when graded relevance judgments are available, as we have in ARQMath, that produced a single figure of merit over a set of ranked lists. Each retrieved document earns a gain value of (0, 1, 2, or 3) discounted by a slowly decaying function of the rank position of each document. The resulting discounted gain values are accumulated and then normalized to [0,1] by dividing by the maximum possible Discounted Cumulative Gain (i.e., from all identified relevant documents, sorted by decreasing order of gain value). This results in normalized Discounted Cumulative Gain (nDCG). The only difference when computing nDCG is that unjudged documents are removed from the ranked list before performing the computation. It has been shown that nDCG has somewhat better discriminative power and somewhat better system ranking stability (with judgement ablation) than the bpref measure <ref type="bibr" coords="15,276.14,310.22,10.51,8.80" target="#b3">[4]</ref> used recently for formula search (e.g., <ref type="bibr" coords="15,458.46,310.22,14.76,8.80" target="#b12">[13]</ref>). Moreover, nDCG yields a single-valued measure with graded relevance, whereas bpref, Precision@k, and Mean Average Precision (MAP) all require binarized relevance judgments. In addition to nDCG , we also compute Mean Average Precision (MAP) with unjudged posts removed (thus MAP ), and Precision at 10 posts (P@10). <ref type="foot" coords="15,207.02,368.44,7.94,6.16" target="#foot_8">14</ref> For MAP and P@10 we used H+M binarization. We removed unjudged posts as a preprocessing step where required, and then computed the evaluation measures using trec_eval. <ref type="foot" coords="15,295.46,392.35,7.94,6.16" target="#foot_9">15</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results</head><p>Table <ref type="table" coords="15,161.69,442.80,12.45,8.80">A1</ref> in the appendix shows the results, with baselines shown first, and then teams and their systems ranked by nDCG . nDCG values can be interpreted as the average (over topics) of the fraction of the score for the best possible that was actually achieved. As can be seen, the best nDCG value that was achieved was 0.345, by the MathDowsers team. For measures computed using H+M binarization we can see that MAP and P@10 generally show system comparison patterns similar to those of nDCG , although with some differences in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Task 2: Formula Retrieval</head><p>In the formula retrieval task, participants were presented with one formula from a 2019 question used in Task 1, and asked to return a ranked list of up to 1,000 formula instances from questions or answers from the evaluation epoch (2018 or earlier). Formulae were returned by their identifiers in math-container tags and the companion TSV L A T E X formula index file, along with their associated post identifiers.</p><p>This task is challenging because someone searching for math formulae may have goals not evident from the formula itself. For example:</p><p>-They may be looking to learn what is known, to form connections between disciplines, or to discover solutions that they can apply to a specific problem. -They may want to find formulae of a specific form, including details such as specific symbols that have significance in a certain context, or they may wish to find related work in which similar ideas are expressed using different notation. For example, the Schrödinger equation is written both as a wave equation and as a probability current (the former is used in Physics, whereas the latter is used in the study of fluid flow). -They may be happy to find formulae that contain only part of their formula query, or they may want only complete matches. For example, searching for n i=1 u i v i could bring up the Cauchy-Schwarz inequality</p><formula xml:id="formula_4" coords="16,166.78,305.18,313.81,29.99">n i=1 u i v i ≤ n i=1 u 2 i 1 2 n i=1 v 2 i 1 2 .</formula><p>For these reasons (among others), it is difficult to formulate relevance judgments for retrieved formulae without access to the context in which the formula query was posed, and to the context in which each formula instance returned as a potentially useful search result was expressed.</p><p>Three key details differentiate Task 2 from Task 1. First, in Task 1 only answer posts were returned, but for Task 2 the formulae may appear in answer posts or in question posts. Second, for Task 2 we distinguish visually distinct formulae from instances of those formulae, and evaluate systems based on the ranking of the visually distinct formulae that they return. We call formulae appearing in posts formula instances, and of course the same formula may appear in more than one post. By formula, we mean a set of formula instances that are visually identical when viewed in isolation. For example, x 2 is a formula, x * x is a different formula, and each time x 2 appears is a distinct instance of the formula x 2 . Systems in Task 2 rank formula instances in order to support the relevance judgment process, but the evaluation measure for Task 2 is based on the ranking of visually distinct formulae. The third difference between Task 1 and Task 2 is that in Task 2 the goal is not answering questions, but rather, to show the searcher formulae that might be useful to them as they seek to satisfy their information need. Task 2 is thus still grounded in the question, but the relevance of a retrieved formula is defined by the formula's expected utility, not just the post in which that one formula instance was found.</p><p>As with Task 1, ranked lists were evaluated using rank quality measures, making this a ranking task rather than a set retrieval task. Unlike Task 1, the design of which was novel, a pre-existing training set for a similar task (the NTCIR-12 Wikipedia Formula Browsing task test collection <ref type="bibr" coords="16,400.80,632.15,15.49,8.80" target="#b19">[20]</ref>) was available to participants. However, we note that the definition of relevance used in Task 2 differs from the definition of relevance in the NTCIR-12 task. This section describes for Task 2 the search topics, the submissions and baselines, the process used for creating relevance judgments, the evaluation measures, and the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Topics</head><p>In Task 2, participating teams were given 87 mathematical formulae, each found in a different question from Task 1 from 2019, and they were asked to find relevant formulae instances from either question or answer posts in the test collection (from 2018 and earlier). The topics for Task 2 were provided in an XML file similar to those of Task 1, in the format shown in Figure <ref type="figure" coords="17,439.19,224.12,3.87,8.80" target="#fig_0">1</ref>. Task 2 topics differ from their corresponding Task 1 topics in three ways:</p><p>1. Topic number: For Task 2, topic ids are in form "B.x" where x is the topic number. There is a correspondence between topic id in tasks 1 and 2. For instance, topic id "B.9" indicates the formula is selected from topic "A.9" in Task 1, and both topics include the same question post (see Figure <ref type="figure" coords="17,447.68,289.11,3.87,8.80" target="#fig_0">1</ref>). 2. Formula_Id: This added field specifies the unique identifier for the query formula instance. There may be other formulae in the Title or Body of the question post, but the query is only the formula instance specified by this Formula_Id. 3. L A T E X: This added field is the L A T E X representation of the query formula instance as found in the question post.</p><p>Because query formulae are drawn form Task 1 question posts, the same L A T E X, SLT and OPT TSV files that were provided for the Task 1 topics can be consulted when SLT or OPT representations for a query formula are needed. Formulae for Task 2 were manually selected using a heuristic approach to stratified sampling over three criteria: complexity, elements, and text dependence. Formulae complexity was labeled low, medium or high by the fourth author. For example, df dx = f (x + 1) is low complexity, n k=0 n k k is medium complexity, and x -</p><formula xml:id="formula_5" coords="17,223.60,460.89,207.77,15.10">x 3 3×3! + x 5 5×5! -x 7 7×7! + • • • = ∞ n=0 (-1) n x (2n+1)</formula><p>(2n+1)×(2n+1)! is high complexity. Mathematical elements such as limit, integral, fraction or matrix were manually noted by the fourth author when present. Text dependence reflected the fourth author's opinion of the degree to which text in the Title and Question fields were likely to yield related search results. For instance, for one Task 2 topic, the query formula is df dx = f (x + 1) whereas the complete question is: "How to solve differential equations of the following form: df dx = f (x + 1) ." When searching for this formula, perhaps the surrounding text could safely be ignored. At most one formula was selected from each Task 1 question topic to produce Task 2 topics. In 12 cases, it was decided that no formula in a question post would be a useful query for Task 2, and thus 12 Task 1 queries have no corresponding Task 2 query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Runs Submitted by Participating Teams</head><p>A total of 11 runs were received for Task 2 from a total of 4 teams, as shown in Table <ref type="table" coords="17,174.77,656.06,3.87,8.80">2</ref>. All were automatic runs. Each run contains at most 1,000 formula instances for each query formula, ranked in decreasing order of system-estimated relevance to that query. For each formula instance in a ranked list, participating teams provided the formula_id and the associated post_id for that formula. Please see the participant papers in the working notes for descriptions of the systems that generated these runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baseline Runs</head><p>We again used Tangent-S <ref type="bibr" coords="18,249.41,216.93,10.51,8.80" target="#b4">[5]</ref> as our baseline. Unlike Task 1, a single formula is specified for each Task 2 query, so no formula selection step was needed. This Tangent-S baseline makes no use of the question text.</p><p>Performance. For the Tangent-S baseline, the minimum retrieval time was 0.238 seconds for topic B.3, and the maximum retrieval time was 30.355 seconds for topic B.51. The average retrieval time for all queries was 3.757 seconds, with a standard deviation of 5.532 seconds. The same system configuration was used as in Task 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Assessment</head><p>Pooling. The retrieved items for Task 2 are formula instances, but pooling was done based on visually distinct formulae, not formula instances (see Figure <ref type="figure" coords="18,134.77,374.73,3.87,8.80">2</ref>). This was done by first clustering all formula instances from all submitted runs to identify visually distinct formulae, and then proceeding down each list until at least one instance of some number of different formulae had been seen. For primary runs and for the baseline run, the pool depth was the rank of the first instance of the 25th visually distinct formula; for secondary runs the pool depth was the rank of the first instance of the 10th visually distinct formulae. Additionally, a pool depth of 1,000 (i.e., all available formulae) was used for any formula for which the associated answer post had been marked as relevant for Task 1. <ref type="foot" coords="18,165.81,468.82,7.94,6.16" target="#foot_10">16</ref> This was the only way in which the post ids for answer posts was used.</p><p>Clustering visually distinct formulae instances was performed using the SLT representation when possible, and the L A T E X representation otherwise. We first converted the Presentation MathML representation to a string representation using Tangent-S, which performed a depth-first traversal of the SLT, with each SLT node generating a single character of the SLT string. Formula instances with identical SLT strings were considered to be the same formula; note that this ignores differences in font. For formula instances with no Tangent-S SLT string available, we removed the white space from their L A T E X strings and grouped formula instances with identical strings. This process is simple and appears to have been reasonably robust for our purposes, but it is possible that some visually identical formula instances were not captured due to LaTeXML conversion failures, or where different L A T E X string produce the same formula (e.g., if subscripts and superscripts appear in a different order).</p><p>Assessment was done on formula instances, so for each formula we selected at most five instances to assess. We selected the 5 instances that were contributed to the pools by the largest number of runs, breaking ties randomly. Out of 5,843 visually distinct formulae that were assessed, 93 (1.6%) had instances in more than 5 pooled posts.</p><p>Relevance definition. The relevance judgment task was defined for assessors as follows: for a formula query, if a search engine retrieved one or more instances of this retrieved formula, would that have been expected to be useful for the task that the searcher was attempting to accomplish?</p><p>Assessors were presented with formula instances, and asked to decide their relevance by considering whether retrieving either that instance or some other instance of that formula would have been useful, assigning each formula instance in the judgment pool one of four scores as defined in Table <ref type="table" coords="19,394.78,263.61,3.87,8.80" target="#tab_1">4</ref>.</p><p>For example, if the formula query was 1 n 2+cos n , and the formula instance to be judged is</p><formula xml:id="formula_6" coords="19,212.51,285.22,31.25,14.56">∞ n-1 1 n 2 ,</formula><p>the assessors would decide whether finding the second formula rather than the first would be expected to yield good results. To do this, they would consider the content of the question post containing the query (and, optionally, the thread containing that question post) in order to understand the searcher's actual information need. Thus the question post fills a role akin to Borlund's simulated work task <ref type="bibr" coords="19,269.78,347.91,9.96,8.80" target="#b2">[3]</ref>, although in this case the title, body and tags from the question post are included in the topic and thus can optionally be used by the retrieval system. The assessor can also consult the post containing a retrieved formula instance (which may be another question post or an answer post), along with the associated thread, to see if in that case the formula instance would indeed have been a useful basis for a search. Note, however, that the assessment task is not to determine whether the specific post containing the retrieved formula instance is useful, but rather to use that context as a basis for estimating the degree to which useful content would likely be found if this or other instances of the retrieved formula were returned by a search engine.</p><p>We then defined the relevance score for a formula to be the maximum relevance score for any judged instance of that formula. This relevance definition essentially asks "if instances of this formula were returned, would we reasonably expect some of those instances to be useful?" This definition of relevance might be used by system developers in several ways. One possibility is using Task 2 relevance judgments to train a formula matching component for use in a Task 1 system. A second possibility is using these relevance judgments to train and evaluate a system for interactively suggesting alternative formulae to users. <ref type="foot" coords="19,463.09,550.20,7.94,6.16" target="#foot_11">17</ref>Assessment System. As in Task 1, we used Turkle to build the assessment system. As shown in Figure <ref type="figure" coords="19,257.17,576.27,4.98,8.80" target="#fig_6">6</ref> (at the end of this document), there are two main panels. In the left panel, the question is shown as in Task 1, but now with the formula query highlighted in yellow. In the right panel, up to five retrieval posts (question posts or answer posts) containing instances of the same retrieved formula are displayed, with the retrieved formula instance highlighted in each case. For example, the formula ∞ n=1 a n shown in Figure <ref type="figure" coords="20,390.27,118.93,4.98,8.80" target="#fig_6">6</ref> was retrieved both in an answer post (shown first) and in a question post (shown second). As in Task 1, buttons are provided for the assessor to record their judgment; unlike Task 1, judgments for each instance of the same retrieved formula (up to 5) are recorded separately, and later used to produce a single (max) score for each visually distinct formula.</p><p>Assessor training. After some initial work on assessment for Task 1, 3 assessors were reassigned to to perform relevance judgements for Task 2, with the remaining 5 continuing to do relevance judgments for Task 1. Two rounds of training were performed.</p><p>In the first training round, the assessors were familiarized with the task. To illustrate how formula search might be used, we interactively demonstrated formula suggestion in MathDeck <ref type="bibr" coords="20,281.43,271.62,15.49,8.80" target="#b15">[16]</ref> and the formula search capability of Ap-proach0 <ref type="bibr" coords="20,171.30,283.57,14.61,8.80" target="#b22">[23]</ref>. Then the task was defined using examples, showing a formula query with some retrieved results, talking through the relevance definitions and how to apply those definitions in specific cases. During the training session, the assessors saw different example results for topics and discussed their relevance based on criteria defined for them with the organizers. They also received feedback from the third author, an expert MSE user. To prepare the judgment pools used for this purpose, we pooled actual submissions from participating teams, but only to depth 10 (i.e., 10 different formulae) for primary runs and the baseline run, and 5 different formulae for alternate runs. The queries used for this initial assessor training were omitted from the final Task 2 query set on which systems were evaluated because they were not judged on full-sized pools.</p><p>All three assessors were then assigned two complete Task 2 pools (for topics B.46 and B.98) to independently assess; these topics were not removed from the collection. After creating relevance judgments for these full-sized pools, the assessors and organizers met by Zoom to discuss and resolve disagreements. The assessors used this opportunity to refine their understanding of the relevance criteria, and the application of those criteria to specific cases. Annotator agreement was found to be fairly good (kappa=0.83). An adjudicated judgment was recorded for each disagreement, and used in the final relevance judgment sets for these two topics.</p><p>The assessors were then each assigned complete pools to judge for four topics, one of which was also assessed independently by a second assessor. The average kappa on the three dual-assessed topics was 0.47. After discussion between the organizers and the assessors, adjudicated disagreements were recorded and used in the final relevance judgments. The assessors then performed the remaining assessments for Task 2 independently. Post Assessment. As we did for Task 1, after assessment for Task 2 was completed, each of the three assessors were given two topics, one completed by each of the other two annotators. Figure <ref type="figure" coords="21,315.79,375.50,4.98,8.80" target="#fig_3">5</ref> shows the Cohen's kappa coefficient values for each assessor and total agreement over all of them. A kappa of 0.30 was achieved on the four-way assessment task, and with H+M binarization the average kappa value was 0.48. Interestingly, the post-assessment agreement between assessors is about the same as Task 1 for four-way agreement (0.29), but H+M binarized agreement is almost 10% higher than Task 1 (0.39). When asked, assessors working on Task 2 (who had all been previously trained on Task 1) reported finding Task 2 assessment to be easier. We note that there were fewer assessors working on Task 2 than Task 1 (3 vs. 5 assessors).</p><p>Additional Training Topics. After the official assessment, to increase the size of the available dataset, an additional 27 topics were annotated. These are available in the ARQMath dataset, and can be used for training models. As a result, 74 topics have been published for Task 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Evaluation Measures</head><p>As for Task 1, the primary evaluation measure for Task 2 is nDCG , and MAP and P@10 were also computed. Participants submitted ranked lists of formula instances, but we computed these measures over visually distinct formulae. To do this, we replaced each formula instance with its associated visually distinct formula, then deduplicated from the top of the list downward to obtain a ranked list of visually distinct formulae, and then computed the evaluation measures. As explained above, the relevance score for each visually distinct formula was computed as the maximum score over each assessed instance of that formula.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Results</head><p>Table <ref type="table" coords="22,162.50,140.73,12.45,8.80" target="#tab_3">A2</ref> in the appendix shows the results, with the baseline run shown first, and then teams and their systems ranked by nDCG . No team did better than the baseline system as measured by nDCG or MAP , although the DPRL team did achieve the highest score for P@10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The ARQMath lab is the first shared-task evaluation exercise to explore Community Question Answering (CQA) for mathematical questions. Additionally, the lab introduced a new formula retrieval task in which both the query and retrieved formulae are considered within the context of their question or answer posts, and evaluation is performed using visually distinct formulas, rather than all formulas returned in a run. For both tasks, we used posts and associated data from the Math Stack Exchange (MSE) CQA forum.</p><p>To reduce assessor effort and obtain a better understanding of the relationship between mathematical CQA and formula search, the formulae used as formula search topics were selected from the Task 1 (CQA) question topics. This allowed us to increase coverage for the formula retrieval task by using relevant posts found in the CQA evaluations as candidates for assessment. To enrich the judgments pools for the CQA task, we added answer posts from the original topic question thread and threads identified as duplicate questions by the MSE moderators.</p><p>In total, 6 teams submitted 29 runs: 5 teams submitted 18 runs for the CQA task (Task 1), and 4 teams submitted 11 runs for the formula retrieval task (Task 2). We thus judge the first year of the ARQMath lab to be successful. Each of these teams had some prior experience with math-aware information retrieval; in future editions of the lab we hope to further broaden participation, particularly from the larger IR and NLP communities.</p><p>Our assessment effort was substantial: 8 paid upper-year or recently graduated undergraduate math students worked with us for over a month, and underwent training in multiple phases. Our training procedure provided our assessors with an opportunity to provide feedback on relevance definitions, the assessment interface, and best practices for assessment. In going through this process, we learned that 1) the CQA task is much harder to assess than the formula retrieval task, as identifying non-relevant answers requires more careful study than identifying non-relevant formulae, 2) the breadth of mathematical expertise needed for the CQA task is very high; this led us to having assessors indicate which questions they wished to assess and us assigning topics according to those preferences (leaving the 10 topics that no assessor requested unassessed), and 3) having an expert mathematician (in this case, a math Professor) involved was essential for task design, clarifying relevance definitions, and improving assessor consistency.</p><p>To facilitate comparison with systems using ARQMath for benchmarking in the future, and to make use of our graded relevance assessments, we chose nDCG <ref type="bibr" coords="23,169.23,118.93,15.49,8.80" target="#b17">[18]</ref> as the primary measure for comparing systems. Additional metrics (MAP and Precision at 10) are also reported to provide a more complete picture of system differences.</p><p>Overall, we found that systems submitted to the first ARQMath lab generally approached the task in similar ways, using both text and formulae for Task 1, and (with two exceptions) operating fully automatically. In future editions of the task, we hope to see a greater diversity of goals, with, for example, systems optimized for specific types of formulae, or systems pushing the state of the art for the use of text queries to find math. We might also consider supporting a broad range of more specialized investigations by, for example, creating subsets of the collection that are designed specifically to formula variants such as simplified forms or forms using notation conventions from different disciplines. Our present collection includes user-generated tags, but we might also consider defining a well-defined tag set to indicate which of these types of results are desired. Instructions: Select the Relevance of the highlighted formula within each post to the query formula (shown at bottom-left).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submit</head><p>How to compute this combinatoric sum?  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix: Evaluation Results</head><p>Table <ref type="table" coords="26,163.47,163.29,10.84,7.93">A1</ref>. Task 1 (CQA) results, averaged over 77 topics. P indicates a primary run, M indicates a manual run, and ( ) indicates a baseline pooled at the primary run depth. For Precision@10 and MAP, H+M binarization was used. The best baseline results are in parentheses. * indicates that one baseline did not contribute to judgment pools. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run Type Evaluation Measures</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,136.16,170.35,107.35,5.10;2,371.39,170.35,102.74,5.10;2,136.16,181.91,33.85,5.10;2,136.16,188.44,232.44,6.16;2,136.16,196.41,232.44,6.16;2,136.16,204.38,144.47,6.16;2,240.07,223.88,11.06,6.12;2,234.07,229.73,23.07,4.37;2,258.49,223.88,4.92,6.12;2,265.11,219.82,3.39,4.35;2,264.61,224.39,4.39,4.37;2,382.13,181.91,58.37,5.10;2,388.29,198.12,34.97,6.12;2,406.20,203.96,23.07,4.37;2,430.62,198.12,4.92,6.12;2,437.24,194.06,3.39,4.35;2,436.75,198.62,4.39,4.37"><head>Task 1 :</head><label>1</label><figDesc>Question Answering Task 2: Formula Retrieval Question I have spent the better part of this day trying to show from first principles that this sequence tends to 1. Could anyone give me an idea of how I can approach this problem?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="13,134.77,266.92,345.83,8.97;13,134.77,277.88,345.82,8.97;13,134.77,288.84,84.56,8.97"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Inter-annotator agreement (Fleiss' kappa) over 8 assessors during Task 1 training (8 topics per round); four-way classification (gray) and two-way (H+M binarized) classification (black).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="14,134.77,244.93,345.82,8.97;14,134.77,255.89,345.83,8.97;14,134.77,266.85,345.82,8.97;14,134.77,277.81,345.83,8.97;14,134.77,288.76,345.83,8.97;14,134.77,299.72,69.63,8.97"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Inter-annotator agreement (Cohen's kappa) over 5 assessors after Task 1 assessment was completed. Each assessor evaluated two topics that had been scored by two of the other assessors. Shown are results for four-way classification (gray) and two-way (H+M binarized) classification (black). Results are provided for each individual Task 1 assessor (as average kappa score), along with the average kappa values over all assessors at right ('Total').</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="21,134.77,248.13,345.82,8.97;21,134.77,259.09,345.82,8.97;21,134.77,270.04,345.82,8.97;21,134.77,281.00,345.83,8.97;21,134.77,291.96,318.34,8.97"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Inter-annotator agreement (Cohen's kappa) over 3 assessors after official Task 2 assessment. Each annotator evaluated two tasks completed by the other two annotators. Shown are four-way classification (gray) and two-way (H+M binarized) classification (black). Results are provided for each individual Task 1 assessor (as average kappa score), along with the average kappa values over all assessors at right ('Total').</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="25,140.62,174.22,16.75,5.19;25,140.62,182.50,35.44,5.19;25,140.62,220.11,45.45,5.19;25,205.26,220.11,158.64,5.19;25,140.62,228.38,128.34,5.19;25,382.80,159.74,48.40,7.28;25,382.79,173.82,13.42,4.18;25,382.79,189.41,16.69,4.18;25,382.79,196.01,2.52,4.18;25,406.48,196.01,31.25,4.18;25,382.79,207.28,8.74,4.18;25,459.94,207.28,38.91,4.18;25,526.52,219.08,2.84,4.18;25,382.79,230.80,69.68,4.18;25,382.79,333.04,13.42,4.18;25,382.79,339.60,12.94,5.19;25,417.03,339.60,90.02,5.19;25,535.78,339.60,52.29,5.19;25,614.68,339.60,37.10,5.19;25,382.79,356.64,19.77,4.18;25,382.79,363.25,5.80,4.18;25,409.83,363.25,61.63,4.18;25,488.65,363.25,72.20,4.18;25,583.67,363.25,41.91,4.18;25,646.72,363.25,7.77,4.18;25,382.79,370.11,169.56,4.18;25,384.91,246.91,9.27,4.18;25,384.91,257.74,16.12,4.18;25,384.91,268.58,7.76,4.18;25,384.91,279.32,25.00,4.18;25,384.91,307.76,27.04,4.18;25,384.91,318.60,25.02,4.18;25,384.91,386.23,9.27,4.18;25,384.91,397.06,16.12,4.18;25,384.91,407.89,7.76,4.18;25,384.91,418.72,25.00,4.18;25,384.91,447.08,27.04,4.18;25,384.91,457.92,25.02,4.18"><head>ThreadI</head><label></label><figDesc>have the sum I know the result is but I don't know how you get there. How does one even begin to simplify a sum like this that has binomial coefficients. Let be a series such that for each , and is decreasing. Suppose that converges. Prove that also converges. I try to prove by using definition but I got nowhere . Can anyone guide me ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="25,129.08,494.79,547.09,8.97;25,129.08,505.75,547.09,8.97;25,129.08,516.71,390.65,8.97;25,423.66,389.58,104.09,67.46"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Turkle Assessment Interface. Shown are hits for Formula Retrieval (Task 2). In the left panel, the formula query is highlighted. In the right panel, one answer post and one question post containing the same retrieved formula are shown. For Task 1, a similar interface was used, but without formula highlighting, and just one returned answer post viewed at a time.</figDesc><graphic coords="25,423.66,389.58,104.09,67.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,167.43,115.30,280.49,92.72"><head>Table 3 .</head><label>3</label><figDesc>Retrieval Time in Seconds for Task 1 Baseline Systems.</figDesc><table coords="10,167.43,141.60,280.49,66.42"><row><cell></cell><cell cols="3">Run Time (seconds)</cell></row><row><cell>System</cell><cell>Min (Topic)</cell><cell>Max (Topic)</cell><cell>(Avg, StDev)</cell></row><row><cell>TF-IDF (Terrier)</cell><cell>0.316 (A.42)</cell><cell>1.278 (A.1)</cell><cell>(0.733, 0.168)</cell></row><row><cell>Tangent-S</cell><cell cols="3">0.152 (A.72) 160.436 (A.60) (6.002, 18.496)</cell></row><row><cell cols="4">TF-IDF + Tangent-S 0.795 (A.72) 161.166 (A.60) (6.740, 18.483)</cell></row><row><cell>Approach0</cell><cell>0.007 (A.3)</cell><cell cols="2">91.719 (A.5) (17.743, 18.789)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,136.16,115.30,341.01,164.50"><head>Table 4 .</head><label>4</label><figDesc>Relevance Scores, Ratings, and Definitions for Tasks 1 and 2.</figDesc><table coords="11,136.16,140.53,341.01,139.27"><row><cell></cell><cell></cell><cell>Task 1: Question Answering</cell></row><row><cell cols="2">Score Rating</cell><cell>Definition</cell></row><row><cell>3</cell><cell>High</cell><cell>Sufficient to answer the complete question on its own</cell></row><row><cell>2</cell><cell>Medium</cell><cell>Provides some path towards the solution. This path might come from clar-</cell></row><row><cell></cell><cell></cell><cell>ifying the question, or identifying steps towards a solution</cell></row><row><cell>1</cell><cell>Low</cell><cell>Provides information that could be useful for finding or interpreting an</cell></row><row><cell></cell><cell></cell><cell>answer, or interpreting the question</cell></row><row><cell>0</cell><cell cols="2">Not Relevant Provides no information pertinent to the question or its answers. A post</cell></row><row><cell></cell><cell></cell><cell>that restates the question without providing any new information is con-</cell></row><row><cell></cell><cell></cell><cell>sidered non-relevant</cell></row><row><cell></cell><cell></cell><cell>Task 2: Formula Retrieval</cell></row><row><cell cols="2">Score Rating</cell><cell>Definition</cell></row><row><cell>3</cell><cell>High</cell><cell>Just as good as finding an exact match to the query formula would be</cell></row><row><cell>2</cell><cell>Medium</cell><cell>Useful but not as good as the original formula would be</cell></row><row><cell>1</cell><cell>Low</cell><cell>There is some chance of finding something useful</cell></row><row><cell>0</cell><cell cols="2">Not Relevant Not expected to be useful</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="26,160.41,243.85,294.20,326.83"><head>Table A2 .</head><label>A2</label><figDesc>Task 2 (Formula Retrieval) results, averaged over 45 topics and computed over deduplicated ranked lists of visually distinct formulae. P indicates a primary run, and ( ) shows the baseline pooled at the primary run depth. For MAP and P@10, relevance was thresholded H+M binarization. All runs were automatic. Baseline results are in parentheses.</figDesc><table coords="26,160.41,243.85,294.20,326.83"><row><cell>Run</cell><cell>Data P</cell><cell>M</cell><cell cols="3">nDCG MAP P@10</cell></row><row><cell>Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Linked MSE posts</cell><cell>n/a ( )</cell><cell></cell><cell cols="3">(0.279) (0.194) (0.384)</cell></row><row><cell>Approach0 *</cell><cell>Both</cell><cell></cell><cell>0.250</cell><cell>0.099</cell><cell>0.062</cell></row><row><cell>TF-IDF + Tangent-S</cell><cell>Both ( )</cell><cell></cell><cell>0.248</cell><cell>0.047</cell><cell>0.073</cell></row><row><cell>TF-IDF</cell><cell>Text ( )</cell><cell></cell><cell>0.204</cell><cell>0.049</cell><cell>0.073</cell></row><row><cell>Tangent-S</cell><cell>Math ( )</cell><cell></cell><cell>0.158</cell><cell>0.033</cell><cell>0.051</cell></row><row><cell>MathDowsers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">alpha05noReRank alpha02 alpha05translated Run alpha05 Baseline alpha10 Tangent-S DPRL PSU PSU1 TangentCFTED PSU2 TangentCFT PSU3 TangentCFT+ MIRMU MIRMU Ensemble SCM SCM Formula2Vec MIaS Ensemble Formula2Vec Formula2Vec CompuBERT SCM NLP_NITS DPRL DPRL4 formulaembedding Math Both Both Both Data P Both Both Math ( ) ( 0.506 ) (0.288) ( 0.478 ) 0.345 0.139 0.161 0.301 0.069 Evaluation Measures 0.075 0.298 0.074 nDCG MAP P@10 0.079 0.278 0.063 0.073 0.267 0.063 0.079 Both 0.263 0.082 Math 0.420 0.258 0.502 0.116 Both 0.228 0.054 Math 0.392 0.219 0.396 0.055 Both 0.211 0.046 Both 0.135 0.047 0.207 0.026 Both 0.238 0.064 Math 0.119 0.056 0.058 0.135 Both 0.224 0.066 Math 0.108 0.047 0.076 0.110 Both 0.155 0.039 Math 0.100 0.033 0.051 0.052 Both 0.050 0.007 Math 0.077 0.028 0.044 0.020 Both 0.009 0.000 Math 0.059 0.018 0.049 0.001 Both 0.060 0.015 0.020 0.026 0.005 0.042</cell></row><row><cell>DPRL2</cell><cell>Both</cell><cell></cell><cell>0.054</cell><cell>0.015</cell><cell>0.029</cell></row><row><cell>DPRL1</cell><cell>Both</cell><cell></cell><cell>0.051</cell><cell>0.015</cell><cell>0.026</cell></row><row><cell>DPRL3</cell><cell>Both</cell><cell></cell><cell>0.036</cell><cell>0.007</cell><cell>0.016</cell></row><row><cell>zbMATH</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>zbMATH</cell><cell>Both</cell><cell></cell><cell>0.042</cell><cell>0.022</cell><cell>0.027</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,657.44,169.42,7.47"><p>https://www.cs.rit.edu/~dprl/ARQMath</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="4,144.73,646.48,108.24,7.47"><p>https://www.w3.org/Math</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="4,144.73,657.44,98.83,7.47"><p>https://dlmf.nist.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="5,144.73,657.44,197.66,7.47"><p>https://archive.org/download/stackexchange</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4" coords="6,144.73,657.44,136.48,7.47"><p>https://dlmf.nist.gov/LaTeXML</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5" coords="7,144.73,635.53,338.85,7.47"><p>https://drive.google.com/drive/folders/1ZPKIWDnhMGRaPNVLi1reQxZWTfH2R4u3</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6" coords="7,144.73,646.48,178.84,7.47"><p>https://github.com/ARQMath/ARQMathCode</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7" coords="7,144.73,656.20,290.58,8.97"><p>Note that participating systems did not have access to this information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_8" coords="15,144.73,612.36,335.87,8.97;15,144.73,623.32,335.87,8.97;15,144.73,634.28,335.86,8.97;15,144.73,645.24,70.51,8.97"><p>Pooling to at least depth 20 ensures that there are no unjudged posts above rank 10 for any primary or secondary submission, and for four of the five baselines. Note, however, that P@10 cannot achieve a value of 1 because some topics have fewer than 10 relevant posts.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_9" coords="15,144.73,657.44,178.84,7.47"><p>https://github.com/usnistgov/trec_eval</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_10" coords="18,144.73,645.24,335.87,8.97;18,144.73,656.20,83.76,8.97"><p>One team submitted incorrect post id's for retrieved formulae; those post id's were not used for pooling.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_11" coords="19,144.73,645.24,335.86,8.97;19,144.73,656.20,116.36,8.97"><p>See, for example, MathDeck<ref type="bibr" coords="19,261.60,645.24,13.51,8.97" target="#b15">[16]</ref>, in which candidate formulae are suggested to the users during formula editing.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. <rs type="person">Wei Zhong</rs> suggested using Math Stack Exchange for benchmarking, made Approach0 available for participants, and provided helpful feedback. <rs type="person">Kenny Davila</rs> helped with the Tangent-S formula search results. We also thank our student assessors from <rs type="institution">RIT</rs>: <rs type="person">Josh Anglum</rs>, <rs type="person">Wiley Dole</rs>, <rs type="person">Kiera Gross</rs>, <rs type="person">Justin Haverlick</rs>, <rs type="person">Riley Kieffer</rs>, <rs type="person">Minyao Li</rs>, <rs type="person">Ken Shultes</rs>, and <rs type="person">Gabriella Wolf</rs>. This material is based upon work supported by the <rs type="funder">National Science Foundation (USA)</rs> under Grant No. <rs type="grantNumber">IIS-1717997</rs> and the <rs type="funder">Alfred P. Sloan Foundation</rs> under Grant No. <rs type="grantNumber">G-2017-9827</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_XdHtuaU">
					<idno type="grant-number">IIS-1717997</idno>
				</org>
				<org type="funding" xml:id="_Ap3PsuM">
					<idno type="grant-number">G-2017-9827</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="23,142.95,425.72,337.64,8.97;23,151.52,436.67,45.42,8.97" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<title level="m" coord="23,299.62,425.72,143.43,8.97;23,463.96,425.72,16.63,8.97;23,151.52,436.67,16.76,8.97">NTCIR-10 math pilot task overview</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>NT-CIR</note>
</biblStruct>

<biblStruct coords="23,142.95,447.68,337.64,8.97;23,151.52,458.64,190.30,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="23,380.52,447.68,100.08,8.97;23,151.52,458.64,33.24,8.97">NTCIR-11 Math-2 task overview</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schubotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,206.06,458.64,27.40,8.97">NTCIR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="88" to="98" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.95,469.65,337.64,8.97;23,151.52,480.61,280.61,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="23,204.04,469.65,276.55,8.97;23,151.52,480.61,115.53,8.97">The IIR evaluation model: a framework for evaluation of interactive information retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Borlund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,274.33,480.61,86.47,8.97">Information Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="8" to="11" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.95,491.62,337.64,8.97;23,151.52,502.58,329.07,8.97;23,151.52,513.53,244.20,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="23,268.39,491.62,194.50,8.97">Retrieval evaluation with incomplete information</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,151.52,502.58,329.07,8.97;23,151.52,513.53,169.69,8.97">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.95,524.54,337.63,8.97;23,151.52,535.50,329.08,8.97;23,151.52,546.46,329.07,8.97;23,151.52,557.42,47.09,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="23,256.75,524.54,223.83,8.97;23,151.52,535.50,115.72,8.97">Layout and semantics: Combining representations for mathematical formula search</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,288.84,535.50,191.75,8.97;23,151.52,546.46,284.40,8.97">Proceedings of the 40th International ACM SI-GIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SI-GIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1165" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.95,568.43,337.64,8.97;23,151.52,579.39,306.11,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="23,240.40,568.43,192.22,8.97">A survey on retrieval of mathematical knowledge</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Guidi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Coen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="23,452.96,568.43,27.64,8.97;23,151.52,579.39,141.38,8.97">CICM. Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">9150</biblScope>
			<biblScope unit="page" from="296" to="315" />
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.95,590.40,337.63,8.97;23,151.52,601.35,329.08,8.97;23,151.52,612.31,306.88,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="23,244.09,601.35,198.13,8.97">SemEval-2019 Task 10: Math Question Answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Petrescu-Prahova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,463.04,601.35,17.55,8.97;23,151.52,612.31,278.21,8.97">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>the 13th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.95,623.32,337.64,8.97;23,151.52,634.28,329.07,8.97;23,151.52,645.24,329.08,8.97;23,151.52,656.20,62.49,8.97" xml:id="b7">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="23,395.87,623.32,84.72,8.97;23,151.52,634.28,231.92,8.97">Intelligent Computer Mathematics -12th International Conference, CICM 2019</title>
		<title level="s" coord="23,234.08,645.24,196.08,8.97">Proceedings, Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Kaliszyk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Brady</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Kohlhase</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Coen</surname></persName>
		</editor>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">July 8-12, 2019. 2019</date>
			<biblScope unit="volume">11617</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.95,119.07,337.64,8.97;24,151.52,130.03,329.08,8.97;24,151.52,140.99,329.07,8.97;24,151.52,151.95,202.27,8.97" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="24,408.79,119.07,71.80,8.97;24,151.52,130.03,329.08,8.97;24,151.52,140.99,170.53,8.97">Derivation of new readability formulas (automated readability index, fog count and Flesch reading ease formula) for Navy enlisted personnel</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P</forename><surname>Fishburne</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">S</forename><surname>Chissom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
		<respStmt>
			<orgName>Naval Technical Training Command Millington TN Research Branch</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct coords="24,142.61,162.91,337.98,8.97;24,151.52,173.87,329.07,8.97;24,151.52,184.83,198.69,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="24,375.14,162.91,105.45,8.97;24,151.52,173.87,113.33,8.97">Learning to automatically solve algebra word problems</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,287.02,173.87,193.57,8.97;24,151.52,184.83,170.03,8.97">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,195.79,337.98,8.97;24,151.52,206.74,329.07,8.97;24,151.52,217.70,329.07,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="24,352.12,195.79,128.48,8.97;24,151.52,206.74,260.88,8.97">Program induction by rationale generation: Learning to solve and explain algebraic word problems</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,432.53,206.74,48.06,8.97;24,151.52,217.70,301.42,8.97">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,228.66,337.98,8.97;24,151.52,239.62,329.08,8.97;24,151.52,250.58,115.60,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="24,365.26,228.66,115.33,8.97;24,151.52,239.62,182.66,8.97">Finding old answers to new math questions:the ARQMath lab at CLEF</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,379.13,239.62,101.47,8.97;24,151.52,250.58,86.93,8.97">European Conference on Information Retrieval</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,261.54,337.98,8.97;24,151.52,272.50,329.08,8.97;24,151.52,283.46,329.08,8.97;24,151.52,294.42,107.10,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="24,444.76,261.54,35.83,8.97;24,151.52,272.50,227.55,8.97">Tangent-CFT: An embedding model for mathematical formulas</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,403.24,272.50,77.36,8.97;24,151.52,283.46,329.08,8.97;24,151.52,294.42,31.91,8.97">Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval (ICTIR)</title>
		<meeting>the 2019 ACM SIGIR International Conference on Theory of Information Retrieval (ICTIR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,305.38,337.98,8.97;24,151.52,316.33,232.43,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="24,314.11,305.38,166.48,8.97;24,151.52,316.33,32.81,8.97">Characterizing searches for mathematical concepts</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,205.57,316.33,149.72,8.97">Joint Conference on Digital Libraries</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,327.29,337.98,8.97;24,151.52,338.25,223.95,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="24,242.23,327.29,238.36,8.97;24,151.52,338.25,25.76,8.97">The logic theory machine-a complex information processing system</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,184.63,338.25,162.17,8.97">IRE Transactions on information theory</title>
		<imprint>
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,349.21,337.98,8.97;24,151.52,360.17,329.07,8.97;24,151.52,371.13,310.83,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="24,429.65,349.21,50.94,8.97;24,151.52,360.17,310.66,8.97">MathSeer: A math-aware search interface with intuitive formula editing, reuse, and lookup</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Nishizawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dmello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,151.52,371.13,187.65,8.97">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="470" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,382.09,337.98,8.97;24,151.52,393.05,329.07,8.97;24,151.52,404.01,116.23,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="24,453.14,382.09,27.45,8.97;24,151.52,393.05,118.37,8.97">Terrier information retrieval platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,290.45,393.05,186.26,8.97">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="517" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,414.96,337.98,8.97;24,151.52,425.92,317.94,8.97" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="24,237.87,414.96,242.72,8.97;24,151.52,425.92,131.80,8.97">On information retrieval metrics designed for evaluation with incomplete relevance assessments</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,290.53,425.92,86.93,8.97">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="447" to="470" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,436.88,337.98,8.97;24,151.52,447.84,329.07,8.97;24,151.52,458.80,50.41,8.97" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="24,346.80,436.88,133.80,8.97;24,151.52,447.84,225.70,8.97">Challenges of mathematical information retrieval in the NTCIR-11 Math Wikipedia Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schubotz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Youssef</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Markl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">S</forename><surname>Cohl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,398.59,447.84,23.62,8.97">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="951" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,469.76,337.97,8.97;24,151.52,480.72,168.21,8.97" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="24,437.98,469.76,42.61,8.97;24,151.52,480.72,87.93,8.97">NTCIR-12 MathIR task overview</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Topic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,260.74,480.72,30.32,8.97">NTCIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,491.68,337.97,8.97;24,151.52,502.64,329.08,8.97;24,151.52,513.59,42.48,8.97" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="24,257.93,491.68,218.70,8.97">Recognition and retrieval of mathematical expressions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blostein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,151.52,502.64,281.29,8.97">International Journal on Document Analysis and Recognition (IJDAR)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="331" to="357" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,524.55,337.97,8.97;24,151.52,535.51,329.08,8.97;24,151.52,546.47,197.11,8.97" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="24,378.05,524.55,102.53,8.97;24,151.52,535.51,150.90,8.97">Accelerating substructure similarity search for formula retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,324.10,535.51,37.34,8.97">ECIR (1)</title>
		<title level="s" coord="24,368.59,535.51,112.01,8.97;24,151.52,546.47,27.77,8.97">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12035</biblScope>
			<biblScope unit="page" from="714" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,557.43,337.98,8.97;24,151.52,568.39,329.07,8.97;24,151.52,579.35,100.37,8.97" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="24,253.96,557.43,226.63,8.97;24,151.52,568.39,103.36,8.97">Structural similarity search for formulas using leaf-root paths in operator subtrees</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,275.17,568.39,186.07,8.97">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="116" to="129" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
