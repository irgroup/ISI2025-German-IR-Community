<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.79,115.96,325.76,12.62;1,185.35,133.89,244.66,12.62">CLRG ChemNER: A Chemical Named Entity Recognizer @ ChEMU CLEF 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,386.77,171.56,54.52,8.74"><forename type="first">Lalitha</forename><surname>Devi</surname></persName>
							<email>sobha@au-kbc.org</email>
							<affiliation key="aff0">
								<orgName type="department">AU-KBC Research Centre</orgName>
								<orgName type="laboratory">Computational Linguistics Research Group</orgName>
								<orgName type="institution">MIT Campus of Anna University</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.79,115.96,325.76,12.62;1,185.35,133.89,244.66,12.62">CLRG ChemNER: A Chemical Named Entity Recognizer @ ChEMU CLEF 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A50ACBD364B29B1DC24466CC82918C86</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Chemical named entity recognition</term>
					<term>Artificial Neural Networks</term>
					<term>Conditional random fields</term>
					<term>â€¢ Patent Documents</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our system developed for ChEMU @ CLEF Cheminformatics Elsevier Melbourne University lab, Named Entity Recognition (NER) task for identifying chemical compounds as well as their types in context, i.e., to assign the label of a chemical compound according to the role which the compound plays within a chemical reaction from patent documents. We have presented two systems which use Conditional random fields (CRFs) algorithms and Artificial Neural Networks (ANN). In this work we used feature set that includes linguistic, orthographical and lexical clue features. In the development of systems, we have used only the training data provided by the track organizers and no other external resources or embedding models were used. We obtained an F-score of 0.6640 using CRFs and F-Score of 0.3764 using ANN on the test data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>CLEF 2020 ChEMU NER task aims to automatically identify chemical compounds and their specific types, i.e. to assign the label of a chemical compound according to the role it plays in a chemical reaction. In addition to chemical compounds, the task also requires identification of the temperature and the reaction time at which the chemical reaction was carried out, the yields obtained for the final chemical product and the label of the reaction. The focuses of this task is mainly on information extraction from chemical patents. This is a challenging task as patents are written very differently as compared to scientific literature. When writing scientific papers, authors strive to make their words as clear and straightforward as possible, whereas patent authors often seek to protect their knowledge from being fully disclosed <ref type="bibr" coords="1,300.99,603.52,9.96,8.74" target="#b7">[8]</ref>. Thus the main challenge for Natural Language Processing (NLP) in patent documents arises from its writing style, very long winding complex sentences and listing of chemical compounds. As syntactic deep parsing is difficult for such sentence constructions, for this work we decided to use shallow parsing. The paper describes the work we have done in developing NER systems for this task "ChEMU NER task".</p><p>We pre-processed the data provided by the task organizers to the required format to develop our NER systems. Subsequently, features were extracted and trained for the identification of the entities from the corpus using Machine learning (ML) algorithms. In section 2, we briefly review the recent literature. In the following section 3, features and the method used to develop the language models are described. Results are discussed in section 4. The paper ends with the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Literature Review</head><p>In recent years Deep Learning is flourishing as a well-known ML methodology for NLP applications. By using the multilayer neural architecture it can learn the hidden patterns from the enormous amount of data and handles the complex problems. This section briefly explains the recent research works in the field of NER using Deep Learning. The earlier work on neural network was done by Gallo et.al <ref type="bibr" coords="2,156.68,369.19,10.52,8.74" target="#b1">[2]</ref> to classify named entities in ungrammatical text. Their implementation of Multi-Layer Perceptron (MLP) is called as Sliding Window Neural (SwiN) which was specifically developed for grammatically problematic text where the linguistic features could fail. The Deep Neural Framework was developed by Yao et al. <ref type="bibr" coords="2,159.47,417.01,15.50,8.74" target="#b10">[11]</ref> to identify the biomedical named entities. They have trained the word representation model on PubMed database with the help of skip-gram model. Xia et al. <ref type="bibr" coords="2,182.08,440.92,15.50,8.74" target="#b9">[10]</ref> built a single neural network for identifying multi-level nested entities and non-overlapping NEs. Kuru et al. <ref type="bibr" coords="2,331.51,452.88,10.52,8.74" target="#b3">[4]</ref> used character level representation to identify named entities. They have utilized Bi-LSTMs to predict the tag distribution for each character. Wei et al. <ref type="bibr" coords="2,312.56,476.79,10.52,8.74" target="#b8">[9]</ref> have developed a CRF based neural network for identifying the disease names. Along with word embeddings their system has utilized words, PoS information, chunk information and word shape features. Hong et al. <ref type="bibr" coords="2,221.25,512.66,10.52,8.74" target="#b2">[3]</ref> developed a deep learning architecture for BioNER which is called as DTranNER. It learns the label to label transition using the contextual information. The Unary-Network concentrates on the tag-wise labelling and the pair-wise network predict the transition suitability between labels. Then these networks are plugged into the CRF of the deep learning framework. In the recent past, the models combining word level and character lever representations are being used. These methods concatenate word embeddings with LSTMs (or Bi-LSTMs) over the characters of a word, passing this representation through another sentence-level Bi-LSTM, and predicting the final tags using a final softmax or CRF layer. Lample et al. <ref type="bibr" coords="2,284.06,620.25,10.52,8.74" target="#b5">[6]</ref> introduced this architecture and achieved 85.75%, 81.74%, 90.94%, 78.76% F-scores on Spanish, Dutch, English and German NER dataset respectively from CoNLL 2002 and 2003. Dernoncourt et al. <ref type="bibr" coords="2,134.77,656.12,10.52,8.74" target="#b0">[1]</ref> implemented this model in the Neuro NER toolkit with the main goal of providing easy usability and allowing easy plotting of real time performance and learning statistics of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section we present our systems developed using Condition Random Fields (CRFs) and Artificial Neural Networks (ANN). For our work we use CRF++ tool and Scikit python package. CRF++ <ref type="foot" coords="3,309.66,210.19,3.97,6.12" target="#foot_0">1</ref> tool is an open source implementation of CRFs and is a general purpose tool. Our NER system follows a pipeline architecture, where the data is first pre-processed to required format that is needed to train the system. After training the system the NEs are automatically identified from the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Selection</head><p>Feature selection is an important step in the ML approach for NER. Features play an important role in boosting the performance of the system. Features selected must be informative and relevant. We have used word level features, grammatical features, functional terms features that are detailed below:</p><p>1. Word level features: Word level features include Orthographical features and Morphological features.</p><p>(a) Orthographical features contain Capitalization, combination of digits, symbols and words and Greek words.</p><p>(b) Prefix and suffix of chemical entities is considered as morphological features.</p><p>2.Grammatical features: Grammatical features include word, PoS, chunks and combination of word, PoS and chunk.</p><p>3. Functional term feature: Functional term helps to identify the biological named entities and categorize them to various classes. Example: Alkyl, acid, alkanylene.</p><p>Grammatical features of Part-of-Speech (PoS) and chunk information are obtained using automatic tools. More details about the tools are given in next sub-section. The morphological features are obtained by extracting 'n' last and starting characters of chemical entities. After performing a few experiments, it was identified that n=4 is the optimal one. A Functional terms lexicon was collected from online sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-processing</head><p>The data is pre-processed using a sentence splitter and tokenizer and is converted into column format with entities tagged using the file containing detailed chemical mention annotation (BRAT format annotation file). The sentence splitter and the tokenizer used are rule based engines, developed in-house. In these engines we have done a modification by adding a special processing to accommodate long entity names with more than 200 characters. We have split these long names into two tokens and then combined it as one after PoS tagging and Phrase chunking is completed.</p><p>Then the data is annotated for syntactic information of Part-of-speech (PoS) and Phrase Chunk information (Noun Phrase, Verb phrase) using fnTBL tool <ref type="bibr" coords="4,134.77,190.90,9.96,8.74" target="#b6">[7]</ref>, an open source tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Named Entity Identification</head><p>The features from the pre-processed data are extracted as explained in section 3.1.The data format is similar for both the algorithms. The systems are trained using the same features extracted from the data. Using these models the chemical named entities from the test data were automatically identified. Chemical entity mention in patents requires the detection of the start and end indices corresponding to all chemical entities. Hence we converted the output from the system to the required BRAT format for task submission. The NER language models developed using CRFs, used the features as explained in section 3.1 from training. Using the NER model the NEs are automatically identified from the testing corpus. All the features were extracted from the training corpus provided by the organizers and no other external resources have been used. Similarly, we have followed for the ANN system also. ANN system is described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Artificial Neural Networks (ANN)</head><p>A Multi-Layer Perceptron (MLP) is a feed forward Artificial Neural Network (ANN). The input layer receives the input data in the numerical form, the output layer takes the decision about the input and the hidden layers exists between these two acts as the computational engine.</p><p>The three important steps involved in neural network are 1) each input is multiplied by a weight 2) all the weighted inputs are added together with the bias and 3) the sum is passed through the activation function. The input node accepts the information in a numerical form and depending on the weight and the transfer function, the activation value which is the weighted sum of inputs will be passed to the next node. The activation function is used to monitor the threshold level and convert the unbounded value into a bounded one. Each node in the network runs the activation value and tweak the sum based on its transfer function. The activation function runs through the entire network until it reaches an output node. The traditional systems used sigmoid or the hyperbolic tangent activation function.</p><p>In this work, Multilayer Perceptron (MLP) network is used. MLP is a combination of layers of perceptrons connected together. The first layer's output goes as the input to the next layer until it reaches the output layer. The hidden layer is the layer that exists between the input and output layer Feed forward networks like MLP have two passes, namely forward and backward passes. In the forward pass, propagate forward to get the output and compare it with the real value to get the error. In order to reduce the error multilayer perceptrons propagate backward and adjust the weights. This process of back-propagation is used to adjust the weight and bias relative to the error. This process continues until the estimated output is obtained. ReLu activation function is used in MLP. The feed forward network consists of two motions namely forward and backward pass. The training process comprises of 3 steps, they are forward pass, error calculation and backward pass. In forward pass the input data is multiplied by its weight and added to the bias at each node and passes through the hidden layer to the output layer. The cost function is used to predict the performance of the model, which can be computed as the difference between the predicted and the expected value. Once the loss is calculated we have to back propagate the loss in order to update the weights of each node using the gradient descent function. The weights are being tweaked according to the gradient flow in that direction. The main intention here is to minimize the loss.</p><p>In this work we have used the python package of Scikit-Learn's Multi-Layer Perceptron. The process of converting the input data into numerical feature vectors is called as vectorization and it involves mainly three steps namely tokenization, counting and normalization. The resultant data is called as Bag of words representation. In this form rather than the relative position of the words in the document the input text is represented using the word occurrences. We have used the countvectorizer to represent the data into bag of words format. It converts the text data into the numerical features. The TFIDFVectorizer is used to convert the bag of words into the matrix of TFIDF features. After initiating the size of the hidden layer and determining the activation and optimization the data is given for the training process. The ReLu activation function is used for the hidden layers of the present MLP implementation. The stochastic gradient optimizer Adam is used for the weight optimization.</p><p>The number of layers we used for the hidden layer is 30, the activation function utilized for the hidden layer is ReLu (Rectified Linear unit function), 'adam' stochastic gradient-based optimizer is used as a solver for the weight optimization. 'Alpha' regularization parameter is set to 0.0001. The learning rate schedule used for weight updation is 'constant'. It is the constant learning rate provided by the initial learning rate which helps to control the step-size in updating the weights and the 'learning rateinit' value is set as 0.001. Batch size refers to the number of training examples used in one iteration. The batch size is assigned as mini batches for stochastic optimizers and it is set to 200 by default. The architecture of MLP implementation is shown in figure <ref type="figure" coords="6,377.38,251.31,3.87,8.74" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>The system outputs on the test data were evaluated by the track organizers, precision, recall and F score were calculated. The test results are tabulated in Table <ref type="table" coords="6,162.58,339.12,4.98,8.74" target="#tab_0">1</ref> and<ref type="table" coords="6,191.09,339.12,3.87,8.74" target="#tab_1">2</ref>. Table <ref type="table" coords="6,230.39,339.12,4.98,8.74" target="#tab_0">1</ref> provides the test results of the system developed using CRFs and Table <ref type="table" coords="6,209.82,351.07,4.98,8.74" target="#tab_1">2</ref> presents results of system using ANN. The system based on CRFs had given a very good precision. The recall is low and especially for the entities "YIELD OTHER" and "YIELD PERCENT". This could have been improved by using post processing rules. The results obtained using ANN are lower than the CRFs based system. This clearly shows that the training data size is not sufficient for ANN. The ANN system requires used of external resources such as pre-trained word embeddings and other available annotated resources. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We submitted two systems developed using Machine Learning (ML) techniques, Condition Random Fields (CRFs) and Artificial Neural Networks (ANN). A two stage pre-processing is done on development data 1) the formatting stage, where the sentence splitting, tokenizing and the data conversion to column format and 2) the data annotation stage, where the data is annotated for syntactic information of Part-of-speech (POS) and Phrase Chunk information (Noun Phrase, Verbphrase) are performed. For both POS and Chunk information, fnTBL <ref type="bibr" coords="7,467.31,415.33,9.96,8.74" target="#b6">[7]</ref>, an open source tool is used. We have used the training data provided by the task organizers and have not used any external resources or pre-trained language models. The language models are developed using CRFs and ANN. The CRF++ tool is used for developing the CRF model. The ANN application uses the Scikit python package. ANN uses a Multilayer Perceptron (MLP). ReLu activation function is used in MLP. The stochastic gradient optimizer Adamis used for the weight optimization. It can adjust and calculate the learning rates for different parameters at each node. We obtained an F-score of 0.6640 using CRFs and F-Score of 0.3764 using ANN for the test data. It can be observed from the results CRFs performed better for the given training data. This shows ANN's require more training data or pre-trained models. A better solution can be arrived at by combining ANN and CRFs. In future we would like to combine ANN and CRFs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,216.65,313.77,182.05,7.89;5,134.77,115.84,345.83,183.16"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. ANN Architecture -of MLP Network</figDesc><graphic coords="5,134.77,115.84,345.83,183.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,203.98,384.19,207.41,159.97"><head>Table 1 .</head><label>1</label><figDesc>Test Results of CRFs based NER system</figDesc><table coords="6,203.98,414.95,207.41,129.21"><row><cell>NE Label</cell><cell cols="2">Precision Recall F1 Score</cell></row><row><cell>EXAMPLE LABEL</cell><cell>0.9732</cell><cell>0.4155 0.5823</cell></row><row><cell cols="2">OTHER COMPOUND 0.9378</cell><cell>0.5656 0.7057</cell></row><row><cell cols="2">REACTION PRODUCT 0.8600</cell><cell>0.4118 0.5569</cell></row><row><cell cols="2">REAGENT CATALYST 0.8214</cell><cell>0.7495 0.7838</cell></row><row><cell>SOLVENT</cell><cell>0.8776</cell><cell>0.7066 0.7828</cell></row><row><cell cols="2">STARTING MATERIAL 0.7192</cell><cell>0.7862 0.7512</cell></row><row><cell>TEMPERATURE</cell><cell>0.9802</cell><cell>0.7288 0.8360</cell></row><row><cell>TIME</cell><cell>0.9954</cell><cell>0.4779 0.6457</cell></row><row><cell>YIELD OTHER</cell><cell>0.9315</cell><cell>0.1545 0.2651</cell></row><row><cell>YIELD PERCENT</cell><cell>0.8571</cell><cell>0.0154 0.0303</cell></row><row><cell>Average</cell><cell>0.8793</cell><cell>0.5334 0.6640</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,203.98,115.91,207.41,159.97"><head>Table 2 .</head><label>2</label><figDesc>Test Results of ANN based NER system</figDesc><table coords="7,203.98,146.67,207.41,129.21"><row><cell>NE Label</cell><cell cols="2">Precision Recall F1 Score</cell></row><row><cell>EXAMPLE LABEL</cell><cell>0.5455</cell><cell>0.0172 0.0333</cell></row><row><cell cols="2">OTHER COMPOUND 0.5977</cell><cell>0.2430 0.3455</cell></row><row><cell cols="2">REACTION PRODUCT 0.5755</cell><cell>0.2573 0.3556</cell></row><row><cell cols="2">REAGENT CATALYST 0.8620</cell><cell>0.5521 0.6731</cell></row><row><cell>SOLVENT</cell><cell>0.8958</cell><cell>0.4066 0.5593</cell></row><row><cell cols="2">STARTING MATERIAL 0.6486</cell><cell>0.4480 0.5299</cell></row><row><cell>TEMPERATURE</cell><cell>0.8239</cell><cell>0.2369 0.3680</cell></row><row><cell>TIME</cell><cell>0.9504</cell><cell>0.2544 0.4014</cell></row><row><cell>YIELD OTHER</cell><cell>0.1429</cell><cell>0.0114 0.0211</cell></row><row><cell>YIELD PERCENT</cell><cell>0.1429</cell><cell>0.0334 0.0542</cell></row><row><cell>Average</cell><cell>0.6686</cell><cell>0.2619 0.3764</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,656.80,132.67,7.86"><p>https://taku910.github.io/crfpp/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,634.88,342.24,7.86;7,146.91,645.84,333.68,7.86;7,146.91,656.80,97.80,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,380.10,634.88,100.49,7.86;7,146.91,645.84,254.47,7.86">Neuroner: an easy-to-use program for named-entity recognition based on neural networks</title>
		<author>
			<persName coords=""><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ji</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.05487</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,138.35,119.67,342.24,7.86;8,146.91,130.63,333.68,7.86;8,146.91,141.59,142.81,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,342.61,119.67,137.98,7.86;8,146.91,130.63,56.67,7.86">Named entity recognition by neural sliding window</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Binaghi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Carullo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Lamberti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,223.95,130.63,256.65,7.86;8,146.91,141.59,30.78,7.86">The Eighth IAPR International Workshop on Document Analysis Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="567" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,152.55,342.24,7.86;8,146.91,163.48,333.68,7.89;8,146.91,174.47,25.60,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,261.94,152.55,218.66,7.86;8,146.91,163.51,190.89,7.86">DTranNER: biomedical named entity recognition with deep learning-based label-label transition model</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,345.20,163.51,83.11,7.86">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="73" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,185.43,342.24,7.86;8,146.91,196.39,333.68,7.86;8,146.91,207.34,151.77,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,277.57,185.43,199.09,7.86">Charner: Character-level named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kuru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,159.57,196.39,321.02,7.86;8,146.91,207.34,68.09,7.86">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="911" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,218.30,342.24,7.86;8,146.91,229.26,333.68,7.86;8,146.91,240.22,214.95,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,321.23,218.30,159.36,7.86;8,146.91,229.26,199.17,7.86">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,367.02,229.26,113.57,7.86;8,146.91,240.22,130.86,7.86">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,251.18,342.24,7.86;8,146.91,262.14,71.43,7.86;8,255.79,262.14,224.80,7.86;8,146.91,273.10,135.99,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,255.79,262.14,176.78,7.86">architectures for named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,138.35,284.06,342.24,7.86;8,146.91,295.02,250.78,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,274.83,284.06,186.40,7.86">Transformation-based learning in the fast lane</title>
		<author>
			<persName coords=""><forename type="first">Grace</forename><surname>Ngai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,146.91,295.02,166.66,7.86">Proceedings of North Americal ACL 2001</title>
		<meeting>North Americal ACL 2001</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,305.98,342.24,7.86;8,146.91,316.91,118.12,7.89" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,227.96,305.98,222.95,7.86">Patents and Scientific Papers: Quite Different Concepts</title>
		<author>
			<persName coords=""><forename type="first">Valentinuzzi</forename><surname>Max</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,458.45,305.98,22.14,7.86;8,146.91,316.93,21.69,7.86">IEEE Pulse</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="53" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,327.89,342.25,7.86;8,146.91,338.85,333.68,7.86;8,146.91,349.78,64.15,7.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,333.58,327.89,147.02,7.86;8,146.91,338.85,329.43,7.86">Disease named entity recognition by combining conditional random fields and bidirectional recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,146.91,349.81,37.30,7.86">Database</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,360.77,337.97,7.86;8,146.91,371.73,252.25,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,396.09,360.77,84.49,7.86;8,146.91,371.73,69.56,7.86">Multi-grained named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08449</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.62,382.69,337.97,7.86;8,146.91,393.65,333.68,7.86;8,146.91,404.58,154.17,7.89" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,351.80,382.69,128.79,7.86;8,146.91,393.65,148.39,7.86">Biomedical named entity recognition based on deep neutral network</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Anwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,302.65,393.65,177.94,7.86;8,146.91,404.61,45.32,7.86">International Journal of Hybrid Information Technology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="279" to="288" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
