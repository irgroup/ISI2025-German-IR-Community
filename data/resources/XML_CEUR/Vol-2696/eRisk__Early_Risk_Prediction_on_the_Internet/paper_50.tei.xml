<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,170.13,115.96,275.09,12.62;1,169.70,133.89,275.96,12.62;1,261.17,151.82,93.01,12.62;1,232.77,171.66,149.81,10.52">Early Risk Detection of Self-Harm and Depression Severity using BERT-based Transformers iLab at CLEF eRisk 2020</title>
				<funder ref="#_V59JMhT">
					<orgName type="full">Agencia Estatal de Investigación / Project</orgName>
				</funder>
				<funder>
					<orgName type="full">FEDER / Ministerio de Ciencia, Innovación y Universidades</orgName>
				</funder>
				<funder>
					<orgName type="full">Consellería de Educación</orgName>
				</funder>
				<funder ref="#_yd9MVCy">
					<orgName type="full">European Regional De-velopment Fund</orgName>
					<orgName type="abbreviated">ERDF</orgName>
				</funder>
				<funder>
					<orgName type="full">Universidade e Formación Profesional</orgName>
				</funder>
				<funder ref="#_pf7jYe4 #_6QATefZ #_hzuHfew">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,212.74,207.69,115.93,8.74"><forename type="first">Rodrigo</forename><surname>Martínez-Castaño</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS)</orgName>
								<orgName type="institution">Universidade de Santiago de Compostela</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.56,207.69,49.82,8.74"><forename type="first">Amal</forename><surname>Htait</surname></persName>
							<email>amal.htait@strath.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.76,219.65,63.81,8.74"><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
							<email>leif.azzopardi@strath.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.51,219.65,77.63,8.74"><forename type="first">Yashar</forename><surname>Moshfeghi</surname></persName>
							<email>yashar.moshfeghi@strath.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,170.13,115.96,275.09,12.62;1,169.70,133.89,275.96,12.62;1,261.17,151.82,93.01,12.62;1,232.77,171.66,149.81,10.52">Early Risk Detection of Self-Harm and Depression Severity using BERT-based Transformers iLab at CLEF eRisk 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">94701EF7AEB57C4B4530D1F3673B3C45</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Self-Harm</term>
					<term>Depression</term>
					<term>Classification</term>
					<term>Social Media</term>
					<term>Early Detection</term>
					<term>BERT</term>
					<term>XLM-RoBERTa</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper briefly describes our research groups' efforts in tackling Task 1 (Early Detection of Signs of Self-Harm), and Task 2 (Measuring the Severity of the Signs of Depression) from the CLEF eRisk Track. Core to how we approached these problems was the use of BERT-based classifiers which were trained specifically for each task. Our results on both tasks indicate that this approach delivers high performance across a series of measures, particularly for Task 1, where our submissions obtained the best performance for precision, F1, latencyweighted F1 and ERDE at 5 and 50. This work suggests that BERTbased classifiers, when trained appropriately, can accurately infer which social media users are at risk of self-harming, with precision up to 91.3% for Task 1. Given these promising results, it will be interesting to further refine the training regime, classifier and early detection scoring mechanism, as well as apply the same approach to other related tasks (e.g., anorexia, depression, suicide).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The eRisk CLEF track aims to explore the development of methods for early risk detection on the Internet, their evaluation, and the application of such methods for improving the health and well being of individuals <ref type="bibr" coords="2,380.54,118.99,8.07,8.74" target="#b7">[8]</ref><ref type="bibr" coords="2,388.60,118.99,4.03,8.74" target="#b8">[9]</ref><ref type="bibr" coords="2,388.60,118.99,4.03,8.74" target="#b9">[10]</ref><ref type="bibr" coords="2,392.63,118.99,12.10,8.74" target="#b10">[11]</ref>. Early detection technologies can be employed in different areas, particularly those related to health and safety. For instance, in <ref type="bibr" coords="2,286.14,142.90,10.52,8.74" target="#b8">[9]</ref> they examined whether it was possible to identify grooming activities of paedophiles given posts to online forums. While in <ref type="bibr" coords="2,146.84,166.81,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="2,164.00,166.81,11.62,8.74" target="#b10">11]</ref>, they explored whether it was possible to detect users that were depressed or anorexic from their posts, and crucially how quickly this could be detected. This year the focus is on detecting the early signs of self-harm from people's posts to social media (Task 1), and whether it is possible to infer how depressed people are given such posts (Task 2) <ref type="bibr" coords="2,353.95,214.64,14.61,8.74" target="#b11">[12]</ref>. Below is an elaborated description of each task.</p><p>Task 1: Early Detection of Signs of Self-Harm. This first task consists of triggering alerts for users that present early signs of committing self-harm. A tagged set of users and their posts to Reddit<ref type="foot" coords="2,330.36,266.64,3.97,6.12" target="#foot_0">3</ref> groups was provided for training purposes. The different methods were benchmarked using a system that simulates a real-time scenario introduced in <ref type="bibr" coords="2,305.68,292.13,14.61,8.74" target="#b10">[11]</ref>. The posts from the users of the test dataset are served in rounds, one post at a time (simulating their live posting to the Reddit groups). The task then is to provide a decision about each user given their posts, and to do so as early as possible (i.e., with the fewest posts). For the evaluation, the correctness of the prediction (i.e., whether the user will cause self-harm or not) is not the only factor taken into account, but also the delay taken to emit the alerts. Clearly, the sooner a person who is likely to self-harm is identified, the sooner the intervention can be provided.</p><p>Task 2: Measuring the Severity of the Signs of Depression. This task consists of automatically estimating the level of several symptoms associated with depression. For that, a questionnaire with 21 questions related to different feelings and well-being (e.g., sadness, pessimism, fatigue) is provided. Each question has between four and seven possible answers which are related to different levels of severity (or relevance) of the symptom or behaviour. A sample of users with their answers to the questionnaire and their writings at Reddit was given. To benchmark the different approaches, a new set of users and their writings is provided, for which every team has to predict their answers.</p><p>Thus, the goal of this paper is to explore the potential of a BERT-based classifier coupled with a novel scoring mechanism for the early detection of selfharm and depression. This paper is structured as follows. In Section 2 we describe our general approach for both tasks by using BERT-based models for sentence classification. In Section 3 and Section 4 we explain how the classifiers were trained and applied for Task 1 and Task 2 respectively. Section 5 covers the analysis of our results, where our approach performs the best across a number of metrics for both tasks. Finally, in Section 6 we summarise the contributions of these working notes.</p><p>A breakthrough in the use of machine learning for Natural Language Processing (NLP) appeared with the generative pre-training of language models on a diverse corpus of unlabelled text, such as ELMo <ref type="bibr" coords="3,323.60,173.73,14.61,8.74" target="#b14">[15]</ref>, BERT <ref type="bibr" coords="3,378.71,173.73,9.96,8.74" target="#b3">[4]</ref>, OpenAI GPT <ref type="bibr" coords="3,462.33,173.73,14.61,8.74" target="#b15">[16]</ref>, XLM <ref type="bibr" coords="3,161.85,185.69,9.96,8.74" target="#b5">[6]</ref>, and RoBERTa <ref type="bibr" coords="3,247.96,185.69,9.96,8.74" target="#b6">[7]</ref>. Such a technique demonstrated large gains on a variety of NLP tasks (e.g., sequence or token classification, question answering, semantic similarity assessment, document classification). In particular, BERT (Bidirectional Encoder Representations from Transformers) <ref type="bibr" coords="3,397.33,221.55,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="3,409.51,221.55,7.01,8.74" target="#b2">3]</ref>, the model by Google AI, proved to be one of the most powerful tools for text classification <ref type="bibr" coords="3,465.10,233.51,15.50,8.74" target="#b12">[13,</ref><ref type="bibr" coords="3,134.77,245.46,12.73,8.74" target="#b13">14,</ref><ref type="bibr" coords="3,149.16,245.46,7.01,8.74" target="#b4">5]</ref>. BERT is based on the Transformer architecture <ref type="bibr" coords="3,369.82,245.46,15.50,8.74" target="#b17">[18]</ref> and it was trained for both masked word prediction and next sentence prediction at the same time. As input, BERT takes two concatenated segments of text which are delimited with special tokens and whose length respects a defined maximum. The model was pre-trained on a huge dataset of unlabelled text. It is typically used within a text classifier for sentence tokenisation and text representation. A standard BERT classifier is presented in Figure <ref type="figure" coords="3,274.65,317.19,4.98,8.74" target="#fig_0">1</ref> where a sentence is tokenised, represented in embeddings and then classified. The results are normalised between 0 and 1 using the softmax function, representing the probability of the input sentence to belong to a certain class (e.g., the probability of the sentence to be written by a self-harmer). As for RoBERTa <ref type="bibr" coords="4,226.76,118.99,10.52,8.74" target="#b6">[7]</ref> (a replication study of BERT pre-training by Facebook AI), it shares a similar architecture with BERT but with a different pre-training approach. RoBERTa was trained over ten times more data, the next sentence prediction objective was removed, and the masked word prediction task was improved with the introduction of a dynamic masking pattern applied to the training data.</p><p>In another attempt to improve the language model, Facebook AI presented XLM-RoBERTa <ref type="bibr" coords="4,208.20,204.64,10.52,8.74" target="#b1">[2]</ref> with the pre-training of multilingual language models. This new improvement led to significant performance gains in text classification. For our participation at the eRisk challenges of 2020, variety of pre-training language models were tested: BERT, DistillBERT, RoBERTa, and XLM-RoBERTa, among others. However, the best performance was achieved when using XLM-RoBERTa on our training data. In our work, we used Ernie<ref type="foot" coords="4,397.54,262.84,3.97,6.12" target="#foot_1">4</ref> , a Python library for sentence classification built on top of Hugging Face Transformers<ref type="foot" coords="4,432.32,274.80,3.97,6.12" target="#foot_2">5</ref> , the main library that implements state-of-the-art general-purpose Transformer-based architectures.</p><p>Most of the pre-training language models, including XLM-RoBERTa, have a maximum input length of 512 tokens. In our work, we experimented with input sentences of sizes between 32 and 128 tokens due to GPU memory restrictions. The best results were achieved with an input size of 128 tokens. Note that Reddit posts are usually shorter than 128 tokens. Therefore, using an input size larger than 128 would not substantially increase performance, but it would significantly increase the required computational resources. In the few cases where the Reddit posts were longer, we split them based on punctuation marks in an attempt to respect the context of the writings posted by the users. When training the classifiers, the weights of the pre-trained base models (e.g., XLM-RoBERTa) are updated, in addition to the classification head.</p><p>For our participation at the eRisk challenges of 2020, both Task 1 and Task 2, we used the previously explained approach for sentence classification. However, in each task, the employed training schedule and training data were varied and tailored to fit the task scenarios, as explained in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task 1 -Early Risk Detection of Self-Harm</head><p>We trained a number of different language models based on the original BERT architecture with a classification head to predict whether a sentence was written by a subject that self-harms or not. Those models are the base to predict if a user is likely to self-harm and thus, triggering an alert, given a stream of texts. All of our final models were based on XLM-RoBERTa, which demonstrated better performance for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>To train our models, we avoided using the training dataset provided by the eRisk organisers for two reasons. First, during the beginning of our experimentation, we found that the results obtained with our BERT-based approach were not promising enough to beat the existing approaches used in 2019. Second, the training dataset matches the test data of the eRisk 2019's task. Taking it out from the training stage led us to be able to compare our results with the obtained by the last year's participants in our search for models with greater performance.</p><p>The data collected and used for training our models were obtained from the Pushshift Reddit Dataset <ref type="bibr" coords="5,246.45,234.72,10.52,8.74" target="#b0">[1]</ref> through its public API <ref type="foot" coords="5,356.45,233.15,3.97,6.12" target="#foot_3">6</ref> , which exposes a repository with constantly updated and almost complete dataset of all the public Reddit data. We downloaded all the available submissions and comments written to the most popular subreddit about self-harm (r/selfharm). From those posts, we extracted 42, 839 authors. In addition, we collected all of the posts in any other subreddit for those authors (selfharm-users-texts dataset). Then, we obtained an equivalent amount of random users from which we also extracted all their posts (random-users-texts dataset). We filtered the obtained datasets in several ways. First, we checked that there were not any user collision between the two collections. After identifying some of the main self-harm related subreddits (r/selfharm, r/Cutters, r/MadeOfStyrofoam, r/SelfHarmScars, r/StopSelfHarm, r/CPTSD and r/SuicideWatch), we removed the users from random-userstexts having at least one post in any of them. All the users with more than 5, 000 submissions were removed since those with an extremely high number of posts seem more likely to be bots. Besides, the vast majority of the users had posted fewer times so we presumed to have more chances to profile the average user below that threshold. We also pruned the less active users under 50 submissions. The number of sentences was expanded by splitting the users' texts that were too long for the parameters we utilised in our models. Otherwise, the sentences would be truncated during training, potentially losing valuable information. We split the large posts into groups of contiguous sentences of approximately the maximum length in tokens utilised in our models and following the punctuation marks hierarchy (e.g., prioritising the splits on full stops over commas). As commented before, a maximum length of 128 tokens was set so the models could be fine-tuned in commercial GPUs.</p><p>We created several datasets mainly derived from selfharm-users-texts and random-users-texts for training our model candidates. These datasets are presented in Table <ref type="table" coords="5,235.05,569.50,3.87,8.74" target="#tab_0">1</ref>, and explained below:</p><p>-A manually created dataset:</p><p>• real-selfharmers-texts: This dataset was created with the aim of obtaining a bigger but similar dataset to the one provided by the eRisk organisers. We manually tagged 354 users as real self-harmers from the users of the selfharm-users-texts dataset. Then, we filtered the last 1, 000 submissions and comments for every user. We also pruned the writing sequences just before their first writing at r/selfharm. After that, we filtered the users with at least 10 writings remaining, ending up with a total of 120 real self-harmers. For the negative class, we took a sample of random users from the dataset random-users-texts in the same proportion as in the provided training data: ∼ 7.3 random users per self-harmer.</p><p>-Datasets automatically generated from selfharm-users-texts and random-users-texts after removing the users from real-selfharmerstexts. In Figure <ref type="figure" coords="6,227.15,238.60,3.87,8.74" target="#fig_1">2</ref>, we show the distribution of posts per user for the original datasets (selfharm-users-texts and random-users-texts) and the derived ones utilised to train the final classifiers:</p><p>• users-texts-200k: This dataset was generated by random sampling 200K writings from both selfharm-users-texts (as self-harmers) and random-users-texts (as non self-harmers), with 100K from each dataset. Note that we experimented by replicating last years' task with different sizes of sampling such as 2K, 20K, 100K, 300K, 400K and 500K writings, but the best results were achieved with a sampling size of 200K writings.</p><p>• users-texts-2m: This dataset is a variant of users-texts-200k; a balanced dataset with ten times more sentences, totalling 2M writings. Note that, during our experimentation replicating last years' task, using a training set larger than 200K did not improve the results except for the ERDE 5 metric with the 2M writings.</p><p>• users-submissions-200k: This dataset was generated in a similar procedure as users-texts-200k, with 200K random sampled writings, but with the difference of avoiding comments. Therefore, sampling users' submissions exclusively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method</head><p>For our participation in Task 1 of eRisk we trained three models for binary sentence classification, all of them based on the XLM-RoBERTa-base language model (since it behaved better than other variants we tried such as BERT, DistillBERT, XLNet, etc.):</p><p>xlmrb-selfharm-200k trained with the dataset users-texts-200k.</p><p>xlmrb-selfharm-2m trained with the dataset users-texts-2m.</p><p>xlmrb-selfharm-sub-200k trained with the dataset users-submissions-200k.</p><p>We established for those models a maximum length of tokens as 128 per sentence, a training rate of 2e -5 and a validation size of the 20%.</p><p>In order to predict if a user has or has not risk of self-harm, we averaged the predicted probability of the known writings for every user. We omitted the prediction of sentences with less than 10 tokens as we concluded that the performance on smaller sentences is poor. Since the provided training set was the test set of the last year's task, we used it to compare the performance of our models with the participants of the previous year. We defined several parameters to determine if the system should trigger an alert given a list of known user's texts: the minimum average probability threshold (θ), the minimum number of texts necessary to trigger an alert, and the maximum number of texts that the system will take into account to make its decisions on the subjects. Given a growing list of texts from a user, the system will trigger an alert if the average probability of the known texts for that user is greater or equal than θ, the number of known texts is greater or equal to the minimum, and lower or equal to the maximum.</p><p>The parameters were adjusted in five variants by finding their optimal values for F1 and the eRisk related metrics: latency-weighted F1, ERDE 5 and ERDE 50 with the real-selfharmers-texts dataset. For example, in Figure <ref type="figure" coords="8,445.71,214.79,4.98,8.74">3</ref> it can be observed that the best value for latency-weighted F1 with any θ is obtained when waiting for at least 10-12 texts for xlmrb-selfharm-200k. We chose the model with the best performance for each target metric. The selected parameters for each variant can be observed in Table <ref type="table" coords="8,322.95,262.61,4.98,8.74" target="#tab_1">2</ref> and the results obtained with the real-selfharmers-texts dataset are shown in Table <ref type="table" coords="8,379.55,274.57,3.87,8.74" target="#tab_2">3</ref>.</p><p>After choosing the parameters with the real-selfharmers-texts dataset, we tested the classifiers with the last year's test data for the same task as showed in Table <ref type="table" coords="8,174.91,310.59,3.87,8.74" target="#tab_3">4</ref>, where we compare the obtained results with the best performer of 2019 for that task: UNSL. That team obtained the best results for precision, F1, ERDE 5 , ERDE 50 and latency-weighted F1. With the classifiers that we used in our submission, we improved their results for F1, ERDE 5 , ERDE 50 and latencyweighted F1. An analogous approach as the one employed for Task 1, with random posts from users connected solely by a common subreddit, was not possible this time. Therefore, and due to the small dataset for training (only 20 different users), we used the full provided training dataset in order to train the classifiers. For each question of the questionnaire, we modified the training dataset by assigning the same class to all the texts posted by a given user (i.e., each class matches one of the available answers). Thus, we obtained a different training set for each question of the questionnaire, and, therefore, one different multi-class classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Method</head><p>For this task, we applied a similar method as the one employed in Task 1, but we treated the problem as a multi-class labelling problem. We created three variants, only differing in the base language model and the pre-processing of the training data, as it can be observed in Table <ref type="table" coords="10,324.37,304.35,3.87,8.74">5</ref>. For the runs 1 and 2, we expanded the training by splitting texts larger than 128 tokens in the same way as in Task 1. However, for Run 3, sentences larger than 128 tokens were truncated during the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Base LM Strategy</p><formula xml:id="formula_0" coords="10,233.91,393.30,145.04,36.38">1 XLM-RoBERTa-base split 2 RoBERTa-base split<label>3</label></formula><p>RoBERTa-base truncate Table <ref type="table" coords="10,187.58,440.20,4.13,7.89">5</ref>. Base language models and training set variants used for Task 2.</p><p>For each variant, we fine-tuned the base language model with a head for multi-class classification for every question. As shown in Table <ref type="table" coords="10,415.55,499.10,3.87,8.74">6</ref>, we balanced the class weights of every question model for all the variants. The RoBERTabased classifiers were trained for 4 epochs, whereas we executed 5 epochs for the XLM-RoBERTa-based ones. Those numbers of epochs were found to be optimal in all the models we created during our experimentation for Task 1. We established the maximum sentence length to 128 tokens and the learning rate to 2e-5 to train all the models. We assigned a 20% of the training data for validation.</p><p>For a given user and variant, we predict the questionnaire answer in the following way: given a question and the associated classifier, we obtain the softmax prediction vector for every text written by that user and we sum them. The class with the highest accumulated value is the answer to the questionnaire we predict. As in Task 1, during prediction, if the input texts are larger than 128 tokens, we split them and average the predictions of the chunks.</p><p>Table <ref type="table" coords="12,162.27,145.22,4.98,8.74" target="#tab_4">7</ref> shows the performance of our runs for Task 1, while Table <ref type="table" coords="12,429.62,145.22,4.98,8.74">8</ref> shows the performance of our runs for Task 2. In each table, the best scores among all the participants are highlighted in bold. Other runs from other teams have also been included to show the best performing runs for each task on each metric.</p><p>For task 1, the evaluation metrics used were <ref type="bibr" coords="12,345.75,193.50,14.61,8.74" target="#b10">[11]</ref>:</p><p>-The standard classification measures precision (P), recall (R) and F1, are computed with respect to the positive class, since they are the only cases that trigger alerts. -ERDE (Early Risk Detection Error) <ref type="bibr" coords="12,313.91,251.60,9.96,8.74" target="#b7">[8]</ref>, is an error measure that introduces a penalty for late correct alerts (true positives) and depends on the number of user writings seen before the alert. Two sets of user writing numbers are taken into consideration in this challenge: 5 and 50. Contrary to the other metrics, the lower the value of ERDE, the better the performance of the system. -Latency T P measures the delay in detecting true positives, defined as the median number of writings used to detect positive cases. -Speed is the system's overall speed factor, where it will be equal to 1 for a system whose true positives are detected right at the first writing, and almost 0 for a slow system, which detects true positives after hundreds of writings. -Latency-weighted F1 <ref type="bibr" coords="12,257.80,396.45,15.50,8.74" target="#b16">[17]</ref> score is equal to F 1•speed, and a perfect system gets latency-weighted F1 equals to 1.</p><p>For Task 2, the following metrics were used <ref type="bibr" coords="12,342.43,429.72,14.61,8.74" target="#b10">[11]</ref>:</p><p>-AHR (Average Hit Rate) is the average of Hit Rate (HR) across all users, and HR is the ratio of cases where the automatic questionnaire has exactly the same answer as the actual questionnaire. -ACR (Average Closeness Rate) is the average of Closeness Rate (CR) across all users, and CR is equal to (mad -ad)/mad, where mad is the maximum absolute difference, which is equal to the number of possible answers minus one, and ad is the absolute difference between the real and the automated answer.  <ref type="table" coords="13,162.72,574.19,4.13,7.89">8</ref>. The performance for each run we submitted on Task 2: Measuring the severity of the signs of depression, along with the runs from other teams that scored higher.</p><p>For Task 1, our team's performance for each of the key metrics was the best compared to the other teams this year. Given our training schedule which tried to maximise the performance for each metric per run, we can see that no specific run was the best across all the metrics, but rather there is a trade-off between metrics. For example, Run 1 obtains a precision score of 0.913, but has the lowest recall, while Run 4 obtains the highest F1, but not the best precision or recall. Of most interest is the performance on the eRisk-specific metrics, where our runs obtained notably the best results. With Run 0 we obtained a latency-weighted F1 of 0.66, where the second-best result was obtained by the team UNSL with their run 1 at 0.61. For ERDE 5 , Run 2 scored 0.134, whereas the second-best team was again UNSL with their run 1 at 0.172 (where lower is better). For ERDE 50 , our Run 3 obtained a score of 0.071, whereas all the other runs ranged between 0.11 to 0.25.</p><p>For Task 2, our team's performance was the best for ACR, and competitive for the other metrics. For AHR, ADODL and DCHR our performances were within 1-2% of the best performances submitted. Interestingly, while the ADODL scores were around 81-83%, this did not translate into a better classification of depression category as surmised by DCHR, which was 34% at best. This disparity may be due to how we employed the BERT based classifier (i.e., we made separate models to predict the results of each question). However, it may be more appropriate to jointly predict the results of all questions and the final depression category. This is because the questions will have a high correlation between answers, and information for inferring the answer for one question, may be useful in inferring others when taken together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary</head><p>In this paper we have described how we employed a BERT-based classifier for the tasks of the CLEF eRisk Track: Task 1, early risk detection of self-harm; and Task 2, inferring answers to a depression survey. Our results on both tasks indicated that this approach works very well and obtains very good performance (the best on Task 1 and very competitive performance on Task 2). These results are perhaps not too surprising, given the impact that BERT-based models have been making in improving many other tasks. However, a key difference in this work is how we trained the model. In future work, we will explore and compare different training schedules and classifiers extensions for these tasks, but also for other related tasks (e.g., classifying whether someone is like to suffer from anorexia, depression).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,210.50,655.03,194.35,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. BERT-based Classification Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,134.77,359.34,345.83,8.39;7,134.77,370.32,271.23,8.37;7,180.12,115.84,255.11,228.73"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Distribution of the number of posts per user in the datasets selfharm-userstexts, random-users-texts and the derived datasets from them.</figDesc><graphic coords="7,180.12,115.84,255.11,228.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,141.47,505.09,330.87,149.61"><head>Table 1 .</head><label>1</label><figDesc>Some statistics of the datasets used to train the classifiers.</figDesc><table coords="6,141.47,505.09,330.87,131.20"><row><cell>Dataset</cell><cell>Class</cell><cell cols="3">Users Subreddits Sentences</cell><cell>Years</cell></row><row><cell></cell><cell cols="2">selfharm 120</cell><cell>1, 346</cell><cell>8, 943</cell><cell>2013 -2020</cell></row><row><cell>real-selfharmers-texts</cell><cell>random</cell><cell>875</cell><cell>5, 585</cell><cell cols="2">87, 260 2009 -2020</cell></row><row><cell></cell><cell cols="2">selfharm 9, 487</cell><cell>9, 797</cell><cell cols="2">107, 277 2006 -2020</cell></row><row><cell>users-texts-200k</cell><cell cols="2">random 14, 280</cell><cell>9, 793</cell><cell cols="2">107, 152 2006 -2020</cell></row><row><cell></cell><cell cols="2">selfharm 10, 454</cell><cell>26, 931</cell><cell cols="2">1, 075, 476 2006 -2020</cell></row><row><cell>users-texts-2m</cell><cell cols="2">random 17, 548</cell><cell>26, 409</cell><cell cols="2">1, 076, 707 2005 -2020</cell></row><row><cell></cell><cell cols="2">selfharm 10, 319</cell><cell>13, 681</cell><cell cols="2">131, 233 2006 -2020</cell></row><row><cell>users-submissions-200k</cell><cell cols="2">random 15, 937</cell><cell>14, 913</cell><cell cols="2">128, 064 2005 -2020</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,393.97,345.83,270.88"><head>Table 2 .</head><label>2</label><figDesc>Combinations of models and parameters for the five submitted runs. Latency-weighted F1 when varying the minimum number of texts to trigger an alert. Model xlmrb-selfharm-200k with the real-selfharmers-texts dataset. A maximum of 50 posts are taken into account.</figDesc><table coords="8,134.77,393.97,345.83,270.88"><row><cell cols="2">Run Model</cell><cell>Target Metric</cell><cell>θ</cell><cell cols="2">Min. Max.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">posts posts</cell></row><row><cell>0</cell><cell>xlmrb-selfharm-200k</cell><cell cols="2">latency-weighted F1 0.75</cell><cell>10</cell><cell>50</cell></row><row><cell>1</cell><cell>xlmrb-selfharm-2m</cell><cell cols="2">latency-weighted F1 0.76</cell><cell>10</cell><cell>50</cell></row><row><cell>2</cell><cell>xlmrb-selfharm-2m</cell><cell>ERDE 5</cell><cell>0.69</cell><cell>2</cell><cell>5</cell></row><row><cell>3</cell><cell>xlmrb-selfharm-sub-200k</cell><cell>ERDE 50</cell><cell>0.64</cell><cell>45</cell><cell>45</cell></row><row><cell>4</cell><cell>xlmrb-selfharm-200k</cell><cell>F1</cell><cell>0.68</cell><cell>100</cell><cell>100</cell></row><row><cell cols="2">4 Task 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.1 Data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">For our participation in Task 2 of eRisk, we used the training dataset provided</cell></row><row><cell cols="6">by the task's organisers. Both training and test datasets consist of Reddit posts</cell></row><row><cell cols="6">written by users who have answered the questionnaire. The training dataset</cell></row><row><cell cols="6">includes a total of 10, 941 posts by 20 users, and the test dataset includes 35, 562</cell></row><row><cell cols="2">posts by 70 users.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.77,424.12,345.83,187.40"><head>Table 3 .</head><label>3</label><figDesc>Results obtained by our five final variants with the real-selfharmerstexts dataset when using the optimal parameters.</figDesc><table coords="9,142.03,488.89,331.30,122.63"><row><cell>Team</cell><cell>Run P</cell><cell>R</cell><cell cols="3">F1 ERDE ERDE Latency Speed Latency-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>50</cell><cell>TP</cell><cell>weighted F1</cell></row><row><cell cols="5">UNSL 2019 0 0.71 0.41 0.52 0.090 0.073</cell><cell>2</cell><cell>1</cell><cell>0.52</cell></row><row><cell cols="5">UNSL 2019 4 0.31 0.88 0.46 0.082 0.049</cell><cell>3</cell><cell>.99</cell><cell>0.45</cell></row><row><cell>iLab</cell><cell cols="4">0 0.68 0.66 0.67 0.125 0.046</cell><cell>10</cell><cell>0.97</cell><cell>0.64</cell></row><row><cell>iLab</cell><cell cols="4">1 0.69 0.59 0.63 0.124 0.054</cell><cell>10</cell><cell>0.97</cell><cell>0.61</cell></row><row><cell>iLab</cell><cell cols="4">2 0.33 0.71 0.45 0.062 0.057</cell><cell>2</cell><cell>1</cell><cell>0.44</cell></row><row><cell>iLab</cell><cell cols="4">3 0.34 0.83 0.48 0.144 0.045</cell><cell>45</cell><cell>0.83</cell><cell>0.40</cell></row><row><cell>iLab</cell><cell cols="4">4 0,68 0.66 0.67 0.125 0.125</cell><cell>100</cell><cell>0.63</cell><cell>0.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,134.77,622.04,345.82,18.85"><head>Table 4 .</head><label>4</label><figDesc>Results obtained by our five final variants with the 2019 dataset compared to the results obtained by UNSL.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,140.99,548.03,339.60,116.83"><head>Table 7 .</head><label>7</label><figDesc>-ADODL (Average DODL) is the averaged of Difference between Overall Depression Levels (DODL) across all users. DODL computes the overall depression level (sum of all the answers) for the real and automated questionnaire and, next, the absolute difference (ad overall) between the real and the automated score is computed. DODL is normalised into [0,1] as follows: The performance for each run we submitted on Task 1: Early Detection of Signs of Self-Harm. Note that for each bolded metric our run gave the highest performance.</figDesc><table coords="12,140.99,607.83,339.60,57.02"><row><cell>DODL = (63 -ad overall)/63.</cell></row><row><cell>-DCHR (Depression Category Hit Rate) computes the fraction of cases</cell></row><row><cell>where the automated questionnaire led to a depression category (out of 4</cell></row><row><cell>categories: nonexistence, mild, moderate and severe) that is equivalent to</cell></row><row><cell>the depression category obtained from the real questionnaire.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,656.80,80.20,7.86"><p>https://reddit.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="4,155.48,645.84,141.40,7.86"><p>https://github.com/labteral/ernie/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="4,155.48,656.80,188.88,7.86"><p>https://github.com/huggingface/transformers/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="5,144.73,657.44,169.96,7.47"><p>https://pushshift.io/api-parameters/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The first author would like to thank the following funding bodies for their support: <rs type="funder">FEDER / Ministerio de Ciencia, Innovación y Universidades</rs>, <rs type="funder">Agencia Estatal de Investigación / Project</rs> (<rs type="grantNumber">RTI2018-093336-B-C21</rs>), <rs type="funder">Consellería de Educación</rs>, <rs type="funder">Universidade e Formación Profesional</rs> and the <rs type="funder">European Regional De-velopment Fund (ERDF)</rs> (accreditation <rs type="grantNumber">2019-2022 ED431G-2019/04</rs>, <rs type="grantNumber">ED431C 2018/29</rs>, <rs type="grantNumber">ED431C 2018/19</rs>).</p><p>The second and third authors would like to thank the UKRI's <rs type="projectName">EPSRC Project Cumulative Revelations in Personal Data</rs> (Grant Number: <rs type="grantNumber">EP/R033897/1</rs>) for their support. We would also like to thank <rs type="person">David Losada</rs> for arranging this collaboration.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_V59JMhT">
					<idno type="grant-number">RTI2018-093336-B-C21</idno>
				</org>
				<org type="funding" xml:id="_yd9MVCy">
					<idno type="grant-number">2019-2022 ED431G-2019/04</idno>
				</org>
				<org type="funding" xml:id="_pf7jYe4">
					<idno type="grant-number">ED431C 2018/29</idno>
				</org>
				<org type="funding" xml:id="_6QATefZ">
					<idno type="grant-number">ED431C 2018/19</idno>
				</org>
				<org type="funded-project" xml:id="_hzuHfew">
					<idno type="grant-number">EP/R033897/1</idno>
					<orgName type="project" subtype="full">EPSRC Project Cumulative Revelations in Personal Data</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="15,142.96,227.78,337.63,7.86;15,151.52,238.74,329.07,7.86;15,151.52,249.70,218.10,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,464.71,227.78,15.88,7.86;15,151.52,238.74,96.74,7.86">The pushshift reddit dataset</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zannettou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Keegan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Blackburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,271.37,238.74,209.22,7.86;15,151.52,249.70,101.42,7.86">Proceedings of the International AAAI Conference on Web and Social Media</title>
		<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="830" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,259.94,337.63,7.86;15,151.52,270.90,329.07,7.86;15,151.52,281.86,290.55,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="15,373.35,270.90,107.25,7.86;15,151.52,281.86,124.84,7.86">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02116</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.96,292.10,337.63,7.86;15,151.52,303.03,263.45,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,262.62,292.10,217.97,7.86;15,151.52,303.06,110.58,7.86">Open sourcing bert: State-of-the-art pre-training for natural language processing</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,269.17,303.06,61.04,7.86">Google AI Blog</title>
		<imprint>
			<date type="published" when="2018-11-02">November 2 (2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,313.30,337.63,7.86;15,151.52,324.25,329.07,7.86;15,151.52,335.21,25.60,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="15,346.99,313.30,133.60,7.86;15,151.52,324.25,189.89,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.96,345.45,337.63,7.86;15,151.52,356.39,197.55,7.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,311.40,345.45,169.19,7.86;15,151.52,356.41,36.37,7.86">Target-dependent sentiment classification with bert</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,194.81,356.41,51.68,7.86">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="154290" to="154299" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,366.65,337.64,7.86;15,151.52,377.61,97.80,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="15,255.14,366.65,161.94,7.86">Cross-lingual language model pretraining</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.96,387.85,337.64,7.86;15,151.52,398.81,329.07,7.86;15,151.52,409.77,201.31,7.86" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="15,282.11,398.81,198.48,7.86;15,151.52,409.77,34.83,7.86">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.96,420.01,337.63,7.86;15,151.52,430.97,329.07,7.86;15,151.52,441.93,196.72,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,259.23,420.01,221.36,7.86;15,151.52,430.97,11.56,7.86">A test collection for research on depression and language use</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,187.15,430.97,293.44,7.86;15,151.52,441.93,82.13,7.86">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,452.17,337.64,7.86;15,151.52,463.13,329.07,7.86;15,151.52,474.06,49.89,7.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,309.01,452.17,171.59,7.86;15,151.52,463.13,200.15,7.86">CLEF 2017 eRisk overview: Early Risk prediction on the internet: Experimental foundations</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,359.08,463.13,121.51,7.86">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1866</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,484.33,337.97,7.86;15,151.52,495.26,329.07,7.89;15,151.52,506.25,25.60,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,309.78,484.33,170.81,7.86;15,151.52,495.29,177.10,7.86">Overview of eRisk 2018: Early Risk Prediction on the Internet (extended lab overview)</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,335.64,495.29,120.80,7.86">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,516.49,337.98,7.86;15,151.52,527.45,329.07,7.86;15,151.52,538.38,329.07,7.89;15,151.52,549.34,144.47,7.89" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,316.95,516.49,163.65,7.86;15,151.52,527.45,92.41,7.86">Overview of eRisk 2019 Early Risk Prediction on the Internet</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,151.52,549.34,20.12,7.89">LNCS</title>
		<imprint>
			<biblScope unit="volume">11696</biblScope>
			<biblScope unit="page" from="340" to="357" />
			<date type="published" when="2019-09">September. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,559.60,337.97,7.86;15,151.52,570.56,329.07,7.86;15,151.52,581.52,329.07,7.86;15,151.52,592.48,131.29,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,315.66,559.60,164.92,7.86;15,151.52,570.56,88.92,7.86">Overview of eRisk 2020: Early Risk Prediction on the Internet</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,260.50,570.56,220.09,7.86;15,151.52,581.52,329.07,7.86;15,151.52,592.48,46.17,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction Proceedings of the Eleventh International Conference of the CLEF Association</title>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,602.72,337.98,7.86;15,151.52,613.68,329.07,7.86;15,151.52,624.64,247.74,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,268.16,602.72,212.44,7.86;15,151.52,613.68,175.98,7.86">Nikolov-radivchev at semeval-2019 task 6: Offensive tweet classification with bert and ensembles</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Radivchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,349.19,613.68,131.40,7.86;15,151.52,624.64,163.70,7.86">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>the 13th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="691" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,634.88,337.97,7.86;15,151.52,645.84,329.07,7.86;15,151.52,656.80,159.05,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="15,168.02,645.84,308.09,7.86">Multi-label categorization of accounts of sexism using a neural framework</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Abburi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Badjatiya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04602</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,142.62,119.67,337.98,7.86;16,151.52,130.63,329.07,7.86;16,151.52,141.59,97.80,7.86" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m" coord="16,227.51,130.63,179.35,7.86">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,142.62,152.55,337.97,7.86;16,151.52,163.51,329.07,7.86;16,151.52,174.47,329.07,7.86;16,151.52,185.43,70.42,7.86" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<title level="m" coord="16,399.33,152.55,81.26,7.86;16,151.52,163.51,164.34,7.86">Improving language understanding by generative pre-training</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,196.39,337.98,7.86;16,151.52,207.34,329.07,7.86;16,151.52,218.30,202.12,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,292.66,196.39,187.93,7.86;16,151.52,207.34,57.62,7.86">Measuring the latency of depression detection in social media</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sadeque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,230.03,207.34,250.57,7.86;16,151.52,218.30,117.83,7.86">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="495" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,229.26,337.97,7.86;16,151.52,240.22,329.07,7.86;16,151.52,251.18,167.19,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="16,228.73,240.22,100.53,7.86">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,351.88,240.22,128.71,7.86;16,151.52,251.18,73.89,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
