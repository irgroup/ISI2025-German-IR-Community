<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.10,115.90,333.16,12.90;1,134.77,133.83,345.83,12.90;1,223.43,153.68,168.50,10.75">LSACoNet: A Combination of Lexical and Conceptual Features for Analysis of Fake News Spreaders on Twitter Notebook for PAN at CLEF 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,146.39,190.08,89.10,8.64"><forename type="first">Hamed</forename><forename type="middle">Babaei</forename><surname>Giglou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tabriz</orgName>
								<address>
									<settlement>Tabriz</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.94,190.08,57.16,8.64"><forename type="first">Jafar</forename><surname>Razmara</surname></persName>
							<email>razmara@tabrizu.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tabriz</orgName>
								<address>
									<settlement>Tabriz</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.54,190.08,71.01,8.64"><forename type="first">Mostafa</forename><surname>Rahgouy</surname></persName>
							<email>mostafa.rahgouy@partdp.ai</email>
							<affiliation key="aff1">
								<orgName type="department">Part AI Research Center</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,408.88,190.08,55.61,8.64"><forename type="first">Mahsa</forename><surname>Sanaei</surname></persName>
							<email>mahsasanaei97@ms.tabrizu.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tabriz</orgName>
								<address>
									<settlement>Tabriz</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.10,115.90,333.16,12.90;1,134.77,133.83,345.83,12.90;1,223.43,153.68,168.50,10.75">LSACoNet: A Combination of Lexical and Conceptual Features for Analysis of Fake News Spreaders on Twitter Notebook for PAN at CLEF 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2FBF5FDE2A02B4D00B8191F827647D7E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fake News</term>
					<term>False Information</term>
					<term>Feature Combination</term>
					<term>Suspicious Fake News Authors</term>
					<term>Fully Connected Neural Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fake news detection on social medial has attracted a huge body of research as one of the most important tasks of social analysis in recent years. In this task, given a Twitter feed, the goal is to identify fake/real news authors or spreaders. We assume fake news authors mostly like to play with the semantic aspect of news rather than trying to add specific changes to their styles. However, making a change into the semantic aspect of news can cause unwanted changes in style. We hypothesize, by relying on news content, a combination of semantic and coarse-grained features may lead us to common information about the author's style while reviewing the conceptual aspect of author documents. In this paper, we propose the LSACoNet representation using a fully connected neural network (FCNN) classifier that combines different levels of document representation to investigate this hypothesis. Experimental results presented in this paper showed that a combination of representations plays an important role in identifying fake/real news spreaders. Finally, we achieved accuracies of 72.5% and 74.5% in the English and Spanish test datasets, respectively, using presented LSACoNet representation and FCNN classifier.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>False information such as fake news is one of the main threats of our society. In the last years, big social networks like Facebook or Twitter have admitted that their networks had fake and duplicate accounts. Regarding this, fake news are not a new phenomenon, and the exponential growth of social media has offered an easy way for fast propagation. These fake news usually try to deceive users to express specific options. Users play a critical role in the creation and spread of fake news by influencing people to make a decision, support or attack an idea, or even election candidate. This year at author profiling tasks series, the new task got a place to convey our concern to stop spreading fake news, a Profiling Fake News Spreaders on Twitter <ref type="bibr" coords="2,431.30,155.18,16.60,8.64" target="#b16">[16]</ref> task. In this task, we aim to identify possible fake news spreaders on social media as a first step towards preventing fake news from being propagated among online users.</p><p>Task: Given a Twitter feed, determine whether its author is keen to be a spreader of fake news.</p><p>The main goal was aimed to investigating if it is possible to discriminate authors that have shared some fake news in the past from those that, to the best of our knowledge, have never done it. Also, this task runs based on a multilingual perspective for English and Spanish languages. The rest of the paper is organized as follows. Section 2 presents related works. Section 3 describes the proposed method. Section 4 describes the performed baselines, experiments, and discusses the obtained results. Finally, section 5 presents our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the fake news challenge (FNC-1) <ref type="bibr" coords="2,285.27,369.51,11.62,8.64" target="#b5">[6]</ref> shared task, studies have been done with 50 participating teams. They performed a detailed feature analysis of participant and concluded that identifying high-performing features for the task yields a new model which mostly rely on the lexical overlap for classification. They believe that this task is challenging since the best performing features are not yet able to resolve difficult cases. Thus, more sophisticated machine learning techniques are needed, which have a deeper semantic understanding. In <ref type="bibr" coords="2,246.67,441.24,15.27,8.64" target="#b18">[18]</ref>, the authors made a study to understand user profiles on social media for fake news detection and proposed a principled way to understand which features of user-profiles are helpful for fake news detection. They concluded that first, there are specific users who are more likely to trust fake news than real news, second, these users reveal different features from those who are more likely to trust real news. These observations showed the importance of feature construction for fake news detection. According to a major study of <ref type="bibr" coords="2,261.59,524.93,11.62,8.64" target="#b5">[6]</ref> and the study of <ref type="bibr" coords="2,345.87,524.93,15.27,8.64" target="#b18">[18]</ref>, we believe, this task is sensitive to feature dimensionality. That is, low-quality features can reduce overall model performance. Feature combination is one of the common actions used to enhance features. In combination methods, different feature vectors are lumped into a single long composite vector, or in addition to the combination of feature vectors, the dimension of feature space is reduced. From an NLP attitude, many methods proposed to employ feature combinations for different studies like fake news challenge. A work done in <ref type="bibr" coords="2,204.54,608.62,16.60,8.64" target="#b21">[21]</ref> has studied false information on Twitter. They found that real tweets contain fewer bias markers, hedges, subjective terms, and less harmful words. They build a model that combined features like graph-based, cues words, and syntax. They concluded, incorporating linguistic features and social network interactions with neural network models improves the classification of suspicious news. However, they are expecting to utilize more sophisticated discourse and pragmatics features and inferring degrees of credibility in their future works. In the work of <ref type="bibr" coords="3,194.50,143.22,10.58,8.64" target="#b6">[7]</ref>, they have used a Long Short-Term Memory (LSTM) network combined with other features such as bag-of-characters (BOC), BOW, and topic model features based on non-negative matrix factorization, Latent Dirichlet Allocation, and Latent Semantic Indexing. They achieved a state-of-the-art result of 60.9% (Macro F1) on the Fake News Challenge (FNC-1) dataset. Similar to this work, at <ref type="bibr" coords="3,398.68,191.04,10.58,8.64" target="#b3">[4]</ref>, an approach was presented that combines lexical, word embeddings, and n-gram features to detect the stance in fake news. Their approach has been tested on the FNC-1 dataset and achieved an accuracy of 59.6% (Macro F1) close to state-of-the-art results using a simple feature representation. Mainly approaches at Fake News Challenge (FNC-1) dataset incorporated a different combination of features, such as word or character n-grams, bag-ofwords, word embeddings, latent semantic analysis features <ref type="bibr" coords="3,371.14,262.77,16.60,8.64" target="#b17">[17]</ref>  <ref type="bibr" coords="3,390.23,262.77,10.58,8.64" target="#b7">[8]</ref>. At another work <ref type="bibr" coords="3,202.59,274.73,15.27,8.64" target="#b13">[13]</ref>, they have used a set of linguistic features like n-grams, punctuation, psycholinguistic, readability, and syntax features. The proposed linguistics-driven approach suggests that to differentiate between fake and genuine content it is worthwhile to look at the lexical, syntactic, and semantic level of a news item in question. They have achieved an accuracy of up to 76% in their own collected dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>We assume authors may convey different concepts when they are tweeting, so differences in concepts can capture fake/real news. Since fake news spreaders can be very smart or complicate their semantic of tweet concepts highly keen to be real but in a different style than usual. According to <ref type="bibr" coords="3,295.02,425.34,16.60,8.64" target="#b15">[15]</ref> coarse-grained features are most likely to find author's styles. So, taking author fingerprinted features into account can be useful in the case of finding author styles. To construct a hypothesis, lets (X i , y i ) be the definition of each user tweets. X i refers to user i tweets. y i describes fake/real news spreader. Suppose i ∈ [1, m], j ∈ [1, n] and m, n be the maximum numbers of users, and each user tweets, respectively. We can define X i = ∪ n j=1 γ j in which γ j refers to array of words which belongs to j-th tweet's for user i-th, and γ k j is the k-th word of array γ j with length of |γ j |. In the following, we will use these notations to introduce our proposed approach in more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Preprocessing</head><p>In the first stage of preprocessing, we used Preprocessor<ref type="foot" coords="3,362.26,581.71,3.49,6.05" target="#foot_0">1</ref> which is a preprocessing library for tweet data written in Python. It used to remove URLs, Hashtags, Mentions, Reserved words (RT, FAV), Emojis, Smileys, and Numbers from X i even those that already masked in the dataset. Next, punctuation removal, stopwords removal, and stemming applied to ∀γ k j , k ∈ |γ j | using NLTK 3.0 Toolkit <ref type="bibr" coords="3,356.67,631.20,10.58,8.64" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Representation Methods</head><p>I. ConceptNet Numberbatch Regarding word embeddings that represent only distributional semantics like Word2Vec or GloVe and word embedding that represent only relational knowledge like ConceptNet, ConceptNet Numberbatch is a hybrid word embedding built using an ensemble approach. It combines data from ConceptNet, Word2Vec, GloVe, and OpenSubtitles 2016 using a variation on retrofitting <ref type="bibr" coords="4,389.66,184.80,15.27,8.64" target="#b19">[19]</ref>.</p><p>-ConceptNet <ref type="bibr" coords="4,204.59,201.91,16.60,8.64" target="#b19">[19]</ref> is a knowledge graph that connects words and phrases of natural language with labeled edges. ConceptNet sources include symmetric and asymmetric relations. Its knowledge is collected from many sources that include expertcreated resources, crowd-sourcing, and games with a purpose. It is designed to allow the applications to better understand the meanings behind the words people use <ref type="bibr" coords="4,167.47,261.68,15.27,8.64" target="#b19">[19]</ref>. -GloVe <ref type="bibr" coords="4,181.32,273.08,16.60,8.64" target="#b12">[12]</ref> is a vector space with meaningful substructure which pre-trained on various datasets. -Word2Vec <ref type="bibr" coords="4,198.38,296.42,16.60,8.64" target="#b10">[11]</ref> is a word vectors pre-trained on the Google News dataset.</p><p>-OpenSubtitles 2016 <ref type="bibr" coords="4,238.57,307.81,16.60,8.64" target="#b20">[20]</ref> is a collection of movie subtitles and used as a part of meta data for training ConceptNet Numberbatch.</p><p>ConceptNet Numberbatch is a multilingual word embedding and represents 78 different languages in 300 dimensions. Words in different languages share a common semantic space, and that semantic space is informed by all of the languages. The f is a representation of semantic space.</p><formula xml:id="formula_0" coords="4,267.86,390.01,79.14,12.69">f : W ord -→ V 1 300</formula><p>In this work, we used ConceptNet Numberbatch version 19.08, and a vocabulary size of 651859 for Spanish, and 516782 for English. Γ uses f to represent word vectors for both words in numberbatch vocabulary and OOV words.</p><formula xml:id="formula_1" coords="4,234.98,449.36,144.21,30.25">Γ (word) = - → f (word), word ∈f - → 0 , word / ∈f</formula><p>Finally, CoN et is a formulation of how we extract averaged feature vectors for X i .</p><p>CoN et :</p><formula xml:id="formula_2" coords="4,242.15,505.24,166.95,41.72">∪ n j=1 γ j -→ n j=1 |γj | k=1 - → Γ (γ k j ) 2 - → Γ (γ k j )• - → Γ (γ k j ) n j=1 |γ j |</formula><p>We skipped stemming in the preprocessing stage for given γ k j due to low accuracy achieved in our experiments. Investigations showed stemming decreases word usage frequency in the data and it leads to poor CoN et vectors. <ref type="bibr" coords="4,289.46,608.62,11.62,8.64" target="#b8">[9]</ref> is a statistical approach to extract relations among words by meaning of their contexts of use in documents. LSA can be accomplished by applying a low-rank Singular Value Decomposition (SVD) on the N-grams/TF-IDF matrices to reduce the number of rows while preserving the similar structure among columns. -N-grams with TF-IDF weighting <ref type="bibr" coords="5,292.27,326.02,15.27,8.64" target="#b9">[10]</ref>, it works by determining the relative frequency of words in a specific document compared to the inverse proportion of that word over the entire document corpus. Words that are common in documents tend to have higher TF-IDF numbers than others. TF-IDF matrix representation of documents presents fingerprinted features for documents. TF-IDF is trained for transforming data into matrix of M tf idf .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Latent Semantic Analysis (LSA)</head><formula xml:id="formula_3" coords="5,278.31,405.49,75.18,12.69">M tf idf : X i → V i d</formula><p>LSA is dimension reduction which is able to capture and represent significant components of the lexis and passage meanings. Also, this has the effect of reducing noise in the data as well as reducing the sparseness of the matrix. From these perspectives, we applied SVD to N-grams and TF-IDF matrices for dimensional reduction with a component number of 200. SVD is a formulation of dimension reduction for our case. SVD is a transformer of M tf idf and M ngram to latent space.</p><formula xml:id="formula_4" coords="5,269.38,509.54,76.10,12.69">SV D : V i d → V i 200</formula><p>We used scikit-learn <ref type="bibr" coords="5,218.35,534.02,11.62,8.64" target="#b1">[2]</ref> python library for our experiments and training N-grams models for both languages. Experimental searches have been done for tuning N-grams and TF-IDF parameters using a 5 and 10 fold cross-validation. Table <ref type="table" coords="5,410.41,557.93,4.98,8.64" target="#tab_0">1</ref> shows a summary of the best achieved parameters for both languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Input Representation</head><p>According to our experiments, single representations mainly are not able to perform well after achieving specific accuracy due to their features overlaps and similarities. We will discuss it in more detail. Regarding the hypothesis of combining, weak learners can boost performance. We hypothesis that combining representations must do the same in most of the cases. To overcome single representation issues and to keep representation combination simple, LSACoNet has been introduced as a concatenation of representations. The ∆ is a transformer which is able to represent a combination of feature vectors for given user tweets in 700 dimensions.</p><formula xml:id="formula_5" coords="6,166.62,176.33,282.12,38.66">∆ : X i -→ (CoN et(X i ), SV D(M tf idf (X i )), SV D(M ngram (X i ))) ∆(X i ) ∈ V i<label>700</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Architecture</head><p>A fully connected feed-forward neural network <ref type="bibr" coords="6,325.45,262.61,11.62,8.64" target="#b4">[5]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>This year, task organizers have provided a training corpus<ref type="foot" coords="6,364.42,444.39,3.49,6.05" target="#foot_1">2</ref> . The corpus is composed of documents in English and Spanish, where each document contains 100 tweets for each author. The statistics of this corpus are presented in Table <ref type="table" coords="6,366.12,469.97,3.74,8.64" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>In order to compare the proposed methods, we implemented 3 baselines as described in bellow, and Table <ref type="table" coords="6,217.46,539.85,4.98,8.64" target="#tab_4">3</ref> (in group 0 for detailed experimental result) shows detailed evaluation results for them.</p><p>-RANDOM: a random prediction model predicts 1 if random value ∈ [0, 0.5] else 0. -TFIDFLSVM: TF-IDF representation contains all words without applying preprocessing and parameter tuning, and linear SVM as a classifier with C = 1. -STATLSVM: includes statistical features like number of characters, URLs, Mentions, Hashtags, RTs, and Emojis with linear SVM as a classifier with C = 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>We conducted a few experiments with different classifiers (Multi-layer Perceptron, Linear/RBF SVM, Logistic Regression, K Nearest Neighbors, Naive Bayes, Ridge classifier -a classifier using ridge regression, Stacking Ensemble), and different representations(N-gram, TF-IDF, LSA, ConceptNet Numberbatch). The differences between experiments are mainly focused on 5/10-fold cross-validation mean accuracy and confidence interval(CI). Most of the models in experiments were suffering from a hight confidence interval. We essentially concentrated on reducing the overfitting impact by reviewing confidence intervals, while boosting model performance on validations using 5/10-fold cross-validation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1: TF-IDF Modeling</head><p>In Experiment 1 we used TF-IDF representation using word usage factor while making a vocabulary for representation. With word usage factor we were able to use the author's fingerprinted words as a representation with ignoring less and most used words with setting lower/upper bound threshold to each term frequency. We used a lower/upper bound term frequency thresholds for both languages. The lower/upper bound term frequency threshold includes 2/2000 for English and 3/4000 for Spanish. In final, terms fall in the range of [L tf , U tf ] considered in making TF-IDF vocabulary. Attained results for this experiment is recorded in Table <ref type="table" coords="7,206.72,544.08,4.98,8.64" target="#tab_4">3</ref> (in the section for detailed experimental results using cross-validation) group 1. We achieved CI close to 0.05 by applying a linear SVM classifier. The ridge classifier also achieved average accuracy result close to linear SVM, however, this model suffers from a high CI.</p><p>Experiment 2: Character N-gram Modeling In Experiment 2 similar to the previous analysis, we have run an investigation with character n-gram representation to explore for better features by keeping only the author's most valuable words. We used a character 3-grams scheme using word usage factor while making vocabulary for representation. Less valuable terms were ignored from the vocabulary by setting a lower bound term frequency threshold of 5 for both languages. In final, terms fall in the range of [L tf , ∞) considered in making representation vocabulary. Accomplished results for this experiment were recorded in Table <ref type="table" coords="8,335.23,143.22,4.98,8.64" target="#tab_4">3</ref> group 2. Presented results are not very promising due to high CI and low accuracy regarding previous experiment models. Most importantly averaged results and CIs are close to baseline models except 2 cases and they are mostly suffering from high CI. More investigations revealed that for Spanish, logistic regression, and ridge classifiers are running well, however, for English, they are performing very low regarding baseline and group-1 models.</p><p>According to the results, character n-gram representation fails in capturing fake/real news spreaders.</p><p>Experiment 3: Punctuation/Character N-gram Modeling In Experiment 3 we considered another study with character 5-grams with considering only marks. We replaced letters in tweets with *. Next, we used the experiment 2 details for modeling logistic regression and linear SVM. Recorded results in Table <ref type="table" coords="8,382.30,293.69,4.98,8.64" target="#tab_4">3</ref> group 3 for both classifiers confirms that extracting character n-gram features could be hard for models to capture fake/real news spreaders due to poor features.</p><p>Experiment 4: Ensemble Learning In Experiment 4, we investigated combining weak learners by applying a stacking ensemble approach with a majority voting scheme. TF-IDF representation using linear SVM, k nearest neighbors, and ridge classifiers were considered for English, and at Spanish, only third learner changed to character 3-gram representation with logistic regression classifier. We achieved accuracies of 0.768/0.764 for averaged 5/10-fold cross-validation respectively. It outperforms current models, however, it suffers from high CI for English. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Final Evaluation</head><p>Following the previous results, for the final evaluation at TIRA platform <ref type="bibr" coords="9,424.73,276.15,15.27,8.64" target="#b14">[14]</ref>, we applied LSACoNet method with FCNN for the classification of real/fake news spreaders. The obtained accuracy results for the final evaluation were as follows: in Spanish, 0.745; in English, 0.725; and 0.735 for both tasks. The official results are shown in Table <ref type="table" coords="9,197.64,323.97,4.98,8.64" target="#tab_4">3</ref> (in detailed results of submissions) for early birds and final evaluation. We gained a better result with LSACoNet and FCNN for English at the final evaluation. However, for Spanish TF-IDF representation with linear SVM performed well with an accuracy of 0.765 at early birds evaluation. In the final evaluation metrics, the best scores of the submissions between the early birds and final submissions of each participant and each language have been considered. This means that in our case we achieved the best score for Spanish in early bird and the best score for English in the final submission so, overall achieved accuracy is 0.745.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we proposed a model for Profiling Fake News Spreader on the Twitter task in PAN 2020. We presented a feature combination model namely LSACoNet to use a different representation of the documents to incorporate with FCNN on detecting fake/real news spreaders on Twitter. In the final, we achieved average accuracy of 0.745. Regarding our manual evaluation, our approach is very capable of distinguishing fake/real news spreaders. In future works, we most likely to try to add feature weighting for representations and use different deep neural network models like RNN and cleverly emotionalized word or character n-gram features to enrich current features to boost the performance of currently existed representation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,141.74,115.83,335.95,202.21"><head>Table 1 .</head><label>1</label><figDesc>Optimized parameters for TF-IDF and N-gram</figDesc><table coords="5,180.95,137.15,250.50,106.39"><row><cell cols="2">Language Model</cell><cell>ngram_range</cell><cell>max_features</cell><cell>norm</cell><cell>Parameters strip_accents max_df</cell><cell>sublinear_tf</cell><cell>stop_words</cell></row><row><cell>English</cell><cell cols="7">TFIDF (2,4) 3000 l2 False 0.7 True english N-gram (1,3) 3000 -False 0.7 --</cell></row><row><cell>Spanish</cell><cell cols="7">TFIDF (3,4) 6000 l2 False 0.2 True spanish N-gram (1,3) 6000 -False 0.2 -spanish</cell></row></table><note coords="5,141.74,271.22,335.95,9.03;5,151.70,283.57,324.94,8.64;5,151.70,295.20,76.31,9.65;5,275.13,307.42,51.09,9.65;5,327.00,305.35,2.82,6.12;5,333.09,307.42,18.54,8.74;5,353.84,305.35,2.82,6.12;5,351.63,311.92,4.15,6.12"><p><p>-N-grams, it converts a collection of text documents to a matrix of token counts as a term frequency(TF) representation. N-gram is trained for transforming data into matrix of</p>M ngram . M ngram : X i → V i d</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,262.61,341.25,128.19"><head></head><label></label><figDesc>(namely FCNN) introduced to tackle fake/real news spreader detection challenge. Proposed FCNN contains an input layer with 1024 neurons, ReLU activation, dropout, and BatchNormalization. Next, FCNN follows 3 hidden layers, each holding 256, 128, and 64 neurons respectively with sigmoid activation and an output layer with 2 neurons, and BatchNormalization.</figDesc><table /><note coords="6,134.77,322.39,333.85,8.64;6,134.77,334.34,331.15,8.64;6,134.77,346.30,325.46,8.64;6,134.77,358.25,335.60,8.64;6,134.77,370.21,339.51,8.64;6,134.77,382.16,260.39,8.64"><p><p><p>At the input layer, BatchNormalization set to normalize the combined features from different representations. To reduce thinking of the network, dropout has been used with a probability of 40% at the input layer. To compile network spars categorical cross-entropy, loss function has been utilized. As an optimizer, Adam applied with a learning rate of 0.002. The process of experimenting with Deep Neural Networks has been done using Keras</p><ref type="bibr" coords="6,227.48,382.16,11.62,8.64" target="#b2">[3]</ref> </p>a deep learning API written in Python.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,134.77,115.83,345.82,162.89"><head>Table 2 .</head><label>2</label><figDesc>Dataset Statistics: Due to privacy reasons URLs, Mentions, and Hashtags are replaced with their specific tags starts/ends with #</figDesc><table coords="7,143.70,148.10,321.78,130.61"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Average Per User</cell><cell></cell></row><row><cell cols="3">Dataset Language Class User No.</cell><cell>Tweets No.</cell><cell>Chars</cell><cell>Emojis</cell><cell>#URL#</cell><cell>RTs</cell><cell>#USER#</cell><cell>#HASHTAG#</cell><cell>Test</cell></row><row><cell></cell><cell>1</cell><cell>150</cell><cell cols="2">100 8864</cell><cell cols="3">4 114.5 8</cell><cell cols="2">15.5 32</cell><cell>100</cell></row><row><cell>Train English</cell><cell>0</cell><cell>150</cell><cell cols="7">100 8902 12 110.5 16 38.5 46</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell>300</cell><cell cols="2">100 8882</cell><cell>8</cell><cell cols="2">112 12</cell><cell>27</cell><cell>39</cell><cell>200</cell></row><row><cell></cell><cell>1</cell><cell>150</cell><cell cols="3">100 9113 12</cell><cell>93</cell><cell>15</cell><cell>40</cell><cell>11</cell><cell>100</cell></row><row><cell>Train Spanish</cell><cell>0</cell><cell>150</cell><cell cols="4">100 11136 28.5 73</cell><cell>29</cell><cell>71</cell><cell>39</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell>300</cell><cell cols="3">100 10124 20</cell><cell>83</cell><cell cols="3">22 55.5 25</cell><cell>200</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.77,450.76,345.82,214.31"><head></head><label></label><figDesc>Experiment 5: Concept Modeling In Experiment 5, we examined linear/rbf SVM and logistic regression classifiers with ConceptNet Numberbatch word embedding. Obtained results are reported in Table3, group 5. Results showed Numberbatch is mostly likely to perform similar to TF-IDF representation.Experiment 6: Concatenation of FeaturesIn Experiment 6, we analyze LSACoNet representations with linear SVM. For analysis, we made a baseline without any specific parameter setting and using maximum feature dimensions. Interestingly we achieved a low CI for both languages with this baseline. It showed how combination of features are capable. Next, LSACoNet representations were evaluated based on the parameter setting mentioned in Table1. Obtained results showed feature combination is a very powerful technique for boosting performance. We reached accuracies of 0.785/0.765 for 5/10-fold cross-validation and lowest possible CI. Detailed results have been recorded in Table3in group 6.Experiment 7: FCNN In Experiment 7, we made a different analysis using LSACoNet representation and CoNet representation. To make conclusions about if FCNN is able to perform better than the models described in previous experiments, CoNet representation is considered as a baseline. We used 5 different test split sizes for this experiment to evaluate LSACoNetFCNN and CoNetFCNN models. Obtained result from this experiment were recorded in Table1(in detailed experimental results with FCNN). We gained average accuracy of 0.79 for LSACoNetFCNN. Both results in experiment 6, and 7 are very promising, and comparing accuracies and CIs of these 2 experiments are not an interesting job to do because of differences in evaluations. Both LSACoNetLSVM and LSACoNetFCNN models are very promising and since for final evaluation, we didn't have any test set to compare these 2 models we simply relied on LSACoNetFCNN as a final model.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,134.77,136.96,345.83,503.07"><head>Table 3 .</head><label>3</label><figDesc>Detailed experimental results using Cross-Validation (5CV/10CV = 5/10-fold Cross Validation), FCNN with test size of 0.1, 0.2, 0.3, 0.4, 0.5 percent from training-set, and Detailed results of submissions. Also, models tagged with &lt; en &gt; and &lt; es &gt; are chosen models for early birds submission. Moreover, tag &lt; BS &gt; was used to show baseline models.CoNetFCNN&lt;BS&gt; en 0.70 0.63 0.71 0.66 0.65 0.67 0.060 FCNN baseline CoNetFCNN &lt;BS&gt; es 0.83 0.80 0.74 0.74 0.79 0.78 0.070 FCNN baseline LSACoNetFCNN en 0.76 0.81 0.80 0.79 0.75 0.78 0.044 Final Model LSACoNetFCNN es 0.80 0.80 0.80 0.81 0.82 0.80 0.018 Final Model Detailed Results of Submissions</figDesc><table coords="10,138.01,190.76,337.40,449.27"><row><cell></cell><cell cols="6">Detailed Experimental Results using Cross-Validation</cell></row><row><cell>Group</cell><cell>Model</cell><cell cols="5">English Mean CI Mean CI Mean CI Mean CI Spanish 5-CV 10CV 5CV 10CV</cell><cell>Average 5CV 10CV</cell></row><row><cell></cell><cell cols="6">RANDOM &lt;BS&gt; 0.560 0.090 0.493 0.159 0.513 0.082 0.539 0.183 0.536 0.516</cell></row><row><cell>0</cell><cell cols="6">TFIDFLSVM &lt;BS&gt; 0.666 0.109 0.633 0.186 0.760 0.108 0.766 0.149 0.710 0.699</cell></row><row><cell></cell><cell cols="6">STATLSVM &lt;BS&gt; 0.613 0.108 0.613 0.169 0.723 0.050 0.720 0.120 0.668 0.666</cell></row><row><cell></cell><cell>LSVM &lt;es&gt;</cell><cell cols="5">0.710 0.068 0.693 0.122 0.820 0.048 0.826 0.077 0.765 0.758</cell></row><row><cell></cell><cell>LSALSVM</cell><cell cols="5">0.700 0.105 0.679 0.133 0.823 0.054 0.816 0.095 0.761 0.745</cell></row><row><cell></cell><cell>KNN</cell><cell cols="5">0.683 0.096 0.700 0.073 0.756 0.074 0.740 0.106 0.719 0.720</cell></row><row><cell>1</cell><cell>LSAKNN MLP</cell><cell cols="5">0.613 0.108 0.653 0.169 0.720 0.032 0.720 0.112 0.666 0.686 0.696 0.133 0.679 0.155 0.806 0.054 0.799 0.119 0.751 0.739</cell></row><row><cell></cell><cell>NB</cell><cell cols="5">0.706 0.088 0.693 0.135 0.736 0.104 0.743 0.115 0.721 0.718</cell></row><row><cell></cell><cell>RIDGE</cell><cell cols="5">0.713 0.104 0.700 0.115 0.820 0.053 0.840 0.083 0.766 0.770</cell></row><row><cell></cell><cell>LSARIDGE</cell><cell cols="5">0.696 0.082 0.676 0.094 0.806 0.061 0.826 0.071 0.751 0.751</cell></row><row><cell></cell><cell>LR</cell><cell cols="5">0.650 0.063 0.663 0.194 0.823 0.033 0.790 0.099 0.736 0.726</cell></row><row><cell>2</cell><cell>LSALR KNN</cell><cell cols="5">0.603 0.099 0.673 0.145 0.776 0.108 0.766 0.103 0.689 0.719 0.550 0.124 0.600 0.163 0.743 0.140 0.753 0.149 0.646 0.676</cell></row><row><cell></cell><cell>LSAKNN</cell><cell cols="5">0.543 0.071 0.586 0.195 0.750 0.119 0.753 0.176 0.646 0.669</cell></row><row><cell></cell><cell>RIDGE</cell><cell cols="5">0.656 0.071 0.696 0.182 0.820 0.038 0.806 0.139 0.738 0.751</cell></row><row><cell></cell><cell>LSARIDGE</cell><cell cols="5">0.636 0.176 0.646 0.221 0.776 0.045 0.780 0.133 0.706 0.713</cell></row><row><cell>3</cell><cell>LR LSVM</cell><cell cols="5">0.610 0.093 0.636 0.205 0.753 0.116 0.750 0.104 0.681 0.693 0.610 0.112 0.636 0.164 0.723 0.143 0.723 0.115 0.666 0.679</cell></row><row><cell>4</cell><cell>ensemble</cell><cell cols="5">0.713 0.092 0.696 0.117 0.823 0.049 0.833 0.073 0.768 0.764</cell></row><row><cell></cell><cell>LSVM &lt;en&gt;</cell><cell cols="5">0.683 0.101 0.669 0.128 0.726 0.045 0.730 0.109 0.704 0.699</cell></row><row><cell>5</cell><cell>RBFSVM</cell><cell cols="5">0.706 0.074 0.706 0.118 0.779 0.082 0.773 0.162 0.742 0.739</cell></row><row><cell></cell><cell>LR</cell><cell cols="5">0.713 0.106 0.713 0.149 0.733 0.059 0.736 0.113 0.723 0.724</cell></row><row><cell>6</cell><cell>LSVM &lt;BS&gt; LSVM</cell><cell cols="5">0.673 0.029 0.690 0.058 0.770 0.025 0.810 0.060 0.721 0.750 0.750 0.037 0.730 0.068 0.820 0.022 0.800 0.061 0.785 0.765</cell></row><row><cell></cell><cell></cell><cell cols="5">Detailed Experimental Results with FCNN</cell></row><row><cell></cell><cell>Model</cell><cell>Lang</cell><cell>0.1</cell><cell cols="3">Accuracy with Test Split Sizes 0.2 0.3 0.4 0.5 Mean CI</cell><cell>Description</cell></row><row><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Submission Type</cell><cell cols="2">English Model</cell><cell>Accuracy</cell><cell>Spanish Model</cell><cell>Accuracy</cell><cell>Average</cell></row><row><cell>8</cell><cell cols="6">Early Birds Submission CoNetLSVM Final Submission LSACoNetFCNN 0.725 LSACoNetFCNN 0.745 0.735 0.685 TFIDFLSVM 0.765 0.725</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,657.08,119.44,7.77"><p>https://github.com/s/preprocessor</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.73,657.08,144.05,7.77"><p>https://doi.org/10.5281/zenodo.3692319</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.61,613.32,336.34,7.77;9,150.95,624.28,213.27,7.77" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,257.69,613.32,221.26,7.77;9,150.95,624.28,97.40,7.77">Natural language processing with Python: analyzing text with the natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,635.17,328.23,7.77;9,150.95,646.13,312.99,7.77;9,150.95,657.08,329.21,7.77;11,150.95,119.96,321.77,7.77;11,150.95,130.92,70.98,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,207.73,657.08,272.43,7.77;11,150.95,119.96,23.75,7.77">API design for machine learning software: experiences from the scikit-learn project</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Buitinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Grobler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Layton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,192.54,119.96,276.34,7.77">ECML PKDD Workshop: Languages for Data Mining and Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="108" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,141.20,176.67,7.77" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<title level="m" coord="11,215.00,141.20,19.11,7.77">Keras</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,151.48,311.29,7.77;11,150.95,162.44,324.89,7.77;11,150.95,173.40,100.86,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,276.77,151.48,177.13,7.77;11,150.95,162.44,49.50,7.77">Stance detection in fake news a combined feature representation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,218.44,162.44,257.41,7.77;11,150.95,173.40,32.03,7.77">Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</title>
		<meeting>the First Workshop on Fact Extraction and VERification (FEVER)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,183.69,282.80,7.77;11,150.95,194.64,120.79,7.77" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<title level="m" coord="11,301.41,183.69,51.93,7.77">Deep Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,204.93,308.53,7.77;11,150.95,215.89,324.46,7.77;11,150.95,226.84,301.09,7.77;11,150.95,237.80,326.90,7.77;11,150.95,248.76,187.09,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,200.53,215.89,259.34,7.77">A retrospective analysis of the fake news challenge stance-detection task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanselowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pvs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Caspelherr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C18-1158" />
	</analytic>
	<monogr>
		<title level="m" coord="11,150.95,226.84,284.13,7.77">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-08">Aug 2018</date>
			<biblScope unit="page" from="1859" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,259.04,313.11,7.77;11,150.95,270.00,311.51,7.77;11,150.95,280.96,224.94,7.77" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,200.53,270.00,258.60,7.77">A retrospective analysis of the fake news challenge stance detection task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanselowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P V</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caspelherr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
		<idno>CoRR abs/1806.05180</idno>
		<ptr target="http://arxiv.org/abs/1806.05180" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,291.25,318.85,7.77;11,150.95,302.20,286.06,7.77;11,150.95,313.16,112.61,7.77" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,341.75,291.25,119.72,7.77;11,150.95,302.20,173.72,7.77">We built a fake news &amp; click-bait filter: What happened next will blow your mind!</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gencheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Koychev</surname></persName>
		</author>
		<idno>CoRR abs/1803.03786</idno>
		<ptr target="http://arxiv.org/abs/1803.03786" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,323.45,308.69,7.77;11,150.95,334.40,163.60,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,295.40,323.45,152.45,7.77">An introduction to latent semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">W</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,150.95,334.40,72.46,7.77">Discourse processes</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="259" to="284" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,344.69,299.49,7.77;11,150.95,355.65,149.32,7.77" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="11,306.20,344.69,132.06,7.77">Introduction to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,365.93,334.74,7.77;11,150.95,376.89,321.23,7.77;11,150.95,387.85,305.81,7.77;11,150.95,398.81,101.62,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,370.57,365.93,106.40,7.77;11,150.95,376.89,160.55,7.77">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,299.61,387.85,157.15,7.77;11,150.95,398.81,29.89,7.77">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,254.82,398.81,112.05,7.77;11,150.95,409.76,313.48,7.77;11,150.95,420.72,93.13,7.77" xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Inc</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,431.01,331.86,7.77;11,150.95,441.96,291.60,7.77;11,150.95,452.92,161.38,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,306.76,431.01,163.80,7.77">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,163.16,441.96,279.39,7.77;11,150.95,452.92,74.37,7.77">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,463.21,318.75,7.77;11,150.95,474.16,167.17,7.77" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07104</idno>
		<title level="m" coord="11,360.82,463.21,100.17,7.77;11,150.95,474.16,16.35,7.77">Automatic detection of fake news</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.24,484.45,335.40,7.77;11,150.95,495.41,322.35,7.77;11,150.95,506.37,178.85,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,333.86,484.45,140.16,7.77">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,259.53,495.41,213.78,7.77;11,150.95,506.37,100.44,7.77">Information Retrieval Evaluation in a Changing World. The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09">Sep 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,516.65,338.35,7.77;11,150.95,527.61,329.64,7.77;11,150.95,538.57,319.68,7.77;11,150.95,549.52,268.42,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,430.28,516.65,50.31,7.77;11,150.95,527.61,313.30,7.77">Cross-domain Authorship Attribution: Author Identification using a Multi-Aspect Ensemble Approach</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rahgouy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Giglou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rahgooy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sheykhlan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mohammadzadeh</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="11,349.07,538.57,117.33,7.77">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="11,150.95,549.52,85.63,7.77">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019-09">Sep 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,559.81,323.93,7.77;11,150.95,570.77,325.30,7.77;11,150.95,581.73,327.13,7.77;11,150.95,592.68,183.41,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,333.29,559.81,132.88,7.77;11,150.95,570.77,218.44,7.77">Overview of the 8th Author Profiling Task at PAN 2020: Profiling Fake News Spreaders on Twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="11,264.50,581.73,213.58,7.77;11,150.95,592.68,82.71,7.77">CLEF 2020 Labs and Workshops, Notebook Papers. CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020-09">Sep 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,602.97,310.15,7.77;11,150.95,613.93,320.84,7.77;11,150.95,624.88,112.61,7.77" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="11,355.69,602.97,96.70,7.77;11,150.95,613.93,205.17,7.77">A simple but tough-to-beat baseline for the fake news challenge stance detection task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">P</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno>CoRR abs/1707.03264</idno>
		<ptr target="http://arxiv.org/abs/1707.03264" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,635.17,315.89,7.77;11,150.95,646.13,324.04,7.77;11,150.95,657.08,103.36,7.77" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,249.86,635.17,208.27,7.77;11,150.95,646.13,31.60,7.77">Understanding user profiles on social media for fake news detection</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,200.51,646.13,274.48,7.77;11,150.95,657.08,25.83,7.77">2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="430" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,119.96,318.76,7.77;12,150.95,130.92,125.79,7.77;12,150.95,141.88,228.10,7.77" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="12,262.27,119.96,198.73,7.77;12,150.95,130.92,37.23,7.77">ConceptNet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
		<ptr target="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4444" to="4451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,152.84,329.19,7.77;12,150.95,163.80,297.20,7.77;12,150.95,174.76,323.99,7.77;12,150.95,185.71,316.19,7.77;12,150.95,196.67,20.92,7.77" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,205.92,152.84,145.36,7.77">Parallel data, tools and interfaces in opus</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,150.95,174.76,323.99,7.77;12,150.95,185.71,230.30,7.77">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC&apos;12). European Language Resources Association (ELRA)</title>
		<editor>
			<persName><forename type="first">)</forename><surname>Chair</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">C C</forename><surname>Choukri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Declerck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Dogan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Maegaard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Mariani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Odijk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Piperidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename></persName>
		</editor>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC&apos;12). European Language Resources Association (ELRA)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,207.63,320.56,7.77;12,150.95,218.59,325.29,7.77;12,150.95,229.55,300.20,7.77;12,150.95,240.51,102.22,7.77" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,319.44,207.63,143.36,7.77;12,150.95,218.59,222.78,7.77">Separating facts from fiction: Linguistic models to classify suspicious and trusted news posts on twitter</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shaffer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hodas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,391.33,218.59,84.92,7.77;12,150.95,229.55,236.35,7.77">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="647" to="653" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
