<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,159.60,115.90,296.16,12.90;1,276.73,133.83,61.90,12.90;1,223.43,153.82,168.50,10.75">Ensemble of ELECTRA for Profiling Fake News Spreaders Notebook for PAN at CLEF 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.87,190.35,76.36,8.64"><forename type="first">Kaushik</forename><forename type="middle">Amar</forename><surname>Das</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Information Technology</orgName>
								<address>
									<settlement>Guwahati</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.68,190.35,51.74,8.64"><forename type="first">Arup</forename><surname>Baruah</surname></persName>
							<email>arup.baruah@gmail.comferdous@iiitg.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Information Technology</orgName>
								<address>
									<settlement>Guwahati</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.87,190.35,107.92,8.64"><forename type="first">Ferdous</forename><forename type="middle">Ahmed</forename><surname>Barbhuiya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Information Technology</orgName>
								<address>
									<settlement>Guwahati</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,423.11,190.35,45.91,8.64"><forename type="first">Kuntal</forename><surname>Dey</surname></persName>
							<email>kuntal.dey@accenture.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Accenture Tech Labs</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,159.60,115.90,296.16,12.90;1,276.73,133.83,61.90,12.90;1,223.43,153.82,168.50,10.75">Ensemble of ELECTRA for Profiling Fake News Spreaders Notebook for PAN at CLEF 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">810C225AD34D8A895CC450A3FBA6078D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an ensemble classifier that uses ELECTRA models for the task of identifying possible Fake News Spreaders on Twitter in PAN at CLEF 2020 lab. Our ensemble is created using 15 models which have been finetuned on the task dataset. Our approach scored an accuracy of 0.70 and 0.69 on the English and Spanish test sets respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fake news is a form of news that is circulated with the aim of deceiving users and manipulating them into formulating specific opinions. With the growth of social media platforms such as Facebook and Twitter, it is now easier than ever to spread fake news. This problem is aggravated further when users knowingly or unknowingly share articles that contain false or misleading information.</p><p>There exist numerous sites that use expert analysis to fact check and debunk fake articles, such as snopes.com, politifact.com etc. The problem of fake news has also been actively tackled by the research community. To list a few, the works in <ref type="bibr" coords="1,415.78,483.55,11.45,8.64" target="#b4">[5,</ref><ref type="bibr" coords="1,427.23,483.55,7.64,8.64" target="#b5">6]</ref> studied the incorporation of emotional features into Long Short Term Memory (LSTM) network for detecting fake news. The authors in <ref type="bibr" coords="1,291.91,507.46,16.60,8.64" target="#b8">[10]</ref> introduced a system called DeClarE which combines evidence collected from the web, language style and trustworthiness of the sources for analysing the credibility of claims in textual form. The work in <ref type="bibr" coords="1,437.47,531.37,16.60,8.64">[17]</ref> investigated the use of user profiles as potential features for improving fake news detection systems.</p><p>With an aim to further investigate this problem, PAN at CLEF'20 introduced the task of Profiling Fake News Spreaders on Twitter <ref type="bibr" coords="1,336.48,579.32,15.27,8.64" target="#b11">[13]</ref>. The objective of this task is to identify whether a Twitter user is a possible fake news spreader given a collection of his tweets. This task is available in English and Spanish. We participated in this task in both languages.</p><p>The rest of this paper is organised as follows. First, the dataset for the task is discussed in ยง2. Our approach is described in ยง3. The performance of our classifier is analysed in ยง4 before concluding in ยง5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>The dataset <ref type="bibr" coords="2,183.90,214.61,16.60,8.64" target="#b13">[15]</ref> for the task of Profiling Fake News Spreaders is featured in two languages: English and Spanish. The number of data samples in each of these two datasets are the same. Each contains 300 authors out of which 150 are labelled as possible fake news spreaders while the rest are labelled as not spreaders. A collection of 100 tweets is given for each of these authors in which we trained our classification systems. The dataset is perfectly balanced as illustrated in Figure <ref type="figure" coords="2,370.12,274.38,3.74,8.64">1</ref>. Additionally, the dataset is anonymized <ref type="bibr" coords="2,195.90,286.34,16.60,8.64" target="#b12">[14]</ref> to protect the tweet author's privacy. As such, identifiers like user handles and URLs have been replaced with'#USER#' and '#URL#' tokens respectively. Some examples of the data are given in Figure <ref type="figure" coords="2,319.65,310.25,3.74,8.64">2</ref>. Some other noteworthy features of the dataset are listed below.</p><p>-By counting the number of unique tweets within the collection of 100 tweets given for each author, we found that overall, only 343 authors have all unique tweets. For each language, about half of the authors had some duplicates. -In the entire dataset, the shortest tweet has 1 word while the longest tweet has 86 words. The tweets contained an average of around 15 words. -While many authors did not use any emojis, 284 to be exact, the rest used emojis in at least one of their tweets.</p><p>We do not know anything about the test set since our models were evaluated using the TIRA system <ref type="bibr" coords="2,210.72,459.78,15.27,8.64" target="#b9">[11]</ref>. TIRA uses blind evaluation, a paradigm in which it runs our models on a hidden test set without exposing any information about it to the participants. We submitted our models adhering to the guidelines given by the organizers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our approach involves an ensemble classifier built using the ELECTRA model <ref type="bibr" coords="3,450.99,222.93,10.58,8.64" target="#b2">[3]</ref>. We chose this model because of it's small size, fast training speed and promising benchmark scores. This made it possible for us to experiment with ensembles using modest computing resources. In this section, we briefly describe the ELECTRA model before moving on to the details about the classifier. In the rest of this section, author and data sample are used interchangeably, since each data sample in the dataset is an author. The code used in this work is available in GitHub<ref type="foot" coords="3,314.08,292.99,3.49,6.05" target="#foot_0">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ELECTRA</head><p>At present, the current state-of-the-art in natural language processing is held by large Transformer-based <ref type="bibr" coords="3,212.23,358.05,16.60,8.64" target="#b16">[18]</ref> models which have been trained using the BERT technique <ref type="bibr" coords="3,466.48,358.05,10.58,8.64" target="#b3">[4]</ref>, for example, RoBERTa <ref type="bibr" coords="3,231.24,370.01,10.58,8.64" target="#b7">[8]</ref>, T5 <ref type="bibr" coords="3,262.19,370.01,15.27,8.64" target="#b10">[12]</ref>, etc. These models are trained in an unsupervised manner on a vast amount of text data and can be fine-tuned for other downstream tasks such as text classification, question answering, etc. The unsupervised task often used for training such models is the prediction of masked tokens. In this task, a small percentage, typically 15%, of tokens in the input data is corrupted with a [MASK] token. The model is trained to correctly predict these masked tokens. This task (also called as a pretraining task) has the disadvantage of only learning from a small portion of the text sequence which is largely computationally inefficient.</p><p>The ELECTRA training method <ref type="bibr" coords="3,283.13,465.81,10.58,8.64" target="#b2">[3]</ref>, which stands for Efficiently Learning an Encoder that Classifies Token Replacements Accurately, aims to address this inefficiency while retaining all the same capabilities of BERT. This is done by using a novel pretraining task, called as replaced token detection, in which a model is trained to distinguish between real input tokens from synthetic but plausible replacements. The model is required to predict over each of the input tokens whether it is the real input token or a replaced one thereby learning from the entire sequence instead just a small percentage of it. This results in ELECTRA performing competitively with other state-of-the-artmodels while using only about 25% of their computing requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Token limit of ELECTRA</head><p>Most BERT-style transformers have a token limit of 512. Models trained using ELEC-TRA have the same limitations. This makes it difficult to directly feed the given data samples into our ELECTRA based classification system. It is because for each author, i.e for each data sample, a collection of 100 tweets is given, whose tokens altogether cross the token limit.</p><p>An obvious method would be to truncate and reduce the number of tokens. But we avoid doing so due to two reasons. Firstly, in the entire set of an author's tweets, not all of them might be fake and vice versa. Secondly, doing so will result in the loss of a lot of information. Hence, to address these issues, in this work, we randomly sample n tweets from an author's set of tweets. The exact implementation details and intuition is explained in ยง3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Random Sampling of an Author's Tweets</head><p>Intuition The intuition behind random sampling is that we do not know which of the tweets from an author's set of tweets are relevant for the classification task. So, at each training epoch, if we randomly sample an author's tweets to feed into the model, the model will have the chance to look at enough of an author's tweets to learn if the author is a fake news spreader or not.</p><p>Implementation While constructing a batch of samples to feed into the model, from each author, randomly n tweets from the collection of 100 are selected. These are then concatenated with special classification tokens as given in Figure <ref type="figure" coords="4,393.94,357.97,3.74,8.64" target="#fig_1">3</ref>. This chosen collection of random tweets for each author is not fixed and is randomly chosen again at every epoch. Therefore, tweets chosen in a previous epoch might get chosen again. We use n = 14 so that the token limit is never exceeded even in edge cases where the tweets might be longer.</p><p>If T a is the set of tweets of an author a, t is a subset of randomly selected n tweets from T a at the i th training epoch such that n โค |T a |, then C a i is the concatenation of the tweets in t, where C a i is defined as</p><formula xml:id="formula_0" coords="4,226.95,480.27,236.35,11.13">C a i = &lt;S&gt; t1 &lt;\S&gt; t2... &lt;\S&gt; tn-1 &lt;\S&gt; tn<label>(1)</label></formula><p>Here is &lt;S&gt; and &lt;\S&gt; are special tokens defined in ELECTRA's vocabulary as CLS_TOKEN and SEP_TOKEN respectively. CLS_TOKEN marks sentences for classification. SEP_TOKEN separates each tweet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ensemble Classifier</head><p>One obvious drawback of random sampling described in ยง3.2 is that looking at only a small random portion of an author's tweets may not enough to make a correct decision.</p><p>To mitigate this problem, we use an ensemble. Our proposed ensemble is built using 15 fine-tuned models each of which is built on top of a pre-trained ELECTRA model.</p><p>Each model of the ensemble looks at a different random sample of an author's tweets and makes a prediction. The final prediction is determined by majority voting where the label with the highest frequency is chosen as the final label for the task. This ensures that a wide range of an author's tweets is looked at before coming to a decision. Ensembling also has the effect of lowering the variance of the model <ref type="bibr" coords="5,367.19,167.13,15.27,8.64" target="#b14">[16]</ref>. The architecture of the models in the ensemble and the training routine is described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-trained ELECTRA Dense Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax Layer</head><p>Random Sampled and Concateneted Tweets of an Author Model Architecture The ensemble is made of 15 fine-tuned models each of which is of the architecture given in Figure <ref type="figure" coords="5,259.72,369.76,3.74,8.64" target="#fig_2">4</ref>. We use only 15 models because adding more does not improve the ensemble <ref type="bibr" coords="5,225.85,381.71,15.27,8.64" target="#b14">[16]</ref>. In each model, 256-dimensional embeddings produced by pre-trained ELECTRA are fed into a tanh-activated dense layer having 256 in-features and 256 out-features. After applying a dropout of 0.1 on the output of the dense layer, the output representation is fed into a sof tmax layer which makes the prediction. The weights of the dense layer and sof tmax layer are randomly initialized in each of the models of the ensemble. Separate ensembles were built for the two languages in the dataset, one ensemble for English and one ensemble for Spanish. For English, we used the pre-trained model called google/electra-small-discriminator from the HuggingFace Transformers Library<ref type="foot" coords="5,167.55,487.64,3.49,6.05" target="#foot_1">4</ref>  <ref type="bibr" coords="5,174.43,489.31,15.27,8.64" target="#b17">[19]</ref>. Since no official pre-trained model was available for Spanish, we used a pre-trained model called skimai/electra-small-spanish from HuggingFace community models hub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Training and Inference</head><p>The same training routine is applied to each of the models in the ensemble. Each model is fine-tuned with a small learning rate of โ 1e -3 using a cross-entropy loss function for 20 epochs. 90% of the data is used as the train set and the remaining 10% is used as the validation set. The percentage of each class is preserved in both of these. Early stopping was used to stop training if validation accuracy did not improve for 4 consecutive epochs. The model is optimized using Ranger optimizer, which is a combination of LookAhead <ref type="bibr" coords="5,335.31,613.97,16.60,8.64" target="#b18">[20]</ref> and RAdam <ref type="bibr" coords="5,405.74,613.97,10.58,8.64" target="#b6">[7]</ref>. The (ฮฑ, k) parameters of the optimizer are set to (0.5, 5). During both training and inference, the random sampling approach described in ยง3.3 is used to feed data into the model. Data is fed into the models in batches of 50. As mentioned in ยง3.4, the final label is determined by majority voting.  <ref type="table" coords="6,454.02,406.22,3.74,8.64">4</ref>. The ensembles scored an accuracy of 0.70 and 0.69 on the English and Spanish test sets respectively as given in Table <ref type="table" coords="6,256.73,430.13,3.74,8.64">4</ref>. Both of the ensembles had almost the same accuracy on the test set. The ensemble for English had a validation set accuracy of 0.87 while the ensemble for Spanish had a validation set accuracy of 0.77. This suggests that the ensembles suffered from over-fitting since there is a notable difference between the validation and test set accuracy. Another explanation could be that random sampling did not sample the relevant tweets for the classifier to be able to differentiate correctly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper explored the application of ensembled ELECTRA models for the task of Profiling Fake News Spreaders in Twitter. Random sampling was used in an attempt to overcome the limitation of the max number of tokens supported by transformer models. In future work, it would be interesting to explore transformer models that do no have such limitations, for example the Longformer <ref type="bibr" coords="7,320.49,143.22,10.58,8.64" target="#b1">[2]</ref>. Also, the study did not make use of the many features of the data. We found that the data had duplicates (see ยง2) that could have been removed during preprocessing. This perhaps might have improved the classifier's performance by preventing duplicates from being sampled. Another promising avenue for future work would to enhance the proposed classifier with emotional signals from the text using lexicons such as EmoLex [9] and SentiSense <ref type="bibr" coords="7,393.59,203.00,10.58,8.64" target="#b0">[1]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,259.66,630.68,96.04,8.12"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>Figure 1. Data distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,248.86,546.29,117.63,8.12"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Concatenation Method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,313.16,345.83,8.12;5,134.77,324.18,227.73,8.06"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. A single classification model of the ensemble. The dense layer is tanh activated and has a dropout of 0.1. The sof tmax layer makes the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,205.24,323.87,204.89,8.12"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Validation accuracy of each model in ensemble</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,242.59,521.69,127.78,52.87"><head>Table 1 .</head><label>1</label><figDesc>Results</figDesc><table coords="6,242.59,521.69,127.78,42.01"><row><cell></cell><cell>Accuracy</cell><cell></cell></row><row><cell cols="3">Language Validation Set Test Set</cell></row><row><cell>English</cell><cell>0.87</cell><cell>0.70</cell></row><row><cell>Spanish</cell><cell>0.77</cell><cell>0.69</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="3,144.73,657.08,198.21,7.77"><p>https://github.com/cozek/profiling-fake-news-spreaders</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="5,144.73,657.08,79.85,7.77"><p>https://huggingface.co</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.61,265.30,314.14,7.77;7,150.95,276.26,205.98,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,295.39,265.30,161.36,7.77;7,150.95,276.26,139.01,7.77">Sentisense: An easily scalable concept-based affective lexicon for sentiment analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gervรกs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,307.87,276.26,22.92,7.77">LREC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,287.90,304.36,7.77;7,150.95,298.86,91.42,7.77" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m" coord="7,282.61,287.90,160.72,7.77">Longformer: The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,310.50,321.16,7.77;7,150.95,321.46,157.87,7.77" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<title level="m" coord="7,331.43,310.50,132.33,7.77;7,150.95,321.46,131.73,7.77">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,333.11,325.47,7.77;7,150.95,344.07,238.30,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,328.63,333.11,139.45,7.77;7,150.95,344.07,144.12,7.77">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,313.31,344.07,49.80,7.77">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,355.71,320.10,7.77;7,150.95,366.67,264.42,7.77;7,150.95,377.63,115.92,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,276.77,355.71,185.95,7.77;7,150.95,366.67,84.80,7.77">An emotional analysis of false information in social media and news articles</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<idno type="DOI">10.1145/3381750</idno>
		<ptr target="https://doi.org/10.1145/3381750" />
	</analytic>
	<monogr>
		<title level="j" coord="7,241.12,366.67,108.02,7.77">ACM Trans. Internet Technol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020-04">Apr 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,389.27,305.13,7.77;7,150.95,400.23,322.37,7.77;7,150.95,411.19,296.29,7.77;7,150.95,422.15,188.81,7.77;7,150.95,433.11,149.54,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,290.21,389.27,157.53,7.77;7,150.95,400.23,31.60,7.77">Leveraging emotional signals for credibility detection</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331285</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331285" />
	</analytic>
	<monogr>
		<title level="m" coord="7,200.51,400.23,272.81,7.77;7,150.95,411.19,150.87,7.77;7,352.37,411.19,33.43,7.77">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="877" to="880" />
		</imprint>
	</monogr>
	<note>SIGIR&apos;19</note>
</biblStruct>

<biblStruct coords="7,142.61,444.75,337.98,7.77;7,150.95,455.71,237.73,7.77" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,369.12,444.75,111.47,7.77;7,150.95,455.71,86.93,7.77">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,142.61,467.36,329.13,7.77;7,150.95,478.31,290.22,7.77;7,139.25,489.96,317.46,7.77;7,150.95,500.92,103.62,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,213.00,478.31,202.03,7.77">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<idno>ArXiv abs/1308.6297</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,269.63,489.96,183.51,7.77">Crowdsourcing a word-emotion association lexicon</title>
		<imprint>
			<date type="published" when="2013">2019. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.24,512.56,335.13,7.77;7,150.95,523.52,312.30,7.77;7,150.95,534.48,285.89,7.77;7,150.95,545.44,227.51,7.77;7,150.95,556.40,162.18,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,329.39,512.56,147.98,7.77;7,150.95,523.52,152.65,7.77">DeClarE: Debunking fake news and false claims using evidence-aware deep learning</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1003" />
	</analytic>
	<monogr>
		<title level="m" coord="7,321.57,523.52,141.69,7.77;7,150.95,534.48,184.53,7.77">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">Oct-Nov 2018</date>
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.24,568.04,335.40,7.77;7,150.95,579.00,306.17,7.77;7,150.95,589.96,72.72,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,333.86,568.04,140.16,7.77">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,259.53,579.00,193.52,7.77">Information Retrieval Evaluation in a Changing World</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09">Sep 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.24,601.60,336.10,7.77;7,150.95,612.56,322.99,7.77" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<title level="m" coord="7,167.65,612.56,280.15,7.77">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.24,624.21,323.93,7.77;7,150.95,635.17,325.30,7.77;7,150.95,646.13,300.97,7.77;7,150.95,657.08,96.22,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,333.29,624.21,132.88,7.77;7,150.95,635.17,218.44,7.77">Overview of the 8th Author Profiling Task at PAN 2020: Profiling Fake News Spreaders on Twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="7,264.50,646.13,117.33,7.77">CLEF 2020 Labs and Workshops</title>
		<title level="s" coord="7,388.31,646.13,63.62,7.77;7,150.95,657.08,19.77,7.77">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nรฉvรฉol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020-09">Sep 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,119.96,327.89,7.77;8,150.95,130.92,316.79,7.77;8,150.95,141.88,23.90,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,229.20,119.96,240.93,7.77;8,150.95,130.92,112.03,7.77">On the implications of the general data protection regulation on the organisation of evaluation tasks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,268.59,130.92,150.59,7.77">Language and Law= Linguagem e Direito</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="117" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,152.84,333.34,7.77;8,150.95,163.80,188.13,7.77" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="8,333.29,152.84,142.29,7.77">Profiling fake news spreaders on twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3692319</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3692319" />
		<imprint>
			<date type="published" when="2020-02">Feb 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,174.76,312.10,7.77;8,150.95,185.71,306.86,7.77;8,150.95,196.67,328.45,7.77;8,150.95,207.63,169.86,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,229.41,174.76,209.42,7.77">Bagging BERT models for robust aggression identification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Risch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krestel</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.trac-1" />
	</analytic>
	<monogr>
		<title level="m" coord="8,150.95,185.71,289.56,7.77">Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying</title>
		<meeting>the Second Workshop on Trolling, Aggression and Cyberbullying<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA</publisher>
			<date type="published" when="2020-05">May 2020</date>
			<biblScope unit="page" from="55" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,218.59,315.89,7.77;8,150.95,229.55,311.83,7.77;8,150.95,240.51,101.11,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,249.86,218.59,208.27,7.77;8,150.95,229.55,31.60,7.77">Understanding user profiles on social media for fake news detection</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,208.48,229.55,254.31,7.77;8,150.95,240.51,27.89,7.77">IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="430" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,251.47,321.33,7.77;8,150.95,262.43,169.11,7.77" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="8,205.76,262.43,88.17,7.77">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,273.39,324.37,7.77;8,150.95,284.34,311.40,7.77;8,150.95,295.30,185.55,7.77" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="8,275.95,284.34,186.41,7.77;8,150.95,295.30,71.51,7.77">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<idno>ArXiv abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,306.26,324.65,7.77;8,150.95,317.22,43.07,7.77" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="8,304.87,306.26,162.02,7.77;8,150.95,317.22,16.93,7.77">Lookahead optimizer: k steps forward, 1 step back</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
