<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.63,112.03,334.09,18.66;1,223.43,132.52,168.50,15.55">Deep Bayes Factor Scoring for Authorship Verification Notebook for PAN at CLEF 2020</title>
				<funder>
					<orgName type="full">North Rhine-Westphalia</orgName>
				</funder>
				<funder ref="#_4pxwmPz">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_XyuFs2b">
					<orgName type="full">Research Training Group &quot;</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.50,169.43,93.56,12.00"><forename type="first">Benedikt</forename><surname>Boenninghoff</surname></persName>
							<email>benedikt.boenninghoff@rub.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Ruhr University Bochum</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.51,169.43,47.88,12.00"><forename type="first">Julian</forename><surname>Rupp</surname></persName>
							<email>julian.rupp@rub.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Ruhr University Bochum</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.84,169.43,69.91,12.00"><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Nickel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bucknell University</orgName>
								<address>
									<settlement>Lewisburg</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,398.07,169.43,71.31,12.00"><forename type="first">Dorothea</forename><surname>Kolossa</surname></persName>
							<email>dorothea.kolossa@rub.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Ruhr University Bochum</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.63,112.03,334.09,18.66;1,223.43,132.52,168.50,15.55">Deep Bayes Factor Scoring for Authorship Verification Notebook for PAN at CLEF 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7694DD2527D3FFFEF0D76CBE6AEB636D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The PAN 2020 authorship verification (AV) challenge focuses on a cross-topic/closed-set AV task over a collection of fanfiction texts. Fanfiction is a fan-written extension of a storyline in which a so-called fandom topic describes the principal subject of the document. The data provided in the PAN 2020 AV task is quite challenging because authors of texts across multiple/different fandom topics are included. In this work, we present a hierarchical fusion of two well-known approaches into a single end-to-end learning procedure: A deep metric learning framework at the bottom aims to learn a pseudo-metric that maps a document of variable length onto a fixed-sized feature vector. At the top, we incorporate a probabilistic layer to perform Bayes factor scoring in the learned metric space. We also provide text preprocessing strategies to deal with the cross-topic issue.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of (pairwise) authorship verification (AV) is to decide if two texts were written by the same person or not. AV is traditionally performed by linguists who aim to uncover the authorship of anonymously written texts by inferring author-specific characteristics from the texts <ref type="bibr" coords="1,225.37,427.48,15.27,12.00" target="#b10">[11]</ref>. Such characteristics are represented by so-called linguistic features. They are derived from an analysis of errors (e.g. spelling mistakes), textual idiosyncrasies (e.g. grammatical inconsistencies) and stylistic patterns <ref type="bibr" coords="1,416.73,451.40,15.27,12.00" target="#b10">[11]</ref>.</p><p>Automated (machine-learning-based) systems have traditionally relied on so-called stylometric features <ref type="bibr" coords="1,223.89,475.31,15.27,12.00" target="#b19">[20]</ref>. Stylometric features tend to rely largely on linguistically motivated/inspired metrics. The disadvantage of stylometric features is that their reliability is typically diminished when applied to texts with large topical variations.</p><p>Deep learning systems, on the other hand, can be developed to automatically learn neural features in an end-to-end manner <ref type="bibr" coords="1,306.63,523.13,10.58,12.00" target="#b4">[5]</ref>. While these features can be learned in such a way that they are largely insensitive to the topic, on the negative side, they are generally not linguistically interpretable.</p><p>In this work we propose a substantial extension of our published ADHOMINEM approach <ref type="bibr" coords="1,173.91,570.95,10.58,12.00" target="#b3">[4]</ref>, in which we interpret the neural features produced by ADHOMINEM not just from a metric point of view but, additionally, from a probabilistic point of view.</p><p>With our modification of ADHOMINEM we were also cognizant of the proposed future AV shared tasks of the PAN organization <ref type="bibr" coords="1,326.87,606.81,15.27,12.00" target="#b15">[16]</ref>. Three broader research questions (cross-topic verification, open-set verification, and "surprise task") are put into the spotlight over the next three years. In light of these challenges we define requirements for automatically extracted neural features as follows:</p><p>-Distinctiveness: Our extracted neural features should contain all necessary information w.r.t. the writing style, such that a verification system is able to distinguish same/different author/s in an open-set scenario. In order to automatically quantify deviations from the standard language, the text sample collection for the training phase must be sufficiently long. -Invariance: Authors tend to shift the characteristics of their writing according to their situational disposition (e.g. their emotional state) and the topic of the text/discourse. Extracted neural features should therefore, ideally, be invariant w.r.t. the topic, the sentiment, the emotional state of the writer, and so forth. -Robustness: The writing style of a text can be influenced, for example, by a desire to imitate another author (e.g. the original author of a fandom topic) or by applying a deliberate obfuscation strategy for other reasons. Our extracted neural features should still lead to reliable verification results, even when obfuscation/imitation strategies are applied by the author. -Adaptability: The writing style is generally also affected by the type of the text, which is called genre. People change their linguistic register depending on the genre that they write in. This, in turn, leads to significant changes in the characteristics of the resulting text. For a technical system, it is thus extremely difficult to establish a common authorship between a WhatsApp message and a formal job application for example. In forensic disciplines, it is therefore important to train classifiers only on one genre at a time. In research, however, it is quite an interesting question how to, e.g., find a joint subspace representation/embedding for text samples across different genres.</p><p>We assume that a single text sample has been written by a single person. If necessary, we need to examine a collaborative authorship in advance <ref type="bibr" coords="2,376.89,427.50,15.27,12.00" target="#b10">[11]</ref>. Dealing with genreadaption or obfuscation/imitation strategies is not part of the PAN 2020/21 AV task. Another open question is the minimum size of a text sample required to obtain reliable output predictions. This question will also be left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ADHOMINEM: Siamese network for representation learning</head><p>Existing AV algorithms can be taxonomically grouped w.r.t. their design and characteristics, e.g. instance-vs. profile-based paradigms, intrinsic vs. extrinsic methods <ref type="bibr" coords="2,450.86,522.21,15.27,12.00" target="#b17">[18]</ref>, or unary vs. binary classification <ref type="bibr" coords="2,257.98,534.17,15.27,12.00" target="#b11">[12]</ref>. We may roughly describe the work flow for a traditional binary AV classifier design as follows: In the feature engineering process, a set of manually defined stylometric features is extracted. Afterwards, a training and/or development set is used to fit a model to the data and to tune possible hyper-parameters of the model. Typically, an additional calibration step is necessary to transform scores provided by the model into appropriate probability estimates. Our modified ADHOMINEM system works differently. We define a deep-learning model architecture with all of its hyper-parameters and thresholds a-priori and let the model learn suitable features for the provided setup on its own. As with most deep-learning approaches, the success of the proposed setup depends heavily on the availability of a large collection of text samples with many examples of representative variations in writing style.</p><p>The majority of published papers, using deep neural networks to build an AV framework, have employed a classification loss <ref type="bibr" coords="3,300.38,128.55,10.58,12.00" target="#b0">[1]</ref>, <ref type="bibr" coords="3,316.68,128.55,15.27,12.00" target="#b16">[17]</ref>. However, metric learning objectives present a promising alternative <ref type="bibr" coords="3,261.51,140.50,10.58,12.00" target="#b4">[5]</ref>, <ref type="bibr" coords="3,278.49,140.50,10.58,12.00" target="#b7">[8]</ref>. The discriminative power of our proposed AV method stems from a fusion of two well-known approaches into a single joint end-toend learning procedure: A precursor of our ADHOMINEM system <ref type="bibr" coords="3,399.58,164.41,11.62,12.00" target="#b3">[4]</ref> is used as a deep metric learning framework <ref type="bibr" coords="3,247.68,176.37,16.60,12.00" target="#b13">[14]</ref> to measure the similarity between two text samples. The features that are implicitly produced by the ADHOMINEM system are then fed into a probabilistic linear discriminant analysis (PLDA) layer <ref type="bibr" coords="3,370.22,200.28,11.62,12.00" target="#b8">[9]</ref> that functions as a pairwise discriminator to perform Bayes factor scoring in the learned metric space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural extraction of linguistic embedding vectors</head><p>A text sample can be understood as a hierarchical structure of ordered discrete elements: It consists of a list of ordered sentences. Each sentence consists of an ordered list of tokens. Again, each token consists of an ordered list of characters. The purpose of ADHOMINEM is to map a document to a feature vector. More specifically, its Siamese topology includes a hierarchical neural feature extraction, which encodes the stylistic characteristics of a pair of documents (D 1 , D 2 ), each of variable length, into a pair of fixed-length linguistic embedding vectors (LEVs) y i :</p><formula xml:id="formula_0" coords="3,237.11,338.63,243.48,18.91">y i = A θ (D i ) ∈ R D×1 , i ∈ {1, 2},<label>(1)</label></formula><p>where D denotes the dimension of the LEVs and θ contains all trainable parameters. It is called a Siamese network because both documents D 1 and D 2 are mapped through the exact same function A θ (•). The internal structure of A θ (•) is illustrated in Fig. <ref type="figure" coords="3,473.12,380.62,3.74,12.00" target="#fig_1">1</ref>. After preprocessing and tokenization (which will be explained in Section 3), the system passes a fusion of token and character embeddings into a two-tiered bidirectional LSTM <ref type="bibr" coords="3,163.85,416.48,16.60,12.00" target="#b12">[13]</ref> network with attentions <ref type="bibr" coords="3,279.47,416.48,10.58,12.00" target="#b1">[2]</ref>. We incorporate a characters-to-word encoding layer to take the specific uses of prefixes and suffixes as well as spelling errors into account. An incorporation of attention layers allows us to visualize words and sentences that have been marked as "highly significant" by the system. As shown in Fig. <ref type="figure" coords="3,457.78,452.35,3.74,12.00" target="#fig_1">1</ref>, the network produces document embeddings, which are converted into LEVs via a fullyconnected dense layer. With this output layer, we can control the output dimension. AV is accomplished by computing the Euclidean distance <ref type="bibr" coords="3,350.87,488.21,16.60,12.00" target="#b13">[14]</ref> </p><formula xml:id="formula_1" coords="3,203.24,504.11,208.37,19.80">d(D 1 , D 2 ) = A θ (D 1 ) -A θ (D 2 ) 2 2 = y 1 -y 2 2 2</formula><p>(2) between both LEVs. If the distance in Eq. ( <ref type="formula" coords="3,315.10,523.08,3.87,12.00" target="#formula_31">2</ref>) is above a given threshold τ , then the system decides on different-authors, if the distance is below τ , then the system decides on same-authors. Details are comprehensively described in <ref type="bibr" coords="3,372.77,546.99,10.58,12.00" target="#b3">[4]</ref>.</p><p>Pseudo-metric: ADHOMINEM provides a framework to learn a pseuo-metric. Since we are using the Euclidean distance in Eq. ( <ref type="formula" coords="3,309.90,580.03,3.87,12.00" target="#formula_31">2</ref>) we have the following properties:  </p><formula xml:id="formula_2" coords="3,134.77,596.97,310.50,81.54">d(D 1 , D 2 ) ≥ 0 (nonnegativity) d(D 1 , D 1 ) = 0 (identity) d(D 1 , D 2 ) = d(D 2 , D 1 ) (symmetry) d(D 1 , D 3 ) ≤ d(D 1 , D 2 ) + d(D 2 , D 3 ) (triangle inequality) Note that we may obtain d(D 1 , D 2 ) = 0 where D 1 = D 2 . LSTMws LSTMws LSTMws LSTMsd LSTMsd LSTMsd α1• α2• αW • β1• β2• βS• + + + + + + . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss function:</head><p>The entire network is trained end-to-end. For the pseudo-metric learning objective, we choose the modified contrastive loss <ref type="bibr" coords="4,351.74,331.64,10.79,12.00" target="#b4">[5]</ref>:</p><formula xml:id="formula_3" coords="4,140.88,348.24,339.71,23.79">L θ = l • max y 1 -y 2 2 2 -τ s , 0 2 + (1 -l) • max τ d -y 1 -y 2 2 2 , 0 2 ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4" coords="4,161.83,375.13,163.75,18.77">l ∈ {0, 1}, τ s &lt; τ d and τ = 1 2 (τ s + τ d ).</formula><p>During training, all distances between same-author pairs are forced to stay below the lower of the two thresholds, τ s . Conversely, distances between different-authors pairs are forced to remain above the higher threshold τ d . By employing this dual threshold strategy, the system is made more insensitive to topical or intra-author variations between documents <ref type="bibr" coords="4,394.77,422.95,15.27,12.00" target="#b13">[14]</ref>, <ref type="bibr" coords="4,416.35,422.95,10.58,12.00" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Two-covariance model for Bayes factor scoring</head><p>Text samples are characterized by a high variability. Statistical hypothesis tests can help to quantify the outputs/scores of our algorithm and to decide whether to accept or reject the decision. ADHOMINEM can be extended with a framework for statistical hypothesis testing. More precisely, we are interested in the AV problem where, given the LEVs of two documents, we have to decide for one of two hypotheses: H s : The two documents were written by the same person, H d : The two documents were written by two different persons.</p><p>In the following, we will describe a particular case of the well-known probabilistic linear discriminant analysis (PLDA) <ref type="bibr" coords="4,285.26,582.91,15.27,12.00" target="#b14">[15]</ref>, which is also known as the two-covariance model <ref type="bibr" coords="4,162.31,594.87,10.58,12.00" target="#b8">[9]</ref>. Let us assume, the author's writing style is represented by a vector x. We suppose that our (noisy) observed LEV y = A θ (D) stems from a Gaussian generative model that can be decomposed as</p><formula xml:id="formula_5" coords="4,210.97,637.45,123.97,24.93">y linguistic embedding vector = x</formula><p>author's writing style</p><formula xml:id="formula_6" coords="4,362.96,635.71,117.63,24.73">+ noise term ,<label>(4)</label></formula><p>where characterizes residual noise, caused by thematic varitions or by significant changes in the process of text production for instance. The idea behind this factor analysis is that the writing characteristics of the author, measured in the observed LEV y, lie in a latent variable x. The probability density functions for x and in Eq. ( <ref type="formula" coords="5,251.85,164.76,3.87,12.00" target="#formula_6">4</ref>) are defined as in <ref type="bibr" coords="5,329.59,164.76,10.79,12.00" target="#b5">[6]</ref>:</p><formula xml:id="formula_7" coords="5,260.84,181.70,219.76,18.77">p(x) = N (x|µ, B -1 ),<label>(5)</label></formula><formula xml:id="formula_8" coords="5,262.59,198.13,218.01,18.77">p( ) = N ( |0, W -1 ),<label>(6)</label></formula><p>where B -1 defines the between-author covariance matrix and W -1 denotes the withinauthor covariance matrix. As mentioned in <ref type="bibr" coords="5,310.70,229.52,10.58,12.00" target="#b6">[7]</ref>, the idea is to model inter-author variability (with the covariance matrix B -1 ) and intra-author variability (with the covariance matrix W -1 ). From Eqs. ( <ref type="formula" coords="5,265.60,253.43,4.18,12.00" target="#formula_7">5</ref>)-( <ref type="formula" coords="5,282.34,253.43,4.18,12.00" target="#formula_8">6</ref>), it can be deduced that the conditional density function is given by <ref type="bibr" coords="5,216.28,265.38,10.79,12.00" target="#b5">[6]</ref>:</p><formula xml:id="formula_9" coords="5,255.19,282.32,225.40,18.77">p(y|x) = N (y|x, W -1 ).<label>(7)</label></formula><p>Assuming we have a set of n LEVs, Y = {y 1 , . . . y n }, verifiably associated to the same author, then we can compute the posterior (see Theorem 1 on page 175 in <ref type="bibr" coords="5,430.14,312.21,15.12,12.00" target="#b9">[10]</ref>):</p><formula xml:id="formula_10" coords="5,247.81,329.14,232.78,18.77">p(x|Y) = N (x|L -1 γ, L -1 ),<label>(8)</label></formula><p>where L = B + nW and γ = Bµ + W n i=1 y i . Let us now consider the process of generating two linguistic embedding vector (LEV) y i , i ∈ {1, 2}. We have to distinguish between same-author and different-author pairs: Same-author pair: In the case of a same-author pair, a single latent vector x 0 representing the author's writing style is generated from the prior p(x) in Eq. ( <ref type="formula" coords="5,434.81,414.56,3.87,12.00" target="#formula_7">5</ref>) and both LEVs y i , i ∈ {1, 2} are generated from p(y|x 0 ) in Eq. <ref type="bibr" coords="5,360.72,426.52,10.58,12.00" target="#b6">(7)</ref>. The joint probability density function is then given by</p><formula xml:id="formula_11" coords="5,149.23,455.56,331.36,30.86">p(y 1 , y 2 |H s ) = p(y 1 , y 2 | x 0 , H s ) p(x 0 |H s ) p(x 0 |y 1 , y 2 , H s ) = p(y 1 |x 0 ) p(y 2 |x 0 ) p(x 0 ) p(x 0 |y 1 , y 2 ) .<label>(9)</label></formula><p>The term p(x 0 |y 1 , y 2 ) can be computed using Eq. ( <ref type="formula" coords="5,342.07,484.59,3.53,12.00" target="#formula_10">8</ref>).</p><p>Different-authors pair: For a different-authors pair, two latent vectors, x i for i ∈ {1, 2}, representing two different authors' writing characteristics, are independently generated from p(x) in Eq. ( <ref type="formula" coords="5,246.93,540.12,3.53,12.00" target="#formula_7">5</ref>). The corresponding LEVs y i are generated from p(y|x i ) in Eq. ( <ref type="formula" coords="5,164.58,552.07,3.53,12.00" target="#formula_9">7</ref>). The joint probability density function is then given by</p><formula xml:id="formula_12" coords="5,149.87,569.16,326.58,30.86">p(y 1 , y 2 |H d ) = p(y 1 |H d ) p(y 2 |H d ) = p(y 1 |x 1 )p(x 1 ) p(x 1 |y 1 ) • p(y 2 |x 2 )p(x 2 ) p(x 2 |y 2 ) . (<label>10</label></formula><formula xml:id="formula_13" coords="5,476.44,574.42,4.15,12.00">)</formula><p>The terms p(x 1 |y 1 ) and p(x 2 |y 2 ) are again obtained from Eq. ( <ref type="formula" coords="5,390.71,598.19,3.53,12.00" target="#formula_10">8</ref>).</p><p>Verification score: The described probabilistic model involves two steps: a training phase to learn the parameters of the Gaussian distributions in Eqs. ( <ref type="formula" coords="5,403.79,641.76,4.43,12.00" target="#formula_7">5</ref>)-( <ref type="formula" coords="5,421.49,641.76,4.43,12.00" target="#formula_8">6</ref>) and a verification phase to infer whether both text samples come from the same author.</p><p>For both steps, we need to define the verification score, which can now be calculated as the log-likelihood ratio between the two hypotheses H </p><p>Eq. ( <ref type="formula" coords="6,154.37,208.25,8.30,12.00" target="#formula_14">11</ref>) is often called the Bayes factor. Since p(y 1 , y 2 |H s ) in Eq. ( <ref type="formula" coords="6,402.19,208.25,3.87,12.00" target="#formula_11">9</ref>) and p(y 1 , y 2 |H d ) in Eq. ( <ref type="formula" coords="6,165.85,220.20,8.30,12.00" target="#formula_12">10</ref>) are independent of x 0 and x 1 , x 2 , we can choose any values for the latent variables, as long as the denominator is non-zero <ref type="bibr" coords="6,331.91,232.16,10.58,12.00" target="#b5">[6]</ref>. Substituting Eqs. ( <ref type="formula" coords="6,422.66,232.16,3.53,12.00" target="#formula_7">5</ref>), ( <ref type="formula" coords="6,439.28,232.16,3.53,12.00" target="#formula_8">6</ref>), ( <ref type="formula" coords="6,455.89,232.16,3.53,12.00" target="#formula_9">7</ref>), <ref type="bibr" coords="6,468.97,232.16,11.62,12.00" target="#b7">(8)</ref> in Eq. ( <ref type="formula" coords="6,165.21,244.11,8.30,12.00" target="#formula_14">11</ref>) and selecting x 0 = x 1 = x 2 = 0, we obtain <ref type="bibr" coords="6,360.10,244.11,11.62,12.00" target="#b8">[9]</ref> score(y</p><formula xml:id="formula_15" coords="6,200.69,261.12,279.90,35.45">1 , y 2 ) = -log N (0|µ, B -1 ) -log N (0|L -1 1,2 γ 1,2 , L -1 1,2 ) + log N (0|L -1 1 γ 1 , L -1 1 ) + log N (0|L -1 2 γ 2 , L -1 2 ),<label>(12)</label></formula><p>where L 1,2 = B+2W , γ 1,2 = Bµ+W (y 1 +y 2 ) and L i = B+W , γ i = Bµ+W y i for i ∈ {1, 2}. As described in <ref type="bibr" coords="6,258.28,312.67,10.58,12.00" target="#b5">[6]</ref>, the score in Eq. ( <ref type="formula" coords="6,343.50,312.67,8.30,12.00" target="#formula_15">12</ref>) can now be rewritten as</p><formula xml:id="formula_16" coords="6,140.51,335.96,340.08,15.44">score(y 1 , y 2 ) = y 1 Λy T 2 + y 2 Λy T 1 + y 1 Γ y T 1 + y 2 Γ y T 2 + y 1 + y 2 T ρ + κ,<label>(13)</label></formula><p>where the parameters Λ, Γ , ρ and κ of the quadratic function in Eq. ( <ref type="formula" coords="6,413.42,360.90,8.30,12.00" target="#formula_16">13</ref>) are given by</p><formula xml:id="formula_17" coords="6,166.13,377.19,275.76,49.04">Γ = 1 2 W T Λ -Γ W , Λ = 1 2 W T ΛW , ρ = W T Λ -Γ Bµ, κ = κ + 1 2 Bµ T Λ -2 Γ Bµ</formula><p>and the auxiliary variables are</p><formula xml:id="formula_18" coords="6,187.88,447.73,239.60,37.65">Γ = B + W -1 , Λ = B + 2W -1 , κ = 2 log det Γ -log det B -log det Λ + µ T Bµ.</formula><p>Hence, the verification score is a symmetric quadratic function of the LEVs. The probability for a same-author trial can be computed from the log-likelihood ratio score as follows:</p><formula xml:id="formula_19" coords="6,184.34,514.08,296.25,30.86">p(H s |y 1 , y 2 ) = p(H s ) p(y 1 , y 2 |H s ) p(H s ) p(y 1 , y 2 |H s ) + p(H d ) p(y 1 , y 2 |H d )<label>(14)</label></formula><p>The AV datasets provided by the PAN organizers are balanced w.r.t. authorship labels. Hence, we can assume p(H s ) = p(H d ) = 1 2 . We can rewrite Eq. ( <ref type="formula" coords="6,399.10,554.34,8.30,12.00" target="#formula_19">14</ref>) as follows:</p><formula xml:id="formula_20" coords="6,148.75,572.71,331.84,30.86">p(H s |y 1 , y 2 ) = p(y 1 , y 2 |H s ) p(y 1 , y 2 |H s ) + p(y 1 , y 2 |H d ) = Sigmoid score(y 1 , y 2 )<label>(15)</label></formula><p>Loss function: To learn the probabilistic layer, we incorporate Eq. ( <ref type="formula" coords="6,407.90,609.40,8.30,12.00" target="#formula_20">15</ref>) into the binary cross entropy:</p><formula xml:id="formula_21" coords="6,176.31,638.29,304.28,18.77">L φ = l • log {p(H s |y 1 , y 2 )} + (1 -l) • log {1 -p(H s |y 1 , y 2 )} ,<label>(16)</label></formula><p>where φ = W , B, µ contains the trainable parameters of the probabilistic layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cholesky decomposition for numerically stable covariance training:</head><p>We can treat φ = W , B, µ given by Eqs. ( <ref type="formula" coords="7,275.06,128.55,4.43,12.00" target="#formula_7">5</ref>)-( <ref type="formula" coords="7,292.77,128.55,4.43,12.00" target="#formula_8">6</ref>) as trainable paramters in our deep learning framework. For both covariance matrices we need to guarantee positive definiteness. Instead of learning W and B directly, we enforce the positive definiteness of them through Cholesky decomposition by constructing trainable lower-triangular matrices L W and L B with exponentiated (positive) diagonal elements. The estimated covariance matrices are constructed via W = L W L T W and B = L B L T B . We computed and updated the gradients of L W and L B with respect to the loss function in Eq. ( <ref type="formula" coords="7,445.71,201.49,7.64,12.00" target="#formula_21">16</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ensemble inference</head><p>Neural networks are randomly initialized, trained on the same, but shuffled data and affected by regularization techniques like dropout. Hence, they will find a different set of weights/biases each time, which in turn produces different predictions. To reduce the variance, we propose to train an ensemble of models and to combine the predictions of Eq. ( <ref type="formula" coords="7,154.96,292.20,8.30,12.00" target="#formula_20">15</ref>) from these models,</p><formula xml:id="formula_22" coords="7,217.18,308.31,263.41,31.18">E p(H s |y 1 , y 2 ) ≈ 1 m m i=1 p Mi (H s |y 1 , y 2 ),<label>(17)</label></formula><p>where M i indicates the i-th trained model. Finally, we determine the non-answers for predicted probabilities, i.e. E p(H s |y 1 , y 2 ) = 0.5, if 0.5δ &lt; E p(H s |y 1 , y 2 ) &lt; 0.5 + δ. Parameter δ can be found by applying a simple grid search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Text preprocessing strategies</head><p>The 2020 edition of the PAN authorship verification task focuses on fanfiction texts, fictional texts written by fans of previous, original literary works that have become popular like "Harry Potter". Usually, authors of fanfiction preserve core elements of the storyline by reusing main characters and settings. Nevertheless, they may also contain changes or alternative interpretations of some parts of the known storyline. The subject area of the original work is called fandom. The PAN organizers are providing unique author and fandom (topical) labels for all fanfiction pairs. The dataset has been derived from the corpus compiled in <ref type="bibr" coords="7,250.15,498.35,10.58,12.00" target="#b2">[3]</ref>. A detailed description of the dataset is given in <ref type="bibr" coords="7,456.45,498.35,15.27,12.00" target="#b15">[16]</ref>.</p><p>As mentioned in the introduction, automatically extracted neural features should be invariant w.r.t. shifts in topic and/or sentiment. Ideally, LEVs should only contain information regarding the writing style of the authors. What is well-established in automatic AV is that the topic of a text generally matters. What is still not clear, however, is how stylometric or neural features are influenced/affected by the topic (i.e. fandom in this case). To increase the generalization capabilities of our model and to increase the model's resilience towards cross-topic fanfiction pairs we devised the following preprocessing strategies, as outlined in Sections 3.1 through 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topic masking</head><p>Experiments show that considering a large set of token types can lead to significant overfitting effects. To overcome this, we reduced the vocabulary size for tokens as well as for characters by mapping all rare token/character types to a special unknown  (&lt;UNK&gt;) token. This is quite similar to the text distortion approach proposed in <ref type="bibr" coords="8,461.50,248.15,15.27,12.00" target="#b20">[21]</ref>. However, even when a rare/misspelled token is replaced by the &lt;UNK&gt; token, it can still be encoded by the character representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sliding window with contextual prefix</head><p>Fanfiction frequently contains dialogues and quoted text. Sentence boundary detectors, therefore, tend to be very error prone and steadily fail to segment the data into appropriate sentence units. We decided to perform tokenization without strict sentence boundary detection and generated sentence-like units via a sliding window technique instead. An example that illustrates the procedure is shown in Fig. <ref type="figure" coords="8,349.95,354.70,3.74,12.00" target="#fig_4">2</ref>. We used overlapping windows to guarantee that semantically and grammatically linked neighboring tokens are located in the same unit. We also added a contextual prefix which is provided by the fandom labels. To initialize the prefix embeddings, we removed all non-ASCII characters, tokenized the fandom string and averaged the corresponding word embeddings. The final sliding window length (in tokens) is given by hop_length + overlapping_length + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data split and augmentation</head><p>To tune our model we split the datasets into a train and a dev set. Table <ref type="table" coords="8,420.18,449.29,4.98,12.00">1</ref> shows the resulting sizes. The size of the train set can then be increased synthetically by dissembling all predefined document pairs and re-sampling new same-author and different-author pairs in each epoch. We first removed all documents in the train set which also appear in the dev set. Afterwards, we reorganized the train set as described in Alg. 1 -3. Assuming the i-th author with i ∈ {1, . . . , N } contributes with N i fanfiction texts, we define a set</p><formula xml:id="formula_23" coords="8,157.54,521.76,181.07,19.97">A (i) = {(a (i) , f<label>(i) 1 , d (i) 1 ), . . . , (a (i) , f (i) Ni , d (i)</label></formula><p>Ni )} containing 3-tuples of the form (a (i) , d</p><formula xml:id="formula_24" coords="8,163.06,537.09,32.65,14.93">(i) j , f (i) j )</formula><p>, where a (i) is the author ID, d (i) j represents the j-th document and f (i) j is the corresponding fandom label. The objective is to obtain a new set D of re-sampled pairs, containing 5-tuples of the form (d (1) , d (2) , f (1) , f (2) , l), where d (1) , d (2) defines the sampled fanfiction pair, f (1) , f (2) are the corresponding fandom labels and l ∈ {0, 1} indicates whether the texts are written by the same author (l = 1) or by different train set dev set test set small dataset 47,340 pairs 5,261 pairs 14,311 pairs large dataset 261,786 pairs 13,779 pairs Table <ref type="table" coords="8,207.22,652.37,3.36,11.67">1</ref>. Dataset sizes (including the provided test set) after splitting.</p><p>Algorithm 1: MAKETWOGROUPS 1 Input: A (1) , . . . A (N ) 2 Output: G (1) , G (2)   3 Initialize G</p><formula xml:id="formula_25" coords="9,137.70,154.75,154.04,122.84">(i) = {∅} for i ∈ {1, 2, 3} 4 for i = 1, . . . , N do 5 if |A (i) | = |{(a (i) , f (i) , d (i) )}| = 1 then // Authors with single doc 6 G (1) ←-G (1) ∪ {(a (i) , f (i) , d (i) )} 7 else if |A (i) | &gt; 1 and |A (i) | mod 2 = 0 then // Number of docs = 2,4,.. 8 G (2) ←-G (2) ∪ {A (i) } 9 else if |A (i) | &gt; 1 and |A (i) | mod 2 = 1 then // Number of docs = 3,5,.. 10 G (3) ←-G (3) ∪ {A (i) } 11 end 12 end</formula><p>/ * Assign one doc of authors in group 3 to group 1, assign all remaining docs to group 2. * / 13 forall A ∈ G (3) </p><formula xml:id="formula_26" coords="9,138.02,304.62,105.09,37.47">do 14 randomly draw (a, f, d) ∈ A 15 G (1) ←-G (1) ∪ {(a, f, d)} 16</formula><p>Table <ref type="table" coords="9,159.19,434.05,4.98,12.00" target="#tab_0">2</ref> reports the evaluation results<ref type="foot" coords="9,281.20,433.19,3.49,8.40" target="#foot_0">3</ref> for our proposed system over the dev set and the test set <ref type="foot" coords="9,162.62,445.15,3.49,8.40" target="#foot_1">4</ref> . Rows 1-3 show the performance on the dev set and rows 6-8 show the corresponding results on the test set. We used the early-bird feature of the challenge to get a first impression of how our model behaves on the test data. The comparatively good results of our early-bird submission on the dev data (see row 1) suggest that our train and dev sets must be approximately stratified. Comparing these results with the significantly lower performance of the early-bird system on the test set (see row 6), however, indicates that there must be some type of intentional mismatch between the train set and the test set of the challenge. We suspect a shift in the relation between authors and fandom topics. For our early-bird submission we did not yet use the provided fandom labels. After the early-bird deadline, however, we incorporated the contextual prefixes. Comparing row 1 (without prefix) with row 4 (prefix included) we observe a noticeable improvement. One possible explanation for this improvement could be that the model is now better able to recognize stylistic variations between authors who are writing in the same fandom-based domain. If we compare rows 4 &amp; 5 with rows 2 &amp; 3, we see the (1) , G (2) } = MAKETWOGROUPS(A (1) , . . . A (N ) ) 5 while |G (2) | &gt; 0 or |G (1) | &gt; 1 do // Sample same-author pair {D, G (1) , G (2) } ←-CLEANAFTERSAMPLING(A, D, G (1) , G (2) )</p><formula xml:id="formula_27" coords="10,174.60,118.30,214.63,54.05">Algorithm 3: SAMPLEPAIRS 1 Input: A (i) = {(a (i) , f (i) 1 , d (i) 1 ), . . . , (a (i) , f (i) N i , d (i) N i )} ∀i ∈ {1, . . . N } 2 Output: D 3 Initialize D = {∅} 4 {G</formula><formula xml:id="formula_28" coords="10,174.92,192.66,103.24,27.91">6 if |G (2) | &gt; 0 then 7 randomly draw A ∈ G (2) 8 G (2) ←-G (2) \ {A}</formula><p>13 end // Sample different-authors pair</p><formula xml:id="formula_29" coords="10,172.43,276.74,72.78,17.85">14 if |G (1) | &gt; 1 then 15</formula><p>randomly draw (a (1) , f (1) , d (1) ) ∈ G (1) and (a (2) , f (2) , d (2) ) ∈ G (1)   16</p><p>G (1) ←-G (1) \ {(a (1) , f (1) , d (1) ), (a (2) , f (2) , d (2) )} 17 D ←-D ∪ {(d (1) , d (2) , f (1) , f (2) , 0)}</p><formula xml:id="formula_30" coords="10,172.43,316.18,130.75,37.97">18 else if |G (2) | &gt; 1 then 19 randomly draw A (1) , A (2) ∈ G (2) 20 G (2) ←-G (2) \ {A (1) A (2) } 21</formula><p>randomly draw (a (1) , f (1) , d (1) ) ∈ A (1) and (a (2) , f (2) , d (2) ) ∈ A (2)   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22</head><p>A (1) ←-A (1) \ {(a (1) , f (1) , d (1) )} and A (2) ←-A (2) \ {(a (2) , f (2) , d (2) )} 23 D ←-D ∪ {(d (1) , d (2) , f (1) , f (2) , 0)} 24 for A ∈ {A (1) , A (2) } do 25 {D, G (1) , G (2) } ←-CLEANAFTERSAMPLING(A, D, G (1) , G  <ref type="formula" coords="10,259.22,467.59,3.87,12.00" target="#formula_3">3</ref>) and ( <ref type="formula" coords="10,290.57,467.59,7.64,12.00" target="#formula_21">16</ref>), we can also take into account the betweenauthor and within-author variations to validate the training progress of our model. Both, between-author and within-author variations can be characterized by determining the entropy w.r.t. the estimated covariance matrices B -1 and W -1 . It is well-known that entropy can function as a measure of uncertainty. For multivariate Gaussian densities, the analytic solution of the entropy is proportional to the determinant of the covariance matrix. From Eq. ( <ref type="formula" coords="10,209.76,542.56,3.87,12.00" target="#formula_7">5</ref>) and ( <ref type="formula" coords="10,240.40,542.56,3.53,12.00" target="#formula_8">6</ref>), we have</p><formula xml:id="formula_32" coords="10,141.59,559.64,339.00,21.88">H N (x|µ, B -1 ) ∝ log det B -1 and H N ( |0, W -1 ∝ log det W -1 .<label>(18)</label></formula><p>Fig. <ref type="figure" coords="10,153.43,580.68,4.98,12.00">3</ref> presents the entropy curves. As expected, during the training, the within-author variability decreased while the between-author variability increased. Fig. <ref type="figure" coords="10,168.81,605.90,4.98,12.00" target="#fig_7">4</ref> shows the attention-heatmaps of two fanfiction excerpts. From a visual inspection of many of such heatmaps we made the following observations: In contrast to the Amazon reviews used in <ref type="bibr" coords="10,266.29,629.81,10.58,12.00" target="#b3">[4]</ref>, fanfiction texts do not contain a lot of "easy-tovisualize" linguistic features such as spelling errors for example. The model focuses on different aspects. Similar to <ref type="bibr" coords="10,248.10,653.72,10.58,12.00" target="#b3">[4]</ref> &lt;Batman&gt; with an update as soon as I can ! ' Can you believe that ? ' Carly was fuming . ' Fourteen boys in one house ? Absolutely archaic &lt;Batman&gt; house ? Absolutely archaic ! There 's hardly enough room for five , maybe ten ... And his eye ! How could they let that happen ? Oh , &lt;Batman&gt; happen ? Oh , I know ! there 's a half dozen kids too many occupying a space for ... Are you even listening to me ? ' She pronouns, conjunctions). Surprisingly, punctuation marks like "..." seem to be less important than observed in <ref type="bibr" coords="11,243.65,421.32,10.58,12.00" target="#b3">[4]</ref>. In the first sentence of excerpt 1, the phrase "stopped and looked" is marked. In the second sentence, the word "look" of this phrase is repeated in the overlapping part but not marked anymore. Contrarily, repeated single words like "Absolutely" in excerpt 2 remain marked. It seems that our model is able to analyze how an author is using a word in a particular context.</p><p>Lastly, to keep the CPU memory requirements as low as possible on Tira <ref type="bibr" coords="11,447.02,481.24,15.27,12.00" target="#b18">[19]</ref>, we fed every single test document separately and sequentially into the ensemble of trained models, resulting in a runtime of approximately 6 hours. This can, of course, be done batch-wise and in parallel for all models in the ensemble to reduce training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and future work</head><p>We presented a new type of authorship verification (AV) system that combines neural feature extraction with statistical modeling. By recombining document-pairs after each training epoch, we significantly increased the heterogeneity of the train data. The proposed method achieved excellent overall performance scores, outperforming all other systems that participated in the PAN 2020 Authorship Verification Task, in both the small dataset challenge as well as the large dataset challenge. In AV there are many variabilities (such as topic, genre, text length, etc.) that negatively affect the system performance. Great opportunities for further gains can, thus, be expected by incorporating compensation techniques that deal with these aspects in future challenges.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,281.29,345.82,11.67;4,134.77,292.58,231.52,10.80"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Flowchart of the neural feature extraction as described in [4]. A given text sample is transformed into the learned metric space by the function A θ (•).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,351.84,134.02,3.76,6.97;6,358.59,128.55,14.39,12.00;6,375.47,130.03,8.41,17.29;6,383.88,134.02,4.15,6.97;6,388.53,128.55,2.77,12.00;6,147.40,145.48,30.78,12.00;6,178.55,151.89,3.97,6.97;6,183.02,147.22,10.31,9.96;6,193.70,151.89,3.97,6.97;6,198.16,147.22,46.45,9.96;6,244.99,151.89,3.97,6.97;6,249.46,147.22,10.31,9.96;6,260.13,151.89,3.97,6.97;6,264.60,146.97,11.18,17.29;6,275.78,147.22,8.13,10.71;6,286.13,146.97,7.75,17.29;6,296.09,147.22,29.30,9.96;6,325.76,151.89,3.97,6.97;6,330.23,147.22,10.31,9.96;6,340.91,151.89,3.97,6.97;6,345.38,146.97,11.18,17.29;6,356.56,147.22,8.52,10.71;6,204.81,162.16,48.84,10.71;6,255.86,161.91,7.75,17.29;6,265.82,162.16,38.33,10.71;6,306.36,161.91,7.75,17.29;6,316.33,162.16,38.33,10.71;6,214.22,177.11,39.26,9.96;6,253.84,181.78,3.97,6.97;6,258.31,176.86,9.33,17.29;6,267.65,177.11,49.82,10.71;6,317.83,181.78,3.97,6.97;6,322.30,176.86,9.33,17.29;6,331.63,177.11,8.34,10.71;6,342.19,176.86,7.75,17.29;6,352.15,177.11,29.30,9.96;6,381.82,181.78,3.97,6.97;6,386.29,176.86,9.33,17.29;6,395.62,177.11,8.34,10.71;6,406.18,176.86,7.75,17.29;6,416.14,177.11,29.30,9.96;6,445.81,181.78,3.97,6.97;6,450.28,176.86,9.33,17.29;6,459.61,177.11,8.34,10.71;6,214.22,191.80,7.75,17.29;6,224.18,192.05,33.95,10.71;6,258.63,191.80,8.65,17.29;6,267.65,196.73,3.97,6.97;6,272.12,192.05,10.31,9.96;6,282.79,196.73,3.97,6.97;6,287.26,192.05,50.00,10.71;6,337.76,191.80,8.65,17.29;6,346.78,196.73,3.97,6.97;6,351.25,192.05,50.00,10.71;6,401.75,191.80,8.65,17.29;6,410.77,196.73,3.97,6.97;6,415.24,192.05,3.87,9.96"><head></head><label></label><figDesc>s and H d : score(y 1 , y 2 ) = log p(y 1 , y 2 |H s )log p(y 1 , y 2 |H d ) = log p(x 0 )log p(x 1 )log p(x 2 ) + log p(y 1 |x 0 ) + log p(y 2 |x 0 )log p(y 1 |x 1 )log p(y 2 |x 2 ) log p(x 0 |y 1 , y 2 ) + log p(x 1 |y 1 ) + log p(x 2 |y 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,186.73,119.79,284.05,8.13;8,258.02,198.47,200.35,8.13;8,208.19,163.76,43.81,8.13;8,249.71,181.98,31.17,8.13;8,304.61,181.97,55.20,8.13;8,144.55,147.04,215.66,8.13"><head>"</head><label></label><figDesc>Yes , Master Luke , " Rey says , a little surprised . " How did you know ? " " You [...] &lt;Star Wars&gt; , a little surprised . " How did you know ? " " You window length hop length overlapping length &lt;Star Wars&gt; " Yes , Master Luke , " &lt;UNK&gt; says , a little surprised .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,172.96,214.58,267.20,11.67"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example of our sliding window approach with contextual prefix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,175.24,222.85,2.49,6.48;10,205.62,221.01,123.95,8.64;10,172.75,231.62,4.98,6.48;10,205.62,231.50,105.83,6.29;10,172.75,240.39,4.98,6.48;10,205.62,240.27,93.94,6.29;10,172.75,250.45,4.98,6.48"><head>9</head><label></label><figDesc>randomly draw (a, f1, d1), (a, f2, d2) ∈ A 10 A ←-A \ {(a, f1, d1), (a, f2, d2)} 11 D ←-D ∪ {(d1, d2, f1, f2, 1)} 12</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="10,404.41,387.00,2.87,6.29;10,172.75,395.89,4.98,6.48;10,205.62,393.78,11.16,9.33;10,172.43,405.45,4.98,6.48;10,193.35,403.35,11.16,9.33;10,172.11,412.91,20.13,9.33;10,134.77,430.42,345.82,12.00;10,134.77,442.37,345.82,12.00;10,134.77,454.33,310.79,12.00;10,149.71,467.59,109.52,12.00"><head></head><label></label><figDesc>proposed ensemble inference strategy. Combining a set of trained models leads to higher scores. Comparing rows 2 &amp; 3 and rows 7 &amp; 8 we find, unsurprisingly, that the training on the large dataset improves the performance results as well.Besides the losses in Eqs. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="11,134.77,378.31,345.83,11.67;11,134.77,389.59,297.54,10.80"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Attention-heatmaps. Blue hues encode the sentence-based attention weights and red hues denote the relative word importance. All tokens are delimited by whitespaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,258.68,653.72,221.91,12.00"><head>Table 2 .</head><label>2</label><figDesc>, the model rarely marked function words (e.g. articles, Results w.r.t the provided metrics on the dev and test sets. Potter&gt; grabbed Scarlet , and rushed out of the common room . ' Draco , let go you 're hurting me . ' He stopped and looked at her bruising &lt;Harry Potter&gt; looked at her bruising wrist . ' Sorry , ' he said letting go . She rubbed it , hissing a bit at the soreness . ' It 's &lt;Harry Potter&gt; . ' It 's alright . Why were you arguing with them in the first place ? ' He hesitated for a moment and answered , ' She just</figDesc><table coords="11,126.88,115.34,346.53,195.80"><row><cell></cell><cell></cell><cell></cell><cell cols="3">ADHOMINEM train set evaluation AUC</cell><cell cols="2">c@1 f_05_u F1 overall</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 early-bird</cell><cell>small</cell><cell cols="2">dev set 0.964 0.919 0.916 0.932</cell><cell>0.933</cell></row><row><cell>0 10 -10</cell><cell>0</cell><cell>20000 update steps log det B -1 log det W -1 40000</cell><cell>2 ensemble 3 ensemble 4 single 5 single 6 early-bird 7 ensemble 8 ensemble</cell><cell>small large small large small small large</cell><cell cols="2">dev set 0.977 0.942 0.938 0.946 dev set 0.985 0.955 0.940 0.959 dev set 0.975 0.943 0.921 0.951 dev set 0.983 0.950 0.944 0.954 test set 0.923 0.861 0.857 0.891 test set 0.940 0.889 0.853 0.906 test set 0.969 0.928 0.907 0.936</cell><cell>0.951 0.960 0.948 0.958 0.883 0.897 0.935</cell></row><row><cell cols="3">Figure 3. Entropy curves.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Fanfiction excerpt 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Fanfiction excerpt 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="11,146.07,239.21,22.07,9.18"><p>&lt;Harry</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="9,144.73,621.57,335.86,10.80;9,144.73,632.53,154.42,10.80"><p>The source code will be publicly available to interested readers after the peer review notification, including the set of hyper-parameters.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="9,139.00,642.63,2.99,7.20;9,144.73,643.68,335.86,10.80;9,144.73,654.64,234.04,10.80"><p><ref type="bibr" coords="9,139.00,642.63,2.99,7.20" target="#b3">4</ref> The test set was not accessible to the authors. Results on the test set were generated by the organizers of the PAN challenge via the submitted program code.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was in significant parts performed on a HPC cluster at <rs type="institution">Bucknell University</rs> through the support of the <rs type="funder">National Science Foundation</rs>, Grant Number <rs type="grantNumber">1659397</rs>. Project funding was provided by the state of <rs type="funder">North Rhine-Westphalia</rs> within the <rs type="funder">Research Training Group "</rs><rs type="projectName">SecHuman -Security for Humans in Cyberspace</rs>."</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4pxwmPz">
					<idno type="grant-number">1659397</idno>
				</org>
				<org type="funded-project" xml:id="_XyuFs2b">
					<orgName type="project" subtype="full">SecHuman -Security for Humans in Cyberspace</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 2: CLEANAFTERSAMPLING 1 Input: A, D, G (1) , G (2)  2 Output: D, G (1) , G (2)    </p><p>authors (l = 0). We obtain re-sampled pairs via D = SAMPLEPAIRS(A (1) , . . . , A (N ) ) in Alg. 3. The epoch-wise sampling of new pairs can be accomplished beforehand to speed up the training phase.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.61,206.72,316.55,10.80;12,150.95,217.67,251.97,10.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,196.78,206.72,246.07,10.80">Author Identification using multi-headed Recurrent Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,150.95,217.67,225.83,10.80">CLEF Evaluation Labs and Workshop -Working Notes Papers</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,227.66,319.66,10.80;12,150.95,238.62,155.08,10.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,279.80,227.66,182.47,10.80;12,150.95,238.62,69.62,10.80">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,238.55,238.62,41.34,10.80">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,248.61,320.84,10.80;12,150.95,259.57,325.28,10.80;12,150.95,270.53,83.18,10.80" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,199.28,259.57,248.55,10.80">The Importance of Suppressing Domain Style in Authorship Analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bischoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Deckers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schliebs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno>CoRR abs/2005.14714</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,280.52,300.98,10.80;12,150.95,291.48,303.33,10.80;12,150.95,302.43,56.03,10.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,358.17,280.52,85.42,10.80;12,150.95,291.48,244.70,10.80">Explainable Authorship Verification in Social Media via Attention-based Similarity Learning</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boenninghoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hessler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Nickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,413.95,291.48,40.34,10.80;12,150.95,302.43,29.89,10.80">Proc. IEEE BigData</title>
		<meeting>IEEE BigData</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,312.42,335.85,10.80;12,150.95,323.38,195.01,10.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,352.69,312.42,125.77,10.80;12,150.95,323.38,98.91,10.80">Similarity Learning for Authorship Verification in Social Media</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boenninghoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,268.50,323.38,51.32,10.80">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,333.37,329.00,10.80;12,150.95,344.33,46.57,10.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,281.90,333.37,117.92,10.80">The speaker partitioning problem</title>
		<author>
			<persName coords=""><forename type="first">Niko</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>De Villiers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,418.28,333.37,53.32,10.80;12,150.95,344.33,20.43,10.80">Proc. Odyssey. ISCA</title>
		<meeting>Odyssey. ISCA</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,354.32,320.85,10.80;12,150.95,365.28,61.62,10.80" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,202.89,354.32,256.88,10.80">A farewell to SVM: Bayes factor speaker detection in supervector space</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct coords="12,142.61,375.27,321.17,10.80;12,150.95,386.23,329.64,10.80" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,180.09,386.23,187.19,10.80">In defence of metric learning for speaker recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">S</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Han</surname></persName>
		</author>
		<idno>CoRR abs/2003.11982</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,396.21,314.37,10.80;12,150.95,407.17,311.50,10.80;12,150.95,418.13,78.69,10.80" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,426.24,396.21,30.75,10.80;12,150.95,407.17,204.03,10.80">Pairwise Discriminative Speaker Verification in the I-Vector Space</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cumani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Laface</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasilakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,361.17,407.17,97.27,10.80">IEEE Trans. Audio, Speech</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Lang. Process</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,428.12,239.94,10.80" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Degroot</surname></persName>
		</author>
		<title level="m" coord="12,201.75,428.12,99.80,10.80">Optimal statistical decisions</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,438.11,292.41,10.80;12,150.95,449.07,302.80,10.80" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,198.77,438.11,109.69,10.80">Authorship attribution analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ehrhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,387.58,438.11,47.07,10.80;12,150.95,449.07,128.19,10.80">Handbook of Communication in the Legal Sphere</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Visconti</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin/Boston</addrLine></address></meeting>
		<imprint>
			<publisher>de Gruyter</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="169" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,459.06,332.41,10.80;12,150.95,470.02,118.05,10.80" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,279.64,459.06,195.01,10.80;12,150.95,470.02,29.42,10.80">Assessing the Applicability of Authorship Verification Methods</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Halvani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Graner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,199.02,470.02,43.84,10.80">Proc. ARES</title>
		<meeting>ARES</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,480.00,302.23,10.80" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,267.53,480.00,93.01,10.80">Long Short-Term Memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,367.51,480.00,50.80,10.80">Neural Comp</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,489.99,328.34,10.80;12,150.95,500.95,104.75,10.80" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,237.49,489.99,233.08,10.80;12,150.95,500.95,15.85,10.80">Discriminative Deep Metric Learning for Face Verification in the Wild</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,185.21,500.95,44.34,10.80">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,510.94,330.00,10.80;12,150.95,521.90,104.34,10.80" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,184.59,510.94,152.17,10.80">Probabilistic Linear Discriminant Analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,184.32,521.90,44.83,10.80">Proc. ECCV</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</editor>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,531.89,334.17,10.80;12,150.95,542.85,329.64,10.80;12,150.95,553.81,316.30,10.80;12,150.95,564.76,190.99,10.80" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,234.75,542.85,245.84,10.80;12,150.95,553.81,16.14,10.80">Overview of the Cross-Domain Authorship Verification Task at PAN 2020</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="12,390.29,553.81,76.96,10.80;12,150.95,564.76,38.13,10.80">CLEF 2020 Labs and Workshops</title>
		<title level="s" coord="12,195.56,564.76,85.63,10.80">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,574.75,334.10,10.80;12,150.95,585.71,150.57,10.80" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,193.57,574.75,282.77,10.80;12,150.95,585.71,55.97,10.80">Deep Dive into Authorship Verification of Email Messages with Convolutional Neural Network</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Litvak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,225.56,585.71,49.82,10.80">Proc. SIMBig</title>
		<meeting>SIMBig</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,595.70,299.60,10.80" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Potha</surname></persName>
		</author>
		<title level="m" coord="12,189.31,595.70,83.28,10.80">Authorship Verification</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>University of the Aegean</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="12,142.24,605.69,335.40,10.80;12,150.95,616.65,296.90,10.80" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,333.86,605.69,140.16,10.80">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,259.53,616.65,124.99,10.80">IR Evaluation in a Changing World</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,626.64,324.81,10.80;12,150.95,637.60,57.14,10.80" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,207.24,626.64,190.36,10.80">A Survey of Modern Authorship Attribution Methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,404.04,626.64,63.01,10.80;12,150.95,637.60,31.00,10.80">J. Assoc. Inf. Sci. Technol</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,647.58,314.26,10.80" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,207.24,647.58,161.18,10.80">Authorship Attribution Using Text Distortion</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,386.38,647.58,43.97,10.80">Proc. EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
