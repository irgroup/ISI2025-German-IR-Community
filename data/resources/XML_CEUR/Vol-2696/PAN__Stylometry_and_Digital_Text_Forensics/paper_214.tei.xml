<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.83,115.90,305.70,12.90;1,192.62,133.83,230.13,12.90;1,223.43,153.76,168.50,10.75">Detecting Fake News Spreaders with Behavioural, Lexical and Psycholinguistic Features Notebook for PAN at CLEF 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,173.48,190.24,61.14,8.64"><forename type="first">HÃ©ctor</forename><surname>Ricardo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.11,190.24,57.42,8.64"><forename type="first">Murrieta</forename><surname>Bello</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.03,190.24,63.98,8.64"><forename type="first">Lukas</forename><surname>Heilmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,389.02,190.24,52.85,8.64"><forename type="first">Esben</forename><surname>Ronan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.83,115.90,305.70,12.90;1,192.62,133.83,230.13,12.90;1,223.43,153.76,168.50,10.75">Detecting Fake News Spreaders with Behavioural, Lexical and Psycholinguistic Features Notebook for PAN at CLEF 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">93893DAE43AE575D903B485892D2B0F3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>fake news detection</term>
					<term>gradient boosting</term>
					<term>liwc</term>
					<term>psycholinguistic features</term>
					<term>behavioural features</term>
					<term>lexical features</term>
					<term>feature engineering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a multilingual approach to identifying fake news spreaders on Twitter data, as part of the PAN 2020 competition for author profiling. We manually engineered domain-specific features covering behavioural, lexical and psycholinguistic aspects and evaluated them using traditional machine learning models. The focus of this paper is exploring the problem domain by testing domain-specific features on different types of classifiers first, and evaluating a pure multilingual approach on combined English and Spanish texts second. Out of the methods tested, the Gradient Boosting classifiers performed best. We extended our experiments to a multilingual design using less preprocessing and feature selection, with a comparable result to the monolingual Gradient Boosting models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fake news has become a catchphrase present all over the political arena in the recent years, and its presence is rapidly increasing. The effects are wide-ranging. There is, for example, an undeniable interference of fake news in traditional political processes like political campaigns. It is believed that fake news generated to favor either of the two nominees in the 2016 presidential campaign was shared up to 37 million times on Facebook <ref type="bibr" coords="1,190.04,533.68,15.27,8.64" target="#b18">[19]</ref>, the top 20 of which generating up to 9 million shares. Fake news reporting the injury of Barack Obama in an explosion caused a wiping out of up to $130 billion in stock value <ref type="bibr" coords="1,243.28,557.59,15.27,8.64" target="#b15">[16]</ref>.</p><p>Fake news is not a new problem, but its ability to be easily and rapidly disseminated in large scale across a population is. Fake news as a term thus uniquely locates its domain in the realm of social media, where its new, pressing importance is found. The rapid democratisation of public expression that social media encapsulates operates hand-in-hand with the new role of fake news.</p><p>The multifaceted nature of this phenomenon, and it's crucial relevance to the functioning of political processes, public discourse, and the well-being of citizens highlights the importance to develop cutting-edge techniques in detection of fake news spreading, so damaging information can be cut off before it spreads beyond a critical mass.</p><p>Our study answers the PAN Shared Task "Profiling Fake News Spreaders on Twitter" <ref type="bibr" coords="2,480.56,167.43,15.90,8.64" target="#b11">[12]</ref>. We approach the problem using a classical machine learning approach utilising Gradient-Boosting methods on domain-specific features, such as lexical features, grammatical features, sentimental analysis with LIWC <ref type="foot" coords="2,298.84,201.62,3.49,6.05" target="#foot_0">1</ref> , SentiWordnet <ref type="foot" coords="2,362.07,201.62,3.49,6.05" target="#foot_1">2</ref> and EmoLex<ref type="foot" coords="2,419.02,201.62,3.49,6.05" target="#foot_2">3</ref> , Twitter-DNA as well as TF-IDF vectorisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Fake news is a complex phenomenon that has elicited a great variety of approaches from different fields of academia. <ref type="bibr" coords="2,248.48,285.95,16.60,8.64" target="#b18">[19]</ref> provides a succinct overview over four broad types of approaches hitherto taken: i) "knowledge-based", which focuses upon the truth-value of the knowledge content of the news; ii) "style-based", which concerns the linguistic manner in which news is communicated; iii) "propagation-based", which focuses on how fake news spreads through a network, and iv) "credibility-based", which investigates the credibility of those who create and those who spread news. We will briefly discuss previous research in the category most relevant to our task: Style-based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Style-based Fake News Detection</head><p>Style-based approaches focus on quantifiable stylistic features of texts, upon which machine learning algorithms can be trained to detect characteristics of the expression of fake news creators and spreader. These tend towards assessing the intention of the text, over its content. There is good experimental evidence in forensic studies to suggest statements derived from factual experiences differ from those derived from statements based in fantasy <ref type="bibr" coords="2,203.26,470.22,15.27,8.64" target="#b18">[19]</ref>. Not only this, but the current state-of-the-art in style-based approaches varies from 60-90% <ref type="bibr" coords="2,254.01,482.17,15.27,8.64" target="#b18">[19]</ref>, which is significantly higher than a normal person's ability to differentiate, as mentioned in the introduction.</p><p>A variety of stylistic features can be studied, two main streams of which <ref type="bibr" coords="2,442.82,506.38,16.60,8.64" target="#b18">[19]</ref> have identified. The first is "attribute-based language features", which derive from psychological deception theories. They include attributes such as: Quantity (counts of characters, words, sentences, etc), sentiment (positivity and negativity), diversity (number of unique words, unique content words, etc), and uncertainty (degree of modal words, quantifiers, generalising terms, etc used). The other category spans "structure-based language features", which describes content style from four different language levels: i) lexicon, ii) syntax, iii) semantic and iv) discourse. The lexicon level assesses frequency of letters and words using n-gram models or TF-IDF. The syntax level uses NLP techniques such as POS-tagging. The semantic level identifies topics that texts cover, using packages such as LIWC. Discourse features include features on the level furthest out, such as Rhetorical Structure Theory (RST). Generally, it seems to be the case that with only one language used, lexicon-level features perform better than all others -11 out of 14 studies studied by <ref type="bibr" coords="3,223.17,155.18,16.60,8.64" target="#b18">[19]</ref> report better performance at the lexicon-level. [?] achieved a very high level performance (75-99% accuracy) detecting fake news on a selection of Bulgarian news sites. They utilised linguistic features (n-grams), credibility-related features such as sentiment polarity, capitalisation and punctuation, as well as semantic embeddings. More recently, <ref type="bibr" coords="3,247.96,203.00,11.62,8.64" target="#b3">[4]</ref> introduced the usage of novel stylometric features such as Twitter DNA, text readability and TF-IDF to assess the author's profile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Author Profiling</head><p>A related task to a style-based fake news detection is author profiling. Author profiling is an old problem where one attempts to infer characteristics about the author of a text based on stylistic analysis. This could be, for example, gender, age, native language, personality, age, economic class. Author profiling on social media inspires its own set of problems but previous PAN tasks have shown a success in predicting a variety of characteristics from Twitter data: Whether or not the user is a bot <ref type="bibr" coords="3,397.23,328.12,10.58,8.64" target="#b2">[3]</ref>, personality traits <ref type="bibr" coords="3,134.77,340.08,15.27,8.64" target="#b14">[15]</ref>, gender <ref type="bibr" coords="3,185.94,340.08,16.60,8.64" target="#b12">[13]</ref> and language variety <ref type="bibr" coords="3,290.27,340.08,15.27,8.64" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multilingual Approaches</head><p>An interesting direction this research is beginning to take is multilingualism. Combining features across different languages was found to outperform those using features from a single language in eight out of twelve studies, according to a survey by <ref type="bibr" coords="3,425.21,417.38,15.27,8.64" target="#b18">[19]</ref>. A recent innovative approach is the one shown by <ref type="bibr" coords="3,298.70,429.34,10.58,8.64" target="#b2">[3]</ref>, which incorporated counting features and psycho-linguistic features in order to assess whether an user is a bot or a human. They proposed an ensemble model utilising a neural network and a fine-tuned BERT model to a high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>The provided dataset had various idiosyncrasies that provided various constraints to analysis. All meta data of users and tweets (e.g. profile picture, timestamps) was omitted -only the textual data remained. This prevented any analysis according to suggestive avatars, or posting time patterns. Beyond that, URLs, hashtags and usernames retweeted were replaced with the respective placeholders #URL#, #HASHTAG#, #USER#. This prevented any classification according to the news they were spreading, and instead forced us to focus on stylistic features specifically.</p><p>The task itself was crucially focused upon classifying fake news spreaders, which meant we had to concentrate upon the stylistic features of a fake news spreader. Knowledgebased approaches did not seem relevant, since tweets did not necessarily make knowledge claims, but often consisted of clickbait-like expression, e.g."#HASHTAG #The One Super Bowl Moment We Can't Stop Watching #URL# #URL#".</p><p>An interesting subtlety that makes this task more difficult, is there may well be a large presence of "naive" fake news spreaders, who only intermittently spread fake news. Thus, some users may only partially have tweets with fake news as content, but they are classified as a whole as fake news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method 4.1 Preprocessing</head><p>To preprocess the raw data, we implemented separate steps that specifically tackled the idiosyncrasies of the dataset. In the first step, we removed all tags which we labelled as user "behavior": Retweets, URLs, hashtags and mentions of users. Further steps provided functionality for: lowercasing words, removing extra whitespaces, expanding contractions, removing punctuation, removing stop words, removing numbers, lemmatizing and removing ellipses. Lemmatisation and lowercasing were reserved for higher level feature extraction like TF-IDF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Extraction</head><p>We chose to extract domain-specific features from the respectively preprocessed texts in English and Spanish. We did this through multiple extraction steps, which we shall illustrate in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioral Features</head><p>The first features to be extracted were the URL, hashtag, retweet and user counts per user. We did this because initial data explorations indicated that fake news spreaders often used more hashtags and retweets than normal users.</p><p>To investigate the structural distribution of these elements we decided to explore a novel tweet history DNA algorithm designed by <ref type="bibr" coords="4,338.96,458.78,10.58,8.64" target="#b4">[5]</ref>. This algorithm was used in the context of bot detection, but it provided a novel way to analyse the behaviour of a Twitter user. We implemented it with some modifications for our purposes. As proposed in <ref type="bibr" coords="4,144.96,494.64,10.58,8.64" target="#b4">[5]</ref>, specific tweet behaviors are represented by the values illustrated in . A weighted linear combination of these behavioural values in each tweet is transformed into a letter using the ASCII index.</p><formula xml:id="formula_0" coords="4,252.07,539.39,228.53,83.13">A n = ï£± ï£´ ï£´ ï£´ ï£´ ï£´ ï£´ ï£´ ï£´ ï£² ï£´ ï£´ ï£´ ï£´ ï£´ ï£´ ï£´ ï£´ ï£³ 0, plain 8, retweet 16, reply 1, has hashtags 2, has mentions 4, has URLs (1)</formula><p>For example, a retweet with hashtags, mentions, but no URLs would be calculated as 0 * 0 + 8 * 1 + 1 * 1 + 2 * 1 + 4 * 0 + 65 = 84 = T (each value multiplied by 1 if the feature is present and 0 if not present). Each resultant letter is concatenated to each other to form a tweet history DNA string 100 letters long. From this string we calculate the frequency of every character in the string so that we finish with a vector of 16 dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lexical Features</head><p>To extract all lexical, grammatical and miscellaneous features we implemented an additional vectorisation step. We counted all character-level features (digits, alpha characters, capital characters, punctuation, etc), all token-level features (word lengths, syllable counts, emoji/emoticon counts, etc), as well as POS tag counts, NER tag counts and others (see table below for more details).</p><p>To extract the emoticons and emojis, we utilised the emot library<ref type="foot" coords="5,410.57,335.55,3.49,6.05" target="#foot_3">4</ref> . To extract NER tags and POS tags we used the open-source SpaCy library. To count misspellings, we utilised the open-source pyspellchecker library <ref type="foot" coords="5,348.36,359.46,3.49,6.05" target="#foot_4">5</ref> . Due to the limited range of the library, it was crucial to subtract any identified named entities from the total count. We furthermore implemented some measures of vocabulary diversity. The first was hapax legomena frequency, which calculates the amount of words that appear only once <ref type="bibr" coords="5,134.77,426.33,15.27,8.64" target="#b9">[10]</ref>. Hapax legomena frequency is commonly used in author profiling <ref type="bibr" coords="5,413.39,426.33,10.58,8.64" target="#b5">[6]</ref>, as often their frequency correlates with the size of the author's vocabulary. Another way to measure vocabulary diversity is by the type-token ratio -the ratio of unique word count to total word count.</p><p>We also implemented a universal quantifier count, since we hypothesised that fake news spreaders often utilise more absolute manners of expression, e.g. "why liberals are always wrong".</p><p>The length of the final vector outputted per user in regard to lexical measures was 166. The exact composition is depicted in Table <ref type="table" coords="5,327.79,556.75,3.74,8.64">4</ref> Psycholinguistic Features Inspired by the work of Joo et. al. <ref type="bibr" coords="6,386.42,465.48,10.58,8.64" target="#b2">[3]</ref>, we considered psycholinguistic lexica as a valuable resources for extracting features. While humans are trained to perceive such underlying mental mechanisms, machines require the help of annotated lexical resources, which we implemented in a variety of manners.</p><p>We started off by incorporating sentiment polarity scores that have proven useful as facilitators for automatic authorship profiling in earlier competitions. <ref type="bibr" coords="6,420.96,526.37,11.62,8.64" target="#b8">[9]</ref> SentiWord-Net 3.0 6 , a publicly available lexical resource for opinion mining in English, holds three sentiment scores for each synonym set present in the popular WordNet 7 : positivity, negativity and objectivity. As an example, the English adjective "amazing" is represented by the values 0.875, 0.125 and 0 for the aforementioned measures. The noun "secret", however, has the scores 0.125, 0.375 and 0.5. Hence, "amazing" can be interpreted as a more positive word, while "secret" reveals a rather negative and objective nature. The objectivity score of each word is calculated with the formula 1 -(positivity + negativity), which consequently serves as a general measure of affectivity. To tackle words with a plurality of meanings, the part-of-speech tag of the given word is integrated. Accordingly, we computed part-of-speech tags per document as a preparatory step. We then queried SentiWordNet through the Natural Language Toolkit<ref type="foot" coords="7,375.29,141.55,3.49,6.05" target="#foot_7">8</ref> (NLTK) and transformed the obtained scores into features.</p><p>Linguistic Inquiry and Word Count<ref type="foot" coords="7,294.46,165.46,3.49,6.05" target="#foot_8">9</ref> (LIWC) is a lexical resource for psycholinguistic measures. By integrating the tool, we intended to capture the author's social and psychological states, which have proven to be effective in author profiling tasks. <ref type="bibr" coords="7,456.21,191.04,11.62,8.64" target="#b2">[3]</ref> To our advantage, the lexicon was available in both English and Spanish. With the purpose of summarizing language variables and linguistic dimensions, the first category LIWC captures includes word frequencies, POS-frequencies, etc. The second category aims at associating a word with certain psychological processes, such as affective processes, social processes (e.g. family), personal concerns, etc. <ref type="bibr" coords="7,376.75,250.82,11.62,8.64" target="#b2">[3]</ref> As we already covered a significant amount of the stylistic features in the aforementioned lexical approach, we directed our attention at the second category.</p><p>To deepen the study of the emotional patterns of fake news spreaders, we investigated the NRC Word-Emotion Association Lexicon<ref type="foot" coords="7,350.83,296.97,6.97,6.05" target="#foot_9">10</ref> (EmoLex) developed by Mohammad et. al.. This vectorizer captures eight basic emotions from Plutchik's wheel of emotions: anger, fear, anticipation, trust, surprise, sadness, joy, and disgust. <ref type="bibr" coords="7,442.93,322.55,19.09,8.64">[8,?]</ref> The lexicon has been used for sentiment and emotion analysis, abusive language detection, personality trait identification, etc. on word-, sentence-, and tweet-level and has proven useful in the SemEval shared task competition 2018. <ref type="bibr" coords="7,346.76,358.42,11.62,8.64" target="#b6">[7]</ref> Token Count Vectorizers An important set of features to employ alongside the aforementioned feature extraction methods is TF-IDF. TF-IDF is important in measuring not just the relevance of a word to a document (using frequency as a heuristic), but it's relative relevance, as compared to the compositions of other documents.</p><p>TF-IDF is composed of: The term frequency within a document, the frequency a term appears within a given document; and the inverse document frequency, which takes the logarithm of the inverse of the frequency of documents in which the term appears. These two are calculated for each term in each document and compiled into a vector. The vector is restricted to a length specified by the hyperparameter "max_features". After experimentation the performance was found to be optimum when max_features was equal to 500. This restriction includes only the most frequent terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Models</head><p>We investigated a variety of different experimental designs which we will now present. We used as input the aforementioned reduced set of features for English and Spanish texts, as well as all features, in order to allow for later comparisons.</p><p>To tune hyperparameters of each algorithm, we utilised a grid search automation. Grid search is an exhaustive brute-force algorithm that takes a list of lists specifying a range of values for each hyperparameter, derives all possible permutations and tests for their accuracy. Each test was achieved by averaging the accuracies of a repeated stratified 10-fold (scikit-learn's implementation). Since we used scikit-learn's implementation, we were unable to perform TF-IDF within each fold, since the module implements cross-validation automatically. We reasoned, however, that since it was only for tuning parameters, it did not matter so much.</p><p>Multinomial Naive Bayes Multinomial Naive Bayes is a supervised machine learning algorithm often used for text classification. It relies upon Bayesian conditional reasoning and generating data from which to classify. One manner of generating the data is by creating Gaussian distributions from the mean and standard deviation of each class, another is by creating simple multinomial distributions. Since "the multinomial distribution describes the probability of observing counts among a number of categories" <ref type="bibr" coords="8,134.77,268.54,15.27,8.64" target="#b17">[18]</ref>, this algorithm seems to be most appropriate for our vectors composed of count features.</p><p>We tested hyperparameters using grid search. The hyperparameters available to tune were: "Alpha", which we tested values between 0.5 and 1.5; and "fit_prior", for which there were values of True and False.</p><p>Random Forest Random Forest is a flexible and quick-to-train ensemble machine learning algorithm based upon decision trees <ref type="bibr" coords="8,321.05,357.99,15.27,8.64" target="#b16">[17]</ref>. Decision trees iteratively split the dataset into subsets according to a quantitative threshold along an axis, the resultant class for the subset derived from a majority vote. Random forest prevents the overfitting tendency of decision trees by averaging the results of decision trees trained upon randomly split training data (bagging).</p><p>Hyperparameters to adjust included number of estimators (decision trees), for which we selected logarithmically interpolated values: 10, 100, 1000, and max_features (for node-splitting), for which we chose: 'sqrt', 'log2'.</p><p>Support Vector Machine (SVM) SVM is a supervised learning algorithm which learns by finding the most optimum hyperplane to linearly separate data into distinct areas, corresponding to a positive and negative classification. The hyperplane is calculated by the utilisation of support vectors, data points close to the hyperplane, the distance (margin) from which is to be maximised. Though in its basic form it is designed for linear classification, it can be mapped to non-linear sets by using different "kernels" <ref type="bibr" coords="8,174.06,543.08,10.58,8.64" target="#b1">[2]</ref>.</p><p>To tune it we utilised scikit-learn's GridsearchCV with a stratified 5-fold cross validation procedure. Kernel parameters we tested were: 'poly' (polynomial), 'rbf' (radial base function), 'sigmoid' and 'linear'. The regularization parameter C values we tested were: 50, 10, 1.0, 0.1, and 0.01. We used "scale" as the gamma hyperparameter value and a linear kernel.</p><p>Gradient Boosting Gradient boosting is a powerful supervised machine learning algorithm that is born out of the theoretical assumption that "weak learners" can be turned into better learners <ref type="bibr" coords="8,211.05,656.44,10.58,8.64" target="#b0">[1]</ref>. Weak learners are observations who as hypotheses perform only slightly better than chance. An ensemble of these can sequentially handle their respective observation and iteratively tackle difficult problems. Though it is comparatively computationally expensive algorithm, it is powerful at grasping subtleties in data, and since our dataset is small, computational expenditure is not as big an issue.</p><p>The hyperparameters we selected to tune are: number of estimators with values of 10, 100, 500, 800, 1000; Learning rate with logarithmic values of: 0.001, 0.01, 0.1, subsample with values of 0.5, 0.7, 1.0. and max depth with values of 3, 7, 9. Since the GradientBoostingClassifier turned out to be our most promising algorithm, we decided to implement a manual grid search that allowed implementation of TF-IDF within each fold, so that we could be assured to discover the best parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We subsequently present our results for the aforementioned methods employed. As we aimed for a stabilized comparison of performances among the different experimental setups, we consistently utilized a stratified 10-fold cross validation with 3 repeats, each with a training size of 80% of the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment 1</head><p>For the first experiment we tested each dataset individually using a variety of machine learning algorithms previously described. First we tested the algorithms on just the domain-specific features as described in the method section. After that we tested them with a combination vector of domain-specific features and the TF-IDF vector. We then tuned the hyperparameters for each one using the SKLearn's GridSearchCV.</p><p>As can be seen in Table <ref type="table" coords="9,245.98,439.54,3.74,8.64" target="#tab_1">2</ref>, the Gradient Boosting algorithm performed significantly better on both the English and Spanish datasets than any of the other algorithms. Random Forest performed next best, with the Support Vector Machine and Multinomial Naive Bayes coming last. Interestingly, in every single case we tested, there was a higher accuracy on the Spanish dataset than on the English dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment 2</head><p>Our multilingual approach consisted of individual preprocessing of the English and the Spanish dataset, followed by extraction of the manually engineered features for each dataset. We deleted language dependent features such as sentimental analysis with Sen-tiNet (only English) and the additional ten LIWC categories for Spanish. We then concatenated the two vectors into one. Subsequently, we split the datasets into training and validation sets (20%) in each fold. Finally, as a further experiment, we merged English and Spanish tweets and fitted a TF-IDF vectorizer. In this setup we did not explore the application of any feature extraction algorithm or hyper-parameter optimization. Since the best algorithm in the monolingual setup was gradient boosting, we decided to use that algorithm for this section. As can be seen in the results table, the gradient boosting algorithm used upon the vector combining all features and TF-IDF functioned best, though not by a particularly large margin. There was however no increase in accuracy compared to the monolingual setups, and it performed significantly worse than upon just the spanish dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Features Accuracy Std Gradient Boosting All 0.71 0.075 Gradient Boosting All + TF-IDF 0.7218 0.0460 Table <ref type="table" coords="10,177.46,484.15,3.36,8.06">3</ref>. Accuracies and standard deviations for experiments in the multilingual setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on test data</head><p>Our final submission was made using the methodology of the second experiment. However during for the test phase, we predicted our answers separately for the Spanish tweet feed and for the English tweet feed. In order to test the efficiency of our algorithm we made use of the software submission platform <ref type="bibr" coords="10,320.89,620.57,16.60,8.64" target="#b10">[11]</ref> To our surprise, the accuracy on the Spanish dataset vastly outperformed the accuracy on the English dataset, with an accuracy of 0.7450 to 0.65550. This was in contrast to our first experiments, where the difference was markedly less.</p><p>Language accuracy English 0.6550 Spanish 0.7450 Table <ref type="table" coords="11,229.44,152.89,3.36,8.06">4</ref>. Results for English and Spanish on the test data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Our study has demonstrated the importance of carefully selected domain-specific features in the domain of fake news identification. These features are integral in classifying monolingual texts as well as multilingual text. We can conclude that it was possible to classify fake news spreaders on a limited dataset consisting of 300 Twitter users, by applying gradient boosting to a set of lexical, behavioural and psycholinguistic features and TFIDF to represent the text. Furthermore, these features have been shown to provide useful information in multilingual environments and are not limited to monolingual contexts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,331.53,556.75,11.21,8.64"><head>Table 1 .</head><label>1</label><figDesc>.2. Index of lexical features implemented in LexicalVectorizer</figDesc><table coords="6,219.74,118.07,175.88,295.10"><row><cell>Description</cell><cell>Number of features</cell></row><row><cell>Stop word count</cell><cell>1</cell></row><row><cell>Digit character count</cell><cell>1</cell></row><row><cell>Alpha character count</cell><cell>1</cell></row><row><cell>Capital character count</cell><cell>1</cell></row><row><cell>Capital character ratio</cell><cell>1</cell></row><row><cell>Capitalised word count</cell><cell>1</cell></row><row><cell>Capitalised word ratio</cell><cell>1</cell></row><row><cell>Punctuation count</cell><cell>1</cell></row><row><cell>Punctuation sequence count</cell><cell>1</cell></row><row><cell>Character count</cell><cell>1</cell></row><row><cell>Word count</cell><cell>1</cell></row><row><cell>Average word length</cell><cell>1</cell></row><row><cell>Syllable count</cell><cell>1</cell></row><row><cell>Average syllable count</cell><cell>1</cell></row><row><cell>Emoji count</cell><cell>1</cell></row><row><cell>Emoji ratio</cell><cell>1</cell></row><row><cell>Emoticon count</cell><cell>1</cell></row><row><cell>Emoticon ratio</cell><cell>1</cell></row><row><cell>Misspell count</cell><cell>1</cell></row><row><cell>Type-token ratio</cell><cell>1</cell></row><row><cell>Universal quantifier count</cell><cell>1</cell></row><row><cell>Hapax count</cell><cell>1</cell></row><row><cell>Individual alphabet count</cell><cell>26</cell></row><row><cell>Individual punctuation count</cell><cell>32</cell></row><row><cell>Individual POS tag counts</cell><cell>68</cell></row><row><cell>Individual NER tag counts</cell><cell>18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,185.14,118.07,245.07,197.56"><head>Table 2 .</head><label>2</label><figDesc>Results for English and Spanish</figDesc><table coords="10,185.14,118.07,245.07,186.70"><row><cell>Language</cell><cell>Model</cell><cell>Features</cell><cell>Accuracy Std</cell></row><row><cell cols="2">English Multinomial Naive Bayes</cell><cell>ALL</cell><cell>0.651 0.089</cell></row><row><cell cols="2">Spanish Multinomial Naive Bayes</cell><cell>ALL</cell><cell>0.703 0.073</cell></row><row><cell cols="4">English Multinomial Naive Bayes ALL + TFIDF 0.651 0.089</cell></row><row><cell cols="4">Spanish Multinomial Naive Bayes ALL +TFIDF 0.703 0.073</cell></row><row><cell>English</cell><cell>Random Forest</cell><cell>ALL</cell><cell>0.703 0.079</cell></row><row><cell>Spanish</cell><cell>Random Forest</cell><cell>ALL</cell><cell>0.72 0.070</cell></row><row><cell>English</cell><cell>Random Forest</cell><cell cols="2">ALL + TFIDF 0.71 0.078</cell></row><row><cell>Spanish</cell><cell>Random Forest</cell><cell cols="2">ALL +TFIDF 0.73 0.076</cell></row><row><cell cols="2">English Support Vector Machine</cell><cell>ALL</cell><cell>0.652 0.075</cell></row><row><cell cols="2">Spanish Support Vector Machine</cell><cell>ALL</cell><cell>0.717 0.085</cell></row><row><cell cols="4">English Support Vector Machine ALL + TFIDF 0.651 0.085</cell></row><row><cell cols="4">Spanish Support Vector Machine ALL +TFIDF 0.711 0.084</cell></row><row><cell>English</cell><cell>Gradient Boosting</cell><cell>ALL</cell><cell>0.669 0.075</cell></row><row><cell>Spanish</cell><cell>Gradient Boosting</cell><cell>ALL</cell><cell>0.73 0.068</cell></row><row><cell>English</cell><cell>Gradient Boosting</cell><cell cols="2">ALL + TFIDF 0.726 0.087</cell></row><row><cell>Spanish</cell><cell>Gradient Boosting</cell><cell cols="2">ALL +TFIDF 0.747 0.064</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,634.79,94.15,7.77"><p>http://liwc.wpengine.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,645.94,103.79,7.77"><p>http://sentiwordnet.isti.cnr.it/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.73,657.08,240.60,7.77"><p>https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,144.73,645.94,105.11,7.77"><p>https://pypi.org/project/emot/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,144.73,657.08,140.78,7.77"><p>https://pypi.org/project/pyspellchecker/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,144.73,646.10,103.79,7.77"><p>http://sentiwordnet.isti.cnr.it/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="6,144.73,657.08,108.00,7.77"><p>https://wordnet.princeton.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="7,144.73,634.79,76.47,7.77"><p>https://www.nltk.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="7,144.73,645.94,94.15,7.77"><p>http://liwc.wpengine.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="7,144.73,657.08,240.60,7.77"><p>https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.61,354.14,316.43,7.77;11,150.95,364.75,81.43,8.12" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,208.98,354.14,100.18,7.77">Stochastic gradient boosting</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,315.08,354.14,143.96,7.77">Computational statistics &amp; data analysis</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="367" to="378" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,375.70,326.99,7.77;11,150.95,386.31,244.81,8.12" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,378.34,375.70,87.25,7.77">Support vector machines</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,150.95,386.66,170.10,7.77">IEEE Intelligent Systems and their applications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="28" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,397.26,332.85,7.77;11,150.95,408.22,211.10,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,220.19,397.26,255.27,7.77;11,150.95,408.22,58.63,7.77">Author Profiling on Social Media: An Ensemble Learning Model using Various Features</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,215.45,408.22,100.29,7.77">Notebook for PAN at CLEF</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,418.83,336.28,7.77;11,150.95,429.79,310.51,7.77;11,150.95,440.75,23.90,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,243.63,418.83,163.32,7.77">Twitter bot detection using diversity measures</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kosmajac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Keselj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,425.36,418.83,53.54,7.77;11,150.95,429.79,277.64,7.77">Proceedings of the 3rd International Conference on Natural Language and Speech Processing</title>
		<meeting>the 3rd International Conference on Natural Language and Speech Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,451.35,324.71,7.77;11,150.95,462.31,86.44,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,243.63,451.35,184.01,7.77">Twitter user profiling: Bot and gender identification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kosmajac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Keselj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,445.40,451.35,21.92,7.77;11,150.95,462.31,60.29,7.77">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,472.91,320.75,7.77;11,150.95,483.87,225.17,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,324.04,472.91,139.33,7.77;11,150.95,483.87,96.62,7.77">Pan 2017: Author profiling-gender and language variety prediction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Martinc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Skrjanec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zupan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pollak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,265.53,483.87,84.45,7.77">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,494.48,330.23,7.77;11,150.95,505.43,311.12,7.77;11,150.95,516.39,175.36,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,401.82,494.48,71.02,7.77;11,150.95,505.43,64.91,7.77">Semeval-2018 Task 1: Affect in tweets</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bravo-Marquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Salameh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,233.90,505.43,228.17,7.77;11,150.95,516.39,57.20,7.77">Proceedings of International Workshop on Semantic Evaluation (SemEval-2018)</title>
		<meeting>International Workshop on Semantic Evaluation (SemEval-2018)<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,526.65,335.76,8.12;11,150.95,537.96,57.53,7.77" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,269.63,527.00,184.83,7.77">Crowdsourcing a word-emotion association lexicon</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="436" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,548.56,337.98,7.77;11,150.95,559.17,294.29,8.12" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,384.60,548.56,95.99,7.77;11,150.95,559.52,142.04,7.77">Automatic author profiling based on linguistic and stylistic features</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">G</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Saikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,298.64,559.52,100.29,7.77">Notebook for PAN at CLEF</title>
		<imprint>
			<biblScope unit="page">1179</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,570.12,337.85,7.77;11,150.95,580.73,123.53,8.12" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,249.57,570.12,142.23,7.77">Hapax legomena and language typology</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">I</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Altmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,397.77,570.12,82.32,7.77;11,150.95,581.08,39.86,7.77">Journal of Quantitative Linguistics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="370" to="378" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,591.69,335.40,7.77;11,150.95,602.65,306.17,7.77;11,150.95,613.60,72.72,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,333.86,591.69,140.16,7.77">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,259.53,602.65,193.52,7.77">Information Retrieval Evaluation in a Changing World</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09">Sep 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,624.21,323.93,7.77;11,150.95,635.17,325.30,7.77;11,150.95,646.13,300.97,7.77;11,150.95,657.08,96.22,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,333.29,624.21,132.88,7.77;11,150.95,635.17,218.44,7.77">Overview of the 8th Author Profiling Task at PAN 2020: Profiling Fake News Spreaders on Twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="11,264.50,646.13,117.33,7.77">CLEF 2020 Labs and Workshops</title>
		<title level="s" coord="11,388.31,646.13,63.62,7.77;11,150.95,657.08,19.77,7.77">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>NÃ©vÃ©ol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020-09">Sep 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,119.96,313.98,7.77;12,150.95,130.92,321.75,7.77;12,150.95,141.88,57.53,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,229.20,119.96,227.02,7.77;12,150.95,130.92,91.00,7.77">Overview of the 7th author profiling task at pan 2019: Bots and gender profiling in twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,259.55,130.92,129.69,7.77">Proceedings of the CEUR Workshop</title>
		<meeting>the CEUR Workshop<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,152.84,325.95,7.77;12,150.95,163.80,329.64,7.77;12,150.95,174.76,104.11,7.77" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="12,312.90,152.84,155.29,7.77;12,150.95,163.80,223.61,7.77">Overview of the 5th author profiling task at pan 2017: Gender and language variety identification in twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1613" to="0073" />
		</imprint>
	</monogr>
	<note>Working notes papers of the CLEF</note>
</biblStruct>

<biblStruct coords="12,142.24,185.71,330.50,7.77;12,150.95,196.67,327.30,7.77;12,150.95,207.63,137.86,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,437.74,185.71,35.00,7.77;12,150.95,196.67,133.73,7.77">Overview of the 3rd author profiling task at pan</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Rangel Pardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Celli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,321.55,196.67,156.71,7.77;12,150.95,207.63,78.86,7.77">CLEF 2015 Evaluation Labs and Workshop Working Notes Papers</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,218.59,267.81,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,195.77,218.59,161.61,7.77">Can &apos;fake news&apos; impact the stock market? by</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Rapoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,359.62,218.59,24.27,7.77">Forbes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,229.55,320.82,7.77;12,150.95,240.51,321.12,7.77;12,150.95,251.12,291.35,8.12" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,432.67,229.55,30.39,7.77;12,150.95,240.51,317.11,7.77">Random forest: a classification and regression tool for compound classification and qsar modeling</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Svetnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Culberson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">P</forename><surname>Feuston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,150.95,251.47,198.71,7.77">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1947" to="1958" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,262.43,317.86,7.77;12,150.95,273.39,104.51,7.77" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="12,206.24,262.43,244.52,7.77">Python data science handbook: Essential tools for working with data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,284.34,297.01,7.77;12,150.95,294.95,277.75,8.12" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="12,235.28,284.34,203.97,7.77;12,150.95,295.30,46.03,7.77">Fake news: A survey of research, detection methods, and opportunities</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zafarani</surname></persName>
		</author>
		<idno>CoRR abs/1812.00315</idno>
		<ptr target="http://arxiv.org/abs/1812.00315" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
