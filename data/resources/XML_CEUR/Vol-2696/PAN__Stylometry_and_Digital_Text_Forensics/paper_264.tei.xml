<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.48,115.90,336.40,12.90;1,253.28,133.83,108.80,12.90">Overview of the Cross-Domain Authorship Verification Task at PAN 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,158.11,171.88,64.29,8.64"><forename type="first">Mike</forename><surname>Kestemont</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Antwerp</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,233.90,171.88,79.14,8.64"><forename type="first">Enrique</forename><surname>Manjavacas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Antwerp</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.52,171.88,44.17,8.64"><forename type="first">Ilia</forename><surname>Markov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Antwerp</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.39,171.88,68.10,8.64"><forename type="first">Janek</forename><surname>Bevendorff</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,150.82,183.83,64.66,8.64"><forename type="first">Matti</forename><surname>Wiegmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.52,183.83,84.36,8.64"><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of the Aegean</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.04,183.83,60.36,8.64"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Leipzig University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,411.08,183.83,48.99,8.64"><forename type="first">Benno</forename><surname>Stein</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.48,115.90,336.40,12.90;1,253.28,133.83,108.80,12.90">Overview of the Cross-Domain Authorship Verification Task at PAN 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D19111FC28FC1CB6EBF4069B112F542A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authorship identification remains a highly topical research problem in computational text analysis with many relevant applications in contemporary society and industry. For this edition of PAN, we focused on authorship verification, where the task is to assess whether a pair of documents has been authored by the same individual. Like in previous editions, we continued to work with (English-language) fanfiction, written by non-professional authors. As a novelty, we substantially increased the size of the provided dataset to enable more datahungry approaches. In total, thirteen systems (from ten participating teams) have been submitted, which are substantially more diverse than the submissions from previous years. We provide a detailed comparison of these approaches and two generic baselines. Our findings suggest that the increased scale of the training data boosts the state of the art in the field, but we also confirm the conventional issue that the field struggles with an overreliance on topic-related information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>From the very beginning, authorship analysis tasks have played a key role within the PAN series. A variety of shared tasks have been developed over the past decade, complemented by the much-needed development of benchmark corpora for problems such as authorship attribution, authorship clustering, and authorship verification -both within and across genres, and within and across languages. Rather than adding new task variants (or repeating existing ones), we decided this year to renew our mission and broaden our perspective, by organizing an annual series of tasks of a gradually increasing difficulty and realism, organized within a three-year strategy <ref type="bibr" coords="1,365.36,588.29,21.79,8.64">(2020)</ref><ref type="bibr" coords="1,387.15,588.29,4.36,8.64">(2021)</ref><ref type="bibr" coords="1,387.15,588.29,4.36,8.64">(2022)</ref><ref type="bibr" coords="1,391.51,588.29,21.79,8.64">(2023)</ref>. In this endeavour, we also aim to integrate as many of the lessons learned from recent editions as possible. Amongst others, we aim to devote explicit care to some of the larger challenges that remain open in the field, such as author-topic orthogonality, cross-genre issues and problems involving texts of unequal lengths. Additionally, based on the corpus construction efforts of Bischoff et al. <ref type="bibr" coords="2,300.16,131.27,10.58,8.64" target="#b5">[6]</ref>, we can provide evaluation data of a more substantial size than in previous years, so as to keep the field up to speed with broader developments in NLP, and especially the emergence of data-hungry methods from representation learning. This endeavour puts demanding constraints on the practical organization of the annual task, but it is our hope that this renewed strategy will be beneficial to participants and the whole field in general.</p><p>The task in year one, which is this year, was deliberately formulated as a closed setting: The test set only contains a subset of authors and topics that were also present in the training set. While presented as an authorship verification task, it could therefore also be seen as an authorship attribution task. The task in year two will move beyond this and is designed in a much more demanding open setting. While the training set in year two will be identical to the one from this year, the test set will no longer contain any authors or topics that were also present in the training set. This setup entails a highly challenging task in (pure) authorship verification. The task in year three (currently termed as "surprise task") is intended to put the participants in the role of judges at court, where the highest possible reliability and dependability will be required. These requirements will be reflected in the evaluation measures. More details on this task will be released in due time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Authorship verification</head><p>The automated authentication of textual documents is an important issue in computer science, with multiple, real-world applications across various domains. Trustworthy, transparent benchmark initiatives are therefore key in reliably establishing the state of the art in the field, stimulating replication, and monitoring progress <ref type="bibr" coords="2,402.26,424.40,15.27,8.64" target="#b26">[27]</ref>. Automatically assessing a document's authorship on the basis of its linguistic and stylistic features is a crucial aspect of this problem <ref type="bibr" coords="2,266.26,448.31,15.77,8.64" target="#b29">[30,</ref><ref type="bibr" coords="2,285.22,448.31,12.45,8.64" target="#b16">17,</ref><ref type="bibr" coords="2,300.87,448.31,11.83,8.64" target="#b21">22]</ref>, which can be modeled in various ways. Over the years, PAN has contributed to the field of authorship identification through organizing an array of shared tasks that were recently surveyed <ref type="bibr" coords="2,393.92,472.22,10.58,8.64" target="#b2">[3]</ref>. Some of the core tasks included:</p><p>-AUTHORSHIP ATTRIBUTION: given a document and a set of candidate authors, determine which of them wrote the document (2011-2012, 2016-2020); -AUTHORSHIP VERIFICATION: given a pair of documents, determine whether they are written by the same author (2013-2015); -AUTHORSHIP OBFUSCATION: given a document and a set of documents from the same author, paraphrase the former so that its author cannot be identified anymore (2016-2018); -OBFUSCATION EVALUATION: devise and implement performance measures that quantify safeness, soundness, and/or sensibleness of an obfuscation software (2016-2018).</p><p>As previously announced <ref type="bibr" coords="2,243.82,632.53,10.58,8.64" target="#b2">[3]</ref>, we have revisited the task of authorship verification this year, which can be formalized as the task of approximating the target function A novelty this year is that, contrary to recent editions, we sought to substantially increase the size of the evaluation dataset. This goal was inspired by the observation that previous competitions attracted relatively few approaches that exploited recent advances from representation learning on the basis of neural networks (such as sentencelevel embeddings). By supplying large evaluation data, we hoped to broaden up the array of submitted approaches (which seems to have been successful). As every year, we asked participants to deploy the verifiers on TIRA for reproducibility sake and for blind evaluation on an unseen test set <ref type="bibr" coords="3,289.34,346.46,15.27,8.64" target="#b27">[28]</ref>. They were expected to produce a score in the form of a bounded scalar between 0 and 1, indicating the probability of the test item being a same-author (SA) pair (rather than a binary choice). The critical threshold for expressing a positive answer was 0.5 (see Section 4).</p><formula xml:id="formula_0" coords="2,134.77,656.12,107.98,9.65">φ : (D k , d u ) → {T, F },</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>In this edition, we have continued working with 'fanfiction' <ref type="bibr" coords="3,373.67,438.12,15.77,8.64" target="#b19">[20,</ref><ref type="bibr" coords="3,391.79,438.12,11.83,8.64" target="#b17">18]</ref>, i.e., fictional texts produced by non-professional authors in the tradition of a specific cultural domain (or 'fandom'), such as a famous author or a specific influential work <ref type="bibr" coords="3,388.20,462.03,15.27,8.64" target="#b13">[14]</ref>. We operationalize cross-domain verification as cross-fandom verification, where a fandom can be roughly interpreted as a mixture of topic and literary genre. Fanfiction is abundantly available on the internet, as the fastest growing form of online writing <ref type="bibr" coords="3,363.82,497.89,15.27,8.64" target="#b9">[10]</ref>, and clearly fits our aims to scale up the datasets in the coming years. This year, two training datasets of different magnitudes ("small" and "large") are provided with text pairs crawled from fanfiction. net, a sharing platform for fanfiction that comes with rich, user-contributed metadata. For the construction of these datasets, we built on the fanfiction.net corpus compiled by Bischoff et al. <ref type="bibr" coords="3,205.64,557.67,11.62,8.64" target="#b5">[6]</ref> as a basis. Participants were allowed to submit systems calibrated on either dataset (or both). This way, we hoped to be able to establish the added value of increasing training resources in authorship verification. Only English-language texts were included to push the size of the data to the maximum: a multilingual counterpart of this dataset, including resource-scarcer languages, remains a desideratum.</p><p>All texts are normalized with regards to punctuation and white space to avoid textual artifacts <ref type="bibr" coords="3,186.31,629.40,11.62,8.64" target="#b3">[4]</ref> and have a length of approximately 21,000 characters. To construct the dataset, we bucketed the texts by author and fandom (topic) to ensure a good mix of the two and, despite the very uneven popularity of fandoms and activity of authors, pre-vent gross overrepresentation of individual fandoms and authors. For the large dataset, 148,000 same-author (SA) and 128,000 different-authors (DA) pairs were drawn from the fanfiction.net crawl. The SA pairs encompass 41,000 authors of which at least four and not more than 400 have written in the same fandom (median: 29). In total, 1,600 fandoms were selected and each single author has written in at least two, but not more than six fandoms (median: 2).</p><p>The pairs were assembled by building all possible n 2 pairings of author texts (n being the actual number of texts from this author) without allowing two pairs with the same author and fandom. If the source texts were longer than 21,000 characters, different random portions were used in each pair. The DA pairs were built from texts of 250,000 authors of which at least two and not more than 800 (twice the number, since each pair consists of two authors now) have written in the same fandom (median: 51). The number of fandoms is the same and largely overlaps with the SA pairs. Each author has written texts in at least one and not more than three fandoms (median: 1).</p><p>The small training set is a subset of the large training set with 28,000 SA and 25,000 DA pairs from the same 1,600 fandoms, but with a reduced author number of 6,400 (4-68 per fandom, median: 7) and 48,500 (2-63 per fandom, median: 38), respectively.</p><p>The test dataset contains 10,000 SA and 6,900 DA pairs from 400 fandoms and 3,500 and 12,000 authors, respectively, which are guaranteed to exist in the training sets, either in a different author-fandom relation or in the same author-fandom relation, but always with a previously unseen text. This creates a closed-set authorship identification scenario, which will be broken in the next year with unseen fandoms and authors. The number of authors per fandom ranges from 2-400 in the SA pairs (median: 14) and 4-800 in the DA pairs (median: 20). The number of fandoms per author are 2-6 (median: 2) and 1-6 (median: 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Framework</head><p>Performance Measures Because of the considerable size of the datasets, we opted for a combination of four performance measures that each focus on different aspects. For each problem (i.e., individual text pair) in the test set, the participating systems submitted a scalar in the [0,1] range, indicating the probability of this being a sameauthor (SA) pair. For difficult cases, the systems could submit a score of exactly 0.5, which was equivalent to a non-response <ref type="bibr" coords="4,302.94,516.68,15.27,8.64" target="#b24">[25]</ref>. The following measures were used to score the submissions:</p><p>-AUC: the conventional area-under-the-curve score, in a reference implementation <ref type="bibr" coords="4,169.69,557.31,15.27,8.64" target="#b25">[26]</ref>, -c@1: a variant of the conventional F1 measure, which rewards systems that leave difficult problems unanswered <ref type="bibr" coords="4,275.14,580.58,15.27,8.64" target="#b24">[25]</ref>, -F1: the well-known performance measure (not taking into account non-answers), in a reference implementation <ref type="bibr" coords="4,273.43,603.85,15.27,8.64" target="#b25">[26]</ref>, -F0.5u: a newly proposed measure that puts more emphasis on deciding same-author cases correctly <ref type="bibr" coords="4,213.11,627.12,10.58,8.64" target="#b4">[5]</ref>.</p><p>The overall score (used to produce the final ranking) is the mean of the scores of all the evaluation measures.</p><p>Baselines We applied two baseline systems (calibrated on the small training set only):</p><p>1. The first method calculates the cosine similarities between TFIDF-normalized, character tetragram representations of the texts in a pair. The resulting scores are shifted using a grid search on the calibration data ("naive" baseline). This is a socalled first-order verifier <ref type="bibr" coords="5,251.43,175.10,15.27,8.64" target="#b18">[19]</ref>. 2. Secondly, we applied a text compression method that, given a pair of texts (t 1 and t 2 ), calculates the cross-entropy of t 2 using the prediction by partial matching model <ref type="bibr" coords="5,195.13,210.97,16.60,8.64" target="#b11">[12]</ref> of t 1 and vice-versa. The mean and absolute difference of the two cross-entropies are used by a logistic regression model to estimate a score in [0, 1] (called "compression" baseline below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Survey of Submissions</head><p>The authorship verification task received 13 submissions from 10 participating teams. Below we present a brief overview of the submitted approaches: <ref type="foot" coords="5,390.44,300.96,3.49,6.05" target="#foot_0">1</ref>Boenninghoff et al. <ref type="bibr" coords="5,228.88,314.58,11.62,8.64" target="#b6">[7]</ref> (boenninghoff20) proposed an approach that combines neural feature extraction with statistical modeling: a deep metric learning framework with a Siamese network topology was used to measure the similarity between two documents; the produced features were fed into a probabilistic linear discriminant analysis layer that served as a pairwise discriminator to perform Bayes factor scoring in the learned metric space. To take into account topic influences, the authors applied several preprocessing steps, which included (1) replacing all rare tokens/character types by a placeholder, (2) a sliding window to perform tokenization without sentence boundary detection, (3) adding a contextual prefix (fandom label), and (4) dissembling all predefined document pairs and re-sampling new SA and DA pairs in each epoch to increase the heterogeneity of the training data.</p><p>Weerasinghe and Greenstadt <ref type="bibr" coords="5,269.78,446.09,16.60,8.64" target="#b31">[32]</ref> (weerasinghe20) extracted stylometric features (including function word frequency, vocabulary richness, character and part-of-speech (POS) tag n-grams, POS tag chunks, and noun/verb phrases) and used the absolute difference between the feature vectors as input to a logistic regression model (small dataset) and a neural network-based model with one hidden layer (large dataset). The applied preprocessing steps consist of tokenization, POS tagging, and generating parse trees; the model optimization was done based on the AUC measure.</p><p>Halvani et al. <ref type="bibr" coords="5,209.50,529.77,16.60,8.64" target="#b12">[13]</ref> (halvani20) used a list of around 1,000 topic-agnostic words and phrases grouped into certain feature categories (e.g., n-grams, sentence starters and endings). Based on such categories, all possible ensembles of feature categories and corresponding thresholds (which were calculated based on the equal error rate of the computed distances for each feature category) were evaluated and the optimal ensemble was selected. The classification was done using the Manhattan metric.</p><p>Kipnis <ref type="bibr" coords="6,179.52,119.31,16.60,8.64" target="#b20">[21]</ref> (kipnis20) addressed the task using an unsupervised approach, which takes word-by-word p-values calculated based on a binomial allocation model of words between the two documents, and combines them into a single score using the higher criticism statistic <ref type="bibr" coords="6,204.72,155.18,10.58,8.64" target="#b8">[9]</ref>. The produced score was converted into a similarity score by evaluating the empirical distribution of the higher criticism associated with document pairs. Araujo-Pino et al. <ref type="bibr" coords="6,224.65,179.09,11.62,8.64" target="#b0">[1]</ref> (araujo20) used a Siamese neural network approach <ref type="bibr" coords="6,451.10,179.09,11.62,8.64" target="#b7">[8]</ref> that receives as input the character n-gram representation (with n varying from 1 to 3) of the document pairs to be compared. The authors experimented with different hyperparameters and trained the model both on the large and the small dataset, using the AUC score as a reference for fine-tuning. The model trained on the small dataset outperformed the model trained on the large one only by a small margin of 0.015 AUC.</p><p>Ga ¸gała <ref type="bibr" coords="6,181.41,250.82,16.60,8.64" target="#b10">[11]</ref> (gagala20) used a data compression method based on the prediction by partial matching model <ref type="bibr" coords="6,245.55,262.77,15.27,8.64" target="#b11">[12]</ref>, and compression-based cosine as similarity measure between text samples <ref type="bibr" coords="6,226.85,274.73,15.27,8.64" target="#b28">[29]</ref>. The method was extended with a context-free grammar character pre-processing: replacing the most frequent character n-grams (n = 2) by a special symbol to reduce the length of the texts and to simplify the distribution of the characters. The authors calibrated their method on a subset of the small dataset, reporting that additional data did not improve the performance of the method.</p><p>Ordoñez et al. <ref type="bibr" coords="6,209.75,334.51,16.60,8.64" target="#b23">[24]</ref> (ordonez20) used a long-sequence transformer, a recent Longformer model <ref type="bibr" coords="6,193.75,346.46,10.58,8.64" target="#b1">[2]</ref>, to process the long fanfiction documents with additional layers to learn both text and fandom-specific features. The fandom information (provided as metadata) was incorporated by the use of a multi-task loss function that optimizes for both authorship verification and topic correspondence classification losses. The authors additionally compared the performance of the approach with a character-based convolutional neural network ((CN) 2 ) with self-attention layers, reporting that the Longformer system outperformed (CN) 2 , and both their baselines by a wide margin, achieving a very high overall score of 0.963 on a held-out subset of the large training corpus. On the official test set, however, only a lower overall score of 0.685 was achieved.</p><p>Ikae <ref type="bibr" coords="6,170.80,454.06,16.60,8.64" target="#b15">[16]</ref> (ikae20) based their approach on text similarity: two documents were considered as an SA pair if the Labbé similarity value <ref type="bibr" coords="6,357.25,466.01,16.60,8.64" target="#b22">[23]</ref> between them exceeded a threshold of 0.5. The 500 most frequent words and punctuation marks were used for a document representation according to their relative frequency.</p><p>Overall, while the majority of the participants addressed the task as a binary classification problem, they did so with a large variety approaches using both deep learning and machine learning techniques, as well as unsupervised text similarity-based methods. This variability in the submitted methods can be attributed to the increased size of the dataset developed for the current edition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation Results</head><p>The evaluation results can be found in Table <ref type="table" coords="6,314.08,611.47,3.74,8.64">1</ref>, where we distinguish between multiple submissions for the same team using a suffix ('-small' or '-large'). A pairwise significance analysis of the F1-scores according to the approximate randomization test <ref type="bibr" coords="6,455.00,635.38,16.60,8.64" target="#b30">[31]</ref> is shown in Table <ref type="table" coords="6,197.64,647.33,3.74,8.64" target="#tab_0">2</ref>.</p><p>Table <ref type="table" coords="7,157.94,107.76,3.36,8.06">1</ref>. Evaluation results for the shared task on authorship verification at PAN 2020 in terms of area under the curve (AUC) of the receiver operating characteristic (ROC), c@1, F0.5u, F1, and an overall score (which determines the order). "Large" ("small") indicates that the large (small) training dataset was used. boenninghoff20-large weerasinghe20-large boenninghoff20-small weerasinghe20-small halvani20-small kipnis20-small araujo20-small niven20-small gagala20-small araujo20-large A number of worthwhile observations can be made. First of all, the top-performing method (boenninghoff20-large) reaches an impressive overall score of 0.935 that significantly outperforms the runner-up, weerasinghe20, which already has a very high overall score of 0.902. The difference between both approaches might correspond to their respective treatment of non-answers, since the difference in performance is the most pronounced for the c@1 and the F1 measures (the latter explicitly ignored the non-answers, which seems to have given boenninghoff20 a competitive edge). Interestingly, both Boenninghoff et al.'s and Weerasinghe and Greenstadt's systems were calibrated on the large dataset and outperform their counterpart trained on the small dataset. While in itself this seems a clear indication that verification systems can benefit from large training datasets, surprisingly enough, this is not in line with the result for araujo20 (where the 'small' system outperformed the 'large' one). Most systems, except three (ordonez20, ikae20 and faber20), outperformed the naive and compression baselines (which furthermore did not produce significantly different results from one another). Interestingly, the cohort following the top-2 performing systems in the final ranking (halvani20-small, kipnis20-small, araujo20-small) achieved overall scores in a highly similar ballpark (in the 0.80s), but often produced only mildly significantly different predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submission</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>In this section, we provide a more in-depth analysis of the submitted approaches and their evaluation results. First, we take a look into the distribution of the submitted verification scores, including that of a meta classifier. We go on to inspect the effect of non-responses, and finally, study how the topic similarity between the text in a test pair might have affected the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distributions</head><p>In Figure <ref type="figure" coords="8,239.45,456.05,4.98,8.64">1</ref> (left), we plot the precision-recall curves for all "small" submissions, including that of a meta classifier that predicts the mean score over all submissions (dotted line). The figure highlights the relatively large head start of boen-inghoff20 and weerasinghe20 regarding AUC in comparison to the runner-up cohort. The latter achieves a higher precision, where the former capitalizes on a solid recall.</p><p>In Figure <ref type="figure" coords="8,190.25,515.83,4.98,8.64">1</ref> (right), the distribution of the scores submitted by the teams are visualized by kernel-density estimates for the individual "small" submissions, as well as the overall distribution (dotted line). The latter clearly shows the effect of so-called "number heaping," with modes at predictable thresholds (0, 0.25, 0.5, 0.75 and 1). Furthermore, the modes clearly suggest that the most successful systems have tended towards self-confident scores, close to 0 and 1. Systems with modes in more hesitant areas (e.g., 0.25-0.50) seem to have under-performed in this respect. This might especially have impacted the AUC scores, because this measure favors bold predictions, provided they are correct. weerasinghe20-small araujo20-small boenninghoff20-small halvani20-small ikae20-small faber20-small gagala20-small kipnis20-small niven20-small Figure <ref type="figure" coords="9,161.50,261.48,3.36,8.06">1</ref>. Left: Precision-recall curves for all "small" submissions (excluding faber20-small for the sake of readability), as well as a meta classifier that is based on the participants' mean score. Right: Kernel-density estimate plots for the distributions of the submitted verification scores (across all and per "small" submission). Non-answers As mentioned above, participants were allowed to leave difficult problems unanswered by giving them a score of exactly 0.5. Such non-responses explicitly affected the c@1 score and were excluded in calculating the F1 score. In Figure <ref type="figure" coords="9,473.11,538.23,3.74,8.64" target="#fig_3">2</ref>, we show the correlation between the absolute number of non-answers submitted and the c@1 score. The results show that only three teams made active use of this option: In particular, Boenninghoff et al. and Kipnis submitted a considerable number of nonanswers (1,082 and 839, respectively), without compromising their overall score. An interesting follow-up question is then, whether Boenninghoff et al.'s top rank is solely due to this non-response strategy. The results in Table <ref type="table" coords="9,331.73,609.96,4.98,8.64" target="#tab_1">3</ref> suggest otherwise: Here, we recomputed the evaluation measures, excluding those pairs for which boenninghoff20-large submitted a non-response. The previous ranking is corroborated (and even reinforced), assuring us that the difference in performance is not solely due to non-responses.  The influence of topic We have trained a reference topic model on the small training dataset, in order to be able to trace the influence of topic on the submitted verifiers.</p><p>Our approach was as follows: The entire corpus was tokenized and POS-tagged using Spacy's standard model for English <ref type="bibr" coords="10,283.61,462.18,15.27,8.64" target="#b14">[15]</ref>, retaining only nouns, adjectives and verbs. Next, a TF-IDF-normalized bag-of-words representation of the corpus was constructed, considering the 5,000 tokens with the highest cumulative corpus frequency, ignoring words that appeared in more than half of the documents or words with an absolute document frequency of less than 100. Next, we fitted a non-negative matrix factorization (NMF) model of 150 dimensions on this data (during 50 iterations). All modeling was done in scikit-learn <ref type="bibr" coords="10,215.29,533.91,15.27,8.64" target="#b25">[26]</ref>. Figure <ref type="figure" coords="10,266.12,533.91,4.98,8.64" target="#fig_4">3</ref> shows word clouds for a cherry-picked subset from the (generally clearly interpretable) topics that were obtained from the model. 2 Finally, this pipeline was applied to the text pairs in the test set and we recorded the cosine similarity between the L1-normalized topic representations for each text. This scalar was used as a proxy for the topic resemblance between two texts. In Figure <ref type="figure" coords="10,188.39,593.68,3.74,8.64">4</ref>, the average verification score of the submitted "small" systems for each pair in the test set is plotted as a function of the topic similarity observed for that pair. The fit of a linear model to this data suggests that there is a considerable correlation (β = 0.28, R 2 = 0.16) between the topic similarity and the likelihood that a system 2 Based on Andreas Mueller's word_cloud package: https://github.com/amueller/word_cloud.  categorizes the pair as an SA instance. This is not necessarily a bad thing, since, overall, SA pairs show a greater topic similarity than the DA pairs, as shown in the violin plot in Figure <ref type="figure" coords="11,173.51,487.14,3.74,8.64" target="#fig_5">5</ref>. Intuitively, this shows that authors tend to write about the same topics.</p><p>However, if we split out the data over correct and incorrect cases, the picture changes: In cases where the meta-classifier proposed a correct solution, the numbers are largely unaltered (β = 0.28, R 2 = 0.17); for the incorrect decisions, however, the explanatory value of the linear model plummets (β = 0.19, R 2 = 0.03). This suggests that for the incorrect cases, the models were generally susceptible to a misleading influence of topic similarity. This hypothesis is supported by the boxplots in Figure <ref type="figure" coords="11,473.11,558.87,3.74,8.64" target="#fig_5">5</ref>. Interestingly, DA pairs which were correctly solved have a lower topical similarity than those that were incorrectly solved (for the SA cases, the relationship is inversed). A nuanced, yet plausible picture emerges from these results: Systems can (and even should) partially rely on topical information when performing large-scale authorship verification, but they should not exaggerate this reliance, as it can be misleading. This year's track on authorship identification at PAN focused on authorship verification. The 2020 edition fits an ambitious, renewed three-year strategy that aims to increase the available benchmark resources in both scope and realism. The initiative continues to attract a healthy number of high-quality submissions (this year, we received 13 submissions by 10 teams). The evaluation measures used here attest to the interesting diversity of the submitted systems, including, for the first time, a number of data-hungry approaches based on deep learning, that, surprisingly enough, remained relatively absent from this community for quite some years. The top-performing methods could boast an impressive performance-we recorded an overall score of 0.935 for the highest-ranking submission by Boenninghoff et al. <ref type="bibr" coords="12,274.93,250.82,14.44,8.64" target="#b6">[7]</ref>-, showing that the field might be closing in on a solution for authorship verification in a closed setup. Our analyses, however, also suggested that the dependence on topical information is still problematic, and that it is one of the main causes driving misclassifications. This observation considerably ups the stakes for next year's edition, that will present a much more challenging task, involving an open setup, with a test set of unseen authors, as well as unseen domains. In this regard, the recent advances in adversarial training for authorship attribution <ref type="bibr" coords="12,468.97,322.55,11.62,8.64" target="#b5">[6]</ref> may prove to be a promising direction for future research in verification as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,246.75,656.12,233.84,9.65;3,134.77,118.99,345.82,9.65;3,134.77,130.95,345.83,9.65;3,134.77,142.90,345.83,9.65;3,134.77,154.86,345.82,9.65;3,134.77,167.13,345.82,8.64;3,134.77,179.09,345.82,8.64;3,134.77,190.72,345.83,9.65;3,134.77,203.00,345.82,8.64;3,134.77,214.95,345.82,8.64;3,134.77,226.91,345.82,8.64;3,134.77,238.55,345.83,9.65;3,134.77,250.50,107.68,9.65"><head></head><label></label><figDesc>where D k is a set of k documents of known authorship by the same author and d u is a document of unknown or questioned authorship. If φ(D k , d u ) = T , then the author of D k is also the author of d u and if φ(D k , d u ) = F , then the author of D k is not the same with the author of d u . In the case of crossdomain verification, D k and d u stem from a different text variety or treat a considerably different content (e.g. topics or themes, genres, registers, etc.). For the present task, we considered the simplest (and most challenging) formulation of the verification task, i.e., we only considered cases where k = 1, rendering D k a singleton; thus, only pairs of documents are examined. Given a training set of such problems, i.e., text pairs, the verification systems of the participating teams had to be trained and calibrated to analyze the authorship of the unseen text pairs (from the test set). We shall distinguish between same-author text pairs (SA: φ(D k , d u ) = T ) and different-author (DA: φ(D k , d u ) = F ) text pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,398.45,475.09,7.73,29.89;7,398.45,447.45,7.73,25.39;7,416.32,475.59,7.77,29.38;7,416.16,422.38,7.73,50.97;7,434.10,446.38,7.77,58.60;7,452.86,455.26,25.61,49.71;7,140.39,513.48,77.80,7.77;7,234.98,513.48,245.28,7.77;7,143.66,523.34,74.53,7.77;7,252.81,523.34,227.45,7.77;7,138.72,533.21,79.47,7.77;7,270.64,533.21,209.62,7.77;7,141.99,543.07,76.20,7.77;7,288.48,543.07,191.78,7.77;7,160.64,552.93,57.56,7.77;7,310.51,552.93,169.75,7.77;7,164.89,562.80,53.31,7.77;7,326.39,562.80,153.87,7.77;7,164.40,572.66,53.79,7.77;7,341.97,572.66,138.29,7.77;7,167.24,582.52,50.95,7.77;7,359.81,582.52,120.45,7.77;7,163.50,592.39,54.69,7.77;7,381.84,592.39,98.42,7.77;7,166.07,602.25,52.12,7.77;7,395.47,602.25,84.79,7.77;7,160.67,611.95,57.52,7.73;7,417.50,612.11,62.76,7.77;7,135.10,621.81,83.09,7.73;7,435.34,621.97,44.92,7.77;7,159.59,631.84,58.60,7.77;7,448.98,631.84,31.28,7.77;7,171.87,641.70,46.32,7.77;7,466.81,641.70,13.45,7.77"><head></head><label></label><figDesc>*** *** *** *** *** *** *** *** *** *** *** *** *** weerasinghe20-large *** *** *** *** *** *** *** *** *** *** *** *** *** boenninghoff20-small *** *** *** *** *** *** *** *** *** *** *** *** weerasinghe20-small *** *** *** *** *** *** *** *** *** *** *** halvani20-small = = *** = = *** *** *** *** *** kipnis20-small ** *** = = *** *** *** *** *** araujo20-small *** ** *** *** *** *** *** *** niven20-small *** *** *** *** *** *** *** gagala20-small = *** *** *** *** *** araujo20-large *** *** *** *** *** baseline (naive)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,139.78,486.77,335.81,8.12"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. The c@1 score per "small" submission as a function of the number of non-answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,134.77,387.81,345.83,8.12;10,134.77,399.11,59.02,7.77;10,141.64,323.11,108.00,54.00"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Word cloud visualizations of 9 cherry-picked dimensions from the NMF model (60 terms per topic).</figDesc><graphic coords="10,141.64,323.11,108.00,54.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="11,134.77,424.73,345.83,8.12;11,134.77,436.03,147.03,7.77"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The distribution of topical similarity, separated over SA and DA pairs and whether the meta-classifier correctly solved the pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.77,164.19,345.83,244.24"><head>Table 2 .</head><label>2</label><figDesc>Significance</figDesc><table coords="7,135.92,164.19,343.52,185.96"><row><cell></cell><cell>AUC</cell><cell>c@1</cell><cell>F0.5u</cell><cell>F1</cell><cell>Overall</cell></row><row><cell>boenninghoff20-large</cell><cell>0.969</cell><cell>0.928</cell><cell>0.907</cell><cell>0.936</cell><cell>0.935</cell></row><row><cell>weerasinghe20-large</cell><cell>0.953</cell><cell>0.880</cell><cell>0.882</cell><cell>0.891</cell><cell>0.902</cell></row><row><cell>boenninghoff20-small</cell><cell>0.940</cell><cell>0.889</cell><cell>0.853</cell><cell>0.906</cell><cell>0.897</cell></row><row><cell>weerasinghe20-small</cell><cell>0.939</cell><cell>0.833</cell><cell>0.817</cell><cell>0.860</cell><cell>0.862</cell></row><row><cell>halvani20-small</cell><cell>0.878</cell><cell>0.796</cell><cell>0.819</cell><cell>0.807</cell><cell>0.825</cell></row><row><cell>kipnis20-small</cell><cell>0.866</cell><cell>0.801</cell><cell>0.815</cell><cell>0.809</cell><cell>0.823</cell></row><row><cell>araujo20-small</cell><cell>0.874</cell><cell>0.770</cell><cell>0.762</cell><cell>0.811</cell><cell>0.804</cell></row><row><cell>niven20-small</cell><cell>0.795</cell><cell>0.786</cell><cell>0.842</cell><cell>0.778</cell><cell>0.800</cell></row><row><cell>gagala20-small</cell><cell>0.786</cell><cell></cell><cell>0.809</cell><cell>0.800</cell><cell>0.796</cell></row><row><cell>araujo20-large</cell><cell>0.859</cell><cell>0.751</cell><cell>0.745</cell><cell>0.800</cell><cell>0.789</cell></row><row><cell>baseline (naive)</cell><cell>0.780</cell><cell>0.723</cell><cell>0.716</cell><cell>0.767</cell><cell>0.747</cell></row><row><cell>baseline (compression)</cell><cell>0.778</cell><cell>0.719</cell><cell>0.703</cell><cell>0.770</cell><cell>0.742</cell></row><row><cell>ordonez20-large</cell><cell>0.696</cell><cell>0.640</cell><cell>0.655</cell><cell>0.748</cell><cell>0.685</cell></row><row><cell>ikae20-small</cell><cell>0.840</cell><cell>0.544</cell><cell>0.704</cell><cell>0.598</cell><cell>0.672</cell></row><row><cell>faber20-small</cell><cell>0.293</cell><cell>0.331</cell><cell>0.314</cell><cell>0.262</cell><cell>0.300</cell></row></table><note coords="7,214.07,378.74,266.52,7.77;7,134.77,389.41,345.83,8.06;7,134.77,400.37,251.05,8.06"><p>of pairwise differences in F1 between submissions. Notation: '=' (not significantly different: p &gt; 0.5), '*' (significantly different: p &lt; 0.05), '**' (very significantly different: p &lt; 0.01), '***' (highly significantly different: p &lt; 0.001).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,134.77,107.76,345.83,69.04"><head>Table 3 .</head><label>3</label><figDesc>Evaluation results for two top-performing systems, excluding the pairs for which boen-ninghoff20-large submitted a non-response.</figDesc><table coords="10,136.93,142.28,341.49,34.52"><row><cell>Submission w/o non-responses</cell><cell>AUC</cell><cell>c@1</cell><cell>F1</cell><cell>F0.5u</cell><cell>Overall</cell></row><row><cell>boenninghoff20-large</cell><cell>0.974</cell><cell>0.930</cell><cell>0.936</cell><cell>0.934</cell><cell>0.943</cell></row><row><cell>weerasinghe20-large</cell><cell>0.957</cell><cell>0.886</cell><cell>0.897</cell><cell>0.888</cell><cell>0.907</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,134.77,117.35,345.82,292.64"><head></head><label></label><figDesc>Topical resemblance between a text pair as a function of the mean prediction by all "small" systems (β = 0.28, R 2 = 0.16).</figDesc><table coords="11,134.77,117.35,320.84,292.64"><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell></row><row><cell>Topic Similarity</cell><cell>0.4 0.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Mean Prediction</cell></row><row><cell cols="3">Different Authors Figure 4. Same Author 0.4 0.0 Topic Similarity 0.2 0.6 0.8 1.0</cell><cell></cell><cell>Different Authors Meta Classifier Correct Meta Classifier Incorrect</cell><cell>Same Author</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,138.25,609.18,342.34,7.77;5,138.25,620.14,342.34,7.77;5,138.25,631.10,342.34,7.77;5,138.25,642.06,342.34,7.77;5,138.25,653.02,339.14,7.77"><p>All but two of the submissions (Niven and Kao (niven20) and Faber and van der Ree (faber20)) are described in the participants' notebook papers. Three of the teams (boenninghoff20, araujo20 and weerasinghe20) submitted two systems, calibrated on the small and the large training datasets, respectively; team ordonez20 only used the large dataset; all others used the small one (or even smaller portions of it, as indicated in private correspondence with the organizers).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank all participants for their much-appreciated contribution to the shared task and we wish to encourage them to stay involved in the community in the next years. We thank the CLEF organizers for their work in organizing the conference, especially in these trying times. Our special thanks go to <rs type="person">Sebastian Bischoff</rs>, <rs type="person">Niklas Deckers</rs>, <rs type="person">Marcel Schliebs</rs>, and <rs type="person">Ben Thies</rs> for sharing the fanfiction.net corpus.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="12,154.69,487.58,307.46,7.77;12,154.68,498.54,299.97,7.77;12,154.68,509.50,284.06,7.77;12,154.68,520.46,80.53,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,362.87,487.58,99.28,7.77;12,154.68,498.54,79.74,7.77">Siamese network applied to authorship verification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Araujo-Pino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gómez-Adorno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Fuentes-Pineda</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="12,154.68,509.50,279.73,7.77">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,154.69,531.42,320.41,7.77" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="12,286.34,531.42,162.61,7.77">Longformer: The long-document transformer</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,154.69,542.38,321.78,7.77;12,154.68,553.34,316.34,7.77;12,154.68,564.29,322.47,7.77;12,154.68,575.25,313.68,7.77;12,154.68,586.21,318.29,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,154.68,564.29,169.80,7.77">Shared tasks on authorship analysis at pan 2020</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Specht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,342.04,575.25,122.86,7.77">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Castells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Martins</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="508" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,154.69,597.17,308.70,7.77;12,154.68,608.13,312.67,7.77;12,154.68,619.09,316.19,7.77;12,154.68,630.05,258.78,7.77;12,154.68,641.01,161.19,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,336.16,597.17,127.23,7.77;12,154.68,608.13,133.86,7.77">Bias Analysis and Mitigation in the Evaluation of Authorship Verification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-1634" />
	</analytic>
	<monogr>
		<title level="m" coord="12,154.68,619.09,298.88,7.77">57th Annual Meeting of the Association for Computational Linguistics (ACL 2019)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Traum</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">Jul 2019</date>
			<biblScope unit="page" from="6301" to="6306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,154.69,119.96,323.53,7.77;13,154.68,130.92,312.86,7.77;13,154.68,141.88,298.11,7.77;13,154.68,152.84,310.17,7.77;13,154.68,163.80,288.85,7.77;13,154.68,174.76,227.75,7.77;13,154.68,185.71,135.34,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,336.16,119.96,138.89,7.77">Generalizing unmasking for short texts</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1068</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1068" />
	</analytic>
	<monogr>
		<title level="m" coord="13,314.16,130.92,153.39,7.77;13,154.68,141.88,298.11,7.77;13,154.68,152.84,157.94,7.77">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="659" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="13,154.69,196.67,312.49,7.77;13,154.68,207.63,314.82,7.77;13,154.68,218.24,224.94,8.12" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="13,203.01,207.63,238.37,7.77">The importance of suppressing domain style in authorship analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bischoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Deckers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schliebs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno>CoRR abs/2005.14714</idno>
		<ptr target="https://arxiv.org/abs/2005.14714" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,154.69,229.55,304.59,7.77;13,154.68,240.51,299.97,7.77;13,154.68,251.47,284.06,7.77;13,154.68,262.43,80.53,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,353.30,229.55,105.97,7.77;13,154.68,240.51,79.74,7.77">Deep bayes factor scoring for authorship verification</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boenninghoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="13,154.68,251.47,279.73,7.77">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,154.69,273.39,317.65,7.77;13,154.68,284.34,312.12,7.77;13,154.68,295.30,318.83,7.77;13,154.68,306.26,145.69,7.77;13,154.68,317.22,271.44,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,367.13,273.39,105.21,7.77;13,154.68,284.34,132.18,7.77">Signature verification using a &quot;Siamese&quot; time delay neural network</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="13,154.68,295.30,189.29,7.77">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cowan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Alspector</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="737" to="744" />
			<date type="published" when="1994">1994</date>
			<publisher>Morgan-Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,154.69,328.18,309.04,7.77;13,154.68,338.79,153.41,8.12" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,226.66,328.18,217.14,7.77">Higher criticism for detecting sparse heterogeneous mixtures</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,449.78,328.18,13.94,7.77;13,154.68,339.14,69.74,7.77">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="962" to="994" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,154.69,350.10,307.19,7.77;13,154.68,361.06,128.89,7.77" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fathallah</surname></persName>
		</author>
		<title level="m" coord="13,202.86,350.10,255.45,7.77">Fanfiction and the Author. How FanFic Changes Popular Cultural Texts</title>
		<imprint>
			<publisher>Amsterdam University Press</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,154.69,372.02,314.95,7.77;13,154.68,382.97,317.31,7.77;13,154.68,393.93,300.56,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,196.96,372.02,272.67,7.77;13,154.68,382.97,29.85,7.77">Authorship verification with prediction by partial matching and context-free grammar</title>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Ga ¸gała</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="13,407.97,382.97,64.03,7.77;13,154.68,393.93,213.46,7.77">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,154.69,404.89,301.06,7.77;13,154.68,415.85,317.59,7.77;13,154.68,426.81,319.27,7.77;13,154.68,437.77,323.19,7.77;13,154.68,448.73,201.47,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,241.99,404.89,213.76,7.77;13,154.68,415.85,118.67,7.77">Cross-domain authorship attribution based on compression: Notebook for PAN at CLEF 2018</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Halvani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Graner</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2125/paper\_90.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="13,154.68,426.81,279.73,7.77">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="13,271.97,437.77,107.26,7.77">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Soulier</surname></persName>
		</editor>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
			<biblScope unit="volume">2125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,154.69,459.69,313.86,7.77;13,154.68,470.65,313.07,7.77;13,154.68,481.60,307.29,7.77;13,154.68,492.56,23.90,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,281.15,459.69,187.40,7.77;13,154.68,470.65,59.34,7.77">Cross-domain authorship verification based on topic agnostic features</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Halvani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Graner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Regev</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="13,436.60,470.65,31.16,7.77;13,154.68,481.60,246.33,7.77">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,154.69,503.52,321.58,7.77;13,154.68,514.48,23.90,7.77" xml:id="b13">
	<monogr>
		<title level="m" coord="13,270.48,503.52,111.44,7.77">The Fan Fiction Studies Reader</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Hellekson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Busse</surname></persName>
		</editor>
		<imprint>
			<publisher>University of Iowa Press</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,154.69,525.44,290.64,7.77;13,154.68,536.40,308.41,7.77" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="13,252.06,525.44,193.26,7.77;13,154.68,536.40,244.67,7.77">spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Montani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="13,154.69,547.36,322.82,7.77;13,154.68,558.32,315.78,7.77;13,154.68,569.28,149.66,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,187.55,547.36,170.57,7.77">UniNE at PAN-CLEF 2020: Author verification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ikae</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="13,255.53,558.32,214.93,7.77;13,154.68,569.28,62.55,7.77">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,154.69,579.89,313.67,8.12;13,154.68,591.19,57.53,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,189.06,580.23,78.73,7.77">Authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,273.31,580.23,175.63,7.77">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="334" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,154.69,602.15,313.34,7.77;13,154.68,613.11,319.94,7.77;13,154.68,624.07,313.95,7.77;13,154.68,635.03,251.98,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,154.68,613.11,260.37,7.77">Overview of the Cross-domain Authorship Attribution Task at PAN 2019</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="13,309.46,624.07,117.33,7.77">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="13,433.27,624.07,35.36,7.77;13,154.68,635.03,48.03,7.77">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019-09">Sep 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,154.69,645.99,323.53,7.77;13,154.68,656.60,278.69,8.12;14,154.68,119.96,176.59,7.77;14,154.68,130.92,154.18,7.77" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,411.72,645.99,66.49,7.77;13,154.68,656.95,84.22,7.77">Authenticating the writings of julius caesar</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Stover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Karsdorp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2016.06.029</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2016.06.029" />
	</analytic>
	<monogr>
		<title level="j" coord="13,244.59,656.95,122.29,7.77">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="86" to="96" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.69,141.74,311.50,7.77;14,154.68,152.70,299.67,7.77;14,154.68,163.66,318.49,7.77;14,154.68,174.62,324.11,7.77;14,154.68,185.58,79.44,7.77" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,203.01,152.70,251.34,7.77;14,154.68,163.66,173.13,7.77">Overview of the author identification task at PAN-2018: cross-domain authorship attribution and style change detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tschuggnall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Specht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,345.77,163.66,127.40,7.77;14,154.68,174.62,76.78,7.77">Working Notes Papers of the CLEF 2018 Evaluation Labs</title>
		<editor>et al.</editor>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018/Cappellato. 2018</date>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.69,196.40,317.78,7.77;14,154.68,207.35,318.55,7.77;14,154.68,218.31,207.43,7.77" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,196.53,196.40,216.86,7.77">Higher criticism as an unsupervised authorship discriminator</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kipnis</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="14,316.07,207.35,157.16,7.77;14,154.68,218.31,120.32,7.77">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.69,229.13,309.52,7.77;14,154.68,239.74,308.28,8.12;14,154.68,251.05,23.90,7.77" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="14,287.29,229.13,173.63,7.77">Computational methods in authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,154.68,240.09,264.20,7.77">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="26" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.69,261.87,301.07,7.77;14,154.68,272.48,238.72,8.12;14,154.68,283.79,320.26,7.77" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="14,234.61,261.87,221.15,7.77;14,154.68,272.83,26.80,7.77">Inter-textual distance and authorship attribution Corneille and Molière</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Labbé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Labbé</surname></persName>
		</author>
		<idno type="DOI">10.1076/jqul.8.3.213.4100</idno>
		<ptr target="https://doi.org/10.1076/jqul.8.3.213.4100" />
	</analytic>
	<monogr>
		<title level="j" coord="14,187.55,272.83,124.42,7.77">Journal of Quantitative Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="231" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.69,294.61,325.77,7.77;14,154.68,305.57,325.29,7.77;14,154.68,316.53,256.22,7.77" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,286.03,294.61,194.43,7.77">Will longformers PAN out for authorship verification?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ordoñez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="14,371.61,305.57,108.37,7.77;14,154.68,316.53,169.12,7.77">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.69,327.35,317.51,7.77;14,154.68,338.31,323.27,7.77;14,154.68,349.27,293.76,7.77;14,154.68,360.22,173.85,7.77" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="14,240.60,327.35,146.56,7.77">A simple measure to assess non-response</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,405.46,327.35,66.74,7.77;14,154.68,338.31,323.27,7.77;14,154.68,349.27,52.91,7.77;14,302.58,349.27,29.49,7.77">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
	<note>HLT &apos;11</note>
</biblStruct>

<biblStruct coords="14,154.69,371.04,316.00,7.77;14,154.68,382.00,315.24,7.77;14,154.68,392.96,322.31,7.77;14,154.68,403.57,192.98,8.12" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="14,299.67,392.96,144.73,7.77">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,450.59,392.96,26.40,7.77;14,154.68,403.92,110.80,7.77">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.69,414.74,306.58,7.77;14,154.68,425.70,288.55,7.77;14,154.68,436.66,319.81,7.77;14,154.68,447.62,304.09,7.77;14,154.68,458.58,304.36,7.77;14,154.68,469.54,308.77,7.77;14,154.68,480.49,304.84,7.77" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="14,154.68,447.62,304.09,7.77;14,154.68,458.58,73.08,7.77">Who wrote the web? revisiting influential author identification research applicable to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Buz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Duffhauss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gülzow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lötzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paßmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Reinke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rettenmeier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rometsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Träger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,323.67,469.54,122.86,7.77">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Hauff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Silvello</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="393" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.69,491.32,320.70,7.77;14,154.68,502.27,309.15,7.77;14,154.68,513.23,314.74,7.77;14,154.68,524.19,283.52,7.77;14,154.68,535.15,168.21,7.77" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="14,337.59,491.32,134.37,7.77;14,263.26,502.27,200.58,7.77;14,154.68,513.23,144.92,7.77">Information Retrieval Evaluation in a Changing World -Lessons Learned from 20 Years of CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-22948-1\_5" />
	</analytic>
	<monogr>
		<title level="s" coord="14,306.68,513.23,116.63,7.77">The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="123" to="160" />
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>TIRA integrated research architecture</note>
</biblStruct>

<biblStruct coords="14,154.69,545.97,306.03,7.77;14,154.68,556.93,320.59,7.77;14,154.68,567.89,225.31,7.77;14,154.68,578.85,293.36,7.77" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="14,251.88,545.97,208.85,7.77;14,154.68,556.93,73.79,7.77">Compression and machine learning: A new perspective on feature space vectors</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<idno type="DOI">10.1109/DCC.2006.13</idno>
		<ptr target="https://doi.org/10.1109/DCC.2006.13" />
	</analytic>
	<monogr>
		<title level="m" coord="14,246.42,556.93,177.32,7.77;14,456.84,556.93,18.43,7.77;14,154.68,567.89,10.65,7.77">Proceedings of the Data Compression Conference</title>
		<meeting>the Data Compression Conference<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">332</biblScope>
		</imprint>
	</monogr>
	<note>DCC &apos;06</note>
</biblStruct>

<biblStruct coords="14,154.69,589.32,296.38,8.12;14,154.68,600.63,324.98,7.77" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="14,210.97,589.67,182.01,7.77">A survey of modern authorship attribution methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.21001</idno>
		<ptr target="https://doi.org/10.1002/asi.21001" />
	</analytic>
	<monogr>
		<title level="j" coord="14,399.30,589.67,27.86,7.77">JASIST</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.69,611.45,318.76,7.77;14,154.68,622.41,136.46,7.77" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="14,210.13,611.45,251.02,7.77">Computer-Intensive Methods for Testing Hypotheses: An Introduction</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Noreen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>A Wiley-Interscience publication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.69,633.23,322.62,7.77;14,154.68,644.18,318.32,7.77;14,154.68,655.14,251.00,7.77" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="14,271.74,633.23,190.06,7.77">A machine learning model for authorship verification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weerasinghe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Greenstadt</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="14,359.40,644.18,113.60,7.77;14,154.68,655.14,163.89,7.77">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
