<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.95,115.96,321.46,12.62;1,178.08,133.89,259.19,12.62;1,243.99,151.82,127.39,12.62;1,371.80,645.84,101.37,7.86;1,144.73,656.80,71.67,7.86">TOBB ETU at CheckThat! 2020: Prioritizing English and Arabic Claims Based on Check-Worthiness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,220.35,189.49,85.27,8.74"><forename type="first">Yavuz</forename><forename type="middle">Selim</forename><surname>Kartal</surname></persName>
							<email>ykartal@etu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="institution">TOBB University of Economics and Technology</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.31,189.49,66.69,8.74"><forename type="first">Mucahid</forename><surname>Kutlu</surname></persName>
							<email>m.kutlu@etu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="institution">TOBB University of Economics and Technology</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.95,115.96,321.46,12.62;1,178.08,133.89,259.19,12.62;1,243.99,151.82,127.39,12.62;1,371.80,645.84,101.37,7.86;1,144.73,656.80,71.67,7.86">TOBB ETU at CheckThat! 2020: Prioritizing English and Arabic Claims Based on Check-Worthiness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">87D4C4B92FE919D0EF09F506D32BA230</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Misinformation has many negative consequences on our daily life. While spread of misinformation is very fast, investigating veracity of claims is slow. Therefore, we urgently need systems helping human fact checkers in the combat against misinformation. In this paper, we present our participation in check-worthiness tasks (i.e., Task 1 and Task 5) of CLEF-2020 Check That! Lab. For English Task 1, we use logistic regression with fined-tuned BERT predictions, POS tags, controversial topics and a hand-crafted word list as features. For English Task 5, we again use logistic regression with fined-tuned BERT predictions and word embeddings as features. For Arabic Task 1, we use a hybrid approach of fined-tuned BERT model with the model used for English Task 5. For the Arabic task, we use AraBert as our Bert model. In the official evaluation of primary submissions, our primary models a) ranked 3 rd in Arabic Task 1 based on P@30 and shared the 1 st rank with another group based on P@5, b) ranked 5 th in English Task 1 based on average precision and shared the 1 st rank with five other groups based on reciprocal rank, P@1, P@3 and P@5 metrics, and c) ranked 3 rd in Task 5 based on average precision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Social media platforms provide an incredibly easy way to share information with others. Any information, including misinformation, can reach millions of people in a very short time. Unfortunately, misinformation spread over Internet cause many unpleasant incidents such as huge changes in stock prices 1 . Since the start of on-going Covid-19 pandemic, we have also witnessed how misinformation can cause unhealthy, potentially deadly, practices such as gargling with bleach to prevent Covid-19 2 .</p><p>In order to combat misinformation, there are many fact-checking websites which manually investigate veracity of claims and share their findings with public. However, misinformation spread much faster than true information <ref type="bibr" coords="2,445.82,142.90,15.50,8.74" target="#b19">[20]</ref> and investigating veracity of claims are extremely time consuming, taking around one day for a single claim <ref type="bibr" coords="2,230.48,166.81,14.61,8.74" target="#b11">[12]</ref>. Considering the vast amount of claims spread on the Internet and high cost of fact-checking, we urgently need systems helping factcheckers to detect check-worthy claims, enabling them to focus on the important claims instead of spending their precious time for less important claims.</p><p>CLEF 2020 Check That! Lab <ref type="bibr" coords="2,286.50,215.16,10.52,8.74" target="#b4">[5]</ref> has organized two different shared-tasks (Task 1 and Task 5) for detecting check worthy claims. Task 1 has two different datasets, consisting of Arabic and English tweets while Task 5 has English political debates and transcribed speeches. In this paper, we present our methods developed for both Task 1 and Task 5.</p><p>In our study, we use two different ranking methodologies including logistic regression model and a hybrid combination of fined-tuned BERT model with logistic regression. We also investigate many different features including wordembeddings, presence of comparative and superlative adjectives, hand-crafted word list, domain-specific controversial topics, POS tags, metadata of tweets, and predictions of fined-tuned BERT models. Based on our experiments on training data, we use the following primary models for each task.</p><p>-Arabic Task 1: Hybrid model using word embeddings and BERT predictions as features for logistic regression model. -English Task 1: Logistic regression model with POS tags, controversial topics, comparative and superlative adjectives, and BERT predictions as features.</p><p>-English Task 5: Logistic regression model with word embeddings and BERT predictions as features.</p><p>CLEF 2020 Check That! Lab used precision@30 (P@30) for Arabic Task 1 and average precision (AP) for English Task 1 and Task 5, as official evaluation metrics. Based on official metrics, our primary models for Arabic Task 1, English Task 1, and Task 5 ranked 3 rd (out of 8 groups), 5 th (out of 12 groups), and 3 rd (out of 3 groups), respectively. However, based on other metrics, our models shared the first rank with others in many cases. In particular, our primary model for Arabic Task 1 shared the 1 st rank with another group based on P@5. In addition, our primary model in Task 1 English shared the 1 st rank with 5 other groups on RR, P@1, P@3 and P@5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are a number of studies in check-worthy claim detection. Hassan et al. <ref type="bibr" coords="2,465.10,620.25,15.50,8.74" target="#b11">[12]</ref> develop ClaimBuster which is one of the first check-worthy claim detection models. ClaimBuster uses many features including part-of-speech (POS) tags, named entities, sentiment, and TF-IDF representations of claims. TATHYA <ref type="bibr" coords="2,443.13,656.12,15.50,8.74" target="#b16">[17]</ref> uses topics detected in all presidential debates from 1976 to 2016, POS tuples, entity history, and bag-of-words as features.</p><p>Gencheva et al. <ref type="bibr" coords="3,221.29,143.98,10.52,8.74" target="#b6">[7]</ref> propose a neural network model with a long list of sentence level and contextual features including sentiment, named entities, word embeddings, topics, contradictions, and others. Jaradat et al. <ref type="bibr" coords="3,413.12,167.89,15.50,8.74" target="#b12">[13]</ref> use similar features with Gencheva et al. <ref type="bibr" coords="3,269.97,179.85,10.52,8.74" target="#b6">[7]</ref> but extend the model for Arabic. In its followup work, Vasileva et al. <ref type="bibr" coords="3,258.04,191.80,15.50,8.74" target="#b18">[19]</ref> propose a multi-task learning model to detect whether a claim will be fact-checked by at least five of 9 reputable fact-checking organizations.</p><p>In 2018, Check That! Lab (CTL) has been organized for the first time in English and Arabic with participation of seven teams <ref type="bibr" coords="3,357.45,240.70,9.96,8.74" target="#b2">[3]</ref>. The participants investigated many learning models such as recurrent neural network (RNN) <ref type="bibr" coords="3,439.44,252.66,14.61,8.74" target="#b9">[10]</ref>, multilayer perceptron <ref type="bibr" coords="3,214.32,264.61,14.61,8.74" target="#b22">[23]</ref>, random forest (RF) <ref type="bibr" coords="3,322.19,264.61,9.96,8.74" target="#b0">[1]</ref>, k-nearest neighbor (kNN) <ref type="bibr" coords="3,451.37,264.61,10.52,8.74" target="#b7">[8]</ref> and gradient boosting <ref type="bibr" coords="3,215.27,276.57,15.50,8.74" target="#b21">[22]</ref> with different sets of features such as bag-of-words <ref type="bibr" coords="3,462.32,276.57,14.61,8.74" target="#b22">[23]</ref>, character n-gram <ref type="bibr" coords="3,211.76,288.52,9.96,8.74" target="#b7">[8]</ref>, POS tags <ref type="bibr" coords="3,271.56,288.52,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="3,288.71,288.52,12.73,8.74" target="#b21">22,</ref><ref type="bibr" coords="3,303.10,288.52,11.62,8.74" target="#b22">23]</ref>, verbal forms <ref type="bibr" coords="3,378.04,288.52,14.61,8.74" target="#b22">[23]</ref>, named entities <ref type="bibr" coords="3,465.09,288.52,15.50,8.74" target="#b21">[22,</ref><ref type="bibr" coords="3,134.77,300.48,11.62,8.74" target="#b22">23]</ref>, syntactic dependencies <ref type="bibr" coords="3,252.88,300.48,15.50,8.74" target="#b22">[23,</ref><ref type="bibr" coords="3,270.04,300.48,11.62,8.74" target="#b9">10]</ref>, and word embeddings <ref type="bibr" coords="3,383.28,300.48,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="3,400.44,300.48,12.73,8.74" target="#b21">22,</ref><ref type="bibr" coords="3,414.82,300.48,11.62,8.74" target="#b22">23]</ref>. On English dataset, Prise de Fer <ref type="bibr" coords="3,224.90,312.43,15.50,8.74" target="#b22">[23]</ref> team achieved the best MAP scores using bag-of-words, POS tags, named entities, verbal forms, negations, sentiment, clauses, syntactic dependency and word embeddings with SVM-Multilayer perceptron learning. On Arabic dataset, the model of Yasser et al. <ref type="bibr" coords="3,331.80,348.30,15.50,8.74" target="#b21">[22]</ref> outperformed the others using POS tags, named entities, sentiment, topics, and word embeddings.</p><p>In CTL'19, chekc-worthiness task has been organized for only English <ref type="bibr" coords="3,454.37,373.28,9.96,8.74" target="#b3">[4]</ref>. 11 teams participated in the task and used varying models such as LSTM, SVM, naive bayes, and logistic regression (LR) with many features including readability of sentences and their context. Copenhagen team <ref type="bibr" coords="3,358.48,409.15,15.50,8.74" target="#b10">[11]</ref> achieved the best overall performance using syntactic dependency and word embeddings with weakly supervised LSTM model.</p><p>The labeled datasets provided by CTL enabled further studies in this task. Lespagnol et al. <ref type="bibr" coords="3,207.75,458.05,15.50,8.74" target="#b14">[15]</ref> explore using SVM, LR, and Random Forests with a long list of features including word-embeddings, POS tags, syntactic dependency tags, entities, and "information nutritional" features which represent factuality, emotion, controversy, credibility, and technicality of statements. Kartal et al. <ref type="bibr" coords="3,448.62,493.91,15.50,8.74" target="#b13">[14]</ref> use logistic regression utilizing BERT model with additional features including word embeddings, controversial topics, hand-crafted list of words, POS tags, presence of comparative and superlative adjectives, and adverbs. They achieve the highest AP scores on both CTL'18 and CTL'19 English datasets. In CTL'20, we use features adapted from Kartal et al. <ref type="bibr" coords="3,293.83,553.69,14.80,8.74" target="#b13">[14]</ref>'s model. However, we also explore additional features such as tweet meta data features. We also investigate a hybrid combination of fine-tuned BERT model with logistic regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>In this section, we explain the features we investigate (Section 3.1) and models to prioritize claims (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Features</head><p>BERT: We first remove mentions and URLs from tweets. For Arabic tweets, we also apply spelling correction using Farasa<ref type="foot" coords="4,322.81,152.27,3.97,6.12" target="#foot_0">3</ref> . Then we fine tune BERT models using respective training data sets. We use multilingual uncased-large BERT model <ref type="bibr" coords="4,164.89,177.75,10.52,8.74" target="#b5">[6]</ref> for English tasks and Ara-BERT model <ref type="bibr" coords="4,358.05,177.75,10.52,8.74" target="#b1">[2]</ref> for the Arabic task. The prediction value of the fined tuned BERT model is used as a feature in the logistic regression model. Word Embeddings (WE): Word embeddings are able to catch semantic and syntactic features of words. Thus, we use word embeddings to capture similarities between claims. Specifically, we represent each sentence as the average vector of its words. We use word2vec models pre-trained on Google news <ref type="bibr" coords="4,417.66,250.08,15.50,8.74" target="#b15">[16]</ref> in Task 5.</p><p>For Task 1, we use fastText models pre-trained on Wikipedia <ref type="bibr" coords="4,415.79,262.03,9.96,8.74" target="#b8">[9]</ref>. Both word embedding models provide a vector size of 300. We exclude out-of-vocabulary words when we use word2vec. Controversial Topics (CT): We use controversial topics feature defined by Kartal et al. <ref type="bibr" coords="4,194.18,310.45,14.61,8.74" target="#b13">[14]</ref>. In this feature, 11 major controversial topics in current US politics (e.g., immigration, gun policy, racism, abortion) are defined. Each topic is represented by the average word embeddings of hand-crafted related words (e.g., "immigrants", "illegal", "borders", "Mexican", "Latino", and "Hispanic" for the immigration topic). We also represent sentences/tweets to be ranked as the average word embeddings excluding stopwords of NLTK <ref type="foot" coords="4,396.28,368.65,3.97,6.12" target="#foot_1">4</ref> . Subsequently, we calculate cosine similarity between sentences/tweets and each topic using their vector representation. This feature is used only for English datasets because this feature is valid only for claims about US politics. Handcrafted Word List (HW): We use handcrafted word list feature defined by Kartal et al. <ref type="bibr" coords="4,202.13,430.59,14.61,8.74" target="#b13">[14]</ref>. In this feature, firstly, 66 words which might be correlated to check-worthy claims are identified (e.g., unemployment). Then, we check whether there is an overlap between lemmas of selected words and lemmas of words in the respective sentence/tweet. Part-of-speech (POS) Tags: Informative words can make a sentence/tweet more likely to be check-worthy. Thus, in this feature set, we use the number of nouns, verbs, adverbs and adjectives in order to catch information load of sentences/tweets. Comparative &amp; Superlative (CS): In this feature, we use the number of comparative and superlative adjectives and adverbs in sentences/tweets, as defined by Kartal et al. <ref type="bibr" coords="4,205.63,551.33,14.61,8.74" target="#b13">[14]</ref>. Tweet Meta Data (TMD): Meta data of tweets might be an indicator for check-worthy claims. For instance, if a tweet is retweeted a lot or shared by an influential people, it might be check-worthy because it reaches to many people and affect people. Specifically, in this feature group, we use the following information about tweets: 1) whether the account is a verified one, 2) whether the tweet is flagged as sensitive content, 3) whether the tweet is quoting another tweet, 4) presence of a URL, 5) presence of a hashtag, 6) whether a user is mentioned, 7) retweet counts, and 8) favorite counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ranking Methodology</head><p>We use two different approaches to prioritize claims based on their check-worthiness using features defined above. Logistic Regression (LR): LR is commonly used in state-of-the-art checkworthy detection models <ref type="bibr" coords="5,245.24,212.89,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="5,262.39,212.89,11.62,8.74" target="#b14">15]</ref>. Thus, we also train a LR model with features defined above. Then we rank claims based on their predicted probabilities of being check-worthy. Hybrid In this model, we apply a hybrid approach combining logistic regression model and BERT model. We first fine tune BERT model as explained above and rank claims using the fine-tuned BERT model. We keep the rankings of the top 10 claims as they are, but re-rank the other claims using logistic regression with word embeddings and BERT features explained above. For Arabic Task 1, we use Ara-Bert as our BERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation</head><p>We use ktrain<ref type="foot" coords="5,197.88,377.07,3.97,6.12" target="#foot_2">5</ref> and huggingface transformers <ref type="bibr" coords="5,342.16,378.65,15.50,8.74" target="#b20">[21]</ref> to fine-tune BERT models with 1 cycle learning rate policy and maximum learning rate of 2e-5 <ref type="bibr" coords="5,444.41,390.60,14.61,8.74" target="#b17">[18]</ref>. We use SpaCy<ref type="foot" coords="5,181.13,400.98,3.97,6.12" target="#foot_3">6</ref> for all syntactic and semantic analyses. We use Scikit toolkit <ref type="foot" coords="5,460.23,400.98,3.97,6.12" target="#foot_4">7</ref> for the implementation of LR. We use default parameters for LR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>Our experiments are in two steps. We first evaluate different models using training datasets. Subsequently, we report results for our models participated in the shared task on the test data. In evaluation of different models with the training data, we use different cases for each task and language because the data formats and sizes are different for each of them. In particular, in Arabic Task 1 , we use 5-fold cross validation. In English Task 1, both training and validation data sets are provided in the development phase of the shared task. Thus, we use the same setup. In English Task 5, transcripts of 50 political debates and speeches are provided. Following the suggestion of the shared task organizers, we use the first 40 files (i.e., debates) as training and remaining 10 files for evaluating different models in the development phase of the shared task.</p><p>We evaluate the models with the following metrics: average precision (AP), precision@1 (P@1), precision@5 (P@5), precision@10 (P@10) and precision@30 (P@30). The official metrics are P@30 for Arabic and AP for English tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>Experiments on Training Data. We first compare performance of different models on Arabic training dataset using 5-fold cross validation. In particular, we use fine-tuned Multilingual BERT (M-BERT) <ref type="bibr" coords="6,335.43,166.10,9.96,8.74" target="#b5">[6]</ref>, Ara-BERT, logistic regression with different combinations of BERT, WE and TMD features defined in Section 2, and our hybrid model.</p><p>The results are shown in Table <ref type="table" coords="6,292.30,202.62,3.87,8.74">1</ref>. Our observations are as follows. Firstly, Ara-BERT outperforms M-BERT, showing superior performance of language specific models compared to multilingual models. Secondly, TMD features do not yield higher prediction accuracy. Lastly, hybrid model outperforms all other models based on all metrics. Thus, we choose hybrid model as our primary model for Arabic Task 1. We also choose the second best model which is LR with Ara-BERT and WE, as our contrastive submission (C1).</p><p>Table <ref type="table" coords="6,164.92,307.70,4.13,7.89">1</ref>. Evaluation results for different models on the training data for Arabic Task 1 using 5-fold cross validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>AP P@1 P@5 P@10 P@30 M-BERT</p><p>. Next, we compare different models using training data provided for English Task 1. In particular, we investigate performance of fined tuned BERT model, logistic regression with different sets of features defined in Section 2, and our hybrid model. In this set of experiments, we also use two different word embedding models, word2vec and fastText (FT) for WE features.</p><p>The results are shown in Table <ref type="table" coords="6,288.06,548.52,3.87,8.74" target="#tab_1">2</ref>. Our observations based on the results are as follows. Firstly, word2vec yields higher AP scores than fastText in our logistic regression model (0.625 vs. 0.573). However, we observe the opposite case in our hybrid model such that fastText yields slightly higher results than word2vec (0.799 vs. 0.805). Secondly, using only BERT outperforms all models that do not use BERT. Thirdly, we achieve our best AP scores results when we use logistic regression with our BERT, POS, CT, and HW features together. Lastly, replacing HW with CS yields slightly lower AP (0.817 vs. 0.821) but higher P@30 (0.867 vs. 0.833). Based on these results, we choose logistic regression with BERT, POS, CT, and HW features, as our primary model. For Task 5, we investigate performance of fined tuned BERT model, logistic regression with different sets of features defined in Section 2, and our hybrid model. The results are shown in Table <ref type="table" coords="7,303.47,312.66,3.87,8.74" target="#tab_2">3</ref>. The primary model for English Task 1 (i.e., LR with POS, CT, HW and BERT features) achieve the best P@30 scores while hybrid model (i.e., primary model for Arabic Task 1) is inferior to other models. Logistic regression with BERT and WE features achieve the best AP scores. Thus, we select this model as our primary model for Task 5. Experiments on Test Data. We train our primary and contrastive models using training data provided in the development phase of the shared task. The results are shown in Table <ref type="table" coords="7,252.17,546.22,3.87,8.74">4</ref>. In Arabic Task 1, our best run (C1) is ranked 2 nd among all best runs per team based on official metric P@30. Our primary model also shares the first rank with another group based on P@5 metric. Considering all runs submitted for Arabic Task 1, our contrastive and primary models are ranked 5 th and 7 th among 28 participants, respectively, based on P@30.</p><p>In English Task 1, our primary model is ranked 5 th among all primary models. However, our primary model and second contrastive model (C2) share the first rank with nine other models based on P@1 and P@5 metrics. Our second contrastive model actually outperforms our primary model and shares the first rank with five other models based on P@10.</p><p>In English Task 5, all our models unfortunately show poor performance on the test dataset. Our primary model ranked third among three primary models. Table <ref type="table" coords="8,164.24,167.27,4.13,7.89">4</ref>. AP, P@1, P@5, P@10 and P@30 scores of our primary and contrastive models on the test data for each task. Official metric results are written in bold. (P) indicates that the respective model is our primary model. (C1) and (C2) represent our first and second contrastive models Task Model AP P@1 P@5 P@10 P@30 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present our participation in Task 1 and Task 5 of CLEF-2020 Check That! Lab. We use three different models for Arabic Task 1, English Task 1, and Task 5 as our primary models. For Arabic Task 1, we propose a hybrid model which uses a fine-tuned BERT model for the top ten claims and then use logistic regression model with BERT and word embedding features to re-rank the remaining claims. For English Task 1, we rank claims using a logistic regression with features including domain-specific controversial topics, prediction of finedtuned BERT model, a handcrafted word list, and POS tags. For English Task 5, we use logistic regression with BERT and word embedding features.</p><p>Our primary models for Arabic Task 1, English Task 1, and Task 5 ranked 3 rd (out of 8 groups), 5 th (out of 12 groups), and 3 rd (out of 3 groups), respectively, based on official evaluation metric of each task. Our models also share the first rank with other groups in Arabic Task 1 and English Task 1 based on various evaluation metrics.</p><p>We believe that misinformation is a global problem. Therefore, we plan to work on different languages and build a multilingual check-worthy claim detection model in the future. Furthermore, the limited number of annotated datasets is one of the main obstacles to develop effective systems. Thus, we also plan to explore weak supervision methods and develop deep learning models for this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,115.91,345.82,130.48"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results for different models on the training data for</figDesc><table coords="7,134.77,126.87,318.53,119.52"><row><cell>Task 1 English</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="5">AP P@1 P@5 P@10 P@30</cell></row><row><cell>BERT</cell><cell>.807</cell><cell>1</cell><cell>1</cell><cell cols="2">.800 .833</cell></row><row><cell>LR w/ word2vec</cell><cell>.625</cell><cell>1</cell><cell cols="3">.800 .600 .600</cell></row><row><cell>LR w/ fastText</cell><cell>.573</cell><cell>0</cell><cell cols="3">.400 .400 .600</cell></row><row><cell>LR w/ {BERT+fastText}</cell><cell>.797</cell><cell>1</cell><cell>1</cell><cell cols="2">.800 .867</cell></row><row><cell cols="2">LR w/ {POS+CT+CS+BERT} .817</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>.867</cell></row><row><cell cols="2">LR w/ {POS+CT+HW+BERT} .821</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>.833</cell></row><row><cell>Hybrid w/ word2vec</cell><cell>.799</cell><cell>1</cell><cell>1</cell><cell cols="2">.800 .833</cell></row><row><cell>Hybrid w/ fastText</cell><cell>.805</cell><cell>1</cell><cell>1</cell><cell cols="2">.800 .867</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,134.77,396.59,345.82,83.31"><head>Table 3 .</head><label>3</label><figDesc>Evaluation results for different models on the training data for</figDesc><table coords="7,134.77,407.55,319.64,72.35"><row><cell>Task 5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">AP P@1 P@5 P@10 P@30</cell></row><row><cell>BERT</cell><cell>0.101 0.0</cell><cell>0.1</cell><cell cols="2">0.11 0.067</cell></row><row><cell>LR w/ {BERT+WE}</cell><cell>0.124 0.2</cell><cell>0.1</cell><cell>0.1</cell><cell>0.06</cell></row><row><cell cols="5">LR w/ {POS+CT+HW+BERT} 0.113 0.1 0.12 0.09 0.07</cell></row><row><cell>Hybrid</cell><cell>0.096 0.0</cell><cell>0.1</cell><cell cols="2">0.11 0.057</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,136.16,241.50,351.16,74.44"><head></head><label></label><figDesc>LR w/ {POS+CT+HW+BERT} .042 .050 .030 .015 .018</figDesc><table coords="8,136.16,241.50,351.16,74.44"><row><cell>Task 1 Arabic (P) Hybrid</cell><cell>.589</cell><cell>-</cell><cell cols="2">.733 .683 .636</cell></row><row><cell>(C1) LR w/ {BERT+WE}</cell><cell>.582</cell><cell>-</cell><cell cols="2">.700 .700 .644</cell></row><row><cell cols="3">(P) LR w/ {POS+CT+HW+BERT} .706 1</cell><cell>1</cell><cell>.900 .660</cell></row><row><cell>Task 1 English (C1) Hybrid w/ fastText</cell><cell cols="2">.564 0</cell><cell>0</cell><cell>.300 .660</cell></row><row><cell cols="3">(C2) LR w/ {POS+CT+CS+BERT} .710 1</cell><cell>1</cell><cell>1</cell><cell>.680</cell></row><row><cell>Task 5 English (P) LR w/ {BERT+WE}</cell><cell cols="2">.018 0</cell><cell>0</cell><cell>.300 .660</cell></row><row><cell>(C1)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="4,144.73,645.84,154.49,7.86"><p>http://qatsdemo.cloudapp.net/farasa/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="4,144.73,656.80,91.20,7.86"><p>https://www.nltk.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="5,144.73,634.88,131.95,7.86"><p>https://pypi.org/project/ktrain/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="5,144.73,645.84,68.45,7.86"><p>https://spacy.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="5,144.73,656.80,92.30,7.86"><p>https://scikit-learn.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,139.11,337.64,7.86;9,151.52,150.07,329.07,7.86;9,151.52,161.03,222.03,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,403.51,139.11,77.09,7.86">IRIT at checkthat!</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Agez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bosc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lespagnol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Petitcol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,189.74,150.07,290.85,7.86;9,151.52,161.03,24.31,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-10">2018. September 10-14, 2018, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,171.29,337.64,7.86;9,151.52,182.25,329.07,7.86;9,151.52,193.21,194.53,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,303.02,171.29,177.57,7.86;9,151.52,182.25,110.56,7.86">Arabert: Transformer-based model for arabic language understanding</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Antoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hajj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,285.20,182.25,195.39,7.86;9,151.52,193.21,117.39,7.86">LREC 2020 Workshop Language Resources and Evaluation Conference 11-16</title>
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,203.48,337.64,7.86;9,151.52,214.43,329.07,7.86;9,151.52,225.39,329.07,7.86;9,151.52,236.35,234.82,7.86" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barron-Cedeno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kyuchukov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">D S</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05542</idno>
		<title level="m" coord="9,349.90,214.43,130.70,7.86;9,151.52,225.39,293.30,7.86">Overview of the clef-2018 checkthat! lab on automatic identification and verification of political claims</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.96,246.62,337.64,7.86;9,151.52,257.57,329.07,7.86;9,151.52,268.53,304.80,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,151.52,257.57,329.07,7.86;9,151.52,268.53,139.57,7.86">Overview of the clef-2019 checkthat! lab on automatic identification and verification of claims. task 1: Check-worthiness</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martino</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,310.96,268.53,117.27,7.86">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,278.80,337.63,7.86;9,151.52,289.76,329.07,7.86;9,151.52,300.72,329.07,7.86;9,151.52,311.67,129.12,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,211.98,300.72,268.61,7.86;9,151.52,311.67,124.68,7.86">Overview of CheckThat! 2020: Automatic identification and verification of claims in social media</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hamdan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">Sheikh</forename><surname>Ali</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,321.94,337.64,7.86;9,151.52,332.90,329.07,7.86;9,151.52,343.86,329.07,7.86;9,151.52,354.81,329.07,7.86;9,151.52,365.77,93.23,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,374.26,321.94,106.34,7.86;9,151.52,332.90,215.11,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,386.50,332.90,94.09,7.86;9,151.52,343.86,329.07,7.86;9,151.52,354.81,173.78,7.86">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="9,142.96,376.04,337.63,7.86;9,151.52,387.00,329.07,7.86;9,151.52,397.95,329.07,7.86;9,151.52,408.91,191.93,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,438.61,376.04,41.98,7.86;9,151.52,387.00,290.03,7.86">A contextaware approach for detecting worth-checking claims in political debates</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gencheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Koychev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,462.94,387.00,17.66,7.86;9,151.52,397.95,329.07,7.86;9,151.52,408.91,100.57,7.86">Proceedings of the International Conference Recent Advances in Natural Language Processing</title>
		<meeting>the International Conference Recent Advances in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
	<note>RANLP 2017</note>
</biblStruct>

<biblStruct coords="9,142.96,419.18,337.63,7.86;9,151.52,430.14,329.07,7.86;9,151.52,441.10,329.07,7.86;9,151.52,452.05,150.49,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,420.71,419.18,59.88,7.86;9,151.52,430.14,273.04,7.86">UPV-INAOEcheck that: Preliminary approach for checking worthiness of claims</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M R</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,446.35,430.14,34.25,7.86;9,151.52,441.10,284.45,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,462.32,337.64,7.86;9,151.52,473.28,329.07,7.86;9,151.52,484.24,188.04,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,404.36,462.32,76.23,7.86;9,151.52,473.28,84.97,7.86">Learning word vectors for 157 languages</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,254.58,473.28,226.01,7.86;9,151.52,484.24,159.79,7.86">Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC 2018)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,494.50,337.97,7.86;9,151.52,505.46,329.07,7.86;9,151.52,516.42,329.07,7.86;9,151.52,527.38,81.69,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,389.38,494.50,91.20,7.86;9,151.52,505.46,329.07,7.86;9,151.52,516.42,329.07,7.86;9,151.52,527.38,11.14,7.86">The copenhagen team participation in the check-worthiness task of the competition of automatic identification and verification of claims in political debates of the clef-2018 checkthat! lab</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,181.97,527.38,21.74,7.86">CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,537.64,337.98,7.86;9,151.52,548.60,329.07,7.86;9,151.52,559.56,329.07,7.86;9,151.52,570.52,200.83,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,377.64,537.64,102.95,7.86;9,151.52,548.60,311.62,7.86">Neural weakly supervised fact check-worthiness detection with contrastive sampling-based ranking loss</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,559.56,324.21,7.86">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">September 9-12, 2019, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,580.78,337.98,7.86;9,151.52,591.74,329.07,7.86;9,151.52,602.70,329.07,7.86;9,151.52,613.66,20.99,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,453.46,591.74,27.13,7.86;9,151.52,602.70,221.84,7.86">Claimbuster: The first-ever end-to-end fact-checking system</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Caraballo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gawsane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sable</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tremayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,385.58,602.70,29.04,7.86">PVLDB</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1945" to="1948" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,623.92,337.98,7.86;9,151.52,634.88,329.07,7.86;9,151.52,645.84,329.07,7.86;9,151.52,656.80,193.02,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,435.77,623.92,44.82,7.86;9,151.52,634.88,211.95,7.86">Claimrank: Detecting check-worthy claims in arabic and english</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Jaradat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gencheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,385.00,634.88,95.59,7.86;9,151.52,645.84,329.07,7.86;9,151.52,656.80,110.76,7.86">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="26" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,119.67,337.97,7.86;10,151.52,130.63,320.32,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,324.49,119.67,156.10,7.86;10,151.52,130.63,193.49,7.86">Too many claims to fact-check: Prioritizing political claims based on check-worthiness</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guvenen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<idno>ArXiv, abs/2004.08166</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,141.59,337.97,7.86;10,151.52,152.55,329.07,7.86;10,151.52,163.51,329.07,7.86;10,151.52,174.47,171.85,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,323.82,141.59,156.77,7.86;10,151.52,152.55,212.02,7.86">Information nutritional label and word embedding to estimate information check-worthiness</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lespagnol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Z</forename><surname>Ullah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,384.88,152.55,95.71,7.86;10,151.52,163.51,329.07,7.86;10,151.52,174.47,53.55,7.86">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="941" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,185.43,337.97,7.86;10,151.52,196.39,283.97,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,364.51,185.43,116.08,7.86;10,151.52,196.39,122.41,7.86">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.62,207.34,337.98,7.86;10,151.52,218.30,329.07,7.86;10,151.52,229.26,329.07,7.86;10,151.52,240.22,72.44,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,331.84,207.34,148.75,7.86;10,151.52,218.30,216.30,7.86">Tathya: A multi-classifier system for detecting check-worthy statements in political debates</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Patwari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bagchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,387.02,218.30,93.57,7.86;10,151.52,229.26,270.59,7.86">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2259" to="2262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,251.18,337.98,7.86;10,151.52,262.14,329.07,7.86;10,151.52,273.10,20.99,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="10,207.29,251.18,273.30,7.86;10,151.52,262.14,226.44,7.86">A disciplined approach to neural network hyper-parameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<idno>ArXiv, abs/1803.09820</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,284.06,337.97,7.86;10,151.52,295.02,329.07,7.86;10,151.52,305.98,329.07,7.86;10,151.52,316.93,93.91,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,450.13,284.06,30.46,7.86;10,151.52,295.02,312.66,7.86">It takes nine to smell a rat: Neural multi-task learning for check-worthiness prediction</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vasileva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,305.98,329.07,7.86;10,151.52,316.93,65.83,7.86">Proceedings of the International Conference on Recent Advances in Natural Language Processing</title>
		<meeting>the International Conference on Recent Advances in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,327.89,337.98,7.86;10,151.52,338.85,110.07,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,284.91,327.89,156.86,7.86">The spread of true and false news online</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,448.39,327.89,28.17,7.86">Science</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="issue">6380</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,349.81,337.97,7.86;10,151.52,360.77,329.07,7.86;10,151.52,371.73,278.54,7.86" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="10,342.33,360.77,138.26,7.86;10,151.52,371.73,151.82,7.86">Huggingface&apos;s transformers: Stateof-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,382.69,337.98,7.86;10,151.52,393.65,329.07,7.86;10,151.52,404.61,164.50,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,305.31,382.69,175.28,7.86;10,151.52,393.65,141.20,7.86">bigir at CLEF 2018: Detection and verification of check-worthy political claims</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yasser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,310.06,393.65,170.53,7.86;10,151.52,404.61,135.62,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,415.56,337.98,7.86;10,151.52,426.52,258.18,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,300.77,415.56,179.83,7.86;10,151.52,426.52,187.47,7.86">A hybrid recognition system for check-worthy claims using heuristics and supervised learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,358.47,426.52,21.74,7.86">CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
