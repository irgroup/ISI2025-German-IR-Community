<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.27,115.96,338.81,12.62;1,137.70,133.89,339.97,12.62">The University of Sheffield at CheckThat! 2020: Claim Identification and Verification on Twitter</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,158.11,171.56,80.68,8.74"><forename type="first">Thomas</forename><surname>Mcdonald</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.42,171.56,53.91,8.74"><forename type="first">Ziqing</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.82,171.56,55.07,8.74"><forename type="first">Yingji</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.19,171.56,78.61,8.74"><forename type="first">Rebekah</forename><surname>Hampson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,159.82,183.51,55.45,8.74"><forename type="first">James</forename><surname>Young</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.66,183.51,49.82,8.74"><forename type="first">Qianyu</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,281.77,183.51,76.81,8.74"><forename type="first">Jochen</forename><forename type="middle">L</forename><surname>Leidner</surname></persName>
							<email>leidner@acm.org</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,385.72,183.51,69.83,8.74"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
							<email>mark.stevenson@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.27,115.96,338.81,12.62;1,137.70,133.89,339.97,12.62">The University of Sheffield at CheckThat! 2020: Claim Identification and Verification on Twitter</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AAFEFF44176934DF1E84966138AEC090</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The spread of misinformation online has been gathering pace in recent years which has led to research into automatic methods for claim verification. The COVID-19 pandemic presents a unique challenge due to the large amount of inaccurate information being shared on social media platforms. This paper describes the University of Sheffield's entry to the CLEF 2020 CheckThat! Lab, which focuses on the problems of determining check-worthiness and verification of claims found in tweets, including those related to COVID-19. For the Tweet Check-Worthiness Task (Task 1), we found that TF-IDF term weightings used by a Random Forest model outperformed more complex approaches employing Word2Vec embeddings and recurrent neural networks, and for the Claim Retrieval Task (Task 2), we found that BM25 similarity score weightings based on TF-IDF term weightings with a Support Vector Machine classifier scoring model outperformed other methods making use of cosine and Euclidean similarity metrics, and regression-based scoring models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The rise of social media platforms has changed the way in which people across the world consume media, including news: these platforms allow individual users to publish and disseminate content, including disinformation and so called 'fake news'. Social media platforms can act as a breeding ground for disinformation and allow controversial posts to spread to millions of users in a very short space of time. Sites such as Instagram, Facebook and Twitter are popular platforms for individuals looking to spread disinformation <ref type="bibr" coords="1,327.08,533.08,14.61,8.74" target="#b17">[18]</ref>. The 2020 CheckThat! Lab took place in the context of the COVID-19 pandemic which swept across the globe during the year. The spread of disinformation in the midst of such a global health emergency can cause physical harm to members of the public through increased panic, unsafe actions and confusion <ref type="bibr" coords="1,298.42,580.90,14.61,8.74" target="#b14">[15]</ref>. Research surrounding the spread of misinformation pertaining to COVID-19 has concluded that different social media platforms contain different levels of misinformation, although misinformation spreads in a similar way on each platform <ref type="bibr" coords="1,320.15,616.77,9.96,8.74" target="#b4">[5]</ref>.</p><p>The primary focus of the 2020 CheckThat! Lab was verification of tweets pertaining to the COVID-19 pandemic <ref type="bibr" coords="2,295.52,130.95,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,307.70,130.95,11.62,8.74" target="#b20">21]</ref>. The University of Sheffield participated in Tasks 1 and 2. The aim of Task 1, entitled 'Tweet Check-Worthiness', is to create a system which can rank a set of tweets according to how likely they are to require manual fact-checking. The aim of task 2, entitled 'Claim Retrieval', is to rank a set of previously verified claims given an input claim such that those which verify the input claim are ranked highly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task 1</head><p>Task 1 focuses on ranking tweets based on whether the claims they contain need to be fact checked. Similar tasks have been run at previous versions of the CheckThat! Lab and entries have applied a range of approaches with varying levels of complexity. The winning approach to Task 1 at CheckThat! 2019 <ref type="bibr" coords="2,470.07,304.87,10.52,8.74" target="#b8">[9]</ref> used Word2Vec embeddings <ref type="bibr" coords="2,257.96,316.82,15.50,8.74" target="#b18">[19]</ref> and syntactic dependencies as features, fed into a recurrent neural network, with Long Short Term Memory (LSTM) units <ref type="bibr" coords="2,462.32,328.78,14.61,8.74" target="#b11">[12]</ref>. Other entries to the 2019 task included the TOBB ETU team's hybrid approach that combined supervised learning with handcrafted rules to rank the statements several times <ref type="bibr" coords="2,192.81,364.64,9.96,8.74" target="#b0">[1]</ref>. More traditional approaches were implemented by other teams, such as UAICS, who tested multiple ML algorithms including Decision Trees, Naïve Bayes, SVM, Random Forest, Logistic Regression and Multi-Layer Perceptron models <ref type="bibr" coords="2,204.89,400.51,14.61,8.74" target="#b16">[17]</ref>. Tokenisation and stop-word removal were both performed as preprocessing steps before TF-IDF weighting was used to extract features for training. Their Naive Bayes model reached first place in the Lab for the P@1, P@3, P@10 and RR evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task 2</head><p>Task 2 focuses on identifying whether previously verified claims contain information that can help support the verification of a new claim. Approaches are given an input claim and a set of claims that have previously been verified. The goal is to produce a ranking of the previously verified claims such that the ones that verify the input claim appear early in the ranking. This is a new task and was not included in the two previous iterations of CheckThat! Nevertheless, there are many similarities between it and Task 2A from CheckThat! 2019 as both tasks focus on document ranking, therefore many techniques utilised by teams during Task 2A last year are relevant to this task. Previous entries included two main approaches to the problem: learning-to-rank (L2R) and text classification for ranking problems <ref type="bibr" coords="2,229.00,607.44,14.61,8.74" target="#b10">[11]</ref>.</p><p>L2R methods <ref type="bibr" coords="2,204.76,632.21,15.50,8.74" target="#b15">[16]</ref> can be divided into three distinct approaches: point-wise, pair-wise and list-wise. The pairwise approach is perhaps the most popular currently due to the fact that it balances performance with computational cost. This technique approximates the L2R problem using binary classification between pairs of documents within the collection. Two teams utilised the pairwise L2R technique at CheckThat! 2019 <ref type="bibr" coords="3,287.24,142.90,9.96,8.74" target="#b7">[8]</ref>, <ref type="bibr" coords="3,303.07,142.90,14.61,8.74" target="#b9">[10]</ref>. One achieved the highest normalised Discounted Cumulative Gain at cutoff 20 (nDCG@20) score of 0.55, although this was still lower than the score achieved by the baseline provided by CLEF (0.61).</p><p>Text classification algorithms can also be applied to ranking problems. Such algorithms can be broadly split into two main categories: those based on traditional machine learning models and those which employ deep learning. Traditional approaches typically exhibit better performance on smaller datasets, whereas deep learning models tend to be the optimal approach when a large amount of data is provided. In last year's task, one team utilised the BERT pre-trained neural language model <ref type="bibr" coords="3,205.39,273.60,9.96,8.74" target="#b6">[7]</ref>, achieving an nDCG@20 score of 0.14 which was much lower than the baseline, as well as the best result obtained by any team using L2R.</p><p>3 Task 1: Tweet Check-worthiness</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Methodology</head><p>The focus of Task 1 was the ranking of a collection of tweets about a given topic based on their check-worthiness <ref type="bibr" coords="3,278.77,370.81,9.96,8.74" target="#b2">[3]</ref>; in this case, the topic was the COVID-19 pandemic. 'Check-worthy' in this context simply means that the content of the tweet is of questionable veracity, could mislead a large number of people, and should be fact-checked manually. A system which can classify tweets as such and automatically disregard tweets which are not check-worthy could be extremely useful, as manual fact-checking is a time intensive process.</p><p>Multiple different models and approaches were tested for Task 1, but the first step taken was to preprocess the English data provided. Firstly, all URLs were stripped from the tweets using a regular expression, emojis and punctuation were removed and the text was converted to lowercase before tokenizing each tweet. Natural Language Toolkit (NLTK) <ref type="bibr" coords="3,289.81,501.51,10.52,8.74" target="#b3">[4]</ref> was then used to remove frequently used stopwords from the list of tokens, before Porter stemming and lemmatization were applied in order to conflate similar terms <ref type="bibr" coords="3,340.08,525.42,14.61,8.74" target="#b19">[20]</ref>.</p><p>In order to establish a baseline, a number of different classifications models were tested, including Naïve Bayes, Random Forest, Gradient Boosting, AdaBoost, k-means and a Support Vector Machine model. Said models were trained on term frequency-inverse document frequency (TF-IDF) weights obtained from the preprocessed text, using unigrams, bigrams and trigrams. After evaluation on the validation set provided, the best performing model from the selection tested was the Random Forest classifier. Random Forest classifiers are versatile models, but the main drawback associated with them is that they are difficult to interpret due to the fact they make predictions using a large ensemble of individual decision trees.</p><p>Following the development of this initial system, a pre-trained Word2Vec embedding based on the GoogleNews dataset was tested for constructing the document vectors in attempt to improve the accuracy of our baseline classifier. A deep learning approach was also tested, employing an LSTM recurrent neural network. The network consisted of several layers including an input layer, word embedding layer, LSTM layer and fully connected layer as well as an output layer. After constructing the neural network, Adam <ref type="bibr" coords="4,371.19,190.72,15.50,8.74" target="#b13">[14]</ref> and BCELoss<ref type="foot" coords="4,451.91,189.15,3.97,6.12" target="#foot_0">1</ref> were used as the optimizer and loss function respectively, and dropout was used to avoid overfitting. In an attempt to overcome the issues associated with training a very complex, deep model on a limited amount of data, a fastText classification model was also tested <ref type="bibr" coords="4,232.90,238.55,14.60,8.74" target="#b12">[13]</ref>. True Label 1 (i.e. check-worthy) Table <ref type="table" coords="4,176.47,485.98,4.98,8.74" target="#tab_0">1</ref> illustrates an example of how our fastText system arrives at a prediction. The original tweet undergoes a series of preprocessing before being fed into the model, which outputs the probability of the tweet in question being checkworthy. In this particular case, the predicted probability is above 0.5, therefore the system determines that the tweet is indeed check-worthy, and this aligns with the true label given in the validation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task 1 Results</head><p>Based on validation set performance, the approach based on fastText was submitted as our primary system with the Random Forest classifier trained on TF-IDF term weightings acting as our contrastive submission. Other approaches based on Word2Vec embeddings and deep recurrent neural networks such as LSTMs yielded poor results on the validation set and were not submitted.</p><p>Table <ref type="table" coords="5,164.13,162.99,4.13,7.89">2</ref>. Task 1 Results for both validation and official test data. Validation results were generated using the initial release of training and validation data while the submitted models were trained on the second release<ref type="foot" coords="5,333.66,183.17,3.65,5.24" target="#foot_2">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Validation Test MAP P@10 P@20 MAP P@10 P@20 fastText (Primary) 0.812 0.800 0.900 0.475 0.200 0.350 Random Forest (Contrastive) 0.810 0.900 0.900 0.646 0.800 0.600 <ref type="table" coords="5,221.68,273.49,4.98,8.74">2</ref> show that our primary submission performed poorly, ranking last out of all systems submitted for Task 1. The fastText model exhibited a large drop in performance when used to evaluate the test set used for ranking, compared with the validation set used for model selection, which leads us to conclude that a considerable amount of overfitting occured during training. This could have been mitigated by choosing a simpler form of fastText model, perhaps only considering bi-grams instead of 4-grams. Conversely, our contrastive submission performed much better than anticipated, outperforming a number of BERT-based models amongst others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results in Table</head><p>4 Task 2: Claim Retrieval</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methodology</head><p>For Task 2 our objective was to construct a system that could rank all verified claims according to their relevance to a given tweet, and return the most relevant claim with its score generated by the system. Learning-to-rank (L2R) techniques are a primary candidate for solving such a problem. In this task, we chose to take the point-wise approach as the Task 2 dataset includes binary labels ('relevant' and 'not relevant').</p><p>In order to apply the L2R algorithm, firstly several features measuring similarity were generated from the raw data: cosine similarity (Eq. 1), BM25 score (Eq. 2) and simple Euclidean distance. Cosine similarity is computed as</p><formula xml:id="formula_0" coords="5,259.89,575.66,220.70,29.12">cos (Q, D) = - → Q × - → D Q D<label>(1)</label></formula><p>where D represents a verified claim and Q a tweet. The BM25 score is computed as</p><formula xml:id="formula_1" coords="6,179.62,128.52,300.97,30.97">BM 25(Q, D) = n i=1 IDF (q i ) f (q i , D) × (k 1 + 1) f (q i , D) + (1 -b + b × |D| |D|avg )<label>(2)</label></formula><p>where q i are the terms in claim Q, f (q i , D) the term frequency of q i in D, |D| is the number of terms in the verified claim and |D| avg is the average length of a verified claim within the collection. b and k 1 are free parameters.</p><p>Two combinations of these features were explored. The first combination consisted of Euclidean distance and cosine similarity based on word embeddings. For the second combination TF-IDF was used to generate cosine similarities between tweets and verified claims then combined using BM25 scores. In addition, both contents and titles of the verified claims were used to generate the introduced features against the tweets to avoid losing any potentially useful information.</p><p>Multiple different machine learning models were implemented as a scoring model, such as Logistic Regression, Random Forest, Gradient Boosted Trees, Linear SVM and Linear Regression. The sum of multiplying each introduced feature's value by its corresponding coefficient was considered as the final score. Class weighting was applied during the training process to accommodate for the high class imbalance in the training data. Approaches based on a linear Support Vector Machine (SVM), logistic regression and linear regression were submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task 2 Results</head><p>Table <ref type="table" coords="6,163.81,459.16,4.13,7.89">3</ref>. Task 2 Results. All models were trained using the third version of the Task 2 data released by the task organisers<ref type="foot" coords="6,286.85,468.37,3.65,5.24" target="#foot_3">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Validation Test MAP@5 P@5 MAP@5 P@5 Linear SVM (Primary) The models which made use of Word2Vec embeddings and Euclidean distance exhibited very poor performance on the validation set provided, far below the random baseline, therefore all three of our submissions for Task 2 made use of TF-IDF weightings and BM25 scores, and simply employed different scoring models. Table <ref type="table" coords="6,198.47,623.26,4.98,8.74">3</ref> provides a summary of the performance of these techniques.</p><p>Whilst none of the systems ranked within the top half of the leaderboard, the overall results from Task 2 were more promising than Task 1, with all three systems exhibiting reasonable performance. As we predicted based on validation set testing, the Linear SVM scoring model outperformed both the Logistic and Linear Regression scoring models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This paper described our submissions to Tasks 1 and 2 of the CLEF 2020 Lab "CheckThat!". The aim in constructing a system for Task 1 was to evaluate the check-worthiness of a series of COVID-19 related tweets. Our approach was inspired by the 2019 edition of the Lab, applying a series of preprocessing and feature engineering steps followed by the implementation of Random Forest and fastText models. Unexpectedly, the best performing model we obtained was a simple approach consisting of TF-IDF term weightings and a Random Forest model, which ranked 17th out 27 systems. We also participated in Task 2 of the Lab, whose aim was to identify previously verified claims that contain information that could support the verification of a new claim. As with Task 1, methods used by teams in the 2019 Lab led us to our chosen approach, a point-wise L2R system. This involved similar preprocessing steps to those used in Task 1, followed by the extraction of BM25 scores and TF-IDF term weightings from the data, which were used to train a range of different forms of scoring model. Our highest scoring model employed a SVM, ranking 13th out of 22 systems.</p><p>Perhaps the simplest avenue for improvement of our systems would be to supplement the training data with external Twitter data, or other labeled text. The datasets provided for the task were relatively small, containing around 1,000 tweets or less, and it is challenging to train complex models on datasets of this size. Other possibilities could include exploring more descriptive features to capture more information about the tweets, such as syntactic dependencies, numbers of mentioned named entities, links and hashtags, or specific indicators for credibility previously proposed for blogs <ref type="bibr" coords="7,293.97,493.49,9.96,8.74" target="#b5">[6]</ref>. Finally, we expect replacing LSTMs by a pre-trained language model would would likely result in superior performance, and such techniques tend to perform well even with limited amounts of data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,140.54,271.61,334.28,162.16"><head>Table 1 .</head><label>1</label><figDesc>Task 1 Prediction Example Original Tweet I just landed at JFK after reporting on #coronavirus in Milan and Lombardy -the epicenter of Italy's outbreakfor @vicenews. I walked right through US customs. They didn't ask me where in Italy I went or if I came into contact with sick people. They didn't ask me anything.</figDesc><table coords="4,140.54,360.13,331.58,73.64"><row><cell>Preprocessed Tweet</cell><cell>land jfk report coronavirus milan lombardy -the</cell></row><row><cell></cell><cell>epicenter italy outbreak-vicenews walk right us custom</cell></row><row><cell></cell><cell>ask italy go come contact sick people ask anything</cell></row><row><cell>FastText Score</cell><cell>0.512</cell></row><row><cell>FastText Prediction</cell><cell>1 (i.e. check-worthy)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.73,645.84,335.87,8.12"><p>https://pytorch.org/docs/master/generated/torch.nn.BCELoss.html (accessed</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="4,144.73,656.80,46.59,7.86"><p>2020-07-14)   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="5,144.73,645.84,335.86,8.12;5,144.73,656.80,12.80,7.86"><p>https://github.com/sshaar/clef2020-factchecking-task1 (accessed 2020-07-14)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="6,144.73,645.84,335.86,8.12;6,144.73,656.80,12.80,7.86"><p>https://github.com/sshaar/clef2020-factchecking-task2 (accessed 2020-07-14)</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.96,569.81,337.63,7.86;7,151.52,580.77,140.28,7.86" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Altun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<title level="m" coord="7,245.07,569.81,235.52,7.86;7,151.52,580.77,65.65,7.86">TOBB-ETU at CLEF 2019: Prioritizing claims based on check-worthiness</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,591.39,337.64,7.86;7,151.52,602.34,329.07,7.86;7,151.52,613.30,329.07,7.86;7,151.52,624.26,219.91,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,182.56,613.30,298.03,7.86;7,151.52,624.26,86.54,7.86">Overview of CheckThat! 2020: Automatic identification and verification of claims in social media</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hamdan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Sheikh Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,245.58,624.26,24.45,7.86">LNCS</title>
		<imprint>
			<biblScope unit="volume">12260</biblScope>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,634.88,337.63,7.86;7,151.52,645.84,329.07,7.86;7,151.52,656.80,255.80,7.86" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barron-Cedeno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">D S</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<title level="m" coord="7,263.45,645.84,217.13,7.86;7,151.52,656.80,227.14,7.86">CheckThat! at CLEF 2020: Enabling the Automatic Identification and Verification of Claims in Social Media</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,119.67,337.63,7.86;8,151.52,130.63,329.07,7.86;8,151.52,141.59,25.60,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,275.88,119.67,204.71,7.86;8,151.52,130.63,195.41,7.86">Natural Language Processing with Python -Analyzing Text with the Natural Language Toolkit</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly</publisher>
			<pubPlace>Sebastopol, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,152.50,337.63,7.86;8,151.52,163.46,329.07,7.86;8,151.52,174.42,52.22,7.86" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cinelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Quattrociocchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Galeazzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Valensise</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brugnoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Zollo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Scala</surname></persName>
		</author>
		<title level="m" coord="8,338.43,163.46,142.16,7.86;8,151.52,174.42,23.55,7.86">The COVID-19 Social Media Infodemic</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,185.33,337.64,7.86;8,151.52,196.29,329.07,7.86;8,151.52,207.24,130.30,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,316.43,185.33,164.16,7.86;8,151.52,196.29,13.63,7.86">Professional credibility: Authority on the web</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Conrad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Leidner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schilder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,186.82,196.29,293.78,7.86;8,151.52,207.24,13.63,7.86">Proceedings of the 2nd ACM workshop on Information credibility on the web</title>
		<meeting>the 2nd ACM workshop on Information credibility on the web</meeting>
		<imprint>
			<publisher>WICOW</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="85" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,218.15,337.63,7.86;8,151.52,229.11,329.07,7.86;8,151.52,240.07,329.08,7.86;8,151.52,251.03,155.96,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,395.60,218.15,85.00,7.86;8,151.52,229.11,266.09,7.86">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="8,445.36,229.11,35.23,7.86;8,151.52,240.07,90.16,7.86">Proceedings of NAACL 2019</title>
		<meeting>NAACL 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,261.94,337.64,7.86;8,151.52,272.90,199.16,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="8,341.84,261.94,138.75,7.86;8,151.52,272.90,124.44,7.86">TheEarthIsFlat&apos;s Submission to CLEF&apos;19CheckThat! Challenge</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Favano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Carman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">L</forename><surname>Lanzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,283.81,337.63,7.86;8,151.52,294.76,329.07,7.86;8,151.52,305.72,329.07,7.86;8,151.52,316.68,278.23,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,358.17,283.81,122.41,7.86;8,151.52,294.76,290.42,7.86">Neural weakly supervised fact check-worthiness detection with contrastive sampling-based ranking loss</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,462.67,294.76,17.92,7.86;8,151.52,305.72,329.07,7.86;8,151.52,316.68,205.85,7.86">20th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2019 Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,327.59,337.97,7.86;8,151.52,338.55,187.35,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,291.64,327.59,188.95,7.86;8,151.52,338.55,111.71,7.86">bigIR at CLEF 2019: Automatic Verification of Arabic Claims over the Web</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,285.50,338.55,24.70,7.86">CLEF</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,349.46,337.98,7.86;8,151.52,360.42,329.07,7.86;8,151.52,371.38,236.50,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,442.93,349.46,37.66,7.86;8,151.52,360.42,329.07,7.86;8,151.52,371.38,25.94,7.86">Overview of the CLEF-2019 CheckThat! Lab: Automatic Identification and Verification of Claims</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,184.85,371.38,128.73,7.86">Task 2: Evidence and Factuality</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,382.28,337.98,7.86;8,151.52,393.22,92.85,7.89" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,287.48,382.28,100.09,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,396.45,382.28,84.14,7.86">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,404.15,337.98,7.86;8,151.52,415.11,82.01,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="8,356.57,404.15,124.02,7.86;8,151.52,415.11,53.35,7.86">Bag of Tricks for Efficient Text Classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,426.02,337.98,7.86;8,151.52,436.98,329.07,7.86;8,151.52,447.94,329.07,7.86;8,151.52,458.90,25.60,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,244.82,426.02,182.98,7.86">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,240.03,436.98,240.56,7.86;8,151.52,447.94,42.85,7.86">3rd International Conference on Learning Representations, ICLR 2015</title>
		<title level="s" coord="8,358.32,447.94,122.27,7.86">Conference Track Proceedings</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,469.80,337.97,7.86;8,151.52,480.76,329.07,7.86;8,151.52,491.72,140.18,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,360.64,469.80,119.95,7.86;8,151.52,480.76,290.32,7.86">Fact-checking as risk communication: the multi-layered risk of misinformation in times of COVID-19</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">M</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Freiling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Beets</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,450.24,480.76,30.35,7.86;8,151.52,491.72,67.20,7.86">Journal of Risk Research</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="8,142.62,502.63,337.98,7.86;8,151.52,513.59,310.37,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="8,194.22,502.63,286.37,7.86;8,151.52,513.59,85.63,7.86">Learning to Rank for Information Retrieval, Foundations and Trends in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Now Publishers</publisher>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>Delft, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,524.50,337.98,7.86;8,151.52,535.46,67.45,7.86" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Lucia-Georgiana</forename><surname>Coca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ciprian-Gabriel</forename><surname>Cusmuliuc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">I</forename></persName>
		</author>
		<title level="m" coord="8,378.67,524.50,96.68,7.86">CheckThat! 2019 UAICS</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,546.37,337.98,7.86;8,151.52,557.32,138.62,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="8,253.04,546.37,195.60,7.86">Media Manipulation and Disinformation Online</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Marwick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Data &amp; Society Research Institute</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,568.23,337.97,7.86;8,151.52,579.19,329.07,7.86;8,151.52,590.15,220.52,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,407.41,568.23,73.17,7.86;8,151.52,579.19,231.41,7.86">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,403.01,579.19,77.58,7.86;8,151.52,590.15,127.04,7.86">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,601.03,329.37,7.89" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="8,207.88,601.06,130.19,7.86">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,345.00,601.06,34.98,7.86">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,611.97,337.97,7.86;8,151.52,622.93,329.07,7.86;8,151.52,633.89,329.07,7.86;8,151.52,644.85,78.13,7.86" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<title level="m" coord="8,442.93,622.93,37.66,7.86;8,151.52,633.89,329.07,7.86;8,151.52,644.85,49.46,7.86">Overview of CheckThat! 2020 English: Automatic identification and verification of claims in social media</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
