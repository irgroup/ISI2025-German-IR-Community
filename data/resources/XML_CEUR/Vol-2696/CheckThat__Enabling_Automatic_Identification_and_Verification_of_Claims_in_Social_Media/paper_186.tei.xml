<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,196.76,115.90,221.84,13.08;1,137.05,133.83,341.27,12.68;1,152.54,151.77,310.29,12.68;1,267.32,169.70,80.73,12.68">QMUL-SDS at CheckThat! 2020: Determining COVID-19 Tweet Check-Worthiness Using an Enhanced CT-BERT with Numeric Expressions</title>
				<funder>
					<orgName type="full">The Alan Turing Institute</orgName>
				</funder>
				<funder ref="#_NHvtDwE">
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder ref="#_BsNsMR4">
					<orgName type="full">QMUL Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,140.76,207.58,71.24,8.80"><forename type="first">Rabab</forename><surname>Alkhalifa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Imam Abdulrahman bin Faisal University</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,228.89,207.58,71.97,8.80"><forename type="first">Theodore</forename><surname>Yoong</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.41,207.58,68.76,8.80"><forename type="first">Elena</forename><surname>Kochkina</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Alan Turing Institute</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,397.06,207.58,70.31,8.80"><forename type="first">Arkaitz</forename><surname>Zubiaga</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.99,219.54,61.59,8.80"><forename type="first">Maria</forename><surname>Liakata</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Alan Turing Institute</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,196.76,115.90,221.84,13.08;1,137.05,133.83,341.27,12.68;1,152.54,151.77,310.29,12.68;1,267.32,169.70,80.73,12.68">QMUL-SDS at CheckThat! 2020: Determining COVID-19 Tweet Check-Worthiness Using an Enhanced CT-BERT with Numeric Expressions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2FE4C9F5ECF73281642E2A307CC8BBC4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the QMUL-SDS team for Task 1 of the CLEF 2020 CheckThat! shared task. The purpose of this task is to determine the check-worthiness of tweets about COVID-19 to identify and prioritise tweets that need fact-checking. The overarching aim is to further support ongoing efforts to protect the public from fake news and help people find reliable information. We describe and analyse the results of our submissions. We show that a CNN using COVID-Twitter-BERT (CT-BERT) enhanced with numeric expressions can effectively boost performance from baseline results. We also show results of training data augmentation with rumours on other topics. Our best system ranked fourth in the task with encouraging outcomes showing potential for improved results in the future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The vast majority of people seek information online and consider it a touchstone of guidance and authority <ref type="bibr" coords="1,254.06,519.04,9.96,8.80" target="#b0">[1]</ref>. In particular, social media has become the key resource to go to for following updates during times of crisis <ref type="bibr" coords="1,400.52,531.00,9.96,8.80" target="#b1">[2]</ref>. Any registered user can share posts on social media without content verification, potentially exposing thousands or millions of other users to harmful misinformation. To prevent the undesired consequences of misinformation spread, there is a need to develop tools to assess the validity of social media posts. This problem has been particularly accentuated in light of the COVID-19 pandemic, accompanied by the rising spread of unverified claims and conspiracy theories about the virus and untested dangerous treatments. Compounded with the devastating effects from the virus alone, the social harms from misinformation spread can be particularly injurious <ref type="bibr" coords="2,175.79,130.89,9.96,8.80" target="#b2">[3]</ref>. The CheckThat! shared task provided a benchmark evaluation lab to develop systems for check-worthiness detection, with the aim of prioritising claims to be provided to fact-checkers.</p><p>In this paper, we present our approaches in tackling the check-worthiness detection task as outlined in Task 1 of the CLEF-2020 CheckThat! Lab. We evaluated several variants of our Convolutional Neural Network (CNN) model with different pre-processing approaches and several BERT embeddings. We also tested the benefits of including the use of external data to augment the training data provided. We submitted three models that have shown the best performance on the development set. Our best performing model utilised a COVID-Twitter-BERT (CT-BERT) enhanced with numeric expressions, which was ranked fourth in the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We organise the related work into two subsections relevant to our proposed methods and the systems we submitted to the evaluation lab: claim check-worthiness and rumour detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Determination of Claim Check-worthiness</head><p>While there is no general streamlined approach to fact-checking, the fact-checking pipeline can be divided into different sub-tasks based on a number of contexts <ref type="bibr" coords="2,467.31,428.68,9.96,8.80" target="#b3">[4]</ref>. The first of the tasks and the one concerning our work consists in producing a list of claims ranked by importance (check-worthiness), in an effort to prioritise claims to be fact-checked. Systems for claim detection (as a classification task) and ranking by check-worthiness include (i) ClaimBuster <ref type="bibr" coords="2,413.89,476.50,9.96,8.80" target="#b4">[5]</ref>, which combines numerous features such as TF-IDF, POS tags and NER on a Support Vector Machine to produce importance scores for each claim, and (ii) Claim-Rank <ref type="bibr" coords="2,161.10,512.36,9.96,8.80" target="#b5">[6]</ref>, which uses a large set of features both from individuals sentences and from surrounding context. More recent methods, such as <ref type="bibr" coords="2,387.81,524.32,9.96,8.80" target="#b6">[7]</ref>, have made use of embedding-based methods such as InferSent for detecting claims by leveraging contextual features within sentences. In previous editions of CheckThat! <ref type="bibr" coords="2,450.44,548.23,9.96,8.80" target="#b7">[8]</ref>, the shared task did not involve the detection of claims, as claims were already given as input.</p><p>Previous work on claim detection by Konstantinovskiy et al. <ref type="bibr" coords="2,409.54,584.33,10.51,8.80" target="#b6">[7]</ref> suggested the use of numeric expressions as a strong baseline for detection of claims. Indeed they showed that the use of numeric expressions as a feature leads to high precision, despite achieving lower recall and overall F1 score than other methods. This is due to the prevailing presence of numeric expressions in check-worthy claims, as opposed to non-check-worthy claims and non-claims. Given the emphasis of the CheckThat! shared task on precision-based evaluation (using mean average precision as a metric), we opted for incorporating numeric expressions in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Rumour Detection</head><p>A rumour is generally defined as an unverified piece of information that circulates. In the same way that a check-worthiness detection looks at claims to be verified, e.g. in the context of a TV debate, rumour detection consists in detecting pieces of information that are in circulation while they still lack verification, generally in the context of breaking news, making it a time-sensitive task <ref type="bibr" coords="3,467.31,222.98,9.96,8.80" target="#b8">[9]</ref>. Rumours differ from check-worthy claims in their nature as well as relevance to the fact-checkers, as not all rumours are necessarily of interest to fact-checkers. Still, both tasks have significant commonalities.</p><p>In our approaches to check-worthiness determination, we try to leverage existing data for rumour detection, consisting of rumours and non-rumours, with the aim of providing additional knowledge that would enrich the task (see §3.1).</p><p>Rumour detection, as the task of detecting unverified pieces of information, has been studied before, for instance through the RumourEval shared tasks held at SemEval 2019 <ref type="bibr" coords="3,211.47,330.58,14.61,8.80" target="#b9">[10]</ref>. Prior to that, Zubiaga et al. <ref type="bibr" coords="3,359.96,330.58,15.49,8.80" target="#b10">[11]</ref> introduced a sequential rumour detection model that leveraged Conditional Random Fields (CRF) for leveraging event context, as well as Zhao et al. <ref type="bibr" coords="3,338.16,354.49,15.49,8.80" target="#b11">[12]</ref> that looked at evidence from others responding to tweets with comments of the form of "is this really true?", which would be indicative of a tweet containing rumourous content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>The task we explore was introduced by Barrón-Cedeño et al. <ref type="bibr" coords="3,403.50,428.65,15.49,8.80" target="#b12">[13]</ref> and is formulated as follows:</p><p>Given a topic and a stream of potentially-related tweets, rank the tweets according to their check-worthiness for the topic, where a check-worthy tweet is a tweet that includes a claim that is of interest to a large audience (especially journalists) and may have a harmful effect.</p><p>For example, consider the target topic-tweet pair:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target topic: COVID-19</head><p>Tweet: Doctors in #Italy warn Europe to "get ready" for #coronavirus, saying 10% of #COVID19 patients need ICU care, and hospitals are overwhelmed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label: Check-worthy</head><p>Although Task 1 is available in both English and Arabic, we focussed solely on the English task <ref type="bibr" coords="4,207.84,130.89,14.61,8.80" target="#b13">[14]</ref>. Tweets in this dataset for this task exclusively covered the pandemic caused by the Coronavirus Disease 2019 (COVID-19). The ultimate objective of ranking the tweets identified as check-worthy claims is to enable prioritisation of claims to fact-checkers.</p><p>The task can be formally described as the following binary classification problem. We define the training set consisting of n labelled tweets as</p><formula xml:id="formula_0" coords="4,134.77,190.72,345.83,21.61">D = {(x i , y i ), 1 ≤ i ≤ n} ∈ (X × {0, 1}) n .</formula><p>Here, x i is the i th feature vector in feature space X which contains the tweet features such as the i th tweet itself t i , the topic, and whether it is a claim or not; and y i ∈ {0, 1} is the label indicating check-worthiness of t i . The objective is to obtain a map h : X → {0, 1}, based on the class probability measure P(y|t), which is subsequently used to rank the tweets in the test set. In our experiments, we made use of three datasets of Twitter posts in English, which include the dataset provided by the organisers (CLEF) and two external publicly available datasets (PHEME, Twitter 15 and Twitter 16) to augment the training set. The PHEME, Twitter 15 and Twitter 16 datasets were chosen for augmentation as these are relatively large datasets annotated for rumour detection task, which is very similar to claim check-worthiness as described in section 2. Table <ref type="table" coords="4,206.83,644.10,4.98,8.80" target="#tab_0">1</ref> shows the number of tweets used in each of the datasets and the class distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>The CLEF dataset contains tweets related to the topic of COVID-19. They were annotated by the task organisers as either check-worthy or not check-worthy thus defining a binary classification task. 6 This dataset is rather small and is limited to individual tweets concerned with a single topic. The dataset is imbalanced with the majority of tweets being not check-worthy.</p><p>The PHEME dataset <ref type="bibr" coords="5,250.07,178.71,10.51,8.80" target="#b8">[9,</ref><ref type="bibr" coords="5,262.66,178.71,12.73,8.80" target="#b14">15]</ref> contains Twitter conversations discussing rumours (defined as unverified check-worthy claims spreading widely on social media) and non-rumours. <ref type="foot" coords="5,233.32,201.06,3.97,6.16" target="#foot_0">7</ref> This dataset contains conversations related to 9 major newsworthy events, such as shooting in Charlie Hebdo, shooting in Ottowa, crash of Germanwings plane. In this work, we use only the source tweets of the conversations in the PHEME dataset (they are conveying the essence of a rumour, rather than the following discussion) in order to have the same input structure as the CLEF dataset. We performed experiments augmenting the training set with both rumours and non-rumours from the PHEME dataset. We found that adding rumours only is more beneficial than adding the full PHEME dataset.</p><p>The Twitter 15 and Twitter 16 datasets <ref type="bibr" coords="5,329.67,298.26,15.49,8.80" target="#b15">[16]</ref> contain Twitter conversations, discussing True, False and Unverified rumours as well as non-rumours on various topics. Here, we do not use all 4-class labels, but instead convert True, False and Unverified classes into single check-worthy class. We also use only source tweets to augment the CLEF training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation</head><p>The CLEF dataset is split into training, development and testing sets. The check-worthiness task is evaluated as a ranking task, i.e. the participant systems should produce a list of tweets with the estimated score for check-worthiness. The official evaluation metric is Mean Average Precision (MAP), but the precisions at rank k (P @5, P @10, P @30) are also reported. Baseline results provided by the organisers are Random Classification (MAP = 0.35) and SVM with N -gram Prediction (MAP = 0.69).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach</head><p>In our approach, we fed a pre-trained word-level vector representation into a CNN model. Using vector representations of words as inputs offers high flexibility, allowing swaps between different pre-trained word vectors during model initialisation to be maintained without additional overhead. For our work, we tested multiple feature representations in order to assess which one would be best for our problem, these include combinations of frequency-based, vectorbased representations and authors' profiles. Since most evaluation tweets were not supplied with author's profiles, and frequency-based models may not be of 6 Annotation rules can be found here: https://github.com/sshaar/ clef2020-factchecking-task1#data-annotation-process great generalisability for unseen data, we reduced our exploration to two dynamic word embeddings, ELMo <ref type="bibr" coords="6,278.62,130.89,15.49,8.80" target="#b16">[17]</ref> and BERT <ref type="bibr" coords="6,349.44,130.89,14.61,8.80" target="#b17">[18]</ref>. Since the former did not provide any increase in performance, we chose the latter to be further explored with different pre-processing techniques. With these settings, we evaluated our model using two variations of the BERT pre-trained architecture, uncased BERT (uncased-BERT) and COVID-Twitter-BERT (CT-BERT) <ref type="bibr" coords="6,219.82,443.93,15.49,8.80" target="#b18">[19]</ref> (see Figure <ref type="figure" coords="6,286.90,443.93,3.87,8.80" target="#fig_0">1</ref>). While both are transformer-based models, CT-BERT is pre-trained on COVID-19 Twitter data using whole word-masked modelling and next sentence prediction.</p><p>In the following sections we describe the main characteristics of our designed system including pre-processing, feature representation, model architecture and hyperparameters used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pre-processing</head><p>We performed standard Twitter data pre-processing in order to improve our system performance. For each model, we implemented different pre-processing variants depending on the vector representation leading to best performance. The pre-processing steps can be summarised as follows.</p><p>(i) Segment2Token: We split each sentence into tokens considering the type of every segment and breaking it into individual tokens. Digits, URLs, accounts and hashtags were replaced by number , url , account , hashtag . respectively. This was implemented using a simple split function with different expression finding methods. By analysing generated segments, we settled on different treatment for special tokens in every tweet. For example, hyperlinks were either completely removed from the dataset or replaced by special tokens. Furthermore, digits and all other numerical expressions that contained '%' or '$' were either removed or tokenised. Tokenising numerical expressions allows the model to generalise better (see §5 (ii) Segment2Root: In NLP, the χ2 -statistical measure tests term-dependency of the tweet being about one of the classes as in <ref type="bibr" coords="7,364.89,312.54,14.61,8.80" target="#b19">[20]</ref>. We used it to analyse the segments of the tweets. In these settings, for few account handles and hashtags with high χ2 -score, we manually combined them depending on their semantic meaning. For instance:</p><formula xml:id="formula_1" coords="7,167.29,380.41,297.71,20.76">Hashtags: #coronavirus, #COVID19', #COVID-19, #COVID19, #Coronavirus, #Corona-virus</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hashtag2Root: coronavirus</head><p>In this case, different hashtags about COVID-19 were all consolidated under the 'coronavirus' umbrella term.</p><p>(iii) Word2id: As with all BERT models, we included classification embedding tokens for every tweet: [CLS] at the beginning and a separator token [SEP] at the end. We then decomposed t i into a sequence of numerical tokens using BERT tokenisation methods. This was done by mapping each token to a unique integer in the corpus' vocabulary.</p><p>(iv) Padding: We ensured that the input sequences in every batch was the same length. This was achieved through increasing the length of some of the sequences by adding more tokens. We tried to reduce the padding by allowing our model to decide the padding length based on a given batch size (set to 10) and the longest sequence within the given batch. For example, if the longest sequence length for a given batch is 20, then all other shorter sentences will be padded to match its length.</p><p>Finally, a look-up table was used for each token from the generated representation, ready to be fed into the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Hyperparameters</head><p>The CNN architecture requires tuning various hyperparameters. These include input representations, number of layers and filters, pooling, and activation functions. We utilised a BERT language model in accompaniment to the the general CNN architecture. Within the model design, we propose variations of a threelayer CNN with 32 filters with different window sizes: 2, 4 and 7. The multiple filters act as feature extractors.</p><p>Additionally, we used an Adam optimiser with learning rate fixed at 2e -5 and number of training epochs set to 8. Our N -gram kernels encompass a Rectified Linear Unit (ReLU) activation function, given by max(0, x). All pooling layers use a max-pooling operation. For the binary classification, we utilise a sigmoid activation function σ(x) for the output layer, defined as</p><formula xml:id="formula_2" coords="8,273.68,280.80,66.30,22.31">σ(x) = 1 1 + e -x</formula><p>To determine the final output labels, we classify check-worthiness based on the indicator variable</p><formula xml:id="formula_3" coords="8,152.49,339.56,104.35,51.46">h(x) =          1 if σ(x) ≥ 1 2</formula><p>, indicating a check-worthy tweet,</p><formula xml:id="formula_4" coords="8,193.82,373.90,63.03,22.31">0 if σ(x) &lt; 1 2</formula><p>, suppressing the tweet as non-check-worthy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>In the following section, we discuss the selection of the models we tried and ultimately submitted to the shared task. We also evaluate and compare their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Selection on the CLEF Development Set</head><p>We performed our model selection using the development set. Details of the preprocessing steps and embeddings applied to each of the eight models we tested are given in Table <ref type="table" coords="8,219.05,531.65,3.87,8.80" target="#tab_2">2</ref>. The performance of the models according to the various precision metrics are shown in Table <ref type="table" coords="8,297.14,543.60,3.87,8.80" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLEF Benchmark Data Experiments:</head><p>Text distortion has been used by <ref type="bibr" coords="8,134.77,584.33,14.61,8.80" target="#b20">[21]</ref>, where their methods were more successful than using the full text in the classification process. Taking inspiration from their work, we used tokenisation for different segments of the tweet where account handles, hashtags, URLs and digits were assigned special tokens and added to the model vocabulary. The goal of this step was to avoid over-fitting the training data. In Model 7, we experimented with ELMo embeddings, which gave the best performance in terms of MAP, in our tests without additional pre-processing and outperforms random baseline (MAP = 0.35). However, Model 7 did not outperform the N -gram baseline (MAP = 0.69), and thus we did not choose it for the test set submission.</p><p>In Model 4, we trained word embeddings on the training set along with the model and combined them with TF-IDF representations. This led to improvements over Models 5-7 and over the N -gram baseline.</p><p>For Model 1, we used intensive pre-processing in tandem with a CNN model with three filters of sizes 2, 4 and 7. On the other hand, in Model 2, only numeric expressions were tokenised the CNN model only had two filters of sizes 2 and 4. Models 1 and 2 displayed the best performance on the development set, and were hence chosen for submission on the testing set. Data Augmentation Experiments: While the task definition states the presence of the target topic when identifying tweet check-worthiness, the dataset provided only covers a single topic, COVID-19. We experimented with training data augmentation using check-worthy tweets from external datasets (PHEME and Twitter15, Twitter 16 as described in section 3.1) (see models 3, 5, 6 in Table <ref type="table" coords="10,162.36,178.71,3.87,8.80" target="#tab_2">2</ref>). These datasets cover different topics, accounts and vocabulary, so incorporating them could contribute to future generalisability of the model. We also performed experiments using only existing external datasets of rumours and non-rumours and omitting the CLEF training data to test the generalisability of the currently available datasets and models to the emergence of new rumour topics (see model 8 in Table <ref type="table" coords="10,260.48,238.49,3.87,8.80" target="#tab_2">2</ref>). The results are presented in Table <ref type="table" coords="10,427.85,238.49,3.87,8.80" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-precision (R = 59)</head><p>Precision@k @1 @3 @5 @10 @20 @50 1 0.81 0.71 1.00 As expected, Model 8, which did not use the CLEF training data, performed worse compared to the models that did make use of the training data provided. However, it outperformed the random baseline (MAP = 0.35) by 10%, showing that there is enough overlap in the task definitions and inherent nature of rumours/check-worthy claims to provide meaningful signal for model training. Models 3, 5 and 6, which augmented the training data, did not perform as well as the models using only the training data. Models 5 and 6 did not outperform the N -gram baseline (MAP = 0.69) provided by the organisers<ref type="foot" coords="11,390.37,117.38,3.97,6.16" target="#foot_1">8</ref> . Model 3 only adds rumours to the training data, thus shifting the class balance in the dataset, and performed better than adding both rumours and non-rumours from PHEME to the training set.</p><p>These results show the importance of model training or fine-tuning on the evaluation domain. The lack of performance improvement could be also due to the differences in the definitions of the tasks and rumours/check-worthy claims by each of the datasets. Moreover, different fact-checking organisations would naturally make different choices when analysing the same data. In <ref type="bibr" coords="11,443.24,214.64,9.96,8.80" target="#b6">[7]</ref>, they found that educational background can lead to bias in annotation efforts for fact-checking. The subjectiveness that underlies check-worthiness thereby adds further complications to the task of ranking by importance. These results also highlight the especially challenging aspects of the need for generalising to new unseen topics, as well as leveraging data from a related task such as that of rumour detection, in detecting tweet check-worthiness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on the CLEF Test</head><p>We selected the best three models based on MAP (see Table <ref type="table" coords="11,404.14,336.89,3.87,8.80" target="#tab_3">3</ref>). Table <ref type="table" coords="11,446.83,336.89,4.98,8.80" target="#tab_4">4</ref> shows the official results obtained by our systems on the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-precision (R = 59)</head><p>Precision@k @1 @3 @5 @10 @20 @50 1 0.71 0.63 Moreover, our approach uses a less complex model which keeps the weights small and results in better overall performance. Therefore, in order for our model to generalise to unseen tweets in the test set, numeric expressions should be unified and model complexity needs to be maintained.</p><p>Model 3: In this submission, we augmented the training data with rumours from the PHEME dataset. While the results of this submission on the development set were the lowest out of the selected three, on the testing set it outperforms Model 1. This shows that external data from a related task adds meaningful signal for model training and contributes to system generalisability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper describes our efforts as participants of the Task 1 of the CheckThat! 2020 evaluation lab, in which we ranked fourth, which was held in conjunction with the CLEF conference. We describe our proposed model that leverages the COVID-Twitter-19 BERT (CT-BERT) word embeddings and performs a special treatment for rare tokens with a CNN relying on the tweet alone.</p><p>The experimental results show that the performance of our model increases significantly by tokenising numerical expressions. The present work is restricted in choosing the best feature representation. In the future, this work can be enhanced in different possible directions. For example, incorporating pragmatic information related to author's profile information. Thus, simulate actual users' behaviour in verifying claims in social media.</p><p>Given the small size of the training data provided by the organisers, we also performed additional experiments leveraging external datasets with the aim of augmenting the training data. External data we incorporated had the challenge of being datasets pertaining to rumour detection and on different topics, hence with slight differences with the task and domain at hand. Our experiments with data augmentation did not lead to improved performance, highlighting that inclusion of external data of a different nature (i.e. in terms of task and domain) is particularly challenging and, if they can provide an improvement to the checkworthiness detection task, more careful integration and adaptation will be necessary.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,234.55,376.73,146.26,7.93;6,134.77,205.77,345.84,156.23"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. General Model Architecture.</figDesc><graphic coords="6,134.77,205.77,345.84,156.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,161.33,340.92,286.59,194.07"><head>Table 1 .</head><label>1</label><figDesc>Number of posts and class distribution in the datasets used</figDesc><table coords="4,161.33,340.92,283.49,171.40"><row><cell></cell><cell>No. of check-</cell><cell>No. of non-check-</cell><cell></cell></row><row><cell>Dataset</cell><cell>worthy tweets</cell><cell>worthy tweets</cell><cell>Total</cell></row><row><cell></cell><cell>(rumours)</cell><cell>(non-rumours)</cell><cell></cell></row><row><cell>CLEF Train</cell><cell>231</cell><cell>441</cell><cell>672</cell></row><row><cell>CLEF Development</cell><cell>59</cell><cell>91</cell><cell>150</cell></row><row><cell>CLEF Test</cell><cell>80</cell><cell>60</cell><cell>140</cell></row><row><cell>PHEME</cell><cell>2402</cell><cell>4023</cell><cell>6425</cell></row><row><cell>Twitter 15</cell><cell>1012</cell><cell>362</cell><cell>1374</cell></row><row><cell>Twitter 16</cell><cell>536</cell><cell>199</cell><cell>735</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,167.29,166.75,297.71,101.00"><head></head><label></label><figDesc>). For example:</figDesc><table coords="7,167.29,194.92,297.71,72.84"><row><cell>Tweet: [N EW S] Naver #BAEKHYUN EXO Baekhyun donates 50</cell></row><row><cell>million won to prevent the spread of Corona 19 @weareoneEXO</cell></row><row><cell>#EXO</cell></row><row><cell>Segment2Token: [N EW S] Naver hashtag EXO Baekhyun do-</cell></row><row><cell>nates number won to prevent the spread of Corona 19 account</cell></row><row><cell>hashtag .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,136.56,298.73,342.23,319.22"><head>Table 2 .</head><label>2</label><figDesc>Description of the models tested.</figDesc><table coords="9,136.56,298.73,342.23,296.04"><row><cell>Model No.</cell><cell>Pre-processing</cell><cell>Embeddings</cell></row><row><cell>1</cell><cell>Frequently mentioned entities replaced with</cell><cell>CT-BERT</cell></row><row><cell></cell><cell>their account name. Hashtags with repeated top-</cell><cell></cell></row><row><cell></cell><cell>ics combined using the χ2 -score, other URLs and</cell><cell></cell></row><row><cell></cell><cell>hashtags tokenised with special tokens.</cell><cell></cell></row><row><cell>2</cell><cell>Special tokens for digits. Account handles, URLs</cell><cell>CT-BERT</cell></row><row><cell></cell><cell>and hashtags removed.</cell><cell></cell></row><row><cell>3</cell><cell>Training set merged with rumours from</cell><cell>BERT-EN</cell></row><row><cell></cell><cell>PHEME. Special tokens for digits.</cell><cell>(Uncased)</cell></row><row><cell>4</cell><cell>Digits, account handles, URLs and hashtags re-</cell><cell>CLEF Train Embeddings</cell></row><row><cell></cell><cell>moved.</cell><cell>+ TF-IDF</cell></row><row><cell>5</cell><cell>Training set merged with Twitter 15 and Twitter</cell><cell>BERT-EN</cell></row><row><cell></cell><cell>16 datasets. Special tokens for digits.</cell><cell>(Uncased)</cell></row><row><cell>6</cell><cell>Training set merged with PHEME, Twitter 15</cell><cell>BERT-EN</cell></row><row><cell></cell><cell>and Twitter 16 datasets. Special tokens for dig-</cell><cell>(Uncased)</cell></row><row><cell></cell><cell>its.</cell><cell></cell></row><row><cell>7</cell><cell>No pre-processing was applied.</cell><cell>ELMo</cell></row><row><cell>8</cell><cell>Trained using PHEME, Twitter 15 and Twitter</cell><cell>BERT-EN</cell></row><row><cell></cell><cell>16 datasets only, without CLEF training data.</cell><cell>(Uncased)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,146.80,334.49,322.61,192.46"><head>Table 3 .</head><label>3</label><figDesc>Model performance on the development set</figDesc><table coords="10,299.62,334.49,169.79,7.92"><row><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>0.95</cell><cell>0.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,134.77,444.49,345.83,191.73"><head>Table 4 .</head><label>4</label><figDesc>Performance of the submitted models on the testing set Tweet: France, Spain and Germany are about 9 to 10 days behind Italy in #COV ID19 progression; the UK and the US follow at 13 to 16 days.Digit2Token: France, Spain and Germany are about number to number days behind Italy in corona virus progression; the UK and the US follow at number to number days.</figDesc><table coords="11,134.77,444.49,345.83,191.73"><row><cell></cell><cell></cell><cell></cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>0.90</cell><cell>0.80</cell><cell>0.64</cell></row><row><cell>2</cell><cell>0.78</cell><cell>0.70</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>0.85</cell><cell>0.70</cell></row><row><cell>3</cell><cell>0.73</cell><cell>0.63</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>0.90</cell><cell>0.85</cell><cell>0.68</cell></row><row><cell cols="9">Models 1 and 2: We found that avoiding extensive tokenisation (Model 1)</cell></row><row><cell cols="9">while merely tokenising numeric expressions yields better results in the test set</cell></row><row><cell cols="9">and allows the model to learn more general patterns from the training set (Model</cell></row><row><cell cols="2">2). For example:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_0" coords="5,144.73,646.48,324.73,7.47;5,144.73,657.44,145.89,7.47"><p>https://figshare.com/articles/PHEME_dataset_for_Rumour_Detection_and_ Veracity_Classification/6392078</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_1" coords="11,144.73,657.44,292.78,7.47"><p>https://github.com/sshaar/clef2020-factchecking-task1#baseline</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgments</head><p>This research utilised Queen Mary's <rs type="institution">Apocrita HPC facility</rs>, supported by <rs type="funder">QMUL Research</rs>-IT. http://doi.org/10.5281/zenodo.<rs type="grantNumber">438045</rs>. This work was also partially supported by <rs type="funder">The Alan Turing Institute</rs> under the <rs type="funder">EPSRC</rs> grant <rs type="grantNumber">EP/N510-129/1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BsNsMR4">
					<idno type="grant-number">438045</idno>
				</org>
				<org type="funding" xml:id="_NHvtDwE">
					<idno type="grant-number">EP/N510-129/1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.95,225.97,337.64,7.92;13,151.52,236.93,329.07,7.92;13,151.52,247.89,161.48,7.92" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,286.23,225.97,194.36,7.92;13,151.52,236.93,232.65,7.92">Online health information seeking: the influence of age, information trustworthiness, and search challenges</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">M S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,395.97,236.93,84.61,7.92;13,151.52,247.89,22.15,7.92">Journal of aging and health</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="525" to="541" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.95,258.22,337.64,7.92;13,151.52,269.18,65.52,7.92" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,195.08,258.22,137.59,7.92">Online social media in crisis events</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Palen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,344.10,258.22,75.01,7.92">Educause quarterly</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="76" to="78" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.95,279.50,337.63,7.92;13,151.52,290.46,329.08,7.92;13,151.52,301.42,298.95,7.92" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,237.33,290.46,243.27,7.92;13,151.52,301.42,19.42,7.92">Misinformation of covid-19 on the internet: infodemiology study</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Cuan-Baltazar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Muñoz-Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Robledo-Vega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Pérez-Zepeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Soto-Vega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,182.54,301.42,143.32,7.92">JMIR public health and surveillance</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">18444</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.95,311.75,337.64,7.92;13,151.52,322.71,109.92,7.92" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="13,260.86,311.75,148.46,7.92">The State of Automated Factchecking</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Babakar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Moy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>UK, Tech. Rep</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">London</note>
</biblStruct>

<biblStruct coords="13,142.95,333.03,337.64,7.92;13,151.52,343.99,329.07,7.92;13,151.52,354.95,329.07,7.92;13,151.52,365.91,68.07,7.92" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,329.66,343.99,150.93,7.92;13,151.52,354.95,80.54,7.92">ClaimBuster: the first-ever end-to-end fact-checking system</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Caraballo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gawsane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Nayak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,244.59,354.95,150.66,7.92">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1945" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.95,376.24,337.63,7.92;13,151.52,387.20,329.07,7.92;13,151.52,398.16,329.07,7.92;13,151.52,409.11,193.26,7.92" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,440.72,376.24,39.86,7.92;13,151.52,387.20,288.25,7.92">A contextaware approach for detecting worth-checking claims in political debates</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gencheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Koychev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,462.94,387.20,17.64,7.92;13,151.52,398.16,329.07,7.92;13,151.52,409.11,40.18,7.92">Proceedings of the International Conference Recent Advances in Natural Language Processing</title>
		<meeting>the International Conference Recent Advances in Natural Language Processing</meeting>
		<imprint>
			<publisher>INCOMA Ltd</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.95,419.44,337.64,7.92;13,151.52,430.40,329.07,7.92;13,151.52,441.36,313.59,7.92" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,402.45,419.44,78.14,7.92;13,151.52,430.40,329.07,7.92;13,151.52,441.36,95.68,7.92">Towards automated factchecking: Developing an annotation schema and benchmark for consistent automated claim detection</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Konstantinovskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Babakar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,258.76,441.36,178.41,7.92">ACM Digital Threats: Research and Practice</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.95,451.69,337.64,7.92;13,151.52,462.65,329.07,7.92;13,151.52,473.60,266.23,7.92" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,444.40,451.69,36.19,7.92;13,151.52,462.65,329.07,7.92;13,151.52,473.60,123.03,7.92">Overview of the clef-2019 checkthat! lab: Automatic identification and verification of claims. task 2: Evidence and factuality</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,296.30,473.60,93.27,7.92">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.95,483.93,337.63,7.92;13,151.52,494.89,329.07,7.92;13,151.52,505.85,118.74,7.92" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,409.35,483.93,71.24,7.92;13,151.52,494.89,176.62,7.92">Detection and resolution of rumours in social media: A survey</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Procter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,339.62,494.89,135.92,7.92">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,516.18,337.98,7.92;13,151.52,527.13,329.07,7.92;13,151.52,538.09,329.07,7.92;13,151.52,549.05,163.50,7.92" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,218.90,527.13,261.69,7.92;13,151.52,538.09,98.00,7.92">Semeval-2019 task 7: Rumoureval, determining rumour veracity and support for rumours</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gorrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Derczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,273.80,538.09,206.79,7.92;13,151.52,549.05,81.57,7.92">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>the 13th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,559.38,337.98,7.92;13,151.52,570.34,329.07,7.92;13,151.52,581.30,74.73,7.92" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,320.79,559.38,159.81,7.92;13,151.52,570.34,59.36,7.92">Exploiting context for rumour detection in social media</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Procter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,235.98,570.34,191.92,7.92">International Conference on Social Informatics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="109" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,591.62,337.98,7.92;13,151.52,602.58,329.07,7.92;13,151.52,613.54,163.53,7.92" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,286.09,591.62,194.51,7.92;13,151.52,602.58,116.69,7.92">Enquiring minds: Early detection of rumors in social media from enquiry posts</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,289.71,602.58,190.88,7.92;13,151.52,613.54,72.24,7.92">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1395" to="1405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,623.87,337.97,7.92;13,151.52,634.83,329.07,7.92;13,151.52,645.79,329.07,7.92;13,151.52,656.74,226.29,7.92" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,281.38,634.83,199.20,7.92;13,151.52,645.79,220.00,7.92">Checkthat! at clef 2020: Enabling the automatic identification and verification of claims in social media</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,395.42,645.79,85.17,7.92;13,151.52,656.74,99.06,7.92">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="499" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,119.62,337.97,7.92;14,151.52,130.58,329.07,7.92;14,151.52,141.54,329.08,7.92;14,151.52,152.50,329.07,7.92;14,151.52,163.46,329.08,7.92;14,151.52,174.42,144.48,7.92" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,214.58,141.54,266.01,7.92;14,151.52,152.50,122.15,7.92">Overview of CheckThat! 2020: Automatic identification and verification of claims in social media</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hamdan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">Sheikh</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,285.69,152.50,41.09,7.92">ser. LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,185.37,337.98,7.92;14,151.52,196.33,329.07,7.92;14,151.52,207.29,176.48,7.92" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,329.35,185.37,151.24,7.92;14,151.52,196.33,67.70,7.92">All-in-one: Multi-task learning for rumour verification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,242.19,196.33,238.40,7.92;14,151.52,207.29,85.63,7.92">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3402" to="3413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,218.25,337.98,7.92;14,151.52,229.21,329.07,7.92;14,151.52,240.17,329.07,7.92;14,151.52,251.13,74.73,7.92" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,295.72,218.25,184.87,7.92;14,151.52,229.21,145.81,7.92">Detect rumors in microblog posts using propagation structure via kernel learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-F</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,319.98,229.21,160.61,7.92;14,151.52,240.17,195.20,7.92">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s" coord="14,397.34,240.17,47.59,7.92">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="708" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,262.09,337.98,7.92;14,151.52,273.05,329.07,7.92;14,151.52,284.00,329.08,7.92;14,151.52,294.96,329.07,7.92;14,151.52,305.92,44.02,7.92" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,187.74,273.05,168.87,7.92">Deep contextualized word representations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,382.17,273.05,98.42,7.92;14,151.52,284.00,329.08,7.92;14,151.52,294.96,174.62,7.92">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="14,142.61,316.88,337.98,7.92;14,151.52,327.84,329.07,7.92;14,151.52,338.80,329.08,7.92;14,151.52,349.76,329.07,7.92;14,151.52,360.72,83.94,7.92" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,375.27,316.88,105.32,7.92;14,151.52,327.84,213.10,7.92">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,387.15,327.84,93.44,7.92;14,151.52,338.80,329.08,7.92;14,151.52,349.76,173.76,7.92">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="14,142.61,371.68,337.98,7.92;14,151.52,382.63,329.07,7.92;14,151.52,393.59,97.06,7.92" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="14,334.63,371.68,145.96,7.92;14,151.52,382.63,259.88,7.92">Covid-Twitter-BERT: A natural language processing model to analyse COVID-19 content on Twitter</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Salathé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kummervold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07503</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.61,404.55,337.98,7.92;14,151.52,415.51,329.08,7.92;14,151.52,426.47,183.82,7.92" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,349.67,404.55,130.92,7.92;14,151.52,415.51,202.97,7.92">Crisislex: A lexicon for collecting and filtering microblogged communications in crises</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Olteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vieweg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,374.03,415.51,106.56,7.92;14,151.52,426.47,155.43,7.92">Eighth international AAAI conference on weblogs and social media</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,437.43,337.98,7.92;14,151.52,448.39,329.07,7.92;14,151.52,459.35,20.99,7.92" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,398.63,437.43,81.97,7.92;14,151.52,448.39,277.27,7.92">Upv-inaoe-autoritascheck that: Preliminary approach for checking worthiness of claims</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,453.43,448.39,21.73,7.92">CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
