<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.74,152.79,293.84,12.64;1,257.57,170.79,80.21,12.64">UAICS at CheckThat! 2020: Fact-checking claim prioritization</title>
				<funder ref="#_3eJvCK3 #_sJsPgtA #_ZTJphT6">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.86,209.94,107.06,8.96"><forename type="first">Ciprian-Gabriel</forename><surname>Cusmuliuc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Cuza&quot; University</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,280.13,209.94,88.55,8.96"><forename type="first">Lucia-Georgiana</forename><surname>Coca</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Cuza&quot; University</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.91,209.94,53.59,8.96"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Cuza&quot; University</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,157.10,231.74,56.05,8.18"><forename type="first">Alexandru</forename><surname>Ioan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Cuza&quot; University</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.74,152.79,293.84,12.64;1,257.57,170.79,80.21,12.64">UAICS at CheckThat! 2020: Fact-checking claim prioritization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">80A721E05A0591B5359BAFCDC179E523</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Naive Bayes</term>
					<term>BERT</term>
					<term>Logistic Regression</term>
					<term>Decision Tree</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Claim proving can be an incredibly challenging task considering the amount of information in the world increases day by day. Journalists and people alike spend a lot of time investigating claims and fact-checking different statements. In order to address this problem CLEF 2020 CheckThat! proposes 5 tasks that each present a different side of the problem. Our team participated in Task 1 and Task 5 which aim to rank statements by check-worthiness. For Task 5, we proposed 3 methods, each based on a different machine learning algorithms, Naïve Bayes, Logistic Regression, and Decision Tree. For Task 1, we created a system based on BERT. For Task 5, the best result we achieved using the official measure MAP was with the Naive Bayes. This paper presents the details and results of our approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Increase in social network popularity has led users to conduct multiple activities on them, such as message exchanging, posting, news reading, commenting, and so on. Instant sharing and broadcasting have enabled users fast and a vast access to information, but all with a cost. The problem arises that news propagation in these platforms is becoming uncontrollable, users frequently read and share information without checking the veracity of a certain claim, leading to misinformation.</p><p>The problem of needing to check the news and claims in these networks is slowly becoming of extremely high interest to major players, such as Facebook and Twitter, as recently they are having talks of integrating such tools in their systems, one such example is Twitter fact checking Donald Trump and labeling his tweets as 'manipulated media' 1 sparking outrage amongst its supporters. This problem is not entirely present in social media, we can see an effort to spread misinformation and propaganda on the entire internet.</p><p>CLEF 2020 CheckThat! <ref type="bibr" coords="2,234.41,174.42,12.11,8.96" target="#b0">[1]</ref>[19] is an evaluation campaign that is being organized as part of CLEF 2020 <ref type="bibr" coords="2,205.90,186.42,11.69,8.96" target="#b1">[2]</ref> and contains 5 tasks, each related to fact-checking. Our team participated in two tasks, Tasks 1 and 5.</p><p>Task 1 requires the development of a system capable of ranking a stream of potentially-related tweets according to their check-worthiness; this task ran in English and Arabic, we participated only in the English version by developing a model based on BERT <ref type="bibr" coords="2,153.35,246.42,10.71,8.96" target="#b2">[3]</ref>, a bidirectional transformer developed by Google with exceptional performance.</p><p>Task 5 has the objective of identifying which sentences from a political debate should be prioritized for fact-checking. In this task, we submitted 3 models based on Naive Bayes, Logistic Regression and Decision Tree.</p><p>This paper describes the participation of team UAICS, from the Faculty of Computer Science, "Alexandru Ioan Cuza" University of Iasi, in Task 1 and 5 at CLEF 2020. The remaining of this paper was organized as follows: Section 2 describes the state of the art, Section 3 gives a description of the tasks. Section 4 details the model we developed and the submitted runs and then Section 5 details the results we obtained, finally Section 5 concludes this paper and presents future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State of the art</head><p>Previous editions of CheckThat! i.e. 2019 and 2018 had fewer tasks; Task 1 was based on the same claim prioritization as task 5 whilst task 2 required to assess which web pages can be useful in human fact-checking and consisted of multiple subtasks. We will only be referring to task 1 as this task is also present in 2020.</p><p>In 2019 approaches for task 1 were very various, the best team "Copenhagen" [4] had a MAP score of .1660 and the system was based on learning dual token embeddings in conjunction with an LSTM <ref type="bibr" coords="2,251.31,495.47,10.70,8.96" target="#b4">[5]</ref>. The network has been pre-trained using previous Trump and Clinton debates while supervising it with ClaimBuster<ref type="foot" coords="2,391.51,506.36,3.24,5.83" target="#foot_0">2</ref> . Other approaches are the following (in order of the ranking): team TheEarthIsFlat <ref type="bibr" coords="2,377.83,519.47,11.69,8.96" target="#b5">[6]</ref> used a feed-forward neural network with two hidden layers, team IPIPAN <ref type="bibr" coords="2,347.35,531.47,11.69,8.96" target="#b6">[7]</ref> used an L1-regularized logistic regression, team Terrier <ref type="bibr" coords="2,246.82,543.47,11.69,8.96" target="#b7">[8]</ref> used SVM <ref type="bibr" coords="2,305.69,543.47,11.69,8.96" target="#b8">[9]</ref> in conjunction with bag-of-words and named entities and team UAICS <ref type="bibr" coords="2,256.92,555.47,16.72,8.96" target="#b9">[10]</ref> used a Naïve Bayes classifier with bag-of-words features.</p><p>In 2018 the best team was still "Copenhagen" <ref type="bibr" coords="2,324.92,579.37,16.61,9.05" target="#b10">[11]</ref> with the lowest MAE of .7050, they used a similar approach as in 2019 being a convolutional neural network <ref type="bibr" coords="2,437.26,591.47,16.61,8.96" target="#b11">[12]</ref> and support vector machine. Other approaches range from random forests, logistic regression, and LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tasks description</head><p>In 2020 there have been 5 tasks that ran in English and Arabic, we only participated in 2 of them, Tasks 1 and 5. In this section, we will shortly present the two tasks we took part in.</p><p>Task 1 requires "given a topic and a stream of potentially-related tweets, rank the tweets according to their check-worthiness for the topic". This task runs in English and Arabic.</p><p>Task 5 requires "given a political debate or a transcribed speech, segmented into sentences, with speakers annotated, identify which sentence should be prioritized for fact-checking". This task is only in English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation metric</head><p>Both tasks use MAP [13] as the official metric which calculates the usual mean of the average precision. Other measures used are the Mean Reciprocal Rank <ref type="bibr" coords="3,413.05,335.49,16.61,8.96" target="#b13">[14]</ref> which allows obtaining reciprocals of the rank of the first relevant document, as well as Mean Precision at k, which performs the average of k best candidates. Details on the measures used can be found in the task overview <ref type="bibr" coords="3,283.13,371.49,10.58,8.96" target="#b0">[1]</ref>. Evaluations are carried out on primary and contrastive runs. Each participant has the right to three models, one primary and two secondary (contrastive). We tried to take advantage of this by submitting 3 models in Task 5.</p><p>In previous years at CheckThat! the evaluation metric was MAE, as it could have been seen in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>Methods and runs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1</head><p>For Task 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Training and test data</head><p>The data provided for this task contained tweets that were split into 2 main categories, train, and dev. The data was provided in both TSV <ref type="foot" coords="3,333.91,543.44,3.24,5.83" target="#foot_1">3</ref> and JSON<ref type="foot" coords="3,381.91,543.44,3.24,5.83" target="#foot_2">4</ref> files. We decided to only use the TSV files as we felt it was easier. The datasets used were: "train" for the training of our model and "dev" to fine-tune the hyperparameters after evaluation. A training example can be seen in Table <ref type="table" coords="3,439.79,580.55,3.76,8.96" target="#tab_0">1</ref>. We considered that the tweet URL and tweet id were irrelevant, so we did not include it in the data sent to our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Preprocessing and feature extraction</head><p>Before feeding the data to the model, we had to preprocess the text. Csv<ref type="foot" coords="4,419.23,323.10,3.24,5.83" target="#foot_3">5</ref> library was used to read from the files provided by the organizer after which we put the data in a list that contained in order the topic id, tweet id, tweet URL, tweet text, claim and label.</p><p>The data would then be sent to a tokenizer, we decided to use BertTokenizer<ref type="foot" coords="4,440.14,359.10,3.24,5.83" target="#foot_4">6</ref> as this was the official method from Huggingface<ref type="foot" coords="4,294.05,371.10,3.24,5.83" target="#foot_5">7</ref> . We would then pad the sentence to a maximum phrase limit that in our case was 121. Example tokenization is the following: After tokenization we loaded each individual field in a torch tensor <ref type="foot" coords="4,414.43,479.96,3.24,5.83" target="#foot_6">8</ref> and inserted them in a TensorDataset<ref type="foot" coords="4,221.57,491.96,3.24,5.83" target="#foot_7">9</ref> that contained: all the sentence ids, each individual tokenized sentence and the labels. A code snippet for this operation is the following: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>In designing the model, we decided to use BERT as a possible solution to the problem at hand. This model was chosen over other language models such as ULMFiT <ref type="bibr" coords="5,442.67,182.46,16.72,8.96" target="#b14">[15]</ref> as multiple papers demonstrate the performance benefits BERT has. For example, <ref type="bibr" coords="5,453.66,194.46,17.04,8.96" target="#b15">[16]</ref> trained an RNN on a very large text collection resulting in a 63.7% accuracy in the Winograd Schema Challenge <ref type="bibr" coords="5,244.01,218.46,16.76,8.96" target="#b16">[17]</ref> while <ref type="bibr" coords="5,287.79,218.46,16.66,8.96" target="#b17">[18]</ref> using a BERT model was able to achieve an accuracy of 72.5% on the same challenge. These results led us to choose the latter model as we feel it best fits our purpose.</p><p>The system consists of a pre-trained model called "bert-large-uncased" 10 , it is a bidirectional transformer that contains 24 layers, 1,024 hidden layers, 16 heads, and 340 million parameters. The training has been done on lower-cased English text.</p><p>We used a combination of BertModel 11 and Adam 12 optimizer in order to get the best results. The hyperparameters are more or less standard, we tuned them empirically and arrived at the following best configuration: batch size 8, 5 epochs, and the Adam learning rate of 5e-5.</p><p>The pipeline of the algorithm implies first preprocessing (tokenize and pad the sentence in order to satisfy the condition of the model). Then, we shuffle the data in order to avoid overfitting, and we start the training with each epoch and we feed the data through the network. Then with backpropagation, we simply update the learning rate and tell the optimizer to update the parameters.</p><p>Evaluation of the trained network is done with the dev dataset, using the saved model in the previous step we feed the data through the network and compute loss on our trial data.</p><p>The experimental setup was done both locally and on the cloud. In the development stage, we trained the model locally using a computer with a 12 core CPU and 32 Gb of Ram which proved very inefficient, the training time took about two days which made us switch to a cloud setup. Using PyTorch 13 we made the switch to training the model on GPU using the Google Collaboratory 14 platform which lowered the training time to about an hour, this made a big difference as now we could make decisions regarding the model much faster, without waiting a long time for it to finish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2</head><p>For Task 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Training and test data</head><p>The data provided contained presidential elections debates and speeches from the United States in 2016. The data was of two main categories, training and test. The training had 50 files while the test had 20 files. The main difference from 2019 is that the organizers provided more training files but also more test scenarios. This can be seen We tried to further augment the models by taking files from 2019 that have not been included in 2020; we took training files but also test files with gold labels.</p><p>One training example with the available columns would be the following: In the training of the model we ignored the and line number, we only fed the preprocessed text and label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Preprocessing and feature extraction</head><p>Before sending the debate text to the machine learning algorithm we performed several preprocessing operations in a pipeline. For all the models we first tokenized the text in order to break the phrase into individual terms. After tokenization, for the contrastive 2 submissions, Logistic Regression we used TF-IDF in order to extract the features in the form of a term frequency matrix. The implementation for TF-IDF was taken from Pyspark 15 and is a combination of 2 steps: HashingTF 16 and IDF 17 . The minimum document frequency for IDF was set to 10.</p><p>For the next two submissions, primary (Naïve Bayes) and contrastive 2 (Decision Tree) we decided to also use a term frequency matrix but with a different implementation; instead of using HashingTF we used CountVectorizer 18 which has a lower information loss and empirically it was observed these two algorithms perform better with this feature extractor. The settings of the CountVectorizer are the following: minimum term frequency is 1 and so is the minimum definition frequency, the maximum definition frequency is 2 63 -1 and the vocabulary size is 2 18 and for IDF we set a minimum term frequency of 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Models</head><p>After preprocessing the data and extracting the features they are sent to the machine learning models. We decided not to use any exterior resources for these models. We trained the algorithms and tested them using the test data from 2019 where we attached the gold labels and used the provided organizers script to calculate MAP, RR, 15 https://spark.apache.org/docs/latest/api/python/index.html 16 https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/mllib/feature/HashingTF.html 17 https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/mllib/feature/IDF.html 18 https://spark.apache.org/docs/2.1.0/ml-features.html#countvectorizer R-P and P@k. This gave us a rough estimation of the performance of the model but also a way to compare with other teams from last year <ref type="bibr" coords="7,343.99,162.42,15.34,8.96" target="#b9">[10]</ref>.</p><p>The first and best model was based on Naïve Bayes which uses the default implementation from Pyspark, we fine-tuned it after multiple sessions of testing and used a multinomial model and set the smoothing to 1. Even though the model is quite simple it is very powerful. It can be seen from the results in Table <ref type="table" coords="7,366.03,210.42,4.98,8.96" target="#tab_3">4</ref> sometimes it is twice as accurate as other algorithms.</p><p>The second-best algorithm was Logistic Regression, initially this model performed poorly however we figured out that by increasing the minimum document frequency on IDF to 10 would increase its performance. The parameters of this model are the following: maximum iteration is set to 100, the regression parameter is 0, the tolerance value was 1e-6 and the aggregation depth was set to 2.</p><p>The third best was Decision Tree; this model uses a minimum document frequency in IDF of 3. We tried making it as best as we could and in order not to overfit the model we arrived at the following parameters: maximum depth was 30, we increased the maximum bins to 128 which allows the algorithm to consider more split candidates and make fine-grained split decisions, there were a minimum 5 instances per node and increased the maximum memory limit of the model to 4096Mb.</p><p>We trained the models locally, on CPU, the training time was rather fast, Decision Tree was the slowest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In this section, the results of the two task submissions will be discussed. Table <ref type="table" coords="7,442.66,435.47,4.98,8.96">3</ref> illustrates the results for Task 1, there were 12 teams and we ranked 11 th with a MAP of 0.4950 while the best result had a MAP of 0.8064 (team Accenture). It should be noted here that contrastive 1 is better than our primary, we trained the primary with more epochs and wrongly evaluated as an increase in performance. Table <ref type="table" coords="7,161.06,495.47,4.98,8.96" target="#tab_3">4</ref> presents the result in Task 5 where we ranked 2 nd out of 3 teams with a MAP of 0.0515, the best being 0.0867 (team NLPIR01), and the worst 0.0183, almost 5 times worse than our submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3. Task 1 Results</head><p>Sub. MAP RR R-P P@1 P@3 P@5 P@10 P@20 P@30 Accenture 0.8064 1.00 0.7167 1.0000 1.0000 Sub. MAP RR R-P P@1 P@3 P@5 P@10 P@20 P@30 NLPI R01 For Task 5 the best result and the primary submission was of the Naïve Bayes model, contrastive 1 is 2 nd place and it is the Logistic Regression algorithm, and finally contrastive 2 is based on a Decision Tree. The results are in accordance with what we have tested locally, we feel that the performance is good and that the models performed well in the evaluation stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Error analysis</head><p>The performance of both tasks is good; for Task 1 the main drawback of the model is the fact that we did not arrive at a finished product, we feel that the design of the model needs improvement, we do not believe it is able to extract relevant information thus an augmentation with a general knowledge ontology such as WikiData <ref type="foot" coords="8,395.47,388.98,6.48,5.83" target="#foot_8">19</ref> would be a great addition.</p><p>For Task 5, the models are in a much more mature state as the performance show, we feel that they have reached their limit, the error stems from the lack of understanding of the sentence thus needing a much more complex system, probably based on a language model such as BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed solutions for two of CLEF CheckThat! 2020 tasks, one approach is based on a bidirectional transformer and the others are based on machine learning. We achieved good results with all the submission and for future we would like to fine-tune our models in order to have a much better MAP, there is much room for improvement in Task 1 and for task 5 a language model approach would be interesting to see in action.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,243.41,458.08,108.59,8.10"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Tokenization example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,124.70,526.74,312.02,6.44;4,124.70,538.74,81.62,6.44;4,124.70,550.74,331.22,6.44;4,124.70,562.74,81.62,6.44;4,124.70,574.74,283.22,6.44;4,124.70,586.74,81.62,6.44;4,124.70,598.74,345.62,6.44;4,124.70,610.74,67.22,6.44;5,124.70,150.54,20.06,8.96"><head></head><label></label><figDesc>all_topic_id_id = torch.tensor([f.topic_id_id for f in features], dtype=torch.long) all_tweet_text_id = torch.tensor([f.tweet_text_id for f in features], dtype=torch.long) all_claim_id = torch.tensor([f.claim_id for f in features], dtype=torch.long) dataset = TensorDataset(all_topic_id_id,all_tweet_text_id, all_claim_id) return dataset 4.1.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,127.58,150.42,340.27,95.24"><head>Table 1 .</head><label>1</label><figDesc>Training example.</figDesc><table coords="4,127.58,163.98,340.27,81.68"><row><cell>Topic id Tweet id</cell><cell>Tweet URL</cell><cell>Tweet Text</cell><cell cols="2">Claim Check wor-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>thiness</cell></row><row><cell></cell><cell>https://twit-</cell><cell>Since this will</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell>ter.com/Eric-</cell><cell>never get reported</cell><cell></cell></row><row><cell>covid-19 1234964</cell><cell>Trump/sta-</cell><cell>by the media […]</cell><cell></cell></row><row><cell></cell><cell>tus/12349646530143</cell><cell></cell><cell></cell></row><row><cell></cell><cell>84644</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,143.30,222.35,289.34,41.46"><head>Table 2 .</head><label>2</label><figDesc>Training example.</figDesc><table coords="6,143.30,240.90,289.34,22.91"><row><cell>Line no.</cell><cell>Speaker</cell><cell>Text</cell><cell>Label</cell></row><row><cell>1</cell><cell>Trump</cell><cell>So Ford is leaving.</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,131.78,574.79,328.75,57.35"><head>Table 4 .</head><label>4</label><figDesc>Task 5 Results    </figDesc><table coords="7,333.79,574.79,126.74,8.96"><row><cell>1.00 1.0000 0.9500 0.7400</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,129.98,686.59,112.98,8.10"><p>https://idir.uta.edu/claimbuster/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,129.98,675.55,152.21,8.10"><p>https://www.imf.org/external/help/tsv.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="3,129.98,686.59,123.72,8.10"><p>https://www.json.org/json-en.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="4,129.98,642.55,149.65,8.10"><p>https://docs.python.org/3/library/csv.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="4,129.98,653.59,255.55,8.10"><p>https://huggingface.co/transformers/model doc/bert.html#berttokenizer</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="4,129.98,664.51,82.72,8.10"><p>https://huggingface.co/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="4,129.98,675.55,155.92,8.10"><p>https://pytorch.org/docs/stable/tensors.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="4,129.98,686.59,257.47,8.10"><p>https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_8" coords="8,136.10,686.59,94.49,8.10"><p>https://www.wikidata.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by project <rs type="projectName">REVERT</rs> (<rs type="projectName">taRgeted thErapy for adVanced colo-rEctal canceR paTients</rs>), Grant Agreement number: <rs type="grantNumber">848098</rs>, <rs type="grantNumber">H2020-SC1-BHC-2018-2020/H2020-SC1-2019-Two-Stage-RTD</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_3eJvCK3">
					<orgName type="project" subtype="full">REVERT</orgName>
				</org>
				<org type="funded-project" xml:id="_sJsPgtA">
					<idno type="grant-number">848098</idno>
					<orgName type="project" subtype="full">taRgeted thErapy for adVanced colo-rEctal canceR paTients</orgName>
				</org>
				<org type="funding" xml:id="_ZTJphT6">
					<idno type="grant-number">H2020-SC1-BHC-2018-2020/H2020-SC1-2019-Two-Stage-RTD</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,132.67,177.35,337.99,8.10;9,141.74,188.39,328.91,8.10;9,141.74,199.31,329.05,8.10;9,141.74,210.35,164.25,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,141.74,199.31,329.05,8.10;9,141.74,210.35,36.31,8.10">Overview of CheckThat! 2020: Automatic Identification and Verification of Claims in Social Media</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hamdan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Sheikh Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,194.29,210.35,89.17,8.10">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,221.39,337.76,8.10;9,141.74,232.31,328.87,8.10;9,141.74,243.35,328.78,8.10;9,141.74,254.39,328.64,8.10;9,141.74,265.34,115.69,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,306.49,232.31,164.11,8.10;9,141.74,243.35,95.19,8.10">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,252.89,243.35,217.63,8.10;9,141.74,254.39,121.66,8.10">Proceedings of the Eleventh International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="9,271.59,254.39,169.80,8.10">Lecture Notes in Computer Science (LNCS)</title>
		<meeting>the Eleventh International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,276.38,338.09,8.10;9,141.74,287.42,271.10,8.10" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,351.19,276.38,119.56,8.10;9,141.74,287.42,178.43,8.10">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,298.34,337.97,8.10;9,141.74,309.38,328.89,8.10;9,141.74,320.42,328.79,8.10;9,141.74,331.34,238.10,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,356.23,298.34,114.41,8.10;9,141.74,309.38,280.58,8.10">Neural Weakly Supervised Fact Check-Worthiness Detection with Contrastive Sampling-Based Ranking Loss</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/paper_56.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,438.89,309.38,31.73,8.10;9,141.74,320.42,245.40,8.10">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-09">2019. September 9-12, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,342.38,337.92,8.10;9,141.74,353.42,240.14,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,283.08,342.38,88.55,8.10">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<idno type="PMID">9377276</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,388.15,342.38,71.78,8.10">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,364.25,338.12,8.19;9,141.74,375.38,328.99,8.10;9,141.74,386.42,329.01,8.10;9,141.74,397.34,45.33,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,293.69,364.25,177.09,8.19;9,141.74,375.38,54.58,8.10">TheEarthIsFlat&apos;s submission to CLEF&apos;19 Check-That! challenge</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Favano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Carman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lanzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,211.37,375.38,259.36,8.10;9,141.74,386.42,119.88,8.10">CLEF 2019 Working Notes. Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,268.03,386.42,147.09,8.10">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,408.38,337.97,8.10;9,141.74,419.42,328.75,8.10;9,141.74,430.34,328.73,8.10;9,141.74,441.40,136.56,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,254.45,408.38,216.18,8.10;9,141.74,419.42,119.95,8.10">The IPIPAN team participation in the check-worthiness task of the CLEF2019 CheckThat! lab</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gasior</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Przyby La</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,277.25,419.42,193.24,8.10;9,141.74,430.34,204.60,8.10">CLEF 2019 Working Notes. Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,354.80,430.34,115.68,8.10;9,141.74,441.40,33.29,8.10">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,452.44,337.98,8.10;9,141.74,463.36,328.92,8.10;9,141.74,474.40,328.70,8.10;9,141.74,485.44,156.12,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,291.17,452.44,179.48,8.10;9,141.74,463.36,136.30,8.10">Entity detection for check-worthiness prediction: Glasgow Terrier at CLEF CheckThat!</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,312.19,463.36,158.47,8.10;9,141.74,474.40,224.21,8.10">CLEF 2019 Working Notes. Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,372.61,474.40,97.83,8.10;9,141.74,485.44,52.91,8.10">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,496.36,337.85,8.10;9,141.74,507.40,226.88,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,266.89,496.36,88.71,8.10">Support-vector networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00994018</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,373.19,496.36,65.42,8.10">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.40,518.44,338.20,8.10;9,141.74,529.36,328.80,8.10;9,141.74,540.40,224.78,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,326.70,518.44,24.37,8.10">UAICS</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Coca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Cusmuliuc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,368.35,518.44,102.25,8.10;9,141.74,529.36,297.04,8.10">CLEF 2019 Working Notes. Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,446.66,529.36,23.89,8.10;9,141.74,540.40,121.51,8.10">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.40,551.44,338.16,8.10;9,141.74,562.36,328.63,8.10;9,141.74,573.40,328.83,8.10;9,141.74,584.44,23.25,8.10" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="9,339.55,551.44,131.01,8.10;9,141.74,562.36,328.63,8.10;9,141.74,573.40,243.44,8.10">The Copenhagen Team Participation in the Factuality Task of the Competition of Automatic Identification and Verification of Claims in Political Debates of the CLEF-2018 Fact Checking Lab</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Larseny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">CLEF</note>
	<note>Working Notes</note>
</biblStruct>

<biblStruct coords="9,132.40,595.36,338.33,8.10;9,141.74,606.40,328.93,8.10;9,141.74,617.47,328.94,8.10;9,141.74,628.39,328.98,8.10;9,141.74,639.43,299.20,8.10;9,124.82,650.47,11.37,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,141.74,606.40,328.93,8.10;9,141.74,617.47,113.57,8.10">Application of the residue number system to reduce hardware costs of the con-volutional neural network implementation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">V</forename><surname>Valueva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">N</forename><surname>Nagornov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">A</forename><surname>Lyakhov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Valuev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">I</forename><surname>Chervyakov</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.matcom.2020.04.031</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,273.37,617.47,160.78,8.10">Mathematics and Computers in Simula-tion</title>
		<idno type="ISSN">0378-4754</idno>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="232" to="243" />
			<date type="published" when="2020">2020</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note>Convolutional neural networks are a promising tool for solving the problem of pattern recognition 13</note>
</biblStruct>

<biblStruct coords="9,141.74,650.47,328.96,8.10;9,141.74,661.39,181.17,8.10" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Beitzel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">C</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<title level="m" coord="9,438.21,650.47,32.50,8.10;9,141.74,661.39,94.90,8.10">Encyclopedia of Database Systems</title>
		<editor>
			<persName><surname>Map. In</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Özsu</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.40,672.43,338.17,8.10;9,141.74,683.47,149.77,8.10" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,216.37,672.43,81.53,8.10">Mean Reciprocal Rank</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,411.69,672.43,58.88,8.10;9,141.74,683.47,63.37,8.10">Encyclopedia of Database Systems</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Özsu</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,150.35,338.12,8.10;10,141.74,161.39,59.55,8.10" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,253.97,150.35,185.18,8.10">Fine-tuned language models for text classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno>CoRR, abs/1801.06146</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,172.31,338.23,8.10;10,141.74,183.35,59.55,8.10" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,265.01,172.31,173.11,8.10">A simple method for commonsense reasoning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1806.02847</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,194.39,338.32,8.10;10,141.74,205.31,329.12,8.10;10,141.74,216.26,191.47,8.19" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,325.51,194.39,114.30,8.10">The winograd schema challenge</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,455.26,194.39,15.46,8.10;10,141.74,205.31,329.12,8.10;10,141.74,216.26,74.26,8.18">Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning</title>
		<meeting>the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="552" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,227.39,338.44,8.10;10,141.74,238.31,279.04,8.10" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="10,447.70,227.39,23.14,8.10;10,141.74,238.31,188.98,8.10">A surprisingly robust trick for winograd schema challenge</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kocijan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Cretu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yordanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<idno>CoRR, abs/1905.06290</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,249.35,338.27,8.10;10,141.74,260.42,328.81,8.10;10,141.74,271.34,328.56,8.10;10,141.74,282.38,77.54,8.10" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,359.81,260.42,110.74,8.10;10,141.74,271.34,279.79,8.10">Overview of CheckThat! 2020 English: Automatic Identification and Verification of Claims in Social Media</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,438.46,271.34,31.84,8.10;10,141.74,282.38,55.02,8.10">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
