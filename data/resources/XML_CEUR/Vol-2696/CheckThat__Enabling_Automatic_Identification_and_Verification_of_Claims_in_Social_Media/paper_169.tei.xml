<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.15,115.96,331.06,12.62;1,160.80,133.89,293.76,12.62;1,142.77,151.82,329.81,12.62">UNIPI-NLE at CheckThat! 2020: Approaching Fact Checking from a Sentence Similarity Perspective Through the Lens of Transformers</title>
				<funder ref="#_HA8Snzr">
					<orgName type="full">Italian Ministry of Education and Research (MIUR)</orgName>
				</funder>
				<funder ref="#_edt5Aud">
					<orgName type="full">University of Pisa</orgName>
				</funder>
				<funder>
					<orgName type="full">NVIDIA Corporation</orgName>
				</funder>
				<funder>
					<orgName type="full">(Departments of Excellence)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,139.19,189.64,73.75,8.74"><forename type="first">Lucia</forename><forename type="middle">C</forename><surname>Passaro</surname></persName>
							<email>lucia.passaro@fileli.unipi.italessandro.lenci</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.50,189.64,89.39,8.74"><forename type="first">Alessandro</forename><surname>Bondielli</surname></persName>
							<email>alessandro.bondielli@unifi.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Florence</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,329.78,189.64,74.17,8.74"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,433.88,189.64,42.28,8.74;1,282.60,201.60,45.69,8.74"><forename type="first">Francesco</forename><surname>Marcelloni</surname></persName>
							<email>francesco.marcelloni@unipi.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.15,115.96,331.06,12.62;1,160.80,133.89,293.76,12.62;1,142.77,151.82,329.81,12.62">UNIPI-NLE at CheckThat! 2020: Approaching Fact Checking from a Sentence Similarity Perspective Through the Lens of Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3C817876DD02E84090272337D0DBC61F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a Fact Checking system based on a combination of Information Extraction and Deep Learning strategies to approach the task named "Verified Claim Retrieval" (Task 2) for the CheckThat! 2020 evaluation campaign. The system is based on two main assumptions: a claim that verifies a tweet is expected i) to mention the same entities and keyphrases, and ii) to have a similar meaning. The former assumption has been addressed by exploiting an Information Extraction module capable of determining the pairs in which the tweet and the claim share at least a named entity or a relevant keyword. To address the latter, we exploited Deep Learning to refine the computation of the text similarity between a tweet and a claim, and to actually classify the pairs as correct matches or not. In particular, the system has been built starting from a pre-trained Sentence-BERT model, on which two cascade fine-tuning steps have been applied in order to i) assign a higher cosine similarity to gold pairs, and ii) classify a pair as correct or not. The final ranking produced by the system is the probability of the pair labelled as correct. Overall, the system reached a 0.91 MAP@5 on the test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The great proliferation of online misinformation and fake news in the last few years encouraged the development of several fact-checking initiatives by various actors including journalists, governments, organizations, and companies. In the past, fact-checking was typically performed manually, resulting in the collection of large amounts of annotated resources for this specific task. More recently, researchers have started to use such resources with the aim of training automatic fact-checking systems <ref type="bibr" coords="1,230.16,614.47,15.50,8.74" target="#b18">[19,</ref><ref type="bibr" coords="1,247.33,614.47,12.73,8.74" target="#b24">25,</ref><ref type="bibr" coords="1,261.71,614.47,11.62,8.74" target="#b32">33]</ref>. A common phenomenon in social media is that viral claims often come back after a while <ref type="bibr" coords="2,324.04,118.99,14.61,8.74" target="#b24">[25]</ref>, increasing the probability that a particular claim has been previously fact-checked by a trusted organization. Therefore, systems able to decide whether a claim has been already fact-checked have become particularly relevant, because they contribute to breaking down the costs of verifying both old and new viral claims. In this scenario, the CLEF2020-CheckThat! task 2 <ref type="bibr" coords="2,221.30,178.77,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,233.48,178.77,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="2,242.89,178.77,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="2,252.30,178.77,12.73,8.74" target="#b25">26]</ref> has been organized with the goal of supporting journalists and fact-checkers when trying to determine whether a claim has been already fact-checked.</p><p>The goal of the task is specified as follows: "Given a check-worthy claim and a dataset of verified claims, rank the verified claims, so that those that verify the input claim (or a sub-claim in it) are ranked on top". <ref type="foot" coords="2,385.08,236.97,3.97,6.12" target="#foot_0">3</ref>This paper describes a system that approaches such task by exploiting a combination of Information Extraction (IE) and Deep Learning (DL) strategies to associate a tweet with the most probable claim that verifies it. The task is indeed strongly related to the concepts of information extraction and text similarity. Intuitively, to guess if two claims are related to each other, it is important to establish whether i.) they share some linguistic properties (e.g., mentioned entities) and ii.) they are in general semantically similar. In order to deal with i.), traditional IE methods are very useful and accurate <ref type="bibr" coords="2,389.55,334.19,15.50,8.74" target="#b20">[21]</ref> when extracting information such as Named Entities (e.g., persons, locations and organizations) and content words (e.g., nouns, verbs). On the other hand, ii.) requires a deeper representation of text meaning, which can be obtained with Neural Language Models (NLMs) <ref type="bibr" coords="2,206.61,382.01,9.96,8.74" target="#b2">[3]</ref>. State-of-the-art NLMs <ref type="bibr" coords="2,323.37,382.01,15.50,8.74" target="#b11">[12,</ref><ref type="bibr" coords="2,340.53,382.01,12.73,8.74" target="#b21">22]</ref> based on Transformer architectures and attention mechanisms <ref type="bibr" coords="2,288.78,393.96,15.50,8.74" target="#b30">[31]</ref> have become increasingly popular in the last couple of years, thanks to their ability to model whole text sequences and generate pre-trained representations that can be fine-tuned for different tasks. An important feature of the word representations produced by such models is that they are contextualized (i.e., they differ depending on the word context), thereby improving model performance in tasks based on word <ref type="bibr" coords="2,406.37,453.74,15.50,8.74" target="#b22">[23]</ref> and sentence <ref type="bibr" coords="2,134.77,465.69,15.50,8.74" target="#b23">[24]</ref> similarity.</p><p>The rest of the paper is organized as follows: Section 2 presents an overview of both fact-checking frameworks and NLP resources relevant for the task. Section 3 describes the proposed approach to solve the fact-checking task, consisting in the creation of two fine-tuned models to handle the claim semantic relatedness. Sections 4 and 5 focus on results and discussion, respectively. Finally, Section 6 draws some conclusions and describes future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>A key aspect of the process of building trustworthy data sets of fake and reliable news is actually how the Fact-Checking process is performed <ref type="bibr" coords="2,394.31,603.17,9.96,8.74" target="#b4">[5]</ref>. In the last years, several approaches have been proposed for different purposes. For example, Fact Check Explorer, developed by Google, <ref type="foot" coords="2,309.61,625.51,3.97,6.12" target="#foot_1">4</ref> browses and searches for fact checks by exploiting mentions and topics and by offering several filters to refine the queries. Similarly, ClaimsKG <ref type="bibr" coords="3,262.50,130.95,15.50,8.74" target="#b28">[29]</ref> offers a Knowledge Graph to search the claims containing particular named entities or keyphrases.</p><p>Over many years, fact-checking has been performed manually by journalists, by exploiting available tools online <ref type="bibr" coords="3,312.42,170.51,14.61,8.74" target="#b24">[25]</ref>. Earliest work on automated factchecking define the task as the assignment of a truth value to a claim made in a particular context <ref type="bibr" coords="3,226.75,194.42,14.61,8.74" target="#b31">[32]</ref>. Most of the approaches on automated fact-checking exploit the reliability of a source and the stance of its claims with respect to other claims and already verified information. The assignment of the truth value is often based on the way in which particular claims (or rumors) are spread on social media <ref type="bibr" coords="3,190.33,242.24,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="3,202.51,242.24,7.75,8.74" target="#b8">9,</ref><ref type="bibr" coords="3,211.91,242.24,12.73,8.74" target="#b12">13,</ref><ref type="bibr" coords="3,226.31,242.24,12.73,8.74" target="#b27">28]</ref> or on the Web <ref type="bibr" coords="3,305.20,242.24,15.50,8.74" target="#b16">[17,</ref><ref type="bibr" coords="3,322.36,242.24,11.62,8.74" target="#b19">20]</ref>. Other approaches use Wikipedia <ref type="bibr" coords="3,134.77,254.20,15.50,8.74" target="#b17">[18,</ref><ref type="bibr" coords="3,151.93,254.20,12.73,8.74" target="#b29">30]</ref> or other knowledge graphs <ref type="bibr" coords="3,285.90,254.20,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="3,303.06,254.20,12.73,8.74" target="#b26">27]</ref> to fact-check claims. More recently, a novel approach has been proposed that exploits Sentence-BERT <ref type="bibr" coords="3,418.15,266.15,15.50,8.74" target="#b23">[24]</ref> to re-rank claims <ref type="bibr" coords="3,165.26,278.11,15.50,8.74" target="#b24">[25]</ref> in order to predict whether a claim has been fact-checked before.</p><p>Indeed, DL models have proven to be among the most effective techniques for Language Modelling. In addition, the availability of new DL architectures such as Transformers <ref type="bibr" coords="3,229.15,317.67,15.50,8.74" target="#b11">[12]</ref> has led to a significant performance improvement in a wide range of NLP tasks. Transformers have two main advantages over previous Language Modelling architectures. First, thanks to the attention mechanism each element of a text sequence can access information of all the other elements. Thus, the meaning of words (and sentences) in context can be modelled more effectively. Second, the Transformer architecture is geared towards exploiting the full potentialities of transfer learning for NLP tasks. The idea behind transfer learning is that the knowledge learnt on a more general task can be exploited to specialize a model on new problems for which the amount of data is much more limited. One particular instance of transfer learning consists of two different training paradigms, namely pre-training and fine-tuning. During pre-training, language models are typically trained with an unsupervised learning tasks on vast collections of textual data. For example, models can be trained to predict specific words in a sequence based on their surrounding context, and to predict whether two sentences are sequential or not <ref type="bibr" coords="3,334.45,485.05,14.61,8.74" target="#b11">[12]</ref>. During fine-tuning, the pretrained model is further trained, this time for a limited number of epochs, on supervised learning tasks such as for example sequence labeling or sequence pair classification. The main idea is that, the initial weights (or a subset thereof) of the pre-trained model, are further adjusted to model the fine-tuning task. Typically, the pre-training step is very time-consuming and computationally expensive, but the same resulting model can be used as a starting point to solve a wide range of tasks. On the other hand, the fine-tuning step is less resourcedemanding and requires less labelled data.</p><p>Transformer-based architectures such as BERT <ref type="bibr" coords="3,356.31,596.34,15.50,8.74" target="#b11">[12]</ref> and XLNet <ref type="bibr" coords="3,425.90,596.34,15.50,8.74" target="#b34">[35]</ref> have obtained state-of-the-art results in most NLP tasks they have been applied to. One advantage of such architectures is that they learn contextualized representations that allow models to capture word polysemy. Conversely, more traditional language models such as Skip-Gram and Continuous-Bag-of-Words algorithms <ref type="bibr" coords="3,470.07,644.16,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="3,134.77,656.12,12.73,8.74" target="#b14">15,</ref><ref type="bibr" coords="3,149.16,656.12,12.73,8.74" target="#b15">16]</ref> learn non-contextualized embeddings and store a single vector for each word type belonging to the training set, independently of its context. Moreover, Transformers have been also exploited to obtain context-aware sentence representations that have been proven to enable semantic comparison of sentences with promising performances <ref type="bibr" coords="4,264.48,154.86,14.61,8.74" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The UNIPI-NLE approach</head><p>Given a tweet and a set of already verified claims (vclaims), the goal of the task is to predict, for every target tweet-vclaim pair, the likelihood of the vclaim verifying the tweet. Indeed, among the target tweet-vclaim pairs, there exists only a gold pair whose vclaim verifies the tweet, which therefore is a correct match. The goal is achieved by ranking, for each tweet, the claims that are more likely to verify it. The dataset is composed of three elements:</p><p>1. the verified claims used for fact checking, each of them provided with an identifier, a title, and the actual claim; 2. the training tweets, associated with an identifier and a textual content; 3. the correct pairing between tweets and verified claims. The UNIPI-NLE system is based on two main assumptions: the claims that verify a tweet are expected to mention the same entities and keyphrases and should have a similar meaning. To address the first point, among target tweetvclaim pairs, we identify the subset of candidate pairs (also referred as potential pairs) in which the tweet and the vclaim share at least a named entity or a content word. We refer to the step of identifying candidate pairs among target ones as the IE step. The DL modules described below have been fed only with the portion of the dataset consisting of such candidate pairs. In order to estimate the text similarity between a tweet and a vclaim, we exploit Siamese BERT networks <ref type="bibr" coords="4,134.77,476.79,15.50,8.74" target="#b23">[24]</ref> to create a language model that is able to better deal with sentence-level textual similarity. This model is then used to learn if a claim can be used to verify a tweet. In particular, we perform two cascade fine-tuning steps aimed at i.) assigning a higher cosine similarity to gold tweet-vclaim pairs and ii.) actually classifying a target tweet-vclaim pair, and more specifically a candidate tweet-vclaim pair, as a correct match (gold) or not.</p><p>Figure <ref type="figure" coords="4,183.50,548.52,4.98,8.74" target="#fig_1">1</ref> shows the neural components of the system architecture. The first white stack (bert-base-uncased + bert-base-nli-mean-tokens) consists of the pre-trained model released by <ref type="bibr" coords="4,340.96,572.43,15.50,8.74" target="#b23">[24]</ref> and trained on SNLI <ref type="bibr" coords="4,470.07,572.43,10.52,8.74" target="#b5">[6]</ref> and MultiNLI dataset <ref type="bibr" coords="4,243.91,584.39,15.50,8.74" target="#b33">[34]</ref> to create universal sentence embeddings. The black boxes show our two fine-tuned models: Our Sentence-BERT model (bert-base-nli-factcheck-cos) follows a training paradigm similar to the one described in <ref type="bibr" coords="4,191.53,620.25,14.61,8.74" target="#b23">[24]</ref>, but it is specifically geared to assigning a higher cosine similarity to gold tweet-vclaim pairs. The last level of the architecture represents the final Transformer-based classifier trained to decide, given a candidate tweetvclaim pair, whether the tweet is actually verified by that claim or not. The classifier fine-tunes bert-base-nli-factcheck-cos on the fact-checking task, by labelling candidate tweet-vclaim pairs as correct matches (gold) or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Information Extraction (IE) step</head><p>Starting from the assumption that similar claims tend to mention the same entities and keyphrases, we developed an IE module to find potential tweetvclaim pairs. Such a module is based on Stanza <ref type="bibr" coords="5,348.19,473.41,14.61,8.74" target="#b20">[21]</ref>, a state of the art natural language analysis package. We processed each text fragment (i.e., a tweet, a vclaim or a vclaim title) with Sentence Splitting, PoS-tagging, Lemmatization, and Named Entity Recognition. Thus, each text is associated with its keywords, consisting of its content words (nouns, verbs, and adjectives) and named entities. Given a tweet, in order to retrieve potential claims that verify it, we used two different functions based on the keywords: IE function -the overlapping score is simply computed by counting the number of elements (cf. the keywords field in Table <ref type="table" coords="5,373.81,578.18,4.98,8.74" target="#tab_0">1</ref> and<ref type="table" coords="5,400.44,578.18,31.30,8.74" target="#tab_1">Table 2</ref>) shared by the tweet and the claim. Candidate tweet-vclaim pairs are required to share at least one lowercased element (named entity or content word). IEElastic function -it exploits Elasticsearch<ref type="foot" coords="5,360.09,612.34,3.97,6.12" target="#foot_2">5</ref> to find the potential candidate pairs. Specifically, for each tweet, candidate claims consist of the top 1, 000 matches ranked by relevance, using the scoring function provided by the task organizers for the baseline. Such scoring function is an Elasticsearch multi-match query based on both the vclaim and its title and the tweet itself.</p><p>Candidate tweet-claim pairs obtained with the IE overlapping function were used to train the model bert-base-nli-factcheck-cos. Candidate tweet-claim pairs obtained with the IE and IEElastic functions have been used at inference time to obtain the final predictions submitted for evaluation, namely respectively T2-EN-UNIPI-NLE-BERT2IE (contrastive run) and T2-EN-UNIPI-NLE-BERT2IEElastic (primary run). Moreover, we used both the IE and IEElastic functions to simply rank the potential claims associated with each tweet according to the overlapping score. The score provided by IEElastic corresponds to the task official baseline. The results of these rankings are reported in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fine-tuning of Transformer models</head><p>Siamese BERT networks <ref type="bibr" coords="6,244.95,578.47,15.50,8.74" target="#b23">[24]</ref> can be used to create language models specialized on tasks related to Semantic Textual Similarity (STS) <ref type="bibr" coords="6,371.25,590.42,14.61,8.74" target="#b9">[10]</ref>. In order to train our system to recognize gold tweet-vclaim pairs, we first model the textual similarity between tweets and claims belonging to the same candidate tweet-vclaim pairs. To this purpose, we started from one of the available fine-tuned Sentence-BERT models, <ref type="foot" coords="6,167.75,636.67,3.97,6.12" target="#foot_3">6</ref> namely bert-base-nli-mean-tokens <ref type="bibr" coords="6,344.83,638.24,14.61,8.74" target="#b23">[24]</ref>. Such model was originally trained on SNLI <ref type="bibr" coords="7,207.57,118.99,10.52,8.74" target="#b5">[6]</ref> and MultiNLI dataset <ref type="bibr" coords="7,318.38,118.99,15.50,8.74" target="#b33">[34]</ref> and tested on the STSbenchmark <ref type="bibr" coords="7,134.77,130.95,14.61,8.74" target="#b9">[10]</ref>. The training phase was performed by classifying a pair of sentences with the labels entail, contradict, and neutral while evaluation was performed on the STSbenchmark <ref type="bibr" coords="7,204.69,154.86,15.50,8.74" target="#b9">[10]</ref> dataset, which contains sentence pairs and their similarity score. The trained model was exploited to infer sentence pair similarity via cosine.</p><p>The bert-base-nli-mean-tokens achieved 77.12 Pearson correlation with gold scores on the STSbenchmark test set. We added two levels of fine-tuning to this model, in order to adapt the sentence pair similarity task to the fact-checking one (i.e., gold tweet-vclaim pairs are associated with the maximum cosine similarity), as well as to fact-check a pair with a classification layer (i.e., gold tweet-vclaim pairs are associated with the positive label). To this end, we exploited both the vclaim text content and its title, namely the vclaim title.</p><p>In fact, each vclaim is provided with a title that can be considered as a summary of the vclaim itself and therefore, very similar to it. The usage of both the vclaim and vclaim title for training has two main advantages. First, it allows to increase the size of the dataset so that the model is shown more positive examples, that are under-represented. Second, it helps to add variability to the training examples, both positive and negative. For example, a title may contain an acronym such as "KKK", whereas the claim may contain its extended form, in this case "Ku Klux Klan". In our experiments we noticed that such a variability was very helpful to improve the overall performances of our models.</p><p>Sentence pair similarity We modeled the fine-tuning step like Reimers and Gurevych <ref type="bibr" coords="7,180.58,410.76,15.50,8.74" target="#b23">[24]</ref> to estimate the semantic similarity between two sentences. The authors used the STSbenchmark <ref type="bibr" coords="7,278.90,422.71,15.50,8.74" target="#b9">[10]</ref> dataset, containing pairs of sentences with a similarity score ranging from 0 (no similarity) to 5 (maximum similarity). The Sentence-BERT model was fine-tuned using the regression objective function on the training set <ref type="bibr" coords="7,202.73,458.58,14.61,8.74" target="#b23">[24]</ref>. Therefore, for each epoch, loss was computed by considering the correlation between the gold similarity judgments and the predicted cosine similarity between sentence embeddings.</p><p>Our goal was to train the model to identify the gold tweet-vclaim pair among the set of potential ones (i.e., those filtered with the IE step). To this aim, we tried to separate the gold tweet-vclaim pairs from the other candidates. In particular, given the assumption that a claim that verifies a tweet is semantically similar to it, we built our training set as follows:</p><p>1. we created two positive examples from a gold tweet-vclaim pair, the first one composed by the tweet and the vclaim itself (tweet-vclaim pair), and the second one composed by the tweet and the title of the claim (tweetvclaim title). Both the positive pairs were assigned with a cosine similarity value of 1.0. This forces the model to boost the similarity between the texts belonging to gold pairs; 2. for each gold tweet-vclaim pair, 20 other tweet-vclaim pairs were randomly selected as negative examples from the list of candidate pairs obtained with the IE overlapping function (cf. Section 3.1). The similarity of the negative examples was computed as the cosine similarity between vectors predicted by bert-base-nli-mean-tokens, modified by the tanh function. This has the effect of decreasing the cosine similarity, thus effectively penalising negative examples.</p><p>The bert-base-nli-factcheck-cos model was trained for 4 epochs with a batch size of 8, and 10% of data was used for the model warm up.</p><p>Classification The bert-base-nli-factcheck-cos model is used to initialize the weights for the classifier. In this case, the model is trained on a simple binary classification task to distinguish between matching (gold) pairs, labelled as 1, and non-matching ones, labelled as 0. Similarly to the previous fine-tuning step, we selected negative examples among candidate tweet-vclaim pairs returned by the IE module. Like for the sentence pair similarity model, for each tweet, the tweetvclaim and the tweet-vclaim title pairs were used as positive examples. However, in this case 2 negative examples were selected among the tweet-vclaim candidate pairs, in order to better balance the training data for the classification.</p><p>Our model bert-base-nli-factcheck-clas is therefore a Transformer with a classification head on top of it, implemented with the Huggingface library. 7  The model was trained for 3 epochs with a batch size of 8. We used the AdamW optimizer with a learning rate of 2e -5 <ref type="bibr" coords="8,310.23,357.79,14.61,8.74" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference step</head><p>The inference step was performed by classifying the candidate tweet-vclaim pairs. To retrieve the potential candidates, in fact, we applied the functions IE and IEElastic described in section 3.1 to obtain, respectively, the T2-EN-UNIPI-NLE-BERT2IE and the T2-EN-UNIPI-NLE-BERT2IEElastic predictions. Moreover, we also tested a run in which we classified all the target tweet-vclaim pairs with no-preselection. The results of this additional experiment are shown in Table <ref type="table" coords="8,163.24,477.03,3.87,8.74">4</ref>. In all cases, we used the probability of the class 1 predicted by the bert-base-nli-factcheck-clas model to rank the vclaims for each tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The evaluation metric used in the competition is the Mean Average Precision @5 (MAP@5) calculated over the gold ranking. The overall performance of our models is reported in Table <ref type="table" coords="8,263.02,566.39,3.87,8.74" target="#tab_2">3</ref>. For the sake of comparison, we also show the performances obtained by the top models and by the official baseline as well.</p><p>Our systems, namely the T2-EN-UNIPI-NLE-BERT2IE and the T2-EN-UNIPI-NLE-BERT2IEElastic, which differ for the overlapping function used at inference time, obtained respectively 0.9160 and 0.9120 (cf. tables 4 and 5).</p><p>Moreover, in order to explain the effectiveness of each module for the final predictions, we computed their performances on the test set. Table <ref type="table" coords="8,429.88,638.12,4.98,8.74" target="#tab_4">6</ref> shows the performances of each module obtained on the task by ranking the claim for a tweet according to several measures. More specifically, for each module, we report the model name, the type of the fine-tuning we applied, the function used at inference time for selecting candidates and the MAP@5 obtained with the official scorer.</p><p>As for the IE step, given a tweet, we ranked the claims according to the overlapping function for both the IE and the IEElastic methods. The IEElastic method coincides actually with the baseline provided by the task organizers. To assess the performances of the Sentence-BERT model fine-tuned on cosine similarity, namely the bert-base-nli-factcheck-cos, we ranked the claim according to the adjusted cosine similarity. Finally, we show the final submitted results. At inference time, the model bert-base-nli-factcheck-clas was fed with the candidate tweet-vclaim pairs calculated with both the IE and the IEElastic methods. In addition, we also report the results obtained by making the predictions for all the target tweet-vclaim pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Several remarks can be made to comment our results. By looking at the model scores, we see that our approach is able to outperform the baseline by a wide margin, despite the fact that the Elasticsearch based approach proposed by the task organizer was shown to be very effective nonetheless. In addition, our system ranked second among the participants of the task, obtaining performances that are only slightly worse than the winning system, which obtained a MAP@5 of 0.938.</p><p>Moreover, we can draw some interesting insights by considering the various steps and data selection strategies. We notice that the IE baseline appears to be less effective as a standalone tool for selecting the best candidates among claims for each tweet, with results well below the IEElastic one. However, the IE method performs optimally when used as a selection criterion at inference time. In fact, we experimented three methods for selecting candidate pairs at inference time. In addition to the IE method and the IEElastic one, we assessed the performance with no pre-selection as well. In this case, the classifier was shown with all possible tweet-vclaim pairs. We see that the IE method performs best, but only slightly better than IEElastic. However, both pre-selection methods outperform the model for which no pre-selection is made. We can argue that this is because, during training, our objective was to enable the classifier to distinguish between the gold tweet-vclaim pair and other pairs that share similar features but are in fact incorrect. Therefore, the classifier may be more prone to errors when tweet-vclaim pairs, which differ greatly from each other, are shown as it never saw such examples during training. Experimental results seem to confirm such hypothesis. This could be seen as a potential shortcoming for the classifier itself. However, we can argue that considering the system as a whole, it can bring two potential advantages. First, the classifier needs less negative examples for an effective training. We can argue that it is more difficult to decide between two similar claims for a tweet, rather than between two very different ones. Therefore, we chose to train the classifier to solve the "harder" problem, and addressed the "simpler" one with a less sophisticated, yet effective, approach. Second, the classification of each tweet-vclaim pair is time consuming. On our machine, equipped with a Nvidia TitanXp graphic card, the inference step considering all pairs took around ten hours. When performing the preselection of pairs with our IE method, the inference step took less than four hours. The IE method is efficient because it only needs to extract content words and named entities for each pair, a task that is almost trivial in terms of time complexity with modern NLP toolkits and current hardware. Finally, it is interesting to point out the contribution of the cosine similarity adaptation performed with Sentence-BERT. Clearly, the model itself does not perform well on the present task. However, two observations can be made. First, during development we noticed that, by using a standard BERT model such as BERT-base-uncased for representing sentences (i.e., by averaging word-level representations obtained from the model), ranking claims based on cosine similarity was completely ineffective, obtaining a very low MAP@5. Instead, by exploiting a pre-trained Sentence-BERT model, we obtained much more encouraging results, that were subsequently improved thanks to our cascade fine-tuning strategy. This serves as additional evidence for the fact that standard BERT models are not able to represent sentences in a semantically proper way, as already claimed in the literature <ref type="bibr" coords="11,196.62,528.65,14.61,8.74" target="#b23">[24]</ref>. Second, by exploiting the fine-tuned Sentence-BERT model (bert-base-nli-factcheck-cos) for obtaining the initial weights for the classifier (bert-base-nli-factcheck-clas), we clearly outperformed a model based on BERT-base-uncased and trained in the same way. More specifically, on the development set we obtained a 0.72 MAP@5 for a bert-base-uncased fine-tuned model and a MAP@5 of 0.78 for the bert-base-nli-factcheck-cos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future directions</head><p>The approach to Fact Checking performed by the UNIPI-NLE team is based on a combination of IE and DL strategies. The choice has been led by the assumptions that supporting claims tend to mention the same entities and keywords of the target tweet, and are semantically similar to it. On the one hand, a standard IE module extracts relevant words and entities from texts and is used to construct the training set for the following DL modules and to constrain the inference process. In fact, the IE step is also crucial to turn down the processing time by filtering the candidate pairs to be classified. Transformers, on the other hand, are very useful to carry out effective transfer learning, by fine-tuning large pretrained models for specific tasks such as the fact-checking one. In this paper, fine-tuning is exploited both for modeling textual similarity and for classifying text pairs to decide if a member of the pair verifies the other.</p><p>The UNIPI-NLE approach strongly outperforms the baseline and ranked second among the primary submissions of the task. In the future, we plan to perform some additional hyperparameter tuning on the models. Moreover, we would like to test this approach in similar tasks such as Fake News identification. We are confident that by exploiting the dynamic selection of training data in addition to an effective and efficient information extraction strategy, we will obtain strong performances also to solve this harder task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,345.28,345.82,8.74;4,134.77,357.24,345.83,8.74"><head></head><label></label><figDesc>The training set consists of 1, 003 tweets (803 for training, 200 for development) and 10, 373 already verified claims. The test set consists of 200 additional tweets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,245.95,334.74,120.38,7.89;5,265.20,345.73,81.88,7.86;5,221.22,115.83,172.92,204.14"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Neural modules of the UNIPI-NLE system.</figDesc><graphic coords="5,221.22,115.83,172.92,204.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,159.23,121.59,293.83,229.90"><head>Table 1 .</head><label>1</label><figDesc>Example of relevant lemmas and named entities in a claim.</figDesc><table coords="6,159.23,121.59,293.83,224.42"><row><cell>vclaim and title</cell><cell>keywords</cell></row><row><cell>title: Was Sen. Chuck Schumer a Client of 'Hollywood Madam' Heidi Fleiss? vclaim: Sen. Chuck Schumer's name and/or phone number were found in "Hollywood Madam" Heidi Fleiss's black book of clients.</cell><cell>['chuck 'heidi fleiss's', 'chuck schumer', schumer', 'hollywood', 'hollywood', 'heidi fleiss', 'sen.', 'chuck', 'schumer', 'phone', 'num-ber', 'find', 'hollywood', 'madam', 'heidi', 'fleiss', 'black', 'book', 'client', 'sen.', 'chuck', 'schumer', 'client', 'hollywood', 'madam',</cell></row><row><cell></cell><cell>'heidi', 'fleiss']</cell></row><row><cell>tweet</cell><cell>keywords</cell></row><row><cell>Chuck Schumer was one of Hedil Fleiss' top clients. Look it up.</cell><cell>['chuck schumer', 'hedil fleiss', 'doug masters', 'january 23, 2019',</cell></row><row><cell>Doug Masters (@protestertrophy)</cell><cell></cell></row><row><cell>January 23, 2019</cell><cell></cell></row></table><note coords="6,311.32,321.71,141.73,7.86;6,311.32,332.67,141.73,7.86;6,311.32,343.62,118.66,7.86"><p>'chuck', 'schumer', 'hedil', 'fleiss', 'client', 'look', 'doug', 'masters', '@protestertrophy', 'january']</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,168.01,366.11,279.34,7.89"><head>Table 2 .</head><label>2</label><figDesc>Example of relevant lemmas and named entities in a tweet.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.77,121.59,345.83,123.95"><head>Table 3 .</head><label>3</label><figDesc>Performance of the UNIPI-NLE models against the top performing models and the official baseline.</figDesc><table coords="9,174.98,121.59,262.32,90.48"><row><cell>Team</cell><cell>type</cell><cell>MAP@1</cell><cell>MAP@3</cell><cell>MAP@5</cell></row><row><cell>Buster.ai</cell><cell>primary</cell><cell>0.897</cell><cell>0.926</cell><cell>0.929</cell></row><row><cell>Buster.ai</cell><cell>contr.-2</cell><cell>0.907</cell><cell>0.937</cell><cell>0.938</cell></row><row><cell>UNIPI-NLE</cell><cell>primary</cell><cell>0.877</cell><cell>0.907</cell><cell>0.912</cell></row><row><cell>UNIPI-NLE</cell><cell>contr.-1</cell><cell>0.877</cell><cell>0.913</cell><cell>0.916</cell></row><row><cell cols="2">Task Organizers baseline</cell><cell>0.767</cell><cell>0.812</cell><cell>0.815</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,151.64,121.59,314.96,327.49"><head>Table 5 .</head><label>5</label><figDesc>T2-EN-UNIPI-NLE-IEElastic</figDesc><table coords="10,151.64,121.59,291.12,327.49"><row><cell>metric</cell><cell cols="2">@depth score</cell><cell>metric</cell><cell cols="2">@depth score</cell></row><row><cell>MAP</cell><cell>1</cell><cell>0.877</cell><cell>MAP</cell><cell>1</cell><cell>0.877</cell></row><row><cell>MAP</cell><cell>3</cell><cell>0.913</cell><cell>MAP</cell><cell>3</cell><cell>0.907</cell></row><row><cell>MAP</cell><cell>5</cell><cell>0.916</cell><cell>MAP</cell><cell>5</cell><cell>0.912</cell></row><row><cell>MAP</cell><cell>10</cell><cell>0.917</cell><cell>MAP</cell><cell>10</cell><cell>0.913</cell></row><row><cell>MAP</cell><cell>20</cell><cell>0.917</cell><cell>MAP</cell><cell>20</cell><cell>0.913</cell></row><row><cell>MAP</cell><cell>all</cell><cell>0.917</cell><cell>MAP</cell><cell>all</cell><cell>0.913</cell></row><row><cell>Precision</cell><cell>1</cell><cell>0.879</cell><cell>Precision</cell><cell>1</cell><cell>0.879</cell></row><row><cell>Precision</cell><cell>3</cell><cell>0.322</cell><cell>Precision</cell><cell>3</cell><cell>0.317</cell></row><row><cell>Precision</cell><cell>5</cell><cell>0.195</cell><cell>Precision</cell><cell>5</cell><cell>0.194</cell></row><row><cell>Precision</cell><cell>10</cell><cell>0.098</cell><cell>Precision</cell><cell>10</cell><cell>0.098</cell></row><row><cell>Precision</cell><cell>20</cell><cell>0.049</cell><cell>Precision</cell><cell>20</cell><cell>0.049</cell></row><row><cell>Precision</cell><cell>all</cell><cell>0.000</cell><cell>Precision</cell><cell>all</cell><cell>0.000</cell></row><row><cell>Rec Rank</cell><cell>1</cell><cell>0.879</cell><cell>Rec Rank</cell><cell>1</cell><cell>0.879</cell></row><row><cell>Rec Rank</cell><cell>3</cell><cell>0.915</cell><cell>Rec Rank</cell><cell>3</cell><cell>0.909</cell></row><row><cell>Rec Rank</cell><cell>5</cell><cell>0.918</cell><cell>Rec Rank</cell><cell>5</cell><cell>0.914</cell></row><row><cell>Rec Rank</cell><cell>10</cell><cell>0.919</cell><cell>Rec Rank</cell><cell>10</cell><cell>0.915</cell></row><row><cell>Rec Rank</cell><cell>20</cell><cell>0.919</cell><cell>Rec Rank</cell><cell>20</cell><cell>0.915</cell></row><row><cell>Rec Rank</cell><cell>all</cell><cell>0.919</cell><cell>Rec Rank</cell><cell>all</cell><cell>0.915</cell></row><row><cell cols="3">Table 4. T2-EN-UNIPI-NLE-IE</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,151.33,121.59,309.63,145.87"><head>Table 6 .</head><label>6</label><figDesc>Results calculated for each module of the architecture.</figDesc><table coords="11,151.33,121.59,309.63,123.61"><row><cell>Model</cell><cell>Fine-tuining</cell><cell cols="2">Inference MAP@5</cell></row><row><cell>bert-base-nli-factcheck-clas</cell><cell>classification</cell><cell>IE</cell><cell>0.916</cell></row><row><cell>bert-base-nli-factcheck-clas</cell><cell>classification</cell><cell>IEElastic</cell><cell>0.912</cell></row><row><cell>bert-base-nli-factcheck-clas</cell><cell>classification</cell><cell>-</cell><cell>0.89</cell></row><row><cell>IEElastic baseline</cell><cell>-</cell><cell>IEElastic</cell><cell>0.815</cell></row><row><cell>IE baseline</cell><cell>-</cell><cell>IE</cell><cell>0.74</cell></row><row><cell>bert-base-nli-factcheck-cos</cell><cell>cosine similarity</cell><cell>IE</cell><cell>0.41</cell></row><row><cell>bert-base-nli-factcheck-cos</cell><cell cols="2">cosine similarity IEElastic</cell><cell>0.35</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,646.48,250.48,7.47"><p>https://github.com/sshaar/clef2020-factchecking-task2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,144.73,657.44,207.12,7.47"><p>http://toolbox.google.com/factcheck/explorer</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="5,144.73,657.44,108.27,7.47"><p>https://www.elastic.co/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="6,144.73,657.44,221.74,7.47"><p>https://github.com/UKPLab/sentence-transformers</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>We gratefully acknowledge the support of <rs type="funder">NVIDIA Corporation</rs> with the donation of the Titan Xp GPU used for this research. This work was partially supported by the <rs type="funder">University of Pisa</rs> in the context of the project "<rs type="projectName">Event Extraction for Fake News Detection" in the framework of the</rs> <rs type="programName">MIT-Unipi program</rs>, and by the <rs type="funder">Italian Ministry of Education and Research (MIUR)</rs> in the framework of the <rs type="projectName">CrossLab project</rs> <rs type="funder">(Departments of Excellence)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_edt5Aud">
					<orgName type="project" subtype="full">Event Extraction for Fake News Detection&quot; in the framework of the</orgName>
					<orgName type="program" subtype="full">MIT-Unipi program</orgName>
				</org>
				<org type="funded-project" xml:id="_HA8Snzr">
					<orgName type="project" subtype="full">CrossLab project</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,490.64,337.63,7.86;12,151.52,501.60,329.07,7.86;12,151.52,512.56,329.07,7.86;12,151.52,523.52,329.07,7.86;12,151.52,534.47,62.50,7.86" xml:id="b0">
	<analytic>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Névéol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,385.78,501.60,94.81,7.86;12,151.52,512.56,329.07,7.86;12,151.52,523.52,258.98,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction Proceedings of the Eleventh International Conference of the CLEF Association (CLEF 2020)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,545.88,337.64,7.86;12,151.52,556.84,329.07,7.86;12,151.52,567.80,329.07,7.86;12,151.52,578.75,190.89,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="12,182.56,567.80,298.03,7.86;12,151.52,578.75,86.54,7.86">Overview of CheckThat! 2020: Automatic identification and verification of claims in social media</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hamdan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Sheikh Ali</surname></persName>
		</author>
		<editor>Arampatzis et al.</editor>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,590.16,337.63,7.86;12,151.52,601.09,283.71,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,355.72,590.16,124.87,7.86;12,151.52,601.12,22.40,7.86">A neural probabilistic language model</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,181.47,601.12,147.98,7.86">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02">Feb. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,612.52,337.63,7.86;12,151.52,623.48,329.07,7.86;12,151.52,634.41,88.81,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,364.59,612.52,116.00,7.86;12,151.52,623.48,81.78,7.86">Enriching word vectors with subword information</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,240.54,623.48,240.05,7.86;12,151.52,634.44,13.87,7.86">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,645.84,337.63,7.86;12,151.52,656.77,190.88,7.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,272.26,645.84,208.33,7.86;12,151.52,656.80,23.96,7.86">A survey on fake news and rumour detection techniques</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondielli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Marcelloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,182.55,656.80,83.53,7.86">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">497</biblScope>
			<biblScope unit="page" from="38" to="55" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,119.67,337.63,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,329.07,7.86;13,151.52,152.55,208.42,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,375.87,119.67,104.72,7.86;13,151.52,130.63,153.57,7.86">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,325.60,130.63,154.99,7.86;13,151.52,141.59,207.68,7.86">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,163.52,337.64,7.86;13,151.52,174.48,329.07,7.86;13,151.52,185.44,329.07,7.86;13,151.52,196.40,223.88,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,296.91,163.52,183.69,7.86;13,151.52,174.48,185.40,7.86">Finding credible information sources in social networks based on content and social structure</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">L</forename><surname>Pirolli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,356.99,174.48,123.60,7.86;13,151.52,185.44,329.07,7.86;13,151.52,196.40,129.92,7.86">2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and 2011 IEEE Third International Conference on Social Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,207.37,337.64,7.86;13,151.52,218.33,242.91,7.86" xml:id="b7">
	<monogr>
		<title level="m" coord="13,382.62,207.37,97.97,7.86;13,151.52,218.33,214.23,7.86">Working Notes of CLEF 2020-Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,229.29,337.63,7.86;13,151.52,240.25,329.07,7.86;13,151.52,251.21,25.60,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,310.10,229.29,132.53,7.86">Information credibility on twitter</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Poblete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,463.04,229.29,17.55,7.86;13,151.52,240.25,270.68,7.86">Proceedings of the 20th international conference on World wide web</title>
		<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,262.18,337.97,7.86;13,151.52,273.14,329.07,7.86;13,151.52,284.10,329.07,7.86;13,151.52,295.06,329.07,7.86;13,151.52,306.02,25.60,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,403.46,262.18,77.13,7.86;13,151.52,273.14,311.37,7.86">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,151.52,284.10,329.07,7.86;13,151.52,295.06,20.48,7.86">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,316.99,337.97,7.86;13,151.52,327.92,329.07,7.89;13,151.52,338.90,25.60,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,151.52,327.95,218.95,7.86">Computational fact checking from knowledge networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Ciampaglia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shiralkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">M</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Menczer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Flammini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,377.77,327.95,35.43,7.86">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">128193</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,349.87,337.98,7.86;13,151.52,360.83,329.07,7.86;13,151.52,371.79,329.07,7.86;13,151.52,382.75,329.07,7.86;13,151.52,393.71,329.07,7.86;13,151.52,404.67,25.60,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,364.92,349.87,115.67,7.86;13,151.52,360.83,213.60,7.86">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,385.79,360.83,94.80,7.86;13,151.52,371.79,329.07,7.86;13,151.52,382.75,174.01,7.86">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="13,142.62,415.64,337.97,7.86;13,151.52,426.60,329.07,7.86;13,151.52,437.56,329.07,7.86;13,151.52,448.51,329.07,7.86;13,151.52,459.47,133.22,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,200.54,426.60,280.05,7.86;13,151.52,437.56,80.66,7.86">SemEval-2019 task 7: RumourEval, determining rumour veracity and support for rumours</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gorrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Derczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,254.83,437.56,225.76,7.86;13,151.52,448.51,72.35,7.86">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>the 13th International Workshop on Semantic Evaluation<address><addrLine>Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Minneapolis</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,470.44,337.98,7.86;13,151.52,481.40,313.59,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,258.33,470.44,154.13,7.86">Decoupled weight decay regularization</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,445.36,470.44,35.23,7.86;13,151.52,481.40,284.92,7.86">Proceedings of the 2019 International Conference on Learning Representations</title>
		<meeting>the 2019 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,492.37,337.98,7.86;13,151.52,503.30,239.18,7.89" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="13,349.03,492.37,131.57,7.86;13,151.52,503.33,109.59,7.86">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,514.30,337.98,7.86;13,151.52,525.26,329.07,7.86;13,151.52,536.22,329.07,7.86;13,151.52,547.18,307.11,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,387.82,514.30,92.78,7.86;13,151.52,525.26,213.28,7.86">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,385.64,525.26,94.95,7.86;13,151.52,536.22,284.42,7.86;13,209.37,547.18,31.47,7.86">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>NIPS&apos;13</note>
</biblStruct>

<biblStruct coords="13,142.62,558.15,337.98,7.86;13,151.52,569.11,329.07,7.86;13,151.52,580.06,262.41,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,265.03,558.15,215.56,7.86;13,151.52,569.11,71.26,7.86">Leveraging joint interactions for credibility analysis in news communities</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,244.46,569.11,236.13,7.86;13,151.52,580.06,177.61,7.86">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,591.03,337.98,7.86;13,151.52,601.99,329.07,7.86;13,151.52,612.95,208.14,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,283.11,591.03,197.48,7.86;13,151.52,601.99,142.15,7.86">Combining fact extraction and verification with neural semantic matching networks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,316.14,601.99,164.45,7.86;13,151.52,612.95,83.11,7.86">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6859" to="6866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,623.92,337.98,7.86;13,151.52,634.88,329.07,7.86;13,151.52,645.84,329.07,7.86;13,151.52,656.80,47.10,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,360.55,623.92,120.05,7.86;13,151.52,634.88,260.39,7.86">Where the truth lies: Explaining the credibility of emerging claims on the web and social media</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Strötgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,432.51,634.88,48.08,7.86;13,151.52,645.84,281.75,7.86">Proceedings of the 26th International Conference on World Wide Web Companion</title>
		<meeting>the 26th International Conference on World Wide Web Companion</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1003" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,119.67,337.98,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,329.07,7.86;14,151.52,152.55,47.10,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,360.55,119.67,120.05,7.86;14,151.52,130.63,260.39,7.86">Where the truth lies: Explaining the credibility of emerging claims on the web and social media</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Strötgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,432.51,130.63,48.08,7.86;14,151.52,141.59,281.75,7.86">Proceedings of the 26th International Conference on World Wide Web Companion</title>
		<meeting>the 26th International Conference on World Wide Web Companion</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1003" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,164.30,337.98,7.86;14,151.52,175.26,329.07,7.86;14,151.52,186.22,329.07,7.86;14,151.52,197.18,92.15,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,377.72,164.30,102.87,7.86;14,151.52,175.26,227.80,7.86">Stanza: A Python natural language processing toolkit for many human languages</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,403.39,175.26,77.21,7.86;14,151.52,186.22,329.07,7.86;14,151.52,197.18,63.48,7.86">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,208.93,337.98,7.86" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<title level="m" coord="14,204.74,208.93,247.27,7.86">Improving language understanding by generative pre-training</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,220.68,337.97,7.86;14,151.52,231.61,270.35,7.89" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="14,442.07,220.68,38.52,7.86;14,151.52,231.64,172.97,7.86">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,331.32,231.64,53.37,7.86">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,243.39,337.98,7.86;14,151.52,254.35,329.07,7.86;14,151.52,265.31,329.07,7.86;14,151.52,276.27,71.90,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,264.57,243.39,216.03,7.86;14,151.52,254.35,61.99,7.86">Sentence-BERT: Sentence embeddings using siamese BERT-networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,237.42,254.35,243.17,7.86;14,151.52,265.31,130.30,7.86">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,288.02,337.98,7.86;14,151.52,298.98,329.07,7.86;14,151.52,309.94,329.07,7.86;14,151.52,320.90,167.64,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="14,382.54,288.02,98.06,7.86;14,151.52,298.98,148.78,7.86">That is a known lie: Detecting previously fact-checked claims</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,320.24,298.98,160.35,7.86;14,151.52,309.94,197.26,7.86">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3607" to="3618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,332.65,337.97,7.86;14,151.52,343.61,329.07,7.86;14,151.52,354.57,329.07,7.86;14,151.52,365.53,149.30,7.86" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="14,442.93,343.61,37.66,7.86;14,151.52,354.57,329.07,7.86;14,151.52,365.53,47.58,7.86">Overview of CheckThat! 2020 English: Automatic identification and verification of claims in social media</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<editor>Cappellato et al.</editor>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,377.28,337.98,7.86;14,151.52,388.24,329.07,7.86;14,151.52,399.19,312.73,7.86" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="14,402.82,377.28,77.77,7.86;14,151.52,388.24,177.81,7.86">Finding streams in knowledge graphs to support fact checking</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shiralkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Flammini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Menczer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Ciampaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,353.51,388.24,127.08,7.86;14,151.52,399.19,200.06,7.86">Proceedings of the 2017 IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>the 2017 IEEE International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="859" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,410.95,337.98,7.86;14,151.52,421.88,329.07,7.89;14,151.52,432.86,25.60,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="14,335.24,410.95,145.35,7.86;14,151.52,421.91,106.94,7.86">Fake news detection on social media: A data mining perspective</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,266.02,421.91,159.43,7.86">ACM SIGKDD explorations newsletter</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="36" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,444.62,337.98,7.86;14,151.52,455.57,329.07,7.86;14,151.52,466.53,282.20,7.86" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="14,252.03,455.57,210.07,7.86">Claimskg: a knowledge graph of fact-checked claims</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tchechmedjiev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fafalios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Boland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gasquet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zapilko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dietze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Todorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,151.52,466.53,158.61,7.86">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="309" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,478.28,337.97,7.86;14,151.52,489.24,329.07,7.86;14,151.52,500.20,329.07,7.86;14,151.52,511.16,329.07,7.86;14,151.52,522.12,300.11,7.86" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="14,393.95,478.28,86.64,7.86;14,151.52,489.24,177.70,7.86">FEVER: a large-scale dataset for fact extraction and VERification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,350.92,489.24,129.67,7.86;14,151.52,500.20,329.07,7.86;14,151.52,511.16,161.47,7.86">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="14,142.62,533.87,337.97,7.86;14,151.52,544.83,329.07,7.86;14,151.52,555.79,167.19,7.86" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="14,228.73,544.83,100.53,7.86">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,351.88,544.83,128.71,7.86;14,151.52,555.79,73.89,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,567.54,337.98,7.86;14,151.52,578.50,329.07,7.86;14,151.52,589.46,329.07,7.86;14,151.52,600.42,114.19,7.86" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="14,250.52,567.54,226.05,7.86">Fact checking: Task definition and dataset construction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,165.81,578.50,314.78,7.86;14,151.52,589.46,100.61,7.86">Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science</title>
		<meeting>the ACL 2014 Workshop on Language Technologies and Computational Social Science<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,612.17,337.97,7.86;14,151.52,623.13,329.07,7.86;14,151.52,634.09,135.39,7.86" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="14,212.45,612.17,268.14,7.86;14,151.52,623.13,35.49,7.86">liar, liar pants on fire&quot;: A new benchmark dataset for fake news detection</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,329.44,623.13,19.07,7.86">ACL</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Kan</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="422" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,645.84,337.98,7.86;14,151.52,656.80,329.07,7.86;15,151.52,119.67,329.07,7.86;15,151.52,130.63,329.07,7.86;15,151.52,141.59,149.49,7.86" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="14,309.53,645.84,171.07,7.86;14,151.52,656.80,153.91,7.86">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,326.50,656.80,154.09,7.86;15,151.52,119.67,329.07,7.86;15,151.52,130.63,110.66,7.86">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="15,142.62,152.55,337.98,7.86;15,151.52,163.51,329.07,7.86;15,151.52,174.47,256.06,7.86" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="15,455.76,152.55,24.83,7.86;15,151.52,163.51,268.54,7.86">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,442.65,163.51,37.94,7.86;15,151.52,174.47,162.77,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="5753" to="5763" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
