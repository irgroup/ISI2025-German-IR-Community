<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.91,115.96,67.90,12.62;1,441.26,115.96,12.19,12.62;1,134.77,133.89,345.83,12.62;1,146.10,151.82,323.14,12.62;1,260.59,169.76,82.58,12.62">NLP&amp;IR</title>
				<funder ref="#_ebNs3CN">
					<orgName type="full">Spanish Ministry of Science and Innovation</orgName>
				</funder>
				<funder ref="#_53fcNve #_BGWnF5x #_P2S5wKV #_FMuC3kB #_h7dHJxa">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,148.85,207.98,99.80,8.74"><forename type="first">Juan</forename><forename type="middle">R</forename><surname>Martinez-Rico</surname></persName>
							<email>jrmartinezrico@invi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">Universidad Nacional de Educación a Distancia (UNED)</orgName>
								<address>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.20,207.98,67.77,8.74"><forename type="first">Lourdes</forename><surname>Araujo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">Universidad Nacional de Educación a Distancia (UNED)</orgName>
								<address>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Instituto Mixto de Investigación</orgName>
								<orgName type="institution">-Escuela Nacional de Sanidad (IMIENS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.24,207.98,92.46,8.74"><forename type="first">Juan</forename><surname>Martinez-Romo</surname></persName>
							<email>juaner@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">Universidad Nacional de Educación a Distancia (UNED)</orgName>
								<address>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Instituto Mixto de Investigación</orgName>
								<orgName type="institution">-Escuela Nacional de Sanidad (IMIENS)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.91,115.96,67.90,12.62;1,441.26,115.96,12.19,12.62;1,134.77,133.89,345.83,12.62;1,146.10,151.82,323.14,12.62;1,260.59,169.76,82.58,12.62">NLP&amp;IR</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">36453AC21D155A391881719E0B0531A9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>check-worthiness</term>
					<term>claim retrieval</term>
					<term>embeddings</term>
					<term>graph features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Check-Worthiness and Claim Retrieval are two of the first tasks to be performed on the Fake News detection pipeline. In this article we present our approach to these tasks presented in the 2020 edition of the CheckThat! Lab. In the task 1, Tweet Check-Worthiness English, we propose a Bi-LSTM model with Glove Twitter embeddings where the number of inputs has been increased with a graph generated from the additional information provided for each tweet. In task 1 Arabic we have followed a similar approach but using a feed forward neural network model with Arabic embeddings. For the task 5, Debate Check-Worthiness, we propose a naive Bi-LSTM model with Glove embeddings. Finally, our approach to the task 2, Claim Retrieval, is based in a feed forward neural network model with features such as cosine similarity over Universal Sentence Encoder embeddings of tweets and claims, and other linguistic features extracted from both elements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the problems that current society faces is the spreading of fake news. The influence of this type of news in the results of the United States presidential elections a few years ago, and more recently, the disinformation that they are causing in the current pandemic of COVID2019 are two examples of this phenomenon. To combat this problem, sites dedicated to checking the veracity of the news that circulate daily in traditional media and on social networks have proliferated. These sites normally carry out their work through human experts who review the news in circulation every day. To facilitate the work of these experts, some systems that prioritize the claims to be verified or return a list of claims that verify another given as input have been proposed. On the other hand, these two tasks can also be part of a larger system of claim verification and detection of fake news.</p><p>This article describes the participation of the NLP&amp;IR@UNED<ref type="foot" coords="2,436.70,226.36,3.97,6.12" target="#foot_0">3</ref> team in tasks T1, T2 and T5 of the CheckThat! Lab at CLEF2020 <ref type="bibr" coords="2,384.18,239.89,14.09,8.74" target="#b0">[1]</ref> <ref type="bibr" coords="2,398.27,239.89,14.09,8.74" target="#b1">[2]</ref>. Tasks T1 and T5 are dedicated to prioritizing claims to be verified (check-worthiness) and task T2 is a claim retrieval task.</p><p>The rest of the article is organized as follows: in section 2 we describe the different approaches that we have tried to face each task, in section 3 we discuss the results we have obtained, and section 4 is devoted to reflect our conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task 1 -Tweet Check-Worthiness in English and Arabic</head><p>The purpose of this task is, given a stream of tweets and one or more topics, sort the tweets according to their check-worthiness for the topic to which they are supposed to belong. Three topics have been provided for the Arabic language: "The protests in Lebanon", "Wassim Youssef" and "Turkey enters Syria", and all English language tweets belong to topic "COVID19". The official evaluation measure for Arabic language is P@30 and for English language is MAP.</p><p>To address these two versions of the task 1, five different models have been analyzed. All models use cross entropy as a loss function and accuracy as a metric <ref type="foot" coords="2,162.47,504.26,3.97,6.12" target="#foot_1">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model 1</head><p>The first of these models<ref type="foot" coords="2,291.01,540.87,3.97,6.12" target="#foot_2">5</ref> is a feed forward neural network (FFNN) whose input is made up of word embeddings of the first n words of the tweet text, followed by a 1D global max pooling layer that operates on the n word vectors, a hidden layer, and a sigmoid final layer of size 1 that provides the check-worthiness score between 0 and 1.</p><p>Embedding features Word embeddings can be self-generated during training, or they can be preloaded at startup. For this second option, English pretrained Twitter Glove <ref type="bibr" coords="3,193.83,142.90,13.21,8.74" target="#b2">[3]</ref> embeddings of dimension 200 <ref type="foot" coords="3,340.04,141.33,3.97,6.12" target="#foot_3">6</ref> and Glove Arabic embeddings of dimension 256<ref type="foot" coords="3,208.16,153.28,3.97,6.12" target="#foot_4">7</ref> have been used respectively in each version of task 1.</p><p>Additionally, in the Arabic version of task 1, the title and the description of the topic to which each tweet belongs have been concatenated to the text of the tweet to form the model input.</p><p>To preprocess the raw Arabic text, we use as tokenizer the simple word tokenize from Camel Tools <ref type="bibr" coords="3,210.00,214.64,12.19,8.74" target="#b3">[4]</ref>. The first 25 tokens of each tweet and the first 100 tokens of the topic title and topic description concatenation have been selected to form an input of size 125.</p><p>For the English language, the NLTK<ref type="foot" coords="3,314.16,248.93,3.97,6.12" target="#foot_5">8</ref>  <ref type="bibr" coords="3,318.63,250.50,10.52,8.74" target="#b4">[5]</ref> tokenizer has been used, and the first 50 tokens have been selected as input.</p><p>Graph features Taking advantage of the fact that the organizers have provided the complete information of each tweet in a json file, we have implemented a process to increase the size of the input with tweets related to the current tweet. To do this, from the information of the tweets contained in the training, dev and test datasets, we extract triples of type tweet-hashtags-hashtag, tweet-quoted-tweet, tweet-reply status-tweet, tweet-contain url-url and tweet-has mention-user, and build a graph per dataset with these triples.</p><p>After this, for each tweet present in the datasets, we search for the first three tweets that are neighbors of the current one, for example, we would go from the tweet node to a hashtag node and from this we would select three tweet nodes. If there were no hashtag neighbor nodes or the hashtag neighbor nodes did not have three or more related tweet nodes, from the initial tweet node we search for tweet nodes behind relations quoted, reply status, contain url and has mention in this order. The result is that the texts of up to three tweets can be concatenated to the text of the original tweet.</p><p>As we will see later in section 3.1, the model can be executed indistinctly with the inputs from a single instance, or with the inputs concatenated from several instances if we make use of the tweets graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model 2</head><p>The second model is a CNN fed by the same features as Model 1. The input and embedding layers are followed by two pairs of convolutional 1D and max pooling 1D layers, a flatten layer that feeds a dense layer, and finally a dense sigmoid layer of size 1 at the output. Each convolutional 1D layer has a kernel size of 5 and the max pooling 1D layers have a pool size of 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model 3</head><p>The third model is a LSTM network fed by the same features as Model 1. In this case, after the input and embedding layers there is a LSTM layer that feds directly the dense sigmoid layer of size 1 that forms the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model 4</head><p>The fourth model is a Bi-LSTM network fed by the same features that previous models. After the input and embedding layers there are two bidirectional LSTM layers followed by a dense layer that precedes the output sigmoid layer.</p><p>Model 5 The last model is again a FFNN but in this case the input is made up of tf-idf vectors. To build this input we have followed the same strategy that in the embedding and graph features of previous models: in Arabic language the title and the description of the topic have been concatenated to the twitter text, and up to three additional tweets have been added to the input from the graph extracted from the tweet information.</p><p>The network is made up of three pairs of dense and batch normalization layers and the size of the dense layers decreases by 50% in each stage. These six layers are followed by a batch normalization layer, a dropout layer, and finally a sigmoid output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task 5 -Debate Check-Worthiness in English</head><p>The objective of this task is, given a transcripted political debate segmented into sentences and with the speakers annotated, sort the sentences according to their check-worthiness. The official evaluation measure for this task is MAP.</p><p>In this task, the five models described in section 2.1 have been used with slight variations. First, FFNN models have also been run without a hidden layer. On the other hand, in addition to word embeddings and tf-idf vectors, another type of input data has been used in this model as we explain below.</p><p>To perform a text analysis of the sentences we have prepared a version of the English Regressive Imagery Dictionary (RID) <ref type="bibr" coords="4,342.78,441.22,12.91,8.74" target="#b5">[6]</ref> <ref type="bibr" coords="4,355.68,441.22,12.91,8.74" target="#b6">[7]</ref> with a format compatible with that used by liwc python module <ref type="foot" coords="4,297.44,451.60,3.97,6.12" target="#foot_6">9</ref> . This dictionary contains 3150 words and roots in 48 categories, and these in turn are grouped into three main categories: primary, secondary and emotion.</p><p>The input vector consists of 51 decimal numbers, one for each category. For each instance of the training and test datasets, we calculate the percentage of words that are in those 51 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Task 2 -Claim Retrieval in English</head><p>In this task, for each check-worthy tweet provided, a ranked list of claims must be returned, reflecting which claims best support that tweet. The official evaluation measure for this task is MAP@5.</p><p>In our approach we have used an FFNN similar to model 5 described above, and for this model we build a dataset in the following way: for each claim we calculate its sentence embedding v c with Universal Sentence Encoder <ref type="bibr" coords="4,444.02,624.83,13.41,8.74" target="#b7">[8]</ref>, the ratio of different tokens to total tokens rt c , the average number of characters per word avc c , the number of verbs vn c , the number of nouns nn c , the ratio of content words 10 rcw c regarding the total number of words, and the ratio of content tags 11 rct c with respect to the total of tags.</p><p>The same values v t , rt t , avc t , vn t , nn t , rcw t , rct t are calculated for each tweet, and for each claim title v ct , rt ct , avc ct , vn ct , nn ct , rcw ct , rct ct .</p><p>Combining claims and tweets and claim titles and tweets we obtain the features for our dataset shown in table <ref type="table" coords="5,294.41,191.01,3.87,8.74" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim -Tweet</head><p>Claim title -Tweet  3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task 1 -Tweet Check-Worthiness in English and Arabic</head><p>In task 1, for both, English and Arabic languages, we have increased the size of the input by relying on the creation of a graph with which we retrieve tweets related to each instance of the datasets based on the hypothesis that the relationships tweet-hashtags-hashtag, tweet-quoted-tweet, tweet-reply status-tweet, tweetcontain url-url and tweet-has mention-user can enrich the information provided to the different classifiers. To verify this, we have performed a grid search with different parameters that were applied or not according to the model used. These parameters have been: graph generated inputs, pretrained embeddings, number of epochs, batch size, hidden layer size, activation type, optimizer type, dropout, number of epochs and batch size. All models use the adam optimizer with its default parameter values except the model 4 which uses nadam.</p><p>In Arabic there was no dev dataset so we extracted 20% of the training dataset instances as a dev dataset.</p><p>Table <ref type="table" coords="6,176.93,118.99,4.98,8.74" target="#tab_1">2</ref> shows the best results obtained for the different combinations of the use of pretrained embeddings and graph features and the optimal parameters for each model in Arabic language on the dev dataset, and table <ref type="table" coords="6,424.71,142.90,4.98,8.74" target="#tab_3">3</ref>  After observing the results obtained for the Arabic language, we can see that in almost all cases the use of Glove Arabic embeddings improves the result, and that the expansion of inputs through the tweet graph improves five of the nine possible configurations, one of them (FFNN + Graphs + Embedings) being the one that obtains the best global result.</p><p>For the English language again it is confirmed that the use of Glove embeddings improve performance in all cases. The graph features do not show a homogeneous behavior, although the best value is obtained for the Bi-LSTM model with embeddings and graph features.</p><p>Regarding the official results, in Arabic language our best run was the Bi-LSTM model with Glove Arabic embeddings and graph features, sent as con-trastive2 which obtained a P@30 of 0.5333, ranking 13th out of 28 runs sent by all the teams, while the FFNN (primary) and CNN (contrastive1 ) models with the same features obtained a P@30 of 0.3917 (ranking 19th) and 0.4833 (ranking 16th) respectively. In English language our best run was the Bi-LSTM model (primary) which obtained a MAP of 0.6069 (ranking 20th) and the FFNN (con-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task 5 -Debate Check-Worthiness in English</head><p>In task 5, different parameters and settings have also been analyzed. In this case, a dev dataset was not available either, so we have partitioned the training dataset leaving 20% of it as a dev dataset. Given the great imbalance of classes (4027 negative instances and 42 positive instances) we have opted for an oversampling strategy to equalize the number of positive and negative instances. In this task, an additional FFNN model 6 that makes use of features derived from a RID text analysis has been used. Table <ref type="table" coords="7,176.93,559.58,4.98,8.74" target="#tab_5">4</ref> shows the best results obtained for the different combinations of the use of pretrained embeddings and oversampling, and the optimal parameters for each model. All models use the adam optimizer with its default parameter values.</p><p>As we can see, the use of oversampling in general does not improve the behavior of the different models. The best results in FFNN models are obtained with the use of RID-based features without oversampling, outperforming FFNN models with inputs based on embeddings and TF/IDF vectors. It is also clearly seen, as was the case in task 1, that the use of 6B-100D Glove pretrained embed-dings substantially improve the performance of all the models in which it can be used.</p><p>The model that outperforms the rest by far is the Bi-LSTM with Glove embeddings. All of the runs that we submitted for task 5 were based on this model. The contrastive2 run used oversampling, while the primary and con-trastive1 runs did not use oversampling, and shared the same parameters. The only difference between them was a different random weight initialization.   <ref type="table" coords="8,177.19,632.21,4.98,8.74" target="#tab_7">5</ref> shows the official results of this task. Our primary and contrastive1 runs based on the Bi-LSTM model with Glove embeddings obtained the first positions in the classification.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task 2 -Claim Retrieval in English</head><p>In this task we have used an FFNN with 1000 elu 12 units in the first hidden layer, 500 elu units in the second hidden layer and 250 elu units in the last hidden layer, and Universal Sentence Encoder embeddings of tweets, claims and claim titles. We have used the adam optimizer with its default values and we have trained the model for 50 epochs with a batch size of 128.</p><p>In our experiments on the dev dataset we were able to verify that the use of the features derived from the claim title do not provide improvements in the performance offered by the claim-tweet features. Table <ref type="table" coords="9,358.88,383.05,4.98,8.74" target="#tab_9">6</ref> shows the results obtained with both configurations. Both sets of features exceed the MAP@5 of 0.609 obtained by the baseline based on Elasticsearch provided by the organization.</p><p>We have submitted three runs with two different settings. In the first one, sent as primary, we have made use of the seven features described in section 2.3 involving claims and tweets. With this configuration we obtained a MAP@5 of 0.8560, placing our team in fourth place.</p><p>In the contrastive1 run we used all features and the contrastive2 was identical to the primary run but with a different random initialization. With these two configurations we obtained respectively a MAP@5 of 0.8390 and 0.8550, confirming that the seven Claim title -Tweet features do not improve performance when used together with the seven Claim -Tweet features.  In this paper, we present our approximation to the tasks tweet check-worthiness, debate check-worthiness and claim retrieval at the CLEF-2020 Check-That! Lab.</p><p>Examining the results obtained in the two check-worthiness tasks to which we have participated, three if we take into account that the first was defined in two different languages, we can see that the use of pretrained embeddings of the appropriate language, significantly improves the performance of the models with respect to generating these embeddings during the training phase.</p><p>In the two versions of task 1, we have used a particular method to increase the information that reaches the input of the models, using a graph constructed from the tweets provided in the datasets that collects the relationships tweethashtags-hashtag, tweet-quoted-tweet, tweet-reply status-tweet, tweet-contain urlurl and tweet-has mention-user, between tweets. Although this mechanism has not behaved in a homogeneous way throughout the different models, it has been the one that has obtained the highest MAP values in the dev dataset in both the Arabic and English versions of this task.</p><p>In task 5 we have made use of RID-based features and, although we have not sent any run with them, in our tests with FFNN models on the dev dataset, we have obtained good results compared to embeddings and tf-idf based features, so this type of text analysis-based features can be an alternative to more wellknown ones like LIWC <ref type="bibr" coords="10,227.94,368.87,15.10,8.74" target="#b8">[9]</ref>. We did not have these features prepared in time for task 1 but we think they can be applied to tweet texts and it will be a job to be done in the future.</p><p>MAP values in this task are certainly low. We think that the large class imbalance and the small number of positive instances can cause these instances to not be correctly characterized by the models.</p><p>In the claim retrieval task we have assumed that the similarity between claim, claim title and tweet would allow a selection of claims appropriate to the requirements of this task and for this, we have implemented a mixed strategy using sentence embeddings and stylometric features based on token counts and ratios, far exceeding the provided baseline with these features.</p><p>On the other hand, the balance of our participation in these tasks has been positive, obtaining first place in task 5, fourth in task 2 and more discreet results in task 1.</p><p>In future work we plan to continue experimenting with graph features to increase the size of the inputs or the number of training instances, combining this strategy with more sophisticated language representations such as BERT <ref type="bibr" coords="10,446.47,560.15,20.36,8.74" target="#b9">[10]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,190.42,242.99,104.59,7.86;5,315.26,242.99,109.34,7.86;5,190.42,259.43,54.72,7.86;5,315.26,259.43,59.82,7.86;5,190.42,275.87,73.42,7.86;5,315.26,275.87,78.17,7.86;5,190.42,292.31,62.13,7.86;5,315.26,292.31,66.88,7.86;5,190.42,308.75,64.95,7.86;5,315.26,308.75,69.70,7.86;5,190.42,325.19,75.18,7.86;5,315.26,325.19,70.76,7.86;5,190.42,341.62,66.01,7.86;5,315.26,341.62,79.94,7.86"><head></head><label></label><figDesc>simct = cosine sim(vc, vt) simctt = cosine sim(vct, vt) drtct = rtc-rtt dctctt = rtct-rtt davcct = avcc-avct davcctt = avcct-avct dvnct = vnc-vnt dvnctt = vnct-vnt dnnct = nnc-nnt dnnctt = nnct-nnt drcwct = rcwc-rcwt drctctt = rctct-rctt drctct = rctc-rctt drcwctt = rcwct-rcwt</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,255.31,372.72,104.75,8.74"><head>Table 1 :</head><label>1</label><figDesc>Task 2 features</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,142.90,349.73,305.45"><head>Table 2 :</head><label>2</label><figDesc>shows best results for English language. Task 1 -Arabic parameter analysis</figDesc><table coords="6,136.76,190.83,347.74,226.87"><row><cell>Model</cell><cell cols="7">Graph Emb. Activation H. layer Dropout Ep./Bch. MAP</cell></row><row><cell>Model 1</cell><cell>N</cell><cell>N</cell><cell>relu</cell><cell>2000</cell><cell>-</cell><cell>50/8</cell><cell>0.1175</cell></row><row><cell>(FFNN)</cell><cell>N</cell><cell>Y</cell><cell>relu</cell><cell>2000</cell><cell>-</cell><cell>50/8</cell><cell>0.1399</cell></row><row><cell></cell><cell>Y</cell><cell>N</cell><cell>relu</cell><cell>2000</cell><cell>-</cell><cell>50/8</cell><cell>0.1238</cell></row><row><cell></cell><cell>Y</cell><cell>Y</cell><cell>relu</cell><cell>2000</cell><cell>-</cell><cell>50/8</cell><cell>0.1454</cell></row><row><cell>Model 2</cell><cell>N</cell><cell>N</cell><cell>relu</cell><cell>100</cell><cell>-</cell><cell>25/8</cell><cell>0.1201</cell></row><row><cell>(CNN)</cell><cell>N</cell><cell>Y</cell><cell>relu</cell><cell>100</cell><cell>-</cell><cell>25/8</cell><cell>0.1436</cell></row><row><cell></cell><cell>Y</cell><cell>N</cell><cell>relu</cell><cell>100</cell><cell>-</cell><cell>25/8</cell><cell>0.1225</cell></row><row><cell></cell><cell>Y</cell><cell>Y</cell><cell>relu</cell><cell>100</cell><cell>-</cell><cell>25/8</cell><cell>0.1313</cell></row><row><cell>Model 3</cell><cell>N</cell><cell>N</cell><cell>tanh</cell><cell>-</cell><cell>0.1</cell><cell>5/32</cell><cell>0.0646</cell></row><row><cell>(LSTM)</cell><cell>N</cell><cell>Y</cell><cell>tanh</cell><cell>-</cell><cell>0.1</cell><cell>5/32</cell><cell>0.0821</cell></row><row><cell></cell><cell>Y</cell><cell>N</cell><cell>tanh</cell><cell>-</cell><cell>0.1</cell><cell>5/32</cell><cell>0.0634</cell></row><row><cell></cell><cell>Y</cell><cell>Y</cell><cell>tanh</cell><cell>-</cell><cell>0.1</cell><cell>5/32</cell><cell>0.0797</cell></row><row><cell>Model 4</cell><cell>N</cell><cell>N</cell><cell>tanh</cell><cell>10</cell><cell>0.1</cell><cell>5/32</cell><cell>0.1145</cell></row><row><cell>(Bi-LSTM)</cell><cell>N</cell><cell>Y</cell><cell>tanh</cell><cell>10</cell><cell>0.1</cell><cell>5/32</cell><cell>0.1131</cell></row><row><cell></cell><cell>Y</cell><cell>N</cell><cell>tanh</cell><cell>10</cell><cell>0.1</cell><cell>5/32</cell><cell>0.1211</cell></row><row><cell></cell><cell>Y</cell><cell>Y</cell><cell>tanh</cell><cell>10</cell><cell>0.1</cell><cell>5/32</cell><cell>0.1210</cell></row><row><cell>Model 5</cell><cell>N</cell><cell>-</cell><cell>relu</cell><cell>500</cell><cell>0.4</cell><cell>10/8</cell><cell>0.0665</cell></row><row><cell>(FFNN TF/IDF)</cell><cell>Y</cell><cell>-</cell><cell>relu</cell><cell>500</cell><cell>0.4</cell><cell>10/8</cell><cell>0.0640</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,134.77,367.91,345.83,57.06"><head>Table 3 :</head><label>3</label><figDesc>Task 1 -English parameter analysis</figDesc><table coords="7,134.77,404.27,345.83,20.69"><row><cell>trastive1 ) and CNN (contrastive2 ) models obtained a MAP of 0.5546 (ranking</cell></row><row><cell>22th) and 0.5193 (ranking 23th), respectively.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,149.71,577.32,255.32,63.62"><head>Table 4 :</head><label>4</label><figDesc>Task 5 -English parameter analysis Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,237.70,247.56,139.94,8.74"><head>Table 5 :</head><label>5</label><figDesc> </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="9,133.85,615.24,296.68,49.42"><head>Table 6 :</head><label>6</label><figDesc>Task 2 -Feature set comparison on dev dataset</figDesc><table /><note coords="9,133.85,655.03,7.31,5.24;9,144.73,656.80,96.15,7.86"><p>12 Exponential linear unit.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,612.96,178.21,7.86"><p>Identified as NLPIR01 in the official results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,144.73,623.92,335.86,7.86;2,144.73,634.88,79.87,7.86"><p>We plan to release the source code at https://github.com/jrmtnez/NLP-IR-UNEDat-CheckThat-2020.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="2,137.50,644.07,3.65,5.24;2,144.73,645.84,262.10,7.86;2,144.73,656.80,111.25,7.86"><p><ref type="bibr" coords="2,137.50,644.07,3.65,5.24" target="#b4">5</ref> All models have been implemented in Tensorflow 2.1 with Keras: https://www.tensorflow.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="3,144.73,634.88,164.03,7.86"><p>https://nlp.stanford.edu/projects/glove/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="3,144.73,645.84,186.78,7.86"><p>https://github.com/tarekeldeeb/GloVe-Arabic</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="3,144.73,656.80,91.20,7.86"><p>https://www.nltk.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6" coords="4,144.73,656.80,123.22,7.86"><p>https://pypi.org/project/liwc/</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work has been partially supported by the <rs type="funder">Spanish Ministry of Science and Innovation</rs> within the projects <rs type="projectName">PROSA-MED</rs> (<rs type="grantNumber">TIN2016-77820-C3-2-R</rs>), <rs type="projectName">DOTT-HEALTH</rs> (<rs type="grantNumber">PID2019-106942RB-C32</rs>) and <rs type="projectName">EXTRAE II</rs> (<rs type="grantNumber">IMIENS 2019</rs>). NNP", "<rs type="projectName">NNPS</rs>", "<rs type="programName">VB", "VBD", "VBG", "VBN", "VBP", "VBZ"</rs>, "JJ", "<rs type="programName">JJR"</rs>, "<rs type="programName">JJS", "RB", "RBR", "RBS</rs>" and "<rs type="projectName">WRB</rs>"</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ebNs3CN">
					<idno type="grant-number">TIN2016-77820-C3-2-R</idno>
					<orgName type="project" subtype="full">PROSA-MED</orgName>
				</org>
				<org type="funded-project" xml:id="_53fcNve">
					<idno type="grant-number">PID2019-106942RB-C32</idno>
					<orgName type="project" subtype="full">DOTT-HEALTH</orgName>
				</org>
				<org type="funded-project" xml:id="_BGWnF5x">
					<idno type="grant-number">IMIENS 2019</idno>
					<orgName type="project" subtype="full">EXTRAE II</orgName>
				</org>
				<org type="funded-project" xml:id="_P2S5wKV">
					<orgName type="project" subtype="full">NNPS</orgName>
					<orgName type="program" subtype="full">VB&quot;, &quot;VBD&quot;, &quot;VBG&quot;, &quot;VBN&quot;, &quot;VBP&quot;, &quot;VBZ&quot;</orgName>
				</org>
				<org type="funding" xml:id="_FMuC3kB">
					<orgName type="program" subtype="full">JJR&quot;</orgName>
				</org>
				<org type="funded-project" xml:id="_h7dHJxa">
					<orgName type="project" subtype="full">WRB</orgName>
					<orgName type="program" subtype="full">JJS&quot;, &quot;RB&quot;, &quot;RBR&quot;, &quot;RBS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,612.96,337.64,7.86;10,151.52,623.92,329.07,7.86;10,151.52,634.88,329.07,7.86;10,151.52,645.84,329.07,7.86;10,151.52,656.80,213.23,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,400.95,634.88,79.64,7.86;10,151.52,645.84,324.51,7.86">Overview of Check-That! 2020: Automatic Identification and Verification of Claims in Social Media</title>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barron-Cedeno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maram</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reem</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fatima</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolay</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bayan</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Hamdan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaden</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zien</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07997[cs</idno>
		<idno>arXiv: 2007.07997</idno>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,119.67,337.64,7.86;11,151.52,130.63,329.07,7.86;11,151.52,141.59,329.07,7.86;11,151.52,152.55,329.07,7.86;11,151.52,163.51,329.07,7.86;11,151.52,174.47,329.07,7.86;11,151.52,185.43,100.55,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,392.00,130.63,88.59,7.86;11,151.52,141.59,329.07,7.86;11,151.52,152.55,22.83,7.86">CheckThat! at CLEF 2020: Enabling the Automatic Identification and Verification of Claims in Social Media</title>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maram</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reem</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fatima</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Haouari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,362.02,163.51,118.58,7.86;11,151.52,174.47,24.37,7.86">Advances in Information Retrieval</title>
		<title level="s" coord="11,182.35,174.47,141.05,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Joemon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emine</forename><surname>Jose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">João</forename><surname>Yilmaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pablo</forename><surname>Magalhães</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Castells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mário</forename><forename type="middle">J</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Flávio</forename><surname>Silva</surname></persName>
		</editor>
		<editor>
			<persName><surname>Martins</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="499" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,196.39,337.64,7.86;11,151.52,207.34,329.07,7.86;11,151.52,218.30,299.46,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,425.25,196.39,55.34,7.86;11,151.52,207.34,122.17,7.86">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,291.70,207.34,188.89,7.86;11,151.52,218.30,197.73,7.86">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,229.26,337.63,7.86;11,151.52,240.22,329.07,7.86;11,151.52,251.18,329.07,7.86;11,151.52,262.14,57.34,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="11,448.72,240.22,31.87,7.86;11,151.52,251.18,324.98,7.86">CAMeL Tools: An Open Source Python Toolkit for Arabic Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">Ossama</forename><surname>Obeid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nasser</forename><surname>Zalmout</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salam</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dima</forename><surname>Taji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mai</forename><surname>Oudah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bashar</forename><surname>Alhafni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Go</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fadhl</forename><surname>Eryani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Erdmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,273.10,337.64,7.86;11,151.52,284.06,197.72,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,309.00,273.10,167.66,7.86">NLTK: The Natural Language Toolkit</title>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<idno type="arXiv">arXiv:cs/0205028</idno>
		<idno>arXiv: cs/0205028</idno>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,295.02,337.64,7.86;11,151.52,305.98,236.34,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="11,232.68,295.02,247.91,7.86;11,151.52,305.98,44.99,7.86">Romantic Progression: The Psychology of Literary History, Hemisphere</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Martindale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975. 1975</date>
			<publisher>Google Scholar</publisher>
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,316.93,337.64,7.86;11,151.52,327.89,329.07,7.86;11,151.52,338.85,105.64,7.86" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Martindale</surname></persName>
		</author>
		<title level="m" coord="11,227.37,316.93,253.22,7.86;11,151.52,327.89,208.23,7.86">The clockwork muse: The predictability of artistic change. The clockwork muse: The predictability of artistic change</title>
		<meeting><address><addrLine>New York, NY, US</addrLine></address></meeting>
		<imprint>
			<publisher>Basic Books</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page">411</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,349.81,337.64,7.86;11,151.52,360.77,329.07,7.86;11,151.52,371.73,258.66,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,462.93,360.77,17.66,7.86;11,151.52,371.73,92.22,7.86">Universal sentence encoder</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rhomni</forename><surname>St John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mario</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.96,382.69,337.63,7.86;11,151.52,393.65,323.66,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,464.71,382.69,15.88,7.86;11,151.52,393.65,222.01,7.86">The development and psychometric properties of LIWC</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryan</forename><forename type="middle">L</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kayla</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kate</forename><surname>Blackburn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="11,142.61,404.61,337.97,7.86;11,151.52,415.56,329.07,7.86;11,151.52,426.52,132.38,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="11,439.96,404.61,40.63,7.86;11,151.52,415.56,293.36,7.86">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
