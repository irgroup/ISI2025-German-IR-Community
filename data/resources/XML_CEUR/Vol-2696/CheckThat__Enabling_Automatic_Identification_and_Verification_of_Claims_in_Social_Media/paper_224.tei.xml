<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.88,115.96,343.60,12.62;1,135.40,133.89,344.55,12.62;1,187.84,151.82,239.67,12.62">EvolutionTeam at CLEF2020 -CheckThat! lab : Integration of linguistic and sentimental features in a fake news detection approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.38,189.79,74.31,8.74"><forename type="first">Ibtissam</forename><surname>Touahri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Sciences</orgName>
								<orgName type="institution">University Mohamed First</orgName>
								<address>
									<settlement>Oujda</settlement>
									<country key="MA">Morocco</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,314.38,189.79,83.60,8.74"><forename type="first">Azzeddine</forename><surname>Mazroui</surname></persName>
							<email>azze.mazroui@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Sciences</orgName>
								<orgName type="institution">University Mohamed First</orgName>
								<address>
									<settlement>Oujda</settlement>
									<country key="MA">Morocco</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.88,115.96,343.60,12.62;1,135.40,133.89,344.55,12.62;1,187.84,151.82,239.67,12.62">EvolutionTeam at CLEF2020 -CheckThat! lab : Integration of linguistic and sentimental features in a fake news detection approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C062400F43CBBB9DBD05A2D043A28D12</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fact-checking</term>
					<term>sentiment features</term>
					<term>unsupervised approach</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Misinformation is a growing problem around the web. The spread of such a phenomenon may impact public opinions. Hence fake news detection is indispensable. The first step for fact-checking is the selection of check worthy tweets for a certain topic, then ranking sentences from related web pages according to the carried evidence. Afterward, the claim will be verified according to evident sentences. At CLEF2020 -CheckThat! lab, three tasks run in Arabic, namely check-worthiness on tweets, evidence retrieval, and claim verification that corresponds respectively to task1, task3, and task4. We participated in the three tasks. We integrated manual sentiment features as well as named entities to detect fake news. The integration of sentiment information in the first task caused result degradation since there may be an overlap between check worthy and not check worthy tweets. For the second task, we explored the effect of sentiment presence and we used cosine similarity as a similarity measure between the claim and a specific snippet. The third task is a classification task based on sentiment and linguistic features to compute the overlap and the contradiction between the claim and the detected check worthy sentences. The results of task1 and task3 leave large room for improvement, whereas the results of task 4 are promising since our system reached 0.55 of F1-measure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Interaction with others online through social media has become indispensable. Social media are not only a way to communicate but also a warehouse of news used by people who seek news and information from social media rather than news organizations. People publish their opinions as if they were facts which can mislead the orientation of the public opinion and have negative effects on the psychology of the people. A high proportion of people is exposed to misleading or false claims. For example during coronavirus pandemic, claims about COVID-19 appeared without trustful reference. Many stories have been found such as the theory that the spread of COVID-19 is caused by 5G technology. The spread of such claims affected people understanding of the pandemic. Fake news has known explosive growth in recent years especially on social media where a large amount of data is uncontrolled. The extensive spread of this phenomenon may impact individuals and society negatively. In recent years, fake news appear to mislead the orientation of public opinion for commercial and political purposes. Facts are ignored when shaping public opinion, since appealing to emotions works better as it has a potential impact on the person. Social media publish fake news to affect reader psychology and hence increase readership. With offensive and deceptive words, social media users can get affected by these fake news easily, which brings tremendous effects on society. The identification of fake news is hard and time consuming. To improve information trustworthiness, we should build systems to detect fake news in real time. Thus, many studies addressed the automation of fake news detection process to facilitate their verification among which <ref type="bibr" coords="2,311.93,322.23,9.96,8.74" target="#b6">[7]</ref>, <ref type="bibr" coords="2,328.54,322.23,10.52,8.74" target="#b0">[1]</ref> and <ref type="bibr" coords="2,361.75,322.23,9.96,8.74" target="#b4">[5]</ref>.</p><p>In the following, we describe our participation in CLEF2020 -CheckThat! lab. The paper is organized as follows, we present previous works, afterward we define the tasks in which we participated, then we describe the external resources used by our system as well as the system approach and we give the obtained results for the classification task (task 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous works</head><p>This paper investigates the principal approaches used to define news factuality. In the following, we present some previous works that aimed to detect fake news. Hansen et al. <ref type="bibr" coords="2,199.10,476.79,10.52,8.74" target="#b4">[5]</ref> presented an automatic fact-checking system to detect fake news based on a neural ranking model to check sentence worthiness. The model represents each word in a sentence by both its embedding and syntactic dependencies aiming to capture both semantic and the role of syntax to affect the semantic of terms in the same sentence. The check worthiness ranking is based on a neural network model trained on large a amount of unlabelled data through weak supervision. Shu et al. <ref type="bibr" coords="2,182.11,560.48,10.52,8.74" target="#b8">[9]</ref> presented a review of detecting fake news on social media, they addressed their effect on psychology and social theories. They reported the representative datasets, existing algorithms from a data mining perspective, and the evaluation metrics used to detect fake news. They discussed the challenges of this task, related research areas, and future research directions for fake news detection on social media. Zafarani et al. <ref type="bibr" coords="2,199.85,632.21,15.50,8.74" target="#b10">[11]</ref> presented a paper that introduces the characteristics of fake news that differentiate it from similar concepts such as misinformation to present fake news detection strategies systematically.</p><p>Atanasova et al. <ref type="bibr" coords="3,207.43,118.99,10.52,8.74" target="#b0">[1]</ref> presented an overview of task1 of the CheckThat! Lab 2019. They reported that eleven teams out of 47 participating teams submitted runs. From the evaluation results, the best performing approaches used logistic regression and neural networks. The best system achieved a mean average precision of 0.166 . The obtained results need improvement, and hence the authors released all datasets and scoring scripts to enable further research in check-worthiness estimation. Hasanain et al. <ref type="bibr" coords="3,201.68,202.68,10.52,8.74" target="#b6">[7]</ref> presented an overview of Task 2 at CheckThat! Lab 2019. The authors provided an annotated Arabic dataset to detect fake news. They used normalized discounted cumulative gain (nDCG) for ranking and F1 for classification. They reported that four teams submitted runs. They released all the datasets and the evaluation scripts from the lab to enable further researches. Haouari et al. <ref type="bibr" coords="3,198.81,262.46,10.52,8.74" target="#b5">[6]</ref> presented their participation in Task 2 of CLEF-2019 Check-That! Lab. Their runs achieved the best performance in subtasks A and B. Whereas the runs of subtasks C and D, achieved the median performance among participating runs. Subtask B is a classification task, hence they proposed a classification model that uses source popularity features as well as named entities. Their model achieved an F1 score of 0.31. For subtask C, they used BOW and named entities to train a model, they achieved an F1 score of 0.4. For subtask D, they proposed a classification model based on sentiment features. Ghanem et al. <ref type="bibr" coords="3,200.67,358.10,10.52,8.74" target="#b2">[3]</ref> presented their participation at CheckThat!-2019 lab -Task 2 on Arabic claim verification. They proposed a cross-lingual approach to detect claims factuality. Their approach achieved 0.62 as F1 in subtask-D.</p><p>3 Tasks description CLEF2020 -CheckThat! lab proposed many tasks among which three tasks that run in Arabic. We have participated in the three tasks, namely task1, task3 and task4. In the following, we describe each task according to its presentation by the lab organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task1 : Tweet Check-Worthiness</head><p>The organizers gave a set of topics and their corresponding potentially-related tweets. This task aims to verify whether a tweet is check worthy. A tweet is considered check worthy if it carries harmful content or it is of interest to a large audience. This task is a ranking task that aims to rank the tweets according to their check-worthiness for the topic. The official measure used for evaluation is P@30 for the Arabic dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task3 : Evidence Retrieval</head><p>For this task, the organizers presented a set of topics and the corresponding claims and a set of text snippets extracted from potentially-relevant webpages. The task aims to return for a given claim a ranked list of evidence snippets that support or refute the claim, namely the ones that are useful in verifying it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task4 : Claim Verification</head><p>The task presents a dataset that contains 201 check-worthy claims related to 12 topics. For these topics, a set of potentially related web pages is given. The task aims to use the data to predict claims veracity. The task is a classical binary classification task that uses true or false tags to tag a specific claim according to its veracity. Precision, recall, and F1-measure are used as evaluation measures and the macro-averaged F1 is used as the official measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">External resources</head><p>This section aims to describe the resources used by our system besides the ones presented by the task organizers. We constructed four lexicons, namely sentiment, offense, sarcasm, and named entities lexicons. The lexicons are described in the following: Sentiment lexicon: the lexicon contains 9858 sentimental terms. This lexicon is a combination of many resources which are: Lexicon1(SemEval<ref type="foot" coords="4,217.72,325.63,3.97,6.12" target="#foot_0">1</ref> ): a lexicon that contains sentimental terms and their corresponding sentiment intensity. Lexicon2 (MPQA<ref type="foot" coords="4,214.90,349.54,3.97,6.12" target="#foot_1">2</ref> : the Arabic version of the original MPQA lexicon that contains sentimental terms. Lexicon3 (ENGAR): is an English sentiment lexicon created by <ref type="bibr" coords="4,425.56,375.02,10.52,8.74" target="#b7">[8]</ref> and then translated into Arabic by the authors of this paper. Lexicon4: a sentimental lexicon extracted from a corpus collected from Hespress<ref type="foot" coords="4,134.77,409.32,3.97,6.12" target="#foot_2">3</ref> Facebook page. The lexicons have been verified semantically by the authors of this paper. We give in Table <ref type="table" coords="4,194.27,434.80,4.98,8.74" target="#tab_0">1</ref> the statistics of sentiment lexicons. Offense lexicon: the offensive lexicon is sharper than the negative sentiment lexicon. We constructed a lexicon by extracting offensive terms from the offensive corpus collected by the organizers of the offensive language detection shared task <ref type="foot" coords="4,156.13,566.96,3.97,6.12" target="#foot_3">4</ref> . The lexicon contains 1120 offensive terms. Sarcasm lexicon: the lexicon contains 148 sarcasm indicators extracted manually from the ironic corpus that was created by <ref type="bibr" coords="4,324.28,592.45,9.96,8.74" target="#b3">[4]</ref>. Named entities lexicon : the lexicon contains the names of religions, countries and known personalities. The terms of this lexicon were collected by google queries and then were expanded by the authors of this paper. a) Religion lexicon: contains 9 religions which give 104 terms of the corresponding adjectives and nouns. b) Nationality lexicon: contains 194 countries. We enhanced the names of the countries by the corresponding nationalities. c) Named entities: contains named entities extracted from the offensive corpus. The terms target religions (</p><p>), countries ( ), backgrounds ( ), sports teams ( ), political parts ( ), genders ( ), famous personalities ( ).</p><p>The lexicon contains 216 terms. We give in Table <ref type="table" coords="5,211.42,247.41,4.98,8.74" target="#tab_1">2</ref> examples of the mentioned lexicons. The presence of sentiment and sarcasm terms within an expression may indicate that the expression is an opinion not a fact. The cause behind using offense lexicon is that it may define harmful expressions. We use named entities to define trustful sources since a text that contains named entities tends to be more factual. The lexicons have been created for sentiment analysis purposes, they have not been made publically available yet. We use them as external resources that cover a large set of sentiment terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">System approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Text extraction and preprocessing</head><p>We extract texts of tweets, claims, and snippets from the given JSON object using regular expressions. For task 4, instead of using Jsoup that is a java library that parses HTML documents as in <ref type="bibr" coords="5,295.11,529.65,15.50,8.74" target="#b9">[10]</ref> to extract the content of relevant web pages, we use the text snippets that were extracted from these pages. We give a standard representation to the extracted text, we preprocessed the claims and the text snippets extracted from potentially relevant webpages by removing all characters other than the Arabic letters. We tokenize each text into terms using space delimiter. Hence, each claim and text snippet will be represented by a set of terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Task1</head><p>In this task, we aim to rank the top 500 tweets related to each topic according to their check-worthiness. In the following, we use the resources created by our system to rank tweets besides other features. We use different features namely the title and the description of the topic, sentiment, offense and named entities features. We weigh each feature using a weight that represents its importance. F 1 : represents the weighted intersection between the topic title and tweet text. We give the title the weight 3 as it is the most important part of the content. F 2 : is the weighted intersection between the topic description and the tweet text. We give the description the weight 2 since it contains important information. F 3 : represents the occurrence of named entities in the tweet text. Texts that contain named entities are check worthy as they represent a trustful source of information. We give this feature the weight 1. F 4 : represents the occurrence of offense lexicon terms in the tweet text. The offense lexicon is an indicator of the presence of harmful content. We give this feature the same weight as named entities. F 5 : represents the weighted occurrence of sentiment terms in the tweet text. The text that contains sentiment lexicon tends to be an opinion not a fact. This feature is given a negative weight -1. All the features are given a positive weight except sentiment features since checkworthy tweets tend to be facts rather than opinions, hence we give a negative weight to the present sentiment terms. We give positive weight to offense feature since from the definition, check-worthy tweets are the ones that carry harmful content. Since the title and description are related to the topic, then each tweet is given a score based on the product of the mentioned features weights and their intersection with the tweet text. In other words, whenever a topic title or description term matches tweet text term, we increment the value by 3, 2 or 1 according to the corresponding weight for each feature. The same for other features. The score is then divided by the sum of feature sizes which gives a normalized score. Table <ref type="table" coords="6,176.58,442.03,4.98,8.74" target="#tab_2">3</ref> gives an example of initial values. Using Formula (1) we compute the normalized score for each tweet. L i is the length of each lexicon. The statistics of each lexicon are given in Table <ref type="table" coords="6,283.68,465.94,3.87,8.74" target="#tab_1">2</ref>. </p><formula xml:id="formula_0" coords="6,334.14,649.70,146.45,9.65">F i = Intersection i × W eight i (1)</formula><p>This approach showed degraded results in comparison to the approach [10] that uses only document parts as features to rank web pages. In the official results it reached only 0.28 using P@30 which was under the baseline. The reason for this may be the intersection between check worthy and less check worthy terms that match the extracted features which means that they may characterize both of which or rather the selected features match more less check worthy tweets which made the ranking difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Task3</head><p>We aim to rank a list of text snippets based on their evidence namely their usefulness for fact checking. We extract a text snippet and compare it with a tweet text. Then we affect a score to each snippet on the results of (2) that gathers cosine similarity between the tweet and the text snippet and weighs using a negative weight the intersection between a specific snippet and the sentiment lexicon. In other words if five sentiment terms are present in the text snippet, then the intersection is five. The negative weight is given to differentiate between facts and opinions. We rank the top 100 evidence text snippets corresponding to each tweet based on the relation (2) and also using only on the cosine similarity score. When multiple snippets have equal cosine similarity score we break the tie by considering both of which if they are ranked with the top 100. Using cosine similarity only shows better results than adding sentiment information. This may be explained by the fact that sentiment terms may appear in a factual text without the intention of the holder to express an opinion. However the obtained results for this task were degraded by reaching 0.05 only using P@10 metric. Score = cosineSimilarity -0.5 × intersection</p><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Task4</head><p>In this task, we aim to classify claims as true or false. We compare each claim with the text snippets extracted from the relevant web pages. We calculate factuality based on a snippet information using two values identical and opposite, we define each of which by: Identical : represents the concordance between the claim terms and the text terms.</p><p>Opposite: is a negative value that represents the number of claim terms which opposite appear in the text.</p><p>In order to define the negated terms we use a list of negation words. If a claim term matches a text term then based on table 4, we can define whether they are identical. We use snippets extracted from potentially relevant Web pages. For a given Web page, if a snippet supports the claim as none of its terms are negated, whereas, a second snippet contradicts the claim as one or many of its terms among the ones that match claim terms are negated. Then the claim is false at the current Web page level. According to the relation (3), if the factuality is greater than 0, then the claim is true. Unless, the factuality will be negative which means the presence of snippets opposite to the claim. Thus, wherever our system finds contradicting snippets, it tags the claim as false, otherwise, it gives it true tag. This threshold has been chosen since according to Baly et al. in <ref type="bibr" coords="8,470.07,334.82,10.52,8.74" target="#b1">[2]</ref> a major part of documents which represent snippets can support true claims, however, a major part can support also false claims this means that even when enlarging the threshold we will encounter the mentioned constraint and hence we chose the presence of opposite snippets as an indicator. Then the factuality of a claim according to the potentially relevant Web pages is the major score of true and false values of the initial factuality Factuality Initial calculated using each Web page. In other words if the Factuality Initial is true according to two Web pages and false according to three Web pages then the factuality of the claim is false.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F actuality</head><formula xml:id="formula_1" coords="8,267.56,467.56,213.03,9.65">Initial = Identical × opposite<label>(3)</label></formula><p>In table <ref type="table" coords="8,187.07,487.13,4.98,8.74" target="#tab_4">5</ref> we give the results of Task4 using the mentioned criteria. The second test is based on the following criterion, if the factuality is false according to the aforementioned criteria or a sarcasm indicator is present in the text, then the claim is false. The presence of sarcasm features augments the probability of the analyzed sentence to be fake. The second test uses a list of negation terms that contains 189 terms. Adding sarcasm features doesn't generate any improvement, which may be due to the weak intersection between sarcasm lexicon and the analyzed text and hence we don't report the results of the corresponding test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we presented our participation in CLEF2020 -CheckThat! lab. We aimed to build a sentiment aware fake news detection system. We participated in three tasks, we based approaches on various mathematical dependencies. We enhanced the used approach by adding sentiment features to define the impact of it on the detection of fake news. The challenge of this paper wasn't the integration of sentiment features only, but also we aimed to base our system on an unsupervised approach to overcome the difficulties of datasets collection and annotation and also to reduce the time of fact-checking taken when building supervised models. The obtained results in the classification task are promising, however, there is a large room for improvement when it comes to ranking tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,143.40,466.43,323.06,38.27"><head>Table 1 .</head><label>1</label><figDesc>Statistics of sentiment lexicons</figDesc><table coords="4,143.40,485.48,323.06,19.22"><row><cell>Lexicon</cell><cell cols="5">Lexicon1 Lexicon2 Lexicon3 Lexicon4 Total</cell><cell>Total unique</cell></row><row><cell cols="2">Statistics 980</cell><cell>4166</cell><cell>3504</cell><cell>1778</cell><cell>10428</cell><cell>9858</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,195.80,277.15,212.88,62.80"><head>Table 2 .</head><label>2</label><figDesc>Statistics and examples of lexicon terms</figDesc><table coords="5,195.80,297.94,210.22,42.00"><row><cell>Lexicon</cell><cell cols="2">Sentiment Offense</cell><cell cols="2">Sarcasm Named</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>entities</cell></row><row><cell>Size</cell><cell>9858</cell><cell>1120</cell><cell>148</cell><cell>514</cell></row><row><cell>Example</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,151.46,497.93,277.21,171.33"><head>Table 3 .</head><label>3</label><figDesc>Example of initial values</figDesc><table coords="6,151.46,518.73,277.21,150.53"><row><cell cols="3">Feature</cell><cell cols="3">Topic title Topic de-</cell><cell>Sentiment Offense</cell><cell>Named</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>scription</cell><cell></cell><cell>entities</cell></row><row><cell cols="2">Size</cell><cell></cell><cell>12</cell><cell>36</cell><cell></cell><cell>100</cell><cell>50</cell><cell>10</cell></row><row><cell cols="3">Weight</cell><cell>3</cell><cell>2</cell><cell></cell><cell>-1</cell><cell>1</cell><cell>1</cell></row><row><cell cols="4">Intersection 2</cell><cell>5</cell><cell></cell><cell>3</cell><cell>1</cell><cell>2</cell></row><row><cell cols="3">with the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">tweet</cell><cell></cell><cell></cell><cell></cell></row><row><cell>5 n=1 F i 5 n=1 L i</cell><cell>=</cell><cell cols="3">6 + 10 -3 + 1 + 2 12 + 36 + 100 + 50 + 10</cell><cell cols="2">=</cell><cell>16 208</cell><cell>;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,160.37,115.91,289.66,94.66"><head>Table 4 .</head><label>4</label><figDesc>Concordance between claim and text terms</figDesc><table coords="8,160.37,134.97,289.66,75.61"><row><cell></cell><cell>Claim term</cell><cell></cell><cell>Text term</cell></row><row><cell></cell><cell>Negated</cell><cell></cell><cell>Negated</cell><cell>Concordance</cell></row><row><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>No</cell></row><row><cell>*</cell><cell></cell><cell>*</cell><cell></cell><cell>Identical</cell></row><row><cell>*</cell><cell></cell><cell></cell><cell>*</cell><cell>Opposite</cell></row><row><cell></cell><cell>*</cell><cell>*</cell><cell></cell><cell>Opposite</cell></row><row><cell></cell><cell>*</cell><cell></cell><cell>*</cell><cell>Identical</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,160.37,519.45,263.53,60.19"><head>Table 5 .</head><label>5</label><figDesc>Task 4 official results</figDesc><table coords="8,160.37,538.50,263.53,41.14"><row><cell>Class</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>FALSE</cell><cell>0.9273</cell><cell>0.1250</cell><cell>0.1667</cell><cell>0.1429</cell></row><row><cell>TRUE</cell><cell></cell><cell>0.9682</cell><cell>0.9560</cell><cell>0.9620</cell></row><row><cell>Average</cell><cell></cell><cell>0.5466</cell><cell>0.5613</cell><cell>0.5524</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.73,623.92,220.00,7.86"><p>http://www.saifmohammad.com/WebPages/SCL.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,634.88,144.18,7.86"><p>http://www.purl.org/net/ArabicSA</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.73,645.84,144.65,7.86"><p>https://fr-fr.facebook.com/Hespress</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.73,656.80,205.95,7.86"><p>https://sites.google.com/site/offensevalsharedtask/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,368.98,337.63,7.86;9,151.52,379.94,329.07,7.86;9,151.52,390.90,284.57,7.86" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<title level="m" coord="9,151.52,379.94,329.07,7.86;9,151.52,390.90,139.57,7.86;9,312.51,390.90,94.91,7.86">Overview of the clef-2019 checkthat! lab: Automatic identification and verification of claims. task 1: Check-worthiness</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>CLEF (Working Notes)</note>
</biblStruct>

<biblStruct coords="9,142.96,402.22,337.64,7.86;9,151.52,413.18,329.07,7.86;9,151.52,424.14,97.80,7.86" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08012</idno>
		<title level="m" coord="9,469.08,402.22,11.52,7.86;9,151.52,413.18,261.97,7.86">Integrating stance detection and fact checking in a unified corpus</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.96,435.46,337.63,7.86;9,151.52,446.41,329.07,7.86;9,151.52,457.37,137.66,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,151.52,446.41,324.71,7.86">Upv-uma at checkthat! lab: Verifying arabic claims using a cross lingual approach</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Glavas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M R</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,165.60,457.37,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,468.69,337.64,7.86;9,151.52,479.65,329.07,7.86;9,151.52,490.61,271.04,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,415.36,468.69,65.24,7.86;9,151.52,479.65,232.17,7.86">Idat at fire2019: Overview of the track on irony detection in arabic tweets</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Karoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Benamara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,405.47,479.65,75.12,7.86;9,151.52,490.61,196.22,7.86">Proceedings of the 11th Forum for Information Retrieval Evaluation</title>
		<meeting>the 11th Forum for Information Retrieval Evaluation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,501.93,337.63,7.86;9,151.52,512.89,329.07,7.86;9,151.52,523.85,329.07,7.86;9,151.52,534.81,25.60,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,425.82,501.93,54.77,7.86;9,151.52,512.89,311.33,7.86">Neural checkworthiness ranking with weak supervision: Finding sentences for fact-checking</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alstrup</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Grue Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,523.85,267.96,7.86">Companion Proceedings of the 2019 World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="994" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,546.13,337.64,7.86;9,151.52,557.09,250.39,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,299.94,546.13,180.65,7.86;9,151.52,557.09,105.12,7.86">bigir at clef 2019: Automatic verification of arabic claims over the web</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,278.34,557.09,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,568.41,337.64,7.86;9,151.52,579.36,329.07,7.86;9,151.52,590.32,267.89,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,442.93,568.41,37.66,7.86;9,151.52,579.36,329.07,7.86;9,151.52,590.32,123.52,7.86">Overview of the clef-2019 checkthat! lab: Automatic identification and verification of claims. task 2: Evidence and factuality</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,295.82,590.32,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,601.64,337.63,7.86;9,151.52,612.60,329.07,7.86;9,151.52,623.56,100.35,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,266.47,601.64,214.12,7.86;9,151.52,612.60,43.22,7.86">Opinion observer: analyzing and comparing opinions on the web</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,217.35,612.60,263.24,7.86;9,151.52,623.56,15.36,7.86">Proceedings of the 14th international conference on World Wide Web</title>
		<meeting>the 14th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="342" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,634.88,337.64,7.86;9,151.52,645.81,329.07,7.89;9,151.52,656.80,25.60,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,335.24,634.88,145.35,7.86;9,151.52,645.84,106.94,7.86">Fake news detection on social media: A data mining perspective</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,266.02,645.84,159.43,7.86">ACM SIGKDD explorations newsletter</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="36" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,119.67,337.98,7.86;10,151.52,130.63,218.37,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,254.49,119.67,226.10,7.86;10,151.52,130.63,73.85,7.86">Automatic verification of political claims based on morphological features</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Touahri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mazroui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,246.32,130.63,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,141.59,337.97,7.86;10,151.52,152.55,329.07,7.86;10,151.52,163.51,328.28,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,319.89,141.59,160.70,7.86;10,151.52,152.55,116.72,7.86">Fake news research: Theories, detection strategies, and open problems</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zafarani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,288.54,152.55,192.06,7.86;10,151.52,163.51,234.77,7.86">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3207" to="3208" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
