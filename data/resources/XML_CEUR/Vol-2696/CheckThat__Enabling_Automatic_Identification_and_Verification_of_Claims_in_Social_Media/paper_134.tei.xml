<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.84,115.96,317.69,12.62;1,182.11,133.89,251.13,12.62;1,257.57,151.82,100.22,12.62">Team Buster.ai at CheckThat! 2020: Insights And Recommendations To Improve Fact-Checking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,237.61,189.49,76.00,8.74"><forename type="first">Mostafa</forename><surname>Bouziane</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Buster.AI</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.66,189.49,51.83,8.74"><forename type="first">Hugo</forename><surname>Perrin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Buster.AI</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,194.64,201.45,73.08,8.74"><forename type="first">Aurélien</forename><surname>Cluzeau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Buster.AI</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.76,201.45,59.67,8.74"><forename type="first">Julien</forename><surname>Mardas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Buster.AI</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.15,201.45,57.57,8.74"><forename type="first">Amine</forename><surname>Sadeq</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Buster.AI</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.84,115.96,317.69,12.62;1,182.11,133.89,251.13,12.62;1,257.57,151.82,100.22,12.62">Team Buster.ai at CheckThat! 2020: Insights And Recommendations To Improve Fact-Checking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3E8292CC22122FCF8A737AD111EAFE32</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fact-Checking</term>
					<term>Veracity</term>
					<term>Evidence-based Verification</term>
					<term>Fake News Detection</term>
					<term>Computational Journalism</term>
					<term>Natural Language Processing</term>
					<term>Deep Learning</term>
					<term>Language Model</term>
					<term>Sentence Similarity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As part of the CheckThat! 2020 Task 2, we investigated sentence similarity using transformer models. In Task 2, the goal was to effectively rank claims based on their relevancy compared to an input tweet. While setting our baseline on sentence similarity for fact-checking, we gathered insights we felt compelled to share in this paper. We learned how multi-modal data utilization could foster significant uplifts in model performance. We also gained knowledge on which hybrid training and strong sampling worked best for fact-checking applications, and wanted to share our interpretation of the results we got. Finally, we want to explain our recommendations on data augmentations. All of the above allowed us to set our baseline in fact-checking in the CLEF Checkthat! 2020 Task 2 competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In the run-up to elections in many economic powers, being able to discern deception from real news has never been more critical. Moreover, in recent years, the actual cost of inaccurate news has been assessed more thoroughly. With occurrences like the Bloomberg dubious report almost wiping up 6 billion euros of Vinci market value, resulting in a 5 million euros fine for the media conglomerate recently <ref type="bibr" coords="1,172.36,556.63,15.50,8.74" target="#b12">[13]</ref>  <ref type="bibr" coords="1,191.11,556.63,9.96,8.74" target="#b3">[4]</ref>, it is also manifest that fake news can have a significant impact on economic and financial markets. As we are observing during the 2019/2020 coronavirus crisis <ref type="bibr" coords="1,212.94,580.54,9.96,8.74" target="#b7">[8]</ref>, false health information, potentially causes grave harm to public safety and societies as a whole, from the economic to the political stability of entire countries. While most of the effort to combat them is handmade, we believe automation is vital to ensure propelling this fight to a broader impact.</p><p>In CheckThat! 2020 Task 2 <ref type="bibr" coords="2,271.69,118.99,14.61,8.74" target="#b9">[10]</ref>, as descibed in <ref type="bibr" coords="2,357.22,118.99,9.96,8.74" target="#b8">[9]</ref>, the intended goal was to effectively rank claims based on their relevancy to an input tweet. We had to output a list of numerical scores for sentences in the database, the higher the individual score, the more accurately related the match with the input text is. It became natural that we judged our results with the Mean Average Precision metric (MAP), evaluating if the valid claim is in the top-scored evidences. We will expand in the background section on the several approaches that have been tried. Figure <ref type="figure" coords="2,193.43,202.68,4.98,8.74" target="#fig_0">1</ref> explains our strategy: after preprocessing the data and filtering out potentially unsuitable claims, we can finally estimate the matching probabilities with our neural network on acceptable claims. This filtering enables faster computations while preserving most of the performance that we have when computing scores for all documents of the database. We consequently trained our transformer model to learn sentence similarity on a semantic level, which we will detail in the methodology section.</p><p>Solving such a task will assist human fact-checkers in increasing their checking throughput drastically. A desirable algorithm for this task will permit obtaining source candidates automatically, diminishing proof searching times for human fact-checkers, as explained in Figure <ref type="figure" coords="2,334.08,323.37,3.87,8.74" target="#fig_0">1</ref>. It is noteworthy that sentence correspondence analysis only serves as a guide for humans, and cannot replace them for the ultimate purpose of fact-checking. Indeed, the necessity for human settlements can further ensure trust from everyone on the exactness of the factchecking process, confidence, which is also strengthened by the associated facts from cited references so anyone can verify them. Furthermore, grand strides have been made for general interpretable artificial intelligence, and resolving such a task benefits in achieving this in the factchecking setting. Being able to find matches on an evidence base allows the algorithm to provide its sources for individuals to gauge its prediction quality and exhibits where an automatic entailment model built on top would acquire its reference material from.</p><p>For all those reasons, this aim has a practical significance and usefulness. After focusing on what has been done already to solve such a problem, we will succinctly present our competition-winning submission. We will outline the underlying architecture and how we trained it, which will be a strong starting point to describe the insights we gained from the competition. The first recommendation we share is how multi-modal knowledge can be leveraged to enhance score quality. Then we want to discuss the use of external datasets, which helped us propelling our algorithm farther and our interpretation of why specific datasets appeared to help more than others. Building on these observations, we want to highlight how proper sampling is required to train models efficiently. Finally, we will give a concise overview of other data augmentations that have impacted our model both for training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>In Task 2, we recall the goal is to rank claims based on their correspondence with the input statement. So, chiefly our goal is to build a sentence similarity model to learn semantics and hidden meanings. The latest advances in Natural Language Processing (NLP) suggest to use Transformer models and attention models. The Transformer <ref type="bibr" coords="3,271.18,392.84,15.50,8.74" target="#b13">[14]</ref> idea placed the latest paradigm for solving sequence-to-sequence tasks while handling long-range dependencies with ease. This architecture is based on self-attention, which allows the model to look at the other words in the input sequence to understand a particular word in the sequence better. Such attention is instrumental when the sentence is too long, or it contains a hidden context, this is why research has devised different variants of Transformer models, of which Bidirectional Encoder Representations from Transformers (BERT) is the most notable and constitutes state of the art <ref type="bibr" coords="3,459.74,476.53,9.96,8.74" target="#b1">[2]</ref>.</p><p>BERT's introduction caused a massive surprise in the NLP community, as it outperformed previous models in so many distinct tasks. This considerable success comes from BERT being one of the first models that were non-directional. Previous models based on recurrent neural nets (RNNs) or convolutional neural nets (CNNs) implied a notion of direction, insofar as they implied a specific sequentiality when reading the input. For example, these models will predict a missing word in a sentence using either previous words or the following words, but can solely combine both approaches as an ensembling method. Nevertheless, with the BERT approach based on transformers, this prediction can be made using all available words simultaneously through the attention-based architecture, making BERT much more potent in creating context-based embeddings. These context-aware representations are of interest for the task at hand, since they allow elegant intertwined semantic links between query input and potential candidate evidence. In addition to a new non-directional approach, BERT-like models also introduced a new way of encoding input data, which makes BERT able to receive a single sentence as input or a two-sentence input, making it suitable for such a large field of different tasks. Moreover, the pre-trained weights on vast amounts of data were released, leading to a sizable interest by the community and a substantial amount of analysis, understanding, and improvement of the architecture, creating a series of BERT based papers with several adjustments and specifications <ref type="bibr" coords="4,243.97,178.77,9.96,8.74" target="#b5">[6]</ref>.</p><p>We also deem noteworthy to draw parallels between the CLEF Task 2 natural pipeline and that of some of the pipelines proposed in Open Domain Question Answering, like has been posted recently by HuggingFace labs <ref type="bibr" coords="4,417.46,215.67,9.96,8.74" target="#b2">[3]</ref>, since both exhibit the same kind of filtering/classify pipeline. This blog post has not inspired our work, as it just was published at the time of writing, but it is nonetheless an interesting source to reframe our current work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology: Our submission overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Definition and dataset</head><p>Task 2 aims to build tools that verify tweets based on a database of genuine claims. When we feed a tweet to the model, the comparison made with the claims database will produce a ranking from the most to the least relevant claims extracted from the database. Pairs of tweet and related claim constitute the samples of the available dataset; the tweet is the input that we want to verify, and the related claim can be retrieved from the database of verified claims.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed approach</head><p>A rank-based method is an obvious thinking starting point to solve this task: a method, where the model will have to learn patterns to rank pairs based on how related they are. Nevertheless, for this specific task, we want to exploit the fact that a single claim can be related to the input, simplifying the ranking task to either being related, i.e., top-ranked, or not related, i.e., ranked second or more.</p><p>A rank-based model can be the right approach if the rank scores are smoothed and well distributed over all claims, which is hard to achieve, since we do not have this information in the provided dataset.</p><p>Training: Therefore, we instead opted for a binary classification approach, as follows:</p><p>-Using genuine examples (pair tweet/claim) provided in the dataset.</p><p>-Generating wrong examples using several sampling strategies.</p><p>-Using a binary cross-entropy loss.</p><p>Inference: For all our submissions, once the model is trained properly, we pair each tweet in the test set with all the verified claims in the database, and we then apply the neural network with a composite score s function for ranking, that we defined as in Equation <ref type="formula" coords="5,270.44,372.82,3.87,8.74" target="#formula_0">1</ref>, with p p being the probability of being related, and p n being the probability of not being related to the query. It is important to mention that we replace the last sigmoid activation by a softmax activation for the last layer, even though in most cases sigmoid and softmax perform similarly, since the values sum up to roughly one already on the right data. Nevertheless, softmax will prove a more reliable activation on unknown data.</p><formula xml:id="formula_0" coords="5,237.13,470.15,243.46,24.00">s (p n , p p ) = p p if p p ≥ p n 1 -p n otherwise<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network architecture</head><p>Our network relies upon on a RoBERTa checkpoint <ref type="bibr" coords="5,359.97,548.52,10.52,8.74" target="#b6">[7]</ref> to begin training. Therefore, the underlying architecture is a basic BERT architecture with a 12-layer, 768-hidden, 12-heads configuration, which amounts to 125 million parameters <ref type="bibr" coords="5,134.77,584.39,14.61,8.74" target="#b16">[17]</ref>. Those parameters result from the need to have both robust and fast inference, which it both achieved in practice, as it achieved the top score of around 93% MAP at depth 5, with an inference time below 0.1 seconds per pair. In absolute fairness, we shall point out that the complexity of inference, with m being the number of facts in our database, and n being the number of queries, is in O(nm), which means our approach requires a pre-filtering to scale with a broader knowledge base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmentations and dataset</head><p>We must now disclose the external data we tried to provide as a supplement for training. We first tried using SciFact <ref type="bibr" coords="6,299.45,150.87,14.61,8.74" target="#b14">[15]</ref>, a scientific claim verification corpus, comprising 1 409 claims fact-checked against 5 183 abstracts with entailment labels. Secondly, we tried augmenting our corpus with FEVER <ref type="bibr" coords="6,425.78,174.78,14.61,8.74" target="#b11">[12]</ref>, a quite substantially large dataset with 185 445 short claims, a few orders of magnitude larger than SciFact and CLEF datasets. Finally, we also used Liar <ref type="bibr" coords="6,433.23,198.69,14.61,8.74" target="#b15">[16]</ref>, which constituted around 12 800 claims, placing it as a middle-ground in terms of size and featuring short claims and evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results: Our insights</head><p>For all comparisons here, we used our checkpoints obtained at early training to avoid overfitting issues to falsify any results, and we will mention whenever we use a model checkpoint for our submissions for reference. We used Whoosh <ref type="bibr" coords="6,470.08,302.31,10.52,8.74" target="#b0">[1]</ref> as a baseline instead of elastic search, and it provides a clean Python API, and obtained decent metrics compared to ElasticSearch in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-modal data utilization</head><p>We define multi-modal information in this context as any information coming from either external links, pictures, videos, or audio information contained in the textual data; information, which we eventually translated into text data that can be fed to the model alongside the original tweet, as described in Figure <ref type="figure" coords="6,134.77,423.85,3.87,8.74">3</ref>. The extraction process mentioned can be applied on many kinds of data; from image reverse search to get a title for pictures, to fancier image/video description networks, or even speech-to-text algorithms, the possibilities are varied. For our submission, we mainly converted links to the page name associated with the linked page, as well as getting a title for images referred to in tweets, with an image reverse search. Table <ref type="table" coords="6,256.45,483.63,4.98,8.74" target="#tab_0">1</ref> sums up the results we have in terms of inference, and we can notice a significant uplift in performance, since, with the same model checkpoint, and no retraining, we achieve around 3% better performance, which amounts to more than half of the potential performance left to gain in this case.</p><p>The importance of such preprocess arises from the inherent multimedia essence of Twitter data, which can also be found in most news outlets nowadays. Hence, we preconize easing up the learning and inference process by adding those additional pieces of information.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid training</head><p>This section will explain how we used hybrid training with different external data and their effect on the overall results.</p><p>FEVER: Sampling pairs from the FEVER dataset during training seems to help the model improve its performance. One reason is that those pairs have similar syntactic structures and lengths to the ones available in Task 2. The performance was stable on the dev set; the results on the test set, where we had better MAP at depth k values, confirmed this.</p><p>SciFact: Sampling pairs from the SciFact dataset during training did not improve the model performance. One reason is that examples from Scifact have varying numbers of tokens, which are ampler compared to our core data, making it hard for the model to converge and generalize well.</p><p>Liar: Same as SciFact, the Liar dataset did not improve the performance of the model, since the structure of the examples does not match with the ones we have in Task 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling scheme</head><p>We also advocate for sensible sampling schemes to ensure training on the optimal information signal. When scrutinizing the CLEF dataset, we observe that naive sampling will often sample what we call negative samples, i.e., samples not related to the query we pair it with. Those negative samples are utterly unrelated to the said query, leading to learning a more trivial solution of not pairing sentences which do not share anything meaningful. However, this naive approach leads to subpar MAP, as it will have a hard time distinguishing close evidence and rank them in an unsatisfiable manner as a consequence. We can illustrate this effect with the following example: with the query "The Pope is blue with yellow stripes." and most evidence talking about age and place of birth of various people, plus a few sentences about the Pope, a non-optimal sampling could lead to the algorithm only giving a high score to anything with "The Pope" contained in it, and therefore, poorly scoring the other sentences about the Pope potentially. We solved this, inspiring ourselves from a reinforcement learning principle called experience replay <ref type="bibr" coords="9,283.60,118.99,9.96,8.74" target="#b4">[5]</ref>, in which we use past predictions to learn more efficiently. In our case, we went towards this idea by sampling according to indexed searches, which produced relatively good results that could fool our algorithm trained with naive sampling, forcing it to learn the proper semantics to distinguish between syntactically and lexically close sentences. Ideally, we would take the top ranked results of our model to generate hard examples, however we chose to avoid the incurred computationnal cost, while using ElasticSearch results as a proxy for our model ranking. This sampling is a fast and accurate enough approximation of the reinforced mechanism of sampling new data based on our algorithm's error.</p><p>We used a balanced sampling, and we fully acknowledge that doing this will introduce a bias in the distribution of our dataset, which can have negative impact on the learning; nevertheless, we advocate that it is worth leveraging towards the fact-checking goal. In this context, we sample pairs of query and evidence, therefore balancing means we sample as much matching pairs than non-matchihng pairs. In Table <ref type="table" coords="9,268.76,298.32,3.87,8.74" target="#tab_2">3</ref>, we have evidence it can be hugely beneficial to the training process, as it helps to gain a tremendous amount of performance, using the same network as our submission, making the difference between a usable and unusable solution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional augmentations</head><p>In this section, we will present some other strategies that we tried but did not seem to impact the improvement of the model's performance significantly.</p><p>Named entity recognition (NER) based augmentation : Given that Task 2 data contains a lot of named entities, which are a crucial indicator of similarity between sentences, we tried to run a named entity recognition model on pairs and augment them with the detected labels. The model is a fine-tuned version of the bert-base multilingual cased model weights from HuggingFace's transformers <ref type="bibr" coords="9,193.18,620.25,14.61,8.74" target="#b16">[17]</ref>. The results did not improve the overall performance, maybe because our augmentation strategy was not robust since we only add a separator and the detected labels at the end of each sentence, which is not very efficient since many sentences will have common labels and this will prevent the model from a better convergence. A better strategy to try would be to add the label next to the detected entity so that the model fits well on this kind of augmentation, and will make more sense of the relation between the words and the added labels. Another strategy would be to use the one-hot encoding of the named entities and stack them with BERT embeddings before the classifier layers; that way, the model will learn how to use those added encodings without having much negative impact on the semantics learned from the original sentence.</p><p>ConceptNet <ref type="bibr" coords="10,214.61,203.03,17.82,8.77" target="#b10">[11]</ref> based augmentation : ConceptNet, a general knowledge graph, treats words and phrases of natural language as nodes that it connects with labeled edges, therein representing the general knowledge involved in understanding language, facilitating comprehension of the meanings hidden behind everyday use of a word. The intuition behind using ConceptNet for our task is that combining it with words/sentence embeddings empowers an understanding that is not accessible solely from distributional semantics. As with NER, we augmented pairs using ConceptNet API, with semantic relations it produces. Whenever we detect a part of speech in a sentence, i.e., verbs, nouns, or adjectives, we append to sentences the relations retrieved from the graph (e.g., "chanting is synonym of shouting"). When adding those augmentations, the results did not vary significantly, since we gained around 0.5% in MAP at depth 1 on dev set, and almost stable for the other depths. Again, the strategy of adding a separator and the augmentation at the end of the sentence may not be the most pertinent. Indeed, ConceptNet supplies plenty of distinct relations such as "synonym of", "antonym of", "related to", "can be done with"..., so choosing the right relation is of great importance, and cannot be done straightforwardly, as the correct relation to use may depend on context. Back and forth translation : In order to augment the positive examples in the dataset, one can think of using back and forth translation strategy, which means : given a source S (English in our case) and target T (anything but English) language, we want to feed the example to translation algorithms following two steps:</p><p>-Step one: the example will be fed to a translation algorithm from S to T, that way we will get a new sentence in the target language.</p><p>-Step two: the new sentence will be fed to a translation algorithm from T to S, that way we will get a new sentence in the source language, semantically the same as the first one but with a different formulation. Doing so, the dataset will be more diverse terms of syntax, and will help the model to generalize better. The model are MarianMT models from Hugging-Face's transformers <ref type="bibr" coords="11,222.32,339.94,14.61,8.74" target="#b16">[17]</ref>.</p><p>This strategy did not help the baseline model to improve because the results were stable (Figure <ref type="figure" coords="11,222.80,363.85,3.87,8.74">5</ref>). We believe that this is due to the fact that the dataset is not big enough, so the augmentations did not have much effect on the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In conclusion, our first recommendation for fact-checking was to ensure any piece of information, no matter the medium, is leveraged, to avoid as much as possible, relying on hidden information/context or implicit knowledge. Then, we wanted to stress the importance of consistent extra datasets, and how they helped training, even when dealing with unrelated subjects, as it was the case for us with FEVER, but also how they would impair learning if their size are not consistent. We also explained the importance of proper sampling. Last but not least, we presented a few augmentations that we experimented with, primarily semantic and lexical related augmentations, and hopefully gave inspiration for more augmentations to help fact-checking efforts in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,247.59,596.15,120.18,7.89;2,143.41,416.83,328.56,164.55"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Fact-checking pipeline</figDesc><graphic coords="2,143.41,416.83,328.56,164.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,258.38,594.41,98.59,7.89;4,143.41,410.24,328.55,169.40"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Task 2 definition</figDesc><graphic coords="4,143.41,410.24,328.55,169.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,249.83,216.84,115.69,7.89;7,143.41,115.84,328.54,86.23"><head>Fig.</head><label></label><figDesc>Fig. 3. Multi-modal pipeline</figDesc><graphic coords="7,143.41,115.84,328.54,86.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.77,216.84,345.82,130.39"><head>Table 1 .</head><label>1</label><figDesc>3. Multi-modal pipeline Metrics table on dev set. Results comparison between baseline, with and without multi-modal information.</figDesc><table coords="7,151.41,249.36,312.52,76.01"><row><cell cols="5">Metric Depth Whoosh baseline score Without multi-modal With multi-modal</cell></row><row><cell>MAP</cell><cell>1</cell><cell>0.804</cell><cell>0.96</cell><cell>0.99</cell></row><row><cell>MAP</cell><cell>3</cell><cell>0.838</cell><cell>0.967</cell><cell>0.992</cell></row><row><cell>MAP</cell><cell>5</cell><cell>0.843</cell><cell>0.97</cell><cell>0.992</cell></row><row><cell>MAP</cell><cell>10</cell><cell>0.846</cell><cell>0.97</cell><cell>0.992</cell></row><row><cell>MAP</cell><cell>20</cell><cell>0.847</cell><cell>0.97</cell><cell>0.992</cell></row><row><cell>MAP</cell><cell>all</cell><cell>0.847</cell><cell>0.97</cell><cell>0.992</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,326.43,345.83,97.88"><head>Table 2 .</head><label>2</label><figDesc>Metrics table on test set. Results comparison between baseline, with and without sampling from FEVER dataset.</figDesc><table coords="8,217.92,326.43,179.51,76.01"><row><cell cols="4">Metric Depth without FEVER with FEVER</cell></row><row><cell>MAP</cell><cell>1</cell><cell>0.8970</cell><cell>0.9070</cell></row><row><cell>MAP</cell><cell>3</cell><cell>0.9260</cell><cell>0.9370</cell></row><row><cell>MAP</cell><cell>5</cell><cell>0.9290</cell><cell>0.9380</cell></row><row><cell>MAP</cell><cell>10</cell><cell>0.9290</cell><cell>0.9380</cell></row><row><cell>MAP</cell><cell>20</cell><cell>0.9290</cell><cell>0.9380</cell></row><row><cell>MAP</cell><cell>all</cell><cell>0.9290</cell><cell>0.9380</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.77,365.18,345.82,97.88"><head>Table 3 .</head><label>3</label><figDesc>Metrics table on dev set. Results comparison between with and without proper sampling.</figDesc><table coords="9,186.47,365.18,242.41,76.01"><row><cell cols="4">Metric Depth With proposed sampling With naive sampling</cell></row><row><cell>MAP</cell><cell>1</cell><cell>0.96</cell><cell>0.1</cell></row><row><cell>MAP</cell><cell>3</cell><cell>0.967</cell><cell>0.1</cell></row><row><cell>MAP</cell><cell>5</cell><cell>0.97</cell><cell>0.1</cell></row><row><cell>MAP</cell><cell>10</cell><cell>0.97</cell><cell>0.1</cell></row><row><cell>MAP</cell><cell>20</cell><cell>0.97</cell><cell>0.1</cell></row><row><cell>MAP</cell><cell>all</cell><cell>0.97</cell><cell>0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,134.77,440.42,345.83,97.88"><head>Table 4 .</head><label>4</label><figDesc>Metrics table on dev set. Results comparison between with and without using ConceptNet.</figDesc><table coords="10,202.30,440.42,210.76,76.01"><row><cell cols="4">Metric Depth without ConceptNet with ConceptNet</cell></row><row><cell>MAP</cell><cell>1</cell><cell>0.96</cell><cell>0.965</cell></row><row><cell>MAP</cell><cell>3</cell><cell>0.967</cell><cell>0.967</cell></row><row><cell>MAP</cell><cell>5</cell><cell>0.967</cell><cell>0.968</cell></row><row><cell>MAP</cell><cell>10</cell><cell>0.967</cell><cell>0.969</cell></row><row><cell>MAP</cell><cell>20</cell><cell>0.967</cell><cell>0.970</cell></row><row><cell>MAP</cell><cell>all</cell><cell>0.967</cell><cell>0.970</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,134.77,180.84,345.83,97.88"><head>Table 5 .</head><label>5</label><figDesc>Metrics table on dev set. Results comparison between with and without using translation augmentations.</figDesc><table coords="11,171.80,180.84,271.75,76.01"><row><cell cols="4">Metric Depth without translation augmentation with augmentation</cell></row><row><cell>MAP</cell><cell>1</cell><cell>0.96</cell><cell>0.95</cell></row><row><cell>MAP</cell><cell>3</cell><cell>0.967</cell><cell>0.96</cell></row><row><cell>MAP</cell><cell>5</cell><cell>0.967</cell><cell>0.965</cell></row><row><cell>MAP</cell><cell>10</cell><cell>0.967</cell><cell>0.965</cell></row><row><cell>MAP</cell><cell>20</cell><cell>0.967</cell><cell>0.965</cell></row><row><cell>MAP</cell><cell>all</cell><cell>0.967</cell><cell>0.965</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,602.49,186.89,8.37" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="11,207.44,602.49,93.89,7.86">Whoosh documentation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chaput</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,613.21,337.63,8.37;11,151.52,624.16,329.07,7.86;11,151.52,635.12,28.16,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="11,422.01,613.21,58.58,7.86;11,151.52,624.16,232.32,7.86">pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bert</surname></persName>
		</author>
		<idno>CoRR abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,645.84,337.64,8.37;11,151.52,656.80,103.03,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,207.24,645.84,273.35,7.86;11,151.52,656.80,74.74,7.86">Explain anything like i&apos;m five: A model for open domain long form question answering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,119.67,337.63,8.37" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,296.65,119.67,151.68,7.86">Le faux vinci ou l&apos;éloge de la prudence</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Kadri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>De Presse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,130.63,337.64,8.37;12,151.52,141.59,292.20,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="12,198.79,130.63,228.69,7.86">Reinforcement learning for robots using neural networks</title>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie-Mellon Univ Pittsburgh PA School of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct coords="12,142.96,152.55,337.64,8.37;12,151.52,163.51,122.69,7.86" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>ArXiv abs/2003.07278</idno>
		<title level="m" coord="12,334.73,152.55,141.34,7.86">A survey on contextual embeddings</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,174.47,337.63,8.37;12,151.52,185.43,329.07,8.37;12,151.52,196.39,213.95,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,368.09,185.43,112.50,7.86;12,151.52,196.39,82.49,7.86">A robustly optimized BERT pretraining approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Roberta</surname></persName>
		</author>
		<idno>CoRR abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,207.34,337.64,8.37;12,151.52,218.30,55.09,7.86" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Nations</surname></persName>
		</author>
		<title level="m" coord="12,206.01,207.34,271.14,7.86">During this coronavirus pandemic, &apos;fake news&apos; is putting lives at risk</title>
		<imprint>
			<publisher>Unesco</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,229.26,337.63,8.37;12,151.52,240.22,329.07,7.86;12,151.52,251.18,329.07,7.86;12,151.52,262.14,302.79,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,442.28,229.26,38.31,7.86;12,151.52,240.22,210.54,7.86">That is a known lie: Detecting previously fact-checked claims</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,385.47,240.22,95.12,7.86;12,151.52,251.18,303.55,7.86;12,179.17,262.14,32.88,7.86">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Online</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (Online</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="3607" to="3618" />
		</imprint>
	</monogr>
	<note>ACL &apos;20</note>
</biblStruct>

<biblStruct coords="12,142.61,273.10,337.98,8.37;12,151.52,285.57,329.07,6.85;12,151.52,295.02,329.07,8.37;12,151.52,305.98,329.07,7.86;12,151.52,316.93,329.07,7.86;12,151.52,327.89,78.97,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,217.74,295.02,262.86,7.86;12,151.52,305.98,242.37,7.86">Overview of the CLEF-2020 CheckThat! lab on automatic identification and verification of claims in social media: English tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,411.14,305.98,69.45,7.86;12,151.52,316.93,237.54,7.86">Working Notes of CLEF 2020-Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,338.85,337.98,8.37;12,151.52,349.81,213.41,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,305.38,338.85,175.22,7.86;12,151.52,349.81,81.89,7.86">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
		<idno>CoRR abs/1612.03975</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,360.77,337.98,8.37;12,151.52,371.73,329.07,7.86;12,151.52,382.69,329.07,7.86;12,151.52,393.65,329.07,7.86;12,151.52,404.61,329.07,7.86;12,151.52,415.56,50.68,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,445.78,360.77,34.81,7.86;12,151.52,371.73,233.59,7.86">FEVER: a large-scale dataset for fact extraction and VERification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,407.44,371.73,73.15,7.86;12,151.52,382.69,329.07,7.86;12,151.52,393.65,219.21,7.86">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s" coord="12,307.33,404.61,169.45,7.86">Association for Computational Linguistics</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="12,142.61,426.52,272.32,8.37" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="12,197.19,426.52,187.08,7.86">French hoax costs bloomberg 5m euros in fines</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Times</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,437.48,337.97,8.37;12,151.52,448.44,329.07,8.37;12,151.52,459.40,95.01,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="12,337.51,448.44,104.92,7.86">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>CoRR abs/1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,470.36,337.98,8.37;12,151.52,481.32,256.23,8.37" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="12,216.26,481.32,163.48,7.86">Fact or fiction: Verifying scientific claims</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,492.28,337.98,8.37;12,151.52,503.24,329.07,7.86;12,151.52,514.19,329.07,7.86;12,151.52,525.15,226.38,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,239.12,492.28,241.48,7.86;12,151.52,503.24,35.49,7.86">A new benchmark dataset for fake news detection</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Liar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,206.30,503.24,274.29,7.86;12,151.52,514.19,86.88,7.86">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="422" to="426" />
		</imprint>
	</monogr>
	<note>liar pants on fire. Short Papers</note>
</biblStruct>

<biblStruct coords="12,142.61,536.11,337.98,8.37;12,151.52,547.07,329.07,8.37;12,151.52,558.03,329.07,7.86;12,151.52,568.99,28.16,7.86" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="12,425.25,547.07,55.35,7.86;12,151.52,558.03,231.28,7.86">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<idno>ArXiv abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
