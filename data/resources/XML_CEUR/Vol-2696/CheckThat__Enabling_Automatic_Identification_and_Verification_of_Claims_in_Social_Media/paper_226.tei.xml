<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.75,115.96,319.86,12.62;1,173.72,133.89,267.92,12.62;1,216.45,151.82,182.46,12.62">Accenture at CheckThat! 2020: If you say so: Post-hoc fact-checking of claims using transformer-based models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.45,189.78,63.85,8.74"><forename type="first">Evan</forename><surname>Williams</surname></persName>
							<email>e.m.williams@accenture.com</email>
						</author>
						<author>
							<persName coords="1,304.08,189.78,66.78,8.74"><forename type="first">Paul</forename><surname>Rodrigues</surname></persName>
							<email>paul.rodrigues@accenture.com</email>
						</author>
						<author>
							<persName coords="1,241.33,201.74,60.36,8.74"><forename type="first">Valerie</forename><surname>Novak</surname></persName>
							<email>vnovak@umd.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Accenture</orgName>
								<address>
									<addrLine>800 N. Glebe Rd</addrLine>
									<postCode>22209</postCode>
									<settlement>Arlington</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.75,115.96,319.86,12.62;1,173.72,133.89,267.92,12.62;1,216.45,151.82,182.46,12.62">Accenture at CheckThat! 2020: If you say so: Post-hoc fact-checking of claims using transformer-based models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">531BF79812B03647FB429E3E2F2E9167</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>fact checking</term>
					<term>fact identification</term>
					<term>Arabic</term>
					<term>BERT</term>
					<term>RoBERTa</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce the strategies used by the Accenture Team for the CLEF2020 CheckThat! Lab, Task 1, on English and Arabic. This shared task evaluated whether a claim in social media text should be professionally fact checked. To a journalist, a statement presented as fact, which would be of interest to a large audience, requires professional fact-checking before dissemination. We utilized BERT and RoBERTa models to identify claims in social media text a professional fact-checker should review, and rank these in priority order for the fact-checker. For the English challenge, we fine-tuned a RoBERTa model and added an extra mean pooling layer and a dropout layer to enhance generalizability to unseen text. For the Arabic task, we fine-tuned Arabic-language BERT models and demonstrate the use of back-translation to amplify the minority class and balance the dataset. The work presented here was scored 1st place in the English track, and 1st, 2nd, 3rd, and 4th place in the Arabic track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural Language Processing (NLP) has been driving Artificial Intelligence research since the 1950s, but recently increased in distinction due to the quantity of text that can be utilized as well as new techniques to extract even more value from text. In 2018, a surge of research produced deep learning architectures in NLP which beat state of the art on a multitude of tasks, such as sentiment analysis, question answering, and semantic similarity, in a variety of languages.</p><p>Since the innovation of ULMFit <ref type="bibr" coords="2,276.78,118.99,14.61,8.74" target="#b11">[12]</ref>, numerous new architectures have been introduced, such as ELMo <ref type="bibr" coords="2,241.02,130.95,14.61,8.74" target="#b16">[17]</ref>, BERT <ref type="bibr" coords="2,291.73,130.95,9.96,8.74" target="#b8">[9]</ref>, ERNIE <ref type="bibr" coords="2,341.89,130.95,14.61,8.74" target="#b25">[26]</ref>, RoBERTa <ref type="bibr" coords="2,409.05,130.95,14.61,8.74" target="#b13">[14]</ref>, GPT-2 <ref type="bibr" coords="2,462.33,130.95,14.61,8.74" target="#b17">[18]</ref>, GPT-3 <ref type="bibr" coords="2,168.17,142.90,9.96,8.74" target="#b5">[6]</ref>, and others, yielding breakthrough innovations and increased performance, nearly month after month. These architectures require massive amounts of training data, which can be expensive to train on high-performance computing clusters <ref type="bibr" coords="2,171.59,178.77,14.61,8.74" target="#b24">[25]</ref>. However, they facilitate the practice of transfer learning. A base model trained on a large amount of general text data can then be fine-tuned, or customized for a specific problem and domain/genre, using text with far less annotated data than previous systems required. This use of transfer learning allows us to effectively craft custom cutting-edge models to solve a wide range of classification problems.</p><p>While these architectures are often utilized to improve NLP tasks, the application of transformer-based transfer learning approaches are less often demonstrated as components in decision-support systems which aid the workflow of subject matter experts. We do see these technologies being used in the medical field (e.g. <ref type="bibr" coords="2,193.10,298.32,14.76,8.74" target="#b19">[20]</ref>), and anticipate there will be many more applications coming. The CheckThat! Lab poses one such application, which could reduce information burden in the workflow of a journalist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">CheckThat! Lab</head><p>We participated in Task 1 of the 2020 CheckThat! challenge. <ref type="bibr" coords="2,418.62,366.16,10.52,8.74" target="#b4">[5]</ref> Organizers distributed collections of tweets in English and in Arabic for training, annotated for topic group, whether the tweet was a claim, and whether the tweet was check-worthy, along with Twitter provided meta-data. <ref type="bibr" coords="2,380.78,402.02,15.50,8.74" target="#b23">[24,</ref><ref type="bibr" coords="2,397.93,402.02,12.73,8.74" target="#b9">10]</ref> Participants in the challenge utilized this data to train a model that could receive a list of novel tweets, classify each for check-worthiness, and rank the group of tweets by how check-worthy they were. Evaluation of the model was performed on a second test dataset provided for each language. These test datasets were held back by the organizers until shortly before the competition end time. Organizers provided this dataset unlabeled, and participants provided the labels and ranking to the organizers. Organizers evaluated the ranking produced by participating groups to a withheld labeled and ranked list. Participants were permitted to submit one primary run and up to 3 contrasting runs. The official metric for Arabic was Precision @ 30 (P@30). Precision @ k is the number of relevant results in the top k claims in the ranked list. The official metric for English was Mean Average Precision (mAP), or the mean of the average precision scores for each of the claims.</p><p>Provided Data Tweets were collected by CheckThat! organizers using keyword watchlists, consisting of usernames, hashtags, or key words, designed around a variety of topic areas.</p><p>For English, one topic was provided related to COVID-19, and filtered for tweets that mentioned #COVID19, #CoronavirusOutbreak, #Coronavirus, #Corona, #CoronaAlert, #CoronaOutbreak, corona, and COVID-19. This topic was the same in train, test, and the evaluation set.</p><p>For Arabic, the training data included three topics-Protests in Lebanon, Emirati cleric Wassim Youssef, as well as Turkey's intervention in Syria. Testing data included topics such as Deal of the Century, The Houthis in Yemen, COVID-19, Feminists, Events in Libya, The group of resident non-citizens in Kuwait, Algeria, as well as Boycotting Countries &amp; Promoting Rumors against Qatar. We note that the topics provided between train and test datasets differ, with no overlap.</p><p>The topic word lists were used by the organizers to collect posts on Twitter. Annotators were presented these posts and were asked to evaluate each for checkworthiness. Check-worthiness was defined as "a tweet that includes a claim that is of interest to a large audience (especially journalists), might have a harmful effect, etc." <ref type="bibr" coords="3,186.62,250.50,10.52,8.74" target="#b7">[8]</ref> Tweets were assigned check-worthiness labels after review by two annotators as well as a review by a third expert annotator. Check-worthiness was evaluated on the following three criteria <ref type="bibr" coords="3,331.39,274.41,9.96,8.74" target="#b3">[4]</ref>:</p><p>-Do you think the claim in the tweet is of interest to the public? -To what extent do you think the claim can negatively affect the reputation of an entity, country, etc.? -Do you think journalists will be interested in covering the spread of the claim or the information discussed by the claim?</p><p>In examining the labeled training data, we confirmed nuanced differences between tweets that were check-worthy and tweets that were not. For example, the tweet below, which was taken from the English task development data, initially appears to be peddling a false COVID-19 claim. However, the rest of the tweet makes it clear that the author is joking, which is presumably why this tweet was not labeled as being check-worthy.</p><p>"ALERT The corona virus can be spread through money. If you have any money at home, put on some gloves, put all the money in to a plastic bag and put it outside the front door tonight. I'm collecting all the plastic bags tonight for safety. Think of your health."</p><p>In contrast, the tweet below, which was labeled check-worthy, is spreading harmful COVID-19 misinformation which could dissuade people from getting tested. We had concern that nuanced text like this may be difficult to discriminate and rank accurately.</p><p>For a journalist, the task of identifying noteworthy claims for the vetting process may be intuitive. Their knowledge of the material, background in academic training, and experience as a journalist inform their processes and decisionmaking. Our learner is not coached, trained, or experienced in this area beforehand. It receives the data and annotations provided by the annotators and learns the patterns of language to replicate their decision process.</p><p>2 Transformer Architectures and Pre-trained Models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">BERT</head><p>Bidirectional Encoder Representations from Transformers (BERT) models have fundamentally changed the NLP landscape. The original BERT model's architecture consists of 12 transformers stacked on top of one another with a hidden size of 768 and 12 self-attention heads. <ref type="bibr" coords="4,313.94,199.76,10.52,8.74" target="#b8">[9]</ref> BERT models are trained by performing unsupervised tasks, namely masked token prediction (Masked LM) and prediction of future sentences (Next Sentence Prediction) on massive amounts of data. BERT utilizes a WordPiece tokenization scheme. <ref type="bibr" coords="4,389.01,235.62,14.61,8.74" target="#b21">[22]</ref>, and was trained on Wikipedia and the BooksCorpus <ref type="bibr" coords="4,301.34,247.58,14.61,8.74" target="#b29">[30]</ref>. At the time of release, BERT was state-of-the-art in 11 NLP tasks.</p><p>Since initial release, many pre-trained BERT neural networks have been released. These can be focused on new languages, or differ in size. They can be either smaller and more efficient, or larger and more comprehensive, than the original release <ref type="bibr" coords="4,201.76,307.46,14.61,8.74" target="#b26">[27]</ref>. Any of these pre-trained models could serve as a base model for fine-tuning to new datasets and new tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">RoBERTa</head><p>RoBERTa, developed by Liu et al. <ref type="bibr" coords="4,296.95,370.29,14.61,8.74" target="#b13">[14]</ref>, is an derivative of BERT which introduced modifications to the training process. The primary modifications are the provision of more training data, increasing pre-training steps with bigger batches over more data, removing Next Sentence Prediction, training on longer sequences, and dynamically changing the masking pattern applied to the training data <ref type="bibr" coords="4,175.15,430.06,14.61,8.74" target="#b13">[14]</ref>. While RoBERTa also requires sub-word tokenization, RoBERTa uses a Byte-Pair Encoding (BPE) instead of WordPiece. <ref type="bibr" coords="4,386.56,442.02,15.49,8.74" target="#b22">[23]</ref> The base-roberta model was pre-trained on 160GB of text extracted from BookCorpus, English Wikipedia, CC-News, OpenWebText, and Stories (a subset of CommonCrawl Data) <ref type="bibr" coords="4,163.40,477.88,14.61,8.74" target="#b13">[14]</ref>.</p><p>At the time of release, the RoBERTa architecture achieved state-of-the-art results on publicly available benchmark datasets such as GLUE <ref type="bibr" coords="4,429.01,501.90,14.61,8.74" target="#b27">[28]</ref>, RACE <ref type="bibr" coords="4,134.77,513.85,14.61,8.74" target="#b12">[13]</ref>, and SQuAD <ref type="bibr" coords="4,212.46,513.85,14.61,8.74" target="#b18">[19]</ref>. Like BERT, RoBERTa models come in a variety of sizes, and choosing a model requires a trade-off between computational efficiency and model size.</p><p>While some new architectures have been released which exceed RoBERTa's performance, RoBERTa remains an accessible framework and continues to be one of the most highly ranked architectures on the SuperGLUE leaderboard. <ref type="foot" coords="4,469.69,572.16,3.97,6.12" target="#foot_0">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">AraBERT</head><p>AraBERT is an Arabic model developed by Wissam Antoun, Fady Baly, and Hazem Hajj at the American University of Beirut <ref type="bibr" coords="4,357.94,636.57,9.96,8.74" target="#b2">[3]</ref>. The aubmindlab/arabert series of models were pre-trained on Arabic documents retrieved from the web, as well as two publicly available corpora: the 1.5 billion word Arabic Corpus, and the 1 billion word Open Source International Arabic News Corpus (OSIAN). No token count was provided for the web scraped documents. <ref type="bibr" coords="5,391.00,154.86,9.96,8.74" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">ArabicBERT</head><p>ArabicBERT is an Arabic model developed by Ali Safaya, Moutasem Abdullatif, and Deniz Yuret KUIS of Koc University. <ref type="bibr" coords="5,350.36,221.76,15.50,8.74" target="#b20">[21]</ref> ArabicBERT was trained on Wikipedia, and the OSCAR corpus <ref type="bibr" coords="5,305.42,233.72,14.61,8.74" target="#b15">[16]</ref>, which utilized web data from Com-monCrawl. The corpus used to create the pre-trained model was, in total, 8.5 billion words.</p><p>3 Quantitative Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Label Balance</head><p>The datasets for both the English and the Arabic Challenges were imbalanced. The English Task 1 datasets contained a development dataset of 150 tweets and a training dataset of 672 tweets containing 39% and 34% check-worthy tweets respectively. The Arabic Task 1 training dataset provided 1,500 labeled tweets, 458 of which (31%) were labeled check-worthy.</p><p>We will discuss provisions we make for the Arabic imbalance later in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Vocabulary Analysis</head><p>When utilizing pre-trained models, vocabulary used to create these models plays a critical role. The process of fine-tuning does not allow for the addition of additional vocabulary, so these systems fallback to subword units during tokenization. Because we were evaluating a corpus that contained emerging topics (such as COVID-19), and our pre-trained models were created at different points between 2018 and 2020, we wanted to understand what our pre-trained models contained. We hypothesized that the models with the greatest token overlap would perform the best.</p><p>English The token overlap between the English test dataset and RoBERTa's vocabulary file was roughly 850 tokens (54%), with RoBERTa containing about 50K items in its vocabulary. Many tokens missing from the RoBERTa vocabulary were related to the coronavirus topic, including several terms for COVID-19 as well as named entities, emoji, foreign languages in non-Latin script, misspellings and slang/internet chat language (LMAOOO). No analysis was performed on the BERT vocabulary file.</p><p>Arabic The three Arabic model vocabularies contained 64K WordPieces ( aubmindlab/bert-base-arabert), 64K WordPieces (aubmindlab/bert-base-arabertv01 ) and 32K WordPieces (asafaya/bert-base-arabic). A rough tokenization and cleaning of the tweets in the test data set resulted in roughly 15K unique tokens. The overlap between the three Arabic model vocabulary and the Arabic test data set was roughly 8.5K tokens or 56% of the tokens in the test data (aubmindlab/bertbase-arabertv01 ), 5.5K tokens or 36% of the tokens in the test data (asafaya/bertbase-arabic) and 3.5K or 23% of the tokens in the the test data (aubmindlab/bertbase-arabert). Some categories of vocabulary found in the test data set, but missing from the top performing model, included English words or loan words in Arabic script, colloquial/slang, misspellings/missing spaces, named entities (names of people and places), emoji and tokens in Latin script. The asafaya/bert-basearabic Arabic model vocabulary also included a lot of longer WordPieces that were unlikely to be found in data. Additionally, even though the test data set contained short vowels, none of the Arabic model vocabularies had any short vowels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach and Results</head><p>The datasets provided for English and Arabic contained Twitter metadata fields, but we discard these. Our methodology only utilizes the message text of the Tweet as well as the check-worthy field containing a binary label where the positive class denoted a check-worthy claim. <ref type="foot" coords="6,327.29,386.60,3.97,6.12" target="#foot_1">5</ref>Competition rules required that tweets most likely to be check-worthy needed to appear at the top of each topic. To generate rankings, we took the positive and negative class scores, generated by a sequence classification head on top the pooled output of the neural network models (whether it be BERT, RoBERTa, AraBERT, or ArabicBERT), and passed those scores through a softmax function to normalize the classification outputs. We then subtracted the negative class probability from the positive class probability. This yielded interpretable, normalized scores between 1 and -1, where higher scores reflected our model's confidence that a tweet was check-worthy. We then sorted by the difference of probabilities to produce the ranked tweets submitted to the organizers of the conference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">English</head><p>Classification For our internal evaluations, we split the English training data provided into 80% training and 20% validation sets. We used the development set as was provided by the organizers.</p><p>We evaluated three baseline models. We fine-tuned the data over 2 epochs on the original English BERT model <ref type="bibr" coords="6,299.13,615.52,9.96,8.74" target="#b8">[9]</ref>, a BERT model trained on COVID-19</p><p>Twitter data <ref type="bibr" coords="7,194.84,118.99,14.61,8.74" target="#b14">[15]</ref>, and the original English RoBERTa model <ref type="bibr" coords="7,404.38,118.99,14.61,8.74" target="#b13">[14]</ref>. We assumed that the COVID-19 Twitter model would generate the highest accuracy given its deep contextual knowledge of both Twitter data and COVID-19, but of the three models, RoBERTa generated the highest precision and recall for both the positive and negative class. We chose to eliminate the previous two models and focus on optimizing RoBERTa. <ref type="foot" coords="7,270.15,177.20,3.97,6.12" target="#foot_2">6</ref>In our internal evaluations, we noticed the model overfitting quickly. To help prevent this, we added an extra mean pooling layer and dropout layer to the model. Our pooling layer takes the weights from the last layer, which were overfitting, and averages them with weights from the second-to-last layer. This reduces overfitting by smoothing out some of the weights originally calculated in the final layer. Dropout is a regularization technique that reduces overfitting by randomly omitting (or zeroing out) hidden units from the network during each training step at a probability specified by the user <ref type="bibr" coords="7,325.24,274.63,14.61,8.74" target="#b10">[11]</ref>. By adding these two layers to the end of our RoBERTa model, we were able to improve accuracy on our test set and reduce overfitting.</p><p>After a grid search, we fine-tuned with 2 epochs, a batch size of 32, and Adam optimization with a learning rate of 1.5e-5. The RoBERTa model was fine-tuned using the Keras API to TensorFlow.</p><p>This output was then fed through a softmax function, and the difference between the positive and negative class likelihoods were used to rank tweets within each pre-labeled topic category.</p><p>Results Results of our fine-tuned RoBERTa model can be found in Table <ref type="table" coords="7,463.40,401.67,4.98,8.74">1</ref> as RoBERTa. This submission placed first place among all competing teams with a mAP of 0.8064. Our contribution narrowly beat out the second place results, which likely utilized a similar model. We did not submit our BERT model or COVID Twitter models for formal evaluation.</p><p>Table <ref type="table" coords="7,214.51,479.48,4.13,7.89">1</ref>. Accenture results from CheckThat! Task1 English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entry</head><p>mAP RR R-P P@1 P@3 P@5 P@10 P@20 P@30 RoBERTa 0.8064 1.0000 0.7167 1.0000 1.0000 1.0000 1.0000 0.9500 0.7400</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Arabic</head><p>Classification For our internal evaluations, we split the Arabic training data provided into 70% training, 20% validation, and 10% held-out sets. We evaluated four baseline Arabic BERT models retrieved from Huggingface, without any parameter tuning. <ref type="bibr" coords="7,232.50,624.17,14.61,8.74" target="#b28">[29]</ref>. These models were Hate-speech-CNERG/ dehatebertmono-arabic <ref type="bibr" coords="7,191.88,636.13,9.96,8.74" target="#b1">[2]</ref>, asafaya/bert-base-arabic <ref type="bibr" coords="7,316.28,636.13,14.61,8.74" target="#b20">[21]</ref>, aubmindlab/bert-base-arabert <ref type="bibr" coords="7,467.31,636.13,9.96,8.74" target="#b2">[3]</ref>, and aubmindlab/bert-base-arabertv01 <ref type="bibr" coords="8,301.32,118.99,9.96,8.74" target="#b2">[3]</ref>. Out of four, we found three to have promise, aubmindlab/bert-base-arabertv01, aubmindlab/bert-base-arabert, and asafaya/bertbase-arabic.</p><p>Classes were imbalanced in the Arabic training dataset with 30% of tweets labeled as part of the check-worthy class. In order to address the imbalanced classes, we chose to upsample the positive class using machine translation via Amazon Web Services (AWS) Translate.</p><p>Tweets from the positive class in the training and development sets were translated to English and then back to Arabic (ar→en→ar), appended to our training dataset, and assigned a label of check-worthy. This improved both precision and recall for check-worthy tweets, but slightly harmed the precision and recall for tweets that were not check-worthy. As the goal is to surface and rank the positive class at various levels of precision, a reduction in the F1-score of the negative class was acceptable for improving the F1-score of the positive class.</p><p>After a grid search, our final models were fine-tuned with 2 epochs, a learning rate of 2e-05, Adam optimization, and a batch size of 32. We used a Huggingface BERT sequence classification function <ref type="bibr" coords="8,298.50,311.61,17.07,8.74" target="#b28">[29]</ref> and, like with English, added a linear layer on top of the pooled output.</p><p>This output was then fed through a softmax function, and the difference between the positive and negative class likelihoods were used to rank tweets within each pre-labeled topic category.</p><p>Results Results for our Arabic evaluations can be found in Table <ref type="table" coords="8,421.86,391.99,3.87,8.74">2</ref>. Our official submission to the competition was AraBERT v0.1 Upsampled and was evaluated in 1st place with a P@30 of 0.7000. Our comparative models AraBERT v1.0 Upsampled<ref type="foot" coords="8,215.30,426.28,3.97,6.12" target="#foot_3">7</ref> , AraBERT v0.1 Unmodified, and ArabicBERT-Base Upsampled were evaluated in 2nd, 3rd, and 4th place with P@30 scores of .6750, .6694, and .6639 respectively.</p><p>The benefit of back-translation to upsample the minority class can be seen by comparing AraBERT v0.1 Upsampled (P@30 of 0.7000) with AraBERT v0.1 Unmodified (P@30 of of 0.6694). These were the same model architectures, with identical hyperparameters, but one had upsampled data, and the other did not.</p><p>Comments: Preprocessing Once we had Arabic model performance baselines, we experimented with various preprocessing techniques. We assumed that these steps would reduce noise and help the Arabic BERT models better map words to tokens in its vocabulary. We performed internal evaluations involving variations of removing diacritics, stopwords, urls, punctuation, and also of splitting Table <ref type="table" coords="9,217.28,115.91,4.13,7.89">2</ref>. Accenture results from CheckThat! Task1 Arabic Entry P@5 P@10 P@15 P@20 P@25 P@30 AP AraBERT v0.1 Upsampled 0.7333 0.7167 0.7167 0.6875 0.6933 0.7000 0.6232 AraBERT v1.0 Upsampled 0.6667 0.7417 0.7333 0.7125 0.6900 0.6750 0.5967 AraBERT v0.1 Unmodified 0.6833 0.7083 0.7111 0.6833 0.6833 0.6694 0.6035 ArabicBERT-Base Upsampled 0.6000 0.6917 0.6944 0.6833 0.6667 0.6639 0.5947 underscores. We tested each of these preprocessing functions alone, as well as in combination with other preprocessing functions. We saw no increase in precision or recall from these steps. In fact, many combinations of these functions brought down our overall accuracy. We ultimately chose to forego all preprocessing.</p><p>Comments: Machine Translation Back-translation provides the model with alternative ways to express similar concepts. This makes the model more robust to vocabulary not present in the training data.</p><p>We evaluated three strategies to augment the corpus using translation data.</p><p>adding back-translated data (ar→en→ar) adding the English translation (ar→en) adding both the English and back-translated Arabic text (ar→en; ar→en→ar).</p><p>We found the back-translated Arabic (without English) (ar→en→ar) had the provided the largest increase in accuracy on our internal evaluations. English was chosen as an intermediary language due solely to the fact that AWS has strong English NLP support. Future research may explore which intermediary language translations can offer the largest performance boosts. While we may have benefited from exploring intermediary language alternatives 8 , we had to leave this for future work due to constraints in both time and budget.</p><p>We recognize that this translation approach resulted in label leakage into the hold-out and validation sets, resulting in overfitting on our internal evaluations. However by expanding the contextual vocabulary of the model, we had the intuition this would yield increased performance on the unseen test set.</p><p>Of all of the preprocessing and tuning steps we tried on our internal evaluations, none resulted in a larger accuracy boost than adding this back-translated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work</head><p>New pre-trained neural network models are being released at a rapid pace. The trend is that they are getting larger-trained with more parameters, on larger quantities of text. Additionally, their baseline capabilities are expanding. Work like that which is presented here can be easily updated to take advantage of these new models as they become available. The workflow a year from now will be the same, but performance will improve. Today, BERT and similar pre-trained models have become the new baseline. These systems yield fantastic results, with little training data required for fine-tuning.</p><p>As larger models are created and released, the models become more difficult to understand. Classification and ranking is helpful to support SMEs performing their work, but full decision support systems cannot be black boxes, and need to be able to explain why they made the suggestions they did. We are working on improving the explainability of these models to provide better support to decision makers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper introduced work by Accenture on using BERT and RoBERTa models to classify and rank unsubstantiated claims in social media for professional factchecking. We demonstrate 5 models. We submitted one model to the English track, and placed 1st with a mAP of .8064. We submitted 4 models to the Arabic track, yielding 1st (P@30=.7000), 2nd (P@30=.6750), 3rd (P@30=.6694), and 4th (P@30=.6639) place.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="4,144.73,656.80,188.61,7.86"><p>https://super.gluebenchmark.com/leaderboard</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="6,144.73,634.88,335.86,7.86;6,144.73,645.84,335.86,7.86;6,144.73,656.80,42.76,7.86"><p>We tried concatenating the text field with the pre-labeled topicID field, but this did not improve the model's performance at all, so we chose to exclude topic labels from the model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="7,144.73,656.80,303.83,7.86"><p>In hindsight, these two should have been contributed for formal evaluation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="8,144.73,612.96,335.87,7.86;8,144.73,623.92,335.86,7.86;8,144.73,634.88,335.87,7.86;8,144.73,645.84,335.86,7.86;8,144.73,656.80,42.83,7.86"><p>This is a rapidly evolving area of NLP. At the time of the challenge, documentation was not yet published for AraBERT v1.0. We did not realize v1.0 required running Farasa<ref type="bibr" coords="8,174.32,634.88,9.73,7.86" target="#b0">[1]</ref> as a preprocessing step for tokenization before utilization. We expect an Upsampled v1.0 to beat an Upsampled v0.1 when utilizing the necessary Arabic segmenter.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,392.22,337.64,7.86;10,151.52,403.18,329.07,7.86;10,151.52,414.14,329.07,7.86;10,151.52,425.10,25.60,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,373.63,392.22,106.96,7.86;10,151.52,403.18,79.31,7.86">Farasa: A fast and furious segmenter for arabic</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Abdelali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Darwish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,250.45,403.18,230.14,7.86;10,151.52,414.14,284.02,7.86">Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: Demonstrations</title>
		<meeting>the 2016 conference of the North American chapter of the association for computational linguistics: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,436.37,337.64,7.86;10,151.52,447.33,287.62,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,357.70,436.37,122.89,7.86;10,151.52,447.33,121.55,7.86">Deep learning models for multilingual hate speech detection</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Aluru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06465</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.96,458.60,337.63,7.86;10,151.52,469.56,329.07,7.86;10,151.52,480.52,329.07,7.86;10,151.52,491.48,291.90,8.12" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,290.48,458.60,190.11,7.86;10,151.52,469.56,95.43,7.86">AraBERT: Transformer-based model for arabic language understanding</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Antoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hazem</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2003.00104v2.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,271.06,469.56,209.53,7.86;10,151.52,480.52,329.07,7.86;10,151.52,491.48,37.21,7.86">Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection</title>
		<meeting>the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,502.75,337.63,7.86;10,151.52,513.71,329.07,7.86;10,151.52,524.67,329.07,7.86;10,151.52,535.63,329.07,7.86;10,151.52,546.59,62.50,7.86" xml:id="b3">
	<analytic>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Névéol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,385.78,513.71,94.81,7.86;10,151.52,524.67,329.07,7.86;10,151.52,535.63,258.98,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction Proceedings of the Eleventh International Conference of the CLEF Association (CLEF 2020)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,557.86,337.64,7.86;10,151.52,568.81,329.07,7.86;10,151.52,579.77,329.07,7.86;10,151.52,590.73,190.89,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,182.56,579.77,298.03,7.86;10,151.52,590.73,86.54,7.86">Overview of CheckThat! 2020: Automatic identification and verification of claims in social media</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hamdan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Sheikh Ali</surname></persName>
		</author>
		<editor>Arampatzis et al.</editor>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,602.00,337.63,7.86;10,151.52,612.96,329.07,7.86;10,151.52,623.92,329.07,7.86;10,151.52,634.88,329.07,7.86;10,151.52,645.84,329.07,7.86;10,151.52,656.80,143.08,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,442.07,645.84,38.53,7.86;10,151.52,656.80,114.40,7.86">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,119.67,337.64,7.86;11,151.52,130.63,242.91,7.86" xml:id="b6">
	<monogr>
		<title level="m" coord="11,382.62,119.67,97.97,7.86;11,151.52,130.63,214.23,7.86">Working Notes of CLEF 2020-Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,140.96,337.64,8.11;11,151.52,152.56,266.10,7.47" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Committee</surname></persName>
		</author>
		<ptr target="https://sites.google.com/view/clef2020-checkthat/tasks/tasks-1-5-check-worthiness" />
		<title level="m" coord="11,221.73,140.96,131.50,7.86">Tasks 1 &amp; 5: Check-worthiness</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,162.24,337.63,7.86;11,151.52,173.20,329.07,7.86;11,151.52,184.15,97.80,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,383.13,162.24,97.46,7.86;11,151.52,173.20,255.86,7.86">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.62,194.48,337.98,7.86;11,151.52,205.44,329.07,7.86;11,151.52,216.39,329.07,7.86;11,151.52,227.35,33.28,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="11,329.77,205.44,150.82,7.86;11,151.52,216.39,263.76,7.86">Overview of CheckThat! 2020 Arabic: Automatic identification and verification of claims in social media</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hamdan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<editor>Cappellato et al.</editor>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,237.68,337.97,7.86;11,151.52,248.63,329.07,7.86;11,151.52,259.59,128.82,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<title level="m" coord="11,151.52,248.63,300.08,7.86">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.62,269.92,337.98,7.86;11,151.52,280.85,251.47,8.14" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="11,246.70,269.92,201.95,7.86">Fine-tuned language models for text classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno>CoRR abs/1801.06146</idno>
		<ptr target="http://arxiv.org/abs/1801.06146" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,291.20,337.98,7.86;11,151.52,302.16,169.13,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="11,340.93,291.20,139.66,7.86;11,151.52,302.16,140.46,7.86">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,312.48,337.98,7.86;11,151.52,323.44,329.07,7.86;11,151.52,334.37,321.62,8.14" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="11,274.64,323.44,205.95,7.86;11,151.52,334.40,34.83,7.86">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR abs/1907.11692</idno>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,344.72,337.97,7.86;11,151.52,355.68,329.07,7.86;11,151.52,366.64,97.80,7.86" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Salathé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">E</forename><surname>Kummervold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07503</idno>
		<title level="m" coord="11,339.89,344.72,140.70,7.86;11,151.52,355.68,263.21,7.86">Covid-twitter-bert: A natural language processing model to analyse COVID-19 content on twitter</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.62,376.96,337.98,7.86;11,151.52,387.92,329.07,7.86;11,151.52,398.88,329.07,7.86;11,151.52,409.84,329.07,8.12;11,151.52,421.44,122.88,7.47" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,356.16,376.96,124.43,7.86;11,151.52,387.92,256.57,7.86">A monolingual approach to contextualized word embeddings for mid-resource languages</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Ortiz Suárez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.156</idno>
		<ptr target="http://dx.doi.org/10.18653/v1/2020.acl-main.156" />
	</analytic>
	<monogr>
		<title level="m" coord="11,418.67,387.92,61.91,7.86;11,151.52,398.88,329.07,7.86">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,431.12,337.98,7.86;11,151.52,442.08,328.14,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,195.45,442.08,166.59,7.86">Deep contextualized word representations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,383.14,442.08,67.85,7.86">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,452.40,337.97,7.86;11,151.52,463.36,329.07,7.86;11,151.52,474.32,66.81,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="11,442.07,452.40,38.52,7.86;11,151.52,463.36,176.52,7.86">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>OpenAI</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,484.64,337.98,7.86;11,151.52,495.60,88.70,7.86" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<title level="m" coord="11,284.91,484.64,195.69,7.86;11,151.52,495.60,60.02,7.86">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,505.92,337.98,7.86;11,151.52,516.88,329.07,7.86;11,151.52,527.84,69.14,7.86" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rasmy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhi</surname></persName>
		</author>
		<title level="m" coord="11,350.84,505.92,129.76,7.86;11,151.52,516.88,329.07,7.86;11,151.52,527.84,40.47,7.86">Med-bert: pre-trained contextualized embeddings on large-scale structured electronic health records for disease prediction</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,538.16,337.97,7.86;11,151.52,549.12,329.07,7.86;11,151.52,560.08,212.80,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,301.88,538.16,178.71,7.86;11,151.52,549.12,179.01,7.86">Kuisail at semeval-2020 task 12: Bert-cnn for offensive speech identification in social media</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Safaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abdullatif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,351.61,549.12,128.98,7.86;11,151.52,560.08,184.13,7.86">Proceedings of the International Workshop on Semantic Evaluation (SemEval)</title>
		<meeting>the International Workshop on Semantic Evaluation (SemEval)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,570.40,337.98,7.86;11,151.52,581.36,329.07,7.86;11,151.52,592.32,113.78,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="11,271.92,570.40,140.49,7.86">Japanese and Korean voice search</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,458.45,570.40,22.14,7.86;11,151.52,581.36,324.28,7.86">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,602.64,337.98,7.86;11,151.52,613.60,85.11,7.86" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="11,298.12,602.64,182.47,7.86;11,151.52,613.60,56.44,7.86">Neural machine translation of rare words with subword units</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,623.92,337.97,7.86;11,151.52,634.88,329.07,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,149.30,7.86" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="11,442.93,634.88,37.66,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,47.58,7.86">Overview of CheckThat! 2020 English: Automatic identification and verification of claims in social media</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<editor>Cappellato et al.</editor>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,119.67,337.98,7.86;12,151.52,130.63,208.99,7.86" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="12,297.23,119.67,183.36,7.86;12,151.52,130.63,33.25,7.86">The cost of training NLP models: A concise overview</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08900v1</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,141.59,337.97,7.86;12,151.52,152.55,329.06,7.86;12,151.52,163.51,133.43,7.86" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="12,200.47,152.55,250.88,7.86">Ernie: Enhanced representation through knowledge integration</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,174.47,337.98,7.86;12,151.52,185.43,234.36,7.86" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<title level="m" coord="12,350.65,174.47,129.94,7.86;12,151.52,185.43,205.68,7.86">Well-read students learn better: On the importance of pre-training compact models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,196.39,337.98,7.86;12,151.52,207.34,328.24,7.86" xml:id="b27">
	<monogr>
		<title level="m" type="main" coord="12,422.24,196.39,58.35,7.86;12,151.52,207.34,299.56,7.86">Glue: A multitask benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,218.30,337.98,7.86;12,151.52,229.26,329.07,7.86;12,151.52,240.20,286.84,7.89" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="12,340.68,229.26,139.91,7.86;12,151.52,240.22,151.82,7.86">Huggingface&apos;s transformers: Stateof-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<idno>ArXiv abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,251.18,337.98,7.86;12,151.52,262.14,329.07,7.86;12,151.52,273.07,329.07,8.14;12,151.52,284.70,122.39,7.47" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="12,198.44,262.14,282.16,7.86;12,151.52,273.10,160.07,7.86">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno>CoRR abs/1506.06724</idno>
		<ptr target="http://arxiv.org/abs/1506.06724" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
