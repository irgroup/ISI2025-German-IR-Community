<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.01,115.96,295.34,12.62;1,236.58,133.89,142.20,12.62">Unsupervised Pre-training for Biomedical Question Answering</title>
				<funder>
					<orgName type="full">Chan Zuckerberg Initiative</orgName>
				</funder>
				<funder>
					<orgName type="full">UMass Amherst Center for Data Science and the Center for Intelligent Information Retrieval</orgName>
				</funder>
				<funder ref="#_YnZEPqb #_djAYYYd">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,137.97,171.56,97.78,8.74"><forename type="first">Vaishnavi</forename><surname>Kommaraju</surname></persName>
							<email>vkommaraju@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,250.87,171.56,97.87,8.74"><forename type="first">Karthick</forename><surname>Gunasekaran</surname></persName>
							<email>kgunasekaran@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.88,171.56,31.14,8.74"><forename type="first">Kun</forename><surname>Li</surname></persName>
							<email>kunli@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,410.15,171.56,60.00,8.74"><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
							<email>tbansal@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,171.64,183.51,82.49,8.74"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
							<email>mccallum@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.69,183.51,65.65,8.74"><forename type="first">Ivana</forename><surname>Williams</surname></persName>
							<email>iwilliams@chanzuckerberg.com</email>
							<affiliation key="aff1">
								<orgName type="department">Chan Zuckerberg Initiative</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.27,183.51,78.98,8.74"><forename type="first">Ana-Maria</forename><surname>Istrate</surname></persName>
							<email>aistrate@chanzuckerberg.com</email>
							<affiliation key="aff1">
								<orgName type="department">Chan Zuckerberg Initiative</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.01,115.96,295.34,12.62;1,236.58,133.89,142.20,12.62">Unsupervised Pre-training for Biomedical Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">49CCBCFE29F094F57496804053C5777F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Biomedical question answering</term>
					<term>self-supervised data generation</term>
					<term>language models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore the suitability of unsupervised representation learning methods on biomedical text -BioBERT, SciBert, and BioSentVec -for biomedical question answering. To further improve unsupervised representations for biomedical QA, we introduce a new pre-training task from unlabeled data designed to reason about biomedical entities in the context. Our pre-training method consists of corrupting a given context by randomly replacing some mention of a biomedical entity with a random entity mention and then querying the model with the correct entity mention in order to locate the corrupted part of the context. This de-noising task enables the model to learn good representations from abundant, unlabeled biomedical text that helps QA tasks and minimizes the train-test mismatch between the pre-training task and the downstream QA tasks by requiring the model to predict spans. Our experiments show that pre-training BioBERT on the proposed pre-training task significantly boosts performance and outperforms the previous best model from the 7th BioASQ Task 7b-Phase B challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate systems for biomedical question-answering have the potential to be useful for a range of problems including clinical decision making, researching disease treatments and symptoms, answering user questions and more. The task of machine reading comprehension (MRC) aims to evaluate this ability, where the model is presented with a context along with a question regarding the context and is expected to predict the answer to the question. MRC has received significant interest, where specially in the general domain several large datasets for supervised learning of MRC models have been proposed <ref type="bibr" coords="2,383.74,118.99,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="2,400.90,118.99,11.62,8.74" target="#b27">28]</ref>. Recently, selfsupervised pre-training of transformer models <ref type="bibr" coords="2,335.42,130.95,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="2,347.60,130.95,12.73,8.74" target="#b16">17,</ref><ref type="bibr" coords="2,361.98,130.95,12.73,8.74" target="#b23">24]</ref> with language modeling objectives has been shown to learn good feature representations and improve performance on many question-answering tasks <ref type="bibr" coords="2,344.01,154.86,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="2,356.19,154.86,7.75,8.74" target="#b8">9,</ref><ref type="bibr" coords="2,365.59,154.86,11.62,8.74" target="#b9">10]</ref>.</p><p>In the biomedical domain, large MRC datasets have been scarce as annotating data can be expensive, and typical datasets, for example training dataset of BioASQ competitions <ref type="bibr" coords="2,230.73,191.12,14.61,8.74" target="#b19">[20]</ref>, are significantly smaller than general domain datasets like SQuAD <ref type="bibr" coords="2,190.00,203.07,14.61,8.74" target="#b17">[18]</ref>, which limits the accuracy of supervised models for biomedical QA. To overcome this, several recent methods have leveraged different avenues of distant supervision to create large datasets for learning MRC models <ref type="bibr" coords="2,452.92,226.98,15.50,8.74" target="#b14">[15,</ref><ref type="bibr" coords="2,470.07,226.98,7.01,8.74" target="#b7">8]</ref>. Moreover, following the success of unsupervised pre-training <ref type="bibr" coords="2,398.17,238.94,9.96,8.74" target="#b5">[6]</ref>, recent methods have pre-trained BERT language model on biomedical datasets <ref type="bibr" coords="2,414.87,250.89,15.50,8.74" target="#b11">[12,</ref><ref type="bibr" coords="2,432.02,250.89,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="2,441.43,250.89,7.01,8.74" target="#b1">2]</ref>, which has been shown to learn better representations for biomedical text, improving performance for biomedical MRC <ref type="bibr" coords="2,283.59,274.80,15.50,8.74" target="#b11">[12,</ref><ref type="bibr" coords="2,300.75,274.80,11.62,8.74" target="#b26">27]</ref>.</p><p>In this work, we focus on learning good representations of biomedical text from unsupervised data that are helpful for QA. To this end, we consider three popular methods for unsupervised representations in the biomedical domain: BioBERT <ref type="bibr" coords="2,180.69,323.01,14.61,8.74" target="#b11">[12]</ref>, SciBERT <ref type="bibr" coords="2,246.38,323.01,9.96,8.74" target="#b3">[4]</ref>, and BioSentVec <ref type="bibr" coords="2,335.86,323.01,9.96,8.74" target="#b4">[5]</ref>. We evaluate the performance of these methods with fine-tuning on three MRC tasks: factoid, list and yes/no questions from the BioASQ Task 8b challenge. Further, since transfer learning from general domain QA datasets has been useful for improving performance on biomedical QA <ref type="bibr" coords="2,203.07,370.83,15.50,8.74" target="#b22">[23,</ref><ref type="bibr" coords="2,220.24,370.83,11.62,8.74" target="#b11">12]</ref>, we evaluate transfer from SQuAD <ref type="bibr" coords="2,390.29,370.83,15.50,8.74" target="#b17">[18]</ref> and PubMedQA <ref type="bibr" coords="2,134.77,382.79,10.52,8.74" target="#b7">[8]</ref> datasets when using the pre-trained models and find improvements when using these datasets for additional fine-tuning. Finally, to leverage abundant unlabelled biomedical data, we develop a new pre-training method for improving biomedical MRC performance.</p><p>Our pre-training method is focused on learning good representations of biomedical text and developing reasoning about entities in context to help MRC tasks. The pre-training approach involves finding mentions of entities, using a biomedical named entity tagger <ref type="bibr" coords="2,247.27,466.87,14.61,8.74" target="#b21">[22]</ref>, and corrupting a random entity mention in the context by replacing it with another random entity mention from the corpus. The model is then queried with the correct entity mention -similar to a question in an MRC model -and is required to predict the location of the corrupted entity mention from the context. As the method does not require expensive human annotation it benefits from training on large amounts of biomedical text data. We use a large corpora of Pubmed abstracts for this pre-training. Moreover, the pre-training approach minimizes train-test mismatch for MRC and we can reuse the entire MRC model used in pre-training (including classification layers) and fine-tune it for any particular biomedical MRC task.</p><p>We evaluate these approaches on the tasks of factoid, yes/no and list questions using BioASQ 7b challenge dataset and submit the trained models for the BioASQ Task 8b Biomedical Semantic QA challenge. The main observations from this work include:</p><p>1. Self Supervised de-noising approach improves the performance for all three question types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Performance of BioBERT and</head><p>SciBERT is observed to be comparable. 3. Using general domain QA data, such as SQuAD and PubmedQA, for additional fine-tuning of the pre-trained model improves performance on biomedical QA. 4. BioSentVec can be used to supplement BioBERT/SciBERT model's performance but doesn't perform well on its own.</p><p>We describe the modeling approach that we consider as well as the pretrained models used in Section 2, describe the proposed self-supervised de-noising approach in Section 3, present our experimental results and analysis in Section 5, discuss related work in Section 4, and conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>We discuss here the pre-trained models considered and the QA model used for the three types of questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pre-trained models: BioBERT and SciBERT</head><p>We evaluate the performance of pre-trained language models-BioBERT <ref type="bibr" coords="3,465.09,347.11,15.50,8.74" target="#b11">[12]</ref> and SciBERT <ref type="bibr" coords="3,200.82,359.06,10.52,8.74" target="#b3">[4]</ref> for our task. BioBERT and SciBERT are transformer <ref type="bibr" coords="3,465.10,359.06,15.50,8.74" target="#b20">[21]</ref> based models. The input to these is the tokenized question concatenated with corresponding passage (either abstract or snippet) using a separator token <ref type="bibr" coords="3,467.31,382.97,9.96,8.74" target="#b5">[6]</ref>. The input is also prefixed with a special CLS token which can be used as a sentence representation. The representation of each token in input is composed of the concatenation of the embeddings for the token, segment, and its position. These embeddings are then passed through multiple layers of self-attention which yield contextualized representations for each token of the input. For factoid and list type questions we utilize these contextualized token representations whereas yes/no questions utilize the CLS representation from the final layer.</p><p>BioBERT <ref type="bibr" coords="3,194.22,489.66,15.50,8.74" target="#b11">[12]</ref> model was the first BERT model <ref type="bibr" coords="3,368.45,489.66,10.52,8.74" target="#b5">[6]</ref> trained on biomedical text using the pre-training method introduced by BERT <ref type="bibr" coords="3,392.09,501.61,9.96,8.74" target="#b5">[6]</ref>. It is pre-trained on 18B words of PubMed (from abstracts and full text articles) apart from Wikipedia and Books corpus originally used in the BERT training. BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on the biomedical corpora. In three representative biomedical NLP tasks including biomedical named entity recognition, relation extraction, and question answering, BioBERT outperforms most of the previous state-of-the-art models.</p><p>SciBERT <ref type="bibr" coords="3,190.47,608.30,10.52,8.74" target="#b3">[4]</ref> was trained on papers from the corpus of semanticscholar.org. The corpus size was 1.14M research papers with 3.1B tokens and uses the full text of the papers in training, not just abstracts. SciBERT has its own vocabulary (scivocab) that's built to best match the training corpus. The training procedure of SciBERT is similar to BioBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question answering model</head><p>Our text representations come from BioBERT or SciBERT models. There are then passed through task-specific layers for each QA task: yes/no, factoid and list. The weights of the task-specific layers as well as the BioBERT/SciBERT weights are all fine-tuned during the training process. We discuss the model variations for the three tasks.</p><p>Yes/No: The CLS token embedding from the final transformer layer is fed into a fully connected layer to obtain the logit s for the binary classification. The probability of a sequence being "yes" is given by:</p><formula xml:id="formula_0" coords="4,269.83,264.03,74.01,22.49">p = 1 1 + exp -(c•s)</formula><p>where c is the representation of the [CLS] token obtained from the final layer of BioBERT and s is a learnable vector embedding. The cross entropy loss between the predicted yes probability and the corresponding ground truth is used as the loss function.</p><p>Factoid/List: The final layer has a start and end vector denoted by S and E which are trainable parameters. We compute the probabilities for the i th token to be the start of the answer and the j th token to be the end of the answer as:</p><formula xml:id="formula_1" coords="4,230.03,409.16,153.10,26.29">p s i = exp S•ti i exp S•ti , p e i = exp E•ti i exp E•ti</formula><p>where t i denotes the i th token's representation from the final layer of BioBERT/S-ciBERT. The loss is defined by taking the mean of negative log likelihood of start and end probabilities. For selecting a span, the score of candidate span from token t i to token t j is then defined as S • t i + E • t j 3 Leveraging Unlabeled Data for QA</p><p>Obtaining training data for Question Answering (QA) is often time-consuming and expensive. Most of existing QA datasets are only available for general domains and biomedical QA datasets are often small. We consider approaches to leverage unlabeled text to help learn better QA models. Recently, an approach to use unlabeled text to generate (context, question, answer) tuples for training factoid models was proposed <ref type="bibr" coords="4,261.40,608.30,14.61,8.74" target="#b12">[13]</ref>. However, this approach relies on a translation model to generate questions and good models for biomedical domain are not readily available. Moreover it only applied to factoid and list type questions.</p><p>We consider this approach and introduce a simpler approach which we found to work well in practice on biomedical QA tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised QA by cloze translation [13]</head><p>There are two major components of this approach. First, a named entity extractor detects named entities from a given context. One of the occurrence of the entity is selected as the required answer for the context. Now to create the question, the sentence in which that entity occurs is taken and the entity is replaced with a MASK token. After this step, a cloze type question can be generated for the sentence containing the MASK token. Then, based on the name entity that were masked and the masked statement, a rule-based approach generates a question by selecting a wh* type words (Where, When, How, What, Wh**). For example, if the noun phrase is a number (eg :"2020"), the question type words is most likely to be "When". Since the rule based approaches can be prone to errors, the authors <ref type="bibr" coords="5,220.26,257.26,15.50,8.74" target="#b12">[13]</ref> proposed using a seq2seq model to transform the cloze style question into a natural question.</p><p>3.2 Self-supervised de-noising The above method requires a trained seq2seq model and based on preliminary analysis we found many errors in question generation when the model was applied to biomedical text. We thus propose here a simpler de-noising approach that doesn't require a question generation component but still helps learning better QA models from unlabeled data. Since QA tasks often involve questions about named entities <ref type="bibr" coords="6,230.89,118.99,14.61,8.74" target="#b12">[13]</ref>, our approach is focused on learning about mentions of entities in context by finding incorrect entity mentions in a corrupted context. Moreover, our approach can also help learning about yes/no type of questions as we describe below.</p><p>Factoid/List: The method involves corrupting a given context by randomly replacing a biomedical entity name present in the context with another entity name from same entity type. We then query the model with correct entity name as the question and the answer to this question would be the incorrect entity that it is replaced with. In the process of locating the corrupted entity location, the model would understand the semantic meaning of the context. Figure <ref type="figure" coords="6,442.01,236.34,9.96,8.74" target="#fig_0">1a</ref> shows an example for Factoid/List de-noising where the correct entity "Nivolumab" is replaced by the wrong entity name "Bortezomib" in the context (left shows the original context with all entities marked in green, and in the right we have the corrupted context with incorrect entity in red). The correct entity name is posed as the question and the model has to predict the location where its occurrence was corrupted with another entity name.</p><p>Yes/No: The generation of QA data is slightly different for factoid type of questions and Yes/No type. In case of Yes/No, we select one biomedical entity name present in the context. For a yes instance, we feed the correct biomedical entity name in the question along with the unmodified context to the model and the answer would be "Yes". For a no instance, we replace the biomedical entity name in the context randomly with another biomedical entity, then we pair the same wrong biomedical entity name as the question and feed it to the model and the answer to this question would be a "No". Figure <ref type="figure" coords="6,385.52,413.46,10.52,8.74" target="#fig_0">1b</ref> shows the example for yes/no type where on the left we have the correct context with the entity name highlighted in green and on the right we have the modified context which remains same for a "yes" type and changed entity name in red for "no" type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Along with the fast development of the NLP area, QA in the biomedical domain has received much attention from the research community. In BioASQ 2015, Yenala et al <ref type="bibr" coords="6,189.57,524.61,15.50,8.74" target="#b24">[25]</ref> presented a PubMed search engine which leveraged web search results and domain words, and a new answering ranking rule to improve the question processing. The authors' approach relies on using the PubMed search engine to retrieve relevant documents, and then extract the snippet based on number of common domain words of the top 10 sentences of the retrieved documents and the question. At the same time, Zhang et al. <ref type="bibr" coords="6,378.86,584.39,15.50,8.74" target="#b29">[30]</ref> presented a generic retrieval model based on sequential dependence model, word embedding and ranking model for document retrieval. The proposed approach has two stepssplit the top-ranked documents into sentences, and apply the same approach as Yenala et al <ref type="bibr" coords="6,190.12,632.21,15.50,8.74" target="#b24">[25]</ref> for snippets retrieval.</p><p>Lee et al. <ref type="bibr" coords="6,193.03,644.16,15.50,8.74" target="#b10">[11]</ref> have introduced KSAnswer biomedical QA system in BioASQ 2016. KSAnswer was tested in the BioASQ task 4b phase A challenge. The model aims to retrieve candidate snippets using a cluster-based language model. Further, it re-ranks the retrieved top-N snippets using five independent similarity models depending on shallow semantic analysis. SentiWordNet based lexical resource to generate the exact answers for yes/no questions was proposed <ref type="bibr" coords="7,465.10,154.86,15.50,8.74" target="#b18">[19]</ref> in 2017 BioASQ challenge. The authors proposed a UMLS meta-thesaurus and term frequency metrics for answering factoid and list questions whereas a retrieval model based on UMLS concepts was used for generating ideal answers.</p><p>Since biomedical QA datasets are usually small, many approaches have focused on generating QA data from other tasks. EmrQA <ref type="bibr" coords="7,393.75,215.03,15.50,8.74" target="#b14">[15]</ref> proposed using annotated data from other clinical task by converting them into a QA format using question generation templates. Jin et al. <ref type="bibr" coords="7,343.19,238.94,10.52,8.74" target="#b7">[8]</ref> introduced a novel biomedical question answering dataset created from PubMed abstracts which involves answering a question by yes/no/maybe. This dataset contains some expert annotated data, unlabeled and artificially generated data which is used to finetune BioBERT model. The authors mention that each instance is composed of question (derived from title), context (derived from abstract), a long answer (conclusion of abstract) and yes/no/maybe summarizing the conclusion. Since general domain QA data is abundant, transfer learning from general domain QA data, such as SQuAD <ref type="bibr" coords="7,206.69,334.58,14.61,8.74" target="#b17">[18]</ref>, has also been found beneficial for biomedical QA <ref type="bibr" coords="7,445.10,334.58,15.50,8.74" target="#b22">[23,</ref><ref type="bibr" coords="7,462.26,334.58,11.62,8.74" target="#b11">12]</ref>.</p><p>Pre-trained language models <ref type="bibr" coords="7,279.27,346.93,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="7,291.45,346.93,12.73,8.74" target="#b16">17,</ref><ref type="bibr" coords="7,305.84,346.93,12.73,8.74" target="#b23">24]</ref> have shown success in learning general purpose representations which improve performance on a number of tasks including QA <ref type="bibr" coords="7,195.44,370.84,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="7,207.61,370.84,11.62,8.74" target="#b11">12]</ref>. Unsupervised approach to QA by using cloze statements as questions was proposed <ref type="bibr" coords="7,239.07,382.80,15.50,8.74" target="#b12">[13]</ref> for general domain QA. SpanBERT <ref type="bibr" coords="7,415.00,382.80,10.52,8.74" target="#b8">[9]</ref> changed the BERT training objective to a span-based objective demonstrating improvements. BioBERT <ref type="bibr" coords="7,180.13,406.71,15.50,8.74" target="#b11">[12]</ref> and SciBERT <ref type="bibr" coords="7,261.00,406.71,10.52,8.74" target="#b3">[4]</ref> were introduced as BERT [6] models trained on biomedical and scientific text, respectively. Biomedical QA using BioBERT was used <ref type="bibr" coords="7,157.10,430.62,15.50,8.74" target="#b26">[27]</ref> in 2019 BioASQ Task B challenge. Their model outperformed previous state-of-the-art models.</p><p>Our pre-training approach requires a model for finding mentions of named entities in biomedical text. There are now increasingly accurate models that can be leveraged for this purpose <ref type="bibr" coords="7,259.08,478.83,15.50,8.74" target="#b21">[22,</ref><ref type="bibr" coords="7,276.24,478.83,7.75,8.74" target="#b6">7,</ref><ref type="bibr" coords="7,285.65,478.83,12.73,8.74" target="#b11">12,</ref><ref type="bibr" coords="7,300.04,478.83,7.01,8.74" target="#b2">3]</ref>. Recently, some pre-training approaches have been proposed to incorporate factual knowledge into pre-trained models <ref type="bibr" coords="7,465.09,490.79,15.50,8.74" target="#b15">[16,</ref><ref type="bibr" coords="7,134.77,502.74,12.73,8.74" target="#b28">29]</ref> which also require extracting such named entities from text. While these have not been applied to biomedical domain and are not focused on QA tasks, they show promise in incorporating factual knowledge in pre-trained models. Our QA targeted self-supervised de-noising method has a similar motivation and helps incorporate knowledge about biomedical named entities during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We primarily use the BioASQ Task 8b-Phase B BioQA dataset to train our model and participate in the challenge. To compare our results with previous models, we train and evaluate our model on BioASQ 7b-Phase B BioQA dataset. The dataset contains four main types of questions -Yes/No, Factoid, List and Summary questions. The task 8b consists of 3243 question, answer pairs in the training set. The task 8b test dataset is released incrementally in 5 phases over the period of March -May 2020. Each phase has 100 test questions varying across different question types. Since, we cannot have access to the test sets to evaluate how each model we build performs, we train and evaluate our model on BioASQ Task 7b training and golden enriched test data respectively. The Table <ref type="table" coords="8,162.18,202.68,4.98,8.74" target="#tab_0">1</ref> shows the statistics of the question types in 7b/8b training and 7b test sets. We create two variations of the dataset. In one variation we use abstract from the documents as the context and in another version we use the snippets provided as the context.</p><p>We use additional training data to pre-train our model. We adopt techniques to generate more training data from PubMed abstracts as discussed in the earlier section of self-supervised de-noising. In case of Yes/No, more adversarial examples were created by pairing random context to questions and answering them as no type question to address class imbalance. The PubMed database has over 30 million citations for biomedical literature from MEDLINE, life science journals, and online books.</p><p>Further, we use other extractive question answering dataset like SQuAD v1.1 for factoid/list type questions and SQuAD 2.0 and PubmedQA for yes/no type to train the respective model. Stanford Question Answering Dataset (SQuAD) <ref type="bibr" coords="8,134.77,370.05,15.50,8.74" target="#b17">[18]</ref> is a reading comprehension dataset consisting of over 100,000 questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage which is equivalent to a factoid answer to a question and the passage is the context. PubmedQA <ref type="bibr" coords="8,189.33,417.87,10.52,8.74" target="#b7">[8]</ref> dataset with 1k expert-annotated QA consists of yes/no/maybe answers to each question. All the 'maybe' type questions are removed to match the yes/no BioASQ format. We convert all datasets to one unified format which is the SQuAD dataset format where each instance has a question, context and exact answer depending on the type of question.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Total</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>In case of factoid, we take the highest probability answer as the exact answer. We have three evaluation metrics for factoid-Strict Accuracy, Lenient Accuracy and MRR. Strict Accuracy evaluates the exact match of the predicted answer.</p><p>If the exact answer is present in the top 5 predictions then we increment the lenient score.</p><p>In case of list, we set a threshold of 0.42 and consider all the predictions above this threshold as the list of answers to the question. The evaluation metrics for list are Precision, Recall and F1 score.</p><p>In case of yesno, we use the first [CLS] from output layer and use a fully connected layer along with dropout to obtain the logit values. The positive logit values denote an 'yes' while negative denotes a 'no'. For each question, all the logit values for all question-context are added together and if the final value Table <ref type="table" coords="10,161.64,336.89,3.87,8.74">5</ref>: Performance comparison between different variations of models for Factoid and List type of question answers on BioASQ 7b test data obtained is positive then its classified as an 'yes' otherwise as 'no'. The evaluation metrics used for the yes/no task are Accuracy, F1 score, F1 yes and F1 no scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Models</head><p>We develop different models for each type of questions. For all the tasks, our base model is either BioBERT or SciBERT. We fine tune the base model using one or more datasets chosen from SQuAD, PubMedQA, De-noising/Unsupervised Cloze Translation(UCT), PubMED, BioASQ depending on the task. The order in which the datasets are mentioned in the results Table <ref type="table" coords="10,387.19,507.77,4.98,8.74">5</ref> is the order of finetuning our base model. We also use BioSentVec <ref type="bibr" coords="10,257.48,531.95,9.96,8.74" target="#b4">[5]</ref>, <ref type="bibr" coords="10,274.26,531.95,15.50,8.74" target="#b25">[26]</ref> model to get the embeddings and compute the similarity score between the question/context and predicted answers. We add this score to the BioBERT predicted scores to make the final predictions. In case of yes/no, a three layer neural network is trained using the embeddings to get the classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>In this section, we present the results of our models that are evaluated on the previous edition of BioASQ 7b, Phase B Biomedical Semantic QA challenge test dataset and on the recent BioASQ 8b, Phase B challenge. The results obtained by the different BioBERT variations on BioASQ 7b test data can be seen in Tables <ref type="table" coords="11,166.18,290.41,4.98,8.74" target="#tab_3">4</ref> and<ref type="table" coords="11,194.02,290.41,3.87,8.74">5</ref>. A comparison of BioBERT and SciBERT are made in Tables <ref type="table" coords="11,475.61,290.41,4.98,8.74" target="#tab_1">2</ref> and<ref type="table" coords="11,153.87,302.37,3.87,8.74">5</ref>. The results obtained from the leader board of BioASQ 8b challenge <ref type="bibr" coords="11,460.34,302.37,10.51,8.74" target="#b0">[1]</ref> is shown in Table <ref type="table" coords="11,204.01,314.32,3.87,8.74" target="#tab_4">6</ref>.</p><p>Results on BioASQ 7b test sets Various experiments are performed and performance of the models are evaluated for the task of question answering that includes yes/no, factoid and list type of questions. The 7b test batches are used for evaluating the model performance. The results are shown in Tables <ref type="table" coords="11,455.42,380.75,4.98,8.74" target="#tab_3">4</ref> and<ref type="table" coords="11,134.77,392.70,3.87,8.74">5</ref>. We reproduced the previous winners results using our base model, BioBERT with SQuAD dataset <ref type="bibr" coords="11,229.61,404.66,15.50,8.74" target="#b26">[27]</ref> The general trend observed is that adding more data tends to improve the performance of the model. This can be clearly seen from data illustrated in the Tables <ref type="table" coords="11,166.81,440.66,4.98,8.74" target="#tab_3">4</ref> and<ref type="table" coords="11,195.90,440.66,3.87,8.74">5</ref>. Even when fine-tuned with additional non-biomedical dataset such as SQuAD, the model is able to perform better than baseline. The use of PubmedQA along with SQuAD datasets for fine-tuning made the Yes/No model more robust.</p><p>It is observed that training the model with unsupervised and self supervised data before fine-tuning with BioASQ data presents a significant boost to the test performance. Unsupervised data generation approach led to increase in model performance for list and factoid. The performance boost can be seen in-spite of the fact that the data generated by unsupervised approach was not of particularly high quality. It can be attributed to the improvement of the model's generalization ability with the huge amount of data. We can also note that using the BioSentVec in the list and factoid along with BioBERT also aids the results. For the factoid and list type question, the BioSentVec is used to generate the similarity scores and use it to balance the power of BioBERT logit in the last layer. The gain obtained on the testing set is consistent with our hypothesis that the correct answer should be similar to the question semantically.</p><p>It can be seen that when the model is trained with self supervised de-noising approach it either out performed all the variations tried by significant margin or matched unsupervised approach for 7b test. In all three question types, the increase in performance can be clearly seen. In case of Yes/No type, the performance of the model greatly improved by the de-noising the data. However, it is noticed that having too many datasets for fine-tuning with de-noising approach resulted in slight decrease in the performance comparatively. The key takeaway is that self supervised approach performed well with less fine-tuning data and trained faster with fewer epochs than other approaches.</p><p>A comparison is carried out to analyse performance of BioBERT and SciB-ERT models in comparable data settings and no significant difference in performance is observed. Tables <ref type="table" coords="12,250.46,215.15,4.98,8.74">5</ref> and<ref type="table" coords="12,277.90,215.15,4.98,8.74" target="#tab_3">4</ref> clearly show that there is no much difference in performance between SciBERT and BioBERT. In case of Yes/No with only BioASQ training, it can be seen that the accuracy of SciBERT is higher. This is mainly due to the large imbalance in the test set towards the positive class ("yes") and the F1 scores are almost the same for both BioBERT and SciBERT with BioASQ training alone.</p><p>Results on BioASQ 8b Challenge The Table <ref type="table" coords="12,352.41,307.37,4.98,8.74" target="#tab_4">6</ref> reports the results obtained on the BioASQ 8b challenge which is conducted in 5 phases. The results are put up on the leaderboard <ref type="bibr" coords="12,236.44,331.28,9.96,8.74" target="#b0">[1]</ref>. We show the comparison of our model performance with the best performing team on the leaderboard for each batch and in each of three type of questions. The challenge ranks the performance based on the Yes/No Accuracy, Factoid-MRR and List-F Measure metrics. The observations made from the Tables <ref type="table" coords="12,233.37,379.10,4.98,8.74" target="#tab_3">4</ref> and<ref type="table" coords="12,261.04,379.10,4.98,8.74">5</ref> can be validated from the 8b challenge results.</p><p>In each of the different phases, models with different configurations are submitted for evaluation. In case of Yes/No Question type, for the test batch 1 BioBERT model fine-tuned on BioASQ 8b training dataset is used. In case of test batch 2, the submission is made by adding in SQuAD fine-tuning to the BioBERT model. For the 3rd and 4th test phases, the submission is made with fine-tuning on PubmedQA and SQuAD datasets. The 5 th test phase submission is carried out with our best model which is with de-noising method.</p><p>In case of Factoid, for the test batch 1, the BioBERT model fine-tuned on SQuAD and BioASQ 8b training data is submitted. For test batch 2, we modified the 8b training data by adding a start index of the exact answer while fine-tuning the model. The BioBERT model fine-tuned with unsupervised data is submitted in the test batch 3 while SciBERT base model and with SQuAD is submitted in test batch 4. The last and final submission is made with BioBERT model pretrained on de-noising data.</p><p>In case of List, the model variations follow as mentioned in Factoid. We submitted BioBERT fine-tuned on SQuAD and BioASQ 8b data for first two test batches with modifications in computing the exact answer start for the second batch. The BioBERT model fine-tuned with unsupervised data is submitted in the test batch 3. BioBERT and SciBERT models fine-tuned on SQuAD and unsupervised data are in submitted test batch 4. The 5 th test phase submission is carried out with our best model that includes training with de-noising data.</p><p>Interestingly, unlike the trend observed in 7b test results 5 for self-supervised de-noising and unsupervised approaches, pre-training with de-noising approach gave better MRR and F-Measure for factoid and list respectively. The BioASQ 8b test batch 5 gave an MRR=0.6354 for de-noising approach while it is only 0.5604 for unsupervised approach in case of factoid. Similarly for list, we achieved F-measure=0.4421 using de-noising data approach while it is 0.2745 for unsupervised approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work we evaluated pre-trained models, BioBERT and SciBERT, for biomedical QA. We proposed a novel approach to pre-training, self-supervised de-noising, which enables learning good representations for QA tasks. The experimental results show that the approach improves the performance of the models in all the QA tasks. One main advantage of this approach is that it is simple and does not require expensive annotation, enabling large-scale pre-training. In the future, it will be interesting to extend the approach to include more complex reasoning required for QA, for example reasoning about multiple entities in context, and including natural form questions generated either through templates or a learned model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,548.71,345.82,8.74;5,134.77,560.66,345.83,8.74;5,134.77,572.62,141.53,8.74;5,134.77,427.04,345.84,93.20"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Self-supervised de-noising approach: The original context is shown on the left, and the modified context is on the right with the entities highlighted in green and incorrect entity in red</figDesc><graphic coords="5,134.77,427.04,345.84,93.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,134.77,499.05,345.83,165.81"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics of 8b and 7b data sets</figDesc><table coords="8,134.77,499.05,345.83,165.81"><row><cell cols="5">Yes/No List Factoid Summary</cell></row><row><cell cols="4">8b train 3243 881 664 941</cell><cell>777</cell></row><row><cell cols="4">7b train 2747 745 556 779</cell><cell>667</cell></row><row><cell>7b test 500</cell><cell>140</cell><cell>88</cell><cell>162</cell><cell>110</cell></row><row><cell>5.2 Implementation details</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">SciBERT and BioBERT based on BERT-Base-Uncased model with 12-layer,</cell></row><row><cell cols="5">768-hidden, 16-heads, 340M parameter is used. Each of the datasets are pre-</cell></row><row><cell cols="5">trained for varying no of epochs. For all models, SQuAD dataset is pre-trained</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,165.95,189.68,283.46,124.01"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison between BioBERT vs SciBERT for Yes/No question answers using different datasets and tested on BioASQ 7b test data.</figDesc><table coords="9,173.78,250.46,270.85,63.24"><row><cell>Dataset</cell><cell>Factoid</cell><cell></cell><cell>List</cell><cell></cell></row><row><cell></cell><cell cols="4">Variant SAcc LAcc Precision Recall MacroF1</cell></row><row><cell>BioASQ</cell><cell>BioBERT 0.23 0.38</cell><cell>0.55</cell><cell>0.32</cell><cell>0.38</cell></row><row><cell></cell><cell>SciBERT 0.23 0.36</cell><cell>0.49</cell><cell>0.28</cell><cell>0.33</cell></row><row><cell cols="2">SQuAD + BioASQ BioBERT 0.26 0.49</cell><cell>0.60</cell><cell>0.45</cell><cell>0.48</cell></row><row><cell></cell><cell>SciBERT 0.26 0.49</cell><cell>0.61</cell><cell>0.47</cell><cell>0.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.77,317.20,345.83,177.10"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison between BioBERT vs SciBERT for Factoid/List question answers using different datasets and tested on BioASQ 7b test data.</figDesc><table /><note coords="9,134.77,389.92,345.82,8.74;9,134.77,401.87,345.82,8.74;9,134.77,413.83,345.83,8.74;9,134.77,425.78,345.82,8.74;9,134.77,437.74,345.83,8.74;9,134.77,449.69,345.82,8.74;9,134.77,461.65,345.83,8.74;9,134.77,473.60,345.83,8.74;9,134.77,485.56,161.28,8.74"><p>for 2 epochs. A batch size of 16 or 32 with maximum sequence length of 384 is used for all the models. In case of Yes/No, model is pretrained with PubMedQA labelled dataset for 8 epochs and de-noising data for 2 epochs. The last step of training is carried out with BioASQ dataset for 2 to 4 epochs depending on other datasets the model is pre-trained on. The learning rate used for yes/no models is 5e-6 and dropout of 0.1. In case of Factoid, the learning rate used is 5e-6. The model is pretrained on de-noising data for 3 epochs and fine-tuned on BioASQ training data for 8 epochs. For List, similar configurations as factoid is exercised and set a threshold of 0.42.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,134.77,117.72,345.83,215.80"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison between different variations of models forYes/No question answers on BioASQ 7b test data</figDesc><table coords="10,138.90,117.72,340.40,215.80"><row><cell>Model</cell><cell></cell><cell cols="2">Yes/No</cell><cell></cell></row><row><cell></cell><cell cols="4">Acc F1Score F1 Yes F1 No</cell></row><row><cell>BioBERT -BioASQ</cell><cell cols="2">0.76 0.76</cell><cell cols="2">0.78 0.74</cell></row><row><cell>BioBERT-SQuAD + BioASQ</cell><cell cols="2">0.80 0.78</cell><cell>0.84</cell><cell>0.7</cell></row><row><cell>BioBERT-PubMedQA + BioASQ</cell><cell cols="2">0.83 0.81</cell><cell cols="2">0.87 0.76</cell></row><row><cell>BioBERT-SQuAD + Denoising + BioASQ</cell><cell cols="2">0.88 0.88</cell><cell cols="2">0.90 0.86</cell></row><row><cell>BioBERT-SQuAD + PubMedQA + BioASQ</cell><cell cols="2">0.83 0.81</cell><cell cols="2">0.87 0.76</cell></row><row><cell cols="3">BioBERT-SQuAD + PubMedQA + Denoising + BioASQ 0.86 0.86</cell><cell cols="2">0.88 0.73</cell></row><row><cell>BioSentVec -3layer NN + BioASQ</cell><cell cols="2">0.66 0.64</cell><cell cols="2">0.74 0.55</cell></row><row><cell>Model</cell><cell>Factoid</cell><cell></cell><cell>List</cell><cell></cell></row><row><cell></cell><cell cols="4">SAcc LAcc Precision Recall MacroF1</cell></row><row><cell>BioBERT -BioASQ</cell><cell>0.23 0.38</cell><cell>0.55</cell><cell>0.32</cell><cell>0.38</cell></row><row><cell>BioBERT-SQuAD + BioASQ</cell><cell>0.26 0.49</cell><cell>0.60</cell><cell>0.45</cell><cell>0.48</cell></row><row><cell>BioBERT-SQuAD + Denoising + BioASQ</cell><cell>0.28 0.54</cell><cell>0.68</cell><cell>0.49</cell><cell>0.51</cell></row><row><cell>BioBERT-SQuAD + UCT + BioASQ</cell><cell>0.28 0.54</cell><cell>0.70</cell><cell>0.46</cell><cell>0.50</cell></row><row><cell cols="2">BioBERT + BioSentVec-SQuAD + UCT + BioASQ 0.31 0.58</cell><cell>0.73</cell><cell>0.35</cell><cell>0.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,134.77,117.32,345.82,123.41"><head>Table 6 :</head><label>6</label><figDesc>BioASQ challenge 8b results per batch. The table here shows our results and the best results among other teams in the leader board.</figDesc><table coords="11,138.18,117.32,341.36,99.67"><row><cell>Batch</cell><cell>Yes/No</cell><cell>Factoid</cell><cell>List</cell><cell></cell></row><row><cell></cell><cell cols="5">System Acc F1 Yes F1 No F1 Score System SAcc LAcc MRR System Precision Recall Macro F1</cell></row><row><cell>1</cell><cell cols="5">Ours 0.6800 0.7778 0.4286 0.6032 Ours 0.3750 0.5938 0.4688 Ours 0.4875 0.2983 0.3448</cell></row><row><cell></cell><cell cols="5">Others 0.8800 0.9091 0.8235 0.8663 Others 0.3438 0.6250 0.4583 Others 0.3884 0.5629 0.4315</cell></row><row><cell>2</cell><cell cols="2">Ours 0.7778 0.8519 0.5556 0.7037 Ours 0.1200 0.2000 0.1480 Ours</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="5">Others 0.9444 0.9630 0.8889 0.9259 Others 0.2800 0.4400 0.3533 Others 0.5643 0.4643 0.4735</cell></row><row><cell>3</cell><cell cols="5">Ours 0.9032 0.9143 0.8889 0.9016 Ours 0.3214 0.4643 0.3810 Ours 0.7361 0.4833 0.5229</cell></row><row><cell></cell><cell cols="5">Others 0.9032 0.9189 0.8800 0.8995 Others 0.3214 0.5357 0.3970 Others 0.5278 0.4778 0.4585</cell></row><row><cell>4</cell><cell cols="5">Ours 0.8077 0.8387 0.7619 0.8003 Ours 0.5000 0.7059 0.5637 Ours 0.5753 0.4182 0.4146</cell></row><row><cell></cell><cell cols="5">Others 0.8462 0.8571 0.8333 0.8452 Others 0.5588 0.7353 0.6384 Others 0.5375 0.5089 0.4571</cell></row><row><cell>5</cell><cell cols="5">Ours 0.7941 0.8205 0.7586 0.7896 Ours 0.5625 0.7188 0.6354 Ours 0.5972 0.3819 0.4421</cell></row><row><cell></cell><cell cols="5">Others 0.8529 0.8649 0.8387 0.8518 Others 0.5313 0.7188 0.6120 Others 0.5516 0.5972 0.5618</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>This work was supported in part by the <rs type="funder">UMass Amherst Center for Data Science and the Center for Intelligent Information Retrieval</rs>, in part by the <rs type="funder">Chan Zuckerberg Initiative</rs>, and in part by the <rs type="funder">National Science Foundation</rs> under Grant No. <rs type="grantNumber">IIS-1514053</rs> and <rs type="grantNumber">IIS-1763618</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YnZEPqb">
					<idno type="grant-number">IIS-1514053</idno>
				</org>
				<org type="funding" xml:id="_djAYYYd">
					<idno type="grant-number">IIS-1763618</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.96,522.46,337.64,8.12;13,151.52,534.07,221.74,7.47" xml:id="b0">
	<monogr>
		<ptr target="http://participants-area.bioasq.org/results/8b/phaseB/" />
		<title level="m" coord="13,151.53,522.46,217.84,7.86">Bioasq 8th edition taskb challenge leaderboard</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,545.09,337.63,7.86;13,151.52,556.05,329.07,7.86;13,151.52,567.00,97.80,7.86" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcdermott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03323</idno>
		<title level="m" coord="13,226.54,556.05,182.28,7.86">Publicly available clinical bert embeddings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.96,578.67,337.63,7.86;13,151.52,589.63,329.07,7.86;13,151.52,600.59,151.33,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,368.06,578.67,112.53,7.86;13,151.52,589.63,329.07,7.86;13,151.52,600.59,14.79,7.86">Simultaneously linking entities and extracting relations from biomedical text without mention-level supervision</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,187.15,600.59,21.29,7.86">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7407" to="7414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,612.26,337.64,7.86;13,151.52,623.21,44.81,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="13,277.20,612.26,203.40,7.86;13,151.52,623.21,16.13,7.86">Scibert: A pretrained language model for scientific text</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,634.88,337.64,7.86;13,151.52,645.84,329.07,7.86;13,151.52,656.80,58.36,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,264.01,634.88,216.59,7.86;13,151.52,645.84,35.74,7.86">Biosentvec: creating sentence embeddings for biomedical texts</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,216.02,645.84,264.57,7.86">IEEE International Conference on Healthcare Informatics (ICHI)</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2018">2019. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,119.67,337.63,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,329.07,7.86;14,151.52,152.55,329.07,7.86;14,151.52,163.51,86.01,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,354.85,119.67,125.74,7.86;14,151.52,130.63,205.07,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,380.67,130.63,99.92,7.86;14,151.52,141.59,329.07,7.86;14,151.52,152.55,174.01,7.86">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="14,142.96,173.70,337.63,7.86;14,151.52,184.65,329.07,7.86;14,151.52,195.61,329.07,7.86;14,151.52,206.57,134.13,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,367.37,173.70,113.21,7.86;14,151.52,184.65,311.09,7.86">Marginal likelihood training of bilstm-crf for biomedical named entity recognition from disjoint label sets</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,151.52,195.61,329.07,7.86;14,151.52,206.57,40.95,7.86">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2824" to="2829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,216.76,337.64,7.86;14,151.52,227.72,278.48,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,375.21,216.76,105.39,7.86;14,151.52,227.72,156.50,7.86">Pubmedqa: A dataset for biomedical research question answering</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,329.40,227.72,71.92,7.86">EMNLP/IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,237.90,337.64,7.86;14,151.52,248.86,329.07,7.86;14,151.52,259.82,97.80,7.86" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<title level="m" coord="14,457.55,237.90,23.04,7.86;14,151.52,248.86,263.82,7.86">Spanbert: Improving pre-training by representing and predicting spans</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.62,270.01,337.98,7.86;14,151.52,280.97,315.13,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="14,349.92,270.01,130.68,7.86;14,151.52,280.97,149.07,7.86">Unifying question answering and text classification via span extraction</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09286</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.62,291.15,337.97,7.86;14,151.52,302.11,329.07,7.86;14,151.52,313.07,329.07,7.86;14,151.52,324.03,329.07,7.86;14,151.52,334.99,329.07,8.11;14,151.52,346.59,85.23,7.47" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,151.52,302.11,329.07,7.86;14,151.52,313.07,164.34,7.86">KSAnswer: Question-answering system of kangwon national university and sogang university in the 2016 BioASQ challenge</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">K</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-3106</idno>
		<ptr target="https://www.aclweb.org/anthology/W16-3106" />
	</analytic>
	<monogr>
		<title level="m" coord="14,337.89,313.07,142.70,7.86;14,151.52,324.03,36.01,7.86">Proceedings of the Fourth BioASQ workshop</title>
		<meeting>the Fourth BioASQ workshop<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">Aug 2016</date>
			<biblScope unit="page" from="45" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,356.13,337.97,7.86;14,151.52,367.09,329.07,7.86;14,151.52,378.05,159.05,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="14,428.04,356.13,52.55,7.86;14,151.52,367.09,324.76,7.86">Biobert: pretrained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08746</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.62,388.24,337.98,7.86;14,151.52,399.19,329.07,7.86;14,151.52,410.15,169.18,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,287.34,388.24,193.25,7.86;14,151.52,399.19,21.94,7.86">Unsupervised question answering by cloze translation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,193.94,399.19,286.65,7.86;14,151.52,410.15,76.28,7.86">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4896" to="4910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,420.34,337.98,7.86;14,151.52,431.27,283.32,7.89" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,363.11,420.34,117.48,7.86;14,151.52,431.30,128.33,7.86">Neural machine reading comprehension: Methods and trends</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,286.89,431.30,67.38,7.86">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">3698</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,441.48,337.97,7.86;14,151.52,452.44,241.72,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,345.09,441.48,135.50,7.86;14,151.52,452.44,158.32,7.86">emrqa: A large corpus for question answering on electronic medical records</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pampari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,330.91,452.44,33.66,7.86">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,462.63,337.97,7.86;14,151.52,473.59,329.07,7.86;14,151.52,484.55,329.07,7.86;14,151.52,495.51,329.07,7.86;14,151.52,506.46,110.45,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,177.37,473.59,219.70,7.86">Knowledge enhanced contextual word representations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,420.74,473.59,59.85,7.86;14,151.52,484.55,329.07,7.86;14,151.52,495.51,329.07,7.86;14,151.52,506.46,34.83,7.86">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,516.65,337.97,7.86;14,151.52,527.58,282.12,7.89" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,442.07,516.65,38.52,7.86;14,151.52,527.61,172.97,7.86">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,331.32,527.61,53.37,7.86">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,537.80,337.98,7.86;14,151.52,548.75,329.07,7.86;14,151.52,559.71,329.08,7.86;14,151.52,570.67,156.73,7.86;14,339.37,571.32,141.22,7.47;14,151.52,582.28,38.16,7.47" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,381.37,537.80,99.23,7.86;14,151.52,548.75,182.27,7.86">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1264</idno>
		<ptr target="http://dx.doi.org/10.18653/v1/D16-1264" />
	</analytic>
	<monogr>
		<title level="m" coord="14,344.91,548.75,135.68,7.86;14,151.52,559.71,292.21,7.86">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,591.82,337.98,7.86;14,151.52,602.78,329.07,7.86;14,151.52,613.74,206.65,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,275.71,591.82,204.88,7.86;14,151.52,602.78,16.79,7.86">A biomedical question answering system in bioasq 2017</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">O</forename><surname>El Alaoui</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-2337</idno>
		<ptr target="https://doi.org/10.18653/v1/W17-2337" />
	</analytic>
	<monogr>
		<title level="m" coord="14,188.23,602.78,251.90,7.86">BioNLP at Association for Computational Linguistics (ACL&apos;17)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,623.92,337.98,7.86;14,151.52,634.88,329.07,7.86;14,151.52,645.84,329.07,7.86;14,151.52,656.80,182.25,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,451.23,634.88,29.36,7.86;14,151.52,645.84,324.84,7.86">Bioasq: A challenge on large-scale biomedical semantic indexing and question answering</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Almirantis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Artieres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Alvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zschunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,187.10,656.80,118.00,7.86">AAAI Fall Symposium Series</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,119.67,337.97,7.86;15,151.52,130.63,329.07,7.86;15,151.52,141.59,167.19,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="15,228.73,130.63,100.53,7.86">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,351.88,130.63,128.71,7.86;15,151.52,141.59,73.89,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,152.55,337.97,7.86;15,151.52,163.48,260.11,7.89" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="15,271.92,152.55,208.67,7.86;15,151.52,163.51,44.36,7.86">Pubtator: a web-based text mining tool for assisting biocuration</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,202.99,163.51,88.21,7.86">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">W1</biblScope>
			<biblScope unit="page" from="518" to="W522" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,174.47,337.98,7.86;15,151.52,185.43,329.07,7.86;15,151.52,196.39,255.00,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="15,318.70,174.47,161.89,7.86;15,151.52,185.43,89.28,7.86">Neural domain adaptation for biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,262.38,185.43,218.22,7.86;15,151.52,196.39,171.04,7.86">Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</title>
		<meeting>the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="281" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,207.34,337.98,7.86;15,151.52,218.30,329.07,7.86;15,151.52,229.26,97.80,7.86" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="15,455.76,207.34,24.83,7.86;15,151.52,218.30,264.15,7.86">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.62,240.22,337.98,7.86;15,151.52,251.18,327.44,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="15,415.11,240.22,65.48,7.86;15,151.52,251.18,252.62,7.86">IIITH at bioasq challange 2015 task 3b: Bio-medical question answering system</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yenala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kamineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Chinnakotla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,425.59,251.18,24.70,7.86">CLEF</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,262.14,337.97,7.86;15,151.52,273.07,329.07,7.89;15,151.52,284.06,172.40,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="15,340.98,262.14,139.61,7.86;15,151.52,273.10,213.34,7.86">Biowordvec, improving biomedical word embeddings with subword information and mesh</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yijia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-019-0055-0</idno>
		<ptr target="https://doi.org/10.1038/s41597-019-0055-0" />
	</analytic>
	<monogr>
		<title level="j" coord="15,371.98,273.10,58.43,7.86">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,295.02,337.98,7.86;15,151.52,305.98,151.17,7.86" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="15,354.23,295.02,126.36,7.86;15,151.52,305.98,122.49,7.86">Pre-trained language model for biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,316.93,337.97,7.86;15,151.52,327.89,329.07,7.86;15,151.52,338.85,97.80,7.86" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11880</idno>
		<title level="m" coord="15,330.04,316.93,150.55,7.86;15,151.52,327.89,261.15,7.86">A survey on machine reading comprehension: Tasks, evaluation metrics, and benchmark datasets</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.62,349.81,337.98,7.86;15,151.52,360.77,329.07,7.86;15,151.52,371.73,323.67,7.86" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="15,393.24,349.81,87.36,7.86;15,151.52,360.77,182.88,7.86">Ernie: Enhanced language representation with informative entities</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,354.34,360.77,126.25,7.86;15,151.52,371.73,230.76,7.86">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,382.69,337.98,7.86;15,151.52,393.65,329.07,7.86;15,151.52,404.61,132.60,7.86" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="15,151.52,393.65,329.07,7.86;15,151.52,404.61,58.23,7.86">A generic retrieval system for biomedical literatures: Ustb at bioasq2015 question answering task</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,230.74,404.61,24.70,7.86">CLEF</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
