<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.89,115.96,343.58,12.62;1,194.99,133.89,225.37,12.62">Transferability of Natural Language Inference to Biomedical Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.42,171.56,64.35,8.74"><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.23,171.56,57.01,8.74"><forename type="first">Mujeen</forename><surname>Sung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,145.03,183.51,62.61,8.74"><forename type="first">Gangwoo</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.04,183.51,18.82,8.74"><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">AIR Lab</orgName>
								<orgName type="institution">Hyundai Motor Company</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,232.70,207.42,58.25,8.74"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
							<email>kangj@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.89,115.96,343.58,12.62;1,194.99,133.89,225.37,12.62">Transferability of Natural Language Inference to Biomedical Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5BFDA7E65569FFAC709D6C806A2F9924</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Transfer Learning</term>
					<term>Domain Adaptation</term>
					<term>Natural Language Inference</term>
					<term>Biomedical Question Answering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Biomedical question answering (QA) is a challenging task due to the scarcity of data and the requirement of domain expertise. Pre-trained language models have been used to address these issues. Recently, learning relationships between sentence pairs has been proved to improve performance in general QA. In this paper, we focus on applying BioBERT to transfer the knowledge of natural language inference (NLI) to biomedical QA. We observe that BioBERT trained on the NLI dataset obtains better performance on Yes/No (+5.59%), Factoid (+0.53%), List type (+13.58%) questions compared to performance obtained in a previous challenge (BioASQ 7B Phase B). We present a sequential transfer learning method that significantly performed well in the 8 th BioASQ Challenge (Phase B). In sequential transfer learning, the order in which tasks are fine-tuned is important. We measure an unanswerable rate of the extractive QA setting when the formats of factoid and list type questions are converted to the format of the Stanford Question Answering Dataset (SQuAD).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Biomedical question answering (QA) is a challenging task due to the limited amount of data and the requirement of domain expertise. To address these issues, pre-trained language models <ref type="bibr" coords="1,261.06,580.77,15.50,8.74" target="#b12">[13,</ref><ref type="bibr" coords="1,277.77,580.77,12.73,8.74" target="#b25">26]</ref> are used and further fine-tuned on a target task <ref type="bibr" coords="1,155.93,592.72,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="1,167.55,592.72,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="1,176.41,592.72,7.75,8.74" target="#b6">7,</ref><ref type="bibr" coords="1,185.27,592.72,12.73,8.74" target="#b18">19,</ref><ref type="bibr" coords="1,199.11,592.72,12.73,8.74" target="#b19">20,</ref><ref type="bibr" coords="1,212.95,592.72,12.73,8.74" target="#b30">31,</ref><ref type="bibr" coords="1,226.80,592.72,12.73,8.74" target="#b31">32,</ref><ref type="bibr" coords="1,240.64,592.72,11.62,8.74">36]</ref>. Although the pre-trained language models improve performance on the target tasks, the models are still short of the upper-bound performance in biomedical QA. Sequential transfer learning is based on transfer learning and it is used to further improve biomedical QA performance <ref type="bibr" coords="2,440.33,130.95,12.08,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,452.41,130.95,12.08,8.74" target="#b19">20,</ref><ref type="bibr" coords="2,464.48,130.95,12.08,8.74">36]</ref>. For example, fine-tuning on both the SQuAD dataset <ref type="bibr" coords="2,384.71,142.90,15.50,8.74" target="#b27">[28]</ref> and the BioASQ dataset <ref type="bibr" coords="2,169.62,154.86,15.50,8.74" target="#b32">[33]</ref> results in higher performance than fine-tuning on only the BioASQ dataset. In the general QA domain, learning relationships between sentence pairs first is effective in sequential transfer learning <ref type="bibr" coords="2,347.43,178.77,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="2,359.76,178.77,12.73,8.74" target="#b10">11,</ref><ref type="bibr" coords="2,374.31,178.77,12.73,8.74" target="#b26">27,</ref><ref type="bibr" coords="2,388.87,178.77,12.73,8.74" target="#b33">34,</ref><ref type="bibr" coords="2,403.41,178.77,11.62,8.74">35]</ref>. Thus, in this paper, we fine-tune the task of NLI <ref type="bibr" coords="2,289.60,190.72,12.11,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,301.71,190.72,12.11,8.74" target="#b9">10]</ref> to improve performance in biomedical QA. We find that performance improves when the objective function of the finetuned task becomes similar to the function of the downstream task. We also find that applying the NLI task to the biomedical QA task addresses task discrepancy. Task discrepancy refers to the several differences in the distribution of context length, objective function, and domain shift between various fine-tuned tasks.</p><p>Specifically, we focus on reducing the discrepancy of context length distribution between NLI and biomedical QA to improve sequential transfer learning performance on the target task. To reduce the discrepancy, we only unify the distributions of context length of the fine-tuned tasks. We reduce the SQuAD context to a single sentence containing the ground truth answer spans <ref type="bibr" coords="2,437.05,310.67,14.61,8.74" target="#b22">[23]</ref>. Finetuning on a unified distribution reduces the time to train and perform inference on the BioASQ dataset by 52.95% and 25%, respectively. Finally, we measure an unanswerable rate of the extractive QA setting when the format of the BioASQ dataset is converted to the format of the SQuAD dataset.</p><p>Our contributions are as follows:</p><p>(i) We show that fine-tuning on an NLI dataset is effective in Yes/No, Factoid, and List type questions in BioASQ dataset. (ii) We demonstrate that unifying the distributions of context length between fine-tuned tasks improves the sequential transfer learning performance of biomedical QA. (iii) In the Factoid and List type questions, we measure an unanswerable rate of the extractive QA setting, when the format of the BioASQ dataset is converted to that of the SQuAD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Transfer Learning Transfer learning, also known as domain adaptation, refers to the situation of knowledge learned in a previous task to a subsequent task. In various fields including image processing or natural language processing (NLP), many studies have shown the effectiveness of transfer learning based on deep neural networks <ref type="bibr" coords="2,207.10,584.39,15.50,8.74" target="#b14">[15,</ref><ref type="bibr" coords="2,223.89,584.39,12.73,8.74" target="#b21">22,</ref><ref type="bibr" coords="2,237.91,584.39,12.73,8.74" target="#b23">24,</ref><ref type="bibr" coords="2,251.94,584.39,12.73,8.74" target="#b30">31,</ref><ref type="bibr" coords="2,265.96,584.39,11.62,8.74">37]</ref>. More recently, especially in NLP, pre-trained language models such as ELMo <ref type="bibr" coords="2,274.84,596.34,15.50,8.74" target="#b25">[26]</ref> and BERT <ref type="bibr" coords="2,343.81,596.34,15.50,8.74" target="#b12">[13]</ref> have been used for transfer learning <ref type="bibr" coords="2,174.13,608.30,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="2,186.63,608.30,12.73,8.74" target="#b10">11,</ref><ref type="bibr" coords="2,201.34,608.30,12.73,8.74" target="#b12">13,</ref><ref type="bibr" coords="2,216.05,608.30,12.73,8.74" target="#b17">18,</ref><ref type="bibr" coords="2,230.76,608.30,12.73,8.74" target="#b18">19,</ref><ref type="bibr" coords="2,245.48,608.30,12.73,8.74" target="#b20">21,</ref><ref type="bibr" coords="2,260.19,608.30,11.62,8.74" target="#b24">25]</ref>. In the biomedical domain, unsupervised pretraining has been used for biomedical contextualized representations <ref type="bibr" coords="2,438.89,620.25,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="2,450.88,620.25,12.73,8.74" target="#b15">16,</ref><ref type="bibr" coords="2,465.10,620.25,11.62,8.74" target="#b19">20]</ref>.</p><p>BioBERT <ref type="bibr" coords="2,179.80,632.21,15.50,8.74" target="#b19">[20]</ref> was fine-tuned on biomedical corpora (e.g., PubMed and PubMed Central) using BERT, and BioBERT can be employed for various tasks in the biomedical or clinical domain <ref type="bibr" coords="2,265.97,656.12,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="2,277.78,656.12,7.75,8.74" target="#b8">9,</ref><ref type="bibr" coords="2,286.82,656.12,12.73,8.74" target="#b15">16,</ref><ref type="bibr" coords="2,300.85,656.12,12.73,8.74" target="#b16">17,</ref><ref type="bibr" coords="2,314.87,656.12,12.73,8.74" target="#b24">25,</ref><ref type="bibr" coords="2,328.90,656.12,11.62,8.74">36]</ref>.</p><p>Transferability of Natural Language Understanding The authors of <ref type="bibr" coords="3,470.08,118.99,10.52,8.74" target="#b1">[2]</ref> transferred the knowledge obtained from the SQuAD dataset to the target BioASQ dataset to address the data scarcity issue. In <ref type="bibr" coords="3,389.88,142.90,15.50,8.74" target="#b19">[20,</ref><ref type="bibr" coords="3,407.88,142.90,11.62,8.74">36]</ref>, the authors adopted sequential transfer learning (e.g., BioBERT-SQuAD-BioASQ) to improve biomedical QA performance. Meanwhile, multiple NLI datasets have been constructed for the general domain <ref type="bibr" coords="3,290.68,178.77,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="3,302.49,178.77,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="3,311.54,178.77,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="3,325.55,178.77,12.73,8.74" target="#b27">28,</ref><ref type="bibr" coords="3,339.58,178.77,8.49,8.74">35</ref>] and domain-specific datasets (e.g., biomedical) have recently been introduced <ref type="bibr" coords="3,345.19,190.72,15.98,8.74" target="#b24">[25,</ref><ref type="bibr" coords="3,361.17,190.72,11.99,8.74" target="#b29">30]</ref>. In <ref type="bibr" coords="3,392.20,190.72,9.96,8.74" target="#b3">[4]</ref>, the authors have found that fine-tuning on the MultiNLI (MNLI) dataset <ref type="bibr" coords="3,375.63,202.68,10.52,8.74" target="#b0">[1]</ref> consistently improves performance on target tasks in terms of all the GLUE benchmarks <ref type="bibr" coords="3,425.32,214.64,14.61,8.74">[35]</ref>. The authors of <ref type="bibr" coords="3,172.82,226.59,15.50,8.74" target="#b11">[12]</ref> have found that applying knowledge from the NLI dataset improves performance on various yes and no type QA tasks in the general domain. Furthermore, the authors of <ref type="bibr" coords="3,227.09,250.50,15.50,8.74" target="#b33">[34]</ref> have used various size datasets in question answering, text classification/regression, and sequence labeling tasks. In this paper, we use the MNLI dataset for improving performance in biomedical QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we outline our problem setting for the downstream task. Our training details are provided in the Appendix A. We explain our method of learning biomedical entity representations using BioBERT. Then we describe how to perform the sequential transfer learning of BioBERT for each biomedical question type of the BioASQ Challenge. Our method can be used to apply BioBERT, which was used for training NLI dataset, to biomedical QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setting</head><p>We converted the format of the BioASQ dataset to the format of the SQuAD dataset. In detail, training instances in the BioASQ dataset are composed of a question (Q), human-annotated answers (A), and relevant contexts (C) (also called snippets). If the span of answers was not provided by human annotators, we first found exact spans in contexts based on human-annotated answers to factoid and list type questions. In this case, we enumerated all the combinations of Q-C-A triplets only when the answer span exactly matches the context. Yes and No answers to Yes/No type questions are not suggested in contexts; thus, we fine-tuned a task-specific binary classifier to predict answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Architecture</head><p>Input sequence X consists of the concatenation of the BERT [CLS] token, Q, and C, with <ref type="bibr" coords="3,191.58,596.34,24.63,8.74">[SEP]</ref> tokens in between Q and C. The sequence is denoted as</p><formula xml:id="formula_0" coords="3,134.77,596.31,345.83,20.72">X = {[CLS] Q [SEP] C [SEP]</formula><p>} where refers to the concatenation of tensors. The hidden representation vector of the i th input token is denoted as h i ∈ R H where H denotes the hidden size. Finally, we fine-tuned the hidden vectors corresponding to each question type, and the vectors were fed into a softmax classifier or binary classifier.</p><p>Yes/No Type For computing the yes probability P yes , we projected a linear transformation matrix M ∈ R 1×H to transform the hidden representation of a [CLS] token C ∈ R H . In binary classification, the sigmoid function can be used to calculate the yes probability as follows:</p><formula xml:id="formula_1" coords="4,264.12,174.06,216.47,22.71">P yes = 1 1 + e -C•M<label>(1)</label></formula><p>The binary cross entropy loss is utilized between the yes probability P yes and its corresponding ground truth answer a yes . Our total loss is computed as below.</p><formula xml:id="formula_2" coords="4,201.38,252.36,279.21,11.72">Loss = -(a yes logP yes + (1 -a yes )log(1 -P yes ))<label>(2)</label></formula><p>Factoid &amp; List type At hidden representation vectors, the start and end vectors of answer spans were computed in one linear transformation matrix M ∈ R 2×H . Let us denote the i th and j th predicted answer tokens as start and end, respectively. The probability of (P start i , P end j ) can be calculated as follows:</p><formula xml:id="formula_3" coords="4,146.01,343.39,334.58,26.69">P i = P start i P end i = e hi•M s t=1 e ht•M , P j = P start j P end j = e hj •M s t=1 e ht•M<label>(3)</label></formula><p>where s denotes the sequence length of BioBERT and • is the dot-product. Our objective function is the negative log-likelihood for the predicted answer with the ground truth answer position. Start and end position losses are computed as below:</p><formula xml:id="formula_4" coords="4,172.34,434.19,308.25,30.20">Loss start = - 1 N N n=1 logP start,n as , Loss end = - 1 N N n=1 logP end,n ae<label>(4)</label></formula><p>where N denotes the batch size, and a s and a e are the ground truth answers of the start and end positions of each instance, respectively. Our total loss is the arithmetic mean of Loss start and Loss end .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transferability in domains and tasks</head><p>Yes/No Type Training a model to classify relationships of sentence pairs can enhance its performance on yes or no type questions in the general domain <ref type="bibr" coords="4,462.33,560.48,14.61,8.74" target="#b11">[12]</ref>.</p><p>Based on this finding, we believe that a classifier could be used for yes and no type questions in biomedical QA. Thus, we fine-tuned BioBERT on the NLI task so that it can be used to answer biomedical yes or no type questions. We used the MNLI dataset because it is widely used and has a sufficient amount of data from various genres. Furthermore, as shown in Table <ref type="table" coords="4,392.84,620.25,4.98,8.74">9</ref> and 10, sequential transfer learning models trained on the MNLI dataset obtained meaningful results. For our learning sequence, we fine-tuned BioBERT on the MNLI dataset which contains the relationships between hypothesis and premise sentences. We composed a sequential transfer learning method, denoted as BioBERT-MNLI-BioASQ. However, using the final layer of the MNLI task instead of the binary classifier to compute P yes does not improve the performance of BioBERT on the BioASQ dataset. For this reason, we added a simple binary classifier on the top layer of BioBERT. Furthermore, the distributions of context length in the MNLI dataset and the distributions of snippets of Yes/No type questions in the BioASQ dataset are similar. Therefore, we did not unify the context length distributions of yes and no type questions.</p><p>Factoid &amp; List Type The order of sequential transfer learning is important in bridging the gap between different tasks. Performance improves when the objective function of the fine-tuned task becomes similar to that of the downstream task in Table <ref type="table" coords="5,229.13,265.54,3.87,8.74">5</ref>. Thus, we used the learning sequence BioBERT-MNLI-SQuAD-BioASQ instead of BioBERT-SQuAD-MNLI-BioASQ. To address the discrepancy of context length distribution between the SQuAD dataset and the BioASQ dataset, we slightly modified the original experimental setting. As suggested in <ref type="bibr" coords="5,178.76,313.36,14.61,8.74" target="#b22">[23]</ref>, we reorganized the context length distributions in the SQuAD dataset which is similar to the MNLI dataset and BioASQ dataset. We developed an extractive QA setting that is scalable to minimal context and that does not use irrelevant sentences in full abstracts <ref type="bibr" coords="5,327.92,349.23,14.61,8.74">[36]</ref>. Therefore, we extracted a sentence containing the ground truth answer span and set as a complete paragraph to construct the minimal context. As a result, we reduced the discrepancy of context length distribution by unifying the context length distributions for our sequential transfer learning. Unifying the distributions of context length reduced the time to train and perform inference on factoid and list type questions. Our method achieved comparable results to those of the baseline method.</p><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Our datasets are based on the pre-processed datasets provided by <ref type="bibr" coords="5,422.18,513.50,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="5,433.74,513.50,12.73,8.74" target="#b27">28,</ref><ref type="bibr" coords="5,447.50,513.50,11.62,8.74">36]</ref>. For the extractive QA setting, we converted the BioASQ dataset format (Yes/No, Factoid, and List type questions) to the format of the SQuAD dataset. In [36], the authors suggested three pre-processing strategies, and for our study, we utilized two of the three strategies: Snippet-as-is and Full-Abstract. However, we added the criterion of having a blank space before and after each biomedical entity. This criterion has shown to improve performance in distinguishing biomedical named entities. The statistics of the pre-processed dataset are listed in Table <ref type="table" coords="5,472.85,597.19,3.87,8.74" target="#tab_8">8</ref>.</p><p>We have made the pre-processed BioASQ datasets publicly available. <ref type="foot" coords="5,444.52,607.57,3.97,6.12" target="#foot_0">3</ref> In the experimental setting, we removed approximately 5K training instances from the SQuAD dataset because their answer spans do not exactly match the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference System</head><p>Yes/No (Macro F1) Factoid (MRR) List (F1)</p><p>Dimitriadis &amp; Tsoumakas <ref type="bibr" coords="6,248.55,136.93,14.33,7.86" target="#b13">[14]</ref> 0.5541 --Hosein et al., <ref type="bibr" coords="6,197.88,147.89,9.73,7.86" target="#b6">[7]</ref> -0.4562 -Oita et al., <ref type="bibr" coords="6,188.87,158.85,9.73,7.86" target="#b4">[5]</ref> 0.4831 --Resta et al., <ref type="bibr" coords="6,193.66,169.81,14.34,7.86" target="#b28">[29]</ref> 0.7873 --Telukuntla et al., <ref type="bibr" coords="6,214.21,180.76,9.73,7.86" target="#b5">[6]</ref> 0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>In First, the Yes/No type question scores obtained by our method are shown in Table <ref type="table" coords="6,175.23,584.39,3.87,8.74" target="#tab_2">2</ref>. We observed that using the SQuAD dataset for intermediate finetuning improves performance <ref type="bibr" coords="6,267.43,596.34,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="6,279.68,596.34,12.73,8.74" target="#b19">20,</ref><ref type="bibr" coords="6,294.14,596.34,11.62,8.74">36]</ref>. Therefore, we evaluated our proposed method of fine-tuning BioBERT using the sequence BioBERT-SQuAD-BioASQ, as done in <ref type="bibr" coords="6,181.94,620.25,15.50,8.74" target="#b19">[20,</ref><ref type="bibr" coords="6,198.60,620.25,11.62,8.74">36]</ref>. BioBERT is trained on the SQuAD dataset for the QA task. Fine-tuning BioBERT with the sequence BioBERT-MNLI-BioASQ significantly improves its performance. BioBERT obtains higher macro F1 scores (+5.55%, +2.65%) than the baseline. We believe selecting yes and no type questions in the BioASQ dataset is similar to deciding the relationship between sentence pairs in the MNLI dataset. We also replaced the binary classifier of BioBERT, which is trained on the BioASQ dataset, with the final layer of the MNLI task, but this did not improve performance. Thus, we fine-tuned the binary classifier to select yes and no type questions.</p><p>When using the factoid and list type questions in the MNLI dataset, we considered the discrepancy of context length distributions. The obtained results are shown in Table <ref type="table" coords="7,224.86,521.67,3.87,8.74" target="#tab_3">3</ref>. In the original experimental setting, full documents in the SQuAD dataset and snippets in the BioASQ dataset were used for training BioBERT. The performance of our method on the 6B test set did not improve. However, we observed that its performance improves with the size of the training set, as shown by the higher performance on the 7B test set compared with that on the 6B test set.</p><p>In the document setting, we used the whole paragraphs and the full abstracts of the SQuAD and BioASQ datasets, respectively. Performance obtained in this setting is lower than that obtained in the original setting due to using longer context rather than snippets in the BioASQ dataset. In other words, rather than using the human annotated corpus (i.e., snippets), the search space in which an answer can be found was expanded to full abstracts. Nevertheless, the performance of our proposed method on the factoid type questions in the 7B test set improved when BioBERT was fine-tuned on the SQuAD dataset.</p><p>For the snippet setting, we unify the distributions of context length in the extractive QA setting. Our method extracts the sentence containing the ground truth answer span, i.e., the minimal context; the performance of our method on the 6B &amp; 7B test sets significantly improved. We recognize that it is hard to prove the generalization of our method because the test sets for the BioASQ dataset are too small and the variance of performance is relatively high. However, we demonstrate our superior performance by reducing the task discrepancy of factoid type questions in 6B &amp; and 7B. Although, we have achieved better performance of list type questions, reducing the discrepancy of context length distribution does not significantly affect. We believe that given the objective function of list type questions, it needs further analyses to demonstrate the generalization of sequential transfer learning with fine-tuning NLI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>Order of Sequential Transfer Learning The BioASQ Challenge Task 8B (Phase B) results are shown in Table <ref type="table" coords="8,304.10,578.58,3.87,8.74" target="#tab_4">4</ref>. Each team was allowed to submit up to five systems with different combinations of features. The 8B ground truth answers are not available so we could not use them for manually evaluating our proposed method. Thus, we report the scores from the leaderboard. <ref type="foot" coords="8,429.91,612.87,3.97,6.12" target="#foot_1">4</ref>In this ablation study, we explore the importance of the order of sequential transfer learning. The results are shown in Table <ref type="table" coords="8,354.09,638.35,3.87,8.74">5</ref>. We found that fine-tuning  <ref type="table" coords="9,164.95,257.87,4.13,7.89">5</ref>. Experiments on the importance of the order of sequential transfer learning. The metrics used for measuring performance on factoid-type questions are strict accuracy (SAcc), lenient accuracy (LAcc), and mean reciprocal rank (MRR). The metrics used for evaluating performance on list-type questions are precision (Prec), recall (Recall), and macro F1 (F1). The best score obtained in each task is in bold.</p><p>BioBERT on the MNLI dataset improved its performance on factoid type questions. On the other hand, its performance on list type questions improved when the objective function of fine-tuned tasks was similar to that of the BioASQ task. In other words, BioBERT needs to be fine-tuned on the SQuAD dataset after fine-tuning it on the MNLI dataset. Unanswerable rate of the Extractive QA Setting So far, the experiments were performed in the extractive QA setting. We manually analyzed differences between the answer span and the context of the human annotated corpus from the BioASQ Challenge Task 7B (Phase B) test set. We used the test set instead of the training set for measuring the unanswerable rate of the extractive QA setting for the following two reasons. First, we wanted to measure the upperbound performance of our proposed method. Second, the training and test data of the BioASQ dataset are similar to those of the dataset from the previous year.</p><p>Table <ref type="table" coords="9,161.74,656.12,4.98,8.74" target="#tab_6">6</ref> shows the unanswerable rate of all batch results of the 7B test set which contains only factoid and list type questions. We calculated the unanswerable rate of the extractive QA setting using the rule Ground Truth Answer does not exactly match the context of the Human Annotated Corpus (Snippet). The rule applies to the following cases: no exact match, lowercase match, additional phrase added, and different type of blank space between the exact answer and snippet. In Table <ref type="table" coords="10,213.88,178.77,3.87,8.74">7</ref>, we randomly sample such cases. Due to the lack of space, we provide more examples of cases at our url 5 . In here, we use the extractive QA setting to measure the upper-bound performance of our method. We hope our analysis is helpful in designing experimental settings. Here, we demonstrate that these differences in attention can be explained, to a large extent, exclusively from a small set of identifiable chemical, physical, and biological properties of genes. Together with knowledge about homologous genes from model organisms, these features allow us to accurately predict the number of publications on individual human genes, the year of their first report, the levels of funding awarded by the National Institutes of Health (NIH), and the development of drugs against disease-associated genes.</p><p>Table <ref type="table" coords="10,163.83,433.49,4.13,7.89">7</ref>. Unanswerable questions of the extractive QA samples used for the BioASQ dataset. We used factoid-and list-type questions from the 7B test set. Context refers to a snippet in the human annotated corpus provided by the organizer of the BioASQ Challenge. No exact matches are in bold and exact matches in lowercase are underlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we used natural language inference (NLI) as a first step in finetuning BioBERT for biomedical question answering (QA). Training BioBERT to classify relationships between sentence pairs improved its performance in biomedical QA. We empirically demonstrated that fine-tuning BioBERT on the NLI dataset improved its performance on the BioASQ dataset from the BioASQ Challenge. We unified the distributions of context length to mitigate the discrepancy between NLI and biomedical QA. Furthermore, the order of sequential transfer learning is important when fine-tuning BioBERT. Finally, when converting the format of the BioASQ dataset to the SQuAD format, we measured the unanswerable rate of the extractive QA setting where an answer does not exactly match the human annotated corpus.</p><p>35. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.: Glue: A multitask benchmark and analysis platform for natural language understanding. In: Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (2018) 36. Yoon, W., Lee, J., Kim, D., Jeong, M., Kang, J.: Pre-trained language model for biomedical question answering. arXiv preprint arXiv:1909.08229 (2019) 37. Yosinski, J., Clune, J., Bengio, Y., <ref type="bibr" coords="13,298.71,185.43,25.17,7.86">Lipson</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Details</head><p>We use BioBERT as learning biomedical entity representation. We utilize a single NVIDIA Titan RTX (24GB) GPU to fine-tune the sequence of transfer learning.</p><p>In MNLI task, we use hyperparameters suggested by Hugging Face. <ref type="foot" coords="13,434.97,562.98,3.97,6.12" target="#foot_2">6</ref> For finetuning, we select the batch size as 12, 24 and a learning rate is within range 1e-6 to 9e-6. In post-processing, we use the abbreviation resolution module called Ab3P 7 to remove the same answer appearance with a different form.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.77,180.76,345.83,81.85"><head>Table 1 .</head><label>1</label><figDesc>BioASQ 7B (Phase B) Challenge results and our results. We use a dash (-) if the paper does not contain results on each question type. All the scores were averaged when the batch results are reported in each paper. In each column, the best score is in bold.</figDesc><table coords="6,299.86,180.76,160.66,7.86"><row><cell>.4486</cell><cell>0.4751</cell><cell>0.2002</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,305.74,345.83,187.82"><head>Table 1 ,</head><label>1</label><figDesc>we compare our results with the best results from last year's BioASQ Challenge Task 7B (Phase B)<ref type="bibr" coords="6,267.59,317.70,8.54,8.74" target="#b4">[5]</ref><ref type="bibr" coords="6,276.14,317.70,4.27,8.74" target="#b5">[6]</ref><ref type="bibr" coords="6,280.41,317.70,8.54,8.74" target="#b6">[7]</ref><ref type="bibr" coords="6,288.95,317.70,12.82,8.74" target="#b13">14,</ref><ref type="bibr" coords="6,301.77,317.70,12.82,8.74" target="#b28">29,</ref>36]. From this comparison, we observe that training BioBERT on the MNLI dataset significantly improves its performance on the Yes/No (+5.59%), Factoid (+0.53%), and List (+13.58%) type questions.</figDesc><table coords="6,139.15,396.36,337.06,97.21"><row><cell></cell><cell cols="2">Yes/No Type</cell></row><row><cell cols="2"># of Tasks Sequence of Transfer Learning</cell><cell>Evaluation Metric</cell></row><row><cell></cell><cell></cell><cell>Accuracy Yes F1 No F1 Macro F1</cell></row><row><cell>6B Test</cell><cell>BioBERT-SQuAD-BioASQ BioBERT-MNLI-BioASQ</cell><cell>0.8518 0.8857 0.9212 0.7798 0.8505 0.9004 0.6896 0.7950</cell></row><row><cell>7B Test</cell><cell>BioBERT-SQuAD-BioASQ BioBERT-MNLI-BioASQ</cell><cell>0.8595 0.8945 0.9275 0.7588 0.8432 0.8990 0.7344 0.8167</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,509.03,345.83,40.77"><head>Table 2 .</head><label>2</label><figDesc>Yes/No type question experiments. Evaluation metrics are accuracy (Accuracy), F1 score, and macro F1 score (Macro F1). The F1-score of yes type questions is denoted as Yes F1, and the F1 score of the no type questions is denoted as No F1. In the columns, the best score obtained in each task is in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,134.77,125.08,345.83,289.87"><head>Table 3 .</head><label>3</label><figDesc>Context Length Discrepancy Experiments. The metrics used to measure performance on factoid type questions are strict accuracy (SAcc), lenient accuracy (LAcc), and mean reciprocal rank (MRR). The metrics used to evaluate performance on list type questions are precision (Prec), recall (Recall), and macro F1 (F1). 'Original' indicates training BioBERT on full documents in SQuAD and snippets in BioASQ. 'Document' indicates that BioBERT was trained on full documents in SQuAD and full abstracts in BioASQ. 'Snippet' denotes training on a unified distribution of minimal context. All five batch results are averaged. In the columns, the best score obtained in each task is in bold.</figDesc><table coords="7,135.98,125.08,343.40,179.30"><row><cell></cell><cell></cell><cell cols="2">Context Length Discrepancy</cell></row><row><cell cols="2"># of Tasks Setting</cell><cell>Sequence of Transfer Learning</cell><cell>Factoid (%)</cell><cell>List (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SAcc LAcc MRR Prec Recall F1</cell></row><row><cell></cell><cell>Original</cell><cell cols="3">BioBERT-SQuAD-BioASQ BioBERT-MNLI-SQuAD-BioASQ 38.80 61.34 47.42 46.60 47.01 42.44 39.80 57.82 47.22 45.02 47.69 42.34</cell></row><row><cell>6B Test</cell><cell>Document</cell><cell cols="3">BioBERT-SQuAD-BioASQ BioBERT-MNLI-SQuAD-BioASQ 39.71 55.10 45.77 46.26 39.23 38.13 39.71 56.37 45.81 46.81 40.26 39.63</cell></row><row><cell></cell><cell>Snippet</cell><cell cols="3">BioBERT-SQuAD-BioASQ BioBERT-MNLI-SQuAD-BioASQ 41.41 57.40 48.05 46.01 45.95 42.75 38.23 57.34 46.24 48.24 46.86 42.83</cell></row><row><cell></cell><cell>Original</cell><cell cols="3">BioBERT-SQuAD-BioASQ BioBERT-MNLI-SQuAD-BioASQ 42.22 61.06 49.85 61.46 54.62 54.19 41.95 58.30 48.66 61.32 52.83 52.36</cell></row><row><cell>7B Test</cell><cell>Document</cell><cell cols="3">BioBERT-SQuAD-BioASQ BioBERT-MNLI-SQuAD-BioASQ 43.34 58.13 49.21 61.01 41.82 45.78 44.46 57.98 50.02 58.30 39.19 43.89</cell></row><row><cell></cell><cell>Snippet</cell><cell cols="3">BioBERT-SQuAD-BioASQ BioBERT-MNLI-SQuAD-BioASQ 45.10 62.45 51.63 60.92 53.12 53.01 40.79 58.93 48.27 60.08 53.96 53.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,134.77,119.52,345.83,219.11"><head>Table 4 .</head><label>4</label><figDesc>BioASQ 8B results obtained by the top three systems. The best scores were obtained from the BioASQ leaderboard (http://participantsarea.bioasq.org/results/8b/phaseB/). We considered a system with different names as one system with the highest scores. We report the macro average scores obtained on all types of questions in the BioASQ dataset. Our systems are in bold.</figDesc><table coords="8,135.77,119.52,343.82,152.99"><row><cell># of Batches</cell><cell>Yes/No</cell><cell></cell><cell>Factoid</cell><cell></cell><cell>List</cell><cell>Macro Avg.</cell></row><row><cell></cell><cell>System Name</cell><cell>Macro F1</cell><cell>System Name</cell><cell>MRR</cell><cell>System Name</cell><cell>F1</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">0.8663 Ours</cell><cell cols="2">0.4438 Ours</cell><cell>0.3718 0.5606</cell></row><row><cell>8B batch 1</cell><cell>FudanLabZhu1</cell><cell cols="2">0.4518 FudanLabZhu1</cell><cell cols="2">0.4557 FudanLabZhu1</cell><cell>0.3408</cell><cell>0.4161</cell></row><row><cell></cell><cell>Umass czi 4</cell><cell cols="2">0.5989 Umass czi 4</cell><cell cols="2">0.3005 Umass czi 4</cell><cell>0.3448</cell><cell>0.4147</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">0.8928 Ours</cell><cell cols="2">0.3533 Ours</cell><cell>0.3798 0.5420</cell></row><row><cell>8B batch 2</cell><cell cols="6">UoT multitask learn 0.7000 UoT multitask learn 0.2800 UoT multitask learn 0.4108</cell><cell>0.4636</cell></row><row><cell></cell><cell>FudanLabZhu4</cell><cell cols="2">0.6303 FudanLabZhu4</cell><cell cols="2">0.2900 FudanLabZhu4</cell><cell>0.4678</cell><cell>0.4627</cell></row><row><cell></cell><cell>Umass czi 4</cell><cell cols="2">0.9016 Umass czi 4</cell><cell cols="2">0.3810 Umass czi 4</cell><cell>0.4522</cell><cell>0.5782</cell></row><row><cell>8B batch 3</cell><cell>Ours</cell><cell cols="2">0.9028 Ours</cell><cell cols="2">0.3601 Ours</cell><cell>0.4520 0.5716</cell></row><row><cell></cell><cell>pa-base</cell><cell cols="2">0.8995 pa-base</cell><cell cols="2">0.3137 pa-base</cell><cell>0.4585</cell><cell>0.5572</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">0.7636 Ours</cell><cell cols="2">0.6078 Ours</cell><cell>0.4037 0.5917</cell></row><row><cell>8B batch 4</cell><cell>91-initial-Bio</cell><cell cols="2">0.7204 91-initial-Bio</cell><cell cols="2">0.5735 91-initial-Bio</cell><cell>0.3905</cell><cell>0.5615</cell></row><row><cell></cell><cell>Features Fusion</cell><cell cols="2">0.7097 Features Fusion</cell><cell cols="2">0.5745 Features Fusion</cell><cell>0.3625</cell><cell>0.5489</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">0.8518 Ours</cell><cell cols="2">0.5677 Ours</cell><cell>0.5582 0.6592</cell></row><row><cell>8B batch 5</cell><cell cols="6">Parameters retrained 0.7509 Parameters retrained 0.5938 Parameters retrained 0.4004</cell><cell>0.5817</cell></row><row><cell></cell><cell>Features Fusion</cell><cell cols="2">0.7509 Features Fusion</cell><cell cols="2">0.6115 Features Fusion</cell><cell>0.3810</cell><cell>0.5811</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,134.77,126.06,344.51,139.70"><head></head><label></label><figDesc>BioASQ 43.31 58.69 49.24 60.77 50.74 50.72 BioBERT-MNLI-SQuAD-BioASQ 42.22 61.06 49.85 61.46 54.62 54.19</figDesc><table coords="9,134.77,126.06,344.51,139.70"><row><cell></cell><cell cols="2">Order Importance</cell></row><row><cell cols="2"># of Tasks Sequence of Transfer Learning</cell><cell>Factoid (%)</cell><cell>List (%)</cell></row><row><cell></cell><cell></cell><cell cols="2">SAcc LAcc MRR Prec Recall F1</cell></row><row><cell></cell><cell>BioBERT-SQuAD-BioASQ</cell><cell cols="2">39.80 57.82 47.22 45.02 47.69 42.34</cell></row><row><cell>6B Test</cell><cell cols="3">BioBERT-SQuAD-MNLI-BioASQ 41.15 57.95 47.29 46.18 44.56 40.98</cell></row><row><cell></cell><cell cols="3">BioBERT-MNLI-SQuAD-BioASQ 38.80 61.34 47.42 46.60 47.01 42.44</cell></row><row><cell></cell><cell>BioBERT-SQuAD-BioASQ</cell><cell cols="2">41.95 58.30 48.66 61.32 52.83 52.36</cell></row><row><cell cols="2">7B Test BioBERT-SQuAD-MNLI-Table</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,134.77,445.13,345.83,94.23"><head>Table 6 .</head><label>6</label><figDesc>Statistics of the unanswerable rate in the extractive QA setting. The cases where Ground Truth Answer does not exactly match the context of the Human Annotated Corpus (Snippet). The unanswerable rate is related to the upper-bound performance of our proposed method.</figDesc><table coords="9,136.10,445.13,343.18,38.10"><row><cell cols="2">Type 7B Batch1 7B Batch2 7B Batch3 7B Batch4 7B Batch5</cell><cell>7B Total</cell></row><row><cell cols="3">Factoid 0.359 (14/39) 0.120 (3/25) 0.310 (9/29) 0.118 (4/34) 0.229 (8/35) 0.216 (35/162)</cell></row><row><cell>List</cell><cell cols="2">0.083 (1/12) 0.235 (4/17) 0.200 (5/25) 0.136 (3/22) 0.500 (6/12) 0.204 (18/88)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="13,134.77,185.43,345.83,289.51"><head>Table 8 .</head><label>8</label><figDesc>, H.: How transferable are features in deep neural networks? In: Advances in NIPS (2014) Statistics of transferred dataset (MNLI &amp; SQuAD) and target dataset (BioASQ).</figDesc><table coords="13,175.06,242.50,259.10,202.29"><row><cell></cell><cell>MNLI</cell><cell>Train</cell><cell></cell><cell>Dev</cell></row><row><cell></cell><cell>Original</cell><cell cols="2">392,702</cell><cell>9,815</cell></row><row><cell cols="2">SQuAD v1.1</cell><cell>Train</cell><cell></cell><cell>Dev</cell></row><row><cell></cell><cell>Original</cell><cell>87,412</cell><cell></cell><cell>10,570</cell></row><row><cell></cell><cell>Snippet</cell><cell>82,280</cell><cell></cell><cell>9,986</cell></row><row><cell cols="2">SQuAD v2.0</cell><cell>Train</cell><cell></cell><cell>Dev</cell></row><row><cell></cell><cell>Original</cell><cell cols="2">130,319</cell><cell>11,873</cell></row><row><cell></cell><cell>BioASQ</cell><cell>6B</cell><cell>7B</cell><cell>8B</cell></row><row><cell>Type</cell><cell>Data Strategy</cell><cell cols="3">Train Test Train Test Train Test</cell></row><row><cell cols="2">Yes/No Snippet-as-is</cell><cell cols="3">9,421 127 10,560 140 11,531 152</cell></row><row><cell></cell><cell>Full-Abstract</cell><cell>7,911</cell><cell>9,403</cell><cell>10,147</cell></row><row><cell>Factoid</cell><cell cols="4">Appended-Snippet 5,953 161 7,179 162 7,896 151</cell></row><row><cell></cell><cell>Snippet-as-is</cell><cell>3,512</cell><cell>4,231</cell><cell>4,759</cell></row><row><cell></cell><cell>Full-Abstract</cell><cell>14,008</cell><cell>15,719</cell><cell>16,879</cell></row><row><cell>List</cell><cell cols="4">Appended-Snippet 10,878 81 12,184 88 13,251 75</cell></row><row><cell></cell><cell>Snippet-as-is</cell><cell>6,922</cell><cell>7,865</cell><cell>8,676</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="5,144.73,656.80,156.05,7.86"><p>https://github.com/dmis-lab/bioasq8b</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="8,144.73,656.80,223.69,7.86"><p>http://participants-area.bioasq.org/results/8b/phaseB/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="13,144.73,634.88,301.17,7.86;13,144.73,645.84,50.79,7.86"><p>https://github.com/huggingface/transformers/tree/master/examples/textclassification</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,186.11,337.63,7.86;11,151.52,197.07,329.07,7.86;11,151.52,208.03,225.27,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,230.95,186.11,249.64,7.86;11,151.52,197.07,69.02,7.86">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName coords=""><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,241.18,197.07,239.42,7.86;11,151.52,208.03,90.80,7.86">Proceedings of the 2018 Conference of the NAACL: Human Language Technologies</title>
		<meeting>the 2018 Conference of the NAACL: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="11,142.96,218.96,337.63,7.86;11,151.52,229.92,215.57,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,219.28,218.96,243.18,7.86">Neural domain adaptation for biomedical question answering</title>
		<author>
			<persName coords=""><surname>Wiese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,229.92,186.90,7.86">Proceedings of the 21st Conference on CoNLL</title>
		<meeting>the 21st Conference on CoNLL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,240.84,337.63,7.86;11,151.52,251.80,327.27,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,233.14,240.84,126.99,7.86">The winograd schema challenge</title>
		<author>
			<persName coords=""><surname>Levesque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,381.73,240.84,98.86,7.86;11,151.52,251.80,298.60,7.86">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,262.72,337.64,7.86;11,151.52,273.68,260.53,7.86" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><surname>Phang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01088</idno>
		<title level="m" coord="11,220.33,262.72,260.27,7.86;11,151.52,273.68,94.59,7.86">Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.96,284.61,337.64,7.86;11,151.52,295.57,145.85,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,211.97,284.61,268.63,7.86;11,151.52,295.57,38.08,7.86">Semantically corroborating neural attention for biomedical question answering</title>
		<author>
			<persName coords=""><surname>Oita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,210.98,295.57,57.72,7.86">ECML PKDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,306.49,337.64,7.86;11,151.52,317.45,205.32,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,252.72,306.49,227.88,7.86;11,151.52,317.45,97.61,7.86">Uncc biomedical semantic question answering systems. bioasq: Task-7b, phase-b</title>
		<author>
			<persName coords=""><surname>Telukuntla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,270.45,317.45,57.72,7.86">ECML PKDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,328.37,337.63,7.86;11,151.52,339.33,174.15,7.86" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><surname>Hosein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09704</idno>
		<title level="m" coord="11,218.45,328.37,262.14,7.86;11,151.52,339.33,8.02,7.86">Measuring domain portability and errorpropagation in biomedical qa</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.96,350.26,337.63,7.86;11,151.52,361.22,329.07,7.86;11,151.52,372.18,241.87,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,209.23,361.22,172.76,7.86">Publicly available clinical bert embeddings</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jindi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mc-Dermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,404.91,361.22,75.69,7.86;11,151.52,372.18,213.19,7.86">Proceedings of the 2nd Clinical Natural Language Processing Workshop</title>
		<meeting>the 2nd Clinical Natural Language Processing Workshop</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,383.10,337.64,7.86;11,151.52,394.06,265.44,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,277.20,383.10,203.40,7.86;11,151.52,394.06,14.95,7.86">Scibert: A pretrained language model for scientific text</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,187.36,394.06,229.60,7.86">Proceedings of the 2019 Conference on EMNLP-IJCNLP</title>
		<meeting>the 2019 Conference on EMNLP-IJCNLP</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,404.98,337.98,7.86;11,151.52,415.94,329.07,7.86;11,151.52,426.90,33.66,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,362.79,404.98,117.80,7.86;11,151.52,415.94,143.86,7.86">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,318.67,415.94,161.93,7.86;11,151.52,426.90,33.66,7.86">Proceedings of the 2015 Conference on EMNLP</title>
		<meeting>the 2015 Conference on EMNLP</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,437.83,337.98,7.86;11,151.52,448.78,329.07,7.86;11,151.52,459.74,97.80,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12651</idno>
		<title level="m" coord="11,384.52,437.83,96.07,7.86;11,151.52,448.78,258.84,7.86">Recall and learn: Finetuning deep pretrained language models with less forgetting</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.62,470.67,337.98,7.86;11,151.52,481.63,329.07,7.86;11,151.52,492.59,329.07,7.86;11,151.52,503.54,170.32,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,151.52,481.63,287.65,7.86">Boolq: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,463.04,481.63,17.56,7.86;11,151.52,492.59,324.97,7.86">Proceedings of the 2019 Conference of the NAACL: Human Language Technologies</title>
		<meeting>the 2019 Conference of the NAACL: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="11,142.62,514.47,337.97,7.86;11,151.52,525.43,329.07,7.86;11,151.52,536.39,265.80,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,354.85,514.47,125.74,7.86;11,151.52,525.43,205.07,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,380.67,525.43,99.92,7.86;11,151.52,536.39,237.13,7.86">Proceedings of the 2019 Conference of the NAACL: Human Language Technologies</title>
		<meeting>the 2019 Conference of the NAACL: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,547.31,337.98,7.86;11,151.52,558.27,86.39,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,288.52,547.31,172.63,7.86">Yes/no question answering in bioasq 2019</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,558.27,57.72,7.86">ECML PKDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,569.20,337.98,7.86;11,151.52,580.15,329.07,7.86;11,151.52,591.11,25.60,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,243.16,569.20,233.88,7.86">Universal language model fine-tuning for text classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,165.52,580.15,211.13,7.86">Proceedings of the 56th Annual Meeting of the ACL</title>
		<meeting>the 56th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="11,142.62,602.04,337.97,7.86;11,151.52,613.00,329.07,7.86;11,151.52,623.96,128.78,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,327.29,602.04,153.29,7.86;11,151.52,613.00,64.04,7.86">Probing biomedical embeddings from language models</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,236.02,613.00,244.58,7.86;11,151.52,623.96,100.11,7.86">Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>the 3rd Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,634.88,337.97,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,178.91,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,190.86,645.84,289.73,7.86;11,151.52,656.80,91.17,7.86">A neural named entity recognition and multi-type normalization tool for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,250.08,656.80,51.69,7.86">IEEE Access</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,119.67,337.97,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,329.07,7.86;12,151.52,152.55,301.38,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,318.43,130.63,162.16,7.86;12,151.52,141.59,181.37,7.86">Probing what different nlp tasks teach machines about function word comprehension</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,353.87,141.59,126.72,7.86;12,151.52,152.55,213.45,7.86">Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Eighth Joint Conference on Lexical and Computational Semantics<address><addrLine>SEM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,164.20,337.98,7.86;12,151.52,175.16,329.07,7.86;12,151.52,186.12,97.80,7.86" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="12,441.62,164.20,38.98,7.86;12,151.52,175.16,261.84,7.86">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,197.77,337.98,7.86;12,151.52,208.73,329.07,7.86;12,151.52,219.69,132.29,7.86" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<title level="m" coord="12,389.57,197.77,91.02,7.86;12,151.52,208.73,285.11,7.86">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<meeting><address><addrLine>Oxford, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Bioinformatics</note>
</biblStruct>

<biblStruct coords="12,142.62,231.34,337.98,7.86;12,151.52,242.30,329.07,7.86;12,151.52,253.26,329.07,7.86;12,151.52,264.22,84.70,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,411.70,231.34,68.89,7.86;12,151.52,242.30,212.82,7.86">Linguistic knowledge and transferability of contextual representations</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,385.13,242.30,95.46,7.86;12,151.52,253.26,237.84,7.86">Proceedings of the 2019 Conference of the NAACL: Human Language Technologies</title>
		<meeting>the 2019 Conference of the NAACL: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="12,142.62,275.87,337.98,7.86;12,151.52,286.83,268.43,7.86" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="12,335.98,275.87,144.61,7.86;12,151.52,286.83,102.07,7.86">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,298.48,337.98,7.86;12,151.52,309.44,329.07,7.86;12,151.52,320.40,178.37,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,322.18,298.48,158.41,7.86;12,151.52,309.44,148.60,7.86">Efficient and robust question answering from minimal context over documents</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,320.54,309.44,160.05,7.86;12,151.52,320.40,45.44,7.86">Proceedings of the 56th Annual Meeting of the ACL</title>
		<meeting>the 56th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="12,142.62,332.05,337.98,7.86;12,151.52,343.01,329.07,7.86;12,151.52,353.97,46.45,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,411.75,332.05,68.85,7.86;12,151.52,343.01,164.69,7.86">How transferable are neural networks in nlp applications?</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,334.66,343.01,145.94,7.86;12,151.52,353.97,46.45,7.86">Proceedings of the 2016 Conference on EMNLP</title>
		<meeting>the 2016 Conference on EMNLP</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,365.62,337.97,7.86;12,151.52,376.58,329.07,7.86;12,151.52,387.54,222.72,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,253.20,365.62,227.39,7.86;12,151.52,376.58,261.15,7.86">Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,432.52,376.58,48.08,7.86;12,151.52,387.54,194.05,7.86">Proceedings of the 18th BioNLP Workshop and Shared Task</title>
		<meeting>the 18th BioNLP Workshop and Shared Task</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,399.19,337.98,7.86;12,151.52,410.15,329.07,7.86;12,151.52,421.11,329.07,7.86;12,151.52,432.07,25.60,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="12,166.36,410.15,169.28,7.86">Deep contextualized word representations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,358.53,410.15,122.06,7.86;12,151.52,421.11,221.64,7.86">Proceedings of the 2018 Conference of the NAACL: Human Language Technologies</title>
		<meeting>the 2018 Conference of the NAACL: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="12,142.62,443.72,337.97,7.86;12,151.52,454.68,329.07,7.86;12,151.52,465.64,211.64,7.86" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<title level="m" coord="12,209.74,454.68,270.85,7.86;12,151.52,465.64,45.40,7.86">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,477.29,337.98,7.86;12,151.52,488.25,329.07,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="12,356.42,477.29,124.17,7.86;12,151.52,488.25,121.69,7.86">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,292.51,488.25,188.08,7.86">Proceedings of the 2016 Conference on EMNLP</title>
		<meeting>the 2016 Conference on EMNLP</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,499.91,337.98,7.86;12,151.52,510.86,207.08,7.86" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="12,347.90,499.91,132.69,7.86;12,151.52,510.86,99.34,7.86">Transformer models for question answering at bioasq 2019</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Resta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Arioli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fagnani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Attardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,272.21,510.86,57.72,7.86">ECML PKDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,522.52,337.98,7.86;12,151.52,533.48,242.27,7.86" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="12,261.50,522.52,219.09,7.86;12,151.52,533.48,27.65,7.86">Lessons from natural language inference in the clinical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shivade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,200.92,533.48,192.87,7.86">Proceedings of the 2018 Conference on EMNLP</title>
		<meeting>the 2018 Conference on EMNLP</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,545.13,337.98,7.86;12,151.52,556.09,25.60,7.86" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="12,196.18,545.13,227.11,7.86">Neural transfer learning for natural language processing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="12,142.62,567.74,337.98,7.86;12,151.52,578.70,329.07,7.86;12,151.52,589.66,63.61,7.86" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="12,248.16,567.74,232.43,7.86;12,151.52,578.70,134.85,7.86">Multiqa: An empirical investigation of generalization and transfer in reading comprehension</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,307.68,578.70,172.92,7.86;12,151.52,589.66,34.94,7.86">Proceedings of the 57th Annual Meeting of the ACL</title>
		<meeting>the 57th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,601.31,337.98,7.86;12,151.52,612.27,329.07,7.86;12,151.52,623.23,329.07,7.86;12,151.52,634.19,206.86,7.86" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="12,151.52,623.23,329.07,7.86;12,151.52,634.19,89.05,7.86">An overview of the bioasq large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Alvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Polychronopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,247.85,634.19,81.85,7.86">BMC bioinformatics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,645.84,337.98,7.86;12,151.52,656.80,322.79,7.86" xml:id="b33">
	<monogr>
		<title level="m" type="main" coord="12,247.27,656.80,227.04,7.86">Exploring and predicting transferability across nlp tasks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mattarella-Micke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
