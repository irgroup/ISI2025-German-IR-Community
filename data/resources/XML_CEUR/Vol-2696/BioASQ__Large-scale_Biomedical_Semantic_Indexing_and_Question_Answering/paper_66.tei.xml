<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.66,115.96,302.04,12.62;1,270.58,133.89,74.21,12.62">Transfer Learning for Biomedical Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,231.24,171.96,62.87,8.74"><forename type="first">Arda</forename><surname>Akdemir</surname></persName>
							<email>aakdemir@hgc.jptshibuya@hgc.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.81,171.96,67.30,8.74"><forename type="first">Tetsuo</forename><surname>Shibuya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.66,115.96,302.04,12.62;1,270.58,133.89,74.21,12.62">Transfer Learning for Biomedical Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4968019ED532F8008E5F6646A8E462E9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Neural Network (DNN) based Machine Learning models achieved remarkable success in many fields of research. Yet, many recent studies show the limitations of these approaches to generalize to unseen examples and to new domains such as the biomedical domain. Besides, supervised-learning based DNN models require a substantial amount of labeled data which is not readily available for many tasks such as the biomedical question answering task. Transfer Learning is shown to mitigate these challenges by transferring information from auxiliary tasks to improve the performance on a source task, and shown to be especially useful for low-resource tasks. These observations and findings motivated us to investigate the effect of transfer learning and multi-task learning on the biomedical question answering task. We proposed a novel multi-task learning model to learn biomedical entities and questions simultaneously. In this work, we explain the three different neural models we used to participate for the BioASQ 8B challenge. Our initial results showed that transferring information from the biomedical entity recognition task brings improvement for the biomedical question answering task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained language models <ref type="bibr" coords="1,262.26,506.42,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="1,279.42,506.42,7.75,8.74" target="#b2">3]</ref> have been frequently leveraged to improve performance on various downstream NLP tasks since their introduction. However, it is shown that the performance of these models, which are trained on general domain corpora, drops significantly when they are tested on a new domain <ref type="bibr" coords="1,160.09,554.25,14.61,8.74" target="#b13">[14]</ref>. This performance drop is higher for domains that have significantly different word distributions, such as the biomedical domain. To mitigate this performance drop, a frequently used approach is to pretrain these models on the target domain, which is also called as domain-adaptation. Recently, Lee et al. <ref type="bibr" coords="1,148.51,602.07,10.52,8.74" target="#b8">[9]</ref> pretrained the BERT <ref type="bibr" coords="1,258.21,602.07,10.52,8.74" target="#b2">[3]</ref> language model on the PubMED articles, which is called BioBERT, and achieved state-of-the-art results for several downstream biomedical tasks. This motivated us to use BioBERT as our baseline model in our experiments.</p><p>Transfer learning is a general term to describe the learning schemes where the information from a source task is used to improve the performance on a target task. It is shown to be especially useful to improve the performance on low-resource tasks <ref type="bibr" coords="2,217.22,179.59,9.96,8.74" target="#b1">[2]</ref>. Ideally, we would like to transfer information from highresource tasks that have a similar domain with the source task to make the most out of transfer learning. Currently available datasets for biomedical question answering is very limited. Relative to the biomedical question answering datasets, the currently available biomedical entity datasets are large. These findings motivated us to apply transfer learning to improve the performance on the biomedical question answering task. Specifically, we claim that the performance on biomedical question answering can be improved by transferring information from the biomedical entity recognition task. We propose a multi-task learning model that learns both biomedical question answering and entity recognition tasks, which have not been implemented before to the best of our knowledge. Our work can be considered as an extension of the previously proposed BioBERT model. Our model differs from the BioBERT model in two main ways. Unlike the BioBERT model, we propose a single neural architecture to simultaneously learn three question types (factoid, yes/no, list). This allows the model to transfer information between different question types. Next, we propose a multi-task learning model to learn the biomedical entity recognition and question answering tasks. BioBERT uses separate architectures for the two downstream tasks. Thus, the pretrained BioBERT model is fine-tuned from scratch for each task. Unlike BioBERT, our model allows transferring information between these two tasks during the fine-tuning step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">BioASQ Challenge</head><p>BioASQ is a challenge on biomedical semantic indexing and question answering <ref type="bibr" coords="2,152.27,488.68,9.96,8.74" target="#b5">[6]</ref>. The challenge aims to advance the state-of-the-art in semantic indexing and question answering, and also establish a reference point for biomedical question answering. More information about the challenge can be obtained from the BioASQ homepage. <ref type="foot" coords="2,244.24,522.97,3.97,6.12" target="#foot_0">1</ref> We participated in the question answering part of the BioASQ 2020 challenge (8B) to test our claim on using transfer learning for biomedical question answering. This paper describes the models we used to make our submissions to the BioASQ 8B challenge. We participated to the challenge with three different neural architectures, and used the BioASQ datasets as our test-bed to compare these proposed models. Our main contributions can be listed as follows:</p><p>-We implemented a novel neural architecture that uses a single model to jointly learn three question types in the BioASQ challenge.</p><p>-We proposed a novel multi-task learning model for entity recognition and question answering for the biomedical domain which have not been employed before to the best of our knowledge. -We analyzed the effect of transferring information from three biomedical entity recognition datasets for the biomedical question answering task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section we describe each model we used during the BioASQ Task 8b: Biomedical Semantic Question Answering. During the task, we made submissions using five different models, three of which used an identical neural architecture, but the final model is determined using different evaluation methods. We used BioBert-based Question Answering Model <ref type="bibr" coords="3,321.49,277.28,15.50,8.74" target="#b16">[17]</ref> as our baseline model, which we refer to as BioBERT baseline. The second model is an extension of the first model, which jointly learns all question types using a single architecture. We refer to this model as BioBERT allquestions. We used three variations of this model for our submissions. Finally, we used a novel multi-task learning model that learns biomedical entities and all question types simultaneously. We refer to this model as BioBERT multitask. For the BioASQ 8B challenge, we only submitted answers for the 'list', 'factoid', and 'yes-no' type questions. 'Summary' type questions require a fundamentally different approach, and was beyond the main scope of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pre-processing</head><p>The raw input format of the BioASQ dataset needs to be pre-processed into the suitable format expected by the BioBERT model. Following Yoon et al. <ref type="bibr" coords="3,462.33,452.88,14.61,8.74" target="#b16">[17]</ref>, we used a similar pre-processing scheme to convert the BioASQ questions into the SQUAD Question Answering format. In the BioASQ dataset, multiple goldanswers are provided for most questions. Gold answers are denoted as spans inside the snippets provided for each question. During pre-processing, we treated each gold-label snippet and question pair as separate examples to increase the size of the training set. During all our experiments, we only made use of the gold-label snippets. We did not analyze the effect of appending additional information from external sources such as the links to related documents provided by the BioASQ organizers. Previously, Yoon et al. <ref type="bibr" coords="3,345.42,560.48,15.50,8.74" target="#b16">[17]</ref> experimented with various pre-processing methods to bring further improvements. They observed that the benefits of each strategy depend on the question type and the test-batch. For this reason, we fixed the pre-processing method throughout our all experiments to make it clear where the improvements for each proposed model come from. Besides, using only the snippets as input to the neural networks significantly reduces the input size and reduces the overall training time. For factoid and list type questions, each gold-label span is used to create a new Question-Passage pair. An example factoid type question and gold-label spans from the provided Table <ref type="table" coords="4,162.17,127.36,3.87,8.74">1</ref>: An example question and the gold-label spans from the provided snippets from the BioASQ 6B dataset. The gold-label spans are shown in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>What is Contrave prescribed for? Answer 1 Contrave, ... for the potential treatment of obesity, is an oral, sustained ... Answer 2 Contrave is a combination of ... for the treatment of obesity, and is used ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer 3</head><p>Contrave, a bupropion and ... for the potential treatment of obesity.</p><p>spans are given in Table <ref type="table" coords="4,245.05,223.28,3.87,8.74">1</ref>. The final predictions for the list type questions are handled during the post-processing step, and explained in the relevant section. Contrary to the previous work that directly adapts the BERT Question Answering Model <ref type="bibr" coords="4,201.50,260.10,10.52,8.74" target="#b2">[3]</ref> by modifying the 'is impossible' field of the SQUAD dataset format for the yes/no type questions, we implemented our own Yes/No component. This enabled us to use the data without adding the 'is impossible' field, making the dataset format more readable and easier to understand for researchers from the biomedical domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BioBert-based Baseline Model</head><p>Pretrained subword contextual embeddings has shown remarkable progress over previous approaches on many downstream Natural Language Processing (NLP) tasks <ref type="bibr" coords="4,162.10,391.19,15.50,8.74" target="#b15">[16,</ref><ref type="bibr" coords="4,179.26,391.19,12.73,8.74" target="#b12">13,</ref><ref type="bibr" coords="4,193.65,391.19,11.62,8.74" target="#b11">12]</ref>. Specifically the transformer-based BERT model <ref type="bibr" coords="4,436.21,391.19,10.52,8.74" target="#b2">[3]</ref> helped achieve state-of-the-art results on many downstream tasks, including question answering.</p><p>The performance of models pretrained on general domain corpora (e.g., Wikipedia articles) drops significantly when tested on niche domains such as the biomedical domain. Motivated with this observation, Lee et al. <ref type="bibr" coords="4,427.79,451.93,10.52,8.74" target="#b8">[9]</ref> proposed 'BioBERT', BERT architecture pretrained on PubMed articles. The proposed model obtained state-of-the-art results on three different downstream biomedical NLP tasks. Recently, Yoon et al. <ref type="bibr" coords="4,299.13,487.79,15.50,8.74" target="#b16">[17]</ref> obtained the best results in the 2019 BioASQ 7B Question Answering Challenge, and achieved state-of-the-art results on all question types (factoid, yes/no, list). In their proposed approach, separate models are trained from scratch for yes/no, and factoid type questions (factoid/list).</p><p>For our baseline model, we used this BioBERT-based approach which we refer to as BioBERT baseline. BERT model is extended with two separate additional neural layers to learn different question types. The overall architectures are given in Figure <ref type="figure" coords="4,176.91,584.39,3.87,8.74">1</ref>. For the yes/no type questions, the output for the first token ([CLS]) ) of the final layer of BERT is given as input to a fully connected layer with 2-dim output representing the scores for yes/no scores. This is followed by a softmax layer to convert these scores into probabilities. Given a sequence of n question tokens Q = q t : 1 ≤ t ≤ n, and m passage tokens</p><formula xml:id="formula_0" coords="4,134.77,632.21,345.83,21.61">P = p t : 1 ≤ t ≤ m , BioBERT outputs m + n + 2 fixed-size (L) vectors V = v j : 1 ≤ j ≤ (m + n + 2).</formula><p>Next, v 1 is multiplied with an (L, 2) dimensional matrix W to generate scores, S = {s yes , s no }, for yes and no answers:</p><formula xml:id="formula_1" coords="5,259.53,143.12,96.29,40.13">V = BioBERT (Q, P ) S = v T 1 W O = Sof tmax(S)</formula><p>where O = o yes , o no represents the probabilities for each answer, which is the final output for the yes/no type questions. Similarly for the factoid/list type questions, each v j is multiplied with an (L, 2) dimensional matrix W 2 to generate scores S 2 = {s start , s end }, which represent the score for the start, end spans for each token p j inside the input passage P :</p><formula xml:id="formula_2" coords="5,258.26,271.71,98.84,27.07">V = BioBERT (Q, P ) S 2 = v T j W</formula><p>For training, each BioBERT-initialized model in Figure <ref type="figure" coords="5,390.15,329.57,4.98,8.74">1</ref> is fine-tuned on the BioASQ-8b for each question type, separately. The main drawback of this previously proposed model is that the common BioBERT layer, which constitutes the majority of the parameters (only a single layer is added for each question type), is fine-tuned separately for each question type. The bottleneck for developing high-performing biomedical question answering systems is the scarcity of the labeled training sets. This approach further limits the training dataset size, and not ideal for low-resource domains like the biomedical domain.  An important part of training joint-learning models is the selection of performance metrics. In the conventional single-task machine learning setting, there is usually a single performance metric. The models are evaluated on a development/validation dataset based on this metric, to determine the best performing model during training. In the joint-learning setting, we can evaluate the models based on their performance on each task separately, or we can evaluate them based on their overall performance. For our submissions for the BioASQ 8B challenge we used the following three joint-learning models:</p><p>-Overall best-performer -Best yes/no model -Best factoid model To determine the best-performer in each three cases, we used the average results over five test-batches of the Bio-ASQ 6B challenge <ref type="bibr" coords="7,377.33,130.95,9.96,8.74" target="#b5">[6]</ref>. All three models are obtained from the same training experiment, and correspond to the checkpoints of the same model instance.</p><p>Fig. <ref type="figure" coords="7,153.45,335.48,3.87,8.74">3</ref>: The proposed multi-task learning model which learns all biomedical question types and the biomedical entities simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multi-task Learning Model</head><p>The multi-task learning model further extends the joint-learner explained in Section 2.3. In this setting, a single neural model is trained for Biomedical Question Answering and Gene/Protein Entity Recognition tasks, simultaneously. The details of the Question Answering component of the model is identical with the joint-learner. In addition, the model contains an entity recognition component consisting of a Fully-Connected layer, followed by a Conditional Random Fields (CRF) layer. CRF-based models are frequently used for the named entity recognition task, to take into account the tag transitions between consecutive tokens <ref type="bibr" coords="7,157.28,524.61,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="7,169.46,524.61,7.01,8.74" target="#b0">1]</ref>. For this reason, we extended the NER-component of the previously proposed BioBERT-based NER model in <ref type="bibr" coords="7,314.33,536.57,10.52,8.74" target="#b8">[9]</ref> to include an additional CRF layer. The overall architecture of this proposed multi-task learner is shown in Figure <ref type="figure" coords="7,152.84,560.48,3.87,8.74">3</ref>. For a sequence of n tokens t i : 1 ≤ i ≤ n, the NER-component receives the BioBERT representation for each token. The subword token representations are then averaged to get the word-level representations. These word-level representations are fed into the FC-layer to generate the scores for each entity label, for each token. The CRF-layer generates the final score for each label by taking into account the transitions between each label. For the NER component, crfloss is used. The loss is calculated as the difference between the total score of all possible label-sequences (all possible paths) and the score of the gold-label sequence (gold-label path): where S denotes the scoring matrix containing scores for each label and word pair, G is the gold-label sequence, and T is the transition matrix containing transition scores between each label. f orward score(S, T) denotes the total score of all paths and path score(S, T, G) is the score of the gold label sequence. Ideally, we want all probabilities to accumulate on the gold-label path so that these two scores will be identical.</p><p>Inference During the inference mode, we used Viterbi decoding <ref type="bibr" coords="8,420.96,308.72,10.52,8.74" target="#b3">[4]</ref> to find the highest scoring label sequence for the entity recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Post-processing</head><p>As explained in the pre-processing section, we divided each question with multiple gold-label snippets into separate inputs. These examples are merged during post-processing to generate a unique answer for each question. For the postprocessing step, we followed <ref type="bibr" coords="8,257.12,419.02,15.50,8.74" target="#b16">[17]</ref> to combine the predictions to the same question for factoid/list type questions. Majority voting is used to find the highest scoring predictions for each factoid/list type question. For each factoid type question, top N highest scoring predictions are returned where N corresponds to the maximum limit allowed for the BioASQ 8B challenge. For the list type questions, we used 0.50 as the probability threshold, and included all answers that have a higher average probability score.</p><p>For the yes/no type questions, we averaged the probability scores for each example belonging to the same question instance to determine the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Settings</head><p>In this section we explain details regarding the experiments we conducted. All experiments are done using a single V100-GPU. For the Question Answering task we used the BioASQ 6B test sets as our validation set, and used the examples in the BioASQ 8B training set, for training. For the entity recognition task, we kept the same train/dev/test split already provided in <ref type="bibr" coords="8,378.63,632.21,9.96,8.74" target="#b8">[9]</ref>. It takes around 4-5 epochs on the training set to achieve the highest performance on the question answering validation sets for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>The entity recognition component of the final multi-task learning model we used for our submissions is trained on the BC2GM dataset <ref type="bibr" coords="9,368.88,160.94,14.61,8.74" target="#b14">[15]</ref>. The dataset contains 20,703 entity mentions in total and annotated using BIO scheme. The first token of each entity is annotated with 'B' and the following tokens are annotated with 'I'. Non-entity tokens are annotated with 'O'. In order to evaluate our proposed multi-task learner, we trained the entity recognition component on three different datasets. We used the BC2GM <ref type="bibr" coords="9,462.32,353.03,14.61,8.74" target="#b14">[15]</ref>, BC4CHEMD <ref type="bibr" coords="9,196.06,364.98,9.96,8.74" target="#b6">[7]</ref>, and BC5CDR <ref type="bibr" coords="9,278.40,364.98,15.50,8.74" target="#b9">[10]</ref> datasets for biomedical entity recognition which contain gene entities, chemical entities and disease mentions respectively. As we had maximum submission limit of five submissions for each test-batch for the BioASQ 8B challenge, we only used the multi-task learning model trained on the BC2GM dataset.</p><p>The BioASQ 8B training set contains 3,243 questions in total. We did not make use of the 777 summary type questions, so our overall training set contained 2466 questions. For training our models we used only the snippets already provided by the challenge organizers as the relevant passage for each question. Each snippet and question is treated as a unique (Q, P ) pair which is given as input to the question answering component, where Q and P represent 'question' and 'passage', respectively. For evaluating our proposed models, we also used the factoid questions from the BioASQ 6B test set <ref type="bibr" coords="9,241.92,644.16,9.96,8.74" target="#b5">[6]</ref>. The test set contains five-batches, and the number of factoid questions for each batch are given in Table <ref type="table" coords="9,369.33,656.12,3.87,8.74" target="#tab_2">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>In this section, we explain how we trained each of the three models we used to make submissions for the BioASQ 8B challenge. In all three models, we initialized the weights of the BERT component using the BioBERT version 1.1 provided by Lee et al. <ref type="bibr" coords="10,194.42,297.67,10.52,8.74" target="#b8">[9]</ref> pretrained on PubMed articles. To have a fair comparison we always used a maximum sequence length of 256, as we observed that going above this value sometimes resulted in memory issues. Table <ref type="table" coords="10,375.76,321.58,4.98,8.74" target="#tab_3">5</ref> gives a comprehensive list of the hyperparameters we used during our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In this section we start by giving the results we obtained for evaluating our proposed multi-task learner. We compare BioBERT multitask, which learns both entity recognition and question answering tasks simultaneously, with the jointlearning model BioBERT allquestions, which only focuses on the question answering task. The BioASQ 8B data is used to train both models, and the factoid type questions from the BioASQ 6B challenge is used to evaluate them, which contains five different test batches. For training the entity recognition component of the multi-task learning model, we used three different biomedical entity datasets. The results for both models are given in Table <ref type="table" coords="11,385.41,572.43,3.87,8.74" target="#tab_4">6</ref>. Our results showed that learning both tasks simultaneously improved the performance for all entity datasets and for all test batches. For all three datasets we observed that the multi-task learning model outperformed the model that only learns the question answering task on all five test-batches. These results verified our initial claim on transfering information from entity recognition task to improve the performance on the target question answering task, and motivated us to apply the proposed multi-task learning model on the BioASQ 8B test sets. Next, we give the results obtained on the BioASQ 8B challenge for each model we explained above. For the first test-batch we only made submissions using two models: BioBERT baseline, BioBERT allquestions. For the other four test-batches we made five submissions using the three models explained above. To be able to make a clean comparison between the proposed models, we kept the post-processings schemes identical for all our submissions. This is necessary to evaluate our claim on using multi-task learning to improve the performance on biomedical question answering task.</p><p>The QAS components of the joint-learning model and the model-task learning model are identical. In order to evaluate our claim on using multi-task learning for question answering, we must compare these models, rather than comparing them with the single-task learning model which uses a different architecture (separate models for each question type). The results show that for the factoid questions, the multi-task learning based model outperformed all three joint-learning models for all four test-batches. This clearly shows that leveraging information obtained about genes and proteins may help improve the final performance on the factoid type questions. The results for list and yes/no type questions are mixed, and the benefits of multi-task learning are unclear for these types.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,183.82,582.79,74.80,7.86;5,346.50,576.19,95.28,7.86"><head></head><label></label><figDesc>(a) Yes/No model. (b) Factoid/List model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,603.73,345.82,8.74;5,134.77,615.69,345.83,8.74;5,134.77,627.64,280.86,8.74;5,152.06,444.95,138.32,130.32"><head>Fig. 1 :Fig. 2 :</head><label>12</label><figDesc>Fig. 1: Overall architectures for training separate models for yes/no and factoid/list question types for the BioBERT baseline model [17]. The common BioBERT model layers are finetuned from scratch for each type.</figDesc><graphic coords="5,152.06,444.95,138.32,130.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,216.11,143.44,139.65,9.65;8,215.71,158.39,37.06,9.65;8,253.49,156.32,12.40,6.12;8,266.61,158.39,16.22,9.65;8,218.22,173.30,87.60,9.68;8,188.63,188.25,238.11,8.77"><head></head><label></label><figDesc>b j = BioBERT (t 1 , ..., t i , ...t n ; j) s j = F C ner (b j ) S = [s 1 , ..., s j , ..., s n ] crf loss = f orward score(S, T) -path score(S, T, G)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,134.77,243.78,345.82,65.43"><head>Table 2 :</head><label>2</label><figDesc>Statistics about the datasets used for the biomedical entity recognition task.</figDesc><table coords="9,194.37,268.04,226.62,41.16"><row><cell>Dataset</cell><cell>Split</cell><cell cols="2"># of Entity Tokens # of Entities</cell></row><row><cell></cell><cell>Train</cell><cell>37,301</cell><cell>15,197</cell></row><row><cell>BC2GM</cell><cell>Development</cell><cell>7,498</cell><cell>3,061</cell></row><row><cell></cell><cell>Test</cell><cell>15,101</cell><cell>6,325</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,134.77,545.47,345.83,54.87"><head>Table 3 :</head><label>3</label><figDesc>Number of questions in the BioASQ 8B dataset. Summary type questions are not used in our work.</figDesc><table coords="9,178.66,569.74,258.05,30.60"><row><cell>Dataset</cell><cell cols="4">Summary Factoid Yes-No List Total</cell></row><row><cell>BioAsq-8B</cell><cell>777</cell><cell>941</cell><cell>644</cell><cell>881 3,243</cell></row><row><cell>BioAsq-8B pre-processed</cell><cell>-</cell><cell cols="3">4,916 9,786 9,472 24,174</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,134.77,127.36,345.82,87.35"><head>Table 4 :</head><label>4</label><figDesc>Number of factoid questions in each test batch of the BioASQ 6B challenge used for evaluating the multi-task learning model.</figDesc><table coords="10,232.05,151.63,151.26,63.08"><row><cell cols="2">Batch Number of Factoid Questions</cell></row><row><cell>batch-1</cell><cell>31</cell></row><row><cell>batch-2</cell><cell>21</cell></row><row><cell>batch-3</cell><cell>32</cell></row><row><cell>batch-4</cell><cell>33</cell></row><row><cell>batch-5</cell><cell>44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,134.77,376.32,345.83,288.53"><head>Table 5 :</head><label>5</label><figDesc>Important hyperparameters used by the models we proposed.Baseline model trainingThe baseline model (BioBERT baseline) is composed of two completely separate neural architectures (one for yes/no and one for factoid/list type questions). In this approach, each architecture is trained separately, only using the corresponding dataset. During pre-processing, list type questions are converted into factoid question format, by treating each answer in the list of answers as a single factoid type answer. After this pre-processing step, the format of the factoid and list type questions become identical, so that the same architecture can be used for training on both types. Yes/No' component in Figure2is used to generate the output of the model. Otherwise, the 'Factoid/List' component is used to generate the 'start' and 'end' scores for each token inside the given passage P . The loss for each input example is backpropagated to update the weights of 1) the questiontype specific component, and 2) the common BioBERT component. This way, we allow information transfer between different question types. Considering the relatively small sizes of the biomedical question answering datasets, this allows a better utilization of what is available. Besides, this approach reduces the total number of parameters of the final model almost by half, as the majority of the trainable parameters are the common BioBERT weights. As we have multiple target performance metrics (overall performance and performances on each question type), we continued the training until we could not observe any improvement for any question type on the question answering validation sets.Multi-task learning model trainingThe multi-task learning model (BioBERT multitask) is simultaneously trained for the question answering and the entity recognition tasks. At each iteration we flip a random coin to determine the task type (QAS or NER), and use the corresponding component from Figure3. Similar to the joint-learning model this allows information transfer from the NER dataset examples for the question answering task. The common BioBERT model is updated using examples from both tasks, which allows us to expose the model for a significantly larger amount of sentences from the biomedical domain. In this work, entity recognition task is used as an auxiliary task to help improve the final performance on the target question answering task. For this reason, training is done until we could not observe any improvement on the question answering validation set.</figDesc><table coords="10,134.77,388.63,345.83,276.22"><row><cell>Name</cell><cell>Final Value</cell><cell>Range</cell></row><row><cell>Maximum Sequence Length</cell><cell>256</cell><cell>[64-512]</cell></row><row><cell>NER learning rate</cell><cell>0.0015</cell><cell>[0.001-0.1]</cell></row><row><cell>QAS learning rate</cell><cell>5e -4</cell><cell>[5e -4 -e -2 ]</cell></row><row><cell>QAS Adam Epsilon</cell><cell>e -8</cell><cell>[e -10 -e -5 ]</cell></row><row><cell>NER Adam Epsilon</cell><cell>e -6</cell><cell>[e -10 -e -4 ]</cell></row><row><cell>Optimizer</cell><cell cols="2">AdamW Adam, AdamW, SGD</cell></row><row><cell>Weight Decay</cell><cell>0.001</cell><cell>[0 -0.01]</cell></row><row><cell cols="3">Joint-learning model training The joint learner (BioBERT allquestions) is</cell></row><row><cell cols="3">trained on all question types at once. At each iteration a (Q, P ) pair is picked</cell></row><row><cell cols="3">randomly from the whole training set. If the picked example is a 'yes/no' type</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,134.77,127.36,345.82,247.38"><head>Table 6 :</head><label>6</label><figDesc>Analysis of multi-task learning of QA and ER using different ER datasets. Results are given for each test batch of the BioASQ6 Challenge. The official metric used for evaluating models is MRR. Best results for each test-batch are denoted in bold.</figDesc><table coords="12,224.93,175.21,168.04,199.53"><row><cell></cell><cell></cell><cell>BioASQ-6</cell></row><row><cell>Model</cell><cell cols="2">Test batch SAcc LAcc MRR</cell></row><row><cell></cell><cell cols="2">batch 1 0.581 0.742 0.646</cell></row><row><cell></cell><cell cols="2">batch 2 0.667 0.857 0.74</cell></row><row><cell>QA Only (Baseline)</cell><cell cols="2">batch 3 0.594 0.781 0.682</cell></row><row><cell></cell><cell cols="2">batch 4 0.545 0.636 0.586</cell></row><row><cell></cell><cell cols="2">batch 5 0.455 0.523 0.485</cell></row><row><cell></cell><cell cols="2">batch 1 0.581 0.71 0.645</cell></row><row><cell></cell><cell cols="2">batch 2 0.714 0.857 0.786</cell></row><row><cell>QA + BC2GM</cell><cell cols="2">batch 3 0.625 0.781 0.698</cell></row><row><cell></cell><cell cols="2">batch 4 0.576 0.636 0.606</cell></row><row><cell></cell><cell cols="2">batch 5 0.477 0.523 0.5</cell></row><row><cell></cell><cell cols="2">batch 1 0.71 0.742 0.72</cell></row><row><cell></cell><cell cols="2">batch 2 0.714 0.857 0.778</cell></row><row><cell>QA + BC4CHEMD</cell><cell cols="2">batch 3 0.656 0.781 0.709</cell></row><row><cell></cell><cell cols="2">batch 4 0.606 0.636 0.616</cell></row><row><cell></cell><cell>batch 5</cell><cell>0.5 0.523 0.511</cell></row><row><cell></cell><cell cols="2">batch 1 0.677 0.742 0.7</cell></row><row><cell></cell><cell cols="2">batch 2 0.714 0.857 0.786</cell></row><row><cell>QA + BC5CDR</cell><cell cols="2">batch 3 0.656 0.781 0.719</cell></row><row><cell></cell><cell cols="2">batch 4 0.576 0.636 0.601</cell></row><row><cell></cell><cell>batch 5</cell><cell>0.5 0.523 0.511</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,656.80,74.30,7.86"><p>http://bioasq.org/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we described the models we used to make submissions for the BioASQ 8B challenge. We proposed a novel multi-task learning model for biomedical entity recognition and question answering tasks. Our results showed that transferring information from the entity recognition task consistently improved the performance on the factoid type questions of the question answering tasks. On all test-batches of both BioASQ 6B and BioASQ 8B challenges, transferring information brought improvement for factoid questions. We believe that further improvements can be achieved by implementing a more sophisticating information sharing between the two tasks. Analyzing the characteristics of each dataset used, can help us understand why transfer learning improves/degrades the performance for each question type. So far we have only considered using domain-adaptive pretrained models (BioBERT-based). Recent work on pretraining showed that task-adaptive pretraining brings additional improvement for low-resource tasks <ref type="bibr" coords="13,403.99,543.63,9.96,8.74" target="#b4">[5]</ref>. Our plan is to incorporate task-adaptive pretraining for the biomedical question answering task.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="14,142.96,142.36,337.63,7.86;14,151.52,153.32,329.07,7.86;14,151.52,164.28,121.43,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,302.92,142.36,177.67,7.86;14,151.52,153.32,30.26,7.86">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,202.10,153.32,278.49,7.86;14,151.52,164.28,28.76,7.86">COLING 2018, 27th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,175.19,337.63,7.86;14,151.52,186.15,329.07,7.86;14,151.52,197.11,270.06,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,207.45,175.19,273.13,7.86;14,151.52,186.15,21.39,7.86">Research on task discovery for transfer learning in deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Akdemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,193.99,186.15,286.60,7.86;14,151.52,197.11,194.65,7.86">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="33" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,208.02,337.63,7.86;14,151.52,218.98,329.07,7.86;14,151.52,229.94,329.07,7.86;14,151.52,240.90,329.07,7.86;14,151.52,251.86,86.01,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,354.85,208.02,125.74,7.86;14,151.52,218.98,205.07,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,380.67,218.98,99.92,7.86;14,151.52,229.94,329.07,7.86;14,151.52,240.90,174.01,7.86">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="14,142.96,262.75,337.63,7.89;14,151.52,273.73,25.60,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,212.08,262.77,88.89,7.86">The viterbi algorithm</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">D</forename><surname>Forney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,309.76,262.77,104.36,7.86">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="268" to="278" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,284.65,337.64,7.86;14,151.52,295.61,329.07,7.86;14,151.52,306.56,130.38,7.86" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10964</idno>
		<title level="m" coord="14,204.38,295.61,272.40,7.86">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.96,317.48,337.63,7.86;14,151.52,328.44,329.07,7.86;14,151.52,339.40,329.07,7.86;14,151.52,351.00,193.49,7.47" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,356.26,317.48,124.33,7.86;14,151.52,328.44,329.07,7.86;14,151.52,339.40,28.51,7.86">Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</title>
		<ptr target="https://www.aclweb.org/anthology/W18-5300" />
	</analytic>
	<monogr>
		<title level="m" coord="14,186.64,339.40,167.19,7.86">Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krithara</surname></persName>
		</editor>
		<meeting><address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11">Nov 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,361.27,337.63,7.86;14,151.52,372.23,329.07,7.86;14,151.52,383.19,215.87,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,351.04,361.27,129.55,7.86;14,151.52,372.23,137.04,7.86">Overview of the BioCreative VI chemical-protein interaction Track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Rabal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Akhondi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,310.09,372.23,170.50,7.86;14,151.52,383.19,103.86,7.86">Proceedings of the sixth BioCreative challenge evaluation workshop</title>
		<meeting>the sixth BioCreative challenge evaluation workshop</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="141" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,394.10,337.63,7.86;14,151.52,405.06,329.07,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="14,453.69,394.10,26.90,7.86;14,151.52,405.06,165.50,7.86">Neural architectures for named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.96,415.97,337.64,7.86;14,151.52,426.93,329.07,7.86;14,151.52,437.86,329.07,7.89;14,151.52,448.85,190.60,7.86;14,367.62,449.50,112.98,7.47;14,151.52,460.45,98.85,7.47" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,151.52,426.93,329.07,7.86;14,151.52,437.89,107.13,7.86">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btz682" />
	</analytic>
	<monogr>
		<title level="j" coord="14,274.03,437.89,58.56,7.86">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2019-09">09 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,470.72,337.98,7.86;14,151.52,481.68,329.07,7.86;14,151.52,492.61,252.99,7.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,307.45,481.68,173.14,7.86;14,151.52,492.64,155.72,7.86">BioCreative V CDR task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,314.25,492.64,37.30,7.86">Database</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,503.55,337.98,7.86;14,151.52,514.51,329.07,7.86;14,151.52,525.47,329.07,7.86;14,151.52,536.43,329.07,7.86;14,151.52,547.39,25.60,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,165.31,514.51,166.11,7.86">Deep contextualized word representations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,352.22,514.51,128.37,7.86;14,151.52,525.47,329.07,7.86;14,151.52,536.43,159.26,7.86">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="14,142.61,558.30,337.98,7.86;14,151.52,569.26,207.29,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,353.95,558.30,126.65,7.86;14,151.52,569.26,124.08,7.86">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,296.49,569.26,33.66,7.86">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,580.18,337.98,7.86;14,151.52,591.11,329.07,7.89;14,151.52,602.09,42.49,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,305.69,580.18,174.91,7.86;14,151.52,591.13,35.25,7.86">Coqa: A conversational question answering challenge</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,194.19,591.13,253.12,7.86">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="249" to="266" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,613.01,337.98,7.86;14,151.52,623.97,185.23,7.86" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<title level="m" coord="14,193.34,613.01,230.13,7.86">Neural Transfer Learning for Natural Language Processing</title>
		<meeting><address><addrLine>Galway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>National University of Ireland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="14,142.61,634.88,337.98,7.86;14,151.52,645.84,329.07,7.86;14,151.52,656.77,244.12,7.89" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,376.49,645.84,104.10,7.86;14,151.52,656.80,99.78,7.86">Overview of biocreative ii gene mention recognition</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">K</forename><surname>Tanabe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Nee Ando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,258.30,656.80,64.83,7.86">Genome biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,119.67,337.97,7.86;15,151.52,130.63,329.07,7.86;15,151.52,141.59,329.07,7.86;15,151.52,152.55,247.15,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="15,288.50,119.67,192.09,7.86;15,151.52,130.63,26.62,7.86">becas: The surprising cross-lingual effectiveness of bert</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">:</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Beto</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,200.61,130.63,279.98,7.86;15,151.52,141.59,329.07,7.86;15,151.52,152.55,161.93,7.86">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="833" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,163.51,337.98,7.86;15,151.52,174.47,329.07,7.86;15,151.52,185.43,318.83,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="15,366.19,163.51,114.41,7.86;15,151.52,174.47,139.41,7.86">Pre-trained language model for biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,315.32,174.47,165.27,7.86;15,151.52,185.43,195.14,7.86">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="727" to="740" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
