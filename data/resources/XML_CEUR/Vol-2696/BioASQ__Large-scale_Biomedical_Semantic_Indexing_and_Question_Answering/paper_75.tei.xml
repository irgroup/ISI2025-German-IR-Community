<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.17,115.96,321.02,12.62;1,154.16,133.89,307.03,12.62">Transformer-Based Open Domain Biomedical Question Answering at BioASQ8 Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,153.49,171.56,70.66,8.74"><forename type="first">Ashot</forename><surname>Kazaryan</surname></persName>
							<email>ashot.kazaryan@jetbrains.com</email>
							<affiliation key="aff0">
								<orgName type="institution">JetBrains Research</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">ITMO University</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.03,171.56,96.36,8.74"><forename type="first">Uladzislau</forename><surname>Sazanovich</surname></persName>
							<email>uladzislau.sazanovich@jetbrains.com</email>
							<affiliation key="aff0">
								<orgName type="institution">JetBrains Research</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">ITMO University</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.66,171.56,77.40,8.74"><forename type="first">Vladislav</forename><surname>Belyaev</surname></persName>
							<email>vladislav.belyaev@jetbrains.com</email>
							<affiliation key="aff0">
								<orgName type="institution">JetBrains Research</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National Research University Higher School of Economics</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.17,115.96,321.02,12.62;1,154.16,133.89,307.03,12.62">Transformer-Based Open Domain Biomedical Question Answering at BioASQ8 Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">51D9ECBC8F63115EF5C95A3CAE116E77</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>BioASQ Challenge</term>
					<term>Biomedical Question Answering</term>
					<term>Open Domain Question Answering</term>
					<term>Information Retrieval</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>BioASQ task B focuses on biomedical information retrieval and question answering. This paper describes the participation and proposed solutions of our team. We build a system based on recent advances in the general domain as well as the approaches from previous years of the competition. We adapt a system based on a pretrained BERT for document and snippet retrieval, question answering and summarization. We describe all approaches we experimented with and show that while neural approaches do well, sometimes baseline approaches have high automatic metrics. The proposed system achieves competitive performance while being general so that it can be applied to other domains as well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>BioASQ <ref type="bibr" coords="1,172.79,460.74,15.50,8.74" target="#b27">[27]</ref> is a large scale competition for biomedical research. It provides evaluation measures for various setups like semantic indexing, information retrieval and question answering, all regarding the biomedical domain. The competition takes place annually online, and each year gains more attention from research groups all around the world. The BioASQ provides necessary datasets, evaluation metrics and leaderboards for each of its sub-challenges.</p><p>More specifically, the BioASQ challenge consists of two major objectives which are called "tasks". The first is semantic indexing, which goal is to construct a search index given a set of documents, such that certain semantic relationships are held between the index terms. The second objective is passage ranking and question answering in various forms, which is given a question to return a piece of text. The returned text must either answer the question directly or contain enough information to derive the answer. In terms of the BioASQ, those objectives are called Task A and Task B, respectively.</p><p>In this work, we explore applications of the state-of-the-art model in natural language processing and deep learning in biomedical question answering. As a result, we develop a system, that is capable of providing answers in the form of documents, snippets, exact answers or abstractive text, given biomedical questions from various domains. We evaluate our system on the recent BioASQ 2020 challenge, where it achieves competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">BioASQ Tasks</head><p>Our team participated in the Task B, which involves information retrieval, question answering, summarization and more. This task uses benchmark datasets containing development and test questions, in English, along with gold standard (reference) answers constructed by a team of biomedical experts. The task is separated into two phases.</p><p>Phase A The first phase measures the ability of systems to answer biomedical questions with a list of relevant documents and snippets of text from retrieved documents. The main metric for documents and snippets is the mean average precision (MAP). The average precision is defined as follows:</p><formula xml:id="formula_0" coords="2,252.58,364.40,109.00,26.33">AP = |L| r=1 P (r) • rel(r) |L R |</formula><p>where |L| is the number of items in a list predicted by the system, |L R | is the number of relevant items. P (r) is a precision when only first r returned items are considered, and rel(r) is equal to 1 if the r-th returned item is relevant. MAP and GMAP are arithmetic and geometric means of all questions in the evaluation set. For the snippets retrieval, precision is measured in terms of characters, and rel(r) is equal to 1 if the returned item has non-zero overlap with at least one relevant snippet. Additional metrics are precision, recall and F1 score. A more detailed description is present in the original paper <ref type="bibr" coords="2,360.86,482.13,14.61,8.74" target="#b12">[13]</ref>.</p><p>Phase B The second phase evaluates the performance of question answering, given a list of relevant documents and snippets from the previous phase. The questions are of several types: questions where the answer is either yes or no ("yes/no"), questions where the answer is a single term ("factoid"), and questions where the answer is a list of terms ("list"). Additionally, each question has an "ideal" answer, where the aim is to measure the systems' ability to generate paragraph sized passage, that answers the question. The metrics of phase B are F1-macro for yes/no questions, mean reciprocal rank (MRR) <ref type="bibr" coords="2,193.37,608.30,15.50,8.74" target="#b29">[29]</ref> for factoid questions and F1 score for list questions. To evaluate answers in natural language the ROUGE <ref type="bibr" coords="2,342.76,620.25,15.50,8.74" target="#b15">[16]</ref> scores are used. We should note that human experts will additionally evaluate all systems after the contest. However, the results are not available at the time of writing this paper, thus we use only the automatic measurements to draw our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>Most of the contemporary large scale QA systems attempt to fill the gap between massive source of knowledge and a complex neural reasoning model. Many of the popular knowledge sources are sets of unstructured or semi-structured texts, like Wikipedia <ref type="bibr" coords="3,182.26,200.26,9.96,8.74" target="#b4">[5]</ref>, <ref type="bibr" coords="3,198.50,200.26,14.61,8.74" target="#b33">[33]</ref>. It is still the case for biomedical domain, where the PubMed <ref type="bibr" coords="3,134.77,212.21,10.52,8.74" target="#b3">[4]</ref> is amongst the largest sources of biomedical scientific knowledge.</p><p>During document retrieval a question answering system can benefit from structured knowledge as well. There is a rich set of different biomedical ontologies like UMLS <ref type="bibr" coords="3,182.83,253.17,10.52,8.74" target="#b1">[2]</ref> or GO <ref type="bibr" coords="3,224.93,253.17,9.96,8.74" target="#b0">[1]</ref>, successful use of which is shown in different QA systems, including ones that were submitted by previous years BioASQ participants <ref type="bibr" coords="3,462.33,265.13,14.61,8.74" target="#b11">[12]</ref>. However, in our work we do not leverage such information and instead explore a more general approach, applicable to any other domain.</p><p>Many systems perform re-ranking after initial document retrieval. Specialized neural models like DRMM <ref type="bibr" coords="3,254.95,318.04,10.52,8.74" target="#b7">[8]</ref> have been successfully used in previous BioASQ challenges <ref type="bibr" coords="3,180.62,330.00,9.96,8.74" target="#b2">[3]</ref>. More recent approaches utilize transformer-based language models <ref type="bibr" coords="3,134.77,341.95,15.50,8.74" target="#b28">[28]</ref> like BERT <ref type="bibr" coords="3,207.11,341.95,10.52,8.74" target="#b5">[6]</ref> for wide variety of tasks. Applications of transformers in document re-ranking had set a new state of the art <ref type="bibr" coords="3,356.68,353.91,14.61,8.74" target="#b20">[21]</ref>, including the last years' BioASQ challenges <ref type="bibr" coords="3,222.50,365.86,14.61,8.74" target="#b21">[22]</ref>. There are also systems that do document re-ranking based on snippet extraction <ref type="bibr" coords="3,257.92,377.82,14.61,8.74" target="#b21">[22]</ref>, but they did not achieve the highest positions. Some systems solve snippet extraction by utilizing methods that were originally developed for document re-ranking. <ref type="bibr" coords="3,314.41,406.83,15.50,8.74" target="#b21">[22]</ref> uses the earlier mentioned DRMM in the Task 7B and achieves top results. <ref type="bibr" coords="3,313.75,418.78,15.50,8.74" target="#b23">[23]</ref> comes up with another neural approach, employing both textual and conceptual information from question and candidate text. In our work, we experiment with different methods and show how strong baselines consistently demonstrate high metrics, given a proper document retriever.</p><p>Deep learning has shown its superiority in question answering. In the Task 5b, <ref type="bibr" coords="3,151.35,495.61,15.50,8.74" target="#b30">[30]</ref> achieve top scores by training an RNN based neural network. However, most of the modern advancements in question answering can be attributed to transformer-based models. Last years' challenges were dominated by systems that used BERT or its task-specific adapations, like BioBERT <ref type="bibr" coords="3,408.94,531.47,14.61,8.74" target="#b34">[34]</ref>, <ref type="bibr" coords="3,430.50,531.47,14.61,8.74" target="#b9">[10]</ref>. In this work we experiment with a similar approach.</p><p>Deep neural and transformer based models in particular have shown their ability to tackle summarization in different setups <ref type="bibr" coords="3,360.79,572.43,14.61,8.74" target="#b16">[17]</ref>, including QA summarization <ref type="bibr" coords="3,170.58,584.39,14.61,8.74" target="#b14">[15]</ref>. However, as <ref type="bibr" coords="3,245.10,584.39,15.50,8.74" target="#b18">[19]</ref> noticed, BioASQ summaries tend to look very similar to the input examples. They exploit this observation and introduce several solutions, based on sentence re-ranking, and achieve top automatic and human scores in several batches. There is also an attempt to utilize pointer-generator networks <ref type="bibr" coords="3,177.86,632.21,15.50,8.74" target="#b26">[26]</ref> for BioASQ ideal questions <ref type="bibr" coords="3,324.29,632.21,9.96,8.74" target="#b6">[7]</ref>. During the competition we extend the snippet re-ranking approach by using transformer models. Moreover, we introduce a fully generational approach, based on transformers as well.</p><p>In this section, we describe the system we implemented for document and snippet retrieval, as well as our question answering system. We provide the results of different approaches which we experimented with during the competition. To assess the performance of different methods more accurately, we merge all test batches of 8B task into one and use resulting 500 questions as an evaluation set. Here and during the competition, we created a development set using 100 questions from 6B and 200 questions from 7B task. Our system evolved from batch to batch, achieving its final shape in batch 5. All the ablation experiments and retrospective evaluations are performed on the system, that was used for the 5-th batch submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document retrieval</head><p>For document retrieval, we implement a system conceptually similar to <ref type="bibr" coords="4,447.95,312.95,14.61,8.74" target="#b20">[21]</ref>. At first we extract a list of N document candidates using Anserini implementation of bm25 algorithm <ref type="bibr" coords="4,222.63,336.86,14.61,8.74" target="#b32">[32]</ref>. Then we use the BERT model to re-rank candidate documents and output at most ten top-scored documents.</p><p>BM25 For initial document retrieval, we used Anserini <ref type="bibr" coords="4,392.53,382.91,14.61,8.74" target="#b32">[32]</ref>. We created an index using the PubMed Baseline Repository of the 2019 year <ref type="bibr" coords="4,423.88,394.87,9.96,8.74" target="#b3">[4]</ref>. For each paper in the PubMed Baseline, we extracted the PubMed identifier, the title, and the abstract. We stored title and abstract as separate fields in the index. We applied default stopwords filtering and Porter stemming to the title and abstract provided with Anserini <ref type="bibr" coords="4,283.15,442.69,14.61,8.74" target="#b31">[31]</ref>. Overall the searcher index contains 19 million documents.</p><p>BERT Re-ranking The initial set of documents obtained with BM25 is passed to the BERT re-ranker, which assigns relevance scores to documents based on a question. We consider all documents with a score higher than a threshold to be relevant and output at most ten papers with the highest scores. To train BERT re-ranker, we created a binary classification dataset. We obtained positive examples from the gold documents of BioASQ dataset. We collected negative examples using BM25 by extracting 200 documents using a question as a query and consider all documents starting from position 100 to be non-relevant if they are not in the gold documents set. As BioASQ dataset for question answering contains questions collected from the past year contests, the relevant documents include only the papers published before the year of the competition. Usually, there are several relevant documents for the question that were published after the year of the contest. To exclude such papers from the negative examples, we calculated the maximum publication year for all relevant documents and filtered all documents published after this year from the negative examples.</p><p>Experiments We evaluated several approaches to document retrieval. First, we evaluated the performance of the BM25 algorithm, and then we applied different modifications of BERT-based re-ranker. We examined the effects of relevance score threshold as well as the number of documents obtained from the BM25 stage. The results are presented in table 1. We can see how BERT-based re-ranker consistently improves base BM25 performance.</p><p>Since our re-ranker is trained to perform logistic regression, we can vary the decision boundary to achieve an appropriate trade-off between precision and recall. However, the MAP metric, which is used as a final ranking measure, does not penalize the system for additional non-relevant documents, which means the system should always output as much documents as possible to achieve the highest score, while reducing its practical usefulness. We decided to orient our system towards both precision and recall and as a result we achieve the highest F1 scores across all batches, while maintaining competitive MAP scores.</p><p>Table <ref type="table" coords="5,164.05,307.19,4.13,7.89">1</ref>. Results of different approaches to document retrieval on the combined test set of 500 questions from 8B. N is the number of documents returned from the BM25 stage. T is the score threshold for relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Precision Recall F-Measure MAP GMAP BM25(N = 10) 0.1190 0.5022 0.1730 0.3579 0.0128 BM25+BERT(N = 50, T = 0.5) 0.2892 0.5158 0.3334 0.3979 0.0155 BM25+BERT(N = 50, T = 0.) 0.1358 0.5481 0.1954 0.4114 0.0221 BM25+BERT(N = 500, T = 0.5) 0.2734 0.5387 0.3249 0.4046 0.0191</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Snippet Retrieval</head><p>Snippet extraction systems extract a continuous span of text from one of the relevant documents for the given question. We observe that snippets from the BioASQ training set are usually one sentence long, thus our system is designed as a sentence retriever and snippet extraction is formulated as a sentence ranking problem. We experiment with both neural and statistical approaches to tackle this challenge.</p><p>Baseline We use a simple statistical baseline for sentence ranking, which is based on measuring entity cooccurrence in question and candidate sentence. For each question and sentence we extract sets of entities Q and S respectively and compute relevance score:</p><formula xml:id="formula_1" coords="5,252.13,623.90,109.89,22.31">relevance(q, s) = |Q ∩ S| |Q|</formula><p>We use ScispaCy <ref type="bibr" coords="5,227.03,656.12,15.50,8.74" target="#b19">[20]</ref> en core web sm model for extracting entitities.</p><p>Word2Vec Similarity One approach of determining sentence similarity is to map both query and candidate into the same vector space and measure the distance between them. For embedding word sequences, we use Word2Vec model, pretrained on PubMed texts <ref type="bibr" coords="6,266.96,154.86,14.61,8.74" target="#b17">[18]</ref>, and compute the mean of individual word embeddings. Suppose the E q and E s are the embeddings of question and snippet correspondingly. The relevance of snippet for a given question is a cosine similarity between embeddings:</p><formula xml:id="formula_2" coords="6,244.96,210.48,124.24,23.22">relevance(q, s) = E q • E s ||E q ||||E s ||</formula><p>BERT Similarity As the transformer pretrained on the biomedical domain should contain a lot of transferable knowledge, we check the zero-shot performance of the pretrained model. Similar to embeddings similarity, we use cosine distance between the embeddings of question and snippet. The embedding of a text span is the contextualized embedding corresponding to the special [CLS] token which is inserted before the tokenized text. The relevance is a cosine similarity between embeddings of question and snippet.</p><p>BERT Relevance As the task of snippet retrieval is very similar to document retrieval, we test a similar approach. We use BERT rel model, trained for document ranking to assign a relevance score to the pair of question and snippet: relevance(q, s) = BERT rel (q, s) Document Scores Finally, after assigning each question-sentence pair a relevance score, we scale the latter by additional score, based on the position of the document, which the candidate sentences are extracted from, in the list of relevant documents. Despite the simplicity of this trick, experiments show considerable improvements of evaluation metrics, which points out a strong correlation between the rank of the abstracts and the rank of the snippets from those abstracts. For each document d i from the list of ranked relevant documents D = d 1 , d 2 , . . . , d n there is a list of sentences S i = s i,1 , s i,2 , . . . , s i,m and the similarity score between query q and sentence s i,j is: score(q, s i,j ) = relevance(q, s i,j ) i</p><p>Experiments We evaluated all described approaches to snippet retrieval. The results are presented in table <ref type="table" coords="6,273.33,584.39,3.87,8.74" target="#tab_0">2</ref>. We can see that heuristic of adding document score into the score of a snippet allows to improve MAP scores for all approaches significantly. In line with the document retrieval, BERT relevance model has higher precision and recall with lower MAP scores. Surprisingly, retrieval based on BioBERT cosine similarly performed well even without training on any BioASQ data. We can consider the approach to be a zero-shot performance of BioBERT on the task of snippet retrieval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Exact answers</head><p>Factoid and List questions. For factoid and list questions we generate answers with a single extractive question-answering system. Its design follows the classical transformer-based approach, described in <ref type="bibr" coords="7,357.63,327.00,9.96,8.74" target="#b5">[6]</ref>. As an underlying neural model, we use ALBERT <ref type="bibr" coords="7,242.22,338.96,15.50,8.74" target="#b13">[14]</ref> finetuned on SQuAD 2.0 <ref type="bibr" coords="7,370.19,338.96,15.50,8.74" target="#b24">[24]</ref> and BioASQ training set. SQuAD is an extractive question answering dataset, so it is well suited for BioASQ tasks. In essence, list and factoid questions can be handled by the same span extraction technique. Thus we can use the same model for both questions types, differing only at the postprocessing stage. Throughout all the 5 batches we experiment mainly at pre-and post-processing stages, without substantial changes in the architecture of the system itself. During preprocessing, we convert input questions to the SQuAD format <ref type="bibr" coords="7,134.77,434.60,14.61,8.74" target="#b25">[25]</ref>, where contexts are built from the relevant snippets, that come with each input question. The postprocessing stage is implemented in the same manner as <ref type="bibr" coords="7,134.77,458.51,14.61,8.74" target="#b34">[34]</ref>. However, for list question we additionally split the resulting extracted spans by "and/or" and "or" conjunctions, which we observed to be frequently used in chemical/gene enumerations in various biomedical abstracts. Table <ref type="table" coords="7,429.77,482.42,4.98,8.74" target="#tab_1">3</ref> shows the importance of this step. Yes/No questions. For yes/no questions we formulate the task as a logistic regression over question-snippet pairs and implement a transformer-based ap-proach, similar to <ref type="bibr" coords="8,212.00,118.99,14.61,8.74" target="#b34">[34]</ref>. We use the ALBERT model and fine-tune it using SQuAD and BioASQ datasets. In the fifth batch, we additionaly use PubMedQA dataset <ref type="bibr" coords="8,134.77,142.90,15.50,8.74" target="#b10">[11]</ref> and replace the model with BioBERT. Despite that PubMedQA contains more than 200 thousand labelled examples, the average question length is twice as large as BioASQ questions' length is. We sampled 2 thousand questions with similar to BioASQ questions distribution and incorporated them into the final train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Summarization</head><p>Phase B also includes summarization objective, where a participating system has to generate a paragraph sized text, answering the question. We come up with different approaches for tackling this challenge.</p><p>Weak baseline BioASQ does not impose any limitations on the source of the summary. We observed that summaries tend to be one or two sentences long, reminding how snippets are composed. Straightforward approach is to use snippets, provided with the question for computing the summary. Our weak baseline selects the first snippet from the question for this purpose.</p><p>Snippet Reranking Naturally, the first snippet may not answer the question directly and clearly, despite being marked as the most relevant. A logical improvement to the baseline is to select the appropriate snippet, potentially in a question-aware manner. To make answers more granular, we split snippets by sentences and the resulting candidate pool contains snippets and snippet sentences. Sometimes, however, snippets are absent for a given question. In that case we extract the candidate sentences from the relevant abstracts. For re-ranking, we use BERT rel trained for document re-ranking, as described in 2.2. Overall, we can describe this system as sentence-level extractive summarization.</p><p>Abstractive Summarization Our final system performs abstractive summarization over provided snippets. We use traditional encoder-decoder transformer architecture <ref type="bibr" coords="8,190.98,518.02,14.61,8.74" target="#b28">[28]</ref>, where the encoder is based on BioMed-RoBERTa <ref type="bibr" coords="8,440.21,518.02,9.96,8.74" target="#b8">[9]</ref>, while the decoder is trained from scratch, following BertSUM <ref type="bibr" coords="8,381.80,529.98,14.61,8.74" target="#b16">[17]</ref>. First, we pretrain the model on a summarization dataset based on PubMed, where the target is an arbitrary span from the abstract and the source is a piece of text, from which the target can be derived. After that, we fine-tune the model to produce summaries given the question and concatenation of relevant snippets from the BioASQ training dataset, separated with a special token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>In this section, we present an official automatic evaluation of our system, comparing to the top competitor system. We denote our system as "PA" which stands for the Paper Analyzer team. We additionally perform a retrospective evaluation of phase A, where the gold answers are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Documents Retrieval</head><p>In table <ref type="table" coords="9,170.97,188.23,3.87,8.74" target="#tab_2">4</ref>, we present the results of our document retrieval system on all batches compared to the top competitor. The final design of our system was implemented only in the fifth batch. So, to evaluate our proposed system against our own and other participants' systems from previous batches, we computed evaluation metrics over golden answers, provided by BioASQ for the Phase B. We were able to fully reproduce official leaderboard scores for the fifth batch and show, that our final system outperforms all our previous submissions. The retrospective evaluation shows that we significantly improved our system during the contest and achieved better results with the final system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Snippet Retrieval</head><p>In table 4, we present the results of our snippet retrieval system on all batches compared to the top competitor. Similar to the document retrieval, we performed a retrospective evaluation on all batches for the final implemented system. The evaluation shows that we significantly improved our system during the contest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Question Answering</head><p>We submitted only baselines for batches 1 and 2, so we present results only for batches starting with 3. Overall, we achieved moderate results on the question answering task, as we mainly focused on Phase A. We believe this was caused by poor selection of the training dataset. We will analyze errors and perform additional experiments in the future. The performance of our system is presented in tables 5 and 6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Summarization</head><p>We evaluated our systems in all the five batches. However, we were able to experiment with only one system per batch. The results are presented in the table 7. We show how simple snippet re-ranker can achieve top scores in automatic evaluation. Meanwhile the abstractive summarizer, while providing readable and coherent responses, achieves lower scores, however still very competitive ones. We hope that human evaluation will show the opposite results. We included side-by-side comparison of answers provided by both systems in the appendix (table <ref type="table" coords="10,163.54,656.12,3.87,8.74">8</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In this work, we demonstrate several approaches for our first participation in BioASQ competition. We propose a working system for each of the evaluation objectives in Task 8B, achieving top positions in several batches. We designate a few areas for improvements. First, a more robust experimentation process is required. After the challenge, we found several errors in document and snippeet retrieval pipelines that could affect the resulting performance. Second, our system does not utilize semantic indexing, which can be implemented using different biomedical ontologies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.77,115.91,345.83,139.15"><head>Table 2 .</head><label>2</label><figDesc>Results of different approaches to snippet retrieval on the combined test set of 500 questions from 8B. "Docs" means scaling the snippet score by the position of the source document.</figDesc><table coords="7,158.45,157.41,298.45,97.65"><row><cell>Method</cell><cell>Precision Recall F-Measure MAP GMAP</cell></row><row><cell>Baseline</cell><cell>0.1631 0.2871 0.1841 0.6521 0.0036</cell></row><row><cell>Baseline + Docs</cell><cell>0.1733 0.2876 0.1934 0.8902 0.0020</cell></row><row><cell>Word2Vec Similarity</cell><cell>0.1702 0.2941 0.1904 0.6408 0.0054</cell></row><row><cell cols="2">Word2Vec Similarity + Docs 0.1727 0.2850 0.1928 0.9350 0.0019</cell></row><row><cell>BERT Similarity</cell><cell>0.1607 0.2621 0.1763 0.6338 0.0031</cell></row><row><cell>BERT Similarity + Docs</cell><cell>0.1733 0.2847 0.1927 0.9374 0.0019</cell></row><row><cell>BERT Relevance</cell><cell>0.1931 0.3383 0.2174 0.6926 0.0102</cell></row><row><cell>BERT Relevance + Docs</cell><cell>0.1921 0.3344 0.2161 0.8098 0.0071</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,525.82,345.83,72.20"><head>Table 3 .</head><label>3</label><figDesc>The performance of the QA model for list questions with and without splitting of answers by conjunctions as a postprocessing step. The evaluation is performed on the first batch of 8B.</figDesc><table coords="7,181.15,567.31,253.05,30.70"><row><cell>Method</cell><cell cols="3">Mean Precision Mean Recall F-Measure</cell></row><row><cell>BioBERT</cell><cell>0.2750</cell><cell>0.2250</cell><cell>0.2305</cell></row><row><cell>BioBERT + conj split</cell><cell>0.3884</cell><cell>0.5629</cell><cell>0.4315</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.77,316.86,345.83,228.96"><head>Table 4 .</head><label>4</label><figDesc>The performance of the document and snippet retrieval system on all batches of task 8B. "final" represents the retrospective evaluation of a system for batch 5 on previous batches. "Top Competitor" is a top-scoring submission from other teams.</figDesc><table coords="9,151.93,360.10,311.50,185.72"><row><cell></cell><cell>Documents</cell><cell>Snippets</cell></row><row><cell>Batch System</cell><cell cols="2">F-Measure MAP GMAP F-Measure MAP GMAP</cell></row><row><cell>1 PA final</cell><cell cols="2">0.3389 0.3718 0.0156 0.1951 0.8935 0.0019</cell></row><row><cell>PA batch-1</cell><cell cols="2">0.2680 0.3346 0.0078 0.1678 0.5449 0.0028</cell></row><row><cell cols="3">Top Competitor 0.1748 0.3398 0.0120 0.1752 0.8575 0.0017</cell></row><row><cell>2 PA final</cell><cell cols="2">0.2689 0.3315 0.0141 0.1487 0.7383 0.0008</cell></row><row><cell>PA batch-2</cell><cell cols="2">0.2300 0.3304 0.0185 0.1627 0.3374 0.0047</cell></row><row><cell cols="3">Top Competitor 0.2205 0.3181 0.0165 0.1773 0.6821 0.0015</cell></row><row><cell>3 PA final</cell><cell cols="2">0.3381 0.4303 0.0189 0.1958 0.9422 0.0028</cell></row><row><cell>PA batch-3</cell><cell cols="2">0.2978 0.4351 0.0143 0.1967 0.6558 0.0062</cell></row><row><cell cols="3">Top Competitor 0.1932 0.4510 0.0187 0.2140 1.0039 0.0056</cell></row><row><cell>4 PA final</cell><cell cols="2">0.3239 0.4049 0.0189 0.1753 0.9743 0.0015</cell></row><row><cell>PA batch-4</cell><cell cols="2">0.3177 0.3600 0.0163 0.1810 0.7163 0.0056</cell></row><row><cell cols="3">Top Competitor 0.1967 0.4163 0.0204 0.2151 1.0244 0.0055</cell></row><row><cell>5 PA final</cell><cell cols="2">0.3963 0.4825 0.0254 0.2491 1.1267 0.0038</cell></row><row><cell>PA batch-5 (final)</cell><cell cols="2">0.3963 0.4825 0.0254 0.2491 1.1267 0.0038</cell></row><row><cell cols="3">Top Competitor 0.1978 0.4842 0.0330 0.2652 1.0831 0.0086</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,134.77,232.88,345.83,107.62"><head>Table 5 .</head><label>5</label><figDesc>The performance of the proposed system on the yes/no questions. "Top Competitor" is a top-scoring submission from other teams.</figDesc><table coords="10,150.14,265.14,315.08,75.36"><row><cell>Batch System</cell><cell>Accuracy F1 yes F1 no F1 macro</cell></row><row><cell>3 ALBERT(SQuAD, BioASQ)</cell><cell>0.9032 0.9189 0.8800 0.8995</cell></row><row><cell>Top competitor</cell><cell>0.9032 0.9091 0.8966 0.9028</cell></row><row><cell>4 ALBERT(SQuAD, BioASQ)</cell><cell>0.7308 0.7879 0.6316 0.7097</cell></row><row><cell>Top competitor</cell><cell>0.8462 0.8571 0.8333 0.8452</cell></row><row><cell cols="2">5 BioBERT(SQuAD, BioASQ, PMQ) 0.8235 0.8333 0.8125 0.8229</cell></row><row><cell>Top competitor</cell><cell>0.8529 0.8571 0.8485 0.8528</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,134.77,390.17,345.82,107.62"><head>Table 6 .</head><label>6</label><figDesc>The performance of the proposed system on the list and factoid questions. "Top Competitor" is a top-scoring submission from other teams.</figDesc><table coords="10,152.03,422.42,311.30,75.36"><row><cell>Batch System</cell><cell cols="3">SAcc LAcc MRR Mean Prec. Rec F-Measure</cell></row><row><cell>3 PA</cell><cell>0.2500 0.4643 0.3137</cell><cell>0.5278 0.4778</cell><cell>0.4585</cell></row><row><cell cols="4">Top Competitor 0.3214 0.5357 0.3970 0.7361 0.4833 0.5229</cell></row><row><cell>4 PA</cell><cell>0.4706 0.5588 0.5098</cell><cell>0.3571 0.3661</cell><cell>0.3030</cell></row><row><cell cols="4">Top Competitor 0.5588 0.7353 0.6284 0.5375 0.5089 0.4571</cell></row><row><cell>5 PA</cell><cell>0.4375 0.6250 0.5260</cell><cell>0.3075 0.3214</cell><cell>0.3131</cell></row><row><cell cols="4">Top Competitor 0.5625 0.7188 0.6354 0.5516 0.5972 0.5618</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,134.77,115.91,345.83,152.75"><head>Table 7 .</head><label>7</label><figDesc>The performance of the proposed system on the ideal answers. "Top Competitor" is a top-scoring submission from other teams chosen by R-SU4 (F1).</figDesc><table coords="11,154.38,148.69,306.60,119.97"><row><cell>Batch System</cell><cell cols="4">R-2 (Rec) R-2 (F1) R-SU4 (Rec) R-SU4 (F1)</cell></row><row><cell>1 Baseline</cell><cell>0.1118</cell><cell>0.1118</cell><cell>0.1116</cell><cell>0.1117</cell></row><row><cell>Top competitor</cell><cell>0.6004</cell><cell>0.3660</cell><cell>0.6035</cell><cell>0.3556</cell></row><row><cell>2 Baseline</cell><cell>0.0600</cell><cell>0.0655</cell><cell>0.0615</cell><cell>0.0650</cell></row><row><cell>Top competitor</cell><cell>0.5651</cell><cell>0.3451</cell><cell>0.5725</cell><cell>0.3376</cell></row><row><cell>3 Snippet Reranking</cell><cell cols="2">0.5235 0.3297</cell><cell>0.5303</cell><cell>0.3256</cell></row><row><cell>Top competitor</cell><cell cols="2">0.4980 0.3833</cell><cell>0.5045</cell><cell>0.3811</cell></row><row><cell>4 Snippet Reranking</cell><cell cols="2">0.5470 0.3087</cell><cell>0.5471</cell><cell>0.3001</cell></row><row><cell>Top competitor</cell><cell>0.5281</cell><cell>0.3069</cell><cell>0.5329</cell><cell>0.2987</cell></row><row><cell cols="2">5 Abstractive Generation 0.3696</cell><cell>0.3006</cell><cell>0.3688</cell><cell>0.2895</cell></row><row><cell>Top competitor</cell><cell cols="2">0.3867 0.3668</cell><cell>0.3805</cell><cell>0.3548</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix: Side-by-Side Comparison of Snippet Ranking and Generative Models. Can prevnar 13 be used in children? Gold Yes, PCV13 is approved for routine vaccination of all infants as a 4-dose series at age 2, 4, 6, and 12-15 months for children who previously received 1 or more doses of the 7-valent pneumococcal conjugate vaccine (PCV7), and for children with underlying medical conditions that increase their risk for pneumococcal disease or its complications. Snippet Ranking PCV13 is approved for routine vaccination of all infants as a 4-dose series at age 2, 4, 6, and 12-15 months for children who previously received 1 or more doses of the 7-valent pneumococcal conjugate vaccine (PCV7), and for children with underlying medical conditions that increase their risk for pneumococcal disease or its complications. Generative PCV13 is approved for routine vaccination of all infants. PCV 13 is a revision of pneumococcal conjugate vaccine that should be included on pharmacy formularies.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,160.23,142.90,320.37,8.74;13,160.23,154.86,208.02,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,277.02,142.90,203.58,8.74;13,160.23,154.86,13.78,8.74">Gene Ontology: tool for the unification of biology</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Ashburner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,201.74,154.86,70.49,8.74">Nature Genetics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,160.23,166.81,320.37,8.74;13,160.23,178.77,320.37,8.74;13,160.23,190.72,115.10,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,257.00,166.81,223.59,8.74;13,160.23,178.77,141.85,8.74">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,328.79,178.77,95.10,8.74">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,160.23,202.68,320.37,8.74;13,160.23,214.64,320.36,8.74;13,160.23,226.59,320.36,8.74;13,160.23,238.55,320.37,8.74;13,160.23,250.50,320.36,9.30;13,160.23,263.17,97.41,8.30" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,262.91,202.68,217.68,8.74;13,160.23,214.64,27.35,8.74">AUEB at BioASQ 6: Document and Snippet Retrieval</title>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Brokos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5304</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-5304" />
	</analytic>
	<monogr>
		<title level="m" coord="13,217.60,214.64,262.99,8.74;13,160.23,226.59,274.80,8.74">Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</title>
		<meeting>the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">Nov. 2018</date>
			<biblScope unit="page" from="30" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,160.23,274.41,320.36,8.74;13,160.23,286.37,320.37,8.74;13,160.23,298.32,136.19,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,300.93,274.41,155.48,8.74">PubMed: the bibliographic database</title>
		<author>
			<persName coords=""><forename type="first">Kathi</forename><surname>Canese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarah</forename><surname>Weis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,160.23,286.37,90.63,8.74">The NCBI Handbook</title>
		<imprint>
			<publisher>National Center for Biotechnology Information (US)</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Internet]. 2nd edition</note>
</biblStruct>

<biblStruct coords="13,160.23,310.28,320.37,8.74;13,160.23,322.23,320.37,8.74;13,160.23,334.19,320.37,9.30;13,160.23,346.86,320.37,8.58" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,251.00,310.28,229.59,8.74;13,160.23,322.23,20.60,8.74">Reading Wikipedia to Answer Open-Domain Questions</title>
		<author>
			<persName coords=""><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p17-1171</idno>
		<ptr target="http://dx.doi.org/10.18653/v1/P17-1171" />
	</analytic>
	<monogr>
		<title level="m" coord="13,211.59,322.23,269.01,8.74;13,160.23,334.19,130.75,8.74">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="13,160.23,358.10,320.37,8.74;13,160.23,370.05,320.37,9.02;13,160.23,382.72,188.22,8.58" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="13,249.89,358.10,230.70,8.74;13,160.23,370.05,141.83,8.74">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018-10">Oct. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,160.23,393.96,320.37,8.74;13,160.23,405.92,269.05,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,353.69,393.96,126.90,8.74;13,160.23,405.92,98.48,8.74">Structured Summarization of Academic Publications</title>
		<author>
			<persName coords=""><forename type="first">Alexios</forename><surname>Gidiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,286.06,405.92,112.20,8.74">PKDD/ECML Workshops</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,160.23,417.87,320.37,8.74;13,160.23,429.83,320.36,8.74;13,160.23,441.78,215.16,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,252.00,417.87,228.59,8.74;13,160.23,429.83,27.35,8.74">A Deep Relevance Matching Model for Ad-hoc Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,213.78,429.83,266.81,8.74;13,160.23,441.78,180.45,8.74">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,160.23,453.74,320.36,8.74;13,160.23,465.69,187.48,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,277.14,453.74,203.44,8.74;13,160.23,465.69,110.43,8.74">Don&apos;t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>
		<author>
			<persName coords=""><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,298.40,465.69,17.47,8.74">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,160.23,477.65,320.36,8.74;13,160.23,489.60,320.37,8.74;13,160.23,501.56,76.03,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,398.83,477.65,81.76,8.74;13,160.23,489.60,225.99,8.74">Measuring Domain Portability and ErrorPropagation in Biomedical QA</title>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,416.97,489.60,63.62,8.74;13,160.23,501.56,45.02,8.74">PKDD/ECML Workshops</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,160.23,513.51,320.36,8.74;13,160.23,525.47,320.36,8.74;13,160.23,537.42,320.36,8.74;13,160.23,549.38,320.37,9.30;13,160.23,562.05,319.87,8.58;13,160.23,574.01,23.69,8.30" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,230.59,513.51,250.00,8.74;13,160.23,525.47,43.54,8.74">PubMedQA: A Dataset for Biomedical Research Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Jin</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1259</idno>
		<ptr target="http://dx.doi.org/10.18653/v1/D19-1259" />
	</analytic>
	<monogr>
		<title level="m" coord="13,233.49,525.47,247.10,8.74;13,160.23,537.42,320.36,8.74;13,160.23,549.38,263.58,8.74">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,160.23,585.25,327.84,8.74;13,160.23,597.20,320.37,8.74;13,160.23,609.16,64.35,8.74" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="13,242.70,585.25,245.37,8.74;13,160.23,597.20,293.62,8.74">A Multi-strategy Query Processing Approach for Biomedical Question Answering: USTB PRIR at BioASQ 2017 Task 5B</title>
		<author>
			<persName coords=""><forename type="first">Zan-Xia</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>BioNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,160.23,621.11,320.37,8.74;13,160.23,633.07,320.36,8.74;13,160.23,645.02,223.93,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,274.20,621.11,206.39,8.74;13,160.23,633.07,187.46,8.74">BioASQ at CLEF2020: Large-Scale Biomedical Semantic Indexing and Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,375.20,633.07,105.39,8.74;13,160.23,645.02,92.98,8.74">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="550" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,160.23,118.99,320.37,8.74;14,160.23,130.95,320.37,9.02;14,160.23,143.62,188.22,8.58" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="14,261.57,118.99,219.02,8.74;14,160.23,130.95,142.96,8.74">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName coords=""><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<ptr target="http://arxiv.org/abs/1909.11942" />
		<imprint>
			<date type="published" when="2019-09">Sept. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,160.23,154.86,320.36,8.74;14,160.23,166.81,320.36,8.74;14,160.23,178.77,131.57,8.74" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="14,247.90,154.86,232.69,8.74;14,160.23,166.81,294.61,8.74">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno>ArXiv abs/1910.13461</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,160.23,190.72,320.36,8.74;14,160.23,202.68,237.06,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,230.07,190.72,240.74,8.74">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,175.45,202.68,143.54,8.74">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,160.23,214.64,320.36,8.74;14,160.23,226.59,160.36,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,299.58,214.64,181.01,8.74;14,160.23,226.59,26.42,8.74">Text Summarization with Pretrained Encoders</title>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,214.00,226.59,74.38,8.74">EMNLP/IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,160.23,238.55,320.36,8.74;14,160.23,250.50,320.37,8.74;14,160.23,262.46,320.37,8.74;14,160.23,274.41,320.37,9.30;14,160.23,287.08,128.79,8.30" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,432.89,238.55,47.70,8.74;14,160.23,250.50,273.09,8.74">Deep Relevance Ranking Using Enhanced Document-Query Interactions</title>
		<author>
			<persName coords=""><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Brokos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1211</idno>
		<ptr target="http://dx.doi.org/10.18653/v1/D18-1211" />
	</analytic>
	<monogr>
		<title level="m" coord="14,461.49,250.50,19.11,8.74;14,160.23,262.46,320.37,8.74;14,160.23,274.41,45.08,8.74">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,160.23,298.32,320.37,8.74;14,160.23,310.28,320.36,8.74;14,160.23,322.23,320.37,8.74;14,160.23,334.19,319.87,9.30;14,160.23,346.86,295.68,8.58" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,334.06,298.32,146.53,8.74;14,160.23,310.28,320.36,8.74;14,160.23,322.23,43.54,8.74">Classification Betters Regression in Query-Based Multi-document Summarisation Techniques for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-43887-6_56</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-030-43887-6_56" />
	</analytic>
	<monogr>
		<title level="j" coord="14,233.95,322.23,246.64,8.74">Communications in Computer and Information Science</title>
		<idno type="ISSN">1865-0937</idno>
		<imprint>
			<biblScope unit="page" from="624" to="635" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,160.23,358.10,320.36,8.74;14,160.23,370.05,320.37,8.74;14,160.23,382.01,320.36,8.74;14,160.23,393.96,320.36,9.30;14,160.23,406.63,319.87,8.58;14,160.23,418.59,23.69,8.30" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,261.48,358.10,219.11,8.74;14,160.23,370.05,125.65,8.74">ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5034</idno>
		<idno type="arXiv">arXiv:1902.07669</idno>
		<ptr target="https://www.aclweb.org/anthology/W19-5034" />
	</analytic>
	<monogr>
		<title level="m" coord="14,312.48,370.05,168.11,8.74;14,160.23,382.01,91.23,8.74">Proceedings of the 18th BioNLP Workshop and Shared Task</title>
		<meeting>the 18th BioNLP Workshop and Shared Task<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08">Aug. 2019</date>
			<biblScope unit="page" from="319" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,160.23,429.83,320.36,8.74;14,160.23,441.78,320.36,8.74;14,160.23,454.46,95.00,8.30" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="14,337.28,429.83,131.55,8.74">Passage Re-ranking with BERT</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<idno>arXiv: 1901.04085 [cs.IR</idno>
		<imprint>
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct coords="14,160.23,465.69,320.36,8.74;14,160.23,477.65,320.37,8.74" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="14,267.38,465.69,213.20,8.74;14,160.23,477.65,27.35,8.74">AUEB at BioASQ 7: Document and Snippet Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Dimitris</forename><surname>Pappas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,217.11,477.65,258.91,8.74">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,160.23,489.60,320.37,8.74;14,160.23,501.56,242.71,9.30" xml:id="b22">
	<monogr>
		<title level="m" coord="14,350.25,489.60,23.02,8.74">Cham</title>
		<editor>
			<persName><forename type="first">Peggy</forename><surname>Cellier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kurt</forename><surname>Driessens</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="607" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,160.23,513.51,320.36,8.74;14,160.23,525.47,320.37,8.74;14,160.23,537.42,320.36,8.74;14,160.23,549.38,320.37,8.74;14,160.23,561.34,127.39,9.30" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,294.55,513.51,186.04,8.74;14,160.23,525.47,253.49,8.74">A Mixed Information Source Approach for Biomedical Question Answering: MindLab at BioASQ 7B</title>
		<author>
			<persName coords=""><forename type="first">Mónica</forename><surname>Pineda-Vargas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,443.65,525.47,36.94,8.74;14,160.23,537.42,206.55,8.74">Machine Learning and Knowledge Discovery in Databases</title>
		<editor>
			<persName><forename type="first">Peggy</forename><surname>Cellier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kurt</forename><surname>Driessens</surname></persName>
		</editor>
		<editor>
			<persName><surname>Cham</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="595" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,160.23,573.29,320.37,8.74;14,160.23,585.25,349.96,8.74;14,160.23,597.20,264.01,9.02" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="14,379.21,573.29,101.38,8.74;14,160.23,585.25,181.63,8.74">Know What You Don&apos;t Know: Unanswerable Questions for SQuAD</title>
		<author>
			<persName coords=""><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<idno>arXiv: 1806.03822</idno>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
	<note>cs.CL</note>
</biblStruct>

<biblStruct coords="14,160.23,609.16,320.36,8.74;14,160.23,621.11,320.37,9.02;14,160.23,633.78,230.92,8.58" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="14,273.46,609.16,207.13,8.74;14,160.23,621.11,78.28,8.74">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName coords=""><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<idno>arXiv: 1606.05250</idno>
		<ptr target="http://arxiv.org/abs/1606.05250" />
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,160.23,118.99,320.36,8.74;15,160.23,130.95,320.36,8.74;15,160.23,142.90,320.37,8.74;15,160.23,154.86,320.37,9.30;15,160.23,167.53,202.02,8.30" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="15,426.31,118.99,54.28,8.74;15,160.23,130.95,242.41,8.74">Get To The Point: Summarization with Pointer-Generator Networks</title>
		<author>
			<persName coords=""><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p17-1099</idno>
		<ptr target="http://dx.doi.org/10.18653/v1/P17-1099" />
	</analytic>
	<monogr>
		<title level="m" coord="15,430.93,130.95,49.65,8.74;15,160.23,142.90,320.37,8.74;15,160.23,154.86,15.02,8.74">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="15,160.23,178.77,320.36,8.74;15,160.23,190.72,320.37,8.74;15,160.23,202.68,319.87,9.30;15,160.23,215.35,8.00,8.30" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="15,275.52,178.77,205.07,8.74;15,160.23,190.72,266.02,8.74">An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-015-0564-6</idno>
	</analytic>
	<monogr>
		<title level="j" coord="15,457.52,190.72,23.08,8.74;15,160.23,202.68,64.43,8.74">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,160.23,226.59,326.09,8.74;15,160.23,238.55,30.44,8.74" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="15,257.50,226.59,105.89,8.74">Attention is All you Need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno>ArXiv abs/1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,160.23,250.50,320.37,8.74;15,160.23,262.46,179.23,8.74" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="15,253.32,250.50,164.81,8.74">The TREC question answering track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,448.11,250.50,32.49,8.74;15,160.23,262.46,95.88,8.74">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">361</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,160.23,274.41,320.37,8.74;15,160.23,286.37,320.37,9.30;15,160.23,299.04,273.76,8.58" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="15,408.12,274.41,72.47,8.74;15,160.23,286.37,109.69,8.74">Neural Question Answering at BioASQ 5B</title>
		<author>
			<persName coords=""><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-2309</idno>
		<ptr target="http://dx.doi.org/10.18653/v1/W17-2309" />
	</analytic>
	<monogr>
		<title level="m" coord="15,298.31,286.37,59.49,8.74">BioNLP 2017</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,160.23,310.28,320.37,8.74;15,160.23,322.23,56.04,8.74" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="15,229.46,310.28,202.97,8.74">The Porter stemming algorithm: then and now</title>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,461.49,310.28,19.11,8.74;15,160.23,322.23,21.52,8.74">Program</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,160.23,334.19,320.36,8.74;15,160.23,346.14,320.36,8.74;15,160.23,358.10,320.36,9.30;15,160.23,370.77,81.22,8.30" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="15,340.35,334.19,140.24,8.74;15,160.23,346.14,100.14,8.74">Anserini: Reproducible Ranking Baselines Using Lucene</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3239571</idno>
		<ptr target="https://doi.org/10.1145/3239571" />
	</analytic>
	<monogr>
		<title level="j" coord="15,288.73,346.14,143.70,8.74">J. Data and Information Quality</title>
		<idno type="ISSN">1936-1955</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018-10">Oct. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,160.23,382.01,320.37,8.74;15,160.23,393.96,227.10,8.74" xml:id="b33">
	<monogr>
		<title level="m" type="main" coord="15,237.26,382.01,243.33,8.74;15,160.23,393.96,23.31,8.74">End-to-end open-domain question answering with bertserini</title>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01718</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,160.23,405.92,320.37,8.74;15,160.23,417.87,320.36,8.74;15,160.23,430.55,286.54,8.58" xml:id="b34">
	<monogr>
		<title level="m" type="main" coord="15,248.79,405.92,231.81,8.74;15,160.23,417.87,43.54,8.74">Pre-trained Language Model for Biomedical Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08229</idno>
		<idno>arXiv: 1909.08229</idno>
		<ptr target="http://arxiv.org/abs/1909.08229" />
		<imprint>
			<date type="published" when="2019-09">Sept. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
