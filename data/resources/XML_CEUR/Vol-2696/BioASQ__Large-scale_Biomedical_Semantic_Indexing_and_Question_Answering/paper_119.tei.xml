<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.81,115.96,335.74,12.62;1,234.59,133.89,139.95,12.62">Query Focused Multi-document Summarisation of Biomedical Texts</title>
				<funder ref="#_eUKfHcj">
					<orgName type="full">CSIRO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,285.01,203.71,78.77,8.74"><forename type="first">Christopher</forename><surname>Jones</surname></persName>
							<email>christopher.jones4@hdr.mq.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">Macquarie University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.82,215.66,70.02,8.74"><forename type="first">Vincent</forename><surname>Nguyen</surname></persName>
							<email>vincent.nguyen@anu.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Macquarie University</orgName>
								<orgName type="institution" key="instit2">Australian National University at BioASQ8b Diego Moll√°</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.81,115.96,335.74,12.62;1,234.59,133.89,139.95,12.62">Query Focused Multi-document Summarisation of Biomedical Texts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">113BB196CA81BFD97A2CA098A96A2F80</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the participation of Macquarie University and the Australian National University for Task B Phase B of the 2020 BioASQ Challenge (BioASQ8b). Our overall framework implements Query focused multi-document extractive summarisation by applying either a classification or a regression layer to the candidate sentence embeddings and to the comparison between the question and sentence embeddings. We experiment with variants using BERT and BioBERT, Siamese architectures, and reinforcement learning. We observe the best results when BERT is used to obtain the word embeddings, followed by an LSTM layer to obtain sentence embeddings. Variants using Siamese architectures or BioBERT did not improve the results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Query focused multi-document summarisation aims to generate the answer to a question by combining information from multiple documents <ref type="bibr" coords="1,418.51,498.28,9.96,8.74" target="#b0">[1]</ref>. This task, therefore, is related to both question answering and text summarisation. There is substantial research in both question answering and text summarisation. In the case of text summarisation, most research focuses on single-document summarisation, and there is also substantial research on multi-document summarisation. However, there is relatively little research on query focused multi-document text summarisation. There are multiple applications where query focused multidocument text summarisation can be useful. A clear example of a useful application is in the domain of biomedicine and clinical medicine, where a doctor or a patient wants to obtain a concise summary of the most relevant evidence related to a particular diagnosis or intervention.</p><p>The BioASQ Challenge<ref type="foot" coords="2,250.36,141.33,3.97,6.12" target="#foot_0">3</ref> organises shared tasks centered on biomedical texts. The focus of this paper is on the 2020 participation of Macquarie University and the Australian National University in Task B Phase B (BioASQ8b), where the aim is to find the "ideal answer" to a question, given a collection of relevant PubMed abstracts. <ref type="foot" coords="2,218.93,189.15,3.97,6.12" target="#foot_1">4</ref> We approach this task as an instance of query focused multi-document extractive summarisation by scoring each candidate sentence and selecting the top-scoring ones to produce the final summary. Macquarie University and the Australian National University submitted independent runs to BioASQ8b, but we both used the starting code of Macquarie University's BioASQ7b participation <ref type="bibr" coords="2,243.62,250.50,14.61,8.74" target="#b9">[10]</ref>. Novel contributions of this paper, compared with previous participation at BioASQ, include:</p><p>1. The incorporation of BERT and BioBERT in the general architecture of <ref type="bibr" coords="2,462.33,282.73,14.61,8.74" target="#b9">[10]</ref>. 2. The use of Siamese architectures in two main setups: 1) sharing weights in the architecture of <ref type="bibr" coords="2,235.33,306.37,14.61,8.74" target="#b9">[10]</ref>, and 2) using Sentence-BERT <ref type="bibr" coords="2,386.63,306.37,14.61,8.74" target="#b10">[11]</ref>. 3. The use of Proximal Policy Optimisation (PPO) and BERT in a Reinforcement Learning approach.</p><p>This paper is structured as follows. Section 2 describes the general framework and baselines based on previous participation at BioASQ. Section 3 explains the incorporation of BERT and BioBERT. Section 4 introduces the incorporation of Siamese architectures. Section 5 describes the use of Reinforcement Learning. Section 6 presents and discusses the results of cross-validation evaluations using the BioASQ8b training data. Section 7 presents and discusses the runs submitted to BioASQ. Finally, Section 8 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">General Framework and Baselines</head><p>The overall architecture of our systems is based on that of Macquarie University's participation to BioASQ7b <ref type="bibr" coords="2,283.40,487.85,14.61,8.74" target="#b9">[10]</ref>. This architecture is described in Fig. <ref type="figure" coords="2,472.84,487.85,3.87,8.74" target="#fig_0">1</ref>. The baseline systems re-use the following options from <ref type="bibr" coords="2,376.50,499.80,15.50,8.74" target="#b9">[10]</ref> to generate the embeddings of the words and sentences.</p><p>-The embedding generator to obtain word embeddings is a matrix of pre-trained embeddings generated by word2vec. We trained word2vec using PubMed documents provided by the organisers of BioASQ. The embeddings had a vector size of 100. -The embedding reductor to obtain sentence embeddings is a pair of forward and backward LSTM chains. The weights of the embedding reductor for the candidate sentence and the question were not shared. -The activation function of the final layer is a linear function for the regression setup and a sigmoid function for the classification setup. -The similarity between the embeddings of the candidate sentence and the question is the element-wise product.</p><p>As in <ref type="bibr" coords="3,177.03,423.17,14.61,8.74" target="#b9">[10]</ref>, we tried a classification setup and a regression setup. In the regression setup ("NNR" in Fig. <ref type="figure" coords="3,276.59,435.12,4.98,8.74" target="#fig_0">1</ref> Table <ref type="table" coords="3,314.70,435.12,3.87,8.74" target="#tab_2">3</ref>), the training data is labelled with the ROUGE-SU4 F1 value of the candidate sentence and the objective function to optimise is the Mean Squared Error. In the classification setup ("NNC" in Fig. <ref type="figure" coords="3,154.84,470.99,4.98,8.74" target="#fig_0">1</ref> Table <ref type="table" coords="3,190.02,470.99,3.87,8.74" target="#tab_2">3</ref>), the 5 candidate sentences with the highest ROUGE-SU4 F1 are labelled as 1, and the rest are labelled as 0. The objective function to optimise in the classification setup is binary cross-entropy.</p><p>In both the classification and the regression setup, the summary is produced by scoring each candidate sentence and extracting the top n sentences to generate the summary, where n depended on the question type and was the same as reported in <ref type="bibr" coords="3,186.57,547.64,15.50,8.74" target="#b9">[10]</ref> (Table <ref type="table" coords="3,236.66,547.64,3.87,8.74" target="#tab_0">1</ref>). 3 Experiments with BERT and BioBERT BERT <ref type="bibr" coords="4,165.52,144.00,10.52,8.74" target="#b1">[2]</ref> has been used in a wide range of NLP tasks, including text classification <ref type="bibr" coords="4,154.43,155.96,9.96,8.74" target="#b8">[9]</ref>, and extractive <ref type="bibr" coords="4,234.21,155.96,10.51,8.74" target="#b7">[8]</ref> and abstractive <ref type="bibr" coords="4,315.98,155.96,10.51,8.74" target="#b6">[7]</ref> summarisation. We have integrated BERT into our general architecture of Fig. <ref type="figure" coords="4,324.72,167.91,4.98,8.74" target="#fig_0">1</ref> as described below. In a first experiment ("BERT untrained" in Table <ref type="table" coords="4,365.85,180.09,3.87,8.74" target="#tab_2">3</ref>), we replaced the embedding generator with BERT using the pre-trained model provided by Huggingface. <ref type="foot" coords="4,154.41,202.42,3.97,6.12" target="#foot_2">5</ref> The resulting word embeddings are now affected by context. The BERT weights were not updated during the training stage. Following the recommendation of the Huggingface library, the embedding reductor of each candidate sentence and the question are the average of the word embeddings.</p><p>In a subsequent experiment ("BERT trained" in Table <ref type="table" coords="4,399.42,252.04,3.87,8.74" target="#tab_2">3</ref>), the embedding generator and reductors are as in BERT untrained, but we allowed the BERT weights to be fine-tuned during the training process.</p><p>We also tried a variant ("BERT LSTM" in Table <ref type="table" coords="4,380.80,288.12,4.43,8.74" target="#tab_2">3</ref>) that uses BERT in the embedding generator as in BERT untrained, but the embedding reductor is a bidirectional LSTM chain as in the NNC baseline. This variant is therefore comparable with the NNC baseline, and the only difference being the use of BERT for the embedding generator.</p><p>In a final series of experiments ("BioBERT untrained" and "BioBERT LSTM" in Table <ref type="table" coords="4,174.39,360.07,3.87,8.74" target="#tab_2">3</ref>), the embedding generator and reductors are as in BERT untrained and BERT LSTM, but the pre-trained model was as provided by the developers of BioBERT<ref type="foot" coords="4,188.45,382.41,3.97,6.12" target="#foot_3">6</ref>  <ref type="bibr" coords="4,196.24,383.98,9.96,8.74" target="#b5">[6]</ref>, who used biomedical documents to pre-train BERT.</p><p>In all of the experiments in this section, the final layer was a classification layer with a sigmoid activation and the objective function to optimise was binary cross-entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments with Siamese Networks</head><p>Siamese networks have been used in applications that include a comparison between documents, for example to determine semantic similarity <ref type="bibr" coords="4,408.85,490.01,14.61,8.74" target="#b10">[11]</ref>. The general idea is to use the same processing module for each of the two documents by sharing the weights of the encoding component that generates the embeddings of the documents. We have used this idea in two main kinds of experiments described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Siamese LSTM</head><p>A straightforward implementation of Siamese Networks ("Siamese LSTM" in Table <ref type="table" coords="4,162.79,601.80,4.43,8.74" target="#tab_2">3</ref>) using the overall architecture of Fig. <ref type="figure" coords="4,341.28,601.80,4.98,8.74" target="#fig_0">1</ref> shares the weights of the embedding reductors of the candidate sentence and the question. This ensures that the sentence embeddings are generated using exactly the same process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentence-BERT</head><p>The second implementation of Siamese Networks uses Sentence-BERT (SBERT) <ref type="bibr" coords="5,478.77,143.46,15.50,8.74" target="#b10">[11]</ref> to determine whether a candidate sentence is similar to the question. In particular, the system uses BERT <ref type="bibr" coords="5,260.23,167.37,10.52,8.74" target="#b1">[2]</ref> in a Siamese setup as described in Fig. <ref type="figure" coords="5,446.59,167.37,3.87,8.74" target="#fig_1">2</ref>. Depending on the settings, the system uses regression, classification or both (multi-task), for prediction.</p><p>-In the regression setup, the cosine similarity between candidate sentence and question embeddings is computed, and the objective function to optimise is the Mean Squared Error (MSE). -In the classification setup, the candidate sentence and question embeddings are concatenated with the element-wise absolute difference between the sentence and question, and a softmax layer is added. The objective function to optimise is binary cross-entropy. -In the multi-task setup, the classification and regression setups are jointly optimised during training. At prediction time, we use either the classification head or the regression head.</p><p>In both the regression and the classification setups, the training data was labelled with the classification labels mentioned in Section 2.</p><p>We also experimented with a BERT and a BioBERT variation<ref type="foot" coords="5,417.00,597.50,3.97,6.12" target="#foot_4">7</ref> of a Sentence Transformer <ref type="bibr" coords="5,192.33,611.03,15.50,8.74" target="#b10">[11]</ref> to ensure that the sentence embeddings reflect a biomedical word-space.</p><p>Table <ref type="table" coords="5,177.11,635.85,4.98,8.74" target="#tab_1">2</ref> shows all the combinations we tried. </p><formula xml:id="formula_0" coords="6,137.28,166.82,319.31,89.58">SBERT R BERT Y Y SBERT C BERT Y Y SBERT M R BERT Y Y Y SBERT M C BERT Y Y Y SBioBERT R BioBERT Y Y SBioBERT C BioBERT Y Y SBioBERT M R BioBERT Y Y Y SBioBERT M C BioBERT Y Y Y</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments with Reinforcement Learning</head><p>Reinforcement learning allows the training process to optimise the target evaluation metric (ROUGE-SU4 F1) directly. Whereas <ref type="bibr" coords="6,361.06,324.84,15.50,8.74" target="#b9">[10]</ref> used the REINFORCE algorithm <ref type="bibr" coords="6,181.42,336.80,14.61,8.74" target="#b13">[13]</ref>, in our participation to BioASQ8b we used the Proximal Policy Optimisation (PPO) approach. We choose PPO for our summarisation task because past research shows that it penalises changes to the policy <ref type="bibr" coords="6,426.59,360.71,14.61,8.74" target="#b12">[12]</ref>, and because we observe a more consistent learning curve using this approach compared to REINFORCE <ref type="bibr" coords="6,210.45,384.62,9.96,8.74" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Approach</head><p>As in past submissions, our reinforcement learning system classifies sentences to be either (0) not included in the summary or (1) included in the summary based on a policy, using the ROUGE-SU4 F1 score directly as the reward. We use the stable baselines library<ref type="foot" coords="6,253.60,471.14,3.97,6.12" target="#foot_5">8</ref>  <ref type="bibr" coords="6,262.10,472.71,10.52,8.74" target="#b2">[3]</ref> to implement the PPO reinforcement learning approach, and apply this to our BioASQ summarisation task environment. We perform some hyperparameter tuning, while leaving the PPO code unmodified as much as possible. We use a horizon (n steps) of 1000 with 4 minibatches each, and run for a total of 500,000 timesteps. When training our model, we choose any action from the probability distribution of the policy function, but when testing we choose the action out of 100 samples.</p><p>The neural network architecture for PPO is shown in Figure <ref type="figure" coords="6,423.06,556.88,3.87,8.74" target="#fig_2">3</ref>. As in <ref type="bibr" coords="6,462.33,556.88,14.61,8.74" target="#b9">[10]</ref>, the inputs of the neural network consist of:</p><p>1. Candidate sentence 2. Question 3. Summary generated so far 4. Sentences after the candidate sentence 5. Entire document 6. Length of summary generated so far For each of the first five inputs, we take the mean of the word2vec word embeddings (each of size 100) to generate the sentence embeddings. The sentence embeddings and the length of the summary are concatenated to form a single layer of size 501. We then feed the combined layer into a simple Multi-Layer Perceptron with two hidden layers each of size 200. The outputs of this neural network become the value function and stochastic policy function for our PPO approach, each of size 2 with linear activations, representing the predicted future rewards and action probabilities respectively for the two actions (classify 0 or 1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">With BERT</head><p>We also apply pre-trained BERT embeddings to our reinforcement learning approach for batches 4 and 5 of BioASQ8b. Our network architecture is the same as in Figure <ref type="figure" coords="7,177.74,608.30,3.87,8.74" target="#fig_2">3</ref>, but we change the embedding generator to generate BERT embeddings instead of word2vec embeddings. We use the PyTorch BERT embedding generator provided by Huggingface 5 and the TensorFlow Multi-Layer Perceptron policy provided by stable baselines 8 .We observe a minor improvement in ROUGE-SU4 F1 score as shown in Section 7 (PPO BERT).</p><p>6 Results and Discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Cross-Validation Results</head><p>All experiments except those described in Sections 5 were evaluated using 10-fold cross-validation using the training data provided by the organisers of BioASQ8b. These results are shown in Table <ref type="table" coords="8,279.59,186.23,3.87,8.74" target="#tab_2">3</ref>. As described in Section 6.2, cross-validation evaluation was not practical for the experiments with reinforcement learning. The baseline runs "firstn", "NNR" and "NNC" confirm <ref type="bibr" coords="8,404.33,512.66,14.80,8.74" target="#b9">[10]</ref>'s observation that the classification setup produces better results than the regression setup. The untrained BERT system did not better the classification system, and the trained BERT system produced worse results.</p><p>We observed no changes in the evaluation results of the BERT trained system as we changed the number of epochs from 1 to 20 epochs. A detailed look at the changes in the loss during training revealed a very small improvement of the loss as we increase the number of epochs, but not enough to reflect a difference in the final ROUGE-SU4 F1 metric. This suggests that a more elaborate finetuning regime with gradual unfreezing as described by <ref type="bibr" coords="8,368.32,620.25,10.52,8.74" target="#b3">[4]</ref> might lead to improved results.</p><p>Given the poor results of the trained BERT system, we kept BERT frozen when we tested the variants using LSTM and using BioBERT.</p><p>BERT and BioBERT followed by an LSTM-based sentence reductor did improve results over the version with a mean of embeddings. Unfortunately, the experiments using the LSTM-based reductor were made after the deadlines for submission of results to BioASQ8b.</p><p>It was surprising to observe that the BioBERT variants performed worse than the BERT variants. This is not in line with the improvement in the performance of BioBERT for the "exact answers" section of BioASQ7b reported in literature <ref type="bibr" coords="9,178.54,203.94,14.61,8.74" target="#b14">[14]</ref>.</p><p>The Siamese LSTM variant did not improve over the classification system. The reason for this might be, as mentioned in Section 7, that questions and candidate sentences are different in nature. We also conducted cross-validation experiments with the SBERT variants but they are not included in Table <ref type="table" coords="9,475.61,253.01,4.98,8.74" target="#tab_2">3</ref> as the results are very different from those of the runs submitted to BioASQ (Section 6) and we suspect that there might have been an error when running these evaluations.</p><p>Table <ref type="table" coords="9,177.11,302.09,4.98,8.74">4</ref> shows and explains the hyperparameters of all systems of Table <ref type="table" coords="9,466.22,302.09,3.87,8.74">4</ref>.</p><p>Table <ref type="table" coords="9,165.24,336.09,4.13,7.89">4</ref>. Hyperparameters for the experiments of Table <ref type="table" coords="9,376.04,336.12,3.58,7.86" target="#tab_2">3</ref>. The choice of dropout and epochs is the result of grid search. A dropout of 0 means that no dropout was applied. The choice of batch size is determined by the GPU capabilities. In all cases, the dimensions of the word and sentence embeddings is 100. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Reinforcement Learning Results</head><p>The evaluation setup for reinforcement learning did not use cross-validation due to the long time it took to train the system (several days). We therefore used a partition of the training data for the train and test sets.</p><p>Table <ref type="table" coords="10,178.05,118.99,4.98,8.74">5</ref> shows the evaluation results of the PPO <ref type="bibr" coords="10,372.35,118.99,15.50,8.74" target="#b12">[12]</ref> reinforcement learning approaches discussed in Section 5, compared with the REINFORCE <ref type="bibr" coords="10,465.10,130.95,15.50,8.74" target="#b13">[13]</ref> approach, using the BioASQ7b and BioASQ8b training datasets.</p><p>Table <ref type="table" coords="10,168.09,173.81,4.13,7.89">5</ref>. Reinforcement Learning Preliminary Results. The results using the BioASQ7b dataset show the mean and standard deviation of 3 runs. The results using the BioASQ8b dataset show the result of a single run. The PPO BERT system was only run with the BioASQ8b dataset and was only submitted to batches <ref type="bibr" coords="10,438.02,206.72,4.61,7.86" target="#b3">4</ref>  The BioASQ7b data set consists of 2,747 questions which we divide into a training set (2,289 questions) and a testing set (458 questions) partitioned 5:1 using a random seed to shuffle the data. The PPO word2vec and REINFORCE word2vec systems were run 3 times each for 500,000 timesteps on the BioASQ7b data set, and the maximum ROUGE-SU4 F1 score reached in each learning curve was averaged across the 3 runs.</p><p>The BioASQ8b data set consists of 3,243 questions which we divide into a training set (2,702 questions) and a testing set (541 questions) partitioned 5:1 using a random seed to shuffle the data. The PPO word2vec, PPO BERT, and REINFORCE word2vec systems were each run once only for 500,000 timesteps on the BioASQ8b data set, and the maximum ROUGE-SU4 F1 score for each run is shown in Table <ref type="table" coords="10,211.64,459.21,3.87,8.74">5</ref>. The models which produced the maximum ROUGE-SU4 F1 score for each system were saved and re-used to generate our PPO submissions to the BioASQ competition in 2020, except for the REINFORCE system which was not included in this year's submission.</p><p>Whereas our evaluation results did not show any clear difference between REINFORCE and PPO on word2vec embeddings, the version using PPO and BERT features showed an improvement.</p><p>The reinforcement learning experiments of Table <ref type="table" coords="10,365.27,542.89,3.87,8.74">5</ref>, however, do not outperform the results of the experiments of Table <ref type="table" coords="10,330.11,554.85,3.87,8.74" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Submissions to BioASQ8b</head><p>Table <ref type="table" coords="10,162.41,608.30,4.98,8.74" target="#tab_5">6</ref> shows the results of Macquarie University's submissions to BioASQ8b. We only report ROUGE-SU4 F1 for comparison with our experiments. Also, as observed by <ref type="bibr" coords="10,191.30,632.21,14.61,8.74" target="#b9">[10]</ref>, F1 scores have a higher correlation with human judgements than recall scores, and similar correlation with human judgements as precision scores. All runs in Table <ref type="table" coords="12,227.49,118.99,4.98,8.74" target="#tab_5">6</ref> have been described except for First n b in batch 2. This is the same as First n in all other batches but the data has been pre-processed differently.</p><p>The classifier system (NNC) produces the best results in most runs, and this is consistent with our cross-validation results (Table <ref type="table" coords="12,357.55,173.44,3.87,8.74" target="#tab_2">3</ref>). However, in contrast with Table <ref type="table" coords="12,161.90,185.39,3.87,8.74" target="#tab_2">3</ref>, in most runs the second best is the regression system (NNR) instead of the untrained BERT system. All systems were re-trained for the BioASQ runs, keeping the same hyperparameters, so that we could use the entire training data, and it is possible that bad luck played a part here, and the BERT system was not trained to its best.</p><p>The reinforcement learning experiments show the lowest evaluation results. We should note, however, that the RL runs submitted to BioASQ7b also had lower results than the other runs, but the human evaluation results ranked them higher than our other runs. Also, the reinforcement learning submissions were not re-trained using the entire training data, which may be worth exploring because PPO had a more consistent learning curve for us than REINFORCE. We are waiting for the human evaluation results of BioASQ8b with anticipation.</p><p>Table <ref type="table" coords="12,176.47,342.09,4.98,8.74" target="#tab_6">7</ref> shows the results of the Australian National University's submissions to BioASQ8b. We observe that the SBERT runs perform worse or on par with the BERT untrained model of Table <ref type="table" coords="13,248.74,130.95,3.87,8.74" target="#tab_5">6</ref>. This might be because questions and answers may not appear closely in an embedding space, and therefore an identical processing of each of them might not be advantageous. For example, a question includes question words whereas the candidate sentences are not normally questions. We also observe that the BioBERT models seem to produce lower results when compared with the BERT untrained model of Table <ref type="table" coords="13,362.65,190.72,3.87,8.74" target="#tab_5">6</ref>. In general, the multitask learning approach performed better than just classification or regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>This paper presents our approaches to query focused multi-document extractive summarisation for BioASQ8b. Our experiments include the use of BERT and BioBERT, Siamese architectures and SBERT, and Reinforcement Learning with PPO.</p><p>We observed that an approach that uses BERT to obtain the word embeddings, followed by LSTM to map these word embeddings to sentence embeddings, had the most promising results. The variant with BioBERT did not present an improvement, and this conflicts with the overall improvement of the use of BioBERT for question answering on biomedical texts. The approaches with Siamese architectures did not present an improvement over the base versions, presumably because questions and candidate sentences have different sentence styles.</p><p>As further research we plan to explore fine-tuning techniques for the BERT and BioBERT variants.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,260.25,345.83,7.89;3,134.77,271.24,345.83,7.86;3,134.77,282.20,345.82,7.86;3,134.77,293.15,345.83,7.86;3,134.77,304.11,345.82,7.86;3,134.77,315.07,345.83,7.86;3,134.77,326.03,178.34,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Base architecture. An embedding generator calculates the embeddings of each of the words of the candidate sentence and the question. These are then passed to an embedding reductor that obtains the overall sentence embeddings of the candidate sentence and the question. A Siamese variant enforced shared weights of the embedding reductors. Then, the sentence position is concatenated with the sentence embedding and the similarity of sentence and question embeddings, implemented as a product. A final layer predicts the label of the sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,343.91,345.82,7.89;5,134.77,354.90,345.83,7.86;5,134.77,365.86,250.12,7.86"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture for the experiments with SBERT. SBERT is used to generate the embeddings of the candidate sentence and the question. The embeddings are then processed in either a regression setup or a classification setup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,134.77,473.22,345.83,7.89;7,134.77,484.20,345.82,7.86;7,134.77,495.16,345.83,7.86;7,134.77,506.12,345.83,7.86;7,134.77,517.08,150.29,7.86"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Architecture for our PPO neural network. An embedding generator calculates the embeddings for each word, and the mean of all word embeddings is used as the sentence embedding. The 6 coloured inputs are combined into a single layer of size 501 and then fed into 2 hidden layers of size 200 each. The outputs are used as the policy function and value function for PPO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,181.32,589.55,252.72,44.22"><head>Table 1 .</head><label>1</label><figDesc>Number of sentences selected, for each question type</figDesc><table coords="3,237.31,609.92,140.74,23.85"><row><cell></cell><cell cols="4">Summary Factoid Yesno List</cell></row><row><cell>n</cell><cell>6</cell><cell>2</cell><cell>2</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,137.28,115.91,340.79,42.81"><head>Table 2 .</head><label>2</label><figDesc>Variants of SBERT used in our experiments</figDesc><table coords="6,137.28,139.87,340.79,18.85"><row><cell>System Name Model</cell><cell>Training</cell><cell>Prediction</cell></row><row><cell></cell><cell cols="2">Classification Regression Classification Regression</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.77,229.56,345.83,256.00"><head>Table 3 .</head><label>3</label><figDesc>Results of experiments using ROUGE SU4 F-score under 10-fold crossvalidation. The table shows the mean and standard deviation across the folds. "firstn" is a baseline that selects the first n sentences. NNR and NNC are described in Section 2. BERT and BioBERT are described in Section 3. Siamese LSTM is described in Section 4.1. The SBERT and SBioBERT runs are described in Section 4.2.</figDesc><table coords="8,136.16,292.02,322.16,193.54"><row><cell>Method</cell><cell>ROUGE-SU4 F1</cell></row><row><cell></cell><cell>Mean ¬± 1 stdev</cell></row><row><cell>firstn</cell><cell>0.261 ¬± 0.011</cell></row><row><cell>NNR</cell><cell>0.264 ¬± 0.008</cell></row><row><cell>NNC</cell><cell>0.271 ¬± 0.013</cell></row><row><cell>BERT untrained</cell><cell>0.270 ¬± 0.014</cell></row><row><cell>BERT trained</cell><cell>0.261 ¬± 0.012</cell></row><row><cell>BERT LSTM</cell><cell>0.274 ¬± 0.010</cell></row><row><cell cols="2">BioBERT untrained 0.262 ¬± 0.010</cell></row><row><cell>BioBERT LSTM</cell><cell>0.264 ¬± 0.012</cell></row><row><cell>Siamese LSTM</cell><cell>0.263 ¬± 0.010</cell></row><row><cell></cell><cell>0.23 0.24 0.25 0.26 0.27 0.28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,134.77,369.00,345.83,181.59"><head></head><label></label><figDesc>The size of the LSTM states and intermediate layers is also 100. The number of candidate sentences is limited to 50. The BERT version is bert-base-uncased provided by https://huggingface.co/.</figDesc><table coords="9,134.77,401.87,345.82,148.72"><row><cell cols="5">The BioBERT version is v1.1-pubmed available at https://github.com/dmis-lab/</cell></row><row><cell>biobert</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">Batch size Dropout Epochs Sentence length</cell></row><row><cell>NNR</cell><cell>1024</cell><cell>0.3</cell><cell>10</cell><cell>300</cell></row><row><cell>NNC</cell><cell>1024</cell><cell>0.3</cell><cell>10</cell><cell>300</cell></row><row><cell>BERT untrained</cell><cell>32</cell><cell>0</cell><cell>50</cell><cell>250</cell></row><row><cell>BERT trained</cell><cell>8</cell><cell>0</cell><cell>1</cell><cell>250</cell></row><row><cell>BERT LSTM</cell><cell>1024</cell><cell>0.6</cell><cell>10</cell><cell>250</cell></row><row><cell>BioBERT untrained</cell><cell>1024</cell><cell>0</cell><cell>10</cell><cell>250</cell></row><row><cell>BioBERT LSTM</cell><cell>1024</cell><cell>0.6</cell><cell>10</cell><cell>250</cell></row><row><cell>Siamese LSTM</cell><cell>1024</cell><cell>0.2</cell><cell>10</cell><cell>300</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,134.77,206.72,345.82,91.62"><head></head><label></label><figDesc>and 5 of BioASQ8b. All results use the ROUGE-SU4 F1 metric.</figDesc><table coords="10,205.53,241.63,204.30,56.70"><row><cell>System</cell><cell>BioASQ7b</cell><cell>BioASQ8b</cell></row><row><cell></cell><cell>Mean ¬± stdev</cell><cell></cell></row><row><cell cols="2">REINFORCE word2vec 0.253 ¬± 0.001</cell><cell>0.265</cell></row><row><cell>PPO word2vec</cell><cell>0.251 ¬± 0.001</cell><cell>0.269</cell></row><row><cell>PPO BERT</cell><cell>-</cell><cell>0.274</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,183.01,221.97,249.34,330.83"><head>Table 6 .</head><label>6</label><figDesc>MQ runs submitted to BioASQ8b</figDesc><table coords="11,183.01,245.93,249.34,306.87"><row><cell cols="3">Batch Run Name Description</cell><cell>ROUGE-SU4 F1</cell></row><row><cell>1</cell><cell>MQ1</cell><cell>First n</cell><cell>0.3302</cell></row><row><cell></cell><cell>MQ2</cell><cell>NNR batchsize=4096</cell><cell>0.3508</cell></row><row><cell></cell><cell>MQ3</cell><cell>NNC batchsize=4096</cell><cell>0.3556</cell></row><row><cell></cell><cell>MQ4</cell><cell>BERT untrained</cell><cell>0.3359</cell></row><row><cell></cell><cell>MQ5</cell><cell>PPO word2vec</cell><cell>0.3386</cell></row><row><cell>2</cell><cell>MQ1</cell><cell>First n b</cell><cell>0.2897</cell></row><row><cell></cell><cell>MQ2</cell><cell>NNR batchsize=4096</cell><cell>0.3360</cell></row><row><cell></cell><cell>MQ3</cell><cell>NNC batchsize=4096</cell><cell>0.3376</cell></row><row><cell></cell><cell>MQ4</cell><cell>BERT untrained</cell><cell>0.3030</cell></row><row><cell></cell><cell>MQ5</cell><cell>PPO word2vec</cell><cell>0.2662</cell></row><row><cell>3</cell><cell>MQ1</cell><cell>First n</cell><cell>0.3506</cell></row><row><cell></cell><cell>MQ2</cell><cell>NNR batchsize=4096</cell><cell>0.3729</cell></row><row><cell></cell><cell>MQ3</cell><cell>NNC batchsize=4096</cell><cell>0.3651</cell></row><row><cell></cell><cell>MQ4</cell><cell>BERT untrained</cell><cell>0.3506</cell></row><row><cell></cell><cell>MQ5</cell><cell>PPO word2vec</cell><cell>0.3018</cell></row><row><cell>4</cell><cell>MQ1</cell><cell>PPO BERT</cell><cell>0.2975</cell></row><row><cell></cell><cell>MQ2</cell><cell>NNR batchsize=4096</cell><cell>0.2973</cell></row><row><cell></cell><cell>MQ3</cell><cell>NNC batchsize=4096</cell><cell>0.2987</cell></row><row><cell></cell><cell>MQ4</cell><cell>BERT untrained</cell><cell>0.2954</cell></row><row><cell></cell><cell>MQ5</cell><cell>PPO word2vec</cell><cell>0.2585</cell></row><row><cell>5</cell><cell>MQ1</cell><cell>PPO BERT</cell><cell>0.3135</cell></row><row><cell></cell><cell>MQ2</cell><cell>NNR batchsize=4096</cell><cell>0.3237</cell></row><row><cell></cell><cell>MQ3</cell><cell>NNC batchsize=4096</cell><cell>0.3276</cell></row><row><cell></cell><cell>MQ4</cell><cell>BERT untrained</cell><cell>0.3316</cell></row><row><cell></cell><cell>MQ5</cell><cell>PPO word2vec</cell><cell>0.3096</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="12,158.87,398.79,297.61,260.08"><head>Table 7 .</head><label>7</label><figDesc>SBERT runs submitted to BioASQ8b</figDesc><table coords="12,158.87,422.74,297.61,236.12"><row><cell cols="2">Batch Run Name</cell><cell>Description</cell><cell>ROUGE-SU4 F1</cell></row><row><cell>2</cell><cell>sbert reg</cell><cell>SBERT R 1 epoch</cell><cell>0.2856</cell></row><row><cell></cell><cell>sbert cls</cell><cell>SBERT C 1 epoch</cell><cell>0.2958</cell></row><row><cell></cell><cell cols="2">multitask sbert reg SBERT M R 1 epoch</cell><cell>0.2857</cell></row><row><cell></cell><cell cols="2">multitask sbert cls SBERT M C 1 epoch</cell><cell>0.2910</cell></row><row><cell>3</cell><cell>sbert reg</cell><cell>SBERT R 20 epochs</cell><cell>0.3241</cell></row><row><cell></cell><cell>sbert cls</cell><cell>SBERT C 20 epochs</cell><cell>0.3349</cell></row><row><cell></cell><cell cols="2">multitask sbert reg SBERT M R 20 epochs</cell><cell>0.3342</cell></row><row><cell></cell><cell cols="2">multitask sbert cls SBERT M C 20 epochs</cell><cell>0.3506</cell></row><row><cell></cell><cell cols="2">sbert 1 epoch cls SBERT C 1 epoch</cell><cell>0.3395</cell></row><row><cell>4</cell><cell>sbert reg</cell><cell>SBioBERT R 20 epochs</cell><cell>0.2601</cell></row><row><cell></cell><cell>sbert cls</cell><cell>SBioBERT C 20 epochs</cell><cell>0.2601</cell></row><row><cell></cell><cell cols="2">multitask sbert reg SBioBERT M R 20 epochs</cell><cell>0.2572</cell></row><row><cell></cell><cell cols="2">multitask sbert cls SBioBERT M C 20 epochs</cell><cell>0.2527</cell></row><row><cell></cell><cell cols="2">sbert cls 1 epoch SBioBERT C 1 epoch</cell><cell>0.2580</cell></row><row><cell>5</cell><cell>sbert reg</cell><cell>SBioBERT R 20 epochs</cell><cell>0.3201</cell></row><row><cell></cell><cell>sbert cls</cell><cell>SBioBERT C 20 epochs</cell><cell>0.3235</cell></row><row><cell></cell><cell cols="2">multitask sbert reg SBioBERT M R 20 epochs</cell><cell>0.3152</cell></row><row><cell></cell><cell cols="2">multitask sbert cls SBioBERT M C 20 epochs</cell><cell>0.3245</cell></row><row><cell></cell><cell cols="2">sbert cls 1 epoch SBioBERT C 1 epoch</cell><cell>0.3231</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,646.48,98.85,7.47"><p>http://www.bioasq.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,144.73,657.44,160.05,7.47"><p>http://www.ncbi.nlm.nih.gov/pubmed</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="4,144.73,646.48,108.27,7.47"><p>https://huggingface.co/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="4,144.73,657.44,165.25,7.47"><p>https://github.com/dmis-lab/biobert</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="5,144.73,657.44,193.49,7.47"><p>https://huggingface.co/gsarti/biobert-nli</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="6,144.73,657.44,188.79,7.47"><p>https://stable-baselines.readthedocs.io/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Research by <rs type="person">Vincent Nguyen</rs> is supported by the <rs type="programName">Australian Research Training Program</rs> and the <rs type="funder">CSIRO</rs> <rs type="grantName">Postgraduate Scholarship</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_eUKfHcj">
					<orgName type="grant-name">Postgraduate Scholarship</orgName>
					<orgName type="program" subtype="full">Australian Research Training Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.96,546.61,337.63,7.86;13,151.52,557.57,329.07,7.86;13,151.52,568.53,115.27,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,205.77,546.61,16.25,7.86;13,252.08,546.61,224.30,7.86">Evaluation of question-focused summarization systems</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,166.28,557.57,314.31,7.86;13,151.52,568.53,40.15,7.86">Proceedings of the Workshop on Task-Focused Summarization and Question Answering</title>
		<meeting>the Workshop on Task-Focused Summarization and Question Answering</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2006</date>
			<biblScope unit="page" from="48" to="55" />
		</imprint>
	</monogr>
	<note>Duc</note>
</biblStruct>

<biblStruct coords="13,142.96,579.79,337.64,7.86;13,151.52,590.75,329.07,7.86;13,151.52,601.71,329.07,8.12;13,151.52,613.31,42.87,7.47" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,364.92,579.79,115.67,7.86;13,151.52,590.75,226.30,7.86">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N19-1423/" />
	</analytic>
	<monogr>
		<title level="m" coord="13,404.85,590.75,51.36,7.86">NAACL-HLT</title>
		<meeting><address><addrLine>Minneapolis, MN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">jun 2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,623.92,337.63,7.86;13,151.52,634.88,329.07,7.86;13,151.52,645.84,329.07,8.12;13,151.52,656.80,104.49,8.12" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Raffin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ernestus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gleave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kanervisto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Traore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://github.com/hill-a/stable-baselines" />
		<title level="m" coord="13,287.23,645.84,62.98,7.86">Stable baselines</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,119.67,337.64,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,329.07,7.86;14,151.52,152.55,329.07,8.11;14,151.52,164.16,104.06,7.47" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,249.30,119.67,231.29,7.86;14,151.52,130.63,14.75,7.86">Universal language model fine-tuning for text classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1031</idno>
		<ptr target="https://www.aclweb.org/anthology/P18-1031" />
	</analytic>
	<monogr>
		<title level="m" coord="14,189.67,130.63,290.91,7.86;14,151.52,141.59,88.91,7.86">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="14,142.96,174.47,337.64,7.86;14,151.52,185.43,264.19,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="14,202.85,174.47,277.74,7.86;14,151.52,185.43,58.54,7.86">Reinforcement Learning For Query-based Multi-document Extractive Summarisation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-01">Jan 2020</date>
		</imprint>
		<respStmt>
			<orgName>Macquarie University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="14,142.96,196.39,337.63,7.86;14,151.52,207.34,329.07,7.86;14,151.52,218.28,329.07,8.14;14,151.52,229.91,188.29,7.47" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,430.36,196.39,50.23,7.86;14,151.52,207.34,329.07,7.86;14,151.52,218.30,11.14,7.86">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<ptr target="https://academic.oup.com/bioinformatics/article/36/4/1234/5566506" />
	</analytic>
	<monogr>
		<title level="j" coord="14,172.06,218.30,58.56,7.86">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2019-09">09 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,240.22,337.64,7.86;14,151.52,251.18,329.07,7.86;14,151.52,262.14,329.07,7.86;14,151.52,273.10,329.07,7.86;14,151.52,284.06,329.07,8.11;14,151.52,295.66,56.98,7.47" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,265.78,251.18,214.82,7.86;14,151.52,262.14,258.85,7.86">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.703" />
	</analytic>
	<monogr>
		<title level="m" coord="14,432.51,262.14,48.08,7.86;14,151.52,273.10,325.26,7.86">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,305.98,337.63,7.86;14,151.52,316.93,329.07,7.86;14,151.52,327.89,329.07,7.86;14,151.52,338.85,329.07,7.86;14,151.52,349.81,329.07,8.12;14,151.52,361.42,85.23,7.47" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,245.38,305.98,191.81,7.86">Text summarization with pretrained encoders</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1387</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1387" />
	</analytic>
	<monogr>
		<title level="m" coord="14,463.03,305.98,17.56,7.86;14,151.52,316.93,329.07,7.86;14,151.52,327.89,329.07,7.86;14,151.52,338.85,167.72,7.86">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11">Nov 2019</date>
			<biblScope unit="page" from="3730" to="3740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,371.73,337.64,7.86;14,151.52,382.69,329.07,7.86;14,151.52,393.65,329.07,8.11;14,151.52,405.25,75.81,7.47" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,244.54,371.73,218.33,7.86">Contextualized weak supervision for text classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mekala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.30" />
	</analytic>
	<monogr>
		<title level="m" coord="14,151.52,382.69,329.07,7.86;14,151.52,393.65,41.98,7.86">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,415.56,337.97,7.86;14,151.52,426.52,329.07,7.86;14,151.52,437.48,329.07,7.86;14,151.52,448.44,247.01,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,249.53,415.56,231.06,7.86;14,151.52,426.52,255.26,7.86">Classification betters regression in query-based multidocument summarisation techniques for question answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moll√°</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,236.76,437.48,239.48,7.86">Machine Learning and Knowledge Discovery in Databases</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Cellier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Driessens</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="624" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,459.40,337.98,7.86;14,151.52,470.36,329.07,7.86;14,151.52,481.32,329.07,7.86;14,151.52,492.28,272.55,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,262.74,459.40,217.86,7.86;14,151.52,470.36,63.65,7.86">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,238.81,470.36,241.78,7.86;14,151.52,481.32,329.07,7.86;14,151.52,492.28,200.94,7.86">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<biblScope unit="page" from="3980" to="3990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,428.79,492.28,51.80,7.86;14,151.52,503.24,329.07,8.11;14,151.52,514.84,104.06,7.47" xml:id="b11">
	<monogr>
		<title/>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1410" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Stroudsburg, PA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,525.15,337.98,7.86;14,151.52,536.11,261.00,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="14,416.80,525.15,63.80,7.86;14,151.52,536.11,94.78,7.86">Proximal policy optimization algorithms</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.62,547.07,337.98,7.86;14,151.52,558.00,259.47,7.89" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,216.68,547.07,263.91,7.86;14,151.52,558.03,88.87,7.86">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,247.33,558.03,69.14,7.86">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,568.99,337.98,7.86;14,151.52,579.95,329.07,7.86;14,151.52,590.91,329.07,7.86;14,151.52,601.87,101.04,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,354.23,568.99,126.36,7.86;14,151.52,579.95,120.17,7.86">Pre-trained language model for biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,417.79,579.95,62.80,7.86;14,151.52,590.91,175.73,7.86">Machine Learning and Knowledge Discovery in Databases</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Cellier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Driessens</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="727" to="740" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
