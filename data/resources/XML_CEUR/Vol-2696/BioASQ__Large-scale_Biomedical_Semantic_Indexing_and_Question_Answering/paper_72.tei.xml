<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.18,152.67,320.78,12.64;1,130.34,170.67,334.73,12.64">NCU-IISR: Using a Pre-trained Language Model and Logistic Regression Model for BioASQ Task 8b Phase B</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.06,209.70,59.67,8.96"><forename type="first">Jen-Chieh</forename><surname>Han</surname></persName>
							<email>joyhan@cc.ncu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Central University</orgName>
								<address>
									<settlement>Taoyuan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.05,209.70,59.38,8.96"><forename type="first">Richard</forename><surname>Tzong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Central University</orgName>
								<address>
									<settlement>Taoyuan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.49,209.70,34.91,8.96"><forename type="first">Han</forename><surname>Tsai</surname></persName>
							<email>thtsai@csie.ncu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Central University</orgName>
								<address>
									<settlement>Taoyuan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.18,152.67,320.78,12.64;1,130.34,170.67,334.73,12.64">NCU-IISR: Using a Pre-trained Language Model and Logistic Regression Model for BioASQ Task 8b Phase B</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">654D884807DD752F83DC626CF8802FC8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Biomedical Question Answering ⸱ Pre-trained Language Model ⸱ Logistic Regression</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent successes in pre-trained language models, such as BERT, RoB-ERTa, and XLNet, have yielded state-of-the-art results in the natural language processing field. BioASQ is a question answering (QA) benchmark with a public and competitive leaderboard that spurs advancement in large-scale pre-trained language models for biomedical QA. In this paper, we introduce our system for the BioASQ Task 8b Phase B. We employed a pre-trained biomedical language model, BioBERT, to generate "exact" answers for the questions, and a logistic regression model with our sentence embedding to construct the top-n sentences/snippets as a prediction for "ideal" answers. On the final test batch, our best configuration achieved the highest ROUGE-2 and ROUGE-SU4 F1 scores among all participants in the 8th BioASQ QA task (Task 8b, Phase B).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since 2018, BioASQ 1 <ref type="bibr" coords="1,214.85,504.71,99.34,8.96" target="#b0">(Tsatsaronis et al., 2015)</ref> has organized eight challenges on biomedical semantic indexing and question answering. This year, the challenges include three main tasks: Task 8a, Task 8b, and Task MESINESP8. We only participated in Task 8b Phase B (QA task), in which participants are given a biomedical question and list of question-relevant articles/snippets as input, and should return either an exact answer or an ideal answer. The task was provided 3,243 training questions that included the previous year's test set with gold annotations, plus 500 test questions for evaluation, divided into five batches of 100 questions each. All questions and answers were con-structed by a team of biomedical experts from around Europe; the questions were categorized into four types: yes/no, factoid, list, and summary. Three types of questions required exact answers: yes/no, factoid, and list. For all four types of questions, participants needed to submit ideal answers. Each participant was allowed to submit a maximum of five results in Task 8b.</p><p>Some QA examples are illustrated in Fig. <ref type="figure" coords="2,314.23,449.13,3.77,8.96" target="#fig_0">1</ref>. Each BioASQ QA instance gives a question and several relevant snippets of PubMed abstracts, including the ID of the full PubMed article. Thus, we formulated the task as query-based multi-document a. extraction for exact answers and b. summarization for ideal answers. In this paper, we employed a pre-trained language model released by BioBERT <ref type="bibr" coords="2,373.99,497.15,65.85,8.96" target="#b1">(Lee et al., 2020)</ref>, which model achieved the highest performance last year. However, BioBERT was not previously used for generating ideal answers. BioBERT is well-constructed for different natural language processing (NLP) tasks like relation classification and identifying the answer phrase of a question by the given paragraph. BERT uses a masking mechanism to train its language model, thus makes the model learn meanings in different situations. Many biomedical task results show that its language model outperforms traditional word presentation. Therefore, we further applied BioBERT's [CLS] embeddings as input to a logistic regression model for predicting ideal answers.</p><p>The sections are organized as follows. Section 2 briefly reviews recent works on QA. The details of our two methods are described separately in Section 3 and 4. Section 5 describes our configurations submitted to the BioASQ challenge. Section 6 gives a summary of our system's performance in the BioASQ QA task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In most QA tasks, such as SQuAD<ref type="foot" coords="3,273.89,176.07,3.24,5.83" target="#foot_0">2</ref>  <ref type="bibr" coords="3,280.13,177.18,186.32,8.96" target="#b2">(Rajpurkar, Zhang, Lopyrev, &amp; Liang, 2016)</ref>, SQuAD 2.0 <ref type="bibr" coords="3,178.58,189.18,132.93,8.96" target="#b3">(Rajpurkar, Jia, &amp; Liang, 2018)</ref>, and PubMedQA <ref type="bibr" coords="3,392.23,189.18,78.22,8.96;3,124.70,201.18,82.65,8.96" target="#b4">(Jin, Dhingra, Liu, Cohen, &amp; Lu, 2019)</ref>, only exact answers are provided for questions. Exact answers almost always appear in the context of the given relevant articles/snippets; thus, these tasks are usually formulated as a sequence to sequence problem. Recently, it was found that significant improvements can be had in many natural language processing (NLP) tasks by using pre-trained contextual representations <ref type="bibr" coords="3,342.55,249.18,80.71,8.96" target="#b5">(Peters et al., 2018)</ref> rather than simple word vectors.</p><p>For instance, Google developed Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" coords="3,195.53,285.21,173.75,8.96" target="#b6">(Devlin, Chang, Lee, &amp; Toutanova, 2018)</ref> to solve the problem of shallow bidirectionality. BERT uses a masked language model (MLM) for the pretraining objective, which MLM randomly masks some tokens from the unlabeled input and then predicts the original vocabulary ID of the masked word based on its context. Because MLM jointly concatenates the left and right context as representation, it can pre-train a deep bidirectional Transformer. In BERT's framework, two steps (pre-training and fine-tuning) have the same architectures but different output layers. During fine-tuning, different down-stream tasks initialize models with the same pre-trained model parameters, and all parameters are fine-tuned using labeled data from each task. BERT is the first fine-tuning-based representation model, and its result outperforms prior models on sentence-level and token-level NLP tasks.</p><p>Many significant sentence-level classification tasks come from the General Language Understanding Evaluation (GLUE<ref type="foot" coords="3,286.85,428.10,3.24,5.83" target="#foot_1">3</ref> ) benchmark <ref type="bibr" coords="3,341.83,429.21,74.49,8.96" target="#b7">(Wang et al., 2018)</ref>. To help machines understand language just like humans, GLUE provides nine diverse sentence understanding tasks; one example is inputting a pair of sentences, for which the system must predict a relationship with one sentence as the premise and the other as the hypothesis. Where most token-level natural language understanding (NLU) models are designed to carry out a specific task using specific domain data, GLUE is an auxiliary dataset for exploring models with an eye to understanding specific linguistic phenomena across different domains; it thus provides a publicly online platform for evaluating and comparing models.</p><p>On the other hand, the two major QA tasks, the Stanford Question Answering Dataset (SQuAD) and SQuAD 2.0, are both token-level tasks. Each instance of the SQuAD gives a question and a passage from Wikipedia, for which the goal is to find the answer text span (start and end position in tokens) in the passage. The SQuAD 2.0 task extends the original SQuAD problem definition by allowing there to be no short answer in the provided paragraph. Each task has an official leaderboard.</p><p>Because these NLP tasks have public leaderboards, they are highly competitive and make for rapid expansion in pre-trained models. BERT provided a good start, after which improved models came out such as RoBERTa <ref type="bibr" coords="3,341.35,633.23,66.59,8.96" target="#b8">(Liu et al., 2019)</ref>, XLNet <ref type="bibr" coords="3,445.66,633.23,24.89,8.96;3,124.70,645.23,50.37,8.96" target="#b9">(Yang et al., 2019)</ref>, ALBERT <ref type="bibr" coords="3,227.57,645.23,73.88,8.96" target="#b10">(Lan et al., 2019)</ref>, and ELECTRA <ref type="bibr" coords="3,379.03,645.23,91.52,8.96;4,124.70,150.18,64.05,8.96" target="#b11">(Clark, Luong, Le, &amp; Manning, 2020)</ref>. These models also achieved state-of-the-art results upon being released. The model Bidirectional Encoder Representations from Transformers for Biomedical Text Mining (BioBERT), based on Google's BERT code, is a language representation model specific to the biomedical domain, pre-trained on large-scale biomedical corpora (1 million articles from PubMed<ref type="foot" coords="4,302.45,197.07,3.24,5.83" target="#foot_2">4</ref> or 270 thousand from PubMed Central<ref type="foot" coords="4,461.50,197.07,3.24,5.83" target="#foot_3">5</ref> ). Taking advantage of being able to apply almost the same architecture across tasks, Bi-oBERT largely outperforms previous models and is state-of-the-art in a variety of biomedical text mining tasks.</p><p>The BioASQ QA task allows participants to only participate in some batches and to return either only exact answers or ideal answers. The ideal answer includes prominent supportive information, whereas the exact answer only returns yes or no for yes/no questions, entity names for factoid questions, or lists of entity names for list questions; ideal answers can thus be seen as the full definition of exact answers. Ideal answers are usually written by biomedical experts and presented in a short text that answers the question. Because most ideal answers cannot be directly mapped to the given relevant articles/snippets, predicting appropriate ideal answers is more complicated than predicting exact answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Similarity Between a Snippet and a Question</head><p>Although the BioASQ QA task provides biomedical questions and relevant snippets of PubMed abstracts, in actuality, ideal answers do not appear verbatim in the relevant snippets. The goal of our method was to select the most relevant snippet for each question in the BioASQ QA instances. To determine the similarity between a question and a snippet, we directly calculated relevance scores using cosine similarity. Cosine similarity is one of the most common text similarity metrics, thus is widely utilized in NLP tasks. Therefore, we first had to transform questions and snippets into vectors. In general, previous works map words to corresponding vectors by taking word2vec <ref type="bibr" coords="4,124.70,495.23,210.51,8.96" target="#b12">(Mikolov, Sutskever, Chen, Corrado, &amp; Dean, 2013)</ref> embeddings trained on a relevant corpus or else adopt existing word embeddings such as GloVe <ref type="bibr" coords="4,376.27,507.23,94.27,8.96;4,124.70,519.23,62.61,8.96" target="#b13">(Pennington, Socher, &amp; Manning, 2014)</ref>, and Wiki-PubMed-PMC <ref type="bibr" coords="4,294.89,519.23,175.51,8.96;4,124.70,531.23,21.58,8.96" target="#b14">(Habibi, Weber, Neves, Wiegandt, &amp; Leser, 2017)</ref>. A lot of word2vec embeddings and TF-IDF vectors were referred to by Diego Mollá's features <ref type="bibr" coords="4,192.41,555.23,88.88,8.96" target="#b15">(Mollá &amp; Jones, 2019)</ref>, and we considered that it can be improved. In other words, TF-IDF regarding some common words (such as articles and conjunctions) as trivial terms so as to more readily identify the major words of sentences, these methods are unable to represent polysemic words. Notably, on the GLUE leaderboard, methods using word2vec embeddings (Skip-gram and CBOW) rank much lower than those using the ensemble mode of ELMo, such as BERT. BERT provides contextual embeddings that can solve the problem of polysemy, so deals well with many different tasks. Therefore, we simplified the procedure of extracting features from BioBERT and only took the pre-trained embeddings of sentences.</p><p>In our method, before separately obtaining the embeddings of a question and a snippet, each sentence was first pre-processed into word pieces with WordPiece tokenization. Then, inputting all word pieces of the sentence to BioBERT, we extracted the features from the last layer of BioBERT. In BERT, the [CLS] token was inserted into input tokens, and its embeddings could be considered as the sentence vector (the features). The step of extracting pre-trained contextual embeddings from BioBERT is diagrammed in Fig. <ref type="figure" coords="5,198.53,497.87,3.77,8.96" target="#fig_1">2</ref>.</p><p>Finally, we used the embeddings (vectors) of a question and snippet pair to calculate their cosine similarity score. Because each question of a BioASQ QA instance typically has more than one snippet, we re-ranked the snippets in order of their similarity scores and took the top 1 snippet as our prediction of the answer (NCU-IISR_2), as that snippet was considered the most relevant to answering the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Logistic Regression of Sentences</head><p>Our approach was inspired by the framework of the logistic regression model proposed by Diego Mollá. The method follows the two steps of his summarization process:</p><p>Step 1, split the input text (snippets) into candidate sentences, and score each candidate sentence.</p><p>Step 2, return the top-n sentences with the highest scores. As stated above, we used the pre-trained language model "BioBERT" to replace their features with word embeddings. We first used NLTK's sentence tokenizer divide snippets into sentences and calculated ROUGE-SU4 F1 scores <ref type="bibr" coords="6,255.29,395.85,45.71,8.96" target="#b16">(Lin, 2004)</ref> between each sentence and the associated question, thereby generating positive and negative instances that became the training set for our logistic regression model. After pre-processing, our logistic regression model was slightly different from the cosine similarity method. First, we input a candidate sentence and a question at the same time and used the fine-tuned BioBERT model for fitting the task. Second, we appended a dense layer with ReLU activation after the output layer of BioBERT, and we used mean squared error as the loss function. We took default settings from BERT trained on SQuAD. We also used [CLS] embeddings as the feature from which to predict the ROUGE-SU4 F1 scores of the test data. In our case, [CLS] embeddings represented the relation between a candidate sentence and a question. Fig. <ref type="figure" coords="6,184.10,515.87,4.98,8.96" target="#fig_2">3</ref> illustrates the modified BioBERT architecture used here. Lastly, we used the prediction values to re-rank the candidate sentences for each question and selected only the top n sentences as our system output (NCU-IISR_3).</p><p>Due to time limitations, we did not finish aspects of the logistic regression model such as fine-tuning the model with all instances, expanding the range of snippets to the full abstract, and comparing activation or loss functions to find a better one. These can be future work and updates addressed in the next challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Submission</head><p>To obtain exact answers, we used the BioASQ-BioBert model <ref type="bibr" coords="6,372.55,644.87,97.80,8.96;6,124.70,656.90,63.09,8.96" target="#b17">(Yoon, Lee, Kim, Jeong, &amp; Kang, 2019)</ref>. This model included two pre-trained weights: one fine-tuned on SQuAD for "yes/no" questions, and the other on SQuAD 2.0 for "factoid and list" questions. We then separately fine-tuned again the yes/no, factoid, and list questions of the QA task. Because BERT performs well on SQuAD, we considered that this method fits suitably into BioASQ's exact answers. We used the open-source code of the BERT and BioBERT pre-trained language model to find the paragraph-sized answer (NCU-IISR_1) additionally in ideal answers. For each training instance, the input is the full PubMed abstracts, and the answer is the snippet.</p><p>Our submitted configurations are summarized in Table <ref type="table" coords="7,359.47,210.18,3.77,8.96">1</ref>. Because our submissions for batch 3 had some errors, Table <ref type="table" coords="7,265.37,222.18,4.98,8.96">1</ref> only shows the results of batches 1, 2, 4, and 5. In our internal experiments with the "NCU-IISR_3" configuration, we observed that most predictions had lengths as long as ideal answers in the training set. Therefore, we simply selected the top 1 sentence as the ideal answer in all types.</p><p>Table <ref type="table" coords="7,243.77,282.02,3.41,8.10">1</ref>. Descriptions of our three systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System Name</head><p>System Description Section Participating Batch</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NCU-IISR_1</head><p>Exact answers: Used BioASQ-BioBert.</p><p>Ideal answers: Referred to the SQuAD training in BERT, used snippets from full PubMed abstracts as instances and fineturned on BioBERT.</p><p>-1, 2, 4, 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NCU-IISR_2</head><p>Ideal answers: Used cosine similarity to select the top 1 snippet. 3 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NCU-IISR_3</head><p>Ideal answers: Used predicted ROUGE-SU4 scores to select the top n sentences of snippets, where n is equal to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">5</head><p>Model performances in predicting exact answers are shown in Table <ref type="table" coords="7,413.47,537.11,3.77,8.96">2</ref>. Irrespective of the question type, most of our results outperformed the median scores. In particular, we won second place on the factoid questions at batch 2 and found that "NCU-IISR_1" generally performed higher in the factoid category than on the other two question types.</p><p>Model performances in predicting ideal answers are shown in Table <ref type="table" coords="7,417.55,585.11,3.77,8.96">3</ref>. With ideal answers, two evaluation metrics are used: ROUGE and human evaluation. Roughly speaking, ROUGE counts the n-gram overlap between an automatically constructed summary and a set of human-written (gold) summaries, with a higher ROUGE score being better. Specifically, ROUGE-2 and ROUGE-SU4 were used to evaluate ideal answers. These automatic evaluations are the most widely used versions of ROUGE and have been discovered to correlate well with human judgments when multiple reference summaries are available for each question.</p><p>Table <ref type="table" coords="8,149.18,149.99,3.41,8.10">2</ref>. Results of each test (except 3) for exact answers in the BioASQ QA task. Total Systems counts the number of participants for each batch in the given category. For example, in batch 2, we took second place in factoid questions out of 24 submitted systems. There were more systems submitted in later batches. Best Score indicates the best result across all participants, and Median Score the median result. The human evaluation results (manual scores)</p><p>have not yet been reported by the organizers. All ideal answers to the systems will also be evaluated by biomedical experts.</p><p>For each ideal answer, the experts give a score ranging from 1-5 on each of four terms: information recall (the answer reports all necessary information), information precision (no irrelevant information is reported), information repetition (the answer does not repeat information multiple times, e.g. when sentences extracted from different articles convey the same information), and readability (the answer is easily readable and fluent). sample of ideal answers will be evaluated by more than one expert in order to measure the inter-annotator agreement.</p><p>Table <ref type="table" coords="9,148.82,185.99,3.41,8.10">3</ref>. Results (ROUGE-2 and ROUGE-SU4 F1 scores) of each test batch (except 3) for ideal answers in the BioASQ QA task. Total Systems counts the number of participants in each batch. In batch 5, our system "NCU-IISR_3" took first place out of 28 submitted systems in both scores. Automatic evaluations in the BioASQ also provide a Recall metric, which shows how many tokens from the prediction appear in the gold answer. For ideal answers, our recall values were lower than the median. The ROUGE-2 and ROUGE-SU4 Recall values for our best system "NCU-IISR_3" are given in Table <ref type="table" coords="9,373.99,620.63,3.77,8.96" target="#tab_2">4</ref>. As mentioned earlier, we only returned the top 1 sentence from the logistic regression model, thus we definitely lost some sentences that would have added to ideal answers. In contrast, Diego Mollá's work concatenated the top-n sentences when answering questions. If we compile answers from more sentences, we may solve the problem of poor Recall scores. This also can be a direction for improvement in the future. In the 8th BioASQ QA task, we employed BioBERT to deal with both exact answers and ideal answers. In generating exact answers, we used BioASQ-BioBert to find the offset (including the start and end positions) of the answer within the given passage (snippets). Our performance was almost always above the median for yes/no, factoid, and list question types. However, when it comes to ideal answers, the BioASQ-BioBert method does not readily recognize the most relevant text. In order to maintain the completeness of ideal answers, we selected the most relevant snippet or sentences rather than taking snippet offsets, which may focus on the wrong position and yield imperfect answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Our results show that in arriving at ideal answers, using the logistic regression model to select sentences performs better than using cosine similarity to choose a snippet. One reason for this improvement might be that a large number of snippets are too lengthy for ideal answers, thus resulting in lower performance. In other words, snippet answers that consist of only trivial information receive lower ROUGE scores. Our method of selecting sentences achieved the best ROUGE-2 and ROUGE-SU4 F1 scores among all participants, but we also note that our Recall scores were much lower than others. This suggests that our potential improvement with the regression method was unable to convert more possible sentences.</p><p>In future work, we may try to solve this problem by referring to other methods and merging in their models. On the other hand, as mentioned previously, we left some work unfinished in the regression experiment. Thus, future directions include completely fine-tuning the model with all instances, expanding the range of snippets to include full abstracts, and comparing activation or loss functions to find a better one. In the regression method, we only processed snippet context and did not use the complete abstracts. Thus, these can be utilized in the future. All told, we hope to keep the base of BioBERT and make an effort to combine it with different approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,172.34,359.90,250.35,8.10;2,126.36,157.44,342.72,193.80"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The QA examples of the BioASQ Task 8b Phase B (QA task).</figDesc><graphic coords="2,126.36,157.44,342.72,193.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,124.70,385.58,345.95,8.10;5,124.70,396.62,317.53,8.10;5,126.60,156.48,342.00,219.96"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of how a single sentence input obtains the pre-trained contextual embeddings (such as ELMo) in the last layer of the pre-trained BioBERT model without fine-tuning.</figDesc><graphic coords="5,126.60,156.48,342.00,219.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,124.70,343.70,345.74,8.10;6,124.70,354.62,243.49,8.10;6,124.68,150.12,344.76,184.92"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration showing how a pair of input sentences (a question and a passage) obtains the contextual embeddings in the last layer of the fine-tuned BioBERT.</figDesc><graphic coords="6,124.68,150.12,344.76,184.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,124.70,149.99,346.04,234.98"><head>Table 4 .</head><label>4</label><figDesc>Recall scores (ROUGE-2 and of ideal answers from test batch 5 in the BioASQ QA task, including the number one to three highest scores and the median score. Our Recall scores were around 0.28 lower than the #1 system.</figDesc><table coords="10,124.70,193.02,285.22,191.96"><row><cell></cell><cell></cell><cell cols="2">Batch 5</cell></row><row><cell></cell><cell>System Name</cell><cell></cell></row><row><cell></cell><cell></cell><cell>ROUGE-2 Recall</cell><cell>ROUGE-SU4 Recall</cell></row><row><cell></cell><cell>#1</cell><cell>0.6646</cell><cell>0.6603</cell></row><row><cell></cell><cell>#2</cell><cell>0.6627</cell><cell>0.6587</cell></row><row><cell></cell><cell>#3</cell><cell>0.6431</cell><cell>0.6399</cell></row><row><cell></cell><cell>NCU-IISR_3</cell><cell>0.3867</cell><cell>0.3805</cell></row><row><cell></cell><cell>Median Score</cell><cell>0.4620</cell><cell>0.4650</cell></row><row><cell></cell><cell>Total Systems</cell><cell></cell><cell>28</cell></row><row><cell>6</cell><cell cols="2">Conclusions &amp; Future Work</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,136.10,675.19,159.47,8.10"><p>https://rajpurkar.github.io/SQuAD-explorer/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,136.10,686.23,143.23,8.10"><p>https://gluebenchmark.com/leaderboard</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="4,136.10,675.19,119.98,8.10"><p>https://pubmed.ncbi.nlm.nih.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="4,136.10,686.23,128.39,8.10"><p>https://www.ncbi.nlm.nih.gov/pmc/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Appreciating <rs type="person">Po-Ting Lai</rs> for giving us suggestions during the challenge and revising the paper.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,124.70,289.10,345.54,8.10;11,160.70,301.10,309.85,8.10;11,160.70,313.10,310.00,8.10;11,160.70,325.10,15.79,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,274.00,301.10,196.55,8.10;11,160.70,313.10,201.21,8.10">An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Alvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">.</forename><surname>Polychronopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,369.07,313.10,73.11,8.10">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,124.70,337.10,346.04,8.10;11,160.70,349.10,309.59,8.10;11,160.70,361.10,123.23,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,407.80,337.10,62.94,8.10;11,160.70,349.10,305.64,8.10">BioBERT: a pretrained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,160.70,361.10,52.03,8.10">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,124.70,373.10,345.80,8.10;11,160.70,385.10,239.90,8.10" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,354.46,373.10,116.05,8.10;11,160.70,385.10,112.17,8.10">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,124.70,397.10,345.66,8.10;11,160.70,409.10,168.38,8.10" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<title level="m" coord="11,277.75,397.10,192.61,8.10;11,160.70,409.10,38.69,8.10">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,124.70,421.10,345.32,8.10;11,160.70,433.10,279.98,8.10" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,368.69,421.10,101.33,8.10;11,160.70,433.10,151.11,8.10">PubMedQA: A Dataset for Biomedical Research Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06146</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,124.70,445.10,345.40,8.10;11,160.70,457.10,306.41,8.10" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m" coord="11,189.25,457.10,149.81,8.10">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,124.70,469.12,345.91,8.10;11,160.70,481.12,309.96,8.10;11,160.70,493.12,68.16,8.10" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,370.22,469.12,100.39,8.10;11,160.70,481.12,232.08,8.10">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,124.70,505.12,346.04,8.10;11,160.70,517.12,310.05,8.10;11,160.70,529.12,99.84,8.10" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,416.30,505.12,54.44,8.10;11,160.70,517.12,281.33,8.10">Glue: A multitask benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,124.70,541.12,345.87,8.10;11,160.70,553.12,287.57,8.10" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,430.12,541.12,40.45,8.10;11,160.70,553.12,159.23,8.10">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">.</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,124.70,565.12,346.02,8.10;11,160.70,577.12,309.92,8.10;11,160.70,589.12,209.67,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,448.78,565.12,21.94,8.10;11,160.70,577.12,245.81,8.10">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,182.75,589.12,183.79,8.10">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct coords="11,124.70,601.12,345.37,8.10;11,160.70,613.12,310.13,8.10;11,160.70,625.12,68.16,8.10" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="11,421.01,601.12,49.06,8.10;11,160.70,613.12,243.06,8.10">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,124.70,637.12,346.01,8.10;11,160.70,649.15,268.58,8.10" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="11,348.05,637.12,122.66,8.10;11,160.70,649.15,140.56,8.10">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,124.70,661.15,345.94,8.10;11,160.70,673.15,309.94,8.10;11,160.70,685.15,200.96,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,429.91,661.15,40.73,8.10;11,160.70,673.15,235.87,8.10">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,174.00,685.15,183.84,8.10">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct coords="12,124.70,151.07,156.50,8.10;12,299.79,151.07,170.92,8.10;12,160.70,163.07,309.69,8.10;12,160.70,175.07,183.42,8.10" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,345.55,151.07,125.15,8.10;12,160.70,163.07,51.13,8.10">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,296.77,163.07,173.62,8.10;12,160.70,175.07,178.57,8.10">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct coords="12,124.70,187.07,345.63,8.10;12,160.70,199.07,310.00,8.10;12,160.70,211.07,28.28,8.10" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,381.88,187.07,88.45,8.10;12,160.70,199.07,217.68,8.10">Deep learning with word embeddings improves biomedical named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Habibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Wiegandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Leser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,385.39,199.07,51.92,8.10">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,124.70,223.07,345.97,8.10;12,160.70,235.07,309.91,8.10;12,160.70,247.07,306.08,8.10" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,238.01,223.07,232.66,8.10;12,160.70,235.07,189.76,8.10">Classification betters regression in query-based multi-document summarisation techniques for question answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,453.24,235.07,17.37,8.10;12,160.70,247.07,302.22,8.10">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct coords="12,124.70,259.07,346.02,8.10;12,160.70,271.10,252.01,8.10" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="12,213.05,259.07,228.90,8.10">ROUGE: A Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-07">2004, jul</date>
			<pubPlace>Barcelona, Spain</pubPlace>
		</imprint>
	</monogr>
	<note>Paper presented at the Text Summarization Branches Out</note>
</biblStruct>

<biblStruct coords="12,124.70,283.10,345.93,8.10;12,160.70,295.10,244.70,8.10" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="12,350.88,283.10,119.75,8.10;12,160.70,295.10,115.86,8.10">Pre-trained Language Model for Biomedical Question Answering</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08229</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
