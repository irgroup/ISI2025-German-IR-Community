<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,177.88,115.96,259.61,12.62;1,180.61,133.89,254.13,12.62">Priberam at MESINESP Multi-label Classification of Medical Texts Task</title>
				<funder ref="#_GVXpCnP">
					<orgName type="full">Lisbon Regional Operational Programme</orgName>
				</funder>
				<funder ref="#_MxkdkRH">
					<orgName type="full">European Regional Development Fund</orgName>
					<orgName type="abbreviated">ERDF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.48,171.57,65.45,8.74"><forename type="first">Rúben</forename><surname>Cardoso</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Priberam Labs</orgName>
								<address>
									<settlement>Lisbon</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.04,171.57,55.69,8.74"><forename type="first">Zita</forename><surname>Marinho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Priberam Labs</orgName>
								<address>
									<settlement>Lisbon</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.00,171.57,63.91,8.74"><forename type="first">Afonso</forename><surname>Mendes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Priberam Labs</orgName>
								<address>
									<settlement>Lisbon</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,381.71,171.57,81.17,8.74"><forename type="first">Sebastião</forename><surname>Miranda</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Priberam Labs</orgName>
								<address>
									<settlement>Lisbon</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,177.88,115.96,259.61,12.62;1,180.61,133.89,254.13,12.62">Priberam at MESINESP Multi-label Classification of Medical Texts Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D1CDE6B9A93908C908A54109E9AE9F55</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical articles are a crucial tool to provide current state of the art treatments and diagnostics to medical professionals. However, existing public databases such as MEDLINE contain over 27 million articles, making the use of efficient search engines crucial in order to navigate and provide meaningful recommendations. Classifying these articles into broader medical topics can improve retrieval of related articles <ref type="bibr" coords="1,420.77,310.75,9.22,7.86" target="#b0">[1]</ref>. The set of medical labels considered for the MESINESP task is on the order of several thousands of labels (DeCS codes), which falls under the extreme multi-label classification problem <ref type="bibr" coords="1,339.72,343.63,9.22,7.86" target="#b1">[2]</ref>. The heterogeneous and highly hierarchical structure of medical topics makes the task of manually classifying articles extremely laborious and costly. It is, therefore, crucial to automate the process of classification. Typical machine learning algorithms become computationally demanding with such a large label set and achieving better recall becomes an unsolved problem. This work presents Priberam's participation at the BioASQ task Mesinesp. We address the large multi-label classification problem through the use of four different models: a Support Vector Machine (SVM) [3], the customised search engine Priberam Search [4], a BERT based classifier <ref type="bibr" coords="1,439.96,442.27,9.22,7.86" target="#b4">[5]</ref>, and a SVM-rank ensemble [6] of all the previous models. Results show that all three individual models perform well and the best performance is achieved by their ensemble, granting Priberam the 6-th place in the present challenge and making it the 2-nd best team.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A growing number of medical articles is published every year, with a current estimated rate of at least one new article every 26 seconds <ref type="bibr" coords="1,375.52,555.24,9.96,8.74" target="#b6">[7]</ref>. The large magnitude of both the documents and the assigned topics renders automatic classification algorithms a necessity in organising and providing relevant information. Search engines have a vital role in easing the burden of accessing this information efficiently, however, these usually rely on the manual indexing or tagging of articles, which is a slow and burdensome process <ref type="bibr" coords="1,313.04,615.02,9.96,8.74" target="#b7">[8]</ref>.</p><p>The Mesinesp task consists in automatically indexing abstracts in Spanish from two well-known medical databases, IBECS and LILACS, with tags from a pool of 34118 hierarchically structured medical terms, the DeCS codes. This trilingual vocabulary (English, Portuguese and Spanish) serves as a unique vocabulary in indexing medical articles. It follows a tree structure that divides the codes into broader classes and more refined sub-classes respecting their conceptual and semantic relationships <ref type="bibr" coords="2,274.15,190.72,9.96,8.74" target="#b8">[9]</ref>.</p><p>In this task, we tackle the extreme multi-label (XML) classification problem. Our goal is to predict for a given article the most relevant subset of labels from an extremely large label set (order of tens of thousands) using supervised training. <ref type="foot" coords="2,171.88,239.76,3.97,6.12" target="#foot_0">1</ref> Typical multi-label classification techniques are not suitable for the XML setting, due to its large computational requirements: the large number of labels implies that both label and feature vectors are sparse and exist in high-dimensional spaces; and to address the sparsity of label occurrence, a large number of training instances is required. These factors make the application of such techniques highly demanding in terms of time and memory, increasing the requirements of computational resources.</p><p>The Mesinesp task is even more challenging due to two reasons: first, the articles' labels must be predicted only from the abstracts and titles; and second, all the articles to be classified are in Spanish, which prevents the use of additional resources available only for English, such as BioBERT <ref type="bibr" coords="2,420.92,363.68,15.50,8.74" target="#b10">[11]</ref> and Clin-icalBERT <ref type="bibr" coords="2,180.57,375.64,14.61,8.74" target="#b11">[12]</ref>.</p><p>This paper describes our participation at the BioASQ task Mesinesp. We explore the performance of a one-vs.-rest model based on Support Vector Machines (SVM) <ref type="bibr" coords="2,168.52,414.30,10.52,8.74" target="#b2">[3]</ref> as well as that of a proprietary search engine, Priberam Search <ref type="bibr" coords="2,467.30,414.30,9.96,8.74" target="#b3">[4]</ref>, which relies on inverted indexes combined with a k-nearest neighbours classifier. Furthermore, we took advantage of BERT's contextualised embeddings <ref type="bibr" coords="2,450.44,438.21,10.52,8.74" target="#b4">[5]</ref> and tested three possible classifiers: a linear classifier; a label attention mechanism that leverages label semantics; and a recurrent model that predicts a sequence of labels according to their frequency. We propose the following contributions:</p><p>-Application of BERT's contextualised embeddings to the task of XML classification, including the exploration of linear, attention based and recurrent classifiers. To the best of our knowledge, this work is the first to apply a pretrained BERT model combined with a recurrent network to the XML classification task. -Empirical comparison of a simple one-vs.-rest SVM approach with a more complex model combining a recurrent classifier and BERT embeddings. -An ensemble of the previous individual methods using SVM-rank, which was capable of outperforming them.</p><p>Currently, there are two main approaches to XML: embedding based methods and tree based methods. Embedding based methods deal with the problem of high dimensional feature and label vectors by projecting them onto a lower dimensional space <ref type="bibr" coords="3,430.67,176.61,11.49,8.74" target="#b7">[8,</ref><ref type="bibr" coords="3,442.16,176.61,11.49,8.74" target="#b12">13]</ref>. During prediction, the compressed representation is projected back onto the space of high dimensional labels. This information bottleneck can often reduce noise and allow for a way of regularising the problem. Although very efficient and fast, this approach assumes however that the low-dimensional space is capable of encoding most of the original information. For real world problems, this assumption is often too restrictive and may result in decreased performance.</p><p>Tree based approaches intend to learn a hierarchy of features or labels from the training set <ref type="bibr" coords="3,206.60,272.25,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="3,223.69,272.25,11.62,8.74" target="#b14">15]</ref>. Typically, a root node is initialised with the complete set of labels and its children nodes are recursively partitioned until all the leaf nodes contain a small number of labels. During prediction, each article is passed along the tree and the path towards its final leaf node defines the predicted set of labels. These methods tend to be slower than embedding based methods but achieve better performance. However, if a partitioning error is made near the top of the tree, its consequences are propagated to the lower levels.</p><p>Furthermore, other methods should be referred due to their simple approach capable of achieving competitive results. Among these, DiSMEC <ref type="bibr" coords="3,431.93,367.89,15.50,8.74" target="#b9">[10]</ref> should be highlighted because it follows a one-vs.-rest approach which simply learns a weight vector for each label. The multiplication of such weight vector with the data point feature vector yields a score that allows the classification of the label. Another simple approach consists of performing a set of random projections from the feature space towards a lower dimension space where, for each test data point, a k-nearest neighbours algorithm performs a weighted propagation of the neighbour's labels, based on their similarity <ref type="bibr" coords="3,357.01,451.57,14.61,8.74" target="#b15">[16]</ref>.</p><p>We propose two new approaches which are substantially distinct from the ones discussed above. The first one uses a search engine based on inverted indexing and the second leverages BERT's contextualised embeddings combined with either a linear or recurrent layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">XML Classification Models</head><p>We explore the performance of a one-vs.-rest SVM model in §3.1, and a customised search engine (Priberam Search) in §3.2. We further experiment with several classifiers leveraging BERT's contextualised embeddings in §3.3. In the end we aggregate the predictions of all of these individual models using a SVM-Rank algorithm in §3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Support Vector Machine</head><p>Our first baseline consists of a simple Support Vector Machine (SVM) using a one-vs.-rest strategy. We train an independent SVM classifier for each possible label. To reduce the burden of computation we only consider labels with frequency above a given threshold f min . Each classifier weight w ∈ R d measures the importance assigned to each feature representation of a given article and is trained to optimise the max-margin loss of the support vectors and the hyper plane</p><formula xml:id="formula_0" coords="4,161.33,165.24,319.26,68.91">x i ∈ R d [3]: min w 1 2 ww T + C l i=1 ξ(w; x i , y i ) (1) s.t. y i (w x i + b ≥ 1 -ξ i )</formula><p>where (x i , y i ) are the article-label pairs, C is the regularisation parameter, b is a bias term and ξ corresponds to a slack function used to penalise incorrectly classified points and w is the vector normal to the decision hyper-plane. We used the abstract's term frequency-inverse document frequency (tf-idf) as features to represent x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Priberam Search</head><p>The second model consists of a customised search engine, Priberam Search, based on inverted indexing and retrieval using the Okapi-BM25 algorithm <ref type="bibr" coords="4,424.37,357.49,9.96,8.74" target="#b3">[4]</ref>. It uses an additional k-nearest neighbours algorithm (k-NN) to obtain the set of k indexed articles closest to a query article in feature space. This similarity is based on the frequency of words, lemmas and root-words, as well as label semantics and synonyms. A score is given to each one of these articles and to each one of their labels and label synonyms, and a weighted sum of these scores yields the final score assigned to each label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">XML BERT Classifier</head><p>Language model pretraining has recently advanced the state of the art in several Natural Language Processing tasks, with the use of contextualised embeddings such as BERT, Bidirectional Encoder Representations from Transformers <ref type="bibr" coords="4,467.30,504.19,9.96,8.74" target="#b4">[5]</ref>. This model consists of 12 stacked transformer blocks and its pretraining is performed on a very large corpus following two tasks: next sentence prediction and masked language modelling. The nature of the pretraining tasks makes this model ideal for representing sentence information (given by the representation of the [CLS] token added to the beginning of each sentence). After encoding a sentence with BERT, we apply different classifiers, and fine tune the model to minimise a multi-label classification loss:</p><formula xml:id="formula_1" coords="4,171.95,610.01,308.64,9.68">BCELoss(x i ; y i ) = y i,j log σ(x i,j ) + (1 -y i,j ) log(1 -σ(x i,j )),<label>(2)</label></formula><p>where y i,j denotes the binary value of label j of article i, which is 1 if it is present and 0 otherwise, x i,j represents the label predictions (logits) of article i and label j, and σ is the sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">In-domain transfer knowledge</head><p>Additionally, we performed an extra step of pretraining. Starting from the original weights obtained from BERT pretrained in Spanish, we further pretrained the model with a task of masked language modelling on the corpus composed by all the articles in the training set. This extra step results in more meaningful contextualised representations for this medical corpus, whose domain specific language might differ from the original pretraining corpora.</p><p>After this, we tested three different classifiers: a linear classifier in §3.3.2, a linear classifier with label attention in §3.3.3 and a recurrent classifier in §3.3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">XML BERT Linear Classifier</head><p>The first and simplest classifier consists of a linear layer which maps the sequence output (the 768 dimensional embedding -corresponding to the [CLS] token) to the label space, composed by 33702 dimensions corresponding to all the labels found in the training set. Such architecture is represented in figure <ref type="figure" coords="5,293.82,328.25,3.87,8.74" target="#fig_0">1</ref>. We minimise binary cross-entropy using sigmoid activations to allow for multiple active labels per instance, see Eq. 2. This classifier is hereafter designated Linear. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">XML BERT With Label Attention</head><p>For the second classifier, we assume a continuous representation with 768 dimensions for each label. We initialise label embeddings as the pooled output embeddings (corresponding to the [CLS] token) of a BERT model whose inputs were the string descriptors and synonyms for each label. We consider a key-query-value attention mechanism <ref type="bibr" coords="5,159.22,596.34,14.61,8.74" target="#b16">[17]</ref>, where the query corresponds to the pooled output of the abstract's contextualised representation and the keys and values correspond to the label embeddings. We further consider residual connections, and a final linear layer maps these results to the decision space of 33702 labels using a linear classifier, as shown in figure <ref type="figure" coords="5,217.25,644.16,3.87,8.74">2</ref>. Once again, we choose a binary cross-entropy loss (Eq.2). This classifier is hereafter designated Label attention.</p><p>Fig. <ref type="figure" coords="6,153.45,253.49,3.87,8.74">2</ref>: XML BERT with Label Attention Classifier: Article's pooled output (blue) is followed by an extra step of attention over the label embeddings (red) which are finally mapped to a XML linear classifier over labels (green).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">XML BERT With Gated Recurrent Unit</head><p>In the last classifier, we predict the article's labels sequentially. Before the last linear classifier used to project the final representation onto the label space, we add a Gated Recurrent Unit (GRU) network <ref type="bibr" coords="6,231.74,354.03,15.50,8.74" target="#b17">[18]</ref> with 768 units that sequentially predicts each label according to label frequency. A flowchart of the architecture is shown in figure <ref type="figure" coords="6,134.77,377.94,3.87,8.74">3</ref>. This sequential prediction is performed until the prediction of the stopping label is reached. Fig. <ref type="figure" coords="6,153.45,508.06,3.87,8.74">3</ref>: XML BERT GRU Classifier: The GRU network precedes the linear layer and sequentially predicts the labels. The symbol + + stands for vector concatenation and l t is the label representation predicted by the GRU at time-step t.</p><p>We consider a binary cross-entropy loss with two different approaches. On the first approach, all labels are sequentially predicted and the loss is computed only after the stopping label is predicted, i.e., the loss value is independent of the order in which the labels are predicted. It only takes into account the final set. This loss is denominated Bag of Labels loss (BOLL) and it is given by:</p><formula xml:id="formula_2" coords="6,247.36,656.09,233.23,9.68">L BOLL = BCELoss(x i ; y i )<label>(3)</label></formula><p>where x i and y i are the total set of predicted logits and gold labels for the current article i, correspondingly. The models trained with this loss are hereafter designated Gru Boll.</p><p>The second approach uses an iterative loss which is computed at each step of the sequential prediction of labels. We compare each predicted label with the gold label, the loss is computed and added to a running loss value. In this case, the loss is denominated Iterative Label loss (ILL):</p><formula xml:id="formula_3" coords="7,237.01,215.14,239.33,23.20">L ILL = t∈T BCELoss(x (t) i ; y (t) i ) (<label>4</label></formula><formula xml:id="formula_4" coords="7,476.34,218.27,4.24,8.74">)</formula><p>where T is the length of the label sequence, t denotes the time-steps taken by the GRU until the "stop label" is predicted, and x (t) i and y (t) i are the predicted logits and gold labels for time-step t and article i, respectively. Models trained with this loss are hereafter designated Gru Ill.</p><p>Although only one of the losses accounts directly for prediction order, this factor is always relevant because it affects the final set of predicted labels. This way, the model must be trained and tested assuming a specific label ordering. For this work, we used two orders: ascending and descending label frequency on the training set, designated Gru ascend and Gru descend, respectively.</p><p>Additionally, we developed a masking system to force the sequential prediction of labels according to the chosen frequency order. This means that at each step the output label set is reduced to all labels whose frequency fall bellow or above the previous label, depending on the monotonically ascending or descending order, respectively. Models in which such masking is used are designated Gru w/ mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ensemble</head><p>Furthermore, we developed an ensemble model combining the results of the previously described SVM, Priberam Search and BERT with GRU models. This ensemble's main goal is to leverage the label scores yielded by these three individual models in order to make a more informed decision regarding the relevance of each label to the abstracts.</p><p>We chose an ensembling method based on a SVM-rank algorithm <ref type="bibr" coords="7,440.46,547.31,10.52,8.74" target="#b5">[6]</ref> whose features are the normalised scores yielded by the three individual models, as well as their pairwise product and full product. These scores are the distance to the hyper-plane in the SVM model, the k-nearest neighbours score for Priberam Search and the label probability for the BERT model.</p><p>An SVM-rank is a variant of the support vector machine algorithm used to solve ranking problems <ref type="bibr" coords="7,237.63,620.25,14.61,8.74" target="#b18">[19]</ref>. It essentially leverages pair-wise ranking methods to sort and score results based on their relevance for a specific query. This algorithm optimises an analogous loss to the one shown in Eq. 1. Such ensemble is hereafter designated SVM-rank ensemble.</p><p>We consider the training set provided for the Mesinesp competition containing 318658 articles with at least one DeCS code and an average of 8.12 codes per article. We trained the individual models with 95% of this data. The remaining 5% were used to train the SVM-rank algorithm. The provided smaller official development set, with 750 samples, was used to fine-tune the individual model's and ensemble's hyper-parameters, while the test set, with 500 samples, was used for reporting final results. These two sets were manually annotated by experts specifically for the MESINESP task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Support Vector Machine</head><p>For the SVM model we chose to ignore all labels that appeared in less than 20 abstracts. With this cutoff, we decrease the output label set size to ≈ 9200. Additionally, we use a linear kernel to reduce computation time and avoid overfitting, which is critical to train such a large number of classifiers. Regarding regularisation, we obtained the best performance using a regularisation parameter set to C = 1.0, and a squared hinge slack function whose penalty over the misclassified data points is computed with an 2 distance.</p><p>Furthermore, to enable more control over the classification boundary, after solving the optimisation problem we moved the decision hyper-plane along the direction of w. We empirically determined that a distance of -0.3 from its original position resulted in the best µF 1 score. This model was implemented using a scikit-learn. <ref type="foot" coords="8,219.86,406.34,3.97,6.12" target="#foot_1">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Priberam Search</head><p>To use the Priberam Search Engine, we first indexed the training set taking into account the abstract text, title, complete set of gold DeCS codes, and also their corresponding string descriptors along with some synonyms provided<ref type="foot" coords="8,456.52,480.06,3.97,6.12" target="#foot_2">3</ref> . We tuned the number of neighbours k = <ref type="bibr" coords="8,308.48,493.59,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="8,325.63,493.59,12.73,8.74" target="#b19">20,</ref><ref type="bibr" coords="8,340.02,493.59,12.73,8.74">30,</ref><ref type="bibr" coords="8,354.41,493.59,12.73,8.74">40,</ref><ref type="bibr" coords="8,368.80,493.59,12.73,8.74">50,</ref><ref type="bibr" coords="8,383.20,493.59,12.73,8.74">60,</ref><ref type="bibr" coords="8,397.59,493.59,12.73,8.74">70,</ref><ref type="bibr" coords="8,411.98,493.59,17.71,8.74">100,</ref><ref type="bibr" coords="8,431.35,493.59,17.71,8.74">200]</ref> in the development set for the k-NN algorithm and obtained the best results for k = 40. To decide whether or not a label should be assigned to an article, we finetuned a score threshold over the interval [0.1, 0.5] using the official development set, obtaining a best performing value of 0.24. All labels with score above the threshold were picked as correct labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">BERT</head><p>For all types of BERT classifiers, we used the Transformers and PyTorch Python packages <ref type="bibr" coords="9,175.78,155.19,15.50,8.74" target="#b19">[20,</ref><ref type="bibr" coords="9,192.57,155.19,11.62,8.74" target="#b20">21]</ref>.</p><p>We initialised BERT's weights from its cased version pretrained on Spanish corpora, bert-base-spanish-wwm-cased <ref type="foot" coords="9,302.22,178.38,3.97,6.12" target="#foot_3">4</ref> .</p><p>We further performed a pretraining on the Mesinesp dataset to obtain better in domain embeddings. For the pretraining and classification task, table <ref type="table" coords="9,447.46,204.73,4.98,8.74" target="#tab_0">1</ref> shows the training hyper-parameters.</p><p>For all the experiments with BERT, the complete set of DeCS codes was considered as the label set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ensemble</head><p>Our ensemble model aggregates the prediction of all the individual contenders and produces a final predicted label set. To improve recall we lowered the thresholds set for each individual model until the value for which the average number of predicted labels per abstract was approximately double the average number of gold labels. This ensures that the SVM-rank algorithm was trained with a balanced set, resulting in a system in which the individual models have very high recall and the ensemble model is responsible for precision. We trained the SVM-rank model with the 5% hold-out data of the training set. Furthermore, SVM-rank returns a score for each label in each abstract, making it necessary to define a threshold for classification. This threshold was fine-tuned over the interval [-0.5, 0.5] using the official Mesinesp development set, yielding a best performing cut-off score of -0.0233.</p><p>We also fine-tuned the regularisation parameter, C. We experimented the values C = [0.01, 0.1, 0.5, 1, 5, 10] obtaining the best performance for C = 0.1. The current model was implemented using a Python wrapper for the dlib C++ toolkit <ref type="bibr" coords="9,166.87,633.54,14.61,8.74" target="#b21">[22]</ref>.</p><p>Table <ref type="table" coords="10,161.73,141.48,4.98,8.74" target="#tab_1">2</ref> shows the µ-precision, µ-recall and µ-F1 metrics for the best performing models described above, evaluated on both the official development and test sets.</p><p>The comparison between the scores obtained for the one-vs.-rest SVM and Priberam Search models shows that the SVM outperforms the k-NN based Priberam Search in terms of µF1, which is mostly due to its higher recall. Note that, although not ideal for multi-label problems, the one-vs.-rest strategy fro the SVM model was able to achieve a relatively good performance, even when the predicted label set was significantly reduced. Table <ref type="table" coords="10,177.51,428.97,4.98,8.74" target="#tab_3">3</ref> shows the performance of several classifiers used with BERT. Note that, for these models, in order to save time and computational resources some tests were stopped before achieving their maximum performance, allowing nonetheless comparison with other models.</p><p>We trained linear classifiers using the BERT model with pretraining on the MESINESP corpus for 660k steps (≈ 19 epochs) and without such pretraining (marked with *). Results show that, even with an under-trained classifier, such pretraining is already advantageous. This pretraining was employed for all models combining BERT embeddings with a GRU classifier. The label-attentive Bert model (Gru Boll ascend) shows negligible impact on performance, when compared with the simple linear classifier (Linear).</p><p>We consider three varying architectures of the Bert-Gru model: Bag of Labels loss (Boll) or Iterative Label loss (Ill), ascending or descending label frequency, and usage or not of masking. Taking into account the best score achieved, the BOLL loss performs better than the ILL loss, even with a smaller number of training steps. For this BOLL loss, it is also evident that the ordering of labels with ascending frequency outperforms the opposite order, and that masking results in decreased performance.</p><p>On the other hand, for the ILL loss, masking improves the achieved score and the ordering of labels with descending frequency shows better results. The best classifier for a BERT-based model is the GRU network trained with a Bag of Labels loss and with labels provided in ascending frequency order (Gru Boll ascend). This model was further trained for a total of 28 epochs resulting in a µF1=0.4918 on the 5% hold-out of the training set. It is important to notice the performance drop from the 5% hold-out data to the official development set. This drop is likely a result of the mismatch between the annotation methods used in the two sets, given that the development set was specifically manually annotated for this task.</p><p>Surprisingly, the BERT based model shows worse performance than the SVM on the test set. Despite their very similar µF1 scores for the development set, the BERT-GRU model suffered a considerable performance drop from the development to the test set due to a decrease in recall. This might indicate some over-fitting of hyper-parameters and a possible mismatch between these two expert annotated sets.</p><p>Additionally, as made explicit in  Finally, table <ref type="table" coords="11,212.61,608.30,4.98,8.74">4</ref> shows additional classification metrics for each one of the submitted systems, as well as their rank within the Mesinesp task. The analysis of such results makes clear that for the three considered averages (Micro, Macro and per sample), the SVM model shows the best recall score. For most of the remaining metrics, the SVM-rank ensemble is able to leverage the capabilities of the individual models and achieve considerable performance gains, particularly noticeable for the precision scores. Table <ref type="table" coords="12,162.25,339.34,3.87,8.74">4</ref>: Micro (µ), macro (Ma) and per sample (Eb) averages of the precision, recall and F1 scores, followed by score position within the Mesinesp task. For each metric, the best performing model is identified in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper introduces three type of extreme multi label classifiers: an SVM, a k-NN based search engine and a series of BERT classifiers. Our one-vs.-rest SVM model shows the best performance on all recall metrics. We further provide an empirical comparison of different variants of multi-label BERT based classifiers, where the Gated Recurrent Unit network with the Bag of Labels loss shows the most promising results. This model yields slightly better results than the SVM model on the development set, however, due to a drop in recall, under-performs it on the test set. Finally, the SVM-rank ensemble is able to leverage the label scores yielded by the three individual models and combine them into a final ranking model with a precision gain on all metrics, capable of achieving the highest µF1 score (being the 6-th best model in the task).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,468.44,345.83,8.74;5,134.77,480.40,345.82,8.74;5,169.35,396.54,276.67,60.38"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: XML BERT Linear Classifier: Flowchart representing BERT's pooled output (in blue) and the simple linear layer (W in green) used as XML classifier.</figDesc><graphic coords="5,169.35,396.54,276.67,60.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,134.77,278.79,345.82,107.43"><head>Table 1 :</head><label>1</label><figDesc>Training hyper-parameters used for BERT's pretraining and classification tasks.</figDesc><table coords="9,214.82,278.79,185.72,81.11"><row><cell>Hyper-parameter</cell><cell cols="2">Pretraining Classification</cell></row><row><cell>Batch size</cell><cell>4</cell><cell>8</cell></row><row><cell>Learning rate</cell><cell>5 • 10 -5</cell><cell>2 • 10 -5</cell></row><row><cell>Warmup steps</cell><cell>0</cell><cell>4000</cell></row><row><cell>Max seq lenght</cell><cell>512</cell><cell>512</cell></row><row><cell>Learning rate decay</cell><cell>-</cell><cell>linear</cell></row><row><cell>Dropout probability</cell><cell>0.1</cell><cell>0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,134.77,262.00,363.85,128.23"><head>Table 2 :</head><label>2</label><figDesc>Micro precision (µP), micro recall (µR) and micro F1 (µF1) obtained with the 4 submitted models for both the development and test sets. For each metric, the best performing model is identified in bold.</figDesc><table coords="10,134.77,262.00,363.85,90.46"><row><cell>Model</cell><cell cols="3">Development set</cell><cell></cell><cell>Test set</cell><cell></cell></row><row><cell></cell><cell>µP</cell><cell>µR</cell><cell>µF1</cell><cell>µP</cell><cell>µR</cell><cell>µF1</cell></row><row><cell>SVM</cell><cell>0.4216</cell><cell>0.3740</cell><cell>0.3964</cell><cell>0.4183</cell><cell>0.3789</cell><cell>0.3976</cell></row><row><cell>Priberam Search</cell><cell>0.4471</cell><cell>0.3017</cell><cell>0.3603</cell><cell>0.4571</cell><cell>0.2700</cell><cell>0.3395</cell></row><row><cell>Bert-Gru Boll ascend</cell><cell>0.4130</cell><cell>0.3823</cell><cell>0.3971</cell><cell>0.4293</cell><cell>0.3314</cell><cell>0.3740</cell></row><row><cell>SVM-rank ensemble</cell><cell>0.5056</cell><cell>0.3456</cell><cell>0.4105</cell><cell>0.5336</cell><cell>0.3320</cell><cell>0.4093</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,134.77,288.21,345.83,228.15"><head></head><label></label><figDesc>table 2, the ensemble combining the results of the SVM, Priberam Search and the best performing BERT based classifier achieved the best performance on the development set, outperforming all the individual models.</figDesc><table coords="11,187.33,361.53,233.73,154.83"><row><cell>BERT classifier</cell><cell>Training steps</cell><cell>µF1</cell></row><row><cell>Linear*</cell><cell>220k</cell><cell>0.4476</cell></row><row><cell>Linear</cell><cell>250k  †</cell><cell>0.4504</cell></row><row><cell>Label attention*</cell><cell>700k</cell><cell>0.4460</cell></row><row><cell>Gru Boll ascend</cell><cell>80k</cell><cell>0.4759</cell></row><row><cell>Gru Boll descend</cell><cell>40k</cell><cell>0.4655</cell></row><row><cell>Gru Boll ascend w/ mask</cell><cell>100k  †</cell><cell>0.4352</cell></row><row><cell>Gru Ill descend</cell><cell>240k  †</cell><cell>0.4258</cell></row><row><cell>Gru Ill descend w/ mask</cell><cell>240k  †</cell><cell>0.4526</cell></row><row><cell>Gru Ill ascend w/ mask</cell><cell>240k  †</cell><cell>0.4459</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,134.77,521.48,345.83,45.16"><head>Table 3 :</head><label>3</label><figDesc>µF1 metric evaluated for the 5% hold-out of the training set. All models have been pretrained on the Mesinesp corpus, except for those duly marked. BOLL: Bag of Labels loss. ILL: Iterative Label loss. *: not pretrained on Mesinesp corpus. †: training stopped before maximum µF1 was reached.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,634.88,335.86,7.86;2,144.73,645.84,335.86,7.86;2,144.73,656.80,182.11,7.86"><p>The task of multi-label classification differs from multi-class classification in that labels are not exclusive, which enables the assignment of several labels to the same article, making the problem even harder<ref type="bibr" coords="2,309.94,656.80,13.52,7.86" target="#b9">[10]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="8,144.73,634.88,59.74,7.86"><p>scikit-learn.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="8,144.73,646.48,325.30,7.47;8,144.73,657.44,32.95,7.47"><p>https://temu.bsc.es/mesinesp/wp-content/uploads/2019/12/DeCS.2019.v5. tsv.zip</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="9,144.73,657.44,155.34,7.47"><p>https://github.com/dccuchile/beto</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>This work is supported by the <rs type="funder">Lisbon Regional Operational Programme</rs> (<rs type="programName">Lisboa 2020</rs>), under the <rs type="programName">Portugal 2020 Partnership Agreement</rs>, through the <rs type="funder">European Regional Development Fund (ERDF)</rs>, within project <rs type="projectName">TRAINER</rs> (<rs type="grantNumber">N o 045347</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GVXpCnP">
					<orgName type="program" subtype="full">Lisboa 2020</orgName>
				</org>
				<org type="funded-project" xml:id="_MxkdkRH">
					<idno type="grant-number">N o 045347</idno>
					<orgName type="project" subtype="full">TRAINER</orgName>
					<orgName type="program" subtype="full">Portugal 2020 Partnership Agreement</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="13,138.35,139.90,342.24,7.86;13,146.91,150.86,327.21,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,210.44,139.90,270.15,7.86;13,146.91,150.86,24.44,7.86">A comparative study of utilizing topic models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,177.91,150.86,181.51,7.86">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="29" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,161.28,342.25,7.86;13,146.91,172.24,242.19,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="13,316.78,161.28,163.82,7.86;13,146.91,172.24,73.43,7.86">Extreme Multi-label Classification from Aggregated Labels</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00198</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,138.35,182.66,342.24,7.86;13,146.91,193.62,261.25,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,351.73,182.66,128.86,7.86;13,146.91,193.62,75.41,7.86">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,228.95,193.62,147.98,7.86">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,204.04,342.24,7.86;13,146.91,215.00,333.68,7.86;13,146.91,225.96,75.54,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,194.60,215.00,179.01,7.86">Automated Fact Checking in the News Room</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Secker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mitchel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Marinho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,392.80,215.00,87.79,7.86;13,146.91,225.96,44.32,7.86">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,236.38,342.24,7.86;13,146.91,247.34,333.68,7.86;13,146.91,258.30,333.67,7.86;13,146.91,269.26,28.16,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,316.80,236.38,163.79,7.86;13,146.91,247.34,174.21,7.86">BERT: pretraining of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,330.34,247.34,150.25,7.86;13,146.91,258.30,333.67,7.86">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,279.68,342.24,7.86;13,146.91,290.64,333.68,7.86;13,146.91,301.60,79.87,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,200.32,279.68,204.83,7.86">Optimizing search engines using clickthrough data</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,412.95,279.68,67.64,7.86;13,146.91,290.64,333.68,7.86;13,146.91,301.60,48.64,7.86">InProceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,312.02,342.24,7.86;13,146.91,322.98,333.68,7.86;13,146.91,333.94,28.16,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,390.49,312.02,90.10,7.86;13,146.91,322.98,181.02,7.86">Proliferations of scientific medical journals: a burden or a blessing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Garba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Makama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Odigie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,335.21,322.98,89.44,7.86">Oman medical journal</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">311</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,344.36,342.24,7.86;13,146.91,355.32,317.89,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,289.36,344.36,136.18,7.86">Deep extreme multi-label learning</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,432.52,344.36,48.07,7.86;13,146.91,355.32,286.65,7.86">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,365.74,342.24,7.86;13,146.91,376.70,276.78,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="13,252.50,365.74,46.25,7.86">Red.bvsalud</title>
		<ptr target="http://red.bvsalud.org/decs/en/about-decs/" />
		<imprint>
			<date type="published" when="2020-05-02">2020. 2 May 2020</date>
			<publisher>VHL Network Portal</publisher>
		</imprint>
	</monogr>
	<note>Decs</note>
</biblStruct>

<biblStruct coords="13,142.62,387.12,337.98,7.86;13,146.91,398.08,333.67,7.86;13,146.91,409.04,129.94,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,247.45,387.12,233.14,7.86;13,146.91,398.08,73.46,7.86">DiSMEC: Distributed Sparse Machines for Extreme Multilabel Classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Babbar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,226.17,398.08,254.42,7.86;13,146.91,409.04,98.70,7.86">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,419.46,337.97,7.86;13,146.91,430.42,333.67,7.86;13,146.91,441.38,59.05,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,383.13,419.46,97.45,7.86;13,146.91,430.42,275.25,7.86">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,429.25,430.42,51.34,7.86;13,146.91,441.38,10.29,7.86">Bioinformatics</title>
		<imprint>
			<date type="published" when="2020-02">2020 Feb</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,451.80,337.97,7.86;13,146.91,462.76,333.68,7.86;13,146.91,473.72,28.16,7.86" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jin</forename><forename type="middle">D</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03323</idno>
		<title level="m" coord="13,161.31,462.76,180.36,7.86">Publicly available clinical BERT embeddings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.62,484.14,337.97,7.86;13,146.91,495.10,114.45,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,214.44,484.14,262.00,7.86">Multilabel classification with principal label space transformation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,146.91,495.10,83.21,7.86">Neural Computation</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,505.52,337.98,7.86;13,146.91,516.48,333.68,7.86;13,146.91,527.44,242.27,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,243.49,505.52,237.10,7.86;13,146.91,516.48,105.53,7.86">Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,260.30,516.48,220.29,7.86;13,146.91,527.44,211.04,7.86">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,537.86,337.98,7.86;13,146.91,548.82,333.67,7.86;13,146.91,559.78,213.31,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,333.34,537.86,147.25,7.86;13,146.91,548.82,232.53,7.86">Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,385.87,548.82,94.72,7.86;13,146.91,559.78,182.09,7.86">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,570.20,337.97,7.86;13,146.91,581.16,161.61,7.86" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Verma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08140</idno>
		<title level="m" coord="13,192.69,570.20,283.89,7.86">An Embarrassingly Simple Baseline for eXtreme Multi-label Prediction</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.62,591.58,337.98,7.86;13,146.91,602.54,333.68,7.86;13,146.91,613.50,123.80,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,203.33,602.54,98.34,7.86">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,309.02,602.54,171.57,7.86;13,146.91,613.50,29.48,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,623.92,337.97,7.86;13,146.91,634.88,333.68,7.86;13,146.91,645.84,333.67,7.86;13,146.91,656.80,149.39,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,172.99,634.88,307.60,7.86;13,146.91,645.84,81.12,7.86">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,234.97,645.84,245.62,7.86;13,146.91,656.80,118.15,7.86">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,119.67,337.98,7.86;14,146.91,130.63,56.06,7.86" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="14,188.28,119.67,169.82,7.86">Learning to rank for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,141.59,337.97,7.86;14,146.91,152.55,333.68,7.86;14,146.91,163.51,145.80,7.86" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="14,267.52,152.55,213.07,7.86;14,146.91,163.51,82.56,7.86">HuggingFace&apos;s Transformers: State-of-the-art Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

<biblStruct coords="14,142.62,174.47,337.97,7.86;14,146.91,185.43,333.67,7.86;14,146.91,196.39,224.62,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,443.83,174.47,36.76,7.86;14,146.91,185.43,249.61,7.86">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,403.03,185.43,77.56,7.86;14,146.91,196.39,128.89,7.86">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,207.34,337.98,7.86;14,146.91,218.30,67.02,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="14,193.44,207.34,144.09,7.86">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,344.48,207.34,136.12,7.86;14,146.91,218.30,35.78,7.86">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
