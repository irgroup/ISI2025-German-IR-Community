<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.86,116.04,343.63,13.99;1,155.95,133.97,303.45,13.99;1,264.52,151.90,86.32,13.99">Overview of the 2020 ImageCLEFdrawnUI Task: Detection and Recognition of Hand Drawn Website UIs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,149.41,189.57,64.80,9.69"><forename type="first">Dimitri</forename><surname>Fichou</surname></persName>
							<email>dimitri.fichou@teleporthq.io</email>
							<affiliation key="aff0">
								<orgName type="department">teleportHQ</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.77,189.57,50.98,9.69"><forename type="first">Raul</forename><surname>Berari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">teleportHQ</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,286.30,189.57,41.26,9.69"><forename type="first">Paul</forename><surname>Brie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">teleportHQ</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,338.13,189.57,63.28,9.69"><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,411.96,189.57,53.98,9.69;1,181.09,201.53,27.41,9.69"><forename type="first">Liviu</forename><forename type="middle">Daniel</forename><surname>Åžtefan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.06,201.53,111.98,9.69"><forename type="first">Mihai</forename><forename type="middle">Gabriel</forename><surname>Constantin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.97,201.53,68.82,9.69"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
							<email>bogdan.ionescu@upb.ro</email>
							<affiliation key="aff1">
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.86,116.04,343.63,13.99;1,155.95,133.97,303.45,13.99;1,264.52,151.90,86.32,13.99">Overview of the 2020 ImageCLEFdrawnUI Task: Detection and Recognition of Hand Drawn Website UIs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BE9426A9743532D6BD26D1DC4C054BD3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays, companies' online presence and user interfaces are critical for their success. However, such user interfaces involve multiple actors. Some of them, like project managers, designers or developers, are hard to recruit and train, making the process slow and prone to errors. There is a need for new tools to facilitate the creation of user interfaces. In this context, the detection and recognition of hand drawn website UIs task was run in its first edition with ImageCLEF 2020. The task challenged participants to provide automatic solutions for annotating different user interfaces elements, e.g., buttons, paragraphs and checkboxes, starting from their hand drawn wireframes. Three teams submitted a total of 18 runs using different object detection techniques and all teams obtained better scores compared to the recommended baseline. The best run in terms of mAP 0.5 IoU obtained a score of 0.793 compared to the baseline score of 0.572. The leading overall precision score was 0.970, compared to the baseline score of 0.947. In this overview working notes paper, we present in detail the task and these results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, there has been a growing interest in data-driven approaches to help user interface (UI) professionals. For instance, Deka et al. <ref type="bibr" coords="1,428.65,507.55,10.52,9.69" target="#b5">[6]</ref> collected the RICO data set containing 72,219 layouts mined from 9,722 mobile applications. Its usefulness was proven by implementing an auto-encoder which queried UIs and retrieved similar layouts. This data set was further used to create the SWIRE data set <ref type="bibr" coords="1,214.40,555.37,9.97,9.69" target="#b8">[9]</ref>, consisting of 3,802 hand drawn UIs. Here, the authors demonstrated the use of nearest neighbour search to retrieve similar UI based on sketch queries. An end-to-end approach was proposed by Bertelami with pix2code <ref type="bibr" coords="1,177.15,591.23,10.52,9.69" target="#b0">[1]</ref> and UI2code <ref type="bibr" coords="1,251.99,591.23,9.97,9.69" target="#b4">[5]</ref>, in both cases the authors used a Convolutional Neural Network (CNN) to encode pixels of a screenshot and a Recurrent Neural Network (RNN) to decode it into a domain specific language translatable into UI code. Online recognition of hand drawn gestures and strokes was also explored by different teams <ref type="bibr" coords="2,218.44,292.89,16.39,9.69" target="#b11">[12,</ref><ref type="bibr" coords="2,234.83,292.89,12.29,9.69" target="#b12">13,</ref><ref type="bibr" coords="2,247.12,292.89,12.29,9.69" target="#b14">15]</ref> to provide support for both mouse and touchpad inputs.</p><p>We proceed in this direction by organizing the first edition of the Detection and Recognition of Hand Drawn Website UIs task, ImageCLEFdrawnUI, with the ImageCLEF<ref type="foot" coords="2,205.85,339.99,3.97,6.80" target="#foot_0">3</ref> benchmarking campaign <ref type="bibr" coords="2,325.78,341.57,14.62,9.69" target="#b10">[11]</ref>, itself part of CLEF <ref type="foot" coords="2,434.92,339.99,3.97,6.80" target="#foot_1">4</ref> (CROSS Language Evaluation Forum). Following a similar format as the 2019 ImageCLE-Fcoral annotation task <ref type="bibr" coords="2,237.36,365.48,9.97,9.69" target="#b3">[4]</ref>, the task requires participants to perform automatic UI element annotation and localisation on hand drawn wireframes of websites and applications (Figure <ref type="figure" coords="2,243.28,389.39,3.88,9.69" target="#fig_0">1</ref>). In this overview working notes paper, we review the task and the submitted runs.</p><p>The rest of the paper is organized as follows: Section 2 presents the data set and how it was collected. Section 3 describes the evaluation methodology. The task results are reported in Section 4. Finally, Section 5 discusses conclusions and future work in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data set</head><p>The data set consists of a collection of paper-drawn website drawings, i.e., wireframes. Each data point represents an abstracted and simplified version of a real website or mobile application. Figure <ref type="figure" coords="2,302.48,538.24,4.98,9.69" target="#fig_0">1</ref> is an example of such wireframes, corresponding to the desktop version of a website. The data set is split into 2,363 images used for training and 587 used for testing. The average number of UI elements per image is 28, with a minimum of 4 and a maximum of 131 elements.</p><p>Creating and labeling such a large number of wireframes would not have been possible in the absence of a common standard. To tackle this, a convention providing 21 of the most common UI elements was provided (Figure <ref type="figure" coords="2,429.57,610.82,3.88,9.69" target="#fig_1">2</ref>), ensuring as such that both annotators and drawers followed a single source of truth. The 21 elements were selected after several discussions with developers and designers, the objective being to cover most use cases.</p><p>The convention was designed to minimize the level of ambiguity for the annotators and the machine learning models. However, wireframes require simple representations and creating such a standard will necessarily produce uncertainty in some edge-cases.</p><p>In a previous iteration of the data set, a series of assumptions were made about the model capacity for distinguishing between elements based only on their position and size. For example, headers and paragraphs were represented using the same squiggly line, with the former being bigger and at the top of the page, while the latter being smaller and disseminated throughout different sections of a layout. In practice, those assumptions were ambiguous for both the annotators and the model, and a visual distinction between the two was established by placing a hash at the start of a header element.</p><p>As a result, the current version of the standard offers a clear representation for each element, while still leaving space for further improvements (i.e. more complicated UI features).</p><p>The following is a set of short descriptions of the 21 UI element classes that can be found throughout the data set:</p><p>-Paragraph: One or multiple lines of handwritten text or horizontal squiggles. -Label: A single line of text (or a squiggly line), with the added constraint of being in the vicinity of an input (checkbox, radio button, toggle, text input, date picker, stepper input or slider). -Heading: One or multiple hashes (#) succeeded by a text or squiggly line.</p><p>-Link: A text or squiggly line enclosed in a pair of square brackets.</p><p>-Button: A small rectangular shape with a single line of text or a squiggly line centered inside its area. -Checkbox: A small square shape, with an X or a tick drawn inside its area.</p><p>-Radio Button: A small circle shape, optionally with a dot inside its area.</p><p>-Toggle: A small rectangle with one half of its area shaded. The rectangle can also possess rounded corners. -Rating: A collection of five star shapes aligned horizontally.</p><p>-Dropdown: A horizontal rectangle, with a V shape drawn in its right-most side. Optionally, the empty space on the left can contain text or a squiggly line. -Text Input: An empty horizontal rectangle.</p><p>-Date Picker: A horizontal rectangle where a dot shape is present in the right-most side and the space on the left is filled with a date text, which has to provide the slash character as a delimiter. For example, valid date texts include 02/03/04 or 20/12/20. -Stepper Input: A horizontal rectangle where the right-most area consists of two small rectangles distributed vertically, with the one on the top containing a caret (^) and the one on the bottom including a V-like shape. These shapes represent the control over the input, increasing or decreasing it by a predefined step. -Slider: A horizontal line with a small marker (such as a circle) between its ends. -Text Area: An empty rectangle with a small triangle shaded in its bottomright corner. -Image: A rectangle or a circle with an X spanning its whole area.</p><p>-Video: A rectangle with a small, right-pointing triangle centered inside its area. -Table : A rectangle subdivided into rectangular areas in a grid-like manner.</p><p>-Container: A rectangle shape which contains other classes of UI elements. Once the convention has been specified, the task of creating the data set could be split between two external teams of drawers and annotators, supplemented by overall supervision of the whole process by our team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Acquisition</head><p>The process was externalized to a team of three professional wireframe drawers. Drawing a wireframe requires recreating an already existing mobile or desktop layout. For the former, the drawers used data points selected from the RICO data set <ref type="bibr" coords="5,173.77,512.66,9.97,9.69" target="#b5">[6]</ref>, which include automatically-generated screenshots of Android applications. For the desktop layouts, a collection of screenshots was compiled by automatically processing a list of popular sites, using an in-house web parser.</p><p>The drawers followed the convention for representing UI elements when creating the wireframes. Omitted elements include the ones which did not fit into any description and those which would have cluttered the general layout. On top of this, the instructions encouraged clearly drawing each element, to ensure that annotation will proceed without difficulty. Drawers were asked to draw only the visible elements of a page. Consequently, UI elements which are concerned with providing the layout (such as containers or line breaks) were represented only when they also provided a clear, visual indication of their presence.</p><p>To diversify the data set, the drawers were asked to use different colors when drawing the wireframes. After drawing was done, each wireframe was  photographed in three different lighting conditions: bright, dark and scanned (Figure <ref type="figure" coords="6,170.68,516.68,3.88,9.69" target="#fig_3">4</ref>). Bright and dark versions of the wireframes were photographed using a Xiaomi Poco F1 smartphone. The scanned versions were obtained using a Canon MF240 scanner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Annotation</head><p>Image annotation was provided by a different team of three data annotators, following the same guidelines and using the desktop application VoTT<ref type="foot" coords="6,448.58,600.06,3.97,6.80" target="#foot_2">5</ref> as an interface (Figure <ref type="figure" coords="6,211.66,613.59,3.88,9.69" target="#fig_2">3</ref>). Each element is annotated using a rectangle shape which covers the object in its entirety, regardless of potential overlap with other elements. Creating an annotation requires two clicks for drawing the bounding In terms of quality control, the annotations were thoroughly verified and corrected by a single member of our team to ensure consistency. The process consisted in checking the alignment of the bounding boxes and rectifying erroneous labels. A small number of inconsistencies were removed as a result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data set analysis</head><p>As shown in Figures <ref type="figure" coords="7,230.07,444.52,4.98,9.69" target="#fig_4">5</ref> and<ref type="figure" coords="7,259.44,444.52,3.88,9.69" target="#fig_5">6</ref>, the number of certain types of elements drawn in wireframes reflect the different use-cases solved by user interfaces. Firstly, buttons and links represent the most common ways of navigating throughout different pages of an interface. Then, elements such as paragraphs, headers or images indicate the content-based nature of UIs. Lastly, elements which describe the layout of the page (such as containers and line breaks) represent 15% of the total number of elements found inside the data set. It must be noted that the distribution of those elements is difficult to compare with the distribution found in real websites and mobile applications, as there are multiple ways to program what appears in the screenshots.</p><p>Consequently, a single wireframe will contain information about website layouts (and implicitly, hierarchies), navigation and content by using a simple, abstracted representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation methodology</head><p>Evaluation was performed using three methods, the overall precision, mean average precision (mAP) and recall <ref type="bibr" coords="7,288.14,656.12,9.97,9.69" target="#b6">[7]</ref>. For all these metrics, each detection is required to be higher than 0.5 IoU to register as a true positive when compared to the ground truth. For mAP and recall, and adapted version of the cocoAPI was used <ref type="foot" coords="8,173.33,141.33,3.97,6.80" target="#foot_3">6</ref> .</p><p>-Overall Precision: It refers to the number of true positive predictions out of the total sum of true positives and false negatives. -mAP@0.5 IoU: The localised Mean Average Precision for each submission.</p><p>-Recall@0.5 IoU: The localised mean recall for each submission, Throughout the contest, overall precision was used as the sole metric for creating the leader board, to be consistent with the other ImageCLEF challenges such as ImageCLEF Coral. However, several discussions with the participants during the competition showed that Recall and Mean Average Precision are also necessary to properly judge the results, so they have been added afterwards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Three teams submitted a total of 18 runs. The task had a submission limit of 10 runs per team. Table <ref type="table" coords="8,241.88,338.36,4.98,9.69">4</ref> displays the overall precision, mean average precision and recall at 0.5 IoU for each run.</p><p>The baseline score was obtained by training a Faster R-CNN <ref type="bibr" coords="8,433.97,362.28,15.50,9.69" target="#b13">[14]</ref> model (with a resnet101 backbone), using Tensorflow's object detection API <ref type="bibr" coords="8,438.16,374.24,15.50,9.69" target="#b9">[10]</ref> on an Amazon Web Services EC2 instance. The instance was equipped with an nVidia K80 GPU, CUDA 10.0 and Python 3.6. In terms of hyperparameters, the batch size was set to 1, the number of steps to 100,000 and the resizing range was set between 600 and 800 pixels. Data augmentation was provided by the built-in techniques of random cropping, horizontal flips, color distortion, and conversion to gray-scale.</p><p>All the participants made use of deep neural networks specifically tailored for object detection, such as Mask R-CNN <ref type="bibr" coords="8,307.50,469.89,9.97,9.69" target="#b7">[8]</ref>, Cascade R-CNN <ref type="bibr" coords="8,398.75,469.89,10.52,9.69" target="#b2">[3]</ref> or YOLOv4 <ref type="bibr" coords="8,467.30,469.89,9.97,9.69" target="#b1">[2]</ref>. The baseline overall precision was surpassed by two of the teams. Furthermore, each team had at least one run which outperformed the baseline mAP and Recall scores. This confirms that this type of network remains the preferred standard for object detection and could be used for larger and more difficult projects in the domain of UI.</p><p>With regards to model settings, team OG_SouL improved their results by implementing a novel multi-pass inference technique for detecting smaller elements. After running the image through Mask R-CNN, the detected bounding boxes were filled with the color white and the image was passed once again to the model, essentially pressuring it into detecting some of the remaining elements. This method improved their mAP score from 0.573 to 0.641.</p><p>Data analysis played an important role in improving the accuracy of the results. The winning team, zip, performed a distribution analysis on the data set and split it according to an 11:1 ratio between train and validation, ensuring that Table <ref type="table" coords="9,163.48,116.11,4.13,8.83">1</ref>. Task results: computed overall precision, mAP@0.5 IoU and Recall@0.5 IoU for each run. The baselines and best values for each metric are in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>Run ID Method Overall Precision mAP@0.5 IoU R@0. the least common elements have not been omitted. The team OG_SouL computed a similarity score between pictures of the same UI layout found throughout the data set and removed them from training to prevent over-fitting.</p><p>Data set augmentation was also provided through a variety of methods. Team CudaMemError1 used YOLOv4 techniques such as CutMix or Mosaic, where different areas of the pictures are cropped and combined with others to produce synthetic data points. Team zip generated 500 new images containing the least common UI elements by cropping them from the original data set, applying affine transformations and pasting them on randomly sized papers which used a light color as background.</p><p>Finally, two of the teams used conversion to black and white or gray scale, taking advantage of the fact that drawn UI elements are agnostic to the color of the instrument used for representing them on paper. OG_SouL showcased an OpenCV RGB to BW pipeline, applying brightness refining, erosion, Otsu's binarization algorithm, Gaussian thresholding and noise removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and conclusions</head><p>An overall precision as high as 0.97 may implicate that the task has already been solved, but this metric does not properly account for a high number of false negatives or poor results on the rarer elements from the data set. Mean Average Precision and Recall are better ways of representing the performance of a model in this case. Accordingly, the best run achieved only 0.79 on the mAP@0.5 IoU measure, indicating that the techniques could still be further improved.</p><p>Since the drawn wireframes have been modeled after real applications and websites, the data set was skewed from the start towards certain classes of UI elements. This meant that the teams went at great lengths to compensate for the lack of uncommon elements. As a result, synthetic data was created either through YOLOv4's 'cut-and-paste' techniques or by cropping the least common elements and applying affine transformations to them. Conversion to gray-scale or black and white proved to be an efficient method for reducing the data set file size and improving the detection score.</p><p>The results and techniques presented by the teams are encouraging and show the untapped potential provided by combining machine learning with user interfaces. For the next editions of the task, we plan to tackle two, more difficult problems regarding UI element detection and processing.</p><p>The first problem consists in predicting the nested structure of a UI based on a wireframe drawing. While the current challenge assumed that UI elements' locations are identified by their absolute positioning, without any hierarchical relationship between them, real-life scenarios presuppose relative positioning and a hierarchy built out of different classes of elements. This particularly challenging task may be solved through a mix of natural language processing and computer vision.</p><p>The second problem would necessitate performing object detection on real screenshots instead of wireframes. In practice, designers often attach screenshots of similar layouts along with the wireframes, with the intention of giving the developer a more refined idea of the task at hand. A data set similar to the current one can be generated by parsing a number of websites, analysing their respective DOM trees and then screen-capturing the visible area. However, due to the nature of most websites found throughout the web, cleaning the whole data set can become a laborious and time-consuming problem. Consequently, manual cleaning will be provided only for the test set, while the training set will be offered in its raw form. The task will require the participants to filter through the data set on their own.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,181.59,235.05,252.17,8.83;2,137.60,115.84,340.12,104.44"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Original screenshot (left) and drawn wireframe (right).</figDesc><graphic coords="2,137.60,115.84,340.12,104.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,162.57,504.20,290.21,8.83;3,173.03,115.83,269.30,373.60"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Ground truth: the 21 visual representations for the UI elements.</figDesc><graphic coords="3,173.03,115.83,269.30,373.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,222.13,311.40,171.10,8.83;5,137.60,115.84,340.14,180.79"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Wireframe annotation using VoTT</figDesc><graphic coords="5,137.60,115.84,340.14,180.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,176.26,273.68,262.85,8.83;6,137.60,115.83,340.16,143.08"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Bright, dark and scanned versions of the same wireframe.</figDesc><graphic coords="6,137.60,115.83,340.16,143.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,184.38,471.25,246.60,8.83;6,180.12,303.41,255.11,153.07"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Distribution of the number of UI elements per image.</figDesc><graphic coords="6,180.12,303.41,255.11,153.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,187.11,300.68,241.13,8.83;7,180.12,115.84,255.11,170.08"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Tree map with the number of UI elements per class.</figDesc><graphic coords="7,180.12,115.84,255.11,170.08" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,646.31,178.88,8.33"><p>https://www.imageclef.org/2020/drawnui</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,144.73,657.44,137.01,8.33"><p>http://www.clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="6,144.73,656.80,152.45,8.74"><p>https://github.com/microsoft/VoTT/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="8,144.73,657.44,188.29,8.33"><p>https://github.com/philferriere/cocoapi/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,138.90,337.64,8.74;11,151.53,149.85,329.07,8.74;11,151.53,160.81,160.12,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,220.48,138.90,260.12,8.74;11,151.53,149.85,41.50,8.74">pix2code : Generating Code from a Graphical User Interface Screenshot</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Beltramelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,199.99,149.85,280.60,8.74;11,151.53,160.81,98.66,8.74">Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems</title>
		<meeting>the ACM SIGCHI Symposium on Engineering Interactive Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,171.03,337.63,8.74;11,151.53,181.97,208.56,8.83" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="11,333.17,171.03,147.42,8.74;11,151.53,181.99,73.66,8.74">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">Y M</forename><surname>Liao</surname></persName>
		</author>
		<idno>ArXiv abs/2004.10934</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,192.21,337.64,8.74;11,151.53,203.17,329.06,8.74;11,151.53,214.13,329.06,8.74;11,151.53,225.09,172.90,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,275.38,192.21,205.22,8.74;11,151.53,203.17,73.21,8.74">Cascade R-CNN: Delving into High Quality Object Detection</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00644</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00644" />
	</analytic>
	<monogr>
		<title level="m" coord="11,237.71,203.17,242.87,8.74;11,151.53,214.13,226.47,8.74">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,235.31,337.63,8.74;11,151.53,246.27,329.06,8.74;11,151.53,257.20,73.99,8.83" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,197.03,246.27,156.99,8.74">Overview of ImageCLEFcoral 2019 task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Clift</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,360.46,246.27,120.12,8.74">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2380</biblScope>
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,267.45,337.63,8.74;11,151.53,278.41,329.07,8.74;11,151.53,289.34,238.01,8.83" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,351.19,267.45,129.40,8.74;11,151.53,278.41,324.66,8.74">From UI Design Image to GUI Skeleton : A Neural Machine Translator to Bootstrap Mobile GUI Implementation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.53,289.37,200.94,8.74">International Conference on Software Engineering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,299.59,337.64,8.74;11,151.53,310.55,329.06,8.74;11,151.53,321.50,329.07,8.74;11,151.53,332.46,329.06,8.74;11,151.53,343.42,166.77,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,224.78,310.55,255.80,8.74;11,151.53,321.50,69.20,8.74">Rico: A mobile app dataset for building data-driven design applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Franzen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hibschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Afergan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1145/3126594.3126651</idno>
		<ptr target="https://doi.org/10.1145/3126594.3126651" />
	</analytic>
	<monogr>
		<title level="m" coord="11,248.49,321.50,232.10,8.74;11,151.53,332.46,235.01,8.74">UIST 2017 -Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,353.64,337.64,8.74;11,151.53,364.57,329.06,8.83;11,151.53,375.56,203.70,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,441.34,353.64,39.25,8.74;11,151.53,364.60,193.54,8.74">The PAS-CAL Visual Object Classes ( VOC ) Challenge</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-009-0275-4</idno>
		<ptr target="https://doi.org/10.1007/s11263-009-0275-4" />
	</analytic>
	<monogr>
		<title level="j" coord="11,353.18,364.60,74.13,8.74">Int J Comput Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,385.78,337.64,8.74;11,151.53,396.71,329.06,8.83;11,151.53,407.70,186.34,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,360.35,385.78,54.88,8.74">Mask R-CNN</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2844175</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2844175" />
	</analytic>
	<monogr>
		<title level="j" coord="11,426.63,385.78,53.97,8.74;11,151.53,396.74,228.01,8.74">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="386" to="397" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,417.92,337.63,8.74;11,151.53,428.88,329.07,8.74;11,151.53,439.84,329.06,8.97;11,151.53,451.44,108.27,8.33" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,313.85,417.92,166.74,8.74;11,151.53,428.88,24.45,8.74">Swire: Sketch-based User Interface Retrieval</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nichols</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300334</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300334" />
	</analytic>
	<monogr>
		<title level="m" coord="11,228.36,428.88,32.68,8.74">CHI &apos;19</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,461.02,337.97,8.74;11,151.53,471.97,329.06,8.74;11,151.53,482.93,329.06,8.74;11,151.53,493.89,329.06,8.74;11,151.53,504.85,239.48,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,361.61,471.97,118.97,8.74;11,151.53,482.93,155.33,8.74">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.351</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.351" />
	</analytic>
	<monogr>
		<title level="m" coord="11,329.54,482.93,151.05,8.74;11,151.53,493.89,180.79,8.74">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-01">2017. 2017-January. 2017</date>
			<biblScope unit="page" from="3296" to="3305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,515.07,337.97,8.74;11,151.53,526.03,329.06,8.74;11,151.53,536.99,329.06,8.74;11,151.53,547.95,329.06,8.74;11,151.53,558.91,329.06,8.74;11,151.53,569.87,329.06,8.74;11,151.53,580.83,329.07,8.74;11,151.53,591.78,329.06,8.74;11,151.53,602.74,329.06,8.74;11,151.53,613.70,329.06,8.74;11,151.53,624.66,34.31,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,295.07,569.87,185.52,8.74;11,151.53,580.83,255.38,8.74">Overview of the ImageCLEF 2020: Multimedia retrieval in medical, lifelogging, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PÃ©teri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Åžtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,426.42,580.83,54.17,8.74;11,151.53,591.78,329.06,8.74;11,151.53,602.74,253.34,8.74">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="11,456.14,602.74,24.44,8.74;11,151.53,613.70,138.61,8.74">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="11,142.62,634.88,337.98,8.74;11,151.53,645.84,329.06,8.74;11,151.53,656.80,166.77,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,341.14,634.88,139.45,8.74;11,151.53,645.84,235.58,8.74">User interface design by sketching: A Complexity Analysis of Widget Representations</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kieffer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Coyette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderdonckt</surname></persName>
		</author>
		<idno type="DOI">10.1145/1822018.1822029</idno>
		<ptr target="https://doi.org/10.1145/1822018.1822029" />
	</analytic>
	<monogr>
		<title level="j" coord="11,396.98,645.84,21.37,8.74">EICS</title>
		<imprint>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,119.67,337.98,8.74;12,151.53,130.63,329.06,8.74;12,151.53,141.59,329.07,8.74;12,151.53,152.55,68.92,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,253.64,119.67,222.98,8.74">Doodle2App: Native app code by freehand UI sketching</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mohian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Csallner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,164.63,130.63,315.96,8.74;12,151.53,141.59,273.47,8.74">Proc. 7th IEEE/ACM International Conference on Mobile Software Engineering and Systems (MOBILESoft), Tool Demos and Mobile Apps Track</title>
		<meeting>7th IEEE/ACM International Conference on Mobile Software Engineering and Systems (MOBILESoft), Tool Demos and Mobile Apps Track</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="12,142.62,163.51,337.97,8.74;12,151.53,174.47,329.06,8.74;12,151.53,185.43,106.85,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,307.34,163.51,173.25,8.74;12,151.53,174.47,172.39,8.74">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,347.89,174.47,132.69,8.74;12,151.53,185.43,78.17,8.74">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,196.39,337.98,8.74;12,151.53,207.35,329.07,8.74;12,151.53,218.30,329.06,8.74;12,151.53,229.26,198.01,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,353.06,196.39,127.54,8.74;12,151.53,207.35,329.07,8.74;12,151.53,218.30,20.10,8.74">!FTL, an Articulation-Invariant Stroke Gesture Recognizer with Controllable Position, Scale, and Rotation Invariances</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderdonckt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Roselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>PÃ©rez-Medina</surname></persName>
		</author>
		<idno type="DOI">10.1145/3242969.3243032</idno>
		<ptr target="https://doi.org/10.1145/3242969.3243032" />
	</analytic>
	<monogr>
		<title level="m" coord="12,178.17,218.30,252.31,8.74">20th ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
