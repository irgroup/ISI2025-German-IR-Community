<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.94,115.96,327.47,12.62;1,184.14,133.89,247.08,12.62">An interactive atomic-cluster watershed-based system for lifelog moment retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.67,171.86,66.87,8.74"><forename type="first">Van-Luon</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science</orgName>
								<orgName type="institution" key="instit2">VNU-HCMC</orgName>
								<address>
									<addrLine>Vietnam {1612362</addrLine>
									<postCode>1512102, 1612904</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,221.08,171.86,71.70,8.74"><forename type="first">Trong-Dat</forename><surname>Phan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science</orgName>
								<orgName type="institution" key="instit2">VNU-HCMC</orgName>
								<address>
									<addrLine>Vietnam {1612362</addrLine>
									<postCode>1512102, 1612904</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.34,171.86,90.49,8.74"><forename type="first">Anh-Vu</forename><surname>Mai-Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science</orgName>
								<orgName type="institution" key="instit2">VNU-HCMC</orgName>
								<address>
									<addrLine>Vietnam {1612362</addrLine>
									<postCode>1512102, 1612904</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,404.40,171.86,60.05,8.74"><forename type="first">Anh-Khoa</forename><surname>Vo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science</orgName>
								<orgName type="institution" key="instit2">VNU-HCMC</orgName>
								<address>
									<addrLine>Vietnam {1612362</addrLine>
									<postCode>1512102, 1612904</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.50,183.81,63.23,8.74"><forename type="first">Minh-Son</forename><surname>Dao</surname></persName>
							<email>dao@nict.go.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,329.25,183.81,50.14,8.74"><forename type="first">Koji</forename><surname>Zettsu</surname></persName>
							<email>zettsu@nict.go.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.94,115.96,327.47,12.62;1,184.14,133.89,247.08,12.62">An interactive atomic-cluster watershed-based system for lifelog moment retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">875EC108D5880C9BCE3A41110DD4E973</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce a new interactive atomic-cluster watershed-based system for lifelog moment retrieval. We investigate three essential components that can help improve accuracy and support both amateur and professional users to enhance their querying based on different content and context hypothesis. These components are (1) the atomic cluster function that clusters dataset to a set of time-consecutive images that shares the same content and context constraints, (2) the text-tosample image generation that helps to overcome the gap between textual queries of users and visual-based feature vectors database, and (3) The interactive interface that assists users to imagine what they want to look for better. The system is customized to meet the challenge of lifelog moment retrieval of imageCLEFlifelog2020. The evaluation and comparison of our method to others confirm the stability of our method when people want to retrieve a large number of results within 100 top results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Finding a moment in our past with a few hints or cues is the activity we probably carry on almost every day. Except for extraordinary people who have a fantastic memory that can recall every moment in their lives within a split-second, ordinary people need more time to narrow down their searching scope from a very abstract level to detail. The same situation happens when people want to find their historical moment from their lifelog data. That leads to the fact that if people can have an interactive system that can help them turn their queries from an amateur sketch to an artist's paint, they will retrieve their moment faster and more precisely <ref type="bibr" coords="1,200.47,579.27,9.96,8.74" target="#b0">[1]</ref>, <ref type="bibr" coords="1,217.07,579.27,10.52,8.74" target="#b1">[2]</ref> Besides, turning a few keywords and less semantic contents of users' text queries to somethings that can be understood by the search engine is another challenge <ref type="bibr" coords="2,178.34,118.99,9.96,8.74" target="#b2">[3]</ref>. There is still a big gap between the natural language spoken by users and machine language designed for search engines <ref type="bibr" coords="2,378.42,130.95,10.52,8.74" target="#b3">[4]</ref> that can prevent the improvement of accuracy. Feature selection is another factor that can assist in bridging this gap <ref type="bibr" coords="2,212.34,154.86,9.96,8.74" target="#b4">[5]</ref>, <ref type="bibr" coords="2,228.94,154.86,9.96,8.74" target="#b5">[6]</ref>, and in support of the well-organized dataset.</p><p>Based on the discussion mentioned above, we design an interactive atomiccluster watershed-based system for lifelog moment retrieval. This system is customized to meet the lifelog moment retrieval (LMRT) challenge of imageCLE-Flifelog2020 <ref type="bibr" coords="2,190.25,203.34,9.96,8.74" target="#b6">[7]</ref>, a lab task of imageCLEF2020 <ref type="bibr" coords="2,339.88,203.34,9.96,8.74" target="#b7">[8]</ref>.</p><p>Our system's main contributions are:</p><p>1. We introduce an atomic cluster, a set of time-consecutive images that shares the same content and context constraints. 2. We build the text-to-sample image generation to overcome the gap between textual queries of users and visual-based feature vectors database. 3. We create an interactive interface to help users imagine what they want to look for better.</p><p>We organize this paper as follows: Section 2 describes our method in details, Section 3 discusses the challenge and evaluates our results, and Section 4 concludes our paper and sketches our plan in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The principal idea of our method bases on the following observations:</p><p>1. daily activities of people can be divided into sequential atomic moments whose content has a consensus of both content and context. In other words, lifelog data recorded during a day can be divided into sequential atomic clusters whose content reflects a unique semantic meaning with a consensus of spatiotemporal dimension. These atomic clusters cannot be divided into smaller clusters. Hence, if we could find one image that matches the query (i.e., seeds), we can count in the atomic cluster the image belongs and the neighbors of the atomic cluster (i.e., watershed). 2. people can decide which data do not satisfy their queries. In other words, people can remove irrelevant data and modify their queries to get more relevant data. Hence, if we can provide people a friendly interface for interactive querying, people can improve the qualification of the querying system. We call the system built upon these observations is an interactive atomiccluster watershed system. The system has four vital components (1) atomiccluster clustering (Cluster function), (2) text-to-sample image generation (Attention function), <ref type="bibr" coords="2,215.66,620.25,12.73,8.74" target="#b2">(3)</ref> querying by text-to-sample images (Query function), ( <ref type="formula" coords="2,472.10,620.25,4.24,8.74">4</ref>) interaction (Interactive function), and (5) querying by user's images (Query function). Algorithm 1 and Figure <ref type="figure" coords="2,295.23,644.16,4.98,8.74" target="#fig_0">1</ref> describe and illustrate how the system works, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations and definitions</head><p>There are several notations and feature vectors which are used for our system. Thus, we define them below:</p><p>-Let Q denote the query sentence.</p><p>-Let I = {I i } i=1..N denote the set of given images (e.g., lifelog).</p><p>-Let C = {C k } denote a set of atomic clusters.</p><p>-Let S denote the set of samples for each query, and S i denote the status of set S at the time i.</p><formula xml:id="formula_0" coords="3,140.99,554.18,123.66,12.48">-Let BoV = {V i-k } k=1..m i=1.</formula><p>.N denote the set of feature vectors of objects extracted from I, where V i-k denotes the feature vector of the k th object of I i and BoV i denotes the set of all object vectors of I i .</p><p>-Let BoV DB denote the database stores all object vectors of all images in I.</p><p>-Let Seed and LM RT denote a set of seeds and lifelog moments, respectively.</p><p>-Let denote V oi is the 1024-D vector representation of the i th object region in the photos. -Let denote p i is output vector of the i th image.</p><p>-Let denote V wi is word embedding vector of the i th word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Query-to-Sample Attention-based Search Engine</head><formula xml:id="formula_1" coords="4,134.77,134.28,221.39,292.82">Input: Q, {Ii} i=1..N Output: LM RT {ONLINE} 1: Sample ⇐ ∅ 2: if type(Q) == "text" then 3: Sample ∪ W ord -V isualAttention(Q, I external ) 4: end if 5: if type(Q) == "image" then 6: Sample ∪ Q 7: end if 8: V sample ⇐ F E(Sample) {OFFLINE} 9: {Cm} ⇐ Cluster({Ii} i=1..N ) 10: BoV ⇐ ∅ 11: ∀i ∈ [1..N ], BoVI i ⇐ F E(Ii) 12: BoVDB ⇐ F AISS(BoV ) {ONLINE} 13: S ⇐ ∅ 14: S 0 = S ⇐ S ∪ {V sample } 15: while S i = S i+1 do 16: S ⇐ S ∪ Query(S, BoVDB) 17: i ⇐ i + 1 18: end while 19: seed ⇐ {Ii|∀V ik ∈ S} 20: LM RT ⇐ {C k |∀j ∈ Seed , Seedj ∈ {C k }} 21: LM RT ⇐ Interactive(LM RT ) 22: return LM RT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">System Workflow</head><p>In this subsection, we give a detailed explanation about our Interactive Multimodal Lifelog Retrieval System depicted in figure <ref type="figure" coords="4,353.77,488.32,4.98,8.74" target="#fig_0">1</ref> and the Algorithm 1.</p><p>There are two stages in our system's workflow: (1) offline stage, and (2) online stage.</p><p>The former stage is for data preprocessing. Firstly, we divide lifelog images into atomic clusters by utilizing Clustering function, described in 2.4. Then, all lifelog images are converted to V sample by using Feature Extraction (FE) function, described in 2.3. In other words, V sample contains feature vectors extracted from images. To make use full of FAISS <ref type="bibr" coords="4,304.86,572.29,9.96,8.74" target="#b8">[9]</ref>, we embed these V sample into a unified database by applying FAISS's function.</p><p>The latter stage is for textual and visual querying. For textual querying, our system activates Attention function, described in 2.5, to generate sample images from texts. Then, sample images (and input images if users carry on visual querying) are fed into the FE function to create related V sample . The V sample is used to find the most similar feature vectors from the FAISS-based database with the predefined similarity threshold. Next, we enrich V sample by adding these found feature vectors and re-querying upon FAISS-based databased until no new feature vectors found. All images that have their features vectors appear in this set are considered as the queried results and set as seeds. The final results are all atomic clusters contained these seeds. Then, users use Interactive tools described in 2.6 to polish the output, so they receive wanted results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feature Extraction</head><p>-V oi is extracted by using object detection model (Faster-RCNN backbone Resnet) in scaled Visual Genome dataset <ref type="bibr" coords="5,337.17,230.18,15.50,8.74" target="#b9">[10]</ref> (removing semantic overlapping classes) -p i is extracted by utilizing place detection model described in 2.4 -V wi is built as follows: Hidden state 768-D vectors extracted from BERT <ref type="bibr" coords="5,465.10,266.37,15.50,8.74" target="#b10">[11]</ref> are combined with one linear Conditional Random Field layer to construct seq2seq model <ref type="bibr" coords="5,216.46,290.28,15.50,8.74" target="#b11">[12]</ref> and output keywords (from a long input query sentence) with their representation vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Atomic-Cluster Clustering</head><p>As mentioned in previous sections, an atomic cluster contains a set of consecutive lifelog images (and related metadata) whose content reflects a particular activity constrained by location, time, and semantic meaning. We have the whole dataset clustered into atomic clusters by two steps (1) enhance the quality of metadata and (2) cluster multimodal data. The former applies a self-supervised learning method to regenerate metadata. By utilizing SimCLR method <ref type="bibr" coords="5,447.69,413.42,14.61,8.74" target="#b12">[13]</ref>, we manually label place names for about 20k images and then train a new model to label the remaining images in a dataset automatically. Finally, we strengthen metadata's location constraints by having more precise place names than the original metadata. The latter utilizes the updated metadata and feature vectors extracted from images as the input of the clustering method proposed in <ref type="bibr" coords="5,453.10,473.20,15.50,8.74" target="#b13">[14]</ref> to form atomic clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Text-to-sample Image Generation</head><p>The essential idea of this function is to replace a textual query with a set of visual queries. First, we create a dataset of objects using open image datasets (e.g., COCO, image365). It means that we have a set of object names, and each object name links to a set of images that contains the object. Then, we parse a textual query to extract the object's names replaced by linked images. Notably, we utilized the attention mechanism <ref type="bibr" coords="5,297.76,596.34,15.50,8.74" target="#b14">[15]</ref> to build our function, as described in Algorithm 2. We firstly utilize Top-Down Attention LSTM in a two-layer LSTM model for captioning images from feature vectors of regions detected by the object detection model <ref type="bibr" coords="5,237.70,632.21,14.61,8.74" target="#b15">[16]</ref>. We then determine a useful feature transformation from word vector space to visual space using a well-trained Bottom-up Attention model. vk ⇐ v j 8:</p><p>vk is the optimized presentation for W ord k in visual space 9: end for 10: return {W ordi : Objj} where i = 1..M, j = 1..N</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Interaction</head><p>After having the first results generated by the query by sample function, users can filter the results using other metadata such as visible objects, places, and time. These metadata are saved as text files using PostgreSQL and stored in Logic Server, as described in 2.7. Besides, users can re-query by manually selecting samples from results visualized on the system's interface or add more query categories by texts. Moreover, users can delete inappropriate images as they think. These images are taken into account by the system to mark as outliers or unnecessary items for the next query. Algorithm 3 explained how the interaction works. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Interactive Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Interactive System Architecture</head><p>To build a flexible system, we design our system following three-tier and threelayer architecture, depicted in figure <ref type="figure" coords="6,297.46,656.12,3.87,8.74">2</ref>. The first layer is the presentation layer Fig. <ref type="figure" coords="7,256.77,351.85,7.75,8.74">2:</ref> The System Architecture on a User Client, the second is the logic layer on Logic Server, and the last one is the core layer on the Core Server.</p><p>At the first layer, it is a convenient web-based interface where users can interact with our system. This interface can easily be installed in a wide range of operating systems. It firstly allows users to type text queries, select filters, and input sample images, which is a powerful tool for users to describe which images they would like to retrieve. Then, these data, along with IDs of removed images (in the case users delete the queried results of the previous interaction), will be pushed to Logic Server. Next, the interface has responsibility for presenting images sent from Logic Server. Before users re-query, they can modify their text query, adjust filters, choose images from other sources, and remove unwanted images. They can re-query until presented images satisfy user's demand. Finally, users use the export function to download images or image IDs.</p><p>At the second layer, Logic Server has responsibility for processing requests from User Client. Firstly, this server converts query to a suitable form and send it to Core Server. Then, Logic Server receives outputted results with IDs of images and IDs of related atomic clusters. The result will be saved directly to Cache, a temporary memory on Logic Server. At the following steps, depending on the type of filters, this server will apply the filters on the whole dataset or only the results stored in Cache. There are two types of filter (1) Extend Filter and (2) Narrow Filter. With the former, Logic Server will find all images whose metadata are matched to this filter before adding these image's IDs to Cache. With the latter, from IDs in Cache, Logic Server will select images whose metadata is fitted to the filter. Finally, the server returns filtered images and ranked clusters to User Client.</p><p>In terms of Core Server, it receives input from Logic Server and sends result reversely after completely processing. Core Server is an always-on server where AI components are deployed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we present our system's experiment results when applying to the lifelog dataset CLEF2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Evaluation Metrics</head><p>The CLEF2020 dataset has been captured by one active lifelogger for 114 days between 2015 and 2018. It contains not only over 191000 lifelog images also metadata, including visual concepts, attributes, semantic content, to name a few. The training set has ten topics, and each topic is described by title and description. These titles are: <ref type="bibr" coords="8,264.79,332.19,12.73,8.74" target="#b0">(1)</ref> Having beers in a bar, (2) Building Personal Computer, (3) In A Toy Shop, (4) Television Recording, (5) Public Transport In Home Country, (6) Seaside Moments, (7) Grocery Stores, (8) Photograph of The Bridge, (9) Car Repair, (10) Monsters. The topic descriptions are used to explain in detail about the content and context of each query. Similar to the training set, the testing set has ten topics which are: (1) Praying Rite, (2) Recall, (3) Bus to work -Bus to home, (4) Bus at the Airport, (5) Medicine cabinet, <ref type="bibr" coords="8,467.86,403.93,12.73,8.74" target="#b5">(6)</ref> Order Food in the Airport, (7) Seafood at Restaurant, (8) Meeting with people, (9) Eating Pizza, (10) Socialising.</p><p>The evaluation metrics are defined by ImageCLEFliflog 2020 as follow:</p><p>-Cluster Recall at X (CR@X) -a metric that assesses how many difference clusters from the ground truth are represented among the top X results; -Precision at X (P@X) -measures the number of relevant photos among the top X results; -F1-measure at X (F1@X) -the harmonic mean of the previous two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation and Comparison</head><p>The ImageCLEFlifelog challenge has five participant teams including: (1) RRibeiro, (2) FatmaBA RegimLab, (3) DCU Team, (4) BIDAL HCMUS (ourselves), ( <ref type="formula" coords="8,472.10,569.31,4.24,8.74">5</ref>) HCMUS. We are ranked in the second position. Table <ref type="table" coords="8,371.08,581.26,4.98,8.74">1</ref> and 2 shows our results running on the training and testing set while table 3 and 4 denote the comparison to the other teams.Figures 3-12 illustrate our results of the testing stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1: Results running on CLEF2020 Training Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Run 1 Run 2 P@10 CR@10 F1@10 P@10 CR@10 F1@10 Run 10 P@10 CR@10 F1@10 P@10 CR@10 F1@10   When comparing the results evaluated by F1@10 and F1@50 metrics, we found that our scores are less fluctuation than the others (some other teams have a massive reduction in their scores), as described in table 3 and 4. That probably could lead to the conclusion that our proposed method is stable, especially if the user wants to retrieve a large of images.</p><p>In some queries, we have worse scores because of misunderstanding the content and context of queries. For instance, query 5 has the title: 'Medicine cabinet' and description: 'Find the moment when u1 was looking inside the medicine cabinet in the bathroom at home', we very confused when trying to confirm whether the lifelogger really looks inside the medicine cabinet or appear nearby (i.e., the medicine cabinet is captured by lifelog camera, but the u1 does not look at it). The result of query 5 is shown in figure <ref type="figure" coords="10,309.91,442.60,3.87,8.74">7</ref>.</p><p>Furthermore, we found that the ground truth could have some incorrect points. We have verified with the organizers that the ground-truth might not be precise. For example, the image ID b00000986 21i6bq 20150225 161718e (in query 9) and the image ID 20160904 120624 000 (in query 5) should have been in the ground-truth. Figures <ref type="figure" coords="10,263.45,502.37,4.98,8.74">7</ref> and<ref type="figure" coords="10,292.10,502.37,9.96,8.74" target="#fig_0">11</ref> illustrate the results of queries 5 and 9, where the red rectangle denotes mentioned images. That probably makes our results not precise enough as we expected. We introduced a new interactive atomic-cluster watershed-based system for lifelog moment retrieval. The system is specially customized to meet the requirement of the imageCLEFlifelog2020 challenges. The system first indexes the database based on atomic clusters that contain similar data based on our similarity measure. The reason behind the atomic clusters is that whenever one image is found, its atomic cluster counts in. We store feature vectors extracted from data in FAISS database for further querying. We convert all textual queries into visual queries by using the attention mechanism approach. The system provides a friendly interactive interface that allows users to select precise results and re-query with modification. Our results are evaluated and compared to other participants with positive accuracy. We will investigate the atomic clustering function to improve the consensus and compact of atomic clusters in the future. Moreover, we will consider wrapping spatiotemporal information to the querying engine by strengthening semantic constraints. Last but not least, we will focus on feature engineering and similarity measures to have a higher accuracy of querying.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,179.42,393.79,256.51,8.74;3,134.77,115.84,345.83,266.43"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: An Interactive Multimodal Lifelog Retrieval System</figDesc><graphic coords="3,134.77,115.84,345.83,266.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="11,162.98,238.14,289.39,8.74;11,186.64,115.84,242.07,110.77"><head>Fig. 3 :Fig. 4 :Fig. 5 :Fig. 6 :Fig. 7 :Fig. 8 :Fig. 9 :Fig. 10 :Fig. 11 :Fig. 12 :</head><label>3456789101112</label><figDesc>Fig.3: The top ten results of query 1 "Praying Rite" (F1@10 = 1)</figDesc><graphic coords="11,186.64,115.84,242.07,110.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,134.77,115.83,345.83,224.49"><head></head><label></label><figDesc></figDesc><graphic coords="7,134.77,115.83,345.83,224.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.77,118.71,245.67,113.85"><head></head><label></label><figDesc>Algorithm 2 Text-to-sample Image GenerationInput: Word set {W ordi} i=1..M , Object set {Objj} j=1..N Output: {W ordi : Objj} map from word to relevant object.1: {Vw i } i=1..M ⇐ W ordEmb(W ord)</figDesc><table coords="6,138.66,167.18,200.77,65.38"><row><cell cols="2">2: Vo j j=1..N ⇐ F E(Obj)</cell></row><row><cell cols="2">3: Training bottom-up attention model as in [15].</cell></row><row><cell cols="2">4: for all k ≤ Vw do</cell></row><row><cell>5:</cell><cell>vk ⇐ N j=1 α k,j vj</cell></row><row><cell>6:</cell><cell>j ⇐ arg maxj α kj</cell></row><row><cell>7:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,195.92,164.41,223.53,159.99"><head>Table 2 :</head><label>2</label><figDesc>Results running on CLEF2019 Testing Set</figDesc><table coords="9,204.67,164.41,197.63,159.99"><row><cell>1</cell><cell>0.7</cell><cell>0.50</cell><cell>0.58 1.00 0.75</cell><cell>0.86</cell></row><row><cell>2</cell><cell cols="2">0.60 0.50</cell><cell>0.55 1.00 0.50</cell><cell>0.67</cell></row><row><cell>3</cell><cell cols="2">0.30 0.50</cell><cell>0.38 0.70 1.00</cell><cell>0.82</cell></row><row><cell>4</cell><cell cols="2">0.70 0.67</cell><cell>0.68 1.00 0.67</cell><cell>0.80</cell></row><row><cell>5</cell><cell cols="2">0.20 0.11</cell><cell>0.14 0.90 0.44</cell><cell>0.60</cell></row><row><cell>6</cell><cell cols="2">0.60 0.50</cell><cell>0.55 0.80 0.50</cell><cell>0.62</cell></row><row><cell>7</cell><cell cols="2">0.50 0.44</cell><cell>0.47 0.80 0.78</cell><cell>0.79</cell></row><row><cell>8</cell><cell cols="2">0.30 0.50</cell><cell>0.38 0.40 0.50</cell><cell>0.44</cell></row><row><cell>9</cell><cell cols="2">0.60 1.00</cell><cell>0.75 0.80 1.00</cell><cell>0.89</cell></row><row><cell>10</cell><cell cols="2">0.30 1.00</cell><cell>0.46 0.30 1.00</cell><cell>0.46</cell></row><row><cell>Query</cell><cell></cell><cell>Run 9</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,136.56,470.06,344.16,154.90"><head>Table 3 :</head><label>3</label><figDesc>Comparison to the other teams (F1@10 metric)</figDesc><table coords="9,136.56,482.40,344.16,142.56"><row><cell>Query</cell><cell cols="6">F1@10 Baseline FatmaBA RegimLab RRibeiro DCU Team BIDAL HCMUS HCMUS</cell></row><row><cell>1</cell><cell>1</cell><cell>0.58</cell><cell>0.95</cell><cell>0.50</cell><cell>1</cell><cell>1</cell></row><row><cell>2</cell><cell>0.4</cell><cell>0</cell><cell>0</cell><cell>0.22</cell><cell>0.55</cell><cell>0.86</cell></row><row><cell>3</cell><cell>0.17</cell><cell>0.29</cell><cell>0.67</cell><cell>0.95</cell><cell>0.67</cell><cell>1</cell></row><row><cell>4</cell><cell>0</cell><cell>0.14</cell><cell>0</cell><cell>0.27</cell><cell>0.62</cell><cell>0.55</cell></row><row><cell>5</cell><cell>0.21</cell><cell>0</cell><cell>0.77</cell><cell>0.68</cell><cell>0.74</cell><cell>0.83</cell></row><row><cell>6</cell><cell>0.13</cell><cell>0.13</cell><cell>0.50</cell><cell>0.25</cell><cell>0.67</cell><cell>0.68</cell></row><row><cell>7</cell><cell>0.24</cell><cell>0</cell><cell>0.67</cell><cell>0.44</cell><cell>0.69</cell><cell>0.57</cell></row><row><cell>8</cell><cell>0</cell><cell>0</cell><cell>0.82</cell><cell>0.33</cell><cell>0.75</cell><cell>0.67</cell></row><row><cell>9</cell><cell>0.57</cell><cell>0.75</cell><cell>0.80</cell><cell>0.68</cell><cell>0.77</cell><cell>0.95</cell></row><row><cell>10</cell><cell>0.50</cell><cell>0</cell><cell>0</cell><cell>0.50</cell><cell>0.50</cell><cell>1</cell></row><row><cell>Avg</cell><cell>0.32</cell><cell>0.19</cell><cell>0.52</cell><cell>0.48</cell><cell>0.69</cell><cell>0.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,136.56,127.36,344.16,154.90"><head>Table 4 :</head><label>4</label><figDesc>Comparison to the other teams (F1@50 metric)</figDesc><table coords="10,136.56,139.70,344.16,142.56"><row><cell>Query</cell><cell cols="6">F1@50 Baseline FatmaBA RegimLab RRibeiro DCU Team BIDAL HCMUS HCMUS</cell></row><row><cell>1</cell><cell>0.63</cell><cell>0.46</cell><cell>0.99</cell><cell>0.17</cell><cell>0.33</cell><cell>0.33</cell></row><row><cell>2</cell><cell>0.49</cell><cell>0.03</cell><cell>0</cell><cell>0.77</cell><cell>0.19</cell><cell>0.32</cell></row><row><cell>3</cell><cell>0.04</cell><cell>0.34</cell><cell>0.95</cell><cell>0.31</cell><cell>0.65</cell><cell>0.33</cell></row><row><cell>4</cell><cell>0</cell><cell>0.07</cell><cell>0</cell><cell>0.16</cell><cell>0.39</cell><cell>0.32</cell></row><row><cell>5</cell><cell>0.17</cell><cell>0</cell><cell>0.65</cell><cell>0.62</cell><cell>0.61</cell><cell>0.70</cell></row><row><cell>6</cell><cell>0.04</cell><cell>0.04</cell><cell>0.17</cell><cell>0.29</cell><cell>0.38</cell><cell>0.23</cell></row><row><cell>7</cell><cell>0.09</cell><cell>0</cell><cell>0.85</cell><cell>0.50</cell><cell>0.67</cell><cell>0.27</cell></row><row><cell>8</cell><cell>0</cell><cell>0.04</cell><cell>0.97</cell><cell>0.08</cell><cell>0.80</cell><cell>0.18</cell></row><row><cell>9</cell><cell>0.18</cell><cell>0.49</cell><cell>0.76</cell><cell>0.28</cell><cell>0.60</cell><cell>0.37</cell></row><row><cell>10</cell><cell>0.50</cell><cell>0.53</cell><cell>0</cell><cell>0.50</cell><cell>0.50</cell><cell>1</cell></row><row><cell>Avg</cell><cell>0.21</cell><cell>0.15</cell><cell>0.53</cell><cell>0.37</cell><cell>0.51</cell><cell>0.41</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This research is conducted under the Collaborative Research Agreement between <rs type="institution">National Institute of Information and Communications Technology and University of Science, Vietnam National University at Ho-Chi-Minh City</rs>.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="14,142.96,634.88,337.64,7.86;14,151.52,645.84,329.07,7.86;14,151.52,656.80,50.68,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,329.60,634.88,150.99,7.86;14,151.52,645.84,33.73,7.86">Ivist: Interactive video search tool in vbs 2020</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,207.57,645.84,199.15,7.86">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="809" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,119.67,337.64,7.86;15,151.52,130.63,329.07,7.86;15,151.52,141.59,329.07,7.86;15,151.52,152.55,326.42,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,155.73,130.63,189.95,7.86">Exquisitor at the video browser showdown 2020</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Jónsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">C</forename><surname>Koelma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rudinac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zahálka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,367.38,130.63,82.95,7.86">MultiMedia Modeling</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W.-H</forename><surname>Cheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W.-T</forename><surname>Chu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-W</forename><surname>Choi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M.-C</forename><surname>Hu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>De Neve</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="796" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,163.52,337.64,7.86;15,151.52,174.48,329.07,7.86;15,151.52,185.44,329.07,7.86;15,151.52,196.40,114.21,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,444.76,174.48,35.84,7.86;15,151.52,185.44,101.90,7.86">Verge in vbs 2020</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Andreadis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moumtzidou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Apostolidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gkountakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Galanopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Michail</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gialampoukidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mezaris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,276.24,185.44,200.07,7.86">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="778" to="783" />
		</imprint>
	</monogr>
	<note>Kompatsiaris</note>
</biblStruct>

<biblStruct coords="15,142.96,207.37,337.64,7.86;15,151.52,218.33,140.89,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,313.13,207.37,129.65,7.86">Spatial keyword search: a survey</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,453.71,207.37,26.88,7.86;15,151.52,218.33,36.49,7.86">Geoinformatica</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="85" to="106" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,229.30,337.64,7.86;15,151.52,240.26,329.07,7.86;15,151.52,251.22,281.49,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,317.95,229.30,162.64,7.86;15,151.52,240.26,34.80,7.86">Feature selection with multi-view data: A survey</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S1566253518303841" />
	</analytic>
	<monogr>
		<title level="j" coord="15,198.04,240.26,76.64,7.86">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="158" to="167" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,262.19,337.64,7.86;15,151.52,273.14,329.07,7.86;15,151.52,284.10,329.07,7.86;15,151.52,295.06,123.41,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,445.82,262.19,34.77,7.86;15,151.52,273.14,192.18,7.86">A survey on semi-supervised feature selection methods</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sheikhpour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Sarram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gharaghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A Z</forename><surname>Chahooki</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0031320316303545" />
	</analytic>
	<monogr>
		<title level="j" coord="15,359.10,273.14,78.88,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="141" to="158" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,306.03,337.63,7.86;15,151.52,316.99,329.07,7.86;15,151.52,327.95,329.07,7.86;15,151.52,338.91,329.07,7.86;15,151.52,349.87,218.76,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,347.33,316.99,133.27,7.86;15,151.52,327.95,260.68,7.86">Overview of ImageCLEF Lifelog 2020:Lifelog Moment Retrieval and Sport Performance Lifelog</title>
		<author>
			<persName coords=""><forename type="first">V.-T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="15,437.16,327.95,43.43,7.86;15,151.52,338.91,199.96,7.86">CLEF2020 Working Notes, ser. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,360.84,337.63,7.86;15,151.52,371.80,329.07,7.86;15,151.52,382.76,329.07,7.86;15,151.52,393.72,329.07,7.86;15,151.52,404.67,329.07,7.86;15,151.52,415.63,329.07,7.86;15,151.52,426.59,329.07,7.86;15,151.52,437.55,329.07,7.86;15,151.52,448.51,329.07,7.86;15,151.52,459.47,329.07,7.86;15,151.52,470.43,61.72,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,279.85,415.63,200.74,7.86;15,151.52,426.59,254.40,7.86">Overview of the ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V.-T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,427.88,426.59,52.71,7.86;15,151.52,437.55,329.07,7.86;15,151.52,448.51,278.90,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction, ser. Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="15,238.33,459.47,164.68,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,481.40,337.64,7.86;15,151.52,492.36,158.15,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="15,310.93,481.40,161.11,7.86">Billion-scale similarity search with gpus</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.61,503.33,337.97,7.86;15,151.52,514.29,329.07,7.86;15,151.52,525.25,329.07,7.86;15,151.52,536.20,296.88,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="15,454.87,514.29,25.71,7.86;15,151.52,525.25,329.07,7.86;15,151.52,536.20,45.96,7.86">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1602.07332" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,547.17,337.98,7.86;15,151.52,558.13,329.07,7.86;15,151.52,569.09,97.10,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="15,392.38,547.17,88.21,7.86;15,151.52,558.13,253.32,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.61,580.06,337.97,7.86;15,151.52,591.02,329.07,7.86;15,151.52,601.98,20.99,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,312.32,580.06,168.27,7.86;15,151.52,591.02,34.26,7.86">Sequence to sequence learning with neural networks</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,208.07,591.02,202.50,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,612.95,337.97,7.86;15,151.52,623.91,329.07,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="15,366.80,612.95,113.79,7.86;15,151.52,623.91,162.67,7.86">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.61,634.88,337.98,7.86;15,151.52,645.84,329.07,7.86;15,151.52,656.80,204.27,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,298.72,634.88,181.88,7.86;15,151.52,645.84,95.55,7.86">An interactive watershed-based approach for lifelog moment retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zettsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,268.36,645.84,212.23,7.86;15,151.52,656.80,101.47,7.86">2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM)</title>
		<imprint>
			<date type="published" when="2019-09">Sep. 2019</date>
			<biblScope unit="page" from="282" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,119.67,337.97,7.86;16,151.52,130.63,329.07,7.86;16,151.52,141.59,329.07,7.86;16,151.52,152.55,132.76,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,156.37,130.63,324.22,7.86;16,151.52,141.59,28.93,7.86">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,203.72,141.59,276.87,7.86;16,151.52,152.55,41.91,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,163.51,337.97,7.86;16,151.52,174.47,329.07,7.86;16,151.52,185.43,132.80,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,323.10,163.51,157.49,7.86;16,151.52,174.47,160.13,7.86">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,334.07,174.47,146.52,7.86;16,151.52,185.43,60.05,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
