<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,126.14,152.67,342.76,12.64;1,194.57,172.54,205.86,10.80">ImageCLEF 2020: An approach for Visual Question Answering using VGG-LSTM for different datasets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,131.18,209.70,56.20,8.96"><forename type="first">Sheerin</forename><surname>Sitara</surname></persName>
							<email>sheerinsitaran@ssn.edu.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">SSN College of Engineering</orgName>
								<address>
									<postCode>-603110</postCode>
									<settlement>Kalavakkam</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,189.91,209.70,63.47,8.96"><forename type="first">Noor</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">SSN College of Engineering</orgName>
								<address>
									<postCode>-603110</postCode>
									<settlement>Kalavakkam</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.09,209.70,76.33,8.96"><forename type="first">Kavitha</forename><surname>Srinivasan</surname></persName>
							<email>kavithas@ssn.edu.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">SSN College of Engineering</orgName>
								<address>
									<postCode>-603110</postCode>
									<settlement>Kalavakkam</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,126.14,152.67,342.76,12.64;1,194.57,172.54,205.86,10.80">ImageCLEF 2020: An approach for Visual Question Answering using VGG-LSTM for different datasets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BE00116E314ED4E2CACA250796512E1D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>VQA</term>
					<term>VGGNet</term>
					<term>LSTM</term>
					<term>medical domain</term>
					<term>augmented dataset</term>
					<term>reduced dataset</term>
					<term>ImageCLEF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent advancement and digitalization in the medical domain requires an image based question answering system to support clinical decisions. This system also helps the patients to know about their present conditions rapidly with more information. As an effort to promote the development, ImageCLEF 2020 organizes third edition of the Visual Question Answering (VQA) Task. In this task, the abnormality related questions are to be answered for the given set of radiology images. In the proposed system, VGGNet based on transfer learning approach and LSTM are used to extract the image and text feature vectors respectively in the encoder stage. Then, both feature vectors are combined and given as input to the decoder for predicting the answer. The purpose of selecting VGGNet and LSTM are: (i). VGGNet is able to extract medical image features effectively in small dataset (ii). LSTM is capable to accommodate significant information of the text. Moreover, the proposed model is evaluated for three datasets namely original dataset (4500 samples), reduced dataset (4348 samples) and augmented reduced dataset (4626 samples). The proposed model resulted in an accuracy of 0.282 and a BLEU score of 0.330 for augmented reduced dataset, which is ranked ninth among all participating group in ImageCLEF 2020 VQA-MED task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The amount of data generated and used in this era are increasing exponentially and medical domain is not an exception. Also, everyone wants to know the answer for everything they come across in the internet world. Multiple search engines are working towards satisfying their knowledge thirst, unfortunately very few image based search engines are available in the market. However, these search engines are generalized and not suitable for medical domain.</p><p>The medical domain is wide, and it needs prior knowledge and analysis to answer the questions. These issues can be addressed by improving the medical image based question answering system. To enhance this research further, ImageCLEF is conducting VQA task in medical domain since 2018 <ref type="bibr" coords="2,305.11,186.18,10.58,8.96" target="#b0">[1]</ref>.</p><p>The Visual Question Answering (VQA) in medical domain helps people (especially partially sighted) in better understanding of their condition and supports clinical decision. The challenges of VQA in medical domain includes: (i). Parameter selection and feature extraction for medical dataset which differs from the real time and abstract dataset (ii). Specific VQA model which works for all medical category is in developing stage. For example in <ref type="bibr" coords="2,211.78,258.21,10.73,8.96" target="#b1">[2]</ref>, different approaches are required to answer different medical questions. The pre-trained model followed by BERT model answers organ, plane and modality related questions whereas abnormality related questions are answered effectively by sequence-to-sequence model (iii). Single optimal model which detects all types of medical abnormalities in different region needs some attention and effort. But, abnormality detection with respect to the particular region are available. For example, in <ref type="bibr" coords="2,134.89,330.21,10.69,8.96" target="#b2">[3]</ref>, bifurcated structure detects four gastrointestinal abnormalities and three dermoscopic lesions in WCE images and PH2 dataset respectively. Two abnormality categories are detected separately and attained an accuracy of 97.8% and 97.5% respectively. (iv). Memory and time constraints.</p><p>The remaining part of the paper spans across following subsections. In Sect. 2, literature survey related to automation in medical domain, inference from VQA task for real world dataset and its recent advancement in medical domain are discussed. Sect. 3 gives brief description about the ImageCLEF VQA-Med 2020 dataset and two other proposed datasets used for analysis and validation. In Sect. 4, the design of the proposed VQA model based on inference attained and its implementation are explained. A brief summary about the result and the respective evaluation of all five runs are given in Sect. 5 and conclusion is given at the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>The recent studies shows a tremendous advancement in the medical domain. One of the best advancement is that the medical data in structured, semi-structured and unstructured formats are digitized. From the last decades, Artificial intelligence (AI) utilizes the digitization advancement and enhances an automation in the medical domain. In <ref type="bibr" coords="2,124.70,567.23,10.66,8.96" target="#b3">[4]</ref>, natural language text (medical history, physical examination result, result of X-ray, ultrasound or ECG ) are collected, analysed and used to find the dependency between features to improve the healthcare quality in multidisciplinary paediatric centre using deep linguistic techniques. The advantage of digitization is also applicable for medical imaging applications like image classification <ref type="bibr" coords="2,312.80,615.26,10.66,8.96" target="#b4">[5]</ref>, caption generation <ref type="bibr" coords="2,407.47,615.26,11.69,8.96" target="#b5">[6]</ref> and computing severity level <ref type="bibr" coords="2,201.54,627.26,10.66,8.96" target="#b6">[7]</ref>. The reliability of these applications are based on the features extracted from the images. At present, the pre-trained models like Convolutional Neural Network (CNN) or pre-trained models like VGGNet or ResNet are playing a vital role in feature extraction for VQA related applications. VQA on medical domain emerged based on the knowledge obtained from real world datasets like MSCOCO dataset, DAQUAR, VQA Dataset, FM-IQA and Visual7W. The inferences are (i). The detailed understanding of the image and complex reasoning are required to answer the visual questions because it selectively targets background details and/or underlying context <ref type="bibr" coords="3,124.70,186.18,10.66,8.96" target="#b7">[8]</ref>. (ii). Questions are arbitrary and it imposes many sub-problems in computer vision like object location, detection and/or counting <ref type="bibr" coords="3,314.41,198.18,10.66,8.96" target="#b8">[9]</ref>. (iii) Improvement in rare question type has negligible impact on overall performance <ref type="bibr" coords="3,330.65,210.18,15.39,8.96" target="#b9">[10]</ref>. (iv). Least contributing question types need to be victimized because it pulls down the overall performance <ref type="bibr" coords="3,450.94,222.18,15.39,8.96" target="#b9">[10]</ref>. (v). Appropriate parameter selection (activation function, large mini-batches, smart shuffling of training data and word embedding by Glove, google images, etc.,) has its own impact in performance of the model. From 2018, ImageCLEF is conducting VQA task in medical domain. The VQA-Med 2018 and VQA-Med 2019 dataset contains organ, plane, modality and abnormality related visual question answer pairs. In these tasks, most of the researcher applied pretrained models like VGGNet, ResNet, etc., to encode medical images and Recurrent Neural Networks (RNN) to generate question encodings. Some of the researchers applied attention based mechanism to extract relevant image features to answer the questions. The highest BLEU, WBSS and CBSS scores obtained in 2018 tasks are 0.162, 0.186 and 0.338 respectively <ref type="bibr" coords="4,242.25,186.18,15.68,8.96" target="#b10">[11]</ref>. In 2019, along with the above approaches, different pooling strategies and transformer-based approaches are also used and attained a highest accuracy and BLEU score as 0.644 and 0.624 respectively <ref type="bibr" coords="4,379.15,210.18,15.39,8.96" target="#b11">[12]</ref>. The overall summary of the ImageCLEF VQA tasks are tabulated in Table <ref type="table" coords="4,361.39,222.18,3.76,8.96" target="#tab_0">1</ref>.</p><p>From the overall inference, VGGNet and LSTM are selected for the implementation of given task on VQA 2020. The advantages of selecting VGGNet <ref type="bibr" coords="4,394.14,246.18,16.72,8.96" target="#b13">[14]</ref> for image feature extraction includes: (i). Built on ImageNet dataset but works for other datasets and tasks. (ii). Outperforms the complex recognition tasks involving less detailed images. (iii). Addresses the vanishing gradient and exploding gradient problem. (iv). Illustrates the importance of deepest model in visual representation.</p><p>The advantages of using LSTM <ref type="bibr" coords="4,264.09,306.21,16.72,8.96" target="#b14">[15]</ref> are (i). Developed for TIMIT dataset, but it can solve any complex sequence learning problem in handwriting recognition, speech recognition, polyphonic music modelling, etc., (ii). The role of hyper parameters with respect to performance in LSTM structure includes: (a). Coupling the inputs, removing forget gates simplifies LSTM structure, reduces the number of parameters and computational cost, without significantly decreasing performance. (b). Gaussian noise is moderately helpful for TIMIT dataset but it is harmful for other datasets. (c). Highest measured interaction between hyper parameters are quite small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Description</head><p>In this section, three medical VQA dataset are discussed along with its description. The three datasets are: ImageCLEF VQA-Med 2020 dataset (Original Dataset (OD)) and two other datasets used with modification (Reduced Dataset (RD) and Augmented Reduced Dataset (ARD)). In Original dataset, (ImageCLEF VQA-Med 2020 dataset), the dataset is divided into three subsets namely training set, validation set and test set as 4000, 500 and 500 with equivalent number of question answer pairs. In addition the dataset consists of abnormality related visual questions for different organs (e.g. lung, skull, spine, gastrointestinal, musculoskeletal), planes (e.g. axial, sagittal and corona) and modalities (e.g. CT, X-ray, MRI). For better learning, the training set and validation set (as a total 4500 samples) are used for training. The Reduced Dataset (RD) consists of 4348 samples (from training and validation set) for training and 500 samples for testing. The reduced dataset is generated by two ways namely (i). Eliminate the least contributing samples (ii). Identify and reduce the number of samples of similar class, when the count deviates much from the remaining classes. These samples degrade the overall performance of the system and hence both approaches are applied.</p><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">System Design</head><p>In this VQA task, the VGGNet and LSTM techniques are used to answer the medical visual questions. The system design of the proposed model is shown in Fig. <ref type="figure" coords="5,431.91,231.18,3.76,8.96">1</ref>. In this, the feature information from the medical image and its question-answer pairs are extracted and concatenated by encoder. Then, the concatenated feature vector is decoded by timestamp to generate the answer, with post-processing at the end. The proposed model consists of five modules namely, (i). Pre-processing (ii). Encoder (iii). Decoder, (iv). Post-processing and (v). Answer prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. System design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pre-processing</head><p>In the pre-processing stage, the input samples are converted to required format for effective image and text processing. As a first step, the images are reshaped to (229, 229) dimension (preferable input size of VGG16/VGG19 network). In text processing, comma is the best separator and hence the question-answer pairs are converted to comma separated file. The already existing comma within the field are converted to related special symbol (here we used semicolon). Otherwise, these commas within the field are encountered as separator, and end up with an imbalanced fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Encoder</head><p>Encoder transforms the feature vectors into the required format for the model to answer the questions. This transformation is required because the type and dimension of features extracted from image and its respective text are different. Hence a dimensionality mapping is required to bridge the gaps and then the features vectors are concatenated.</p><p>To perform this, encoder has three sub-modules namely (i). Image processing (ii). Text processing (iii). Concatenation. The system architecture of the encoder is given in Fig. <ref type="figure" coords="5,466.42,670.82,4.16,8.96">2</ref> shows the each encoding stages along with the size of feature vector before and after concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Processing.</head><p>In the proposed system, the image features are extracted by VGG16/VGG19. The last layer of the VGGNet is frozen and the resulted model is used for image feature extraction as transfer learning approach. The last layer is frozen because VGGNet is trained for ImageNet dataset (1000 classes) but we required the output dimension to be 1024. For this reason, after the last before layer, the dense and fully connected layers are used to adjust the dimension of the image feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Encoder</head><p>Text Processing.</p><p>Text processing, computes the dependency between the words and derives the information from the sequence of input words. The LSTM (an advanced type of RNN) is used to generate the text feature vector. The input text is tokenized into individual words and the minimum and maximum length of question and answer are computed. The LSTM computes the question embedding (using the Glove vector), timestamp by timestamp for the respective samples. This vector is given to the fully connected layer to project it to the same dimensional shape as image feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concatenation.</head><p>The computed feature vectors (image and text feature vector) are combined using element wise multiplication and are later used by decoder for model creation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Decoder</head><p>Both visual and textual features are merged into three dimensional vector (2048-dimensional space) which is a sequence of vectors. As both the image and textual features are represented as sequence of vectors (not as single vector), LSTM is required to feed the concatenated vector to the softmax layer. The system architecture of this sub-module, decoder is shown in Fig. <ref type="figure" coords="7,222.35,218.22,3.91,8.96">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3. Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Post-processing</head><p>In post-processing, the generated answer needs to be converted to the required format as in training set. In this, semicolon in the generated answers are converted back to comma format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Answer Prediction</head><p>In this stage, encoder-decoder model based on VGG-LSTM is generated. The answer for the test set can be predicted by the model. Further, the result can be analysed and evaluated using performance metrics like accuracy and BLEU score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>The proposed model is executed on three datasets (as discussed in Section 3) and analysed using five different combination of techniques, such as: (i). VGG16 (excluding last layer) followed by LSTM for original dataset (ii). Same as (i) for reduced dataset (iii). VGG16 (excluding last layer) followed by LSTM and post-processing at the end for Augmented Reduced Dataset (iv). Same as (iii), but the pre-trained model is VGG19 (v). Similar to first combination but post-processing is included at the end. From the results it is inferred that proposed model with post-processing is included at the end for Augmented Reduced Dataset gives better performance than the other combinations. In The performance of the model depends on appropriate parameter selection also. In this model, RMSPROP optimizer is used with a learning rate of 0.001 and the batch size, epoch and dropout are set to 256, 400 and 0.2 respectively. For training the model using these hyper parameters, each run took approximately 180 minutes in GPU. Among the five runs, third run achieved a better accuracy score of 0.282 and the BLEU score of 0.330. The final result of the leaderboard is given in Table <ref type="table" coords="8,399.48,473.27,4.98,8.96" target="#tab_4">3</ref> where our team achieved 9th place in the listed ranks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this paper, an approach for Visual Question Answering (VQA) on medical domain is implemented for ImageCLEF VQA-Med 2020 dataset and further analysed using two different types of proposed datasets namely: Reduced Dataset (RD) and Augmented Reduced Dataset (ARD). The proposed model has five stages namely: (i). Pre-processing (ii). Encoding (iii). Decoding (iv). Post-processing and (v). Answer prediction.</p><p>In pre-processing, the dataset has been converted to the specific input format as required for VGGNet and LSTM. Then the image and text features are extracted and concatenated. The concatenated feature vector is decoded for next level. In post-processing, the answer is converted to the format as in the training dataset. Finally, the generated model predicts the answer for the test set. Among the five runs of the proposed model the better result is achieved for augmented reduced dataset with an accuracy score of 0.282 and BLEU score of 0.330. In medical VQA domain, large amount of information needs to be extracted and hence it has more memory constraint. This can be addressed with the help of GPU and the selection of optimal hyper parameters. In future, the proposed VQA model can be improvised by developing a design of Convolutional Neural Network (CNN) for medical images and fixing the appropriated hyper parameters with visualization of layers. In addition, the advanced text processing approach like BERT, which represent each sentence in 768-d question feature vector can be included.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,124.68,303.36,345.84,161.04"><head></head><label></label><figDesc></figDesc><graphic coords="6,124.68,303.36,345.84,161.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,124.68,239.40,344.28,168.00"><head></head><label></label><figDesc></figDesc><graphic coords="7,124.68,239.40,344.28,168.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,133.46,282.02,333.08,334.73"><head>Table 1 .</head><label>1</label><figDesc>Brief description of ImageCLEF MED-VQA task for last three years</figDesc><table coords="3,133.46,299.97,333.08,316.78"><row><cell></cell><cell cols="2">Training</cell><cell cols="2">Validation</cell><cell cols="2">Test</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Performance anal-</cell></row><row><cell></cell><cell>set</cell><cell></cell><cell>set</cell><cell></cell><cell>set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ysis</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Category</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Image</cell><cell>QA pairs</cell><cell>Image</cell><cell>QA pairs</cell><cell>Image</cell><cell>QA pairs</cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>BLEU</cell><cell>CBSS</cell><cell>WBSS</cell></row><row><cell>[11]</cell><cell>2278</cell><cell>5413</cell><cell>324</cell><cell>500</cell><cell>264</cell><cell>500</cell><cell>Organ, plane,</cell><cell>modality and</cell><cell>abnormality</cell><cell>-</cell><cell>0.162</cell><cell>0.186</cell><cell>0.338</cell></row><row><cell>[12]</cell><cell>3200</cell><cell>12792</cell><cell>500</cell><cell>2000</cell><cell>500</cell><cell>500</cell><cell>Organ, plane,</cell><cell>modality and</cell><cell>abnormality</cell><cell>0.644</cell><cell>0.624</cell><cell>-</cell><cell>-</cell></row><row><cell>[13]</cell><cell>4000</cell><cell>4000</cell><cell>500</cell><cell>500</cell><cell>500</cell><cell>500</cell><cell>Abnormality</cell><cell></cell><cell></cell><cell>0.496</cell><cell>0.542</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,124.70,174.18,345.66,20.96"><head>Table 2 ,</head><label>2</label><figDesc>OD, RD and ARD represents Original dataset, Reduced Dataset and Augmented Reduced Dataset respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,132.74,209.99,323.80,184.02"><head>Table 2 .</head><label>2</label><figDesc>Brief description about each run</figDesc><table coords="8,132.74,239.94,323.80,154.07"><row><cell>Run number</cell><cell>Dataset</cell><cell>Techniques</cell><cell>Accuracy</cell><cell>BLEU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>score</cell></row><row><cell>1</cell><cell>OD</cell><cell>VGG16 and LSTM</cell><cell>0.274</cell><cell>0.321</cell></row><row><cell>2</cell><cell>ARD</cell><cell>VGG16 and LSTM</cell><cell>0.268</cell><cell>0.320</cell></row><row><cell>3</cell><cell>ARD</cell><cell>VGG16 and LSTM</cell><cell>0.282</cell><cell>0.330</cell></row><row><cell></cell><cell></cell><cell>(Post processing)</cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>RD</cell><cell>VGG19 and LSTM</cell><cell>0.248</cell><cell>0.292</cell></row><row><cell></cell><cell></cell><cell>(Post processing)</cell><cell></cell><cell></cell></row><row><cell>5</cell><cell>OD</cell><cell>VGG16 and LSTM</cell><cell>0.276</cell><cell>0.323</cell></row><row><cell></cell><cell></cell><cell>(Post processing)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,140.90,509.08,316.36,175.74"><head>Table 3 .</head><label>3</label><figDesc>Top 10 ranking of ImageCLEF 2020 VQA-MED</figDesc><table coords="8,140.90,539.03,316.36,145.79"><row><cell>Rank</cell><cell>Team name</cell><cell>Accuracy</cell><cell>BLEU</cell><cell>No. of runs</cell></row><row><cell></cell><cell></cell><cell></cell><cell>score</cell><cell>submitted</cell></row><row><cell>1</cell><cell>z_liao</cell><cell>0.496</cell><cell>0.542</cell><cell>5</cell></row><row><cell>2</cell><cell>TheInceptionTeam</cell><cell>0.480</cell><cell>0.511</cell><cell>5</cell></row><row><cell>3</cell><cell>bumjun_jung</cell><cell>0.466</cell><cell>0.502</cell><cell>5</cell></row><row><cell>4</cell><cell>going</cell><cell>0.426</cell><cell>0.462</cell><cell>5</cell></row><row><cell>5</cell><cell>NLM</cell><cell>0.400</cell><cell>0.441</cell><cell>5</cell></row><row><cell>6</cell><cell>harendrakv</cell><cell>0.378</cell><cell>0.439</cell><cell>7</cell></row><row><cell>7</cell><cell>shengyan</cell><cell>0.376</cell><cell>0.412</cell><cell>5</cell></row><row><cell>8</cell><cell>kdevqa</cell><cell>0.314</cell><cell>0.350</cell><cell>4</cell></row><row><cell>9</cell><cell>sheerin</cell><cell>0.282</cell><cell>0.330</cell><cell>5</cell></row><row><cell>10</cell><cell>umassmednlp</cell><cell>0.220</cell><cell>0.340</cell><cell>4</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>Our profound gratitude to <rs type="institution">SSN College of Engineering, Department of CSE</rs>, for allowing us to utilize the <rs type="institution">High Performance Computing Laboratory and GPU Server</rs> for the execution of this challenge successfully.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,132.67,531.04,337.95,8.10;9,141.74,542.08,328.51,8.10;9,141.74,553.12,328.40,8.10;9,141.74,564.04,328.58,8.10;9,141.74,575.08,328.46,8.10;9,141.74,586.12,328.58,8.10;9,141.74,597.04,328.73,8.10;9,141.74,607.13,328.75,9.05;9,141.74,619.15,313.85,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,424.86,575.08,45.33,8.10;9,141.74,586.12,328.58,8.10;9,141.74,597.04,44.60,8.10">Overview of the ImageCLEF 2020: Multimedia Retrieval in Lifelogging, Medical, Nature and Internet Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S D</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,207.08,597.04,263.39,8.10;9,141.74,607.13,324.87,9.05">Experimental IR Meets Multilinguality, Multimodality and Interaction, Proceedings of the 11 th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="9,238.10,619.15,151.88,8.10">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,630.07,337.90,8.10;9,141.74,641.11,328.65,8.10;9,141.74,652.15,159.67,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,250.57,630.07,219.99,8.10;9,141.74,641.11,172.95,8.10">TUA1 a ImageCLEF 2019 VQA-Med: A Classification and Generation Model based on Transfer Learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,336.20,641.11,134.19,8.10;9,141.74,652.15,82.11,8.10">CLEF 2019 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,663.07,337.93,8.10;9,141.74,674.11,328.75,8.10;10,141.74,149.99,328.87,8.10;10,141.74,161.03,63.05,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,336.07,674.11,134.42,8.10;10,141.74,149.99,79.90,8.10">Technologies for Complex Intelligent Clinical Data Analysis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Baranov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Namazova-Baranova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">V</forename><surname>Smirnov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Devyatkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">O</forename><surname>Shelmanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Vishneva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">V</forename><surname>Antonova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">I</forename><surname>Smirnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,240.05,149.99,189.31,8.10">Annals of the Russian Academy of Medical Sciences</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="160" to="171" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,172.07,337.68,8.10;10,141.74,182.99,328.48,8.10;10,141.74,194.03,328.61,8.10;10,141.74,205.07,99.10,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,154.43,182.99,315.79,8.10;10,141.74,194.03,134.05,8.10">Multiple Abnormality Detection for Automatic Medical Image Diagnosis using Bifurcated Convolutional Neural Network</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hajabdollahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Esfandiarpoor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sabeti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M R</forename><surname>Soroushmehr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Samavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,296.54,194.03,155.59,8.10">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="101792" to="101802" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,215.99,338.07,8.10;10,141.74,227.03,328.75,8.10;10,141.74,238.07,92.07,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,336.19,215.99,134.55,8.10;10,141.74,227.03,122.51,8.10">ImageCLEF 2018: Lesion-based TB-Descriptor for CT Image Analysis</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Snezhko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,283.09,227.03,98.86,8.10">CLEF 2018 Working Notes</title>
		<title level="s" coord="10,388.36,227.03,82.13,8.10;10,141.74,238.07,29.95,8.10">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Belarus</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,248.99,337.84,8.10;10,141.74,260.06,328.69,8.10;10,141.74,271.10,69.51,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,365.12,248.99,105.39,8.10;10,141.74,260.06,108.62,8.10">Overview of the ImageCLEF 2018 Caption Prediction Tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S D</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,270.04,260.06,97.20,8.10">CLEF 2018 Working Notes</title>
		<title level="s" coord="10,373.05,260.06,97.38,8.10;10,141.74,271.10,13.81,8.10">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,282.02,337.58,8.10;10,141.74,293.06,328.89,8.10;10,141.74,304.10,328.64,8.10;10,141.74,315.02,44.08,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,402.76,282.02,67.48,8.10;10,141.74,293.06,328.89,8.10;10,141.74,304.10,59.88,8.10">ImageCLEF 2019: A 2D Convolutional Neural Network Approach for Severity Scoring of Lung Tuberculosis using CT Images</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kavitha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Nandhinee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harshana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Srividya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Harrinei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,220.61,304.10,213.17,8.10">CLEF 2019 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,326.06,337.57,8.10;10,141.74,337.10,328.75,8.10;10,141.74,348.02,72.12,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,141.74,337.10,123.40,8.10">VQA: Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,287.45,337.10,165.41,8.10">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,359.06,337.85,8.10;10,141.74,370.10,280.54,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,222.66,359.06,247.85,8.10;10,141.74,370.10,21.58,8.10">Visual Question Answering: Datasets, Algorithms and Future Challenges</title>
		<author>
			<persName coords=""><forename type="middle">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,181.54,370.10,155.60,8.10">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,381.02,338.04,8.10;10,141.74,392.06,328.78,8.10;10,141.74,403.10,241.53,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,325.61,381.02,144.84,8.10;10,141.74,392.06,160.37,8.10">Tips and tricks for Visual Question Answering: Learning from the 2017 Challenge</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,322.39,392.06,148.13,8.10;10,141.74,403.10,149.83,8.10">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4223" to="4232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,414.02,337.93,8.10;10,141.74,425.06,328.97,8.10;10,141.74,436.12,185.62,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,377.95,414.02,92.38,8.10;10,141.74,425.06,204.92,8.10">Overview of ImageCLEF 2018 Medical Domain Visual Question Answering Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,366.67,425.06,104.04,8.10;10,141.74,436.12,107.98,8.10">CLEF 2018 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,447.04,338.22,8.10;10,141.74,458.08,328.68,8.10;10,141.74,469.12,290.12,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,448.03,447.04,22.58,8.10;10,141.74,458.08,312.00,8.10">VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,141.74,469.12,212.56,8.10">CLEF 2019 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,480.04,338.23,8.10;10,141.74,491.08,328.47,8.10;10,141.74,502.12,328.65,8.10;10,141.74,513.04,54.04,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,449.76,480.04,20.87,8.10;10,141.74,491.08,328.47,8.10;10,141.74,502.12,95.89,8.10">Overview of the VQA-Med Task at ImageCLEF 2020: Visual Question Answering and Generation in the Medical Domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,256.00,502.12,210.53,8.10">CLEF 2020 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,524.08,338.18,8.10;10,141.74,535.12,328.94,8.10;10,141.74,546.04,26.34,8.10" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,253.70,524.08,216.88,8.10;10,141.74,535.12,42.42,8.10">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,203.01,535.12,194.62,8.10">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,557.08,338.12,8.10;10,141.74,568.12,328.86,8.10;10,141.74,579.04,114.07,8.10" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,433.81,557.08,36.71,8.10;10,141.74,568.12,79.98,8.10">LSTM: A Search Space Odyssey</title>
		<author>
			<persName coords=""><forename type="middle">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">B R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,240.98,568.12,225.54,8.10">IEEE Transcations on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,590.08,337.80,8.10;10,141.74,601.12,328.75,8.10;10,141.74,612.07,275.05,8.10" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,405.30,590.08,64.91,8.10;10,141.74,601.12,178.12,8.10">Overcoming Data Limitation in Medical Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,338.35,601.12,132.14,8.10;10,141.74,612.07,197.22,8.10">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
