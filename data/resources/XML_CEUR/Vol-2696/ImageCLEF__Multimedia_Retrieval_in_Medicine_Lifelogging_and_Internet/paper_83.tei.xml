<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,169.17,114.32,277.00,14.35;1,137.90,132.25,339.56,14.35;1,260.80,150.18,93.75,14.35">Coral Reef annotation, localisation and pixel-wise classification using Mask R-CNN and Bag of Tricks</title>
				<funder ref="#_q9FZQRP">
					<orgName type="full">Ministry of Education, Youth and Sports of the Czech Republic</orgName>
				</funder>
				<funder ref="#_SFmwGEd">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,187.96,189.04,52.90,9.96"><forename type="first">Lukáš</forename><surname>Picek</surname></persName>
							<idno type="ORCID">0000-0002-6041-9722</idno>
							<affiliation key="aff0">
								<orgName type="department">of Information</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.72,189.04,59.38,9.96"><forename type="first">Antonín</forename><surname>Říha</surname></persName>
							<idno type="ORCID">0000-0002-6041-9722</idno>
							<affiliation key="aff0">
								<orgName type="department">of Information</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.00,189.04,39.62,9.96"><forename type="first">Aleš</forename><surname>Zita</surname></persName>
							<idno type="ORCID">0000-0001-8119-3147</idno>
							<affiliation key="aff0">
								<orgName type="department">of Information</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,169.17,114.32,277.00,14.35;1,137.90,132.25,339.56,14.35;1,260.80,150.18,93.75,14.35">Coral Reef annotation, localisation and pixel-wise classification using Mask R-CNN and Bag of Tricks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">787DFCCBC7884C48AE5CA38FC2ABEA20</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Computer Vision</term>
					<term>Instance Segmentation</term>
					<term>Convolutional Neural Networks</term>
					<term>Machine Learning</term>
					<term>Object Detection</term>
					<term>Corals</term>
					<term>Biodiversity</term>
					<term>Conservation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes an automatic system for detection, classification and segmentation of individual coral substrates in underwater images. The proposed system achieved the best performances in both tasks of the second edition of the ImageCLEFcoral competition. Specifically, mean average precision with Intersection over Union (IoU) greater then 0.5 (mAP@0.5) of 0.582 in case of Coral reef image annotation and localisation, and mAP@0.5 of 0.678 in Coral reef image pixel-wise parsing. The system is based on Mask R-CNN object detection and instance segmentation framework boosted by advanced training strategies, pseudo-labeling, test-time augmentations, and Accumulated Gradient Normalisation. To support future research, code has been made available at: https://github.com/picekl/ImageCLEF2020-DrawnUI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEFcoral <ref type="bibr" coords="1,233.26,528.70,10.53,9.96" target="#b3">[4]</ref> challenge was organized in conjunction with the Im-ageCLEF 2020 evaluation campaign <ref type="bibr" coords="1,304.37,540.66,15.51,9.96" target="#b11">[12]</ref> at the Conference and Labs of the Evaluation Forum (CLEF 1 ). The main goal for this competition was to create such an algorithm or system that can automatically detect and annotate a variety of benthic substrate types over image collections taken from multiple coral reefs as part of a coral reef monitoring project with the Marine Technology Research Unit at the University of Essex. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivation</head><p>Live corals are an important biological class that has a massive contribution to the ocean ecosystem biodiversity. Corals are key habitat for thousands of marine species <ref type="bibr" coords="2,168.45,360.98,10.53,9.96" target="#b4">[5]</ref> and provide an essential source of nutrition and yield for people in the developing countries <ref type="bibr" coords="2,247.35,372.93,10.52,9.96" target="#b2">[3,</ref><ref type="bibr" coords="2,257.87,372.93,7.01,9.96" target="#b1">2]</ref>. Therefore, automatic monitoring of coral reefs condition plays a crucial part in understanding future threats and prioritizing conservation efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Datasets</head><p>This section will briefly describe the provided data and their subsets: an annotated dataset that contains 440 images, and a testing dataset with 400 images without annotations. Additionally, we introduce an precisely engineered training/validation split of the annotated dataset for the training purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotated dataset -</head><p>The annotated dataset is a combination of 440 images containing 12,082 individual coral objects. Each coral was annotated with expert level knowledge, including segmentation mask, bounding box, and class that represents 1 out of 13 substrate types. The dataset is heavily unbalanced (refer to Table <ref type="table" coords="2,173.24,559.25,3.88,9.96" target="#tab_0">1</ref>), having almost 50% of objects from a single class (Soft Coral) and approximately 8% for the eight least frequent classes. Moreover, images have different colour variations, are heavily blurred, and came from different locations and geographical regions. Furthermore, coral substrates belonging to the same class can be observed in different morphology, colour variations, or patterns. Finally, some images contain a measurement tape that partially covers objects of interest.</p><p>For the network training process evaluation, the annotated dataset needed to be divided into two parts. One used for network optimization and the second for network performance validation. To create these subsets, every tenth image was designated for validation set, the rest was used for training. As the validation set class distribution did not match the training one, particular images from the validation set needed to be replaced by carefully cherry-picked images from the training set. This resulted in an almost perfect split with similar distributions for both, the training and the validation set. This similarity ensured a representative validation process.</p><p>Testing dataset -The testing dataset contains 400 images from four different locations. Namely, the same location as is in the training set, similar location to the training set, geographically similar location to the training set, and geographically distinct location from the training set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">The System</head><p>The proposed object detection and instance segmentation system extends recent state-of-the-art Convolutional Neural Network (CNN) object detection framework (Mask R-CNN <ref type="bibr" coords="3,227.58,573.05,10.80,9.96" target="#b7">[8]</ref>) with additional Bag of Tricks that considerably increased the performance. The TensorFlow Object Detection API<ref type="foot" coords="3,415.76,583.80,3.97,6.97" target="#foot_0">2</ref>  <ref type="bibr" coords="3,423.43,585.00,15.50,9.96" target="#b10">[11]</ref> was used as a deep learning framework for fine-tuning the publicly available checkpoints. All bells and whistles are further described in Section 2. Additionally, approaches that did not contribute positively but could have some potential for future editions of the ImageCLEFcoral competition are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>This section describes all approaches and techniques used in the benthic substrate detection, annotation and segmentation tasks. The modern object detection and instance segmentation methods are summarized, followed by the description of the chosen system and its configuration. Furthermore, all the used bells and whistles (Bag of Tricks) are introduced and described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Object Detection</head><p>Although conventional digital image processing methods are capable of detecting particular local features, modern object detectors based on Deep Convolutional Neural Networks (DCNN) achieve superior performance in object detection and instance segmentation tasks. Several network architectures were pre-selected based on study published by Huang et al. <ref type="bibr" coords="4,321.96,275.67,14.62,9.96" target="#b10">[11]</ref>, namely the Faster R-CNN <ref type="bibr" coords="4,462.32,275.67,14.62,9.96" target="#b17">[18]</ref>, SSD <ref type="bibr" coords="4,156.41,287.63,15.51,9.96" target="#b14">[15]</ref> and Mask R-CNN <ref type="bibr" coords="4,255.94,287.63,9.97,9.96" target="#b7">[8]</ref>. The initial performance experiment was to train these detection frameworks with default or recommended configurations. This experiment revealed the most suitable framework for both the tasks within the ImageCLEFcoral competition -the Mask R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network parameters</head><p>Experiments on the validation set, reveled the best optimizer settings for the framework. These settings were shared between all of our experiments, unless stated otherwise. For detailed description refer to Table <ref type="table" coords="4,379.30,388.63,3.88,9.96" target="#tab_1">2</ref>. By utilizing techniques mentioned above, we have increased the model mAP@0.5 performance by 0.0392 as measured on the validation set.</p><p>Input Resolution -In the task of object detection, primarily where a small object occurs, input resolution plays a crucial role. Theoretically, the higher the resolution is, the more objects will be detected. Unfortunately, the detection of high resolution images is GPU memory-limited. Hence, it always is a trade-off between performance and hardware requirements.</p><p>Backbone -To find the best backbone architecture for Mask R-CNN framework. We performed an experiment over 3 different backbone models including ResNet-50 <ref type="bibr" coords="5,183.67,394.87,9.97,9.96" target="#b8">[9]</ref>, ResNet-101 <ref type="bibr" coords="5,255.04,394.87,9.97,9.96" target="#b8">[9]</ref>, and Inception-ResNet-V2 <ref type="bibr" coords="5,388.15,394.87,14.63,9.96" target="#b19">[20]</ref>. Detailed performance comparison is included in Table <ref type="table" coords="5,305.65,406.83,3.88,9.96" target="#tab_2">3</ref>. Pseudo Labels -Performance of DCNN's heavily depends on the size of the training set. To facilitate this issue, we have developed a naive pseudo-labelling approach inspired by <ref type="bibr" coords="5,230.10,595.12,9.97,9.96" target="#b0">[1]</ref>. In short, already trained network is used to label the unlabelled testing data with so-called weak labels. Only the overconfident detections were used; the rest of the image was blurred out. Even though there is a high chance of overfitting to incorrect pseudo-labels due to the confirmation bias, pseudo-labels can significantly improve the performance of the CNN if pseudo-labelled images are added sensitively.</p><p>Transfer Learning -Big-transfer <ref type="bibr" coords="6,299.40,117.77,15.51,9.96" target="#b12">[13]</ref> or transfer learning is a fine-tuning technique commonly used in deep learning. Rather then initialize the weights of neural network randomly, pretrained weights are used. Furthermore, final model could benefit from similar domain weights. To evaluate a potential of such approach for the purposes of this competition, we experimented with finetuning of the publicly available checkpoints, including ImageNet<ref type="foot" coords="6,415.84,176.34,3.97,6.97" target="#foot_1">3</ref> , iNaturalist 3 , COCO <ref type="bibr" coords="6,168.84,189.50,14.62,9.96" target="#b13">[14]</ref>, PlantCLEF2018 <ref type="bibr" coords="6,265.78,189.50,15.51,9.96" target="#b18">[19]</ref> and PlanCLEF2019 <ref type="bibr" coords="6,376.61,189.50,14.62,9.96" target="#b16">[17]</ref>. The idea was that fine-tuning checkpoints trained on nature-oriented datasets would outperform the non-nature oriented ones. One could assume, that this is caused by significant difference when compared to other domains. Based on that it has been decided to use the COCO pretrained checkpoint which includes both the backbone and region proposed weights. Test Time Augmentations -Test time augmentation is a method of applying transformations on a given image to generate its several slightly different variations that are used to create predictions that, when combined, can improve final prediction. Our submissions utilized augmentations consisting of simple horizontal and vertical flips of the image. Their combinations produced four sets of detections for each image. These sets were then joined using voting strategy described in <ref type="bibr" coords="6,190.47,491.57,15.51,9.96" target="#b15">[16]</ref> by Moshkov et al..</p><p>Ensembles -Ensemble methods combine predictions from multiple models to obtain final output <ref type="bibr" coords="6,224.37,527.89,14.62,9.96" target="#b20">[21]</ref>. These methods can be used to improve accuracy in machine learning tasks. In our work, we utilize a simple method for combining outputs from multiple detection networks based on voting <ref type="bibr" coords="6,395.44,551.80,14.62,9.96" target="#b15">[16]</ref>. Detections describing one object are grouped together by size of the overlap region belonging to the same class. Instances, where majority of the detectors agree on class label and position are replaced by single detection with the highest score.</p><p>Accumulated Gradient Normalization -In order to achieve the best performance possible, we aimed to maximize the resolution of input data. Therefore, Fig. <ref type="figure" coords="8,154.13,290.06,4.13,8.97">2</ref>. Results for all runs submitted in annotation and localisation task by the competition participants, including mAP@0.0 and mAP@0.5 metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Competition Results</head><p>The official competition results are shown in Figure <ref type="figure" coords="8,363.65,375.19,4.98,9.96">2</ref> for annotation and localisation task, and in Figure <ref type="figure" coords="8,258.07,387.14,4.98,9.96">3</ref> for pixel-wise parsing. Our System achieved the best performances in both tasks of the second edition of the ImageCLEFcoral competition. Specifically, mAP@0.5 of 0.582 in case of Coral reef image annotation and localisation (Run ID 68143), and mAP@0.5 of 0.678 in Coral reef image pixel-wise parsing (Run ID 67864). Results of all our submissions are listed in Table <ref type="table" coords="8,198.05,446.92,3.88,9.96" target="#tab_5">5</ref>. Table <ref type="table" coords="8,234.50,446.92,4.98,9.96" target="#tab_6">6</ref> illustrates the performance over different subsets of the test dataset. The system performed comparably over the Same Location (SL), Similar Location (SiL) and Geographically Similar Location (GS) subsets. The performance significantly drops in Geographically Distinct Location (GD). This is probably caused by a lack of diverse training data. The best scoring submission for pixel-wise parsing task was a single Mask R-CNN with ResNet-50 backbone architecture and input resolution of 1000 × 1000. The system was trained for 50 epochs while using heavy augmentations as described in Section 2.3. Additionally, the pseudo-labeling (refer to Section 2.3) was used to increase the training dataset size with overconfident detections from the test set. Finally, the predictions were filtered with confidence threshold of 0.95 to maximize the official mAP metric while still having decent recall score.</p><p>The best scoring submission for annotation and localisation task was an ensemble of two checkpoints of the same Mask R-CNN model with ResNet-50 backbone architecture and input resolution of 1000×1000, one taken after 40 and other one after 50 epochs. The system was trained using heavy augmentations. Furthermore, the predictions were filtered with confidence threshold of 0.999 to maximize the official metric of mAP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Discussion</head><p>The proposed system designed for automatic pixel-wise detection of 13 coral substrates achieved impressive mAP@0.5 of 0.582 in localization task and 0.678, for instance segmentation task of the ImageCLEFcoral competitions. The system is wrapped up around the Mask R-CNN, the state-of-the-art instance segmentation framework, and additional known as well as some unique techniques, e.g., detection ensemble, test time data augmentations, accumulated gradient normalization, and pseudo-labelling. Surprisingly, results for pixel-wise parsing are considerably better. This is unexpected mainly because the test set is the same for both tasks, and our submissions used the same set of detections. Therefore, more similar scores were expected. This led us to believe that annotations for both tasks are not the same. mAP@0.0 mAP@0.5 Fig. <ref type="figure" coords="10,154.13,301.53,4.13,8.97">3</ref>. Results for all runs submitted in pixel-wise parsing task by the competition participants, including mAP@0.0 and mAP@0.5 metrics.</p><p>More in-depth performance examination of our submissions revealed a small regularisation capability related to geographical regions and specific locations. This is indication that the network could be over-fitted on the training dataset location, which have specific distribution of coral species. The system could achieve better performance with class priors corresponding to desired location. If the location transfer is essential, location generalisation should be main goal for the future challenges.</p><p>While comparing the model performance with the top results from the previous edition of this challenge (mAP@0.5 of 0.2427 and 0.0419), our model achieved superior performance. Even though the test datasets are not identical, such difference shows the increasing trend of machine learning model performance. This increase is probably related to a higher number of training images.</p><p>Lastly, due to our GPU memory constraints we were limited to an input image resolution of 1000 × 1000 combined with ResNet-50 backbone. Conducted experiments showed that input resolution of 1200 × 1200 and ResNet-101 would yield better results, therefore usage of GPUs with more memory would lead to a considerable increase of the system's performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,262.19,345.83,8.97;2,134.77,273.15,345.83,8.97;2,134.77,284.11,247.79,8.97;2,134.96,115.82,171.18,128.39"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example training images showing different types of annotations -Bounding Boxes and Segmentation Masks. Every colour represents one substrate type, e.g. yellow represents Soft Coral and red belongs to Hard Coral Boulder.</figDesc><graphic coords="2,134.96,115.82,171.18,128.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,292.72,345.82,196.66"><head>Table 1 .</head><label>1</label><figDesc>Dataset class distribution including training and validation split description. 396 images were used for training; 44 for validation.</figDesc><table coords="3,140.60,324.16,334.17,165.23"><row><cell cols="2">Dataset distribution</cell><cell></cell><cell cols="2">Train. / Val. split</cell></row><row><cell>Substrate type</cell><cell cols="2"># Bboxes Fraction [%]</cell><cell cols="2">Train. Boxes Val. Boxes</cell></row><row><cell>Soft Coral</cell><cell>5,663</cell><cell>46.87</cell><cell>5,035</cell><cell>628</cell></row><row><cell>Sponge</cell><cell>1,691</cell><cell>13.99</cell><cell>1,472</cell><cell>219</cell></row><row><cell>Hard Coral -Boulder</cell><cell>1,642</cell><cell>13.59</cell><cell>1,513</cell><cell>129</cell></row><row><cell>Hard Coral -Branching</cell><cell>1,181</cell><cell>9.774</cell><cell>1,084</cell><cell>97</cell></row><row><cell>Hard Coral -Encrusting</cell><cell>946</cell><cell>7.829</cell><cell>831</cell><cell>115</cell></row><row><cell>Hard Coral -Mushroom</cell><cell>223</cell><cell>1.845</cell><cell>199</cell><cell>24</cell></row><row><cell>Hard Coral -Submassive</cell><cell>198</cell><cell>1.845</cell><cell>162</cell><cell>36</cell></row><row><cell>Hard Coral -Foliose</cell><cell>177</cell><cell>1.464</cell><cell>144</cell><cell>33</cell></row><row><cell>Sponge -Barrel</cell><cell>139</cell><cell>1.150</cell><cell>124</cell><cell>15</cell></row><row><cell>Algae -Macro or Leaves.</cell><cell>92</cell><cell>0.761</cell><cell>81</cell><cell>11</cell></row><row><cell>Soft Coral -Gorgonian</cell><cell>90</cell><cell>0.745</cell><cell>70</cell><cell>20</cell></row><row><cell>Hard Coral -Table</cell><cell>21</cell><cell>0.175</cell><cell>17</cell><cell>4</cell></row><row><cell>Fire Coral -Millepora</cell><cell>19</cell><cell>0.157</cell><cell>15</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,134.77,410.33,345.83,246.81"><head>Table 2 .</head><label>2</label><figDesc>Training and network parameters shared among all experiments.</figDesc><table coords="4,134.77,432.74,345.83,224.40"><row><cell>Parameter</cell><cell>Value</cell><cell>Parameter</cell><cell>Value</cell></row><row><cell>Optimizer</cell><cell>RMSprop</cell><cell>Gradient Clipping</cell><cell>12.5</cell></row><row><cell>Momentum</cell><cell>0.9</cell><cell>Input size</cell><cell>1000 × 1000</cell></row><row><cell>Initial and min LR</cell><cell>0.032 -0.00004</cell><cell>Feature extractor stride</cell><cell>8</cell></row><row><cell>LR decay type</cell><cell>Exponential</cell><cell>Pretrained Checkpoints</cell><cell>COCO</cell></row><row><cell>LR decay factor</cell><cell>0.975</cell><cell>Num epochs</cell><cell>50</cell></row><row><cell>Batch size</cell><cell>1</cell><cell>Gradient accumulation</cell><cell>16</cell></row><row><cell>2.3 Bag of Tricks</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Augmentations -The provided dataset contains 440 images. Considering that</cell></row><row><cell cols="4">44 were used for validation, 396 images is too few for robust network optimiza-</cell></row><row><cell cols="4">tion. To alleviate this issue, multiple data augmentation techniques were utilized.</cell></row><row><cell cols="4">The following methods were included in the final training pipeline:</cell></row><row><cell cols="4">Colour Distortions -Brightness variations with max delta of 0.2, contrast</cell></row><row><cell cols="4">and saturation variations scale each by random value in range of 0.8 -1.25,</cell></row><row><cell cols="4">hue variations offsets by random value of up to 0.02, and random RGB to</cell></row><row><cell cols="3">grayscale conversion with 10% probability.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,134.77,439.46,345.82,99.18"><head>Table 3 .</head><label>3</label><figDesc>Effect of input resolution and backbone architecture on model performance.</figDesc><table coords="5,167.64,460.23,277.00,78.41"><row><cell>Backbone</cell><cell cols="3">Input Resolution mAP@0.5 mAP@0.75</cell></row><row><cell>ResNet-50</cell><cell>600 × 600</cell><cell>0.1826</cell><cell>0.0956</cell></row><row><cell>ResNet-50</cell><cell>800 × 800</cell><cell>0.2077</cell><cell>0.1017</cell></row><row><cell>ResNet-50</cell><cell>1000 × 1000</cell><cell>0.2227</cell><cell>0.1260</cell></row><row><cell>ResNet-50</cell><cell>1200 × 1200</cell><cell>0.2380</cell><cell>0.1579</cell></row><row><cell>ResNet-101</cell><cell>800 × 800</cell><cell>0.2381</cell><cell>0.1453</cell></row><row><cell>Inception-ResNet-V2</cell><cell>800 × 800</cell><cell>0.2362</cell><cell>0.1361</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,134.77,281.76,345.82,105.94"><head>Table 4 .</head><label>4</label><figDesc>Transfer Learning experiment -Effect of pretrained weights on model performance. For this experiment, the Mask R-CNN with ResNet-50 backbone and input size of 800 × 800 was used.</figDesc><table coords="6,173.52,323.53,265.23,64.16"><row><cell>Pretrained weights</cell><cell>mAP@0.5</cell><cell>mAP@0.75</cell></row><row><cell>ImageNet (only backbone)</cell><cell>0.1826</cell><cell>0.0956</cell></row><row><cell>COCO (All Mask R-CNN weights)</cell><cell>0.2077</cell><cell>0.1017</cell></row><row><cell>iNaturalist (only backbone)</cell><cell>0.2091</cell><cell>0.0854</cell></row><row><cell>PlantCLEF2018 (only backbone)</cell><cell>0.1991</cell><cell>0.0914</cell></row><row><cell>PlantCLEF2019 (only backbone)</cell><cell>0.1895</cell><cell>0.0932</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,138.31,114.93,335.65,142.00"><head>Table 5 .</head><label>5</label><figDesc>Submission scores achieved over test set. Official competition metrics.</figDesc><table coords="9,138.31,135.70,335.65,121.24"><row><cell cols="8">Annotation and localisation task submissions</cell><cell></cell><cell></cell></row><row><cell>Submission 1D</cell><cell>2D</cell><cell>3D</cell><cell>4D</cell><cell>5D</cell><cell>6D</cell><cell>7D</cell><cell>8D</cell><cell>9D</cell><cell>10D</cell></row><row><cell cols="10">mAP@0.5 0.347 0.357 0.439 0.565 0.349 0.530 0.377 0.582 0.517 0.415</cell></row><row><cell cols="10">mAP@0.0 0.728 0.712 0.774 0.851 0.709 0.825 0.721 0.853 0.814 0.747</cell></row><row><cell cols="10">Run ID 67857 67858 67862 67863 68093 68094 68138 68143 68145 68146</cell></row><row><cell></cell><cell cols="6">Pixel-wise parsing task submissions</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Submission 1S</cell><cell>2S</cell><cell>3S</cell><cell>4S</cell><cell>5S</cell><cell>6S</cell><cell>7S</cell><cell>8S</cell><cell>9S</cell><cell>10S</cell></row><row><cell cols="10">mAP@0.5 0.441 0.678 0.434 0.629 0.470 0.664 0.407 0.624 0.617 0.507</cell></row><row><cell cols="10">mAP@0.0 0.694 0.845 0.689 0.817 0.701 0.842 0.675 0.813 0.807 0.727</cell></row><row><cell cols="10">Run ID 67856 67864 68092 68095 68137 68139 68140 68142 68144 68147</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,134.77,271.20,345.82,208.25"><head>Table 6 .</head><label>6</label><figDesc>Submission results achieved over 4 subsets of the testing set: Same Location (SL), Similar Location (SiL), Geographically Similar Location (GS), Geographically Distinct Location (GD).</figDesc><table coords="9,137.65,314.38,340.06,165.07"><row><cell></cell><cell cols="8">Annotation and localisation task submissions</cell><cell></cell><cell></cell></row><row><cell>Submission</cell><cell>1D</cell><cell>2D</cell><cell>3D</cell><cell>4D</cell><cell>5D</cell><cell>6D</cell><cell>7D</cell><cell>8D</cell><cell>9D</cell><cell>10D</cell></row><row><cell cols="11">SL mAP@0.5 0.401 0.417 0.489 0.614 0.410 0.566 0.434 0.648 0.547 0.475</cell></row><row><cell cols="11">SiL mAP@0.5 0.234 0.247 0.322 0.440 0.230 0.431 0.254 0.343 0.438 0.258</cell></row><row><cell cols="11">GS mAP@0.5 0.470 0.446 0.508 0.562 0.453 0.516 0.516 0.627 0.533 0.527</cell></row><row><cell cols="11">GD mAP@0.5 0.225 0.230 0.280 0.292 0.231 0.346 0.210 0.329 0.344 0.242</cell></row><row><cell>Run ID</cell><cell cols="10">67857 67858 67862 67863 68093 68094 68138 68143 68145 68146</cell></row><row><cell></cell><cell></cell><cell cols="6">Pixel-wise parsing task submissions</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Submission</cell><cell>1S</cell><cell>2S</cell><cell>3S</cell><cell>4S</cell><cell>5S</cell><cell>6S</cell><cell>7S</cell><cell>8S</cell><cell>9S</cell><cell>10S</cell></row><row><cell cols="11">SL mAP@0.5 0.527 0.744 0.513 0.670 0.545 0.742 0.480 0.663 0.656 0.583</cell></row><row><cell cols="11">SiL mAP@0.5 0.312 0.516 0.309 0.553 0.335 0.448 0.284 0.529 0.546 0.34</cell></row><row><cell cols="11">GS mAP@0.5 0.476 0.588 0.493 0.537 0.553 0.627 0.493 0.586 0.546 0.573</cell></row><row><cell cols="11">GD mAP@0.5 0.276 0.403 0.283 0.439 0.266 0.386 0.267 0.446 0.418 0.291</cell></row><row><cell>Run ID</cell><cell cols="10">67856 67864 68092 68095 68137 68139 68140 68142 68144 68147</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,144.73,655.70,320.13,8.97"><p>https://github.com/tensorflow/models/blob/master/research/object_detection</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="6,144.73,644.74,324.74,8.97;6,144.73,655.70,118.04,8.97"><p>https://github.com/tensorflow/models/blob/master/research/object_detection/ g3doc/tf1_detection_zoo.md</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="7,144.73,655.70,105.55,8.97"><p>https://www.aicrowd.com</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p><rs type="person">Lukáš Picek</rs> was supported by the <rs type="funder">Ministry of Education, Youth and Sports of the Czech Republic</rs> project No. <rs type="grantNumber">LO1506</rs>, and by the grant of the <rs type="projectName">UWB</rs> project No. <rs type="grantNumber">SGS-2019-027</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_q9FZQRP">
					<idno type="grant-number">LO1506</idno>
					<orgName type="project" subtype="full">UWB</orgName>
				</org>
				<org type="funding" xml:id="_SFmwGEd">
					<idno type="grant-number">SGS-2019-027</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>we have decided to train the network on mini-batches of size 1. To overcome disadvantages that comes with using minimal mini-batch size <ref type="bibr" coords="7,390.70,129.72,9.97,9.96" target="#b6">[7]</ref>, the Accumulated Gradient Normalization <ref type="bibr" coords="7,241.30,141.68,15.51,9.96" target="#b9">[10]</ref> technique was utilized. This approach resulted in a considerable performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submissions</head><p>For evaluation of the participants submissions, the AICrowd platform 4 was used. Each participating team was allowed to submit up to 10 submission files following specific requirements for both tasks. We have used allowed maximum for both tasks. Because we have utilized single architecture for both the detection and segmentation tasks, multiple submissions were produced using the same network. Therefore in the following part, we denoted annotation and localisation task submissions by D and pixel-wise parsing task submissions by S. Finally, thresholding was used to discard predictions with low confidence.</p><p>Baseline configuration -As a baseline for all our experiments we used Mask R-CNN with ResNet-50 as a backbone. For training we used parameters and augmentations described in Table <ref type="table" coords="7,305.81,343.53,4.24,9.96">2</ref>.2 and Section 2.3, respectively. Input resolution was 1000 × 1000 pixels.</p><p>Submission 1D/1S -Baseline experiment using a confidence threshold that corresponded to the best F1 score on our validation dataset (0.58).</p><p>Submission 2D -Submission 1D with a fixed programming bug that resulted in few detections being incorrectly generated.</p><p>Submission 3D -Submission 2D with confidence threshold set to 0.95.</p><p>Submission 4D/2S -Baseline configuration that used Pseudo-labels as described in Section 2.3. The confidence threshold was set to 0.95.</p><p>Submission 5D/3S -Baseline configuration that utilized test time augmentations as described in Section 2.3 with confidence threshold of 0.9.</p><p>Submission 6D/4S -Submission 5D/3S with confidence threshold of 0.999.</p><p>Submission 7D/5S -Ensemble of two checkpoints of baseline configuration model. Taken after 40 epochs and 50 epochs. Confidence threshold of 0.9.</p><p>Submission 8D/6S -Submission 7D/5S with confidence threshold of 0.999.</p><p>Submission 9D/8S -Submission 7D/5S with test time augmentations and with confidence threshold of 0.999.</p><p>Submission 10D/10S -Submission 7D/5S with confidence threshold of 0.95.</p><p>Submission 7S -Submission 9D/8S with confidence threshold of 0.9.</p><p>Submission 9S -Submission 9D/8S with modified voting ensemble. Only one detection is sufficient as opposed to majority voting.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,138.38,337.63,8.97;11,151.53,149.33,329.07,8.97;11,151.53,160.29,97.83,8.97" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02983</idno>
		<title level="m" coord="11,448.67,138.38,31.92,8.97;11,151.53,149.33,261.89,8.97">Pseudolabeling and confirmation bias in deep semi-supervised learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.96,170.63,337.63,8.97;11,151.53,181.59,154.34,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="11,210.15,170.63,270.44,8.97;11,151.53,181.59,83.66,8.97">Global status of coral reefs: In combination, disturbances and stressors become ratchets</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Birkeland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="35" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,191.93,337.63,8.97;11,151.53,202.89,329.06,8.97;11,151.53,213.84,25.61,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,394.78,191.93,85.81,8.97;11,151.53,202.89,144.93,8.97">The economic impact of ocean acidification on coral reefs</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">M</forename><surname>Brander</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Rehdanz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Tol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Van Beukering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,303.83,202.89,112.43,8.97">Climate Change Economics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page">1250002</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,224.18,337.63,8.97;11,151.53,235.14,329.06,8.97;11,151.53,246.10,329.06,8.97;11,151.53,257.06,180.36,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,204.96,235.14,275.63,8.97;11,151.53,246.10,68.99,8.97">Overview of the ImageCLEFcoral 2020 task: Automated coral reef image annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">G</forename><surname>Clift</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,242.80,246.10,105.44,8.97">CLEF2020 Working Notes</title>
		<title level="s" coord="11,356.01,246.10,124.58,8.97;11,151.53,257.06,21.70,8.97">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,267.39,337.63,8.97;11,151.53,278.35,285.97,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,333.62,267.39,146.97,8.97;11,151.53,278.35,38.39,8.97">Importance of live coral habitat for reef fishes</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Coker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Pratchett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,196.45,278.35,153.60,8.97">Reviews in Fish Biology and Fisheries</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="126" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,288.69,337.63,8.97;11,151.53,299.65,238.03,8.97" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m" coord="11,265.24,288.69,215.35,8.97;11,151.53,299.65,71.75,8.97">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.96,309.99,337.64,8.97;11,151.53,320.95,329.06,8.97;11,151.53,331.90,183.19,8.97" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m" coord="11,257.48,320.95,223.10,8.97;11,151.53,331.90,16.81,8.97">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.96,342.24,337.63,8.97;11,151.53,353.20,235.66,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,338.35,342.24,44.36,8.97">Mask r-cnn</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,403.98,342.24,76.62,8.97;11,151.53,353.20,189.06,8.97">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,363.54,337.63,8.97;11,151.53,374.50,329.06,8.97;11,151.53,385.46,47.75,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,299.05,363.54,177.60,8.97">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,165.85,374.50,314.74,8.97">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,395.79,337.97,8.97;11,151.53,406.75,133.47,8.97" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Spanakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Möckel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02368</idno>
		<title level="m" coord="11,307.43,395.79,144.08,8.97">Accumulated gradient normalization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.62,417.09,337.97,8.97;11,151.53,428.05,329.06,8.97;11,151.53,439.01,329.06,8.97;11,151.53,449.96,213.41,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,329.87,428.05,150.72,8.97;11,151.53,439.01,117.42,8.97">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,288.08,439.01,192.51,8.97;11,151.53,449.96,120.36,8.97">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7310" to="7311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,460.30,337.97,8.97;11,151.53,471.26,329.06,8.97;11,151.53,482.22,329.06,8.97;11,151.53,493.18,329.06,8.97;11,151.53,504.14,329.06,8.97;11,151.53,515.10,329.06,8.97;11,151.53,526.05,329.07,8.97;11,151.53,537.01,329.06,8.97;11,151.53,547.97,329.06,8.97;11,151.53,558.93,329.06,8.97;11,151.53,569.89,34.31,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,295.07,515.10,185.51,8.97;11,151.53,526.05,255.38,8.97">Overview of the ImageCLEF 2020: Multimedia retrieval in medical, lifelogging, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,426.42,526.05,54.18,8.97;11,151.53,537.01,329.06,8.97;11,151.53,547.97,253.34,8.97">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="11,456.14,547.97,24.44,8.97;11,151.53,558.93,138.61,8.97">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="11,142.62,580.23,337.97,8.97;11,151.53,591.19,257.07,8.97" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m" coord="11,151.53,591.19,228.38,8.97">Big transfer (bit): General visual representation learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,601.52,337.97,8.97;11,151.53,612.48,329.06,8.97;11,151.53,623.44,199.08,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,207.15,612.48,170.66,8.97">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,397.63,612.48,82.96,8.97;11,151.53,623.44,75.99,8.97">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,633.78,337.97,8.97;11,151.53,644.74,329.06,8.97;11,151.53,655.70,107.09,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,151.53,644.74,138.83,8.97">Ssd: Single shot multibox detector</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,312.36,644.74,164.53,8.97">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,118.57,337.98,8.97;12,151.53,129.53,329.06,8.97;12,151.53,140.49,144.38,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,442.39,118.57,38.21,8.97;12,151.53,129.53,324.81,8.97">Test-time augmentation for deep learning-based cell segmentation on microscopy images</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Moshkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mathe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kertesz-Farkas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hollandi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Horvath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,151.53,140.49,67.69,8.97">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,151.45,337.97,8.97;12,151.53,162.41,329.06,8.97;12,151.53,173.37,215.29,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,281.13,151.45,199.46,8.97;12,151.53,162.41,182.80,8.97">Recognition of the amazonian flora by inception networks with test-time class prior estimation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,355.31,162.41,125.28,8.97;12,151.53,173.37,186.62,8.97">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,184.33,337.97,8.97;12,151.53,195.28,329.06,8.97;12,151.53,206.24,329.06,8.97;12,151.53,217.20,78.40,8.97" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,308.96,184.33,171.63,8.97;12,151.53,195.28,150.91,8.97">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,287.20,206.24,193.39,8.97;12,151.53,217.20,29.91,8.97">Advances in Neural Information Processing Systems 28</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,228.16,337.97,8.97;12,151.53,239.12,329.06,8.97;12,151.53,250.08,149.98,8.97" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,276.59,228.16,203.99,8.97;12,151.53,239.12,108.45,8.97">Plant recognition by inception networks with testtime class prior estimation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,283.02,239.12,197.57,8.97;12,151.53,250.08,121.31,8.97">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,261.04,337.97,8.97;12,151.53,272.00,329.06,8.97;12,151.53,282.96,143.19,8.97" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,360.34,261.04,120.25,8.97;12,151.53,272.00,204.40,8.97">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,377.38,272.00,103.20,8.97;12,151.53,282.96,114.51,8.97">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,293.91,337.97,8.97;12,151.53,304.87,25.61,8.97" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="12,227.51,293.91,213.11,8.97">Ensemble machine learning: methods and applications</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
