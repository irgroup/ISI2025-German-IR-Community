<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,171.31,115.96,272.75,12.62">Essex at ImageCLEFcaption 2020 task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.15,153.63,105.26,8.74"><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">University of Essex</orgName>
								<address>
									<settlement>Colchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.21,153.63,114.97,8.74"><forename type="first">Francisco</forename><forename type="middle">Parrilla</forename><surname>Andrade</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">University of Essex</orgName>
								<address>
									<settlement>Colchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,216.87,165.58,57.56,8.74"><forename type="first">Luke</forename><surname>Bentley</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">University of Essex</orgName>
								<address>
									<settlement>Colchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.12,165.58,101.37,8.74"><forename type="first">Arely</forename><forename type="middle">Aceves</forename><surname>Compean</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">University of Essex</orgName>
								<address>
									<settlement>Colchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,171.31,115.96,272.75,12.62">Essex at ImageCLEFcaption 2020 task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D601DFAAC439E1B2923160A208B25FEB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>image understanding</term>
					<term>concept detection</term>
					<term>medical image retrieval</term>
					<term>Densely Connected Convolutional Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The University of Essex participated in the fourth edition of the ImageCLEFcaption task which aims to detect concepts on radiology images as an approach to medical image understanding. In this paper, the University of Essex team presents its participation in the ImageCLEF 2020 caption task based on a retrieval based approach for concept detection. A Densely Connected Convolutional Network is used to encode the images. This paper explores compares several modification of the baseline considering several aspects such as the image modality or the selection of concepts among the top retrieved images. The University of Essex was third best team participating in the task achieving a 0.381 mean F1 score, very close to the results obtained by the top two teams. Code and pre-trained models are available at https://github.com/fjpa121197/ImageCLEFmedEssex2020.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the participation of the School of Computer Science and Electronic Engineering (CSEE) at the University of Essex at ImageCLEFcaption 2020 task <ref type="bibr" coords="1,181.32,485.49,9.96,8.74" target="#b8">[9]</ref>. ImageCLEF <ref type="bibr" coords="1,256.19,485.49,10.52,8.74" target="#b5">[6]</ref> is an evaluation campaign organised as part of the CLEF 1 initiative labs. The ImageCLEFcaption task aims to interpret and summarise the insights gained from medical images. The 2020 edition, similar to 2019, focused on concept detection in a large corpus of radiology images. This task provides tools for radiology image understanding. A detailed description of the data and the task is presented in Pelka et al. <ref type="bibr" coords="1,350.95,545.26,9.96,8.74" target="#b8">[9]</ref>.</p><p>ImageCLEFcaption 2020 task is the forth edition of this successful task. In previous editions <ref type="bibr" coords="1,210.93,569.17,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="1,223.11,569.17,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="1,232.52,569.17,7.75,8.74" target="#b2">3]</ref> multiple approaches have been explored by the participants and retrieval approaches achieved best results <ref type="bibr" coords="1,358.26,581.13,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="1,370.44,581.13,12.73,8.74" target="#b12">13,</ref><ref type="bibr" coords="1,384.83,581.13,7.01,8.74" target="#b0">1]</ref>. Following past year experience, in this paper we proposed a retrieval-based approach where the images are encoded by a Densely Connected Convolutional Network, DenseNets <ref type="bibr" coords="1,467.31,605.04,9.96,8.74" target="#b4">[5]</ref>.</p><p>Several experiments are presented to select the most relevant concepts based on the concepts associated to the top ranked images retrieved. Code and pre-trained models are publicly available <ref type="foot" coords="2,260.49,141.33,3.97,6.12" target="#foot_0">2</ref> .</p><p>The rest of the paper is organised as follows. Section 2 presents collection and the evaluation methodology used in this work. Section 3 explains the techniques proposed in this paper including a detail description of the runs submitted to the ImageCLEFcaption task. The results are presented in Section 4. Finally, the conclusions are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Collection &amp; evaluation</head><p>In this work we used the ImageCLEFmed caption 2020 collection <ref type="bibr" coords="2,420.37,271.95,9.96,8.74" target="#b8">[9]</ref>. It consists on three subsets:</p><p>training set including 64,753 images; validation set including 15,970 images; test set including 3,534 images.</p><p>The images originate from biomedical journal articles extracted from the PubMed Central R (PMC)<ref type="foot" coords="2,252.29,369.07,3.97,6.12" target="#foot_1">3</ref> repository <ref type="bibr" coords="2,309.47,370.64,14.61,8.74" target="#b9">[10]</ref>. Each image is associated to multiple Unified Medical Language System R (UMLS) Concept Unique Identifiers (CUIs) <ref type="bibr" coords="2,168.37,394.55,9.96,8.74" target="#b1">[2]</ref>. The UMLS CUIs associated to the images in the training and validation sets were distributed and include 3,047.</p><p>The UMLS CUIs from the test set were not distributed and, therefore, not used to build the model. The ImageCLEFcaption task <ref type="bibr" coords="2,378.08,431.64,10.52,8.74" target="#b8">[9]</ref> organisers evaluated the submitted runs computing the F1-scores (see <ref type="bibr" coords="2,351.36,443.60,42.62,8.74">Section 4)</ref>.</p><p>In 2020, the ImageCLEFmed caption collection is classified in seven medical image modalities (Angiography, Computer Tomography, Magnetic Resonance, Positron Emissions Tomography, Ultrasound, X-ray and combined modalities in one image).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The proposed approach is based in a content-based image retrieval model, where DenseNets are used for feature extraction (see Section 3.1). A similarity comparison is done between the query image and the images in the training and validation test sets (see Section 3.2). Finally, concept selection is performed to predict the medical concepts for the query image (see Section 3.3).</p><p>Figure <ref type="figure" coords="2,181.14,621.68,4.98,8.74" target="#fig_1">2</ref> shows an overview of the approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature extraction</head><p>Following the success of the AUEB NLP Group at ImageCLEFmed Caption 2019 <ref type="bibr" coords="3,156.93,448.75,9.96,8.74" target="#b6">[7]</ref>, this approach also uses a pre-trained DenseNet model (DenseNet-121) to encode the images, i.e, to extract their relevant features bases on this model. The existing DenseNet-121 has many parameters which require immense computing power and very large scale datasets to be trained from scratch. Hence, transfer learning is used in this work to mitigate this problem as its power in computer vision has been extensively study in the literature <ref type="bibr" coords="3,355.60,508.53,14.61,8.74" target="#b11">[12]</ref>. DenseNet models are Convolutional Neural Networks (CNN) models where each layer is connected directly to other layers <ref type="bibr" coords="3,341.60,534.50,9.96,8.74" target="#b4">[5]</ref>. DenseNet models have been recognised for their ability to reach similar performance to ResNet models, which use double the amount of layers <ref type="bibr" coords="3,273.53,558.41,14.61,8.74" target="#b10">[11]</ref>. DenseNet-121 has 121 layers with trainable weights. The model uses the weights from the ImageNet dataset, which consists of 1.2 million images, and it has 1,000 classes.</p><p>The input image is resized to 64 × 64 and transformed to an array, then a preprocessing module from DenseNet Keras is used. This module is in charge of transforming the pixel values into a 0-1 range, and also to normalise the values based on the ImageNet dataset. The DenseNet-121 model is then used to encode each image representing it as a vector of 4,096 dimensions excluding the classification layer. In particular the following parameters were used:</p><p>-Optimizer : RMSProp -Learning rate: 0.0001 -Batch size: 32 -Momemtum: 0.0</p><p>The model was trained in two phases:</p><p>-1st phase: Only training the classification layers.</p><p>-2st phase: Training a portion of the feature learning layers and the classification layer.</p><p>Each phase consisted on 10 epochs (each epoch consisted of 100 steps, of which 10 steps were for validation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image retrieval</head><p>In this work, the image modality is used to improve the system performance. Each image in the test set is compared to all the images in the training or validation sets belonging to the same image modality as the query image. In the case of run 64104 the images were retrieved from all the training set without considering the modality.</p><p>In order to perform the comparison, Canberra and Manhattan distances are computed given the encoded features (see Section 3.1). This metrics were chosen based on their accuracy and speed of their computational performance. The 10 most similar images to the given query were selected and their associated concepts extracted. Each of the extracted concept is tagged with a score based on its ranked position or the computed distance value (see next Section 3.3 for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Concepts selection</head><p>In order to assign the concepts to the query images in the test set two methodologies were tested:</p><p>Ranking based selection. Each concept is assigned with a score based on the ranking of the 10 retrieved image which they were associated to. If the concept is associated to more than one image, then the value is added to it. For example, the highest ranked image has all its concepts given a value of 10 and the second highest has all its concepts given a value of 9. If the final score (after the addition) given to a concept is equal or over the threshold 20, then the concept is considered relevant to the query image and assigned to it.</p><p>Distance based selection. Each concept is assigned a scored based on the distance value computed of the 10 retrieved image which they were associated to. Similar to the ranked based selection, if the concept is associated to more than one image, then the value is added to it. For each concept final score (after the addition), the mean or percentile (99 or 95) is set as a threshold to select the concept. If the score was equal or over the threshold, then the concept is considered relevant to the query image and assigned to it. During the experimental set up other thresholds were tested such as percentiles 75 and 98 or a normalisation process, however there were no finally submitted to the challenge since mean and percentile 95 and 99 achieved better F1 score on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Runs</head><p>This section provides a detailed description of the runs submitted to ImageCLE-Fcaption 2020 task. The methods used to implement these runs are described in Section 3. Table <ref type="table" coords="5,208.10,656.12,4.98,8.74" target="#tab_0">1</ref> summarises the techniques used by each run. -Run 67416 : This run is similar to the baseline. In this case the image modality information is used in the retrieval step. The top 10 images from the same modality as the query image are retrieved from the training set.</p><p>-Run 63804 : This run is similar to the Run 67416. In this cases, the images are retrieved from both training and validation sets. The modality information is also considered.</p><p>-Run 64394 : This run is similar to the Run 67416. In this run, fine-tuning is applied.</p><p>-Run 68019 : This run is similar to the Run 64394. For this run, distance based selection is used to select the relevant concepts from the retrieved images using the mean of the scores as a threshold.</p><p>-Run 68026 : This run is similar to the Run 64394. For this run, distance based selection is used to select the relevant concepts from the retrieved images setting 95th percentile as a threshold.</p><p>-Run 68025 : This run is similar to the Run 64394. For this run, distance based selection is used to select the relevant concepts from the retrieved images setting 98th percentile as a threshold.</p><p>-Run 68022 : This run is similar to the Run 64394. For this run, distance based selection is used to select the relevant concepts from the retrieved images setting 99th percentile as a threshold.</p><p>-Run 68022 : This run is similar to the Run 68027 but using the Manhattan distance in the retrieval step.</p><p>Table <ref type="table" coords="7,162.77,142.36,4.98,8.74" target="#tab_0">1</ref> presents the official results achieved in the ImageCLEF 2020 Concept Detection Task and their ranks compared with all the 57 runs submitted by the 7 participating teams. This year, our team was the third team with best results. Best results was achieved with run 64394 with F1 score of 0.381, very close to the results of the second and first team which achieved a F1 score of 0.392 and 0.394, respectively. In particular, our best submitted used fine-tuning and Canberra distance to retrieved the top 10 images from the training set considering only the images from the same modality. Ranking based selection was also used to select the relevant concepts from the retrieved images.</p><p>Based on the the results achieved, it is clear that the used of the modality improve the results. Interestingly, we did not find difference when augmenting the set of images in the collection by including the validation set. It might be due of the nature of the images, since the retrieved images belonged to the same modality, including the validation set did not include many new concepts to retrieve.</p><p>Beside the possible advantages that fine-tuning can bring, in the official results, only a small improved is noticed when applying it. Similar when comparing Canberra and Manhattan distances, slightly better results were achieved when using Canberra distance.</p><p>Finally, the method used to select the concepts has a bigger impact on the overall results, achieving the best results when using the ranking based methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper describes the participation of CSEE at the University of Essex at ImageCLEFcaption 2020 task. CSEE proposes a retrieval-based approach using a DenseNet-121 model to encode the images in the collection. CSEE compares different modifications in the baseline to study their effects on the final performance. Best submitted run used fine-tuning per image modality and Canberra distance in the retrieval step. Concepts were selected based on the top 10 ranked images. CSEE was the third best team at the benchmark achieving a F1 score of 0.381, very close to the results obtained by the top two teams. In 2020, the image modality was provided and future improvements can tackle an initial modality classification step as well as training the retrieval step per each modality. Further work is also needed to better understand the effects of the concept selection step.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,354.25,345.82,8.74;3,134.77,366.21,160.98,8.74"><head>-Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Example of an image and the associated UMLS CUIs the validation set of the ImageCLEFcaption 2020 task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,193.55,370.52,228.26,8.74;4,203.93,115.84,207.50,243.16"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Overview of the concept prediction approach.</figDesc><graphic coords="4,203.93,115.84,207.50,243.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.77,127.36,345.83,144.98"><head>Table 1 :</head><label>1</label><figDesc>Description and performance of the runs submitted to ImageCLEF 2020 Concept Detection Task and their ranks compared with all the 57 runs submitted by the 7 participating teams.</figDesc><table coords="6,137.30,166.06,338.08,106.28"><row><cell>Run ID Training</cell><cell cols="6">Per modality Fine-tuning Similarity measure Threshold F1 Score Ranking</cell></row><row><cell>64104 T</cell><cell>No</cell><cell>No</cell><cell>Canberra</cell><cell>20</cell><cell>0.345</cell><cell>26</cell></row><row><cell>67416 T</cell><cell>Yes</cell><cell>No</cell><cell>Canberra</cell><cell>20</cell><cell>0.380</cell><cell>9</cell></row><row><cell>63804 T + V</cell><cell>Yes</cell><cell>No</cell><cell>Canberra</cell><cell>20</cell><cell>0.380</cell><cell>8</cell></row><row><cell>64394 T</cell><cell>Yes</cell><cell>Yes</cell><cell>Canberra</cell><cell>20</cell><cell>0.381</cell><cell>7</cell></row><row><cell>68019 T</cell><cell>Yes</cell><cell>Yes</cell><cell>Canberra</cell><cell>mean</cell><cell>0.280</cell><cell>34</cell></row><row><cell>68026 T</cell><cell>Yes</cell><cell>Yes</cell><cell>Canberra</cell><cell cols="2">95th perc. 0.246</cell><cell>36</cell></row><row><cell>68025 T</cell><cell>Yes</cell><cell>Yes</cell><cell>Canberra</cell><cell cols="2">98th perc. 0.337</cell><cell>31</cell></row><row><cell>68022 T</cell><cell>Yes</cell><cell>Yes</cell><cell>Canberra</cell><cell cols="2">99th perc. 0.379</cell><cell>10</cell></row><row><cell>68027 T</cell><cell>Yes</cell><cell>Yes</cell><cell>Manhattan</cell><cell cols="2">99th perc. 0.378</cell><cell>11</cell></row><row><cell cols="2">Best ImageCLEF2020 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.394</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,140.99,312.80,339.60,44.63"><head>-</head><label></label><figDesc>Run 64104 -baseline: In this run, DenseNet-121 is used to encode the images. The top 10 images are retrieved from the training set using Canberra distance. Ranking based selection is used to select the relevant concepts from the retrieved images.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,144.73,645.84,236.43,7.86"><p>https://github.com/fjpa121197/ImageCLEFmedEssex2020</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,144.73,656.80,147.76,7.86"><p>https://www.ncbi.nlm.nih.gov/pmc/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.96,645.84,337.63,7.86;7,151.52,656.80,329.07,7.86;8,151.52,119.67,329.07,7.86;8,151.52,130.63,116.24,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,200.18,656.80,146.53,7.86">Nlm at imageclef 2017 caption task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="7,369.78,656.80,110.81,7.86;8,151.52,119.67,169.02,7.86">CLEF2017 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,142.60,337.64,7.86;8,151.52,153.53,329.07,7.89;8,151.52,164.52,177.24,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,225.65,142.60,254.94,7.86;8,151.52,153.56,94.56,7.86">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkh061</idno>
		<ptr target="https://doi.org/10.1093/nar/gkh061" />
	</analytic>
	<monogr>
		<title level="j" coord="8,256.23,153.56,99.57,7.86">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Database-Issue</note>
</biblStruct>

<biblStruct coords="8,142.96,176.49,337.64,7.86;8,151.52,187.44,329.07,7.86;8,151.52,198.40,329.07,7.86;8,151.52,209.36,329.07,7.86;8,151.52,220.32,46.58,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,403.70,176.49,76.89,7.86;8,151.52,187.44,329.07,7.86;8,151.52,198.40,121.31,7.86">Overview of Image-CLEFcaption 2017 -the image caption prediction and concept extraction tasks to understand biomedical images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="8,295.77,198.40,184.83,7.86;8,151.52,209.36,98.56,7.86">CLEF2017 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,232.29,337.63,7.86;8,151.52,243.25,329.07,7.86;8,151.52,254.21,329.07,7.86;8,151.52,265.17,125.62,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,442.94,232.29,37.65,7.86;8,151.52,243.25,197.76,7.86">Overview of the ImageCLEF 2018 caption prediction tasks</title>
		<author>
			<persName coords=""><forename type="first">García</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="8,371.05,243.25,109.54,7.86;8,151.52,254.21,175.34,7.86">CLEF2018 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,277.13,337.64,7.86;8,151.52,288.09,329.07,7.86;8,151.52,299.05,186.91,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,405.15,277.13,75.45,7.86;8,151.52,288.09,89.79,7.86">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,261.20,288.09,219.39,7.86;8,151.52,299.05,93.91,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,311.02,337.64,7.86;8,151.52,321.98,329.07,7.86;8,151.52,332.94,329.07,7.86;8,151.52,343.90,329.07,7.86;8,151.52,354.85,329.07,7.86;8,151.52,365.81,329.07,7.86;8,151.52,376.77,329.07,7.86;8,151.52,387.73,329.07,7.86;8,151.52,398.69,329.07,7.86;8,151.52,409.65,329.07,7.86;8,151.52,420.61,34.31,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,295.04,365.81,185.55,7.86;8,151.52,376.77,255.37,7.86">Overview of the ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,426.43,376.77,54.16,7.86;8,151.52,387.73,329.07,7.86;8,151.52,398.69,253.34,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="8,456.14,398.69,24.45,7.86;8,151.52,409.65,138.63,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,142.96,432.58,337.64,7.86;8,151.52,443.53,329.07,7.86;8,151.52,454.49,327.15,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,359.88,432.58,120.72,7.86;8,151.52,443.53,77.87,7.86">AUEB NLP group at Image-CLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,273.46,443.53,106.40,7.86">CLEF2019 Working Notes</title>
		<title level="s" coord="8,388.12,443.53,92.48,7.86;8,151.52,454.49,31.91,7.86">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-09">2019. September 09-12 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,466.46,337.63,7.86;8,151.52,477.42,329.07,7.86;8,151.52,488.38,329.07,7.86;8,151.52,499.34,95.77,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,431.64,466.46,48.95,7.86;8,151.52,477.42,199.23,7.86">Overview of the ImageCLEFmed 2019 concept prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,371.81,477.42,108.79,7.86;8,151.52,488.38,123.15,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,511.31,337.64,7.86;8,151.52,522.26,329.07,7.86;8,151.52,533.22,329.07,7.86;8,151.52,544.18,217.32,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,422.50,511.31,58.09,7.86;8,151.52,522.26,325.22,7.86">Medical image understanding: Overview of the ImageCLEFmed 2020 concept prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,165.67,533.22,231.63,7.86">CLEF2020 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,556.15,337.98,7.86;8,151.52,567.11,329.07,7.86;8,151.52,578.04,263.64,7.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,212.82,556.15,242.65,7.86">PubMed Central: The GenBank of the published literature</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.98.2.381</idno>
		<ptr target="https://doi.org/10.1073/pnas.98.2.381" />
	</analytic>
	<monogr>
		<title level="j" coord="8,463.04,556.15,17.56,7.86;8,151.52,567.11,329.07,7.86">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="381" to="382" />
			<date type="published" when="2001-01">jan 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,590.04,337.98,7.86;8,151.52,600.99,329.07,7.86;8,151.52,611.95,329.07,7.86;8,151.52,622.89,94.91,7.89" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,151.52,600.99,329.07,7.86;8,151.52,611.95,86.79,7.86">Optimize transfer learning for lung diseases in bronchoscopy using a new concept: sequential fine-tuning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">G</forename><surname>Zanjani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,246.71,611.95,233.88,7.86;8,151.52,622.91,35.33,7.86">IEEE journal of translational engineering in health and medicine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,634.88,337.97,7.86;8,151.52,645.84,329.07,7.86;8,151.52,656.80,86.01,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,347.95,634.88,132.64,7.86;8,151.52,645.84,91.71,7.86">How transferable are features in deep neural networks?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,262.39,645.84,213.81,7.86">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,119.67,337.97,7.86;9,151.52,130.63,329.07,7.86;9,151.52,141.59,329.07,7.86;9,151.52,152.55,46.58,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,299.41,119.67,181.18,7.86;9,151.52,130.63,131.85,7.86">Imagesem at imageclef 2018 caption task: Image retrieval and transfer learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="9,302.54,130.63,178.06,7.86;9,151.52,141.59,97.63,7.86">CLEF2018 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
