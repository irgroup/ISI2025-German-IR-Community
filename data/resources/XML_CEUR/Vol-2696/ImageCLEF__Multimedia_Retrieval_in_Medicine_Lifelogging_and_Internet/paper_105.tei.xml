<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,176.19,115.96,262.98,12.62;1,162.10,133.89,291.15,12.62;1,172.62,151.82,270.11,12.62;1,248.97,169.76,117.42,12.62">Revealing Lung Affections from CTs. A Comparative Analysis of Various Deep Learning Approaches for Dealing with Volumetric Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,166.64,207.98,53.02,8.74"><forename type="first">Radu</forename><surname>Miron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SenticLab</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University of Iasi</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.88,207.98,62.38,8.74"><forename type="first">Cosmin</forename><surname>Moisii</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SenticLab</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University of Iasi</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.47,207.98,102.15,8.74"><forename type="first">Mihaela</forename><forename type="middle">Elena</forename><surname>Breaban</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SenticLab</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University of Iasi</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,176.19,115.96,262.98,12.62;1,162.10,133.89,291.15,12.62;1,172.62,151.82,270.11,12.62;1,248.97,169.76,117.42,12.62">Revealing Lung Affections from CTs. A Comparative Analysis of Various Deep Learning Approaches for Dealing with Volumetric Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A1F11CF0BC4A39B1A25893C727DB363F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper presents and comparatively analyses several deep learning approaches to automatically detect tuberculosis related lesions in lung CTs, in the context of the ImageClef 2020 Tuberculosis task. Three classes of methods, different with respect to the way the volumetric data is given as input to neural network-based classifiers are discussed and evaluated. All these come with a rich experimental analysis comprising a variety of neural network architectures, various segmentation algorithms and data augmentation schemes. The reported work belongs to the SenticLab.UAIC team, which obtained the best results in the competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical imaging technologies like Computer Tomography (CT) and Magnetic Resonance (MR) produce high volumes of data in the form of volumetric images. The richness of information they provide is essential to correct diagnosis but brings at the same time new challenges, both for manual/human and automatic/machine processing: these are not only about the size of the produced data but also about the complexity of the diagnosis process itself. With respect to automated diagnosis, the volumetric images, which can be seen both as matrices of pixels/voxels or series of 2D images (usually called slices), produced high effervescence in the deep learning research community, triggering a variety of new architectures and approaches.</p><p>The current paper makes use of deep learning to automatically detect tuberculosis and related affections in lung CTs, in the context of the ImageClef Tuberculosis task <ref type="bibr" coords="1,213.44,601.44,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,225.62,601.44,7.01,8.74" target="#b1">2]</ref>. We investigate three types of approaches, different with respect to the way the volumetric data is given as input to neural network-based classifiers. One type, popular among the participants in the previous year competition <ref type="bibr" coords="2,173.45,130.95,9.96,8.74" target="#b2">[3]</ref>, is based on reducing the volumetric image to a small set of 2D projections. Obviously, this approach consistently reduces the size of the data to be processed by the classifier but inherently may lose important information. The second type exploits the whole data matrix by using 3D convolutions or by fusing the information from the slices. The third type, which was ranked as the winner of the 2020 evaluation session, consists in moving the decision layer from the whole volume of data to the slice level. All these three different approaches come with a variety of neural network architectures, various segmentation algorithms and data augmentation schemes. The work reported stays behind the SenticLab.UAIC team, obtaining the best results in the competition<ref type="foot" coords="2,432.32,236.97,3.97,6.12" target="#foot_0">3</ref>  <ref type="bibr" coords="2,440.11,238.55,9.96,8.74" target="#b0">[1]</ref>.</p><p>The paper is structured as follows. Section 2 describes the challenge and the dataset. Section 3 describes the approaches developed based on reducing the volumetric image to 2D projections, starting with the previous year winning approach reported in <ref type="bibr" coords="2,229.78,286.84,9.96,8.74" target="#b3">[4]</ref>, which we further enhanced to address the 2020 tasks. Section 4 presents the approaches we used to exploit the whole volumetric information. Section 5 describes the architectures used to process the information at slice level and the heuristics used to produce the diagnosis report at the CT level. Because of the large number of approaches we evaluated, some of them abandoned earlier (not submitted in the competition) due to poor results on our local validation data, we report and discuss performance results immediately after each method description. Section 6 summarises the results for the best approaches that were evaluated on the blind test set in the competition and discusses comparatively the performance of the three classes of methods. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ImageClef Tuberculosis: tasks, data, evaluation</head><p>The challenge in the 2020 ImageClef Tuberculosis competition is the automatic detection of tuberculosis and related lesion types in CTs. The CT report to be generated must contain 3 binary labels for each lung, indicating the presence of TB lesions in general, the presence of pleurisy and caverns in particular.</p><p>The training dataset consists of 283 CTs. All CTs present at least one lung affected, 19 have pleurisy and 126 caverns. Because we split each CT into left/right lungs, this translates to 566 inputs, 444 affected, 21 with pleurisy and 145 with caverns.</p><p>The task is therefore a multi-binary classification problem, with three target labels per lung. For each target label the AUC is computed and the ranking is done on a test set, first by computing the average AUC and then by the minimum AUC over the 3 target labels.</p><p>We split the data into train/validation in the same fashion as <ref type="bibr" coords="2,412.41,611.75,10.52,8.74" target="#b3">[4]</ref> setting apart every 4th input into the validation set and use this configuration throughout the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Squeezing Volumetric Data: 2D Projections</head><p>The 3D matrix representing the volumetric image can be reduced to simpler 2D representations by traversing it in each of its three dimensions and computing statistics on numeric vectors. In the case of lungs CTs, a segmentation algorithm is firstly applied to detect the lungs and eliminate the other parts in the CT. Further, we used the method proposed in <ref type="bibr" coords="3,324.28,199.72,9.96,8.74" target="#b3">[4]</ref>, where the mean, the maximum and the standard deviation is computed on each direction, generating three 2D matrices which can be interpreted as an RGB image (a single 2D image with 3 channels). All the processing steps described in <ref type="bibr" coords="3,348.12,235.59,10.52,8.74" target="#b3">[4]</ref> are kept: mask erosion, increasing the voxels intensity in the CT by 1024HU, dividing the mean values and standard deviations values (red and blue channels) by their maximum, dividing the maximum values (green channel) by 1500. At the end, for each lung we have a set of three 2D RGB images, each image corresponding to one of the three dimensions of the 3D matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Impact of Segmentation</head><p>The first step behind all our approaches is image segmentation, with the aim of identifying and isolating each lung in the volumetric image. Because the performance of further processing is greatly influenced by the quality of segmentation (especially in the case of the 2D projection approach where the projections take into account the entire volume), we tested several segmentation methods. The organizers provided for all patients two versions of automatically extracted masks of the lungs: one which relies only on anatomical assumptions <ref type="bibr" coords="3,447.06,434.91,9.96,8.74" target="#b4">[5]</ref>, and one based on non-rigid registration <ref type="bibr" coords="3,291.33,446.86,9.96,8.74" target="#b5">[6]</ref>. Additionally, we used U-net(R231) and U-net(LTRCLobes) which were pre-trained for lung segmentation on large and diverse datasets <ref type="bibr" coords="3,207.91,470.77,10.52,8.74" target="#b6">[7]</ref> <ref type="foot" coords="3,222.25,469.20,3.97,6.12" target="#foot_1">4</ref> . Our experiments show that the first technique based on anatomical assumptions behaves much like region growing not being able to catch holes or necrotic tissue in lungs, the second technique manages to capture necrotic tissue while the ones based on U-net include airpockets, tumors and effusions. The flow of the dataset creation together with some projections corresponding to several segmentation techniques can be seen in fig. <ref type="figure" coords="3,434.84,530.55,3.87,8.74" target="#fig_0">1</ref>.</p><p>Feeding a VGG neural network <ref type="bibr" coords="3,297.88,544.30,10.52,8.74" target="#b7">[8]</ref> with 2D projections obtained on the segmented volumetric image, the average AUC scores obtained on our validation set indicate the registration based method to give the best performance (AUC=0.693) followed at small distance by U-net(R231) (AUC=0.674) and U-net(LTRCLobes) (AUC=0.668), but consistently surpassing the anatomybased method (AUC=0.580).</p><p>Consequently, all our further experiments use the segmentation provided by non-rigid registration <ref type="bibr" coords="3,230.65,629.79,9.96,8.74" target="#b5">[6]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Augmentation</head><p>The images go through a series of augmentations, each with a certain probability of being applied, including: horizontal and vertical flipping, small degrees of rotations, blurring, added gaussian noise, distortions, random cropping, and changing different values of hue, saturation or brightness. We used the Albumentations<ref type="foot" coords="4,186.02,560.00,3.97,6.12" target="#foot_2">5</ref> library for most of these augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The 2D Approach with Preprocessing (PreProcProj )</head><p>In an effort to improve over the last year result, we used the pre-processing provided in <ref type="bibr" coords="4,187.31,624.95,9.96,8.74" target="#b8">[9]</ref>, with the aim to eliminate the small vessels from the projection, thus making the affected area more obvious. We adopted all the pre-processing steps that the authors mention, except the regional maxima calculation. Figure <ref type="figure" coords="5,134.77,130.95,4.98,8.74" target="#fig_1">2</ref> illustrates the difference between projections with and without further preprocessing. For training we chose AlexNet <ref type="bibr" coords="5,274.60,357.00,17.99,8.74" target="#b9">[10]</ref>. The input consists of the three projections of a volume, on each axis. After extracting features from each projection with AlexNet, we concatenate all the features, feed them into a linear layer and predict probabilities for a lung to have affections, caverns, pleurisy or be healthy. With this approach we scored an AUC of 0.793 on the test set. In an attempt to make use of the whole volume at once, we used SqueezeNet in a 3D version, based on the implementation found in the repository <ref type="foot" coords="6,433.99,344.91,3.97,6.12" target="#foot_4">6</ref> . In order to work with volumes of different sizes, we used batch size equal to 1. To handle volumes with a large number of slices, we used Apex<ref type="foot" coords="6,368.97,368.82,3.97,6.12" target="#foot_5">7</ref> library for reducing the burden on our GPU. In a preliminary experiment we considered only the case affected vs. not affected. We noticed the bad results during the training: after some epochs the prediction scores stagnated, for all volumes, between 0.4 and 0.6. We concluded that 3D convolutions are not able to capture the important information on our small training set of volumetric images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Slices fusion</head><p>In our attempt to associate the entire volumetric image to a label, we constructed a hybrid approach. We fed the volume slice by slice into a convolutional neural network, fused the resulted feature maps at channel level and continued with another small convolutional network into a prediction. The initial convolutional neural network is composed from the encoder part of a U-net <ref type="bibr" coords="6,406.94,530.05,17.83,8.74" target="#b13">[14]</ref> architecture which was pretrained on a segmentation task at the end of which we applied a squeeze connection to reduce the number of channels, fused the resulting feature maps so that the slices processed in parallel by the CNN would now be treated as channels of a single input, then applied a resnet-like small network to compile the features into a label. We no longer use the masks to extract the lungs but instead use a simple threshold based segmentation to compute the boundaries of the body and crop out the space around the it. We again split by lung side and used only horizontal flip as a preprocessing. The resulting volume is resized to the fixed size of (128, 256, 256) The network could then be fed images in batches multiple of 128 representing the slices of a volume.</p><p>To make maximum use of the GPU memory, we used the Apex library to train using mixed precision, in a distributed manner on 2 GPUs. We could fit 2 times 128 images into the memory corresponding to 2 volumes.</p><p>The approach turned out to be cumbersome. The time to process an epoch was relatively high and the convergence of the network seemed slow. After 2 days of training we decided to stop and the network reached an AUC of around 0.6 on the hold-out validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Sequencing Volumetric Data: a Slice by Slice Classification Approach</head><p>Having a closer look at the training set, one can observe that usually the lesions on the lungs are located only on a small number of slices from the whole volume.</p><p>A natural idea is to try a 2D model that could differentiate between healthy lung slices and lung slices with lesions (caverns, pleurisy and affections) and construct the CT report based on the findings at slice level. For this purpose we need training data labeled at slice level and not CT level.</p><p>The first approach was to try to automatically detect the slices presenting lesions in the training set, using a lung nodule detector<ref type="foot" coords="7,387.40,368.56,3.97,6.12" target="#foot_6">8</ref> constructed by the winners of a challenge in cancerous nodules detection. The results were bad, the model not being able to recognize the slices showing caverns although these correspond to big, obvious regions.</p><p>Therefore, we started to manually select from each volume of the training set the slices with lesions. We actually found that this was not as time-consuming as we initially thought, by processing only the volumes labeled with lesions, and it definitely was worth the effort, as the increase in performance shows. The caverns are usually big and obvious and the affections are either nodules or dusty lungs images (which may indicate pneumonia), with very rare cases of pneumotorax (the lung disappearing due to the outbreak of a cavern). There are many cases when these lesions appear on a very small number of slices and thus, the two approaches described in sections 3 and 4 might have not been able to reveal them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">InceptionNet</head><p>In our first tries using the slice by slice approach, we used InceptionNet version 3 <ref type="bibr" coords="7,141.21,589.08,14.61,8.74" target="#b14">[15]</ref>. We used the annotated data in different ways. Transforming each slice of a volume into a picture, resizing each image to a size of 299 × 299, cutting the picture in half to obtain the two lungs and using vertical flip for all the pictures which are either affected, with caverns or with pleurisy, are the data pre-processing steps for our first attempt using this approach. This approach does not use the provided segmentation masks at all. We only used 4 labels as output: affected, caverns, ok, pleurisy.</p><p>As input for the neural network we used several versions, having all images as 3-channel images: a) NaiveInception. We added a linear layer on top of the last adaptive average pooling layer of the architecture, keeping the original InceptionNet weights freezed. With this approach we scored 0.86 AUC score on the test set, surpassing this way the best approach based on 2D projections. b)ThresholdInception. The other approaches consist in using some other preprocessing steps. This time, when creating the photos from the 3D volume, we used a Window Width and Window Level equal to 1500, -500 respectively. This way we improved the results to 0.887 mean AUC and to 0.82 min AUC on the test set. c) TwoPicInception. One other approach consists in mimicking the protocol a doctor has to follow in order to decide affections(including caverns) and pleurisy. In order to see the affections more clearly, a doctor uses Window Width and Window Level equal to 1500, -500 respectively, whereas for better visualisation of pleurisy, a doctor looks at pictures with Window Width and Window Level equal to 350, 50 respectively. With this thresholding, the liquid surrounding the pleura becomes more observable. Figure <ref type="figure" coords="8,310.45,353.54,4.98,8.74" target="#fig_3">4</ref> top shows the differences between the two pictures.</p><p>In order to use information from 2 pictures during training, we used two In-ceptionNet modules, with trainable parameters and concatenated the two outputs of the adaptive average pooling layer. The decision was made based on the output of the last linear layer applied on the concatenation discussed above. With this approach we scored 0.89 mean AUC on the test set. d) AttentionInception.We wanted to gain insight into how accurate the methods can be. We tried to check the predictions produced by the methods proposed and discovered that the models found in a big proportion correct slices of the volumes which contained certain affections. In order to work on the explainability of our model, we modified the structure of ThresholdInception, using the idea from <ref type="bibr" coords="8,158.16,500.70,14.61,8.74" target="#b15">[16]</ref>, introducing an attention mechanism. Instead of feeding the output of the last pooling layer into the linear layer, we used dot product attention <ref type="bibr" coords="8,462.33,512.66,14.61,8.74" target="#b16">[17]</ref>. We computed similarity scores between the output of the pooling layer and three different convolutional layers in the architecture. After using the compatibility scores as weights for the features extracted by the three layers, we concatenated the new features. Using a linear layer on top, we predicted scores for the 4 categories. After plotting the attention we noticed that the attention on the first layer selected highlighted the whole lung area -supporting the idea that we don't need the segmentation masks, whereas the second layer of attention highlighted affections on the lungs. With this approach we scored 0.85 mean AUC. We believe this lower performance is due to the fact that the attention on the last layer was not good. Checking the visualisation for that layer we noticed useless areas highlighted (Fig. <ref type="figure" coords="8,235.77,644.16,3.87,8.74" target="#fig_3">4</ref>, bottom). Because of the limit imposed on the number of submissions, we stopped investigating this direction. For all the methods above based on InceptionNet, training was performed for 30 epochs on one GPU Nvidia RTX 2070 with 8GB of memory, using Stochastic gradient descent optimizer. We divided the learning rate at each 10 epochs by 10 and used binary cross-entropy as loss function.</p><p>In order to establish the diagnosis for a volume we applied the following heuristic: we applied the inference step on all the pictures/slices from the volume and for each of the possible classes we took the maximum score encountered; if only one slice was found with an affection score higher than 0.8, then we divided the score of affected by 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">EfficientNet</head><p>In an effort to use a powerful, yet small footprint network, in our last approaches we used efficientnet <ref type="bibr" coords="9,218.97,536.57,15.43,8.74" target="#b17">[18]</ref>, specifically the b4 variant which has only 19M parameters but reaches top 1 accuracy of 82,6% on Imagenet. We used a Pytorch implementation pre-trained on Imagenet <ref type="bibr" coords="9,307.19,560.48,17.97,8.74" target="#b12">[13]</ref>.</p><p>The preprocessing we used here is similar to the ones we used before and took place at run-time on load. We applied the registration-based mask per slice based on a threshold to crop the body and remove much of the surrounding space, split the lungs into left/right (just by using splitting the image in half) and applied the same series of augmentations as in the previous approaches. The split and cropped image has dimension 256 × 256 and after randomly cropping it reduces to 224 × 224. For ease of working we also kept the size of the volumetric image depth to a fixed 128 slices per volume.</p><p>If otherwise specified for this approaches as for the others we used a window level of -500 and range of 1500 corresponding to the most common values used in areas of acute differing attenuation values (example: lungs) where air and vessels will sit side by side.</p><p>As input we tried several options, all of them maintaining 3 channels per image: a) Micro-volumes (MicroVolSlice): The importance of volumetric data is evident. This seemed especially apparent when we try to manually identify caverns which can present as rounded or irregularly shaped black centers surrounded by a white contoure. The caverns can range in size from small with a thin contoure line to large with thick and diffuse borders. The small caverns we found especially hard to identify as it can be confused with a section of a larger blood vessel. As untrained individuals, to eliminate the confusion we traced the potential cavern a few slices up or down to verify if it continues into a vessel or forms a pathology. To try and mitigate this type of confusion in a model we composed the 3 channels of the image from 3 consecutive (or equidistant) slices. In case the slice is at the beginning or end of the sequence we simply duplicated it to fill the channels. are (1500, -500) corresponding to the usual values used for lung imaging, (350, 40) called narrow window (used when examining areas of similar attenuation, for example, soft tissue) and (500, -600) a narrower window of the usual values for lung imaging in an attempt to retain more information around the values corresponding to blood vessels and soft tissues. The result with this method however was not submitted to the site as the result on the hold-out set was poorer than the others. Loss: Cross entropy vs Binary cross entropy : A strange case comes from the fact that using the CrossEntropyLoss (on a multi-label classification) without softmax before the loss (thus assigning a predominant label for each slice) we obtained higher results than using BinaryCrossEntropyLoss. The nature of the results is also very different, the first giving results on the extremes while the latter hovering around 0.5, but both giving decent results around 90% AUC.</p><p>Micro-volumes and just repeating the image gave similar results on the test set (92.2% and 92.4% respectively), however the training on the "naive" case was done on 130 epochs on 3 GPUs with batch 56 × 3 whereas the "microvolumes" case was done on 60 epochs on 2 GPUs with batch 56 × 2. Therefore, the approach with the highest score on our hold out set was c) the simple one which also represented the highest of the submitted scores. Our second highest submitted model is represented by the approach in a) micro-volumes.</p><p>For training we used Nvidia RTX 2070 with 8GB of memory. We again, used the Apex library from NVidia to train using mixed precision and Distributed-DataParallel with one process per GPU.</p><p>6 Comparative results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results on the competition test set</head><p>Table <ref type="table" coords="12,163.60,243.33,4.98,8.74" target="#tab_0">1</ref> summarizes the results obtained in the competition on the test set. Submissions were made only for the methods based on 2D projections and the ones based on predictions at slice level; as shown on the hold-out validation data, the attempts to use the whole volume using 3D convolutions or fusing the information at slice level did not obtain good results and therefore were not used in the competition test phase. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Discussion</head><p>By comparing the results both on our hold-out validation set and on the test set, the following conclusions can be drawn.</p><p>-As indicated by the low training accuracy, the approaches using the entire volumetric data as a whole corresponding to the segmented lungs (described in section 4), involving 3D convolutions or slice fusion, seem to be overwhelmed by the amount of parameters to fit and are not able to identify the lesions in cases where these are small or present only on a few slices of the CT, or either converge slowly.</p><p>-The 2D approaches based on projections computed over the segmented volume (described in section 3) give (unreasonable) good results, which indicates that simple (normalized) statistics like mean, maximum and standard deviation, when used together, are able to catch important information about the presence of lesions in lung CTs. The quality of segmentation of the lungs is of critical importance in this case, as a bad segmentation may introduce noise into the projections. After obtaining the set of 2D projections, data augmentation increased the generalization capability of the classifier. -The best approach, surpassing significantly the ones based on 2D projections, exploits all the information present in the segmented volumetric lungs in a slice-wise manner. Instead of predicting the presence of the affection per CT, we predict it for each slice. To obtain the report back at lung level the probabilities over slices are aggregated by extracting the maximum. An important pre-processing step consisted in fixing the window and range levels to specific values used by radiologists when inspecting lung CTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Volumetric images like CTs and MRIs provide rich information about the internal body structure, necessary in the diagnosis of many affections. With the advancements of neural networks, automatic diagnosis in volumetric images became possible at high precision, useful for prioritizing patients and assisting doctors in final decisions. After a thorough experimental analysis of various architectures, the current paper devised an approach able to produce highly accurate CT reports about the presence of tuberculosis related affections. The method, based on computing predictions at slice level, has, beside high accuracy in predicting lesion type, the advantage of offering more information in terms of localization of the lesions. With a current mean AUC score of 0.924 on test data, its performance can be increased if more data, capturing various cases, is provided in the training phase.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,426.11,345.83,7.89;4,134.77,437.09,345.83,7.86;4,134.77,448.05,345.83,7.86;4,134.77,459.01,98.27,7.86;4,134.77,115.84,351.50,295.50"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Projections dataset creation flow using 4 segmentation variants (in order: growth-based, registration-based, unetLTRCLobes and unetR231). Although the 4 types of projections resulted look only slightly different, the difference in classification scores is significant.</figDesc><graphic coords="4,134.77,115.84,351.50,295.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,138.34,314.34,338.68,7.89;5,188.59,182.70,235.11,116.87"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparison between projection without(left) and with pre-processing(right)</figDesc><graphic coords="5,188.59,182.70,235.11,116.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,134.77,232.36,345.82,7.89;6,134.77,243.34,345.83,7.86;6,134.77,254.30,161.40,7.86;6,134.77,115.84,319.00,101.75"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The AUC score progress on the hold-out validation set for different resnet models. Pink: resnet34 with no augmentations, DarkBlue: resnet34 with augmentations, LightBlue: resnet50 with augmentations</figDesc><graphic coords="6,134.77,115.84,319.00,101.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,134.77,341.12,310.30,7.89;9,134.77,352.10,284.99,7.86;9,160.62,231.89,291.05,94.45"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Top: Comparison between different threshold values for the HU units Bottom: Attention visualization for the three layers from InceptionNet</figDesc><graphic coords="9,160.62,231.89,291.05,94.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,134.77,479.61,345.82,7.89;10,134.77,490.60,164.58,7.86;10,134.77,409.82,168.42,55.02"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Top left: sample of micro-volume images. Top right: Sample of false-color images. Bottom: Sample of "naive" images.</figDesc><graphic coords="10,134.77,409.82,168.42,55.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="11,134.77,408.11,345.82,7.89;11,134.77,419.09,345.82,7.86;11,134.77,430.05,345.83,7.86;11,134.77,441.01,345.82,7.86;11,134.77,451.97,323.64,7.86;11,134.77,115.84,340.25,277.50"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. The schematic of the slice approach (microvolumes). It follows a simple flow. The volumetric image, split by left/right side is cropped using a simple threshold based segmentation, then composed (in this case) to microvolumes, augmented and passed through the model. The output from all the slices of a side of a volume is then aggregated and the max per label selected to compose the final result for a side.</figDesc><graphic coords="11,134.77,115.84,340.25,277.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="12,134.77,334.71,345.83,161.61"><head>Table 1 .</head><label>1</label><figDesc>Results reported on the test set, in the order of submission. The first two entries use 2d projections, while all the others make predictions at slice levels. (CE suffix represents models with CrossEntropyLoss and BCE models with BinaryCrossEntropy-Loss</figDesc><table coords="12,224.09,386.24,167.18,110.08"><row><cell>Method</cell><cell cols="2">mean AUC min AUC</cell></row><row><cell>ResNet50Proj</cell><cell>0.825</cell><cell>0.766</cell></row><row><cell>PreProcProj</cell><cell>0.793</cell><cell>0.703</cell></row><row><cell>NaiveInception</cell><cell>0.860</cell><cell>0.772</cell></row><row><cell>ThresholdInception</cell><cell>0.887</cell><cell>0.821</cell></row><row><cell>AttentionInception</cell><cell>0.853</cell><cell>0.788</cell></row><row><cell>TwoPicInception</cell><cell>0.892</cell><cell>0.830</cell></row><row><cell>NaiveSliceCE</cell><cell>0.924</cell><cell>0.885</cell></row><row><cell>MicroVolSliceCE</cell><cell>0.922</cell><cell>0.860</cell></row><row><cell>NaiveSliceBCE</cell><cell>0.899</cell><cell>0.862</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,656.80,223.17,7.86"><p>https://www.imageclef.org/2020/medical/tuberculosis/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="3,144.73,656.80,148.45,7.86"><p>https://github.com/JoHof/lungmask</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="4,144.73,656.80,233.11,7.86"><p>https://github.com/albumentations-team/albumentations</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3" coords="5,134.77,455.62,287.52,8.77;5,134.77,496.51,298.71,8.74;5,149.71,512.66,330.88,8.74;5,134.77,524.61,345.82,8.74;5,134.77,536.57,345.83,8.74;5,134.77,548.52,345.83,8.74;5,134.77,560.48,345.83,8.74;5,134.77,572.43,345.82,8.74;5,134.77,584.39,345.83,8.74;5,134.77,596.34,345.82,8.74;5,134.77,608.30,345.82,8.74;5,134.77,620.25,345.83,8.74;5,134.77,632.21,345.82,8.74;5,134.77,644.16,345.82,8.74;5,134.77,656.12,23.27,8.74"><p>3.4 The 2D Approach Scoring the Best (ResNet50Proj )We further tried different variants of Resnet<ref type="bibr" coords="5,324.69,496.51,18.01,8.74" target="#b10">[11]</ref> and SqueezeNet<ref type="bibr" coords="5,410.79,496.51,18.15,8.74" target="#b11">[12]</ref>.We extracted the lungs using the registration-based segmentations, computed all 3 projections, split by lung side and processed with the augmentation we listed in section 3.2. We trained the networks and aggregated the results on all 3 projections and computed the mean score to obtain the final results back at CT level. We tried different approaches in aggregating the results including training a small neural network, but found the simpler mean aggregation to give the highest score. We thus obtained our highest score in the 2D approach using a resnet-50 network<ref type="bibr" coords="5,250.12,596.34,15.50,8.74" target="#b10">[11]</ref> pretrained on Imagenet<ref type="bibr" coords="5,368.33,596.34,18.54,8.74" target="#b12">[13]</ref> with an AUC on our hold-out validation set of 0.877; however this result was not submitted. Our first submission to the competition was a resnet34 model with no augmentations which obtained on the hold-out validation set and on the test set the same AUC score of 0.825. In Figure3we can see the AUC progress on different models we tried.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="6,144.73,645.84,191.57,7.86"><p>https://github.com/okankop/Efficient-3DCNNs</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="6,144.73,656.80,139.94,7.86"><p>https://github.com/NVIDIA/apex</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="7,144.73,656.80,267.62,7.86"><p>https://github.com/BCV-Uniandes/LungCancerDiagnosis-pytorch</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="10,144.73,656.80,186.01,7.86"><p>https://radiopaedia.org/articles/windowing-ct</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p>Our strong belief is that data science without domain knowledge can never reach its full potential. We would like to thank <rs type="person">Mirela Iordache</rs>, an outstanding radiologist at the <rs type="institution">Regional Institute of Oncology in Iasi</rs>, for sharing her knowledge and valuable insights on the medical niche approached in the competition.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="13,142.96,612.96,337.64,7.86;13,151.52,623.92,329.07,7.86;13,151.52,634.88,329.07,7.86;13,151.52,645.84,329.07,7.86;13,151.52,656.80,91.48,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,273.60,623.92,206.98,7.86;13,151.52,634.88,137.84,7.86">Overview of ImageCLEFtuberculosis 2020 -automatic CT-based report generation</title>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aleh</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="13,311.33,634.88,169.26,7.86;13,151.52,645.84,69.60,7.86">CLEF2020 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,119.67,337.63,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,329.08,7.86;14,151.52,152.55,329.07,7.86;14,151.52,163.51,329.07,7.86;14,151.52,174.47,329.07,7.86;14,151.52,185.43,329.07,7.86;14,151.52,196.39,329.07,7.86;14,151.52,207.34,329.07,7.86;14,151.52,218.30,329.07,7.86;14,151.52,229.26,329.07,7.86;14,151.52,240.22,329.07,7.86;14,151.52,251.18,36.40,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,252.91,196.39,227.68,7.86;14,151.52,207.34,215.24,7.86">Overview of the imageclef 2020: Multimedia retrieval in medical, lifelogging, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Van-Tu</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tu-Khiem</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liting</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pål</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Triet</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dimitri</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raul</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liviu</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Daniel S ¸tefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Constantin</forename><surname>Gabriel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,387.42,207.34,93.17,7.86;14,151.52,218.30,190.33,7.86;14,422.63,218.30,57.96,7.86;14,151.52,229.26,291.45,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="14,307.10,240.22,169.52,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="14,142.96,262.15,337.63,7.86;14,151.52,273.11,329.07,7.86;14,151.52,284.07,329.07,7.86;14,151.52,295.03,52.99,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,265.32,273.11,215.27,7.86;14,151.52,284.07,243.37,7.86">Overview of imagecleftuberculosis 2019-automatic ctbased report generation and tuberculosis severity assessment</title>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitri</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aleh</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,414.68,284.07,65.91,7.86;14,151.52,295.03,24.79,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,306.00,337.63,7.86;14,151.52,316.96,270.29,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,219.64,306.00,260.95,7.86;14,151.52,316.96,128.44,7.86">Imageclef 2019: Projection-based ct image analysis for tb severity scoring and ct report generation</title>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,299.61,316.96,94.00,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,327.93,337.63,7.86;14,151.52,338.89,329.07,7.86;14,151.52,349.85,329.07,7.86;14,151.52,360.81,329.07,7.86;14,151.52,371.77,329.07,7.86;14,151.52,382.72,120.31,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,226.00,338.89,254.60,7.86;14,151.52,349.85,30.51,7.86">Efficient and fully automatic segmentation of the lungs in ct volumes</title>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oscar</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alfonso Jiménez</forename><surname>Del Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrien</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,317.77,360.81,162.82,7.86;14,151.52,371.77,298.54,7.86">Proceedings of the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI, CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">Orcun</forename><surname>Goksel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Oscar</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alfonso Jiménez</forename><surname>Del Toro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Antonio</forename><surname>Foncubierta-Rodríguez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<meeting>the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI, CEUR Workshop Proceedings</meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,393.70,337.63,7.86;14,151.52,404.66,329.07,7.86;14,151.52,415.61,329.07,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,323.40,393.70,157.19,7.86;14,151.52,404.66,199.26,7.86">Imageclef 2017: Supervoxels and cooccurrence for tuberculosis ct image classification</title>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,370.78,404.66,109.81,7.86;14,151.52,415.61,117.95,7.86">CLEF2017 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,426.59,337.63,7.86;14,151.52,437.54,329.07,7.86;14,151.52,448.50,329.07,7.86;14,151.52,459.46,97.10,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="14,287.21,437.54,193.39,7.86;14,151.52,448.50,255.58,7.86">Automatic lung segmentation in routine imaging is a data diversity problem, not a methodology problem</title>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Hofmanninger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Florian</forename><surname>Prayer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeanny</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Rohrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Helmut</forename><surname>Prosch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georg</forename><surname>Langs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11767</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.96,470.43,337.63,7.86;14,151.52,481.39,275.91,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="14,327.39,470.43,153.20,7.86;14,151.52,481.39,114.45,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.96,492.36,337.64,7.86;14,151.52,503.32,329.07,7.86;14,151.52,514.28,329.07,7.86;14,151.52,525.24,206.60,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,305.23,492.36,175.36,7.86;14,151.52,503.32,192.66,7.86">Automated detection of lung nodules with three-dimensional convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Gustavo</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,363.68,503.32,116.90,7.86;14,151.52,514.28,189.67,7.86">13th international conference on medical information processing and analysis</title>
		<imprint>
			<publisher>ternational Society for Optics and Photonics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10572</biblScope>
			<biblScope unit="page">1057218</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,536.21,337.98,7.86;14,151.52,547.17,329.07,7.86;14,151.52,558.13,160.49,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,389.06,536.21,91.54,7.86;14,151.52,547.17,161.76,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,333.65,547.17,146.93,7.86;14,151.52,558.13,60.05,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,569.10,337.98,7.86;14,151.52,580.06,329.07,7.86;14,151.52,591.02,182.33,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,391.23,569.10,89.37,7.86;14,151.52,580.06,84.43,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,256.49,580.06,224.10,7.86;14,151.52,591.02,91.43,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,601.99,337.98,7.86;14,151.52,612.95,329.07,7.86;14,151.52,623.91,295.51,7.86" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Forrest N Iandola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">J</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m" coord="14,259.52,612.95,221.06,7.86;14,151.52,623.91,129.88,7.86">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.61,634.88,337.98,7.86;14,151.52,645.84,329.07,7.86;14,151.52,656.80,230.40,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,440.91,634.88,39.68,7.86;14,151.52,645.84,165.21,7.86">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,337.36,645.84,143.23,7.86;14,151.52,656.80,118.25,7.86">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,119.67,337.97,7.86;15,151.52,130.63,329.07,7.86;15,151.52,141.59,329.07,7.86;15,151.52,152.55,20.99,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,376.72,119.67,103.87,7.86;15,151.52,130.63,166.31,7.86">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName coords=""><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,339.69,130.63,140.90,7.86;15,151.52,141.59,223.17,7.86">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,163.51,337.98,7.86;15,151.52,174.47,329.07,7.86;15,151.52,185.43,329.07,7.86;15,151.52,196.39,43.13,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,184.42,174.47,231.17,7.86">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,434.71,174.47,45.88,7.86;15,151.52,185.43,323.86,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,207.34,337.98,7.86;15,151.52,218.30,201.41,7.86" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Saumya</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><forename type="middle">A</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02391</idno>
		<title level="m" coord="15,429.31,207.34,51.28,7.86;15,151.52,218.30,35.25,7.86">Learn to pay attention</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.62,229.26,337.98,7.86;15,151.52,240.22,329.07,7.86;15,151.52,251.18,20.99,7.86" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m" coord="15,390.83,229.26,89.76,7.86;15,151.52,240.22,190.68,7.86">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.62,262.14,337.98,7.86;15,151.52,273.10,262.43,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="15,282.36,262.14,198.23,7.86;15,151.52,273.10,95.94,7.86">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
