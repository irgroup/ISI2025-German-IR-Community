<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.45,115.96,328.45,12.62;1,159.81,133.89,295.74,12.62;1,279.29,151.82,56.77,12.62">Chejiao at ImageCLEFmed Tuberculosis 2020: CT Report Generation Based on Transfer learning</title>
				<funder ref="#_eXvYMmW">
					<orgName type="full">NSF of Yunnan Province</orgName>
				</funder>
				<funder ref="#_frEc7py">
					<orgName type="full">Natural Science Foundations of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,210.54,189.50,36.11,8.74"><forename type="first">Jiao</forename><surname>Che</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650091</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.95,189.50,52.98,8.74"><forename type="first">Haiyan</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650091</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,335.36,189.50,64.04,8.74"><forename type="first">Xiaobing</forename><surname>Zhou</surname></persName>
							<email>zhouxb@ynu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650091</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.45,115.96,328.45,12.62;1,159.81,133.89,295.74,12.62;1,279.29,151.82,56.77,12.62">Chejiao at ImageCLEFmed Tuberculosis 2020: CT Report Generation Based on Transfer learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">62E8F6462BBD352B40F70DB5F061747A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Transfer Learning</term>
					<term>ShufflenetV2</term>
					<term>Tuberculosis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tuberculosis is a very common chronic infectious disease, so the study of CT report of tuberculosis has an important impact on clinical treatment. For the 3D CT image, it is easier to process the 3D images by transforming it into the 2D image along three dimensions and preserving the information of each dimension. We participate in the 2020 TB task: CT report generation. In order to improve the effect, we used the Mixup for data enhancement during training. Due to the small amount of data, we choose the ShufflenetV2 network model to use the transfer learning method for training and then fine-tune the model in training. Our team ranks the third in the ImageCLEF med-Tuberculosis 2020 challenge, it achieves mean auc score and min auc score of 0.791 and 0.682, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tuberculosis is a chronic infectious disease caused by Mycobacterium tuberculosis. According to the World Health Organization, about 130 years after the discovery of the disease, the disease is still a persistent threat and the main cause of death worldwide. It can invade many organs, especially pulmonary tuberculosis. Human infection with tubercle bacilli does not necessarily cause disease. When the resistance is reduced or the cell-mediated allergy is increased, it may cause clinical disease. If it can be diagnosed in time and treated reasonably, most of them can be cured clinically. Therefore, the correct detection and diagnosis of tuberculosis are very important.</p><p>At present, the clinical diagnosis of tuberculosis is mainly based on the comprehensive analysis of the patients' clinical manifestations, imaging data, and laboratory examination results. The imaging diagnosis method includes chest Xray and CT examination. Chest X-ray examination can show the lymphadenopathy or miliary change of the lung and mediastinum. CT has a high resolution and can make a detailed evaluation of tuberculosis. It can also provide the basis for drug-resistant tuberculosis and active tuberculosis. Epidemiological investigation shows that there is a big delay between the occurrence of disease symptoms and clinical manifestations or diagnosis of tuberculosis. Finding fast and convenient means of early diagnosis will help to improve the efficiency of early diagnosis of tuberculosis.</p><p>ImageCLEF 2020 is an evaluation campaign organized as part of the CLEF initiative labs <ref type="bibr" coords="2,199.58,205.68,9.96,8.74" target="#b0">[1]</ref>. The campaign offers several research tasks and the Image-CLEFmedical is an area of great interest to us. The ImageCLEFmedical 2020 challenge includes a task based on tuberculosis CT <ref type="bibr" coords="2,366.99,229.59,9.96,8.74" target="#b1">[2]</ref>: To generate automatic lung-wise reports based on the CT image data. This task has an important impact on the clinical diagnosis of doctors and has practical value.</p><p>The rest of this paper is organized as follows: In the second part, we describe the research we did in this competition. In the third part, the specific tasks of this competition and the analysis of the data set are described in detail. Then, in the fourth part, we introduce the network model we used and the transfer learning method. In the fifth part, we introduce the relevant details of the experiment. In the sixth part, we report on our results and those of other teams. Finally, in the seventh part, we conclude and discuss the current and future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In recent years, deep learning has made exciting achievements in the field of medical image analysis and has rapidly become the current research hotspot. Machine learning and deep learning are also explored in the diagnosis of tuberculosis. Deep learning <ref type="bibr" coords="2,237.40,461.84,10.52,8.74" target="#b2">[3]</ref> is also used to diagnose CT images of lung diseases. The deep belief network and convolution neural network are used to classify pulmonary nodules in 3D tomography, and the effect of deep belief network is better than the convolution neural network and sift <ref type="bibr" coords="2,368.75,497.71,9.96,8.74" target="#b3">[4]</ref>. Lakhani et al showed that the combination of Alexnet and Googlenet performs best <ref type="bibr" coords="2,408.43,509.66,9.96,8.74" target="#b4">[5]</ref>.</p><p>In 2018, Xiaoyun Hu et al proposed an artificial intelligence diagnosis and treatment system for CT TB detection, which first preprocessed CT images, used a two-dimensional convolution neural network to segment the lesion area, and then used the residual network to predict the true and false positive. It realized the automatic diagnosis of tuberculosis in CT images and provided doctors with diagnosis and treatment suggestions. This scheme had high recognition accuracy, greatly reduced the missed detection rate, and reduced the cost of manual screening. However, when segmenting the pulmonary tuberculosis lesions, only the two-dimensional characteristics of the image are used, and the influence of the three-dimensional characteristics of the CT image on the final segmentation effect is not considered. It can be seen that there is still much space for research and development in the automatic generation of CT reports of tuberculosis.</p><p>The Tuberculosis task of the ImageCLEF 2020 Challenge is to generate automatic lung-wise reports based on the CT image data. Each report should include the probability scores (ranging from 0 to 1) for each of the three labels and each of the lungs (resulting in 6 entries per CT). The result list of entries includes LeftLungAffected, RightLungAffected, CavernsLeft, CavernsRight, PleurisyLeft, PleurisyRight.</p><p>The 2020 ImageCLEFmed Tuberculosis task continues the 2019 Tuberculosis Subtask #2: CTR -CT report since it has an important outcome that can have a major impact in the real-world clinical routines. In order to make the task both more attractive for participants and practically valuable, the task of CT report generation is lung-based rather than CT-based, which means the labels for left and right lungs will be provided independently. Also, the dataset size is increased compared to the last year. This task includes 283 images in the training set and 120 in the test set. The task is to predict the presence of six types of findings in CT scans. Information about the corresponding labels is listed in Table <ref type="table" coords="3,449.57,320.72,3.87,8.74" target="#tab_0">1</ref>. The images for the ImageCLEF tuberculosis task were provided as NIfTI 3D datasets. The 3D CT images which were provided have a slice size of 512 × 512 pixels and a number of slices varying from 50 to 400. The file format stores raw voxel intensities in Hounsfield units (HU) as well the corresponding image metadata such as image dimensions, voxel size in physical units, slice thickness, etc. Four slices of a CT were randomly selected to show in the Fig.  Both versions of the mask are available on the website for all patients. The first version of segmentation is retrieved using the same technique as the last years. The second version of segmentation is retrieved using non-rigid image registration scheme. The first version of segmentation provides more accurate masks, but it tends to miss large abnormal regions of lungs in the most severe TB cases <ref type="bibr" coords="4,179.74,178.77,9.96,8.74" target="#b5">[6]</ref>. The second segmentation on the contrary provides more rough bounds, but behaves more stable in terms of including lesion areas <ref type="bibr" coords="4,443.54,190.73,9.96,8.74" target="#b6">[7]</ref>. Two different versions of mask slices are shown in the Fig.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model</head><p>In recent years, deep CNN networks such as Resnet <ref type="bibr" coords="4,389.89,476.32,10.52,8.74" target="#b7">[8]</ref> and Densenet <ref type="bibr" coords="4,470.08,476.32,10.52,8.74" target="#b8">[9]</ref> have greatly improved the accuracy of image classification. But in addition to accuracy, computational complexity is also an important index to be considered in the CNN network. Over complex network may be slow, and some specific scenarios need low latency. In order to meet these needs, some lightweight CNN networks such as Mobilenet <ref type="bibr" coords="4,257.86,536.10,15.50,8.74" target="#b9">[10]</ref> and Shufflenet <ref type="bibr" coords="4,342.09,536.10,15.50,8.74" target="#b9">[10]</ref> were proposed, which made a good balance between speed and accuracy.</p><p>ShufflenetV2 is an improved version of ShufflenetV1, and ShufflenetV2 improves the deficiencies of ShufflenetV1. The experimental results show that Shuf-flenetV2 is much faster and more accurate on both platforms than the previous network. ShufflenetV1 uses two technologies: point-by-point group convolution kernel and bottleneck-like structure; and then introduces channel shuffle operation to enable information exchange between different groups of channels and improve accuracy. The point-by-point group convolution and bottleneck structure both increase the memory access cost. This cost cannot be ignored, especially for lightweight models. In addition, using too many packets to generate network fragmentation will reduce the degree of parallelism. Element-level addition operations in shortcut connections are also undesirable. Therefore, in order to achieve higher model capacity and efficiency, the key issue is how to maintain a large number of channels with the same width, neither dense convolution nor too many groups. So our model chooses the lightweight neural network ShufflenetV2 <ref type="bibr" coords="5,462.34,166.82,14.61,8.74" target="#b10">[11]</ref>. The structure of the network model is shown in the following figure.  At the beginning of each unit, the input of the c characteristic channel is divided into two branches by Channel Split, with c -c ′ and c ′ channels respectively. One branch is a shortcut stream, and the other branch contains three convolutions (and the three branches have the same number of channels). Two 1 × 1 convolutions are used. After convolution, the two branches are concatenated (Concat), so that the number of channels remains unchanged. Then Channel Shuffle operation is carried out to ensure the information exchange between the two branches. Depthwise convolution is used to minimize memory accesses with the same channel size. After shuffling, the next unit begins. Note that the Add operation no longer exists in ShufflenetV1, and ReLU and depthwise convolution only exist on one branch. Moreover, three consecutive element-by-element operations "Concat", "Channel Shuffle" and "Channel Split" are merged into one element-wise operation, and these changes are conducive to improving network efficiency. The complete network structure is made up of blocks, and the overall network structure is similar to Shufflenet, except that an additional convolutional layer is added to blend the characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Transfer Learning</head><p>Transfer learning is a machine learning method, which uses the parameters of the trained model developed for task A as the initial point, and re-transfers it to the model developed for task B to help the training of the new model. Using migration parameters to initialize the network can improve generalization performance. Even migrating from tasks that are not particularly similar is better than using random filters (or random parameters).</p><p>In this paper, the method of combining neural networks with transfer learning ideas is adopted. First, transfer learning is used to train the model. After a certain batch of the epoch, the method of fine-tuning is used instead. When applying deep learning in the field of image processing, it will be observed that the features extracted in the first layer are basically similar to Gabor filters and color blobs. Usually, the first layer is not particularly related to the specific image data set, and the last layer of the network is closely related to the selected data set and its task goal; the first layer feature is called general feature, the last layer is called specific feature. Because the data set is small and the data similarity is not high. So we choose to freeze the weight of the first k layers in the pre-training model and then retrain the next n -k layers. Of course, the last layer also needs to be modified according to the corresponding output format. Because the similarity of data is not high, the process of retraining becomes very important. The shortage of the size of the new data set is made up by freezing the front k layer of the pre-training model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>In this challenge, we use projection <ref type="bibr" coords="6,315.52,644.17,15.50,8.74" target="#b11">[12]</ref> to preprocess the original image. So the 3D CT image which is difficult to process can be converted into 2D image processing. It retains the information of each dimension of the 3D image. According to the requirements of the competition, all labels are marked on the left and right lungs, so each CT image is divided into three axes (sagittal, frontal and axial) for projection. Additional enhancement operations are performed on the training image: Mixup data enhancement <ref type="bibr" coords="7,340.15,166.82,14.61,8.74" target="#b12">[13]</ref>, random rotation within 10 degrees, changes in width and height, and random scaling. Data enhancement helps to prevent overfitting and memorizing the details of training images. If the training mode of Mixup is adopted, then two input images are read each time, assuming that they are represented by (X i , Y i ) and (X j , Y j ), then a new image (x, y) can be synthesized by the following two formulas, and then the new image is used for training. It should be noted that more epoch should be trained when training the model in this way. λ in the formula is a super parameter, which is used to adjust the specific gravity of the synthesis. The value range is [0,1].</p><formula xml:id="formula_0" coords="7,265.02,286.37,215.57,27.11">x = λx i + (1 -λ)x j (1) ŷ = λy i + (1 -λ)y j (2)</formula><p>Because this year's CT report is based on lungs, the projected image is directly divided into the left and right lungs. Due to the small amount of data, we use the ratio  A GeForce RTX2080Ti GPU is used in this experiment. We use the method of transfer learning to train 100 epochs in the dataset and then use the method of fine-tuning to modify and compile the model. After the model is built, we need to train 200 epochs. The optimizer we used is Adam, and the learning rate is set to 1e-05, the batch size is set to 16. The best results are obtained with the above parameters. The prediction results of the model we used on the validation set are shown in Table <ref type="table" coords="8,236.19,154.86,4.98,8.74" target="#tab_2">3</ref> below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>This section shows final performance results of submitted runs in the CT report challenge. Imageclef med-Tuberculosis 2020 competition implements two evaluation methods, mean auc and min auc. The ID of our team is "chejiao".During the competition, 120 testing data are supplied and are ranked based on mean auc. Our work is ranked 3 with 0.791 mean auc value and 0.682 min auc. The main reason for our good results is that we used projection to process the data, and used the Mixup method to enhance the data, and finally combined with transfer learning to fine-tune the model. From the experimental results, the ShufflenetV2 model we used also improves the final result. The final submitted result is shown in Fig. <ref type="figure" coords="8,162.89,498.75,4.13,8.74">9</ref>. In this competition, there are only seven cases of left pulmonary pleurisy, and the accuracy of the prediction verification set for pleurisy is low. As a result, the average accuracy is low. For the problem of too little data of pleurisy, we plan to add other authoritative data sets to better training models in future research. The results show that there is still space for improvement.</p><p>In this challenge, due to our lack of medical knowledge, the understanding of CT images is not deep enough. In future work, we will improve our medical knowledge so as to better understand and process data. In order to obtain high accuracy, it is recommended that medical knowledge should be embedded. In future experiments, we plan to use the current popular Efficientnet net model <ref type="bibr" coords="9,465.11,248.14,15.50,8.74" target="#b13">[14]</ref> and the new network design paradigm Regnet <ref type="bibr" coords="9,334.62,260.09,15.50,8.74" target="#b14">[15]</ref> proposed by the Kaiming and others in 2020 to carry out experiments, so as to obtain better results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,427.89,544.60,4.40,8.74"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,218.24,653.51,178.87,7.86;3,236.98,579.68,69.11,50.37"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Different slices of the same CT image</figDesc><graphic coords="3,236.98,579.68,69.11,50.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,367.81,202.68,4.40,8.74"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,206.94,369.19,201.46,7.86;4,155.71,236.93,148.70,108.79"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Different versions of segmentation methods</figDesc><graphic coords="4,155.71,236.93,148.70,108.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,216.19,329.41,182.97,7.86;5,134.76,213.45,345.78,101.16"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The model we used in the competition</figDesc><graphic coords="5,134.76,213.45,345.78,101.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,134.76,653.51,345.83,7.86;5,219.69,428.31,86.39,189.03"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Two different block structures of ShufflenetV2 (DWConv:depthwise convolution)</figDesc><graphic coords="5,219.69,428.31,86.39,189.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,194.27,345.20,286.34,8.74;7,134.76,357.15,255.64,8.74;7,149.71,369.11,330.89,8.74;7,134.76,381.06,345.84,8.74;7,134.76,393.02,345.84,8.74;7,134.76,404.97,345.84,8.74;7,134.76,416.93,345.85,8.74;7,134.76,428.88,345.84,8.74;7,134.76,440.84,345.84,8.74;7,134.76,452.79,345.84,8.74;7,134.76,464.75,73.69,8.74"><head></head><label></label><figDesc>of 3:1 to divide the training set and validation set, i.e., there are 213 images in the training set and 70 in the validation set. We treat this task as multiple binary classification tasks. For different lesions, we adjust the CT value to obtain different two-dimensional projections. Then the projected pictures are fed into our model for training. In order to generate a CT report finally, we first classify leftlungaffed and rightlungaffed by using the binary classification method, then cavernsleft and cavernsright, pleurisyleft and pleurisyright. Six examples are classified three times, and then the six examples are averaged auc. In this experiment, we compare the Mobilenet model with the ShufflenetV2 model. On the validation set, Mobilenet performance results are shown in Table2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,206.14,357.17,203.07,96.33"><head>Table 1 :</head><label>1</label><figDesc>Presence of labels in the Training dataset</figDesc><table coords="3,238.15,379.49,139.05,74.01"><row><cell>Filename</cell><cell>In Training set</cell></row><row><cell>LeftLungAffected</cell><cell>211</cell></row><row><cell>RightLungAffected</cell><cell>233</cell></row><row><cell>CavernsLeft</cell><cell>66</cell></row><row><cell>CavernsRight</cell><cell>79</cell></row><row><cell>PleurisyLeft</cell><cell>7</cell></row><row><cell>PleurisyRight</cell><cell>14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,189.56,495.46,236.22,107.29"><head>Table 2 :</head><label>2</label><figDesc>Mobilenet prediction results on the Validation set</figDesc><table coords="7,249.20,517.75,116.96,85.00"><row><cell>lesion</cell><cell>auc</cell></row><row><cell cols="2">LeftLungAffected 0.783163</cell></row><row><cell cols="2">RightLungAffected 0.843391</cell></row><row><cell>CavernsLeft</cell><cell>0.770299</cell></row><row><cell>CavernsRight</cell><cell>0.798762</cell></row><row><cell>PleurisyLeft</cell><cell>0.625000</cell></row><row><cell>PleurisyRight</cell><cell>0.867188</cell></row><row><cell>Mean auc</cell><cell>0.781300</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.76,185.95,345.85,157.85"><head>Table 3 :</head><label>3</label><figDesc>ShufflenetV2 prediction results on the Validation set</figDesc><table coords="8,134.76,208.24,345.85,135.55"><row><cell>lesion</cell><cell>auc</cell></row><row><cell cols="2">LeftLungAffected 0.887755</cell></row><row><cell cols="2">RightLungAffected 0.926724</cell></row><row><cell>CavernsLeft</cell><cell>0.818376</cell></row><row><cell>CavernsRight</cell><cell>0.859649</cell></row><row><cell>PleurisyLeft</cell><cell>0.625588</cell></row><row><cell>PleurisyRight</cell><cell>0.867188</cell></row><row><cell>Mean auc</cell><cell>0.83088</cell></row><row><cell cols="2">Through experiments we can conclude that the model we used have achieved</cell></row><row><cell>very good results.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,171.13,531.77,273.10,129.21"><head>Table 4 :</head><label>4</label><figDesc>Final result of the ImagelCLEFmed Tuberculosis challenge</figDesc><table coords="8,228.75,554.06,157.86,106.92"><row><cell>Participants</cell><cell cols="2">min auc mean auc</cell></row><row><cell>SenticLab.UAIC</cell><cell>0.924</cell><cell>0.885</cell></row><row><cell>agentili</cell><cell>0.875</cell><cell>0.811</cell></row><row><cell>chejiao</cell><cell>0.791</cell><cell>0.682</cell></row><row><cell cols="2">CompElecEngCU 0.767</cell><cell>0.733</cell></row><row><cell>KDE-lab</cell><cell>0.753</cell><cell>0.698</cell></row><row><cell>Waqas shelkh</cell><cell>0.705</cell><cell>0.644</cell></row><row><cell>uaic2020</cell><cell>0.659</cell><cell>0.562</cell></row><row><cell>JBTTM</cell><cell>0.601</cell><cell>0.432</cell></row><row><cell>sztaki dsd</cell><cell>0.595</cell><cell>0.546</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">Natural Science Foundations of China</rs> under Grants <rs type="grantNumber">61463050</rs>, the <rs type="funder">NSF of Yunnan Province</rs> under Grant <rs type="grantNumber">2015FB113</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_frEc7py">
					<idno type="grant-number">61463050</idno>
				</org>
				<org type="funding" xml:id="_eXvYMmW">
					<idno type="grant-number">2015FB113</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.95,385.20,337.63,7.86;9,151.52,396.16,329.07,7.86;9,151.52,407.12,329.06,7.86;9,151.52,418.08,329.06,7.86;9,151.52,429.03,329.06,7.86;9,151.52,439.99,329.06,7.86;9,151.52,450.95,329.06,7.86;9,151.52,461.91,329.07,7.86;9,151.52,472.87,329.07,7.86;9,151.52,483.83,329.06,7.86;9,151.52,494.79,329.06,7.86;9,151.52,505.75,329.06,7.86;9,151.52,516.71,36.39,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,259.13,461.91,221.46,7.86;9,151.52,472.87,220.44,7.86">Overview of the ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Van-Tu</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tu-Khiem</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liting</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pål</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Triet</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dimitri</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raul</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liviu</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Daniel S ¸tefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Constantin</forename><surname>Gabriel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,389.68,472.87,90.91,7.86;9,151.52,483.83,195.09,7.86;9,423.52,483.83,57.07,7.86;9,151.52,494.79,291.45,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="9,307.10,505.75,169.52,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="9,142.95,527.07,337.63,7.86;9,151.52,538.03,329.06,7.86;9,151.52,548.99,329.07,7.86;9,151.52,559.95,329.06,7.86;9,151.52,570.91,91.48,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,273.60,538.03,206.98,7.86;9,151.52,548.99,137.83,7.86">Overview of ImageCLEFtuberculosis 2020 -automatic CT-based report generation</title>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aleh</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="9,311.33,548.99,169.26,7.86;9,151.52,559.95,69.59,7.86">CLEF2020 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,581.27,337.64,7.86;9,151.52,592.23,100.85,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,381.60,581.27,56.54,7.86">Deep learning</title>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,452.51,581.27,24.07,7.86">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,602.60,337.63,7.86;9,151.52,613.56,329.06,7.86;9,151.52,624.52,310.71,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,210.54,613.56,270.05,7.86;9,151.52,624.52,165.22,7.86">Computer-aided classification of lung nodules on computed tomography images via deep learning technique</title>
		<author>
			<persName coords=""><forename type="first">Kai-Lung</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Che-Hao</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chusnul</forename><surname>Shintami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen-Huang</forename><surname>Hidayati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu-Jen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,324.92,624.52,99.03,7.86">OncoTargets and therapy</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,634.88,337.63,7.86;9,151.52,645.84,329.06,7.86;9,151.52,656.80,160.88,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,315.25,634.88,165.34,7.86;9,151.52,645.84,329.06,7.86;9,151.52,656.80,21.40,7.86">Deep learning at chest radiography: automated classification of pulmonary tuberculosis by using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Paras</forename><surname>Lakhani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baskaran</forename><surname>Sundaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,181.28,656.80,36.93,7.86">Radiology</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="574" to="582" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,119.68,337.63,7.86;10,151.52,130.64,329.06,7.86;10,151.52,141.60,329.06,7.86;10,151.52,152.55,329.07,7.86;10,151.52,163.51,329.07,7.86;10,151.52,174.47,120.30,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,225.98,130.64,254.60,7.86;10,151.52,141.60,30.51,7.86">Efficient and fully automatic segmentation of the lungs in ct volumes</title>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oscar</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alfonso Jiménez</forename><surname>Del Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrien</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,317.77,152.55,162.82,7.86;10,151.52,163.51,298.54,7.86">Proceedings of the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI, CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">Orcun</forename><surname>Goksel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Oscar</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alfonso Jiménez</forename><surname>Del Toro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Antonio</forename><surname>Foncubierta-Rodríguez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<meeting>the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI, CEUR Workshop Proceedings</meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,185.43,337.63,7.86;10,151.52,196.39,329.07,7.86;10,151.52,207.35,329.05,7.86;10,151.52,218.31,36.24,7.86;10,235.16,218.31,136.11,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,323.38,185.43,157.20,7.86;10,151.52,196.39,199.13,7.86">Imageclef 2017: Supervoxels and cooccurrence for tuberculosis ct image classification</title>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,370.56,196.39,110.03,7.86;10,151.52,207.35,220.24,7.86">CLEF2017 Working Notes, Series = CEUR Workshop Proceedings, Year = 2017</title>
		<meeting><address><addrLine>Address = Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date>11-14</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,229.27,337.63,7.86;10,151.52,240.23,329.05,7.86;10,151.52,251.19,182.33,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,391.22,229.27,89.36,7.86;10,151.52,240.23,84.42,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,256.49,240.23,224.08,7.86;10,151.52,251.19,91.43,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,262.14,337.63,7.86;10,151.52,273.10,329.07,7.86;10,151.52,284.06,271.64,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,151.52,273.10,167.65,7.86">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,338.69,273.10,141.90,7.86;10,151.52,284.06,171.53,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,295.02,337.97,7.86;10,151.52,305.98,329.07,7.86;10,151.52,316.94,329.07,7.86;10,151.52,327.90,20.99,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,391.75,305.98,88.85,7.86;10,151.52,316.94,185.50,7.86">Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName coords=""><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mobilenets</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.61,338.86,337.97,7.86;10,151.52,349.82,329.07,7.86;10,151.52,360.77,248.34,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,401.40,338.86,79.18,7.86;10,151.52,349.82,200.01,7.86">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName coords=""><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,369.65,349.82,110.94,7.86;10,151.52,360.77,156.06,7.86">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,371.73,337.98,7.86;10,151.52,382.69,270.29,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,219.64,371.73,260.95,7.86;10,151.52,382.69,128.43,7.86">Imageclef 2019: Projection-based ct image analysis for tb severity scoring and ct report generation</title>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,299.61,382.69,94.00,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,393.65,337.97,7.86;10,151.52,404.61,329.07,7.86;10,151.52,415.57,329.07,7.86;10,151.52,426.53,40.44,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,464.84,393.65,15.74,7.86;10,151.52,404.61,264.98,7.86">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,434.71,404.61,45.88,7.86;10,151.52,415.57,278.86,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,437.49,337.98,7.86;10,151.52,448.45,262.43,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,282.35,437.49,198.24,7.86;10,151.52,448.45,95.95,7.86">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.61,459.40,337.96,7.86;10,151.52,470.36,329.06,7.86;10,151.52,481.32,322.29,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,183.92,470.36,130.31,7.86">Designing network design spaces</title>
		<author>
			<persName coords=""><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,334.86,470.36,145.72,7.86;10,151.52,481.32,212.73,7.86">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
