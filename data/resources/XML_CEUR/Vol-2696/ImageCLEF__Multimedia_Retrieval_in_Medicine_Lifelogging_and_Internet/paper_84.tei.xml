<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.11,115.96,307.13,12.62;1,164.25,133.89,286.85,12.62">Enhanced Localization and Classification of Coral Reef Structures and Compositions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.48,171.56,72.79,8.74"><forename type="first">Kirill</forename><surname>Bogomasov</surname></persName>
							<email>bogomasov@hhu.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Heinrich Heine University</orgName>
								<address>
									<addrLine>Universitätsstraße 1</addrLine>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.79,171.56,60.45,8.74"><forename type="first">Philipp</forename><surname>Grawe</surname></persName>
							<email>grawe@hhu.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Heinrich Heine University</orgName>
								<address>
									<addrLine>Universitätsstraße 1</addrLine>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.03,171.56,62.85,8.74"><forename type="first">Stefan</forename><surname>Conrad</surname></persName>
							<email>stefan.conrad@hhu.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Heinrich Heine University</orgName>
								<address>
									<addrLine>Universitätsstraße 1</addrLine>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.11,115.96,307.13,12.62;1,164.25,133.89,286.85,12.62">Enhanced Localization and Classification of Coral Reef Structures and Compositions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">342D725545EA71C61BDAB16923439EE6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Segmentation</term>
					<term>Image Classification</term>
					<term>Object Localization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The automatic annotation of coral images is important for researching the underwater ecosystem, which is the focus of the Image-CLEFcoral task. We participated by refining our approaches from the last years challenge for localization and classification of corals within images of sea floor. Underwater images bear multiple difficulties which we tackle with applying image enhancement algorithms. To locate and classify the corals we applied multiple deep learning approaches and also revisioned our two-staged algorithm. The results show that deep learning approaches are the most convincing. Still, the localization of corals is the most challenging part for us, but we managed to increase our models performance significantly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Monitoring coral reefs and their health is an important component to understand effects of the climate change on maritime life <ref type="bibr" coords="1,335.57,449.91,9.96,8.74" target="#b7">[8]</ref>. Experts annotate underwater images, who not only have to deal with the complex morphology of the corals but also the large number of pictures. Computer vision based localization and classification of corals seems to be a reasonable solution. Unlike typical datasets for object detection tasks, underwater images hold more problems regarding the image quality and thus need very specific features and preprocessing.</p><p>In this paper we present the improvements of our approaches which are based on the last years ImageCLEFcoral <ref type="bibr" coords="1,290.84,533.60,10.52,8.74" target="#b2">[3]</ref> <ref type="bibr" coords="1,301.36,533.60,10.52,8.74" target="#b4">[5]</ref> submission. Additionally we used another deep learning approach, namely RetinaNet <ref type="bibr" coords="1,354.83,545.55,14.61,8.74" target="#b12">[13]</ref>, since this seemed to be the most promising. The classical machine learning is revisioned, but still not compatible with deep learning approaches regarding its performance. Lastly we implemented a popular suggestion to increase the image quality by preprocessing the images with algorithms made for underwater photography <ref type="bibr" coords="1,408.99,593.37,10.20,8.74" target="#b6">[7]</ref> <ref type="bibr" coords="1,419.19,593.37,10.20,8.74" target="#b0">[1]</ref>.</p><p>Overall we increased the performance of our approaches and can provide more insights, which we present in the following sections.</p><p>The research of last year coral task can be divided into classical feature engineering and deep learning approaches. Caridade and Marçal <ref type="bibr" coords="2,382.88,153.97,10.52,8.74" target="#b3">[4]</ref> used random forest classification, based on a selected feature set, consisting of color and texture features to localize and classify the substrate types. Jaisakthi et al. <ref type="bibr" coords="2,433.56,177.88,15.50,8.74" target="#b9">[10]</ref> used a faster R-CNN to solve this task. Another solution proposal presented by Steffens at el. <ref type="bibr" coords="2,159.55,201.79,15.50,8.74" target="#b21">[22]</ref> is based on a DCNN architecture. Our approach differs from the mentioned research. Considering the different properties, distributions and sizes of the corals, we rely on a combination of both categories of image processing. The good results substantiate our approach and make it one of the most promising so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>For the purpose of the task [9] <ref type="bibr" coords="2,266.25,303.61,10.20,8.74" target="#b5">[6]</ref>, a training dataset with 440 images and 12077 annotated substrates, which are labeled with one of 13 substarte types, is provided. An additional dataset contains 400 raw images, that is used for testing the predictions while no further information about the images is given to the participants.</p><p>Table <ref type="table" coords="2,170.65,393.93,3.87,8.74">1</ref>: Substrate types with their relative frequency in the training set.</p><p>The substrate representatives have a highly imbalanced distribution as shown in table 1. A random split of the data can lead to an even more disadvantageous class distribution, since it can exclude the representatives of the rare classes in one of the sets or increase the impact of frequent classes on the set. We give an example with a split into train and validation subsets with a ratio of 80 : 20 which distribution can be seen in table 2. To prevent the tendency of high imbalance, we propose the following procedure similar to <ref type="bibr" coords="2,336.73,656.12,14.61,8.74" target="#b21">[22]</ref>.</p><p>May P (D) be the relative distribution of classes of the complete image dataset D. Since both splits of the dataset should have the same distribution of classes, P (D) is our target distribution. May A and B, with A ∩ B = ∅, be the two sets after splitting D with the relative distributions P (A) and P (B). The key idea is to swap images between the two initial random splits A and B to make P (A), P (B) and P (D) as similar as possible. Let a, b with a ∈ A and b ∈ B be the two images to swap between A and B. We define A * = (A \ {a}) ∪ {b} and respectively B * = (B \ {b}) ∪ {a}. If the similarity between P (A * ) and P (D) is smaller than the similarity between P (A) and P (D), we swap the items, so that A = A * and B = B * . Since the similarity between P (B * ) and P (D) decreases when the similarity between P (A * ) and P (D) decreases, this approach works w.l.o.g. The loop over the images is running until there are no more swaps, i.e. no swap increases the similarity. To measure this similarity we use Jensen-Shannon divergence <ref type="bibr" coords="3,227.12,274.41,14.61,8.74" target="#b10">[11]</ref>. Since this procedure only converges to a local, but not global optimum, not every random split ends up in the same balanced split but consequently the optimal split is not found every time. Further research is needed to evaluate this approach, in regards of optimizations and metrics. The results of the balancing algorithm are shown in the table <ref type="table" coords="3,385.74,322.23,3.87,8.74" target="#tab_1">3</ref>.</p><p>The Jensen-Shannon divergence between the train and validation set before swapping is 0.040313, whereas after swapping its divergence is 0.0061. This is an improvement by a factor of almost 8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approaches</head><p>The "Coral reef image annotation and localisation task" can be divided into two tasks. Segmentation of various coral objects from images of sea ground and classification of those with their specific type of one of the 13 known types of substrates.</p><p>Due to multiple difficulties that underwater images bear, strategies to enhance the image quality which should help to find better features are used. We applied two of the state-of-the-art deep learning approaches and additionally combined these with an improvement of our own development <ref type="bibr" coords="4,418.60,383.36,9.96,8.74" target="#b2">[3]</ref>. Those approaches are presented in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Enhancement</head><p>Underwater images inherit problems, like the attenuation of light or the suspension of particles reflecting the light. Those conditions distort colors and visibility, which affect the performance of machine learning algorithms. Therefore we applied and evaluated multiple enhancement algorithms, that are specialized on underwater images.</p><p>Ancuti et al. <ref type="bibr" coords="4,209.79,524.61,10.52,8.74" target="#b0">[1]</ref> use Fusion <ref type="bibr" coords="4,274.28,524.61,15.50,8.74" target="#b23">[24]</ref> and work without knowledge of a physical model of the lighting conditions. Two derivations of the image, improving the white balance and the contrast, are fused together using different weight measures to restore the image. The authors show that they retrieve more features using SIFT <ref type="bibr" coords="4,187.96,572.43,15.50,8.74" target="#b14">[15]</ref> through applying their image enhancement. Ghani and Isa <ref type="bibr" coords="4,470.07,572.43,10.52,8.74" target="#b6">[7]</ref> use Rayleigh-stretching, as well as stretching using the HSV color model, to first correct the contrast and then correct the color. Further on the processings are referred to as Fusion and RD. Since we do not have access to already corrected images of the instant dataset, we use three evaluation measures that predict how human would perceive the image, based on learned examples. Namely the measures are BRISQUE <ref type="bibr" coords="4,241.15,644.16,14.61,8.74" target="#b15">[16]</ref>, NIQE <ref type="bibr" coords="4,290.43,644.16,15.50,8.74" target="#b16">[17]</ref> and PIQUE <ref type="bibr" coords="4,362.48,644.16,14.61,8.74" target="#b22">[23]</ref>. For all of them smaller values mean better image quality.</p><p>The evaluation <ref type="bibr" coords="5,218.16,118.99,15.50,8.74" target="#b17">[18]</ref> of the dataset of 2019 showed an improvement in image quality using the two enhancement algorithms, as seen in table 4 as well as in the subsections 4.4 and 5. Example images are shown in figure <ref type="figure" coords="5,410.87,142.90,3.87,8.74" target="#fig_0">1</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Yolo -Improvement</head><p>Neural networks are still state of the art in segmentation and classification tasks. Last year we used Yolov3 <ref type="bibr" coords="5,248.82,572.43,15.50,8.74" target="#b19">[20]</ref> as our main neural network approach. Especially areas with a particularly large denseness of smaller corals were challenging. The reason for this is on one hand Yolo's native ROI restraints which sets a natural limit on the regions considered within a certain area. On the other hand we were limited by the input data size with the largest resolution we could use with GPU of 608 × 608 pixels. Regarding the original resolution of images of 4032 × 3024, we preserve a scaling factor of at least of 5, i.e. each pixel represents an area of more than 25 pixels in the original image. Considering that the smallest corals consists of 12 × 12 pixels, the information loss is significant. We split the images into overlapping subimages of 608 × 608 pixels and trained the network on unscaled input. Consequently we got a large amount of images which also meant that the training time of our network was almost one month. Unfortunately, there was a mistake in the training data, therefore a revision of the results of the last years challenge was just recently announced. This left us with not enough time to retrain our working setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RetinaNET</head><p>For our second neural network approch we chose RetinaNet. The network showed impressive results on the COCO dataset and outperformed Yolov2 with a 17% higher AP 0.5 value. RetinaNet is well suited, since it is able to produce more predictions and is capable to work with less balanced data. At its core, the architecture consists of the following components: a feature pyramid network <ref type="bibr" coords="6,134.77,297.89,15.50,8.74" target="#b11">[12]</ref> (based on Resnet), a regressor for bounding box prediction and a classifier. Basically, it is a one-stage detector. The particular advantage of RetinaNet is the focal loss <ref type="bibr" coords="6,192.51,321.80,14.61,8.74" target="#b13">[14]</ref>. In case of end-to-end object detection, background predictions oftentimes dominate. The optimizer rates the prediction as correct and the loss of the positive background prediction forms the complete return loss. This mostly leads to an optimizer return value of zero for the background areas in case of cross entropy and thus reduce the loss. Focal loss weights the positive samples higher and ensures that the network performs better on unbalanced data. Right suited anchor boxes are the key to quality of object detection for any architecture that works with a "regions of interest". If the anchors are not properly prepared, the network has in many cases no chance of finding particularly small, neither large objects. In our dataset, we experience a wide variety of sizes of corals. Starting from a box size of 18 × 9 to a size of 3966 × 2662 pixels, the standard deviation of the areas is 629 assuming a square size. That means that irregular or peripherally sized objects present a special challenge. To tackle this problem, we chose a solution that was originally used on medical data <ref type="bibr" coords="6,462.33,477.22,14.61,8.74" target="#b24">[25]</ref>.</p><p>In our opinion, the potential for improvement can be easily transferred to coral context, since tumors and nodules as smaller objects are comparable to coral objects of small size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Own Developments</head><p>Besides the deep learning we also increased the performance of our classic feature based approaches. We evaluated the use of principal component analysis (PCA) <ref type="bibr" coords="6,134.77,584.39,15.50,8.74">[19]</ref> to select the best features, which increased the performance slightly. Apart from the features the choice of the classifier is important. Last year we used k-NN, which is depended from the parameter k. To overcome the search for the right parameter we evaluated the use of naïve bayes <ref type="bibr" coords="6,390.01,620.25,15.50,8.74" target="#b20">[21]</ref> for locating and classifying substrates.</p><p>When classifying the coral areas and non-coral areas, the features along with the approach are the same as in <ref type="bibr" coords="6,279.18,656.12,9.96,8.74" target="#b2">[3]</ref>, which is illustrated in figure <ref type="figure" coords="6,422.79,656.12,3.87,8.74" target="#fig_2">2</ref>. A problem that can be seen with k-NN is the low precision. This is due to labeling non-coral areas as coral areas, because most often water gets falsely classified. There are multiple ways to evaluate, as well as multiple things to evaluate. Beside the pure evaluation of the coral and non-coral tiles, we also evaluate the bounding boxes that enclose those coral tiles. In the end the evaluation of the found bounding boxes is more significant, but the pure evaluation of the coral and non-coral tiles helps with the assessment of the performance of finding bounding boxes in the generated black and white images (see figure <ref type="figure" coords="7,333.05,202.68,29.05,8.74" target="#fig_2">2b -2d</ref>). Likewise this creates the opportunity to compare different image enhancement algorithms. All results are discussed in the following.</p><p>Table <ref type="table" coords="7,178.16,241.58,4.98,8.74" target="#tab_3">5</ref> holds the evaluation of the coral/non-coral grid that is compared with a grid representing the ground truth. The results show that the naïve bayes classifier has increased the accuracy, as well as the precision but greatly decreased the recall. PCA on the other hand did not have a big impact.</p><p>It could be shown that image enhancement increases the performance, even if just slightly. Table <ref type="table" coords="7,230.02,304.39,4.98,8.74" target="#tab_4">6</ref> showed that connected components works much better with the naïve bayes classifier that k-NN. The naïve bayes classifier has an insignificantly increased, whereas k-NN has significantly worse performance. We believe that this is caused by k-NN having to much false-positives which results in big boxes, that cover rather more than less area. Another indication for this assumption is the decrease of precision but increase of recall (compare figure <ref type="figure" coords="7,134.77,376.12,8.58,8.74" target="#fig_2">2b</ref>). We also used the same two classifiers for classifying the bounding boxes. While using the same set of features, k-NN outperformed naïve bayes by far. Image enhancement shows its contribution again with tripling the accuracy of the naïve bayes classier, as seen in table <ref type="table" coords="8,295.13,154.86,3.87,8.74" target="#tab_5">7</ref>. This substantiate that the enhancement proposed by Ghani and Isa <ref type="bibr" coords="8,255.48,166.81,10.52,8.74" target="#b6">[7]</ref> leads to increased performance with our dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation of the Submitted runs</head><p>The evaluation was processed on test data that consists of images from four different geographical regions than the training set:</p><p>same location similar location geographically distinct but ecologically connected geographically and ecologically distinct</p><p>In total the test dataset has 400 images, made by 100 images per subset. The results are examined in more detail below, while the interesting and informative values are discussed in the text. For the complete list of results, the reader is referred to the task overview working note <ref type="bibr" coords="8,343.41,464.84,9.96,8.74" target="#b5">[6]</ref>. Each submitted result was produced by one of our approaches, all of which were trained on the full training set.</p><p>The classification of the substrate types based on the classic features alone fails largely with an MAP 0 value of around 27.4. and an MAP 0.5 of 1. Although we improved the MAP 0.5 value compared to last year by 300 percent, the results are still not really useful due to the low absolute values. This applies to both experiments, for the classification by means of k-NN based on the chosen features boosted by PCA and to statistical label assignment as well.</p><p>Our neuronal Network based approaches show a significantly better performance.</p><p>Since we had a limitation in the number of submissions, we chose none linear composition of available options. Our pool of options consisted alongside to classical features of RetinaNet and Yolov3, which both were trained on the unpreprocessed data and also on enhanced images by RD and Fusion. In addition, we worked with a variation of threshold τ ∈ {0.001, 0.1, 0.2, 0.5}, which limits the MAP.  We achieve our best result with an ensemble of RetinaNet and Yolov3. Whereby the predictions of RetinaNet were extended by the predictions of Yolov3. Both Networks were trained on RD-preprocessed images, with a τ of 0.1. The combination of both systems results in a MAP 0.5 of 39.2 % and MAP 0 of 80.6 %. It is noticeable that we predicted very few bounding boxes for a MAP 0.5 both through Yolov3 and through RetinaNet, additionally the found predictions were far from being present in all of the images.</p><p>In case of reduction of the accepted overlap, we get significantly more bounding boxes which also leads to a higher chance of hitting the right coral within the test images on cost of our overall accuracy. However, if we increase the accuracy, the MAP value drops. The following are significant examples: RetinaNet (τ = 0.01) combined with Yolov3 (τ = 0.01) has an MAP 0.5 of 0.303 and an MAP 0 of 0.727, but only an overall accuracy of 7 %. In contrast to it RetinaNet alone produces significantly fewer boxes with (τ = 0.2), but achieves the best overall accuracy of 14.2 % with an MAP 0.5 of only 30.3 and a MAP 0 of 66.3 % while the accuracy is 10 % higher than in our best run.</p><p>A possible explanation is probably that RetinaNet has not finished training in 35 epochs. The large amount of predictions with a low overlap value could probably be boosted by non-maximum suppression. However, we were not aware of this problem until the results were published. When provided with such a variety of possibilities, it can not be clearly determined which enhancement variant is the best choice. Certainly enhanced images improve the predictions, while we suspect that RD is superior to Fusion for coral images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Transferability of the results within the test data subsets</head><p>One major question is the transferability of the results within the test dataset, considering that results, that form the average measure, vary widely. For our best run the MAP 0.5 for same location increases by 6 percent to 45.7 % and by 1 % to 81.5 % the MAP 0 . For similar location, however, it drops to 28.3 % for MAP 0.5 , but reaches a value of 86.4 % for MAP 0 and thus has the highest score among all presented submissions by all participants. Our best approach performs very well on geographically similar data. The MAP 0.5 is 42.6 % and the MAP 0 81.2 %. Overall, this part of the test dataset seems to be less complex, which is shown by the fact that the performance of almost all of our approaches have an increased performance on it.</p><p>It is also worth mentioning that we perform significantly worse on geographically distinct data with just an MAP 0.5 0.125 and an MAP 0 0.362. This tendency is also evident in the other approaches we used. The data seems to differ significantly, we either generalize less or the classes that are harder to recognize are more present. Some additional, yet unknown substrate types may be more dominant in geographically and ecologically distinct rocky reef and lead to distorted results. A further investigation of the dataset is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Overall, our approaches show significantly better results than last year. A comparison of our best approaches between the two years shows that we have improved the MAP 0.5 by 13.4 %. The classification of the tiles into in-and outside boxes could also be improved by a Bayesian classifier, but is still far from being accurate.</p><p>Image enhancement techniques on the last years data were confirmed by the evaluation of the current test dataset, which leads us to the conclusion that the correction of blurry images in terms of contrast and sharpness is necessary. The RD algorithm makes the greatest contribution to improving the quality of the images. Less noteworthy results are made with the classical feature engineering approach. A deeper examination of the features and their information value is needed.</p><p>It can be assumed that the fair partitioning of images according to our balancing strategy, also has significantly contributed to the improved results. Deep learning strategies generalize quiet well and are superior when using for this task. Especially the performance of RetinaNet, since it is not only better on the coco dataset than the state of the art. So far, the complexity of the images can hardly be handled by a single approach. We still see the most potential in an ensemble of several architectures. The combination of advantages of different approaches is the key to a stable solution. Since the amount of data has grown, while remaining relatively small, we still cannot exclude the potential of classic machine learning. Usually neural networks show a much better performance with an increased amount of data. For this reason, the images should be split into overlapping images and thus increase the number of training samples.</p><p>For future approaches, we recommend the usage of more specific features that are suitable for corals, such as used in <ref type="bibr" coords="11,329.07,274.75,9.96,8.74" target="#b1">[2]</ref>. However, we would rather rely on an ensemble of neural networks of the gold standard, which we would reduce in depth to shorten the training time and increase in terms of input resolution to decrease the information loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,204.73,475.67,205.90,8.74;5,148.98,297.81,103.74,138.44"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Comparison of the image enhancements.</figDesc><graphic coords="5,148.98,297.81,103.74,138.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,173.53,247.85,93.09,7.86;9,312.29,247.85,166.00,7.86;9,312.29,258.81,48.42,7.86;9,137.08,401.24,166.00,7.86;9,137.08,412.20,89.26,7.86;9,312.29,401.24,166.00,7.86;9,312.29,412.20,111.66,7.86"><head></head><label></label><figDesc>(a) Grund truth boxes. (b) Inside (white) and outside tiles (black) using k-NN. (c) Inside (white) and outside tiles (black) using PCA and k-NN. (d) Inside (white) and outside tiles (black) with PCA and naive Bayes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,134.77,433.13,345.82,8.74;9,134.77,445.09,224.50,8.74;9,137.08,269.21,166.00,124.50"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: Visualized process of localization corals with our approach. The raw picture is taken from the ImageCLEFcoral dataset<ref type="bibr" coords="9,345.98,445.09,9.96,8.74" target="#b5">[6]</ref>.</figDesc><graphic coords="9,137.08,269.21,166.00,124.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,159.49,400.42,296.37,163.09"><head>Table 2 :</head><label>2</label><figDesc>Substrate distribution before balancing.</figDesc><table coords="3,159.49,412.78,296.37,150.73"><row><cell>Class label</cell><cell cols="2">Relative frequency train Relative frequency valid</cell></row><row><cell cols="2">c algae macro or leaves 0.00799747</cell><cell>0.005827505</cell></row><row><cell>c fire coral millepora</cell><cell>0.00126276</cell><cell>0.00271950272</cell></row><row><cell>c hard coral boulder</cell><cell>0.12722298</cell><cell>0.167443667</cell></row><row><cell cols="2">c hard coral branching 0.0967063</cell><cell>0.101787102</cell></row><row><cell cols="2">c hard coral encrusting 0.07965906</cell><cell>0.0730380730</cell></row><row><cell>c hard coral foliose</cell><cell>0.01515311</cell><cell>0.012810513</cell></row><row><cell cols="2">c hard coral mushroom 0.01894139</cell><cell>0.0167055167</cell></row><row><cell cols="2">c hard coral submassive 0.01746817</cell><cell>0.01243201</cell></row><row><cell>c hard coral table</cell><cell>0.0021046</cell><cell>0.000388500389</cell></row><row><cell>c soft coral</cell><cell>0.47606019</cell><cell>0.442113442</cell></row><row><cell cols="2">c soft coral gorgonian 0.00683995</cell><cell>0.00971250971</cell></row><row><cell>c sponge</cell><cell>0.13869304</cell><cell>0.144910645</cell></row><row><cell>c sponge barrel</cell><cell>0.01189098</cell><cell>0.101010101</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,159.49,127.33,296.37,163.09"><head>Table 3 :</head><label>3</label><figDesc>Substrate distribution after balancing.</figDesc><table coords="4,159.49,139.70,296.37,150.73"><row><cell>Class label</cell><cell cols="2">Relative frequency train Relative frequency valid</cell></row><row><cell cols="2">c algae macro or leaves 0.00758972</cell><cell>0.00735809</cell></row><row><cell>c fire coral millepora</cell><cell>0.00140952</cell><cell>0.00210231</cell></row><row><cell>c hard coral boulder</cell><cell>0.13574759</cell><cell>0.13594954</cell></row><row><cell cols="2">c hard coral branching 0.09779898</cell><cell>0.09775753</cell></row><row><cell cols="2">c hard coral encrusting 0.07828255</cell><cell>0.07813595</cell></row><row><cell>c hard coral foliose</cell><cell>0.01474574</cell><cell>0.0143658</cell></row><row><cell cols="2">c hard coral mushroom 0.01843218</cell><cell>0.01857043</cell></row><row><cell cols="2">c hard coral submassive 0.01637211</cell><cell>0.01646811</cell></row><row><cell>c hard coral table</cell><cell>0.00173479</cell><cell>0.00175193</cell></row><row><cell>c soft coral</cell><cell>0.46882793</cell><cell>0.4688157</cell></row><row><cell cols="2">c soft coral gorgonian 0.0074813</cell><cell>0.00735809</cell></row><row><cell>c sponge</cell><cell>0.14008457</cell><cell>0.13980378</cell></row><row><cell>c sponge barrel</cell><cell>0.01149301</cell><cell>0.01156272</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,134.77,185.56,345.83,65.46"><head>Table 4 :</head><label>4</label><figDesc>Image enhancement algorithms evaluated on ImageCLEF Coral dataset 2019.</figDesc><table coords="5,235.02,209.88,145.33,41.14"><row><cell cols="3">Algorithm BRISQUE NIQE PIQUE</cell></row><row><cell>None</cell><cell>25.98</cell><cell>3.61 26.34</cell></row><row><cell>Fusion</cell><cell>22.66</cell><cell>3.43 30.99</cell></row><row><cell>RD</cell><cell>20.92</cell><cell>2.94 25.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,138.79,425.65,337.77,76.22"><head>Table 5 :</head><label>5</label><figDesc>Evaluation of the coral/non-coral classification, based on the tiles.</figDesc><table coords="7,138.79,438.02,337.77,63.85"><row><cell></cell><cell></cell><cell></cell><cell>Image enhancement</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>None</cell><cell></cell><cell>Fusion</cell><cell></cell><cell>RD</cell><cell></cell></row><row><cell>Approach</cell><cell>Acc Prec</cell><cell>Rec</cell><cell>Acc Prec Rec</cell><cell>Acc</cell><cell>Prec</cell><cell>Rec</cell></row><row><cell>k-NN</cell><cell cols="6">0.617 0.4496 0.5283 0.5823 0.4066 0.4813 0.6146 0.4355 0.4275</cell></row><row><cell cols="7">k-NN with PCA 0.6146 0.4471 0.5343 0.5858 0.4075 0.4637 0.6171 0.4371 0.4134</cell></row><row><cell cols="7">Bayes with PCA 0.6488 0.4545 0.1317 0.6570 0.3550 0.0031 0.6575 0.4857 0.0192</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,134.77,572.80,345.83,88.18"><head>Table 6 :</head><label>6</label><figDesc>Evaluation of the coral/non-coral classification, based on the bounding boxes.</figDesc><table coords="7,138.26,597.12,338.84,63.85"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Image enhancement</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>None</cell><cell></cell><cell>Fusion</cell><cell></cell><cell></cell><cell>RD</cell><cell></cell></row><row><cell>Approach</cell><cell>Acc Prec</cell><cell>Rec</cell><cell>Acc Prec</cell><cell>Rec</cell><cell>Acc</cell><cell>Prec</cell><cell>Rec</cell></row><row><cell>k-NN</cell><cell cols="7">0.4234 0.3920 0.8755 0.3919 0.4386 0.8543 0.4812 0.3973 0.8302</cell></row><row><cell cols="8">k-NN with PCA 0.4088 0.4088 0.8927 0.4097 0.4345 0.83106 0.4879 0.3888 0.8186</cell></row><row><cell cols="8">Bayes with PCA 0.6518 0.4833 0.2558 0.6571 0.3373 0.0025 0.6606 0.5948 0.0254</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,134.77,209.79,345.83,65.86"><head>Table 7 :</head><label>7</label><figDesc>Evaluation of substrate classification. The given values represent the accuracy.</figDesc><table coords="8,243.70,234.11,127.97,41.54"><row><cell></cell><cell>Image enhancement</cell></row><row><cell cols="2">Approach None Fusion RD</cell></row><row><cell>k-NN</cell><cell>0.4332 0.4550 0.4374</cell></row><row><cell>Bayes</cell><cell>0.1261 0.3328 0.3672</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,369.49,337.63,7.86;11,151.52,380.45,329.07,7.86;11,151.52,391.41,148.73,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,357.88,369.49,122.72,7.86;11,151.52,380.45,81.16,7.86">Enhancing underwater images and videos by fusion</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bekaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,274.96,380.45,205.63,7.86;11,151.52,391.41,46.11,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,402.70,337.64,7.86;11,151.52,413.66,329.07,7.86;11,151.52,424.60,329.07,7.89;11,151.52,435.58,171.86,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,312.75,402.70,167.85,7.86;11,151.52,413.66,180.06,7.86">Coral reef image classification employing improved ldp for feature extraction</title>
		<author>
			<persName coords=""><forename type="first">Ani</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mary</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.jvcir.2017.09.008</idno>
		<ptr target="https://doi.org/10.1016/j.jvcir.2017.09.008" />
	</analytic>
	<monogr>
		<title level="j" coord="11,343.43,413.66,137.17,7.86;11,151.52,424.62,18.74,7.86">J. Vis. Comun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="225" to="242" />
			<date type="published" when="2017-11">Nov 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,446.88,337.64,7.86;11,151.52,457.84,329.07,7.86;11,151.52,468.80,329.07,7.86;11,151.52,479.76,295.28,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,319.58,446.88,161.02,7.86;11,151.52,457.84,236.07,7.86">A two-staged approach for localization and classification of coral reef structures and compositions</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bogomasov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grawe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Conrad</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/paper106.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,409.53,457.84,71.06,7.86;11,151.52,468.80,236.73,7.86">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">September 9-12, 2019 (2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,491.06,337.63,7.86;11,151.52,502.01,146.14,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,295.36,491.06,185.23,7.86;11,151.52,502.01,71.71,7.86">Automatic classification of coral images using color and textures</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M R</forename><surname>Caridade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R S</forename><surname>Marçal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,244.28,502.01,24.70,7.86">CLEF</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,513.31,337.63,7.86;11,151.52,524.27,329.07,7.86;11,151.52,535.23,243.53,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,206.53,524.27,167.18,7.86">Overview of ImageCLEFcoral 2019 task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">G</forename><surname>Clift</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="11,398.45,524.27,82.14,7.86;11,151.52,535.23,147.90,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,546.53,337.63,7.86;11,151.52,557.49,329.07,7.86;11,151.52,568.45,329.07,7.86;11,151.52,579.41,88.33,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,204.74,557.49,275.85,7.86;11,151.52,568.45,68.99,7.86">Overview of the ImageCLEFcoral 2020 task: Automated coral reef image annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">G</forename><surname>Clift</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="11,242.79,568.45,105.43,7.86">CLEF2020 Working Notes</title>
		<title level="s" coord="11,355.99,568.45,124.60,7.86;11,151.52,579.41,21.70,7.86">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,590.70,337.64,7.86;11,151.52,601.64,329.07,7.89;11,151.52,612.62,25.60,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,265.08,590.70,215.51,7.86;11,151.52,601.66,226.80,7.86">Underwater image quality enhancement through composition of dual-intensity images and rayleigh-stretching</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S A</forename><surname>Ghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A M</forename><surname>Isa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,385.56,601.66,51.42,7.86">SpringerPlus</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">757</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,623.92,337.64,7.86;11,151.52,634.88,329.07,7.86;11,151.52,645.81,329.07,7.89;12,139.37,119.67,7.17,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,458.56,634.88,22.04,7.86;11,151.52,645.84,222.65,7.86">Coral reefs under rapid climate change and ocean acidification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Hoegh-Guldberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Mumby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Hooten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Steneck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Greenfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Harvell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Sale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Caldeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,380.47,645.84,27.71,7.86">science</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="issue">5857</biblScope>
			<biblScope unit="page" from="1737" to="1739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,151.53,119.67,329.07,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,329.07,7.86;12,151.52,152.55,329.07,7.86;12,151.52,163.51,329.07,7.86;12,151.52,174.47,329.07,7.86;12,151.52,185.43,329.07,7.86;12,151.52,196.39,329.07,7.86;12,151.52,207.34,329.07,7.86;12,151.52,218.30,329.07,7.86;12,151.52,229.26,34.31,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,300.99,174.47,179.60,7.86;12,151.52,185.43,255.37,7.86">Overview of the imageclef 2020: Multimedia retrieval in medical, lifelogging, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,426.43,185.43,54.16,7.86;12,151.52,196.39,329.07,7.86;12,151.52,207.34,253.34,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="12,456.14,207.34,24.45,7.86;12,151.52,218.30,138.63,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="12,142.62,239.55,337.98,7.86;12,151.52,250.50,162.56,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,340.98,239.55,139.61,7.86;12,151.52,250.50,88.12,7.86">Coral reef annotation and localization using faster r-cnn</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Jaisakthi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mirunalini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Aravindan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,260.72,250.50,24.70,7.86">CLEF</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,260.79,337.98,7.86;12,151.52,271.72,168.72,7.89" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,182.68,260.79,203.00,7.86">Divergence measures based on the shannon entropy</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,392.36,260.79,88.24,7.86;12,151.52,271.75,76.58,7.86">IEEE Transactions on Information theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,282.03,337.98,7.86;12,151.52,292.96,329.07,7.89;12,151.52,303.95,131.41,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="12,463.56,282.03,17.03,7.86;12,151.52,292.99,184.33,7.86">Feature pyramid networks for object detection</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CoRR abs/1612.03144</idno>
		<ptr target="http://arxiv.org/abs/1612.03144" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,314.23,337.97,7.86;12,151.52,325.19,329.07,7.86;12,151.52,336.15,86.01,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,372.66,314.23,107.93,7.86;12,151.52,325.19,35.49,7.86">Focal loss for dense object detection</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,206.24,325.19,270.65,7.86">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,346.43,337.98,7.86;12,151.52,357.39,329.07,7.86;12,151.52,368.35,329.07,7.86;12,151.52,379.31,169.48,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,417.66,346.43,62.93,7.86;12,151.52,357.39,100.86,7.86">Focal loss for dense object detection</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.324</idno>
		<ptr target="http://dx.doi.org/10.1109/ICCV.2017.324" />
	</analytic>
	<monogr>
		<title level="m" coord="12,293.22,357.39,187.37,7.86;12,151.52,368.35,98.93,7.86">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10">2017. Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,389.59,337.97,7.86;12,151.52,400.52,219.84,7.89" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,205.63,389.59,234.12,7.86">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,448.31,389.59,32.27,7.86;12,151.52,400.55,132.44,7.86">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,410.83,337.98,7.86;12,151.52,421.76,329.07,7.89;12,151.52,432.75,25.60,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,315.37,410.83,165.22,7.86;12,151.52,421.79,75.65,7.86">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,236.04,421.79,164.87,7.86">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,443.03,337.98,7.86;12,151.52,453.96,284.61,7.89" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,334.38,443.03,146.22,7.86;12,151.52,453.99,63.08,7.86">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,221.69,453.99,122.43,7.86">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,464.27,337.98,7.86;12,134.77,474.55,7.85,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="12,211.85,464.27,240.10,7.86">Comparison of methods for underwater image improvement</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">C</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,474.55,337.97,7.86;12,151.52,485.51,329.07,7.86;12,151.52,496.45,88.24,7.89" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,202.74,474.55,255.62,7.86">Liii. on lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,464.71,474.55,15.88,7.86;12,151.52,485.51,329.07,7.86">The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="559" to="572" />
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,506.75,337.98,7.86;12,151.52,517.71,97.80,7.86" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m" coord="12,260.26,506.75,151.93,7.86">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.61,528.00,337.98,7.86;12,151.52,538.96,323.75,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,216.27,528.00,194.86,7.86">An empirical study of the naive bayes classifier</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Rish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,433.29,528.00,47.30,7.86;12,151.52,538.96,221.80,7.86">IJCAI 2001 workshop on empirical methods in artificial intelligence</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="41" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,549.24,337.97,7.86;12,151.52,560.20,329.07,7.86;12,151.52,571.16,67.45,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,460.24,549.24,20.35,7.86;12,151.52,560.20,325.10,7.86">Deep segmentation: using deep convolutional networks for coral reef pixel-wise parsing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Steffens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ravenscroft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hagras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,165.60,571.16,24.69,7.86">CLEF</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,581.44,337.98,7.86;12,151.52,592.40,329.07,7.86;12,151.52,603.36,309.89,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,151.52,592.40,255.17,7.86">Blind image quality evaluation using perception based features</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Venkatanath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Praneeth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Bh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Channappayya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Medasani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,450.38,592.40,30.22,7.86;12,151.52,603.36,215.69,7.86">Twenty First National Conference on Communications (NCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,613.64,337.98,7.86;12,151.52,624.57,130.54,7.89" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,258.91,613.64,177.55,7.86">Objective image fusion performance measure</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xydeas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Petrovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,443.58,613.64,37.01,7.86;12,151.52,624.60,38.52,7.86">Electronics letters</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="308" to="309" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,634.88,337.98,7.86;12,151.52,645.84,329.07,7.86;12,151.52,656.80,316.65,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,288.90,634.88,191.69,7.86;12,151.52,645.84,139.34,7.86">Improving retinanet for ct lesion detection with dense masks from weak recist labels</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zlocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,309.83,645.84,170.77,7.86;12,151.52,656.80,193.36,7.86">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
