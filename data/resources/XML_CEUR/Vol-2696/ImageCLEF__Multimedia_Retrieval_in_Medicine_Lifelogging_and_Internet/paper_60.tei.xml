<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.60,115.96,322.15,12.62;1,199.78,133.89,215.80,12.62">Lifelog Moment Retrieval with Self-Attention based Joint Embedding Model</title>
				<funder ref="#_VautdWc">
					<orgName type="full">Vingroup Innovation Foundation (VINIF)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,148.77,172.01,112.00,8.74"><forename type="first">Hoang-Phuc</forename><surname>Trang-Trung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">University of Science</orgName>
								<orgName type="institution">VNU-HCM</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Vietnam National University</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.66,172.01,63.80,8.74"><forename type="first">Hoang-Anh</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">University of Science</orgName>
								<orgName type="institution">VNU-HCM</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Vietnam National University</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.72,172.01,71.73,8.74"><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">University of Science</orgName>
								<orgName type="institution">VNU-HCM</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">John von Neumann Institute</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Vietnam National University</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.60,115.96,322.15,12.62;1,199.78,133.89,215.80,12.62">Lifelog Moment Retrieval with Self-Attention based Joint Embedding Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7DFFD81257CC31C93155D1A656B7C5B0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lifelog retrieval</term>
					<term>Image-Text cross-modal retrieval</term>
					<term>User interface</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the swift growth of technology, personal devices like cameras or healthcare sensors are more and more approachable, and many people use these devices to record their daily lives. So there is an increasing need for exploiting that enormous amount of data to understand more about how people live their lives. Thus, we introduce a novel interactive system to retrieve specific moments utilizing textbased queries. We propose Self-Attention based Joint Embedding Model (SAJEM) for that purpose. In our proposed method, we first extract visual and text features, then map them to a single common space, and calculate cosine distance for ranking. Besides, our system has two more auxiliary components using ResNet152 features and metadata of images to help users extend their query results. We also design a web application with an easy-to-use user interface to visualize and retrieve lifelog data. With this solution, we achieve the first rank in Lifelog Moment Retrieval task of ImageCLEF Lifelog 2020 with F1@10 score of 0.811.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lifelogging has been becoming more and more popular in the research communities. Lifelog dataset is the record of "lifeloggers" daily life, which mainly contains images captured by personal cameras and sensor data like location, heart rate, weight, audio, temperature, etc. With the rapid technological progress today, the amount of lifelog data becomes tremendous and almost impossible to handle manually. This motivates many researchers to develop a reliable and convenient system to exploit this lifelog data. The primary usage of this kind of system is to recall designated moments in the past, but it can also be used to analyze human social traits <ref type="bibr" coords="1,188.86,613.26,10.52,8.74" target="#b0">[1]</ref> or monitor user's health.</p><p>Many challenges and tasks in lifelogging have been proposed to create a competitive environment where people can address the problem and share knowledge about it. ImageCLEF <ref type="bibr" coords="2,233.92,142.90,10.52,8.74" target="#b1">[2]</ref> is one of those events which is held annually as part of the CLEF initiative labs. ImageCLEF Lifelog 2020 <ref type="bibr" coords="2,377.82,154.86,10.52,8.74" target="#b2">[3]</ref> includes 2 subtasks: Lifelog Moment Retrieval (LMRT) and Sport Performance Lifelog (SPLL). In this paper, we only focus on LMRT subtask. This year's lifelog dataset is enriched with more than 190,000 images in total (about 4.5 months of data from 3 lifeloggers, 1500-2000 images per day) along with visual concepts, semantic content, biometrics information, music listening history, and computer usage. Our mission is to find specific predefined moments described in 10 test topics which are handed out by organizers.</p><p>To resolve this problem, we propose an interactive retrieval system which allows user to express their queries in multiple ways. Our system depends on two major ideas:</p><p>-We aim to build a model that understand text queries instead of matching words. So we create a Self-Attention based Joint Embedding Model and train it on the COCO dataset <ref type="bibr" coords="2,288.13,320.71,10.52,8.74" target="#b3">[4]</ref> then use it on lifelog domain. We utilize a self-attention mechanism to learn the interaction between words in the text sentences and between objects in the images. With this model, users no longer need to overthink about choosing the right features to express a query, just input a sentence and see the results. We also add some auxiliary components like find similar semantic images or find images by metadata to make our system more powerful and reliable. -An user-friendly web application allows users to split the query into multiple steps for better accuracy and easily choose the desired images for submission.</p><p>The rest of the paper is organized as follows: section 2 lists some related works to our research. In section 3, we introduce SAJEM model and describe our system in detail. Section 4 shows how we apply this system to LMRT subtask in ImageCLEF Lifelog 2020. Finally, section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Lifelogging. Many lifelog retrieval systems have been developed in the past few years. Most of the previous works first extract visual concepts from images and find a way to save that data to a database for efficiently retrieving later on. They usually use textual tags to index an image. These tags include name of the detected objects <ref type="bibr" coords="2,210.31,584.39,7.75,8.74" target="#b4">[5]</ref><ref type="bibr" coords="2,218.06,584.39,3.87,8.74" target="#b5">[6]</ref><ref type="bibr" coords="2,221.93,584.39,7.75,8.74" target="#b6">[7]</ref>, scene <ref type="bibr" coords="2,264.55,584.39,8.19,8.74" target="#b4">[5]</ref><ref type="bibr" coords="2,272.74,584.39,4.10,8.74" target="#b5">[6]</ref><ref type="bibr" coords="2,276.84,584.39,8.19,8.74" target="#b6">[7]</ref> or even optical characters <ref type="bibr" coords="2,407.34,584.39,10.52,8.74" target="#b5">[6]</ref> in the image. A different approach is used in <ref type="bibr" coords="2,273.47,596.34,10.52,8.74" target="#b7">[8]</ref> that they extract a combination of low-level features like HSV histogram and BRIEF features for example-based retrieval.</p><p>After the offline processing stage, a user interface is built to visualize the results and efficiently interact with the indexed database. For systems that leverage textual tags, they often use term queries combined with some techniques to improve accuracy: looking for synonym on thesaurus.com <ref type="bibr" coords="2,370.60,656.12,10.76,8.74" target="#b4">[5,</ref><ref type="bibr" coords="2,381.36,656.12,7.17,8.74" target="#b6">7]</ref>, utilizing pre-trained word embedding <ref type="bibr" coords="3,210.72,118.99,10.52,8.74" target="#b5">[6]</ref> or BERT model <ref type="bibr" coords="3,299.08,118.99,10.52,8.74" target="#b8">[9]</ref> to obtain similar semantic words. Another interesting way of interaction is sketch-based retrieval which is enabled by video retrieval tools like VIRET <ref type="bibr" coords="3,282.02,142.90,14.61,8.74" target="#b9">[10]</ref>, diveXplore <ref type="bibr" coords="3,355.25,142.90,15.50,8.74" target="#b10">[11]</ref> and vitrivr <ref type="bibr" coords="3,426.77,142.90,14.61,8.74" target="#b11">[12]</ref>. In <ref type="bibr" coords="3,462.32,142.90,14.61,8.74" target="#b12">[13]</ref>, they even use virtual reality with distance-based and contact-based interaction to visualize and explore lifelog data.</p><p>Text-based image retrieval. Image-Text matching has been studied for a long time to support many essential applications: image captioning or crossmodal retrieval. Most of the existing approaches can be divided into two categories: designing a network to predict the similarity scores of image-text pairs or finding a joint embedding space under which we can compare image and text representations directly. The former idea is not suitable for retrieval tasks when the dataset is big because we need to run the network to calculate similarity scores between the query and all instances in the dataset at the online test stage. So we choose the latter idea to develop our model. SAJEM model is trained to minimize the Margin Ranking Loss between image and text embeddings with a negative sample mining strategy introduced in <ref type="bibr" coords="3,340.55,298.32,14.61,8.74" target="#b13">[14]</ref>.</p><p>3 Interactive Lifelog Retrieval System</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Overview</head><p>After studying previous works on lifelog retrieval, we find out that there is a severe limitation in object detection based systems: they only focus on the existence of the objects, not interactions between them. We introduce Self-Attention based Joint Embedding Model (SAJEM), a novel model to leverage the interaction between objects and even the context of images. We integrate the model with two more auxiliary components to form a novel system which can handle multiple types of requests:</p><p>-Query by text sentence: The best way to express a query is through a free text sentence. Our system takes a sentence as the input, extract its feature, and map to joint embedding space. Then we use that sentence embedding to match with all image embeddings to find the most relevant images to that sentence. -Query similar images: We calculate the cosine distance between the feature of a query image with all pre-extracted features of lifelog data and output images with the smallest distance. Features are extracted from ResNet152 model to capture the semantic meaning of images. -Query by metadata: Filter data by places and time metadata provided by the organizers.</p><p>Figure <ref type="figure" coords="3,181.70,584.39,4.98,8.74">1</ref> visualizes components and data flow in our system. At the offline stage, SAJEM and ResNet features are extracted for all images in lifelog dataset. Furthermore, we separate the web application into frontend and backend: the frontend module provides an user interface to help users interact with the system, and the backend handles requests from frontend and contacts with models and database. We briefly explain each component of the system in the following subsections.</p><p>Fig. <ref type="figure" coords="4,153.45,321.89,3.87,8.74">1</ref>: Main components of the proposed system. There are 3 components corresponding to 3 types of input: free text sentence, image and metadata. Features of all images in the lifelog dataset are pre-extracted at offline processing stage to reduce computation time at online retrieval stage</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Self-Attention based Joint Embedding Model</head><p>The overall architecture of our proposed model is shown in Figure <ref type="figure" coords="4,451.44,411.99,3.87,8.74" target="#fig_0">2</ref>. Our model consists of two branches corresponding to two domains we are working on: image and text domains. In the image branch, we begin with the features of detected regions in the image generated by the Bottom-Up Attention model <ref type="bibr" coords="4,462.33,447.86,14.61,8.74" target="#b14">[15]</ref>. We then use a Self-Attention Module to learn the interaction between image regions and build a single vector representation for the whole image. In the text branch, we use RoBERTa model <ref type="bibr" coords="4,275.53,483.72,15.50,8.74" target="#b15">[16]</ref> to learn the representation for an input sentence. Finally, Image/Text Feature Encoders are used to project corresponding features to the joint embedding space. Both branches of the model take advantage of the self-attention mechanism (Self-Attention Module in the image branch and RoBERTa in the text branch). Thus we call our model as Self-Attention based Joint Embedding Model.</p><p>Bottom-Up Attention Faster R-CNN: First, we extract the object-level features of an image. Faster R-CNN <ref type="bibr" coords="4,300.77,584.39,15.50,8.74" target="#b16">[17]</ref> is a state-of-the-art model in object detection. It is a two-stage detector. In the first stage, a Region Proposal Network is used to predict multiple bounding box proposals of different scales and aspect ratios. Then they use non-maximum suppression to reduce the number of proposals. In the second stage, each box proposal is transformed into a small feature map by Region of Interest pooling layer then feeds into a CNN to predict class label and class-specific bounding box refinements. In Bottom-Up and Top-Down attention paper <ref type="bibr" coords="5,355.36,395.78,14.61,8.74" target="#b14">[15]</ref>, they use Faster R-CNN with ResNet101 backbone and introduce a simple "hard" attention mechanism: only selects few regions with highest class detection probabilities. The feature of each region is defined as the mean-pooled convolutional feature of that region, so the dimension of each feature vector is 2048. To pretrain this bottom-up model, they initialize Faster R-CNN with ResNet101 (pretrained from ImageNet) then train on the Visual Genome dataset. To learn good feature representations, they add an additional training output for predicting attribute classes along with the old object classes.</p><p>We use the Bottom-Up pre-trained model available on GitHub<ref type="foot" coords="5,420.42,501.80,3.97,6.12" target="#foot_0">4</ref> . Each image is represented by a set of feature vectors (each vector corresponding to a detected region in the image) I = {r 1 , r 2 , ..., r k }, r i ∈ R D (in this case D = 2048). Due to hardware limitations, we only select 15 regions with the highest probabilities.</p><p>Self-Attention Module: We adopt the idea of Multi-Head Self-Attention from Transformer model <ref type="bibr" coords="5,222.09,585.57,15.50,8.74" target="#b17">[18]</ref> with some refinements: use Dot-Product Attention instead of Scaled Dot-Product Attention and only use one head when applying attention.</p><p>As we mention above, each image is now represented by a set of vectors I = {r 1 , r 2 , ..., r k }, r i ∈ R D . We can view it as a matrix I ∈ R k×D . Then, we Fig. <ref type="figure" coords="6,153.45,321.89,3.87,8.74">3</ref>: Self-Attention Module. This module takes image region features from Bottom-Up Faster-RCNN as input and apply Dot-Product Attention on these features.</p><p>apply Self-Attention module as follows:</p><formula xml:id="formula_0" coords="6,134.77,402.76,345.83,94.57">Q = IW Q + b Q K = IW K + b K V = IW V + b V I * = Sof tmax(QK T )V where W Q , W K , W V ∈ R D×D are the weight matrices and b Q , b K , b V ∈ R D are biases of linear transformations, respectively.</formula><p>After applying Self-Attention, we add the residual connection to the original image features, then use Layer Normalization:</p><formula xml:id="formula_1" coords="6,240.81,531.69,133.74,10.81">Output = LayerN orm(I + I * )</formula><p>Finally, we apply Average Pooling over all regions of the image to achieve one D-dimension vector representation for each image.</p><p>RoBERTa as a text feature extractor: In recent years, Transformer-like models <ref type="bibr" coords="6,168.46,608.30,15.50,8.74" target="#b17">[18]</ref> have become very popular in the NLP field. These models leverage attention mechanism to learn good text representations through Mask Language Model and Next Sentence Prediction tasks, then apply it to many down-stream tasks and achieve state-of-the-art results. In particular, BERT (Bidirectional Encoder Representations from Transformers) <ref type="bibr" coords="6,335.42,656.12,15.50,8.74" target="#b18">[19]</ref> use bidirectional transformer (both left-to-right and right-to-left direction) to produce context-aware representations.</p><p>In this work, we use RoBERTa model <ref type="bibr" coords="7,320.75,142.90,14.61,8.74" target="#b15">[16]</ref>, which is the BERT model with some training tricks and more data. We use the pretrained RoBERTa-base model provided by HuggingFace <ref type="bibr" coords="7,248.46,166.81,15.50,8.74" target="#b19">[20]</ref> available on GitHub<ref type="foot" coords="7,354.88,165.24,3.97,6.12" target="#foot_1">5</ref> . Each token corresponds to each word in text sentence or special character like 'CLS' for classification purpose, 'SEP' for separating sentences, etc. Each token is represented by a 768-dim vector. We concatenate the last 4 'CLS' tokens of the last four layers of the model to create a 3072-dim vector to represent for the text sentence. We also fine-tuned this pre-trained model while training to adapt it to this specific domain.</p><p>Joint Embeddings Learning: Given an image and its corresponding caption, we extract their feature vectors by the models mentioned above. Then we project these vectors into a common space by feeding them into Image or Text Feature Encoder, respectively. These encoders are just neural networks with one hidden layer. After transformation, we achieve same dimension vectors for both image and text caption: φ(I) and φ(C).</p><p>We adopt the Margin Ranking Loss from <ref type="bibr" coords="7,340.25,328.28,14.61,8.74" target="#b13">[14]</ref>, which penalizes the model according to negative samples.</p><formula xml:id="formula_2" coords="7,174.98,360.11,265.40,32.80">L(φ(I), φ(C)) = max(0, α + S(φ(I), φ(C) -) -S(φ(I), φ(C))) +max(0, α + S(φ(I) -, φ(C)) -S(φ(I), φ(C)))</formula><p>where α is non-negative margin constant. φ(I) -and φ(C) -are hardest negatives in the current mini-batch for φ(C) and φ(I), respectively. S is the similarity function.</p><p>Training Process: We train our model on MS COCO dataset <ref type="bibr" coords="7,414.08,456.02,9.96,8.74" target="#b3">[4]</ref>. Each image in the training set has five captions, so we can create five training samples with one image. The model was implemented using PyTorch framework and trained on NVIDIA Tesla P100 GPU provided by Google Colab. We also apply some tricks in the training process to improve the result: use different learning rates for RoBERTa and other components of the model, use Adam optimizer and linear learning rate scheduler, freeze RoBERTa model in the first epoch to warm up other components. Finally, we evaluate our model on the MS COCO 5k test set and achieve Recall@10 of 0.732.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Query similar images using ResNet152 features</head><p>Many experiments have shown that feature in the layer before the classification layer of a Convolutional Neural Network can be efficient for representing an image. Adopting that idea, we feed all lifelog images into a ResNet152 model pre-trained on ImageNet to achieve a 2048-dimension vector for each image and then index that the feature vector by image name to retrieve later on easily.</p><p>At the online stage, given an image, we can find the most similar images in terms of semantic meaning by just comparing the cosine distance between features of a reference image and all images in the lifelog dataset. We only allow users to input image in the lifelog dataset, not an arbitrary image so we can easily get the reference feature by searching in the feature dataset, not worry about loading ResNet152 model to memory and computation overhead at runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Query by metadata</head><p>The organizers provide us various information collected by sensors for each moment in the lifelog dataset: date and time, GPS coordinates, semantic name for locations, elevation, speed, heart rate, etc. In Lifelog Moment Retrieval task, we figure out that time and location are essentially useful for many types of queries like "Find the moments when lifelogger was eating seafood in a restaurant in the evening time". To make use of such information, we integrate some features to our system:</p><p>-Get all images in a specific time range of a day, e.g., from 5:30 PM to 9:00 PM. -Get all images taken in a specific location, .e.g, Home, Dublin City University (DCU). -Get all images taken in a time interval before going to a location, e.g., 40 minutes before going Home. -View timeline: using time metadata to traverse back and forth in time from an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">User interface</head><p>After trying out our model with many queries, we find out that lengthy sentences can cause the model to pay attention to too much information and lead to worse performance. To tackle this problem, we build a user interface that allows users to split a query into multiple steps, each step responsible for a small chunk of information of that query. Result of one query can contains a long list of images so we use pagination to prevent users from being overwhelmed with hundreds of images. Moreover, when using pagination, the browser can render a small amount of images at a time instead of loading all at once, thus reducing the waiting time for better user experience.</p><p>We also support users to choose desired images and output submission file for each query. Furthermore, the result page has drag-and-drop feature to rearrange the order of images and remove button to get rid of unwanted images. Our team use the name "HCMUS" to participate in ImageCLEF Lifelog 2020 -Lifelog Moment Retrieval subtask. Figure <ref type="figure" coords="10,330.61,488.75,4.98,8.74">6</ref> show that our team achieve the highest result in term of F1-score at 10 compared to others .</p><p>Table <ref type="table" coords="10,177.11,512.66,4.98,8.74" target="#tab_0">1</ref> shows our detailed result of our best run with each test topic which contains Precision at 10 (P@10), Cluster Recall at 10 (CR@10) and F1 score at 10 (F1@10). Almost all the test topics have high CR@10. This means our system manages to find all the moments that are relevant to the query. Our system is able to do this because of the capability to split a query into multiple steps and handle them one by one. Moreover, thanks to the flexibility of language, we can express a text query in many different ways. For example, we can change the query "He is repairing his car with a wrench in his hand" to passive voice "His car is being repaired with a wrench" or using synonym "He is using a spanner to repair his car". Like mentioned before, we can also split this query into multiple steps: "He is repairing his car" and continue querying on the returned results "He is holding a wrench". This can open a huge potential for our system and help it reduce the risk of handling too long sentence which can hurt the performance. In this section, we show our system's results for some test topics and the process to achieve that results.</p><p>TOPIC "MEDICINE CABINET" Description: Find the moment when u1 was looking inside the medicine cabinet in the bathroom at home.</p><p>Narrative: To be considered relevant, u1 must be at home, looking inside the opening medicine cabinet beside a mirror in the bathroom. The moments that u1 was looking inside the medicine cabinet in other places (not at home and not in the bathroom) or u1 was looking at the closed medicine cabinet are not considered to be relevant.</p><p>Our solution: We can find relevant images for this topic by inputting one of these sentences "looking inside the medicine cabinet in the bathroom" or "looking for medicine in the bathroom". Moreover, we observe that images with "opened medicine cabinet" are very similar to each other, so we use "Find similar images by ResNet152" feature to find remaining images.  Although we can not find all relevant moments for this topic, we manage to find some "hard" images. For example, in the fourth image in figure <ref type="figure" coords="12,455.08,412.52,3.87,8.74" target="#fig_3">8</ref>, the cabinet is on the edge of the image which makes it really hard to be detected.</p><p>TOPIC "SOCIALIZING" Description: Find the moments when u1 was talking to a lady in a red top, standing directly in front of a poster hanging on a wall.</p><p>Narrative: To be relevant, the u1 must be talking with a woman in red, who was standing right in front of a scientific research poster. Our solution: The ground truth of this topic contains two images. The first one we can easily retrieve with sentence "a woman in red top standing in front of a poster". We try dividing the query into two steps: first, get 1000 images with sentence "a woman in red" then continue to filter with sentence "poster on the wall" and search through the result to find the second image successfully.</p><p>TOPIC "BUS TO WORK -BUS TO HOME" Description: Find the moment when u1 was getting a bus to his office at Dublin City University or was going home by bus.</p><p>Narrative: To be relevant, u1 was on the bus, and the destination is his home or his workplace. The moments that u1 was waiting at the bus stop or u1 was traveling on any other public transportations, or the destination is not his home/workplace are not considered relevant.</p><p>Our solution: We use the "Query by metadata" feature to filter all images which had been taken up to 40 minutes before the lifelogger arrived at Work, Dublin City University or Home. Then, we continue filtering on these images with sentence "sitting on bus" to get relevant images. Finally, we use the "View timeline" feature to choose the best suitable candidates for submission.</p><p>Figure <ref type="figure" coords="13,182.26,341.44,4.98,8.74">5</ref> shows the results when querying "sitting on bus" on the images which had been shot up to 40 minutes before the lifelogger got to Work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel lifelog retrieval system which mainly focuses on free text query relied on Self-Attention based Joint Embedding Model. We also integrate two more components: query by ResNet152 and query by metadata to make the system more robust and reliable. We create a web application with a user-friendly user interface to help users interact with these models and visualize lifelog data. We use this system to participate in ImageCLEF Lifelog 2020 LMRT task and achieve the first rank with F1@10 at 0.811.</p><p>Although our system performs well on this task, it still has some typical drawbacks, as in many deep learning models: lack of explanation and reliability for the result. Therefore, the model can output irrelevant images in some cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,321.89,345.83,8.74;5,134.77,333.84,345.83,8.74;5,134.77,345.80,345.82,8.74;5,134.77,357.75,122.09,8.74;5,134.77,115.83,345.82,194.52"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Overall architecture of SAJEM. Image branch (top) detects regions in the image and then learn the interactions between these regions using Self-Attention Module to represent for that image. Text branch (bottom) uses RoBERTa model to encode the text sentence.</figDesc><graphic coords="5,134.77,115.83,345.82,194.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,167.83,417.92,279.71,8.74;9,134.77,464.09,345.81,163.01"><head>Fig. 4 :Fig. 5 :Fig. 6 :</head><label>456</label><figDesc>Fig. 4: User interface for each component of our retrieval system</figDesc><graphic coords="9,134.77,464.09,345.81,163.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,134.77,628.62,345.83,8.74;11,134.77,640.58,345.82,8.74;11,134.77,652.53,70.96,8.74;11,134.77,551.94,345.82,65.15"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Top 4 images when querying with text sentence "looking inside the medicine cabinet in the bathroom". The first and the fourth image can be considered relevant.</figDesc><graphic coords="11,134.77,551.94,345.82,65.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,215.21,363.13,184.94,8.74;12,160.70,115.83,293.95,235.77"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Result for topic "Medicine cabinet"</figDesc><graphic coords="12,160.70,115.83,293.95,235.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="12,229.04,652.53,157.28,8.74;12,160.70,534.41,293.93,106.59"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Result for topic "Socializing"</figDesc><graphic coords="12,160.70,534.41,293.93,106.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="13,190.51,652.53,234.34,8.74;13,152.06,391.80,311.23,249.20"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Result for topic "Bus to work -Bus to home"</figDesc><graphic coords="13,152.06,391.80,311.23,249.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,134.77,115.83,345.82,194.52"><head></head><label></label><figDesc></figDesc><graphic coords="4,134.77,115.83,345.82,194.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,134.77,115.83,345.82,194.52"><head></head><label></label><figDesc></figDesc><graphic coords="6,134.77,115.83,345.82,194.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="10,134.77,200.59,345.81,221.74"><head></head><label></label><figDesc></figDesc><graphic coords="10,134.77,200.59,345.81,221.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,134.77,117.75,345.83,203.32"><head>Table 1 :</head><label>1</label><figDesc>Detailed result of our best run with each test topic in ImageCLEF 2020 LMRT subtask 4.2 Experiment with LMRT test topics</figDesc><table coords="11,136.56,117.75,25.36,7.89"><row><cell>Topic</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="5,144.73,656.80,213.10,7.86"><p>https://github.com/airsplay/py-bottom-up-attention</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="7,144.73,656.80,184.28,7.86"><p>https://github.com/huggingface/transformers</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This research is supported by <rs type="funder">Vingroup Innovation Foundation (VINIF)</rs> in project code <rs type="grantNumber">VINIF.2019.DA19</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VautdWc">
					<idno type="grant-number">VINIF.2019.DA19</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,142.96,363.37,337.63,7.86;14,151.52,374.33,295.99,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="14,312.34,363.37,168.26,7.86;14,151.52,374.33,196.89,7.86">Social relation trait discovery from visual lifelog data with facial multi-attribute framework</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-01">01 2018</date>
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,384.80,337.63,7.86;14,151.52,395.76,329.07,7.86;14,151.52,406.71,329.07,7.86;14,151.52,417.67,329.07,7.86;14,151.52,428.63,329.07,7.86;14,151.52,439.59,329.07,7.86;14,151.52,450.55,329.07,7.86;14,151.52,461.51,329.07,7.86;14,151.52,472.47,329.07,7.86;14,151.52,483.43,329.07,7.86;14,151.52,494.39,45.56,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,248.55,439.59,232.04,7.86;14,151.52,450.55,214.57,7.86">Overview of the ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V.-T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,387.89,450.55,92.70,7.86;14,151.52,461.51,189.62,7.86;14,406.56,461.51,74.03,7.86;14,151.52,472.47,261.47,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="14,217.37,483.43,172.68,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF<address><addrLine>, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-09-22">2020. September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction. Thessaloniki</note>
</biblStruct>

<biblStruct coords="14,142.96,504.85,337.63,7.86;14,151.52,515.81,329.07,7.86;14,151.52,526.77,329.07,7.86;14,151.52,537.73,329.07,7.86;14,151.52,548.69,218.76,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,347.33,515.81,133.27,7.86;14,151.52,526.77,260.68,7.86">Overview of ImageCLEF Lifelog 2020:Lifelog Moment Retrieval and Sport Performance Lifelog</title>
		<author>
			<persName coords=""><forename type="first">V.-T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="14,437.16,526.77,43.43,7.86;14,151.52,537.73,191.55,7.86">CLEF2020 Working Notes, CEUR Workshop Proceedings</title>
		<title level="s" coord="14,450.90,537.73,23.75,7.86">CEUR</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,559.15,337.63,7.86;14,151.52,570.11,329.07,7.86;14,151.52,581.07,241.46,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,236.68,570.11,181.96,7.86">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,443.42,570.11,37.18,7.86;14,151.52,581.07,120.45,7.86">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,591.54,337.63,7.86;14,151.52,602.50,329.07,7.86;14,151.52,613.46,329.07,7.86;14,151.52,624.41,241.15,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,394.27,602.50,86.32,7.86;14,151.52,613.46,238.15,7.86">Smart lifelog retrieval system with habit-based concepts and moment visualization</title>
		<author>
			<persName coords=""><forename type="first">N.-K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-D</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q.-A</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V.-K</forename><surname>Vo-Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V.-T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,409.83,613.46,70.76,7.86;14,151.52,624.41,177.60,7.86">Proceedings of the ACM Workshop on Lifelog Search Challenge</title>
		<meeting>the ACM Workshop on Lifelog Search Challenge</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,634.88,337.64,7.86;14,151.52,645.84,329.07,7.86;14,151.52,656.80,311.76,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,384.15,634.88,96.45,7.86;14,151.52,645.84,290.79,7.86">An interactive approach to integrating external textual knowledge for multimodal lifelog retrieval</title>
		<author>
			<persName coords=""><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,462.94,645.84,17.66,7.86;14,151.52,656.80,238.99,7.86">Proceedings of the ACM Workshop on Lifelog Search Challenge</title>
		<meeting>the ACM Workshop on Lifelog Search Challenge</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="41" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,119.67,337.64,7.86;15,151.52,130.63,329.07,7.86;15,151.52,141.59,329.07,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,258.48,130.63,213.59,7.86">Lifeseeker: Interactive lifelog search engine at lsc 2019</title>
		<author>
			<persName coords=""><forename type="first">T.-K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V.-T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Redondo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,161.63,141.59,248.14,7.86">Proceedings of the ACM Workshop on Lifelog Search Challenge</title>
		<meeting>the ACM Workshop on Lifelog Search Challenge</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="37" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,152.55,337.63,7.86;15,151.52,163.51,329.07,7.86;15,151.52,174.47,169.27,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,435.38,152.55,45.21,7.86;15,151.52,163.51,147.07,7.86">A two-level lifelog search engine at the lsc 2019</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Nguyen Van Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,322.94,163.51,157.65,7.86;15,151.52,174.47,96.50,7.86">Proceedings of the ACM Workshop on Lifelog Search Challenge</title>
		<meeting>the ACM Workshop on Lifelog Search Challenge</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="19" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,185.43,337.64,7.86;15,151.52,196.39,329.07,7.86;15,151.52,207.34,169.27,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,448.46,185.43,32.13,7.86;15,151.52,196.39,155.67,7.86">Vielens, an interactive search engine for lsc2019</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Crane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,328.54,196.39,152.05,7.86;15,151.52,207.34,96.50,7.86">Proceedings of the ACM Workshop on Lifelog Search Challenge</title>
		<meeting>the ACM Workshop on Lifelog Search Challenge</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="33" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,216.04,337.97,10.13;15,151.52,229.26,329.07,7.86;15,151.52,240.22,35.84,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,356.65,218.30,123.93,7.86;15,151.52,229.26,16.72,7.86">Enhanced viret tool for lifelog data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lokoč</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Souček</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Čech</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kovalčík</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,190.49,229.26,253.11,7.86">Proceedings of the ACM Workshop on Lifelog Search Challenge</title>
		<meeting>the ACM Workshop on Lifelog Search Challenge</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="25" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,251.18,337.98,7.86;15,151.52,262.14,329.07,7.86;15,151.52,273.10,192.04,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,442.87,251.18,37.72,7.86;15,151.52,262.14,141.31,7.86">lifexplore at the lifelog search challenge 2018</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Münzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Leibetseder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kletz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Primus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Schoeffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,316.21,262.14,164.38,7.86;15,151.52,273.10,128.49,7.86">Proceedings of the 2018 ACM Workshop on The Lifelog Search Challenge</title>
		<meeting>the 2018 ACM Workshop on The Lifelog Search Challenge</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,284.06,337.98,7.86;15,151.52,295.02,329.07,7.86;15,151.52,305.98,225.63,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,443.82,284.06,36.77,7.86;15,151.52,295.02,204.24,7.86">Retrieval of structured and unstructured data with vitrivr</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rossetto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gasser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Amiri Parian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schuldt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,379.81,295.02,100.78,7.86;15,151.52,305.98,152.86,7.86">Proceedings of the ACM Workshop on Lifelog Search Challenge</title>
		<meeting>the ACM Workshop on Lifelog Search Challenge</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="27" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,316.93,337.97,7.86;15,151.52,327.89,329.07,7.86;15,151.52,338.85,169.27,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,305.95,316.93,174.64,7.86;15,151.52,327.89,109.84,7.86">Virtual reality lifelog explorer: lifelog search challenge at acm icmr 2018</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Duane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Huerst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,284.39,327.89,196.20,7.86;15,151.52,338.85,96.50,7.86">Proceedings of the 2018 ACM Workshop on The Lifelog Search Challenge</title>
		<meeting>the 2018 ACM Workshop on The Lifelog Search Challenge</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="20" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,349.81,337.98,7.86;15,151.52,360.77,329.07,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="15,372.91,349.81,107.68,7.86;15,151.52,360.77,163.35,7.86">Vse++: Improving visualsemantic embeddings with hard negatives</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.62,371.73,337.97,7.86;15,151.52,382.69,329.07,7.86;15,151.52,393.65,329.07,7.86;15,151.52,404.61,132.76,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,156.37,382.69,324.22,7.86;15,151.52,393.65,28.93,7.86">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,203.72,393.65,276.87,7.86;15,151.52,404.61,41.91,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,415.56,337.98,7.86;15,151.52,426.52,329.07,7.86;15,151.52,437.48,195.30,7.86" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="15,264.49,426.52,216.11,7.86;15,151.52,437.48,25.55,7.86">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.62,448.44,337.97,7.86;15,151.52,459.40,329.07,7.86;15,151.52,470.36,132.80,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="15,323.10,448.44,157.49,7.86;15,151.52,459.40,160.13,7.86">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,334.07,459.40,146.52,7.86;15,151.52,470.36,60.05,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,481.32,337.98,7.86;15,151.52,492.28,329.07,7.86;15,151.52,503.24,163.58,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="15,232.58,492.28,98.57,7.86">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,353.53,492.28,127.06,7.86;15,151.52,503.24,72.40,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,514.19,337.98,7.86;15,151.52,525.15,329.07,7.86;15,151.52,536.11,97.10,7.86" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="15,392.38,514.19,88.21,7.86;15,151.52,525.15,253.32,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.61,547.07,337.97,7.86;15,151.52,558.03,329.07,7.86;15,151.52,568.99,299.54,7.86" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="15,342.69,558.03,137.90,7.86;15,151.52,568.99,152.34,7.86">Huggingface&apos;s transformers: Stateof-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
