<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.16,115.96,321.04,12.62;1,184.06,133.89,247.25,12.62;1,185.21,151.82,244.94,12.62">Multi-Label and Cross-Modal Based Concept Detection in Biomedical Images by MORGAN CS at ImageCLEF2020</title>
				<funder ref="#_23a9VYb">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,205.48,189.49,66.60,8.74"><forename type="first">Oyebisi</forename><surname>Layode</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Morgan State University</orgName>
								<address>
									<settlement>Maryland</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,299.25,189.49,106.16,8.74"><forename type="first">Md</forename><forename type="middle">Mahmudur</forename><surname>Rahman</surname></persName>
							<email>md.rahman@morgan.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Morgan State University</orgName>
								<address>
									<settlement>Maryland</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.16,115.96,321.04,12.62;1,184.06,133.89,247.25,12.62;1,185.21,151.82,244.94,12.62">Multi-Label and Cross-Modal Based Concept Detection in Biomedical Images by MORGAN CS at ImageCLEF2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E7DB0F7D6B6A007242809EDE2C3D8DE1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical imaging</term>
					<term>Image annotation</term>
					<term>Deep learning</term>
					<term>Concept detection</term>
					<term>Multi-label classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automating the detection of concepts from medical images still remains a challenging task, which requires further research and exploration. Since the manual annotation of medical images poses a cumbersome and error prone task, the development of concept detection system would reduce the burdens of annotation, interpretation of medical images while providing a decision support system for medical practitioners. This paper describes the participation of the CS department at Morgan State University, Baltimore, USA (Morgan CS) in the medical Concept Detection task of the ImageCLEF2020 challenge. The task involves generating appropriate Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs) for corresponding radiology images. We approached the concept detection task as a multilabel classification problem by training a classifier on several deep features extracted from using pre-trained Convolutional Neural Networks (CNNs) and also by training a deep Autoencoder. We also explored a Recurrent Concept Sequence generator based on using a multimodal technique of combining text and image features for recurrent sequence prediction. Training and evaluation were performed on the dataset (training, validation, and test sets) provided by the CLEF organizer and we achieved our best F1 scores as 0.167 by using DenseNet based deep feature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Diagnostic analysis of medical images such as radiography or biopsy mostly involve interpretations based on observed visual characteristics. In essence, visual characteristics or features from images can be mapped to its corresponding semantic annotations. Neural networks over the last two decades have been successfully modeled to learn such mappings from data <ref type="bibr" coords="2,370.14,130.95,10.52,8.74" target="#b0">[1]</ref> and consequently this paper involves the annotation of medical images to generate condensed textual descriptions in the form of UMLS (Unified Medical Language System) CUIs (Concept Unique Identifiers) <ref type="bibr" coords="2,263.75,166.81,10.52,8.74" target="#b1">[2]</ref> using the dataset under ImageCLEFmed 2020 concept detection task <ref type="bibr" coords="2,238.31,178.77,9.96,8.74" target="#b2">[3]</ref>, which is a subset of a larger Radiology Objects in COntext (ROCO) dataset <ref type="bibr" coords="2,249.01,190.72,9.96,8.74" target="#b3">[4]</ref>. The main objective of this challenge involves automatically identifying the presence of concepts (CUIs) in a large corpus of medical images based on the visual image features. The concept detection task began in 2017 under the ImageCLEF challenge <ref type="bibr" coords="2,302.18,226.59,10.52,8.74">[5]</ref> and the participants were tasked with developing methods for predicting captions and detecting the multilabel concepts over a range of medical and non-medical images in a corpus. For example, our previous participation <ref type="bibr" coords="2,254.79,262.46,10.52,8.74" target="#b5">[6]</ref> in the ImageCLEFmed 2018 challenge involved the use of LSTM architectures in creating models that approached the concept detection task by developing a language model that predicts the probability of the next word (concept) occurring in a text sequence from the features of an image input and the words (concept) already predicted. This year, the task was limited strictly to concept detection in radiology images <ref type="bibr" coords="2,382.63,322.23,9.96,8.74" target="#b2">[3]</ref>. Evaluation criteria for the results obtained is given as the F1 score between the predicted concepts and ground truth concept labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Dataset</head><p>The dataset contains 64,753 radiology images from different modality classes as the training set, 15,970 radiology images as the validation and 3,534 radiology images from the same modality classes as the test set <ref type="bibr" coords="2,363.01,451.51,9.96,8.74" target="#b2">[3]</ref>. The training images are annotated with 3047 unique UMLS concepts serving as the image captions. The maximum length of the concept annotation is 140 and the minimum annotation is 1. The frequency distribution of the 3047 UMLS concepts across the training images is represented in the Table <ref type="table" coords="2,286.63,499.33,3.87,8.74" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We approached the concept detection task by comparing elementary CUI multilabel classification and a recurrent CUI sequence generation using extracted features from varying deep learning architectures. The multilabel classification involves feeding the outputs from a feature extraction network into a fully connected network to obtain a sigmoid activation output representing the CUI label predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Extraction</head><p>Feature extraction is a critical component of medical image analysis. The descriptiveness and discriminative power of features extracted from medical images are critical to achieve good classification and retrieval performances. Instead of using any hand-crafted features, transfer learning techniques can be used to extract features of images from a relatively small dataset using pre-trained Convolutional Neural Network (CNN) models <ref type="bibr" coords="3,273.79,310.56,9.96,8.74" target="#b6">[7]</ref>.</p><p>Visual Feature Extraction To perform deep feature extraction, we chose Densenet169 <ref type="bibr" coords="3,192.92,351.83,10.52,8.74" target="#b8">[9]</ref> and ResNet50 <ref type="bibr" coords="3,271.29,351.83,10.52,8.74" target="#b7">[8]</ref> as our pre-trained CNN models. These models have been trained on the ImageNet <ref type="bibr" coords="3,303.10,363.79,15.50,8.74" target="#b9">[10]</ref> dataset consisting of 1000 categories. The Densenet architecture consists of dense blocks of convolution layers -with consecutive operations of batch normalization (BN) <ref type="bibr" coords="3,357.71,387.70,18.02,8.74" target="#b13">[14]</ref>, followed by a rectified linear unit (ReLU) <ref type="bibr" coords="3,219.31,399.65,14.61,8.74" target="#b14">[15]</ref>, which provides direct connections from any layer in the block to all subsequent block layers <ref type="bibr" coords="3,289.56,411.61,9.96,8.74" target="#b7">[8]</ref>. ResNet, short for Residual Networks is a classic neural network, which is implemented with double -or triple -layer skips that combine features within this residual block of layers and contain nonlinearities (ReLU) and batch normalization in between <ref type="bibr" coords="3,352.26,447.47,9.96,8.74" target="#b7">[8]</ref>. We used the Densenet169 and ResNet50 pre-trained models which is a 169 layered dense network and a 50 layered residual network respectively. Both models have been trained on 1.28 million images <ref type="bibr" coords="3,199.49,483.34,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="3,211.66,483.34,7.01,8.74" target="#b8">9]</ref>. For feature extraction, both models are modified to exclude the final 1000-D classification layer and the output before this classification layer is saved. To obtain our deep features, the input images are first reduced to the required input size of 224×224 and further preprocessed using the Keras <ref type="bibr" coords="3,445.49,519.20,15.50,8.74" target="#b15">[16]</ref> preprocess input function, which preprocesses the input into the format the model requires. Since the DenseNet model had been modified to exclude the final 1000-D classification output, a 4096-D feature vector is obtained as the output from the last Average Pooling layer. Also, a 2048-D feature vector was obtained by passing the 224 × 224 input images through the modified pre-trained ResNet50 model. The extracted features are utilized for transfer learning with multilabel and recurrent CUI sequence classification models built on the Densenet features and a feature fusion of the Densenet and Resnet extracted features.</p><p>Feature Fusion Feature fusion methods have been demonstrated to be effective for many computer vision-based applications <ref type="bibr" coords="3,371.77,656.12,14.61,8.74" target="#b10">[11]</ref>. Combining features learned from various architectures creates an expanded feature learning space. We combined the features obtained from the pretrained DenseNet169 and the ResNet50 models by computing the partial least square canonical correlation analysis (PLS-CCA) <ref type="bibr" coords="4,226.62,154.86,15.50,8.74" target="#b16">[17]</ref> of both feature vectors, the canonical correlation computes a linear combination of the feature elements from both vectors such that the correlation between the vectors is maximized. Before computing the PLS-CCA, the ResNet50 based deep features are resized from the 2048-D vector to 4096-D output. Since the PLS-CCA required both vectors to be the same dimension the resized 4096-D vector is obtained by doubling each element from the 2048-D vector. The PLS-CCA is computed by combining the 4096-D DenseNet with the resized 4096-D Resnet based deep features. For feature vectors X (4096-D DenseNet) and Y (4096-D Resnet), first and second component vectors u and v are obtained such that the correlation corr(X, Y ) is maximized <ref type="bibr" coords="4,422.71,262.46,14.61,8.74" target="#b16">[17]</ref>:</p><formula xml:id="formula_0" coords="4,194.55,280.76,286.04,25.93">corr((X, u), (Y, v)) = u t • X t • Y • v √ u t • X t • X • u √ v t • Y t • Y • v (1)</formula><p>Where,  </p><formula xml:id="formula_1" coords="4,169.39,314.74,229.85,9.65">u = a 1 X 1 , a 2 X 2 • • • a n X n and v = b 1 Y 1 , b 2 Y 2 • • • b n Y n .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction based on Autoencoder</head><p>We also use an encoder-decoderbased framework (Fig. <ref type="figure" coords="4,236.34,620.25,4.43,8.74" target="#fig_1">1</ref>) to extract deep feature representations unique to the dataset. Autoencoders are a type of unsupervised neural network (i.e., no class labels or labeled data) that consist of an encoder and a decoder model <ref type="bibr" coords="4,434.18,644.16,14.61,8.74" target="#b11">[12]</ref>. When trained, the encoder takes input data and learns a latent-space representation of the data. This latent-space representation is a compressed representation of the data, allowing the model to represent it in far fewer parameters than the original data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Multilabel Classification Process Diagram</head><p>The encoder region contracts normalized pixel-wise data from input images into smaller dimensional feature maps using sequential layers of 2D convolutions, batch normalization and ReLU activation. The output from the convolutional blocks is passed to a fully connected layer that represents a 256-D feature space. The decoder expands the 256 fully connected output by applying transposed convolutions that up sample the features back to the original input size. Batch normalization and ReLU activation are also added at each step of the transposed convolution sequence and the encoder filter sizes mirror the decoder filter sizes. The 256-feature output from the encoder is given as the auto-encoded deep feature representation of our input image. The Autoencoder was trained using the Adam optimizer <ref type="bibr" coords="5,226.54,484.40,15.50,8.74" target="#b17">[18]</ref> and a mean squared error loss on the ROCO training dataset for 20 epochs with a batch size of 50. The initial Adam learning was also set to 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Feature Extraction</head><p>The deep text features are extracted from the image concepts by learning and mapping deep feature embeddings that represent the sequence of image concepts. The embeddings are learned during training when a fixed length of CUI sequence is passed to a neural embedding layer. Before passing the CUI sequences to the embedding layer, for each input image, the image concept sequence is tokenized using the Keras text preprocessing library. Since a fixed length of tokenized CUI sequence was required for the embedding layer the differences in CUI sequence length for different input images was accommodated by zero-padding the tokenized sequence up to the maximum CUI sequence length of 140. During training, the embedding layer uses a mask to ignore the padded values and its output is passed to a long short-term memory (LSTM) layer <ref type="bibr" coords="6,198.61,118.99,15.50,8.74" target="#b12">[13]</ref> with 256 memory units. The output from the text encoding block of the embedding and LSTM layer is a 256-D vector holding recurrent information that may be mapped back to the input concept sequence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-label Classification</head><p>The high volume of classification (CUI) labels (3047) and imbalance in the label frequency results in a huge bias towards the multi-label classification problem. The concepts set was split into groups based on the concept frequencies (Table <ref type="table" coords="6,134.77,524.47,4.43,8.74" target="#tab_0">1</ref>) and separate models were trained for classification within the concept set groups. The DenseNet feature, fused DenseNet-ResNet feature and the Autoencoded feature are passed to a stack of fully connected layers for the multilabel prediction in the different dataset groups as shown in Fig. <ref type="figure" coords="6,427.50,560.33,3.87,8.74">2</ref>. The fully connected network is composed of Dense layers stacked together to learn weights for a final sigmoid classification of the concept labels. The expected input for the fully connected classifier is the deep encoded feature vector corresponding to an image while the output is the binary multi-label classification of the concepts associated with the input image features.</p><p>The fully connected classifier was trained over 20 epochs with a learning rate of 1e -3 for an Adam optimizer. Since the concept set was split into groups and a different classifier trained for each concept group the overall CUI prediction for an input image involves the combination of the predictions from all concept group classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Concept Sequence Generation</head><p>The CUI sequence generator involves training a recurrent classifier on a fusion of the extracted image features and the embedded textual features. The text features are obtained by learning the embeddings at training time from the embedding layer stacked with a LSTM layer to give a 256-D text feature output. Since concatenating the image and text feature vectors would require equal feature vector lengths, to combine the 256-D text features and the 4096-D image features, the 4096-D image feature is down-sampled to 256 by passing it through a dense layer with 256 units to give a 256-D feature output. The 256-D image feature and 256-D text feature is passed to a concatenation layer to obtain a 256-D output that is passed to a final dense classification layer for the prediction of the next word in the CUI sequence. The CUI sequence prediction begins when a start signal is passed as the first element in the CUI sequence and the prediction ends when a stop signal is predicted by the classification model as shown in Fig. <ref type="figure" coords="7,134.77,333.85,3.87,8.74" target="#fig_2">3</ref>. The recurrent classifier was trained over 30 epochs with a learning rate of 1e -3 for an Adam optimizer and a batch size of 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussions</head><p>Using the provided test dataset, multiple runs were submitted based on the multi-label classification with DenseNet, DenseNet-ResNet and Auto Encoded features. The result from the recurrent concept sequence generator with DenseNet encoded features was also submitted and the F1 evaluations are represented in Table <ref type="table" coords="7,162.32,447.10,3.87,8.74" target="#tab_1">2</ref>. Our best result with a F1 score of 0.167 was obtained from the multilabel classification of DenseNet feature. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This article describes the strategies of the participation of the Morgan CS group for the concept detection tasks of ImageCLEF2020. We performed multi-label classification of CUIs in different deep feature spaces. We achieved comparable results considering the limited resources (computing and memory power) we had at the time of the submission. Since the ROCO data set is grouped into different modalities, we plan to perform separate multi-label classification for the different modalities in future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,149.71,326.69,330.88,9.65;4,134.77,338.65,345.82,9.65;4,134.77,350.60,345.82,8.74;4,134.77,362.56,330.14,8.74"><head></head><label></label><figDesc>Vectors u and v are obtained by computing the weight vectors [a 1 , a 2 • • • a n ] and [b 1 , b 2 • • • b n ]. We selected the first component 4096-D feature vector from the PLS-CCA computation. The result obtained is representative of the features from the maximized correlation of the DenseNet169 and ResNet50 features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,230.97,573.17,153.43,7.89;4,181.68,408.20,252.00,150.20"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Encoder-Decoder Architecture</figDesc><graphic coords="4,181.68,408.20,252.00,150.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,220.63,415.40,174.10,7.89;6,199.68,174.60,216.01,226.03"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Recurrent CUI Sequence Generator</figDesc><graphic coords="6,199.68,174.60,216.01,226.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,216.41,537.16,182.53,94.81"><head>Table 1 .</head><label>1</label><figDesc>Concept Frequency in Training set.</figDesc><table coords="2,254.71,557.95,105.94,74.02"><row><cell cols="2">Concept Group Frequency</cell></row><row><cell>&gt; 1000</cell><cell>22</cell></row><row><cell>500 -999</cell><cell>298</cell></row><row><cell>200 -499</cell><cell>735</cell></row><row><cell>100 -199</cell><cell>704</cell></row><row><cell>60 -100</cell><cell>704</cell></row><row><cell>&lt; 59</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,136.16,488.52,352.64,176.33"><head>Table 2 .</head><label>2</label><figDesc>F1 scores of submitted run (test set).</figDesc><table coords="7,136.16,509.82,352.64,155.03"><row><cell>Run</cell><cell>Method</cell><cell>F1 Score</cell></row><row><cell>MSU dense fcn</cell><cell>Densenet169 + multilabel classification</cell><cell>0.167</cell></row><row><cell cols="3">MSU dense resnet fcn 1 (Densenet169 + Resnet50) + multilabel classification 0.153</cell></row><row><cell>MSU dense feat</cell><cell>Densenet169 + multilabel classification</cell><cell>0.139</cell></row><row><cell>MSU dense fcn 2</cell><cell>Densenet169 + multilabel classification</cell><cell>0.094</cell></row><row><cell>MSU dense fcn 3</cell><cell>Densenet169 + multilabel classification</cell><cell>0.089</cell></row><row><cell>MSU autoenc fcn</cell><cell>Autoencoder + multilabel classification</cell><cell>0.063</cell></row><row><cell>MSU lstm dense fcn</cell><cell>Desnet169 + Recurrent concept generator</cell><cell>0.062</cell></row><row><cell cols="3">1. MSU dense fcn: This run utilized a multi-label classification model with</cell></row><row><cell cols="3">the training parameters (described in section 2.2) based on the features ex-</cell></row><row><cell cols="3">tracted from a pre-trained DenseNet169. The threshold for the prediction</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,138.97,202.19,341.62,163.73"><head>dense feat, MSU dense fcn 2, MSU dense fcn 3:</head><label></label><figDesc>These runs are variations of the MSU dense fcn run with different prediction score thresholds of 0.5, 0.3 and 0.25 respectively. 4. MSU autoenc fcn: The encoder-decoder model is utilized for this run to obtain the encoded features of the input images. The multi-label classification model (with parameters same as in runs 1,2 and 3) is trained on the Autoencoded features. The threshold for the prediction score from the classification model is also set to 0.4. 5. MSU lstm dense fcn: This run involved the recurrent generation of concepts by utilizing image features extracted from DenseNet169 combined with embedded concept sequences as described in 2.3. The obtained results clearly show the concept prediction challenge as more of a classification problem than a sequence generation task since all multi-label classification approaches performed better.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work is supported by an <rs type="funder">NSF</rs> grant (Award <rs type="grantNumber">ID 1601044</rs>), <rs type="grantName">HBCU-UP Research Initiation Award</rs> (RIA).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_23a9VYb">
					<idno type="grant-number">ID 1601044</idno>
					<orgName type="grant-name">HBCU-UP Research Initiation Award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,602.23,342.24,7.86;8,146.91,613.19,333.69,7.86;8,146.91,624.15,333.69,7.86;8,146.91,635.11,117.24,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,361.24,602.23,119.35,7.86;8,146.91,613.19,88.30,7.86">Show and tell: A neural image caption generator</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298935</idno>
		<ptr target="https://doi.org/DOI:10.1109/CVPR.2015.7298935" />
	</analytic>
	<monogr>
		<title level="m" coord="8,266.54,613.19,214.06,7.86;8,146.91,624.15,83.93,7.86">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,645.84,342.25,7.86;8,146.91,656.77,279.91,7.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,224.35,645.84,256.25,7.86;8,146.91,656.80,91.63,7.86">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,245.75,656.80,74.98,7.86">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,119.67,342.24,7.86;9,146.91,130.63,333.68,7.86;9,146.91,141.59,248.85,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,432.01,119.67,48.58,7.86;9,146.91,130.63,329.27,7.86">Overview of the ImageCLEFmed 2020 Concept Prediction Task: Medical Image Understanding</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,146.91,141.59,169.55,7.86">CEUR Workshop Proceedings (CEUR-WS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,151.57,342.24,7.86;9,146.91,162.53,333.68,7.86;9,146.91,173.48,333.68,7.86;9,146.91,184.44,333.68,7.86;9,146.91,195.38,333.69,7.89;9,146.91,206.36,107.00,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,420.73,151.57,59.86,7.86;9,146.91,162.53,227.35,7.86">Radiology Objects in COntext (ROCO): A Multimodal Image Dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01364-6</idno>
		<ptr target="https://doi.org/doi:10.1007/978-3-030-01364-6" />
	</analytic>
	<monogr>
		<title level="m" coord="9,381.55,162.53,99.04,7.86;9,146.91,173.48,333.68,7.86;9,146.91,184.44,142.60,7.86">Proceedings of the MIC-CAI Workshop on Large-scale Annotation of Biomedical data and Expert Label Synthesis (MICCAI LABELS 2018)</title>
		<title level="s" coord="9,450.24,184.44,30.36,7.86;9,146.91,195.40,136.51,7.86">Lecture Notes in Computer Science (LNCS</title>
		<meeting>the MIC-CAI Workshop on Large-scale Annotation of Biomedical data and Expert Label Synthesis (MICCAI LABELS 2018)<address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-16">September 16, 2018. 2018</date>
			<biblScope unit="volume">11043</biblScope>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,216.34,342.24,7.86;9,146.91,227.30,333.68,7.86;9,146.91,238.25,333.69,7.86;9,146.91,249.21,333.68,7.86;9,146.91,260.17,333.68,7.86;9,146.91,271.13,333.68,8.29;9,146.91,282.09,333.68,7.86;9,146.91,293.05,333.68,7.86;9,146.91,304.01,333.68,7.86;9,146.91,314.97,333.69,7.86;9,146.91,325.93,43.00,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,284.43,271.13,196.16,7.86;9,146.91,282.09,333.68,7.86;9,146.91,293.05,248.97,7.86">Overview of the ImageCLEF 2020: Multimedia Retrieval in Lifelogging, Medical, Nature, and Internet Applications In: Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Daniel S , Tefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gabriel Constantin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,403.93,293.05,76.66,7.86;9,146.91,304.01,274.48,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="9,179.65,314.97,168.35,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25. 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,335.90,342.24,7.86;9,146.91,346.86,333.69,7.86;9,146.91,357.82,333.68,7.86;9,146.91,368.75,214.78,7.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,201.07,335.90,279.52,7.86;9,146.91,346.86,154.38,7.86">A cross modal deep learning based approach for caption prediction and concept detection by CS Morgan State</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,308.15,346.86,172.44,7.86;9,146.91,357.82,134.06,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,453.98,357.82,26.62,7.86;9,146.91,368.78,89.74,7.86">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2125. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,378.75,342.24,7.86;9,146.91,389.69,204.06,7.89" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,268.65,378.75,211.94,7.86;9,146.91,389.71,73.50,7.86">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR, abs 1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,399.69,342.25,7.86;9,146.91,410.65,333.68,7.86;9,146.91,421.60,310.12,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,294.66,399.69,181.75,7.86">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/doi:10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m" coord="9,168.30,410.65,290.13,7.86">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,431.58,342.25,7.86;9,146.91,442.54,333.69,7.86;9,146.91,453.50,333.69,7.86;9,146.91,464.46,101.37,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,404.61,431.58,75.99,7.86;9,146.91,442.54,96.63,7.86">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
		<ptr target="https://doi.org/doi:10.1109/CVPR.2017.243" />
	</analytic>
	<monogr>
		<title level="m" coord="9,276.42,442.54,204.19,7.86;9,146.91,453.50,100.08,7.86">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,474.43,337.98,7.86;9,146.91,485.39,333.68,7.86;9,146.91,496.35,333.69,7.86;9,146.91,507.31,182.08,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,429.56,474.43,51.03,7.86;9,146.91,485.39,176.08,7.86">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2009.5206848" />
	</analytic>
	<monogr>
		<title level="m" coord="9,360.47,485.39,120.12,7.86;9,146.91,496.35,171.39,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,517.28,337.98,7.86;9,146.91,528.24,333.68,7.86;9,146.91,539.20,333.68,7.86;9,146.91,550.16,205.38,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,343.66,517.28,136.93,7.86;9,146.91,528.24,169.55,7.86">Fusion of transfer learning features and its application in image classification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Akilan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">M J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Safaei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CCECE.2017.7946733</idno>
		<ptr target="https://doi.org/doi:10.1109/CCECE.2017.7946733" />
	</analytic>
	<monogr>
		<title level="m" coord="9,346.20,528.24,134.40,7.86;9,146.91,539.20,204.06,7.86">IEEE 30th Canadian Conference on Electrical and Computer Engineering (CCECE)</title>
		<meeting><address><addrLine>Windsor, ON</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,560.14,337.98,7.86;9,146.91,571.09,333.69,7.86;9,146.91,582.03,333.67,7.89;9,146.91,593.01,25.60,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,418.04,560.14,62.55,7.86;9,146.91,571.09,333.69,7.86;9,146.91,582.05,73.30,7.86">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,226.96,582.05,172.60,7.86">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,602.99,337.98,7.86;9,146.91,613.92,303.98,7.89" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,292.00,602.99,96.55,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/DOI:10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j" coord="9,395.67,602.99,80.27,7.86">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,623.92,337.99,7.86;9,146.91,634.88,333.68,7.86;9,146.91,645.81,333.68,7.89;9,146.91,656.80,68.09,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,251.76,623.92,228.84,7.86;9,146.91,634.88,168.03,7.86">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,322.61,634.88,157.99,7.86;9,146.91,645.84,296.49,7.86">ICML&apos;15: Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,119.67,337.98,7.86;10,146.91,130.63,333.68,7.86;10,146.91,141.57,333.69,7.89;10,146.91,152.55,136.52,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,181.85,130.63,298.75,7.86;10,146.91,141.59,93.28,7.86">Digital selection and analogue amplification coexist in a cortexinspired silicon circuit</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">H R</forename><surname>Hahnloser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sarpeshkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
		<idno type="DOI">10.1038/35016072</idno>
		<ptr target="https://doi.org/10.1038/35016072" />
	</analytic>
	<monogr>
		<title level="j" coord="10,249.55,141.59,26.13,7.86">Nature</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="page" from="947" to="951" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,163.51,337.97,8.12;10,146.91,174.47,46.20,7.86" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<title level="m" coord="10,199.73,163.51,19.27,7.86">keras</title>
		<imprint>
			<date type="published" when="2020-07-29">29 Jul 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">GitHub</note>
</biblStruct>

<biblStruct coords="10,142.62,185.43,337.97,7.86;10,146.91,196.39,333.69,7.86;10,146.91,207.34,140.32,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,208.07,185.43,157.42,7.86">Relations Between Two Sets of Variates</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,382.38,185.43,98.21,7.86;10,146.91,196.39,142.93,7.86">Breakthroughs in Statistics: Methodology and Distribution</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Kotz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="162" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,218.30,337.98,7.86;10,146.91,229.26,333.69,7.86;10,146.91,240.22,60.92,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,279.25,218.30,197.05,7.86">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,251.50,229.26,229.10,7.86;10,146.91,240.22,27.64,7.86">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
