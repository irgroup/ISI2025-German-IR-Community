<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.82,114.32,317.72,14.35;1,134.77,132.25,345.83,14.35;1,159.74,150.18,295.89,14.35">HTML Atomic UI Elements Extraction from Hand-Drawn Website Images using Mask-RCNN and novel Multi-Pass Inference Technique</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,206.49,188.45,66.17,9.96"><forename type="first">Prasang</forename><surname>Gupta</surname></persName>
							<email>prasang.gupta@pwc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">PricewaterhouseCoopers US Advisory</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.35,188.45,113.51,9.96"><forename type="first">Swayambodha</forename><surname>Mohapatra</surname></persName>
							<email>swayambodha.mohapatra@pwc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">PricewaterhouseCoopers US Advisory</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.82,114.32,317.72,14.35;1,134.77,132.25,345.83,14.35;1,159.74,150.18,295.89,14.35">HTML Atomic UI Elements Extraction from Hand-Drawn Website Images using Mask-RCNN and novel Multi-Pass Inference Technique</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">41BDAB4FF44869A285F882F87E96727F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>HTML</term>
					<term>UI</term>
					<term>Image Processing</term>
					<term>Deep Learning</term>
					<term>OpenCV</term>
					<term>Mask-RCNN</term>
					<term>Multi-Pass Inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Website UI Design is an integral part of the world, but it is not trivial as there are a huge array of challenges that need to be conquered. A quintessential step of a website design process is to sketch the UI wireframe on paper and translating it into code later on. In an attempt to automate this process, advanced AI algorithms are explored in this study. The final approach comprises of image processing, followed by UI feature identification and localisation using Mask-RCNN and ultimately a novel Multi-Pass inference technique to boost the viability of the model. On the test dataset, the method resulted in an mAP or Mean Average Precision (IoU &gt; 0.5) value of 64.12</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the world going increasingly global and starting to work virtually, websites are more important than ever to expand business and reach out to customers. However, website design requires a very specific set of skills. There are 2 major ways of building a website. The first method is using visual website building tools like Wix <ref type="bibr" coords="1,198.90,499.98,9.97,9.96" target="#b1">[3]</ref>, Constant Contact <ref type="bibr" coords="1,296.54,499.98,9.97,9.96" target="#b0">[1]</ref>, Squarespace [2], etc. and the second is building by programming using languages like HTML, PHP, CSS, JavaScript, etc. The downside of both of these approaches is that they have a very steep learning curve.</p><p>The ImageCLEF 2020 DrawnUI Task <ref type="bibr" coords="1,318.12,554.09,10.52,9.96" target="#b3">[5]</ref> from ImageCLEF 2020 <ref type="bibr" coords="1,436.02,554.09,15.50,9.96" target="#b8">[10]</ref> is formulated to reduce this dependency on the tools and flatten the learning curve by enabling people to create websites using hand-drawn pictures of website interfaces on whiteboard or a piece of paper. This would give a chance to people having no knowledge of the aforementioned tools and languages to create websites easily and quickly. The first step towards making this possible is to come up with a model that correctly identifies the type and the position of various atomic user interface (UI) elements in the wireframe drawing. This information can be leveraged to generate a website layout using various heuristics. The next step to this problem would be to convert this detected layout to code. In this study, we are focusing on the first part of the problem.</p><p>Diving into the details of the implementation, we will discuss about the Dataset used for training the model in Section 2 and will cover the Methodology used in Section 3. Further, we will discuss the Results in Section 4 and present the Conclusions and any scope for Future Work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Set</head><p>ImageCLEF 2020 DrawnUI task <ref type="bibr" coords="2,283.03,583.16,10.52,9.96" target="#b3">[5]</ref> was focused on extracting the atomic UI elements from a hand-drawn image of a website. The dataset provided as part of the challenge contained about 3000 hand-drawn images inspired from mobile application screenshots and actual web pages containing about 1000 different templates.</p><p>The dataset was divided into two parts, the development set which contained 2363 annotated images and a test set containing 587 images which were not     The images in the dataset were of varying sizes. All of them were RGB images in JPEG format. The annotations were provided separately in a CSV file format. The distribution of the width and the height of the images can be seen in Figure <ref type="figure" coords="3,134.77,612.72,3.88,9.96" target="#fig_0">1</ref>. Due to the varying sizes of the images, they need to be resized to a fixed size that will be covered later in the modelling section.</p><p>There were several challenges within the dataset. The first challenge was that there were several repeated images in the development set which would not  The second challenge was that there were some images with a very steep capture angle in the dataset as shown in Figure <ref type="figure" coords="4,345.69,630.98,3.88,9.96" target="#fig_2">3</ref>. This had to be taken care of as the apparent shapes of the UI elements would change drastically when the images are captured at an angle.   The plots <ref type="bibr" coords="5,214.73,559.13,10.52,9.96" target="#b7">[9]</ref> above show the distribution of various class labels in the dataset. The plot on the top shows the number of images out of the development dataset, in which that label was present. The plot on the bottom shows the total number of occurences of the label across the dataset. As we can see from both the plots, the dataset is very skewed towards certain classes.</p><p>The third problem pertained to the "image" class of the dataset. The image class was defined as a rectangle with both the diagonals drawn. However, there Fig. <ref type="figure" coords="6,166.34,281.76,7.75,9.96">6:</ref> The image above is the result of the subtract operation applied on Figure <ref type="figure" coords="6,182.46,293.71,3.88,9.96" target="#fig_1">2</ref>. It can be observed from the image that due to the shift in the capture angles, the subtract function has not worked as expected.</p><p>were several files containing multiple objects overlapping with an image class. There were 2 kinds of overlap, the object over the image, where the diagonal was "hidden" or behind the image where the wireframe of the image would run over these classes. This can be seen in Figure <ref type="figure" coords="6,335.31,377.43,3.88,9.96" target="#fig_3">4</ref>.</p><p>Apart from these challenges, the number of labels were also skewed in the dataset as some labels had plenty of representation, while some labels were present quite rarely. The distribution for the labels can be seen in Figure <ref type="figure" coords="6,461.68,419.10,3.88,9.96" target="#fig_5">5</ref>. It can be seen from the plot that the labels like "button", "paragraph" and "image" are very commonly present while other labels like "textarea", "stepperinput" and "rating" are very sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Pre-Processing</head><p>We have explored several pre-processing techniques to improve the viability of our model. The majority of techniques are based on modifications of the data using OpenCV <ref type="bibr" coords="6,201.03,553.60,10.52,9.96" target="#b2">[4]</ref> on C++. This was chosen to reduce the time taken to perform operations and transformations on the images. Some of the techniques used are described in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Removing duplicate images</head><p>By visual inspection of the dataset as mentioned in Data Set Section, we found that there were repeated images (as can be seen in Figure <ref type="figure" coords="6,426.40,642.94,4.43,9.96" target="#fig_1">2</ref>) and thus, these images would not contribute much to the training of the model. If the Fig. <ref type="figure" coords="7,164.12,281.76,7.75,9.96">7:</ref> The image above is the result of the superposition of two candidate images for similarity check. It can be seen that both of the them are exactly similar but a little shifted. To gauge this, the distance between the images can be calculated using the sum of squares of distances between the centre of the corresponding label bounding boxes of both the images. The red line in the image depicts the distance and the black dot is the center of the bounding box for the corresponding labels in both the images number is huge, then there may be a possibility that our model would start overfitting to these similar images. Hence, to quantify the number of images that are repeated in the dataset, we had to come up with some algorithm for detecting the same. We will be testing the algorithms on the same set of images as shown in Figure <ref type="figure" coords="7,220.26,438.08,3.88,9.96" target="#fig_1">2</ref>.</p><p>One of the methods commonly used for checking if the images are equal or not is the OpenCV subtract method. This method performs a pixel-by-pixel subtraction of the images and returs an image. If the returned image is completely black, then the starting images are same. We tried employing this method to our images, but the results were not good as can be seen in Figure <ref type="figure" coords="7,431.46,504.64,3.88,9.96">6</ref>. This can be owed to the fact that our similar images are not "exactly" same as there are some camera angle changes involved which change the orientation of the image and hence, a pixel-by-pixel subtraction did not yield the best results here.</p><p>The second approach used for finding duplicate images was an algorithm based on finding the smallest distance between two given images. The algorithm included making a list of size 21 (the total number of unique classes present in the dataset) and then populating it for all the images with the number of the classes of each type they have. This was iterated upon and all the image pairs having the same class vector were found. The cartesian distance between these selected pairs was calculated to verify if the images are actually similar or not and if the distance was found to be lesser than a threshold, it was classified as a repeated image pair. The way of calculating the distance between the two images can be seen in Figure <ref type="figure" coords="8,269.80,318.40,3.88,9.96">7</ref>. Employing this algorithm, it was found that there were only 1306 unique images in the development dataset out of the total 2363 images. Rest 1057 images were copies of the images already present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extracting individual elements from the image</head><p>As the underlying shape of each of the label is same and only the localisation of the labels vary across images, we tried extracting the individual elements from the image. These extractions can then later be used for training purposes. Also, these can be used for increasing the number of the classes whose frequency is less in the dataset. This would allow the model to learn the features of the lesser frequent label types as well.</p><p>The approach selected for this was using a DLIB <ref type="bibr" coords="8,368.94,478.45,15.50,9.96" target="#b9">[11]</ref> model to capture the general features of the class labels. The DLIB model was chosen as it could learn the basic structure of the class with very few learning data, as would be the case with the classes having very low representation. A sample DLIB model output for the image class can be seen in Figure <ref type="figure" coords="8,347.50,526.27,3.88,9.96" target="#fig_6">8</ref>. The DLIB models were able to identify most of the label classes, but were unable to segregate the image wherever there was overlapping present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Converting images to Grayscale</head><p>The images provided in the dataset were all 3 channel RGB images. 3 channels might be helpful in problems where the information carried by the colour is needed to be learned by the model and should be used as a feature. But, in our case, colour doesn't matter as we have to detect the features only on the basis of shape. Hence, to prevent throwing off the learning of the model by introducing  colour, all the images were converted to grayscale using OpenCV. A sample grayscale conversion can be seen in Figure <ref type="figure" coords="9,322.47,569.51,3.88,9.96" target="#fig_7">9</ref>.</p><p>Another factor that is helpful in grayscale images is that the number of channels are reduced to 1. Hence, the effective size of the image reduces which leads to a speed up in computation. To increase the visibility of the labels further, the grayscale images were later sharpened using OpenCV. This can be seen in Figure <ref type="figure" coords="9,166.20,654.89,8.49,9.96" target="#fig_8">10</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Converting images to Black &amp; White</head><p>There were several algorithms applied to convert the image from a grayscale image to a Black and White Image. This was carried out to further reduce the effect of the background elements on the model prediction, as grayscale also carries information regarding the shade of the image or background.</p><p>The first approach used to convert the grayscale images to black and white was simple binary thresholding. But, the limitation of the model was that every image had a different optimum threshold and there was no way to find it before-Fig. <ref type="figure" coords="11,154.23,281.76,8.49,9.96" target="#fig_11">13</ref>: The image on the left is the grayscale image while the one on the right is after applying erosion on the image and then applying Otsu's binarization algorithm Fig. <ref type="figure" coords="11,154.23,496.96,12.73,9.96" target="#fig_11">14:</ref> The image on the left is the grayscale image while the one on the right is after applying adaptive Gaussian thresholding on the image.</p><p>hand. Hence, there was a lot of loss of information by this conversion as can be seen in Figure <ref type="figure" coords="11,199.45,558.91,8.49,9.96" target="#fig_11">11</ref>.</p><p>The second approach used is Otsu's binarization algorithm <ref type="bibr" coords="11,414.56,579.55,9.97,9.96" target="#b4">[6]</ref>. The Otsu's algorithm finds a threshold for the image automatically based on its histogram distribution. The image generated using this is shown in Figure <ref type="figure" coords="11,423.16,603.46,8.49,9.96" target="#fig_11">12</ref>. Formally, Otsu's algorithm tries to find a threshold value (t) which minimizes the weighted within-class variance given by the following relation.</p><formula xml:id="formula_0" coords="11,241.08,654.05,239.52,12.69">σ 2 w (t) = q 1 (t)σ 2 1 (t) + q 2 (t)σ 2 2 (t)<label>(1)</label></formula><p>Fig. <ref type="figure" coords="12,158.81,281.76,8.49,9.96" target="#fig_11">15</ref>: The image on the left is the image generated after applying adaptive Gaussian thresholding while the one on the right is after running the C++ code to remove the short connected components from the image, reducing noise.</p><p>where</p><formula xml:id="formula_1" coords="12,221.28,372.37,259.31,80.00">q 1 (t) = t ∑ i=1 P (i) &amp; q 2 (t) = I ∑ i=t+1 P (i) (2) µ 1 (t) = t ∑ i=1 iP (i) q 1 (t) &amp; µ 2 (t) = I ∑ i=t+1 iP (i) q 2 (t)<label>(3)</label></formula><formula xml:id="formula_2" coords="12,175.93,471.72,304.66,30.32">σ 2 1 (t) = t ∑ i=1 [i -µ 1 (t)] 2 P (i) q 1 (t) &amp; σ 2 2 (t) = I ∑ i=t+1 [i -µ 2 (t)] 2 P (i) q 2 (t)<label>(4)</label></formula><p>As an extension of the Otsu's approach, the next approach first Eroded the image to sharpen all the edges and remove connected elements and then applied Otsu on top of it. The outcome for this can be seen in Figure <ref type="figure" coords="12,406.32,538.17,8.49,9.96" target="#fig_11">13</ref>.</p><p>The final approach used is an adaptive approach where a single threshold is not applied globally to the dataset. The threshold value in this case is a Gaussianweighted sum of the neighbourhood values minus a constant. The results using this were good, but contained a lot of noise as can be seen in Figure <ref type="figure" coords="12,425.63,596.21,8.49,9.96" target="#fig_11">14</ref>. However, this was by far the best conversion of grayscale to binary black and white. Hence, this was selected as the final model and the noise was tackled by finding and removing the small connected components of the image using C++. The final image can be seen in Figure <ref type="figure" coords="12,260.04,644.03,8.49,9.96" target="#fig_11">15</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Methods Implemented</head><p>The images were first all transformed into single channel black and white images and then resized into 1024*1024*1, as required by Mask RCNN Architecture <ref type="bibr" coords="13,467.30,559.25,9.97,9.96" target="#b5">[7]</ref>.</p><p>The general architecture of the Mask RCNN model can be found in Figure <ref type="figure" coords="13,467.85,571.21,8.49,9.96" target="#fig_10">16</ref>.</p><p>The dataset was split in the ratio 80:20 with the larger ratio corresponding to the training set and the smaller one corresponding to the validation set. Since there were a few classes that had a small set of images corresponding to them, care was taken to ensure that such images were present in the same ratio while splitting the dataset. The models were trained on a virtual Ubuntu server equipped with a 16GB NVIDIA Tesla V100 GPU Accelerator <ref type="bibr" coords="13,337.96,642.94,15.50,9.96" target="#b12">[14]</ref> hosted on PwC's proprietary cloud platform, Workbench. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 1 : Plain Mask RCNN Model</head><p>Since this challenge mainly involved detection of small website UI elements, Mask RCNN was chosen because it performs better than other models in object detection. Mask RCNN model generates bounding boxes and segmentation masks for each instance of an object in the image, and is based on Feature Pyramid Network (FPN) <ref type="bibr" coords="14,247.31,383.11,15.50,9.96" target="#b10">[12]</ref> and a ResNet-101 backbone <ref type="bibr" coords="14,390.84,383.11,9.97,9.96" target="#b6">[8]</ref>.</p><p>We implemented Transfer Learning by using a pre-trained Mask RCNN model trained on COCO Dataset <ref type="bibr" coords="14,289.19,412.36,14.62,9.96" target="#b11">[13]</ref>. Even though the images contained in the COCO Dataset are not very similar to our dataset, we used it to ensure that our model extracts the high-level features in all images. The 'heads' layer of the model was then trained for 200 epochs at a Learning Rate of 10 -3 . The convergence of the model can be seen in Figure <ref type="figure" coords="14,344.47,460.18,8.49,9.96" target="#fig_11">18</ref>.</p><p>The output from this model, obtained on an image from the test split of the dataset can be seen in Figure <ref type="figure" coords="14,263.01,489.43,8.49,9.96" target="#fig_11">17</ref>. The model performed really well in recognising all the major UI elements in the image. But, the problem was that it was not able to detect smaller UI elements in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 2 : Mask RCNN Model with novel Multi-Pass Inference Technique</head><p>After evaluation, even though the overall precision score of Run 1 was high, the overall recall score was not as high. This meant that the model was not able to detect the smaller UI elements on the image. To improve the previous run, we implemented a novel Multi-Pass Inference Technique.</p><p>The novel Multi-Pass Inference Technique involves getting the predictions on the input image and then filling the corresponding bounding box regions with the background colour (white in this case). The edited image is then passed For this particular run, the 'heads' layer of the model was trained for 100 epochs and the following Learning Scheduler was used to ensure the model converges quickly -Learning Rate of 10 -2 for the first 25 epochs, Learning Rate of 10 -3 for the next 25 epochs, Learning Rate of 10 -4 for the next 25 epochs, Learning Rate of 10 -5 for the last 25 epochs. The convergence of the model can be seen in Figure <ref type="figure" coords="16,213.01,560.05,8.49,9.96" target="#fig_1">20</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 3 : Modified Version of Run-2</head><p>This run was implemented to improve the results obtained from the previous run. The same Multi-Pass Inference technique was implemented with a slight modification to ensure that only the bounding boxes with the highest confidence scores after the second pass were added to the final results of each image. This was done to ensure that the stray elements detected after the white space replacement step are not added to the final results.</p><p>For this particular run, the 'heads' layer of the model was trained for 125 epochs and the following Learning Scheduler was used to ensure the model converges quickly -Learning Rate of 10 -2 for the first 25 epochs, Learning Rate of 5 * 10 -3 for the next 25 epochs, Learning Rate of 10 -3 for the next 25 epochs, Learning Rate of 2 * 10 -4 for the next 25 epochs, Learning Rate of 10 -4 for the last 25 epochs. The convergence of the model can be seen in Figure <ref type="figure" coords="17,432.30,654.89,8.49,9.96" target="#fig_11">21</ref>. The intermediate output and final output generated from this model, on an image from the test split of the dataset can be seen in Figure <ref type="figure" coords="18,406.29,266.68,9.96,9.96" target="#fig_13">22</ref> and 23. There was a visible improvement in recognising smaller UI elements on the image which was also reflected in better Mean Average Precision (mAP) scores as listed in the Table <ref type="table" coords="18,179.31,302.55,3.88,9.96" target="#tab_5">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The predictions on the test set images were collated in a csv file. For each image on the test set, the bounding boxes corresponding to each instance of a detected class and the confidence scores were submitted. The Mean Average Precision (mAP) scores obtained across the three runs can be found as listed in Table <ref type="table" coords="18,469.57,402.72,3.88,9.96" target="#tab_5">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>Throughout the challenge, we experimented with several processing techniques to get the data in the best shape to be trained. We selected Mask R-CNN as our baseline model as it is known to perform well on Object Detection problems and this problem statement was not much different. We also came up with a novel technique, Multi-Pass Inference, which improved the mAP score drastically, hence gaining us the 3rd spot on the leaderboard of the DrawnUI challenge.</p><p>Due to the lack of time, we could not tinker around much with the models as the training takes up a lot of time, being computationally expensive. In the future, we can explore other models as the baseline model which have better performance over Mask R-CNN. One such example of an improved model would be EfficientDet, which is known to perform much better, but is deadly slow to train. Also, there is a lot of scope in expanding the viability of the novel Multi-Pass Inference technique and study the affect of number of passes with performance. There is also scope for experimenting with attention mechanism to focus on those parts of the image which are actually important.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,139.78,236.80,335.79,9.96;3,140.66,248.76,334.03,9.96;3,138.40,260.71,338.56,9.96;3,134.77,284.50,155.62,155.62"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The plot on the left shows the distribution of the height of the images while the one on the right shows the distribution of the width of the images. Both of these histogram plots have been generated keeping the bin size at 10.</figDesc><graphic coords="3,134.77,284.50,155.62,155.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,142.86,450.42,329.62,9.96;3,141.12,462.38,333.10,9.96;3,141.70,474.33,331.96,9.96;3,224.48,486.29,166.40,9.96;3,324.97,284.50,155.62,155.62"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The image on the left and the right are two distinct images from the development dataset. However, taking a closer look at both of the images, it can be seen that both the images are exactly identical and differ only in the tint and a slight camera angle change.</figDesc><graphic coords="3,324.97,284.50,155.62,155.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,134.77,281.76,345.82,9.96;4,146.40,293.71,322.56,9.96;4,229.87,115.83,155.62,155.62"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The image above has been taken at a very steep angle. This converts the straight horizontal lines into diagonals and rectangles into parallelograms.</figDesc><graphic coords="4,229.87,115.83,155.62,155.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,134.77,489.46,345.82,9.96;4,137.90,501.41,339.57,9.96;4,140.85,513.37,333.66,9.96;4,153.11,525.32,309.13,9.96;4,134.77,323.53,155.62,155.62"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The images shown above represent the overlap of different classes on the "image" class. There were 2 types of overlap. The image on the left shows the "under the image" overlap, while the image on the right shows the "over the image" overlap. Both of these are taken from the development dataset.</figDesc><graphic coords="4,134.77,323.53,155.62,155.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,176.96,300.56,10.47,4.12;5,179.34,289.93,14.31,10.63;5,185.48,285.80,10.47,4.12;5,187.86,279.15,12.01,6.65;5,191.70,275.03,10.47,4.12;5,194.08,270.91,10.47,4.12;5,189.67,299.23,10.47,4.12;5,192.05,292.58,12.01,6.65;5,195.88,288.46,10.47,4.12;5,198.26,284.34,10.47,4.12;5,200.65,280.36,10.47,3.98;5,202.94,275.05,10.47,5.31;5,206.01,270.93,10.47,4.12;5,202.44,298.33,10.47,3.57;5,204.50,294.21,10.47,4.12;5,206.88,282.89,14.84,11.33;5,213.42,278.76,10.47,4.12;5,215.80,270.94,12.76,7.82;5,210.33,302.26,12.01,6.65;5,214.17,298.14,10.47,4.12;5,216.55,292.36,11.51,5.78;5,219.89,288.24,10.47,4.12;5,222.27,284.12,10.47,4.12;5,224.64,281.58,10.47,2.55"><head></head><label></label><figDesc>p a r a g r a p h d r o p d o w n c h e c k b o x r a d i o b u t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,140.10,559.13,335.15,9.96;5,159.51,571.09,296.33,9.96;5,136.84,583.04,341.68,9.96;5,134.77,595.00,345.82,9.96;5,144.41,606.95,326.54,9.96"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5:The plots<ref type="bibr" coords="5,214.73,559.13,10.52,9.96" target="#b7">[9]</ref> above show the distribution of various class labels in the dataset. The plot on the top shows the number of images out of the development dataset, in which that label was present. The plot on the bottom shows the total number of occurences of the label across the dataset. As we can see from both the plots, the dataset is very skewed towards certain classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,147.08,257.23,321.19,9.96;8,134.77,269.19,345.82,9.96;8,229.01,281.14,157.34,9.96;8,229.87,115.84,155.62,131.09"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: The image shows the output of the DLIB model trained on just 15 instances. In this very limited learning, it has identified the general structure of the image class with great accuracy.</figDesc><graphic coords="8,229.87,115.84,155.62,131.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="9,136.37,281.76,342.60,9.96;9,242.00,293.71,131.37,9.96;9,134.77,115.83,155.62,155.62"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: The image on the left is the original image while the one on the right is after the grayscale conversion.</figDesc><graphic coords="9,134.77,115.83,155.62,155.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="9,134.77,496.29,345.82,9.96;9,189.92,508.24,235.51,9.96;9,134.77,330.36,155.62,155.62"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: The image on the left is the converted grayscale image while the one on the right is a more refined sharpened grayscale image.</figDesc><graphic coords="9,134.77,330.36,155.62,155.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="10,135.55,281.76,344.26,9.96;10,202.34,293.71,210.69,9.96;10,134.77,115.83,155.62,155.62"><head>Fig. 11 :Fig. 12 :</head><label>1112</label><figDesc>Fig. 11: The image on the left is the grayscale image while the one on the right is the result of a simple thresholding conversion.</figDesc><graphic coords="10,134.77,115.83,155.62,155.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="13,144.75,237.39,325.85,9.96;13,271.47,249.34,72.41,9.96;13,203.93,115.84,207.50,111.25"><head>Fig. 16 :</head><label>16</label><figDesc>Fig. 16: General Architecture of the Mask RCNN Model. Reproduced from Mask-RCNN [7].</figDesc><graphic coords="13,203.93,115.84,207.50,111.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="14,136.34,264.47,342.66,9.96;14,262.47,276.43,90.41,9.96"><head>1 :</head><label>1</label><figDesc>Fig. 18: Graph showing the convergence of Training and Validation Loss of the model used in Run 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="15,137.72,438.37,339.91,9.96;15,137.97,450.33,339.41,9.96;15,148.54,462.28,318.28,9.96;15,140.45,474.24,334.46,9.96;15,143.51,486.19,328.35,9.96;15,141.59,498.15,332.17,9.96;15,140.34,510.10,334.68,9.96;15,192.92,522.06,229.52,9.96;15,229.87,272.45,155.62,155.62"><head>Fig. 19 : 3 :</head><label>193</label><figDesc>Fig. 19: The image on the top left is obtained after passing the image through the model once (Generally used single pass inference). The bounding boxes in this image were filled with white (except the classes where overlapping is happening) and then passed through the model again. The image on the top right is the output of the 2nd pass. Both of the above images are combined together on the basis of confidence scores and IoU overlaps to form the final prediction image on the bottom. The blue bounding boxes in the final image are from Pass 1 while the red boxes are from Pass 2.</figDesc><graphic coords="15,229.87,272.45,155.62,155.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="17,138.88,265.29,337.60,9.96;17,147.35,277.25,320.65,9.96;17,137.95,289.20,339.45,9.96;17,224.02,301.16,167.30,9.96;17,203.93,115.84,207.50,139.16"><head>Fig. 22 :</head><label>22</label><figDesc>Fig. 22: This image shows the intermediate output generated by Mask RCNN Model with Multi-Pass Inference Technique (Run 3) on one of the images belonging to the test split of the dataset after the first pass. A few smaller UI elements are missed out by the model.</figDesc><graphic coords="17,203.93,115.84,207.50,139.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,188.28,126.14,238.79,261.53"><head>Table 1 :</head><label>1</label><figDesc>A list of unique labels present in the data set.</figDesc><table coords="2,254.22,148.16,103.84,239.50"><row><cell>Index</cell><cell>Label</cell></row><row><cell>0</cell><cell>paragraph</cell></row><row><cell>1</cell><cell>dropdown</cell></row><row><cell>2</cell><cell>checkbox</cell></row><row><cell>3</cell><cell>radiobutton</cell></row><row><cell>4</cell><cell>rating</cell></row><row><cell>5</cell><cell>toggle</cell></row><row><cell>6</cell><cell>textarea</cell></row><row><cell>7</cell><cell>datepicker</cell></row><row><cell>8</cell><cell>stepperinput</cell></row><row><cell>9</cell><cell>slider</cell></row><row><cell>10</cell><cell>video</cell></row><row><cell>11</cell><cell>label</cell></row><row><cell>12</cell><cell>table</cell></row><row><cell>13</cell><cell>list</cell></row><row><cell>14</cell><cell>header</cell></row><row><cell>15</cell><cell>button</cell></row><row><cell>16</cell><cell>image</cell></row><row><cell>17</cell><cell>linebreak</cell></row><row><cell>18</cell><cell>container</cell></row><row><cell>19</cell><cell>link</cell></row><row><cell>20</cell><cell>textinput</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,185.23,558.60,24.09,9.96"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="18,135.72,126.14,343.93,100.33"><head>Table 2 :</head><label>2</label><figDesc>Table showing Mean Average Precision over IoU &gt;0.5 (mAP), Overall Precision (OP) and Overall Recall (OR) scores obtained across all runs (with Run IDs as mentioned on the challenge website)</figDesc><table coords="18,143.62,172.47,325.03,54.00"><row><cell></cell><cell>Run ID</cell><cell>Model Description</cell><cell>mAP</cell><cell>OP</cell><cell>OR</cell></row><row><cell>Run 1</cell><cell>67391</cell><cell>Baseline Mask RCNN</cell><cell>57.34</cell><cell>94.04</cell><cell>41.7</cell></row><row><cell>Run 2</cell><cell>67699</cell><cell>Mask RCNN with Multi-Pass Inference Technique</cell><cell>63.73</cell><cell>91.81</cell><cell>50.1</cell></row><row><cell>Run 3</cell><cell>67712</cell><cell>Modified Version of Run 2</cell><cell>64.12</cell><cell>91.71</cell><cell>49.6</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="19,142.96,141.49,320.71,9.22" xml:id="b0">
	<monogr>
		<ptr target="https://www.constantcontact.com/website/" />
		<title level="m" coord="19,151.52,141.49,67.75,8.97">Constant contact</title>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,163.40,169.58,9.22" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Wix</surname></persName>
		</author>
		<ptr target="https://www.wix.com" />
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,174.36,332.16,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="19,203.52,174.36,85.28,8.97">The OpenCV Library</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,295.85,174.36,150.58,8.97">Dr. Dobb&apos;s Journal of Software Tools</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,185.32,337.63,8.97;19,151.53,196.28,329.06,8.97;19,151.53,207.24,329.07,8.97;19,151.53,218.20,329.05,8.97;19,151.53,229.16,95.78,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="19,202.63,196.28,277.96,8.97;19,151.53,207.24,147.21,8.97">Overview of ImageCLEFdrawnUI 2020: The Detection and Recognition of Hand Drawn Website UIs Task</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="19,318.21,207.24,102.52,8.97">CLEF2020 Working Notes</title>
		<title level="s" coord="19,427.05,207.24,53.55,8.97;19,151.53,218.20,124.09,8.97">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,240.12,337.64,8.97;19,151.53,251.07,153.18,8.97" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="19,330.12,240.12,150.48,8.97;19,151.53,251.07,16.32,8.97">Digital image processing using MAT-LAB</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Eddins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,262.03,337.64,8.97;19,151.53,272.99,280.74,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="19,336.77,262.03,44.17,8.97">Mask r-cnn</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,401.81,262.03,78.79,8.97;19,151.53,272.99,186.87,8.97">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,283.95,337.63,8.97;19,151.53,294.91,25.61,8.97" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="19,300.63,283.95,179.96,8.97">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,305.87,337.63,8.97;19,151.53,316.83,289.90,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="19,210.12,305.87,160.24,8.97">Matplotlib: A 2d graphics environment</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Hunter</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCSE.2007.55</idno>
		<ptr target="https://doi.org/10.1109/MCSE.2007.55" />
	</analytic>
	<monogr>
		<title level="j" coord="19,378.91,305.87,101.68,8.97;19,151.53,316.83,47.77,8.97">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="90" to="95" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,327.79,337.97,8.97;19,151.53,338.75,329.06,8.97;19,151.53,349.71,329.06,8.97;19,151.53,360.66,329.06,8.97;19,151.53,371.62,329.06,8.97;19,151.53,382.58,329.06,8.97;19,151.53,393.54,329.07,8.97;19,151.53,404.50,329.06,8.97;19,151.53,415.46,329.06,8.97;19,151.53,426.42,329.06,8.97;19,151.53,437.38,34.31,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="19,295.07,382.58,185.51,8.97;19,151.53,393.54,255.38,8.97">Overview of the ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,426.42,393.54,54.18,8.97;19,151.53,404.50,329.06,8.97;19,151.53,415.46,253.34,8.97">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="19,456.14,415.46,24.44,8.97;19,151.53,426.42,138.61,8.97">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="19,142.62,448.34,337.97,8.97;19,151.53,459.29,125.27,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="19,203.42,448.34,148.70,8.97">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,360.22,448.34,120.37,8.97;19,151.53,459.29,35.78,8.97">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,470.25,337.98,8.97;19,151.53,481.21,329.06,8.97;19,151.53,492.17,218.69,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="19,426.45,470.25,54.15,8.97;19,151.53,481.21,133.57,8.97">Feature pyramid networks for object detection</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,305.79,481.21,174.80,8.97;19,151.53,492.17,133.60,8.97">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,503.13,337.98,8.97;19,151.53,514.09,329.07,8.97;19,151.53,525.05,25.61,8.97" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<title level="m" coord="19,308.33,514.09,172.26,8.97">Microsoft coco: Common objects in context</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,536.01,337.97,8.97;19,151.53,546.97,329.06,8.97;19,151.53,557.92,305.63,9.22" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="19,385.05,536.01,95.54,8.97;19,151.53,546.97,153.84,8.97">Nvidia tesla: A unified graphics and computing architecture</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Lindholm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nickolls</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Oberman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Montrym</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2008.31</idno>
		<ptr target="https://doi.org/10.1109/MM.2008" />
	</analytic>
	<monogr>
		<title level="j" coord="19,314.75,546.97,50.96,8.97">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="39" to="55" />
			<date type="published" when="2008-03">Mar 2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
