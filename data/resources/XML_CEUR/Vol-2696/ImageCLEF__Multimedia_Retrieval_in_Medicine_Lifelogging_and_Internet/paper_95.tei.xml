<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.35,115.96,304.67,12.62;1,177.33,133.89,260.69,12.62;1,178.22,151.82,258.93,12.62">The effects of colour enhancement and IoU optimisation on object detection and segmentation of coral reef structures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,136.92,189.49,65.36,8.74"><forename type="first">Marina</forename><surname>Arendt</surname></persName>
							<email>marina.arendt@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Str. 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Kairos GmbH</orgName>
								<address>
									<settlement>Bochum</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.40,189.49,77.57,8.74"><forename type="first">Johannes</forename><surname>Rückert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Str. 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,196.45,201.45,73.08,8.74"><forename type="first">Raphael</forename><surname>Brüngel</surname></persName>
						</author>
						<author>
							<persName coords="1,367.32,201.45,51.59,8.74;1,199.44,213.40,40.85,8.74"><forename type="first">Christopher</forename><surname>Brumann</surname></persName>
						</author>
						<author>
							<persName coords="1,357.45,213.40,52.52,8.74;1,239.33,225.36,38.66,8.74"><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Str. 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<settlement>Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.35,115.96,304.67,12.62;1,177.33,133.89,260.69,12.62;1,178.22,151.82,258.93,12.62">The effects of colour enhancement and IoU optimisation on object detection and segmentation of coral reef structures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7847765D5BCAD5C39FB3F1EF23326BF6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>underwater colour correction</term>
					<term>box optimisation</term>
					<term>Mask R-CNN</term>
					<term>deep learning</term>
					<term>Jaccard index</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper considers approaches used to localise and annotate coral reef structures in underwater images. Beside the actual localisation and annotation the focus laid on image pre-processing and their evaluation. Underwater images differ from terrestrial images in illumination, acuity and colour which make them more blurred with a green and blue cast. To enhance those physical properties, Image Blurriness and Light Absorption (IBLA) with additional Rayleigh optimisation or additional colour reduction were used. Afterwards, for both competition tasks Mask R-CNN was used, involving on-the-fly data augmentation and oversampling to combat the coral class imbalances. Several types of post-processing were applied to the generated boxes and polygons, mostly to account for the evaluation methodologies. IBLA and Rayleigh pre-processing improved accuracy for the localisation and annotation task, while colour reduction led to overall worse results than the original images and also oversampling led to even worse mean Average Precision (mAP) and only a slightly better average accuracy. For pixelwise parsing IBLA achieved better mAP score but worse accuracy and Rayleigh achieved worse results for mAP and accuracy. Colour reduction worked well and oversampling reduced mAP but strongly improved average accuracy. Concluding, image pre-processing -in particular IBLA and Rayleigh -has improved accuracy for both tasks and only achieved better mAP on the pixel-wise parsing task. In future work, the results could be improved by using larger images, trying other types of oversampling and train separate models for different classes and object size.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper considers the results of the 2020 ImageCLEFcoral challenge which took place for the second time <ref type="bibr" coords="2,274.64,152.18,9.96,8.74" target="#b3">[3]</ref>. Main focus lies on automatic image detection and classification of coral reef structures. ImageCLEFcoral is a subtask of ImageCLEF <ref type="bibr" coords="2,192.12,176.09,9.96,8.74" target="#b7">[7]</ref>. Underwater images differ from terrestrial images in terms of colour, illumination and acuity which can cause problems in automatic detection. Nevertheless, automatic detection and classification is needed as manual detection is cost and time intensive <ref type="bibr" coords="2,294.20,211.96,14.61,8.74" target="#b15">[15]</ref>. To regard the issues with underwater images the approaches focused on image pre-processing to enhance image structure, illumination and colour to test the effect of these steps on detection and segmentation.</p><p>After actual training of the models, several types of post-processing were applied to the generated boxes and polygons, mostly to account for the evaluation methodologies. For this, generated polygons were validated and boxes were shrunk to achieve a better Jaccard index, also known as Intersection over Union (IoU).</p><p>The paper is separated in the sections of data set description and preprocessing showing the main issues with underwater images followed by annotation and localisation (object detection task 1) and pixel-wise parsing (segmentation task 2). The results and discussion build the main part and is completed with the conclusion. All scripts used during this project are available in a GitHub repository <ref type="foot" coords="2,178.60,377.75,3.97,6.12" target="#foot_0">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Image and annotation data</head><p>All details about the task and data can be found in the task overview paper <ref type="bibr" coords="2,134.77,441.72,9.96,8.74" target="#b3">[3]</ref>. The provided version 4 of the data set consists of n = 440 images in the development part (training set) and m = 400 images in the test part. All images are provided in the Joint Photographic Experts Group (JPEG) format with a resolution of 4032 × 3024 px. An annotation file for the development part holds k = 12, 082 annotations, structured in eight variables: image id: Data set-wide unique image identifier, equal to the image filename. substrate: Image identifier-wide unique substrate index, starting by 0. c class: Name of one of the 13 present coral classes. confidence: Confidence of annotation, always 1 for 100% confidence.</p><p>x min: Minimum x-axis bounding box coordinate <ref type="foot" coords="2,350.62,551.01,3.97,6.12" target="#foot_1">5</ref> . y max: Maximum y-axis bounding box coordinate 5 .</p><p>x max: Maximum x-axis bounding box coordinate 5 . y min: Minimum y-axis bounding box coordinate 5 .</p><p>In the following, the results of explorative analyses on annotation data and applied image pre-processing methods are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Explorative data analyses on annotations</head><p>Explorative analyses on annotation data of the training set were conducted using the statistical language R<ref type="foot" coords="3,244.73,159.01,3.97,6.12" target="#foot_2">6</ref>  <ref type="bibr" coords="3,252.01,160.59,15.50,8.74" target="#b12">[12]</ref> in version 4.0.1 and the integrated development environment RStudio<ref type="foot" coords="3,228.79,170.97,3.97,6.12" target="#foot_3">7</ref>  <ref type="bibr" coords="3,237.74,172.54,15.50,8.74" target="#b14">[14]</ref> in version 1.2.5033. Spatial analysis of substrate bounding boxes was performed using the Simple Features for R (sf) package <ref type="foot" coords="3,476.12,182.92,3.97,6.12" target="#foot_4">8</ref>[9] in version 0.9-2.</p><p>During a first screening minor inconsistencies were identified. These comprise (i) a single negative coordinate value of For the negative coordinate the respective substrate was checked manually on the respective image. A sign flip was performed and its bounding box was kept. Dot-sized bounding boxes were removed. Cleansing of annotation data resulted in a total of k = 12, 077 entries, still related to n = 440 images. Presented results are based on the cleansed annotations. a) The Frequency column describes the number in the overall data set and the Images column in how many images this coral type is pictured. Percentages stands for the overall frequency distribution and Maximum is the maximum number of a coral class representatives in one image.</p><p>Annotations comprise 13 substrate classes, listed in Table <ref type="table" coords="4,411.84,118.99,4.98,8.74" target="#tab_1">1</ref> together with their frequencies, overall percentages, presence in images and maximum count they occurred in an image. The top two classes in regards to their frequency account for 60.1% of all annotations while the top five classes account for 92.1%. Soft Coral is the most common class and represents 47.2% of all annotations.</p><p>Statistics on substrate bounding box per-image frequency, aspect ratio (x:y) and area (px) are listed in Table <ref type="table" coords="4,282.11,199.97,3.87,8.74" target="#tab_2">2</ref>. The substrate density in images features a high variety. While 2018 0712 073801 116 is the only image with a single substrate, 2018 0712 073920 154 shows the maximum of 96 substrates in a single image. The median number of substrates in an image is 24, rather few images contain a vast amount. The aspect ratio of substrate bounding boxes also shows a wide span with a minimum of 0.12 and a maximum of 8.51. Highly elongated bounding boxes are present, however, a median of 1.08 and an interquartile range of 0.30 suggest a moderate elongation in most cases. Also the areas of substrate bounding boxes show a wide span and with partially extreme low and high values. For a better understanding square area values are discussed, assuming substrate bounding boxes with an aspect ratio of 1:1. Here, the minimum area is 12.73 px 2 while also a maximum of 3, 249.23 px 2 is present. The median square area is 241.94 px 2 . The spatial analysis of substrate bounding boxes revealed a notable amount of overlaps where up to five boxes shared an intersecting area. The most common overlap scenario involve two and three substrate bounding boxes. For two substrate bounding boxes the mean (median) is 14.90 <ref type="bibr" coords="4,359.02,560.48,17.71,8.74" target="#b12">(12)</ref> overlaps per image, for three substrate bounding boxes it is 2.46 <ref type="bibr" coords="4,317.05,572.43,11.62,8.74" target="#b3">(3)</ref>. A maximum of up to 81 overlaps between two and up to 32 between three substrate bounding boxes indicates numerous redundancies of annotated areas for several images. Overlaps between four and five substrate bounding boxes are rare. Also full overlaps between substrate bounding boxes have been found where a bounding box of a substrate fully covers that of another. For intra-class overlaps 146 full overlaps have been identified, while for inter-class overlaps 509 were found. Especially inter-class overlaps and full overlaps may display a challenging condition for object detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pre-processing</head><p>Underwater images differ from other images in their physical properties. The deeper the images where taken the darker the images get as well as red light is more absorbed than green and blue light. This often results in blurred images with a green and blue cast <ref type="bibr" coords="5,253.85,176.80,14.61,8.74" target="#b10">[10]</ref>.</p><p>The idea was to enhance image quality prior to segmentation and parsing which should lead to enhanced segmentation and parsing results. The best preprocessing steps were chosen by visual inspection. Pre-processing functions <ref type="bibr" coords="5,465.09,213.07,15.50,8.74" target="#b16">[16]</ref> that have been used are described in the following sections. The images were processed first by Image Blurriness and Light Absorption (IBLA) followed by either a transformation of Rayleigh distribution or an octree colour reduction. Figure <ref type="figure" coords="5,165.55,260.89,4.98,8.74">1</ref> (a) and (b) are examples showing the problems with underwater images described above.</p><p>To visualise the colour distribution, normalised histograms were made using the NumPy<ref type="foot" coords="5,242.89,295.58,3.97,6.12" target="#foot_5">9</ref> package. Figure <ref type="figure" coords="5,326.96,297.16,4.98,8.74">2</ref> shows the histograms of images 2018 0714 112438 016 and 2018 0729 112414 024. Clearly seen is the either high intensity of the green channel or the high intensity of the blue channel in combination with very low intensity of the red channel typical for underwater images.</p><p>Image Blurriness and Light Absorption (IBLA) Underwater image restoration based on IBLA was conducted on both the training and test set <ref type="bibr" coords="5,134.77,400.79,15.50,8.74" target="#b10">[10,</ref><ref type="bibr" coords="5,150.26,400.79,11.62,8.74" target="#b16">16]</ref>. IBLA transformation is based on four main steps. First, the image blurriness is analysed and afterwards a smoothed and refined blurriness map is generated to optimise the image. Second, the background light pixels are estimated by image blurriness and variance via a quad-tree algorithm. Third, the actual enhancement using depth estimation based on light absorption and blurriness which results in an optimised depth map. Last, the transmission map is estimated leading to restoration rather than estimation <ref type="bibr" coords="5,363.61,472.52,14.61,8.74" target="#b10">[10]</ref>. The results are shown in Figure <ref type="figure" coords="5,177.83,484.48,9.41,8.74">1c</ref> and Figure <ref type="figure" coords="5,241.36,484.48,8.86,8.74">1d</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enhancement based on Rayleigh distribution</head><p>The method for image enhancement with Rayleigh distribution is separated into two main steps <ref type="bibr" coords="5,440.85,528.33,9.96,8.74" target="#b5">[5]</ref>. First, the contrast is corrected and second, the colour is corrected. For contrast correction a global histogram stretching is implemented followed by division into a lower and an upper side by the average point. Both parts are then Rayleighstretched to the full gray-scale from 0 to 255 and recombined. For colour enhancement the image is transformed into the Hue, Saturation, and Value (HSV) colour model. The saturation and value levels are stretched and reconverted to an Red, Green, Blue colour space (RGB) image. This led to an enhancement in contrast and details and reduced image artefacts <ref type="bibr" coords="5,348.40,623.98,11.15,8.74" target="#b5">[5,</ref><ref type="bibr" coords="5,359.55,623.98,11.15,8.74" target="#b16">16]</ref>. Results are in Figure <ref type="figure" coords="5,471.19,623.98,9.41,8.74">1e</ref> and Figure <ref type="figure" coords="5,185.57,635.93,7.20,8.74">1f</ref>. Colour reduction The colour reduction was conducted using the octree process reducing all IBLA transformed images to maximum 256 colours as implemented in the following code 10 . The octree colour reduction (for instance described in [2, p. 333 sqq.]) results in an image with 256 colours with a harmonised colour distribution <ref type="bibr" coords="7,189.09,457.29,9.96,8.74" target="#b2">[2]</ref>. The results are shown in Figure <ref type="figure" coords="7,347.78,457.29,9.96,8.74">1g</ref> and Figure <ref type="figure" coords="7,411.86,457.29,8.86,8.74">1h</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Mask R-CNN <ref type="bibr" coords="7,195.92,518.71,10.52,8.74" target="#b6">[6]</ref> is an instance segmentation framework which extends Faster R-CNN <ref type="bibr" coords="7,159.65,530.67,15.50,8.74" target="#b13">[13]</ref> with a parallel branch for instance segmentation on region of interests.</p><p>The models described in this paper were trained on a Mask R-CNN implementation using TensorFlow and Keras in Python 3 11 which was patched to support TensorFlow 2.1 12 . All models used weights pre-trained on the MS COCO data set <ref type="bibr" coords="7,206.22,579.05,9.96,8.74" target="#b8">[8]</ref>.</p><p>To speed up on-the-fly pre-processing and avoid padding, all images were resized to 1536 × 1536 beforehand.</p><p>The training was split into two phases: first, only the newly added layers were trained for one epoch. Second, the complete network was trained for the remaining epochs. Then, Polyak averaging <ref type="bibr" coords="8,326.59,142.90,15.50,8.74">[11]</ref> was performed on the top five models based on their .632 error <ref type="bibr" coords="8,279.74,154.86,9.96,8.74" target="#b4">[4]</ref>. For the submission models, early stopping was used based on the average epoch number for the top five models during cross-validation training.</p><p>For on-the-fly data augmentation, the images were randomly rotated (up to ±180 • in each direction), flipped (up/down or left/right) with 33% chance each, and 0 to 5 of blur, sharpen, random crop (up to 20% on each side), Gaussian noise, brightness, hue/saturation, and contrast were changed.</p><p>To combat the class imbalance in the data set, oversampling was performed which entails iterative optimisation of the Shannon entropy of the data set by adding images until the number of images is tripled, with constraints on the number of times a single image can appear in the final data set.</p><p>Considering only the five most frequent classes was a consideration due to the imbalance of the data set and achieved good results in our cross-validation runs.</p><p>Table <ref type="table" coords="8,177.61,330.47,4.98,8.74" target="#tab_3">3</ref> lists the most important parameters used for the different training runs. The training run parameters are listed in Table <ref type="table" coords="8,360.40,342.42,3.87,8.74" target="#tab_4">4</ref>. It includes the submission ID, the run name, which data set was used (original images, IBLA pre-processing, IBLA plus Rayleigh pre-processing, colour reduced, see Section 2.2), whether onthe-fly data augmentation as described above was applied, whether images of size 1024 × 1024 or 1536 × 1536 were used, whether oversampling was used as well as the number of epochs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Annotation and localisation</head><p>The focus of the approach described in this paper was on the annotation and localisation task, and models were trained using the bounding box annotations and optimised against the PASCAL Visual Object Classes (VOC)-style mean Average Precision (mAP) implementation in Mask R-CNN <ref type="bibr" coords="9,395.21,378.53,9.96,8.74" target="#b6">[6]</ref>.</p><p>The different configurations that were analysed (as seen in Table <ref type="table" coords="9,432.25,390.99,4.43,8.74" target="#tab_4">4</ref>) included different data sets based on the pre-processing described in section 2.2, different levels of on-the-fly data augmentation, using two different image sizes, using oversampling, as well as trying to train models for varying numbers of epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pixel-wise parsing</head><p>For this task, similar approaches as for the annotation and localisation task were applied only using the polygon annotations for training. The colour reduction run was skipped due to its poor results for the first task, instead a number of different runs with larger images were included to see the results of more and less training as well as a lower confidence threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Post-processing</head><p>When the evaluation code for the challenge was published and it turned out that the submitted bounding boxes would be evaluated against the polygons instead of the bounding box annotations, it became clear that training with the polygon annotations would be much more effective. For example, this led to an evaluation F1 score of 0.8 for the bounding box training ground truth, whereas the score was 0.99 for the polygon training ground truth. The loss of 0.01 was due to invalid polygons in the ground truth annotations. This value can be increased from 0.8 to 0.9 simply by reducing the size of the bounding boxes by 7.5% on each side as seen in Figure <ref type="figure" coords="10,379.77,130.95,3.87,8.74" target="#fig_1">3</ref>. This post-processing step was therefore used for all submissions of the first task.</p><p>Additionally, an iterative algorithm was created to approximate the best possible rectangular box for a given polygon according to their IoU. This algorithm is described in the next section. To make use of this algorithm, a model was trained on the polygon annotations and the resulting polygons were used to generate boxes.</p><p>For the second task, the polygons generated from binary masks by OpenCV<ref type="foot" coords="10,472.15,213.15,7.94,6.12" target="#foot_6">13</ref>  <ref type="bibr" coords="10,134.77,226.68,10.52,8.74" target="#b1">[1]</ref> were validated against the shapely library 14 which was used in the evaluation script since about 1% of generated polygons were not valid according to the shapely library's definition of a valid polygon and would be ignored by the evaluation script. A valid polygon may not cross itself and may only touch in a single point.</p><p>To clean up invalid polygons, first duplicate points were removed, then the polygons were split in several separated polygons using the touching / selfcrossing points and the biggest polygon was used. Separately, the buffer function provided by the shapely library was used to generate a valid polygon. Then the polygon with the overall least absolute area difference compared to the original, invalid polygon was used. Bounding box IoU optimisation Given a solid polygon P , which is defined by a contour as a set of points. The corresponding minimal enclosing rectangle can be defined by four parameters: R 0 := [x, y, w, h]. Here x and y define the top rectangles left corner and w and h the width and height, respectively. Calculating the IoU of the polygon with the rectangle showed that this value is not necessarily the best to be achieved as can be seen in Figure <ref type="figure" coords="11,357.71,178.77,3.87,8.74" target="#fig_1">3</ref>. This became particularly clear with thin long polygon arms that run parallel to the rectangle edges. In these cases it often led to a higher IoU if the rectangle was slightly reduced for the corresponding side. When considered the minimum bounding box, however, an increase in edge length can only led to an IoU deterioration. Therefore the polygons minimum bounding box was chosen as the algorithm's starting point for maximising the IoU. The parameter space was thus defined by the rectangle's parameters.</p><p>The optimised objective function was IoU(P, R) = |P ∩R| |P ∪R| for the given polygon P and a current rectangle R k at optimisation step k. To maximise the target function, R was iterative changed in all parameters via translation and scaling, so that the most optimised rectangle was used for the next iteration. This process was continued until no further objective function improvement was achieved. Consequently, the rectangle was accepted as optimised. During the optimisation process, step sizes for translation, shrinkage and growth are given by t, s and g. In this application all values were set to four. Each optimisation iteration was then performed by calculating:</p><formula xml:id="formula_0" coords="11,175.32,388.56,305.27,30.06">R k+1 T ← = R k + [-t, 0, 0, 0] R k+1 T → = R k + [t, 0, 0, 0] R k+1 T ↑ = R k + [0, -t, 0, 0] R k+1 T ↓ = R k + [0, t, 0, 0]<label>(1)</label></formula><formula xml:id="formula_1" coords="11,175.32,438.63,305.27,30.06">R k+1 S← = R k + [s, 0, -s, 0] R k+1 S→ = R k + [0, 0, -s, 0] R k+1 S↑ = R k + [0, s, 0, -s] R k+1 S↓ = R k + [0, 0, 0, -s]<label>(2)</label></formula><formula xml:id="formula_2" coords="11,175.32,488.70,305.27,30.06">R k+1 G← = R k + [-g, 0, g, 0] R k+1 G→ = R k + [0, 0, g, 0] R k+1 G↑ = R k + [0, -g, 0, g] R k+1 G↓ = R k + [0, 0, 0, g]<label>(3)</label></formula><p>As can be seen in Equation <ref type="formula" coords="11,279.78,524.61,3.87,8.74" target="#formula_0">1</ref>, the rectangle movement was performed in every possible direction. It should be noted that the rectangle never excesses the bounding box dimensions. Equation <ref type="formula" coords="11,312.75,548.52,4.98,8.74" target="#formula_1">2</ref>shows the corresponding contraction of the rectangle, while Equation 3 calculates its enlargement. Once all possible rectangles R k+1 were calculated, the corresponding objective function with P was evaluated, so that the corresponding improvement factor was known. Subsequently the operation with the most promising improvement factor was performed by setting the corresponding rectangle as the new R k and thus setting the starting point for the next iteration. As soon as no rectangle shows an improvement, R k was no longer updated and was assumed to be optimised with regard to the objective function. An example for an optimised rectangle is shown in Figure <ref type="figure" coords="11,177.83,656.12,3.87,8.74" target="#fig_1">3</ref>.</p><p>For the image annotation and localisation task (see Table <ref type="table" coords="12,399.03,144.74,3.87,8.74" target="#tab_5">5</ref>), on-the-fly data augmentation expectedly reduced over-fitting and led to better results for both mAP and average accuracy. Pre-processing using IBLA and Rayleigh led to lower mAP values but increased average accuracy, while colour reduction produced overall worse results compared to the original images. The Rayleigh run had the highest average accuracy of all runs submitted for the first task, followed by the IBLA and augmentation runs, showing that while the models do not detect objects as accurately as some of the competitors, they classify the detected objects very well. Considering only the 5 most frequent classes achieved worse results than the baseline in both mAP and accuracy, unlike in the cross-validation runs, where it achieved the best F1 score on the validation data.</p><p>Surprisingly, oversampling led to even worse mAP results than the baseline model, while having only slightly better average accuracy. In our cross-validation runs on the other hand, oversampling achieved results on par with the data augmentation run. This may be due to a slightly different class distribution in the test data set, or too many epochs of training for the submission models.</p><p>Using larger images led to better mAP scores but worse average accuracy. Using polygon annotations for training clearly improved the mAP, but did not improve the average accuracy.</p><p>Looking at the per-substrate accuracies, the IoU optimised run produced the worst results with seven classes having 0% accuracy. The run without IoU optimisation and the run with larger images still have six classes without any correct detections, meaning that all runs with larger images have trouble with the less frequent classes. The baseline, for comparison, produced no correct detections for five classes. The oversampling run only has three classes without any correct detections, but three more classes with accuracies below 0.05.</p><p>The runs without oversampling performed better, each of them produced correct detections for all but one class which was the class that had 0% accuracy across all submitted runs of all participants and was only represented by less than 30 instances in the training data set.</p><p>The evaluation code was released only several weeks before the submission deadline for the models, and it produced very different results than the PAS-CAL VOC-style mAP evaluation recommended in the task's description and implemented in Mask R-CNN.</p><p>The evaluation strategy was mainly the same for both tasks. The focus of this work would have been much more on the second task and on training instance segmentation models if the evaluation code had been published earlier. It is easier and more effective to generate suitable boxes from polygons compared to guessing suitable boxes based on the bounding box or just using the bounding box itself.</p><p>In effect, a model which predicts perfect bounding boxes would never be able to exceed an mAP score of about 0.8 based on objects that are shaped in a way that produce bounding boxes with very low IoU. This effect was demonstrated by the bounding box refinement and IoU optimisation, which led to a significant improvement in mAP score.</p><p>For the image pixel-wise parsing task (see Table <ref type="table" coords="13,363.12,383.34,3.87,8.74" target="#tab_6">6</ref>), IBLA achieved a better mAP score than the original data set, but a worse average accuracy than the baseline. Rayleigh performed even worse with a similar mAP score than the baseline but worse average accuracy. Oversampling once again reduced the mAP score, but strongly improved the average accuracy. Similar to the first task, using larger images increased the mAP score but reduced the average accuracy.</p><p>Using larger images and the data set with reduced colours further improved the mAP score, reaching the overall best value for the second task among the submitted runs but produced a much lower average accuracy.</p><p>Training for more or less epochs led to overall worse results. Reducing the confidence threshold led to a very low mAP score but slightly improved the average accuracy, led once again to the highest average accuracy out of all runs submitted for the second task.</p><p>Looking at the overall and per-substrate accuracies the models with larger images interestingly had better accuracies than the one with smaller images, unlike in the first task, where it was the other way around. Excluding the run with 0.4 minimum detection threshold which achieved the highest overall accuracy with a much lower mAP value the oversampling run had the best accuracy.</p><p>The runs without oversampling achieved comparatively worse results with four to five classes having no correct detections whereas all other runs had only the one class without correct detections.</p><p>Post-processing as described in Section 2.1 did not achieved an impact on classification quality as expected. Hence, those results are not shown as they were not finally implemented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Concluding, image pre-processing using IBLA and Rayleigh has improved accuracy for the localisation and annotation task while achieving better mAP on the pixel-wise parsing task. Colour reduction worked well with larger images for the second task in terms of mAP, but falls behind in accuracy.</p><p>Oversampling was overall not successful even though it led to better accuracy in the second task. This result was not reflected in cross-validation analysis on the training data. Therefore, oversampling was used in the majority of submitted runs decreasing the performance especially of the runs with larger images.</p><p>Larger images performed worse in early runs that were not properly finetuned and hence enlargement was not considered in most of the further analysis. All runs with larger images used oversampling which most likely hurt the model performance. A stronger focus on larger images would have been useful, since the results are promising at least for the second task.</p><p>Nevertheless, this work has produced competitive models especially in terms of the classification accuracy that could be improved in the future. Examples are using larger images, trying other types of oversampling, or training separate models for different classes or object sizes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,134.77,601.46,345.83,9.02;6,134.77,613.42,345.83,9.02;6,134.77,625.37,345.83,8.74;6,134.77,637.33,345.83,8.74;6,134.77,649.28,345.83,8.74;6,134.77,661.24,18.27,8.74;6,304.86,672.88,2.56,7.86;6,162.43,479.63,124.49,93.37"><head>Fig. 1 :Fig. 2 :</head><label>12</label><figDesc>Fig. 1: Comparison of the original images 2018 0714 112438 016 (a) and 2018 0729 112414 024 (b) from the training set with their three different transformation (c) -(h). (a) and (b) contain the main problems with underwater images. They are blurry and show a large translation of histograms towards green or blue. The original images (a) and (b) are under copyright of the organisers [3,7].</figDesc><graphic coords="6,162.43,479.63,124.49,93.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,134.77,566.16,345.82,8.74;10,134.77,578.12,345.82,8.74;10,134.77,590.07,345.82,8.74;10,134.77,602.03,176.37,8.74"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: IoU values for a detected polygon. The minimum bounding box shows an IoU of 0.534. After reducing the size by 7.5% in all dimensions the result was increased to 0.593 as the dotted box shows. Applying the iterative optimisation algorithm led to the highest IoU of 0.73.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,222.31,345.83,56.84"><head></head><label></label><figDesc>x min = -1 present in one row (substrate 10 in 2018 0714 112502 024), and (ii) five dot-sized bounding boxes with x min = x max and y min = y max (substrate 17 in 2018 0714 112534 047; substrate 14 in 2018 0714 112535 042; substrate 1 in 2018 0714 112535 050; substrate 21 in 2018 0729 112613 064; substrate 5 in 2018 0729 112458 039).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,134.77,375.79,345.82,184.98"><head>Table 1 :</head><label>1</label><figDesc>Class frequencies, images presence and figures of per-image occurrences in the training set a . Top-5 occurrences are highlighted.</figDesc><table coords="3,136.56,410.05,342.24,150.73"><row><cell>Class</cell><cell cols="4">Frequency Percentages Images Maximum</cell></row><row><cell>Algae Macro or Leaves</cell><cell>91</cell><cell>0.75 %</cell><cell>53</cell><cell>11</cell></row><row><cell>Fire Coral Millepora</cell><cell>19</cell><cell>0.16 %</cell><cell>9</cell><cell>6</cell></row><row><cell>Hard Coral Boulder</cell><cell>1640</cell><cell>13.58 %</cell><cell>398</cell><cell>18</cell></row><row><cell>Hard Coral Branching</cell><cell>1181</cell><cell>9.78 %</cell><cell>349</cell><cell>16</cell></row><row><cell>Hard Coral Encrusting</cell><cell>945</cell><cell>7.82 %</cell><cell>310</cell><cell>21</cell></row><row><cell>Hard Coral Foliose</cell><cell>177</cell><cell>1.47 %</cell><cell>104</cell><cell>11</cell></row><row><cell>Hard Coral Mushroom</cell><cell>223</cell><cell>1.85 %</cell><cell>140</cell><cell>6</cell></row><row><cell>Hard Coral Submassive</cell><cell>198</cell><cell>1.64 %</cell><cell>93</cell><cell>12</cell></row><row><cell>Hard Coral Table</cell><cell>21</cell><cell>0.17 %</cell><cell>21</cell><cell>1</cell></row><row><cell>Soft Coral</cell><cell>5662</cell><cell>46.88 %</cell><cell>425</cell><cell>66</cell></row><row><cell>Soft Coral Gorgonian</cell><cell>90</cell><cell>0.74 %</cell><cell>66</cell><cell>5</cell></row><row><cell>Sponge</cell><cell>1691</cell><cell>14.00 %</cell><cell>342</cell><cell>34</cell></row><row><cell>Sponge Barrel</cell><cell>139</cell><cell>1.15 %</cell><cell>118</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,136.56,404.88,342.24,63.44"><head>Table 2 :</head><label>2</label><figDesc>Statistics on substrate bounding box features of the training set.</figDesc><table coords="4,136.56,427.18,342.24,41.14"><row><cell></cell><cell cols="4">Min. 1st Qu. Median Mean 3rd Qu.</cell><cell cols="2">Max. Std. deviation</cell></row><row><cell>Per-image frequency</cell><cell>1</cell><cell>16</cell><cell>24 27.45</cell><cell>36</cell><cell>96</cell><cell>16.13</cell></row><row><cell>Aspect ratio (x:y)</cell><cell>0.12</cell><cell>0.87</cell><cell>1.08 1.17</cell><cell>1.35</cell><cell>8.51</cell><cell>0.50</cell></row><row><cell>Square area (px 2 )</cell><cell cols="5">12.73 158.39 241.94 412.26 388.81 3,249.23</cell><cell>261.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.77,449.26,345.83,186.97"><head>Table 3 :</head><label>3</label><figDesc>Mask R-CNN parameters used for training variations. Image size, batch size and anchor scales depended on the image size. Learning rate η was reduced for the second phase and was different for oversampling runs.</figDesc><table coords="8,136.56,485.51,294.07,150.73"><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Backbone</cell><cell>Resnet101</cell></row><row><cell>Image size (larger images)</cell><cell>1024 × 1024 (1536 × 1536)</cell></row><row><cell>Batch size (larger images)</cell><cell>2 (1)</cell></row><row><cell>Optimiser</cell><cell>Stochastic gradient descent with Nesterov momentum enabled</cell></row><row><cell>η first phase (oversampling)</cell><cell>0.005 (0.002)</cell></row><row><cell>η second phase (oversampling)</cell><cell>0.0005 (0.0005)</cell></row><row><cell>Learning momentum</cell><cell>0.9</cell></row><row><cell>Weight decay</cell><cell>0.0001</cell></row><row><cell cols="2">Epochs (augmentation, oversampling) 15 (30, 20)</cell></row><row><cell>Minimum detection confidence</cell><cell>0.6</cell></row><row><cell>Anchor scales</cell><cell>32, 64, 128, 256, 512</cell></row><row><cell>Anchor scales larger images</cell><cell>48, 96, 192, 384, 768</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,134.77,127.36,345.82,164.06"><head>Table 4 :</head><label>4</label><figDesc>Run configurations with submission ID, Run name, pre-processing, Augmentation, Larger images, Oversampling, and number of epochs.</figDesc><table coords="9,136.56,157.18,317.98,134.24"><row><cell cols="4">ID Augm. 68184 Colour reduction (CR) IBLA + CR Run name Data set Yes</cell><cell>No</cell><cell>No</cell><cell>30</cell></row><row><cell cols="2">68185 Oversampling</cell><cell>IBLA</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>20</cell></row><row><cell cols="2">68183 Larger images</cell><cell>IBLA</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>20</cell></row><row><cell cols="2">68182 Segmentation</cell><cell cols="2">IBLA (polygons) Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>20</cell></row><row><cell>68181</cell><cell>Segmentation + IoU optimisation</cell><cell cols="2">IBLA (polygons) Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,134.77,284.06,345.83,175.02"><head>Table 5 :</head><label>5</label><figDesc>5-fold cross-validation results on the training data set along with the corresponding submission results for the image annotation and localisation task.</figDesc><table coords="12,418.72,307.95,45.16,7.86"><row><cell>Submission</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,136.56,462.69,342.24,141.15"><head>Table 6 :</head><label>6</label><figDesc>Submission results for the image pixel-wise parsing task.</figDesc><table coords="13,136.56,475.03,342.24,128.81"><row><cell>ID</cell><cell>Run name</cell><cell>MAP 0.5</cell><cell>Avg. accuracy</cell></row><row><cell cols="2">67963 Baseline</cell><cell>.433</cell><cell>.134</cell></row><row><cell cols="2">67964 Augmentation</cell><cell>.449</cell><cell>.140</cell></row><row><cell cols="2">67965 IBLA</cell><cell>.453</cell><cell>.128</cell></row><row><cell cols="2">67967 IBLA + Rayleigh</cell><cell>.435</cell><cell>.120</cell></row><row><cell cols="2">68192 Oversampling</cell><cell>.424</cell><cell>.191</cell></row><row><cell cols="2">67968 Larger images</cell><cell>.469</cell><cell>.174</cell></row><row><cell cols="2">68190 Larger images, colour reduction</cell><cell>.474</cell><cell>.158</cell></row><row><cell cols="2">68191 Larger images, 10 epochs</cell><cell>.416</cell><cell>.128</cell></row><row><cell cols="2">67969 Larger images, 0.4 confidence</cell><cell>.376</cell><cell>.196</cell></row><row><cell cols="2">68189 Larger images, 60 epochs, more augmentation</cell><cell>.371</cell><cell>.157</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,634.88,317.12,7.86"><p>https://github.com/saviola777/fhdo-imageclef2020-coral/, accessed 2020-07-07</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="2,144.73,645.84,335.86,7.86;2,144.73,656.80,96.84,7.86"><p>On the image level the coordinate system origin (x min = 0, y min = 0) is located in the upper left corner.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="3,144.73,634.88,195.79,7.86"><p>https://www.r-project.org/, accessed 2020-07-09</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="3,144.73,645.84,169.44,7.86"><p>https://rstudio.com/, accessed 2020-07-09</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4" coords="3,144.73,656.80,212.67,7.86"><p>https://github.com/r-spatial/sf, accessed 2020-07-09</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5" coords="5,144.73,656.80,164.26,7.86"><p>https://numpy.org/, accessed 2020-07-10</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_6" coords="10,144.73,645.84,165.81,7.86"><p>https://opencv.org/, accessed 2020-07-10</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,144.73,634.88,335.86,7.86;7,144.73,645.84,335.86,7.86;7,133.85,655.03,7.31,5.24;7,144.73,656.80,270.77,7.86;14,134.77,591.91,62.94,10.52" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Abdulla</surname></persName>
		</author>
		<ptr target="https://github.com/DiffPro-ML/MaskRCNN" />
		<title level="m" coord="7,199.27,634.88,281.32,7.86;7,144.73,645.84,62.03,7.86">Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow</title>
		<imprint>
			<date type="published" when="2020-07-07">2020-07-07 12. 2020-07-07</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,613.59,337.64,7.89;14,151.52,624.58,42.49,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,202.78,613.62,85.22,7.86">The OpenCV Library</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,294.80,613.62,148.77,7.86">Dr. Dobb&apos;s Journal of Software Tools</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="120" to="125" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,634.88,337.64,7.86;14,151.52,645.84,329.07,7.86;14,151.52,656.80,171.84,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="14,279.02,634.88,201.58,7.86;14,151.52,645.84,106.33,7.86">Digital Image Processing -An Algorithmic Introduction Using Java</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Burge</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4471-6684-9</idno>
		<ptr target="https://doi.org/10.1007/978-1-4471-6684-9" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edn.</note>
</biblStruct>

<biblStruct coords="15,142.96,119.67,337.63,7.86;15,151.52,130.63,329.07,7.86;15,151.52,141.59,329.07,7.86;15,151.52,152.55,88.33,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,203.13,130.63,277.45,7.86;15,151.52,141.59,71.46,7.86">Overview of the ImageCLEFcoral 2020 Task: Automated Coral Reef Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">G</forename><surname>Clift</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="15,244.70,141.59,104.67,7.86">CLEF2020 Working Notes</title>
		<title level="s" coord="15,356.77,141.59,123.82,7.86;15,151.52,152.55,21.70,7.86">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,163.51,337.63,7.86;15,151.52,174.44,328.62,7.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,258.56,163.51,222.04,7.86;15,151.52,174.47,28.30,7.86">Improvements on cross-validation: the 632+ bootstrap method</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,187.62,174.47,191.29,7.86">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">438</biblScope>
			<biblScope unit="page" from="548" to="560" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,185.43,337.64,7.86;15,151.52,196.36,329.07,7.89;15,151.52,207.34,195.38,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,276.13,185.43,204.46,7.86;15,151.52,196.39,248.76,7.86">Underwater image quality enhancement through composition of dual-intensity images and Rayleigh-stretching</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S A</forename><surname>Ghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A M</forename><surname>Isa</surname></persName>
		</author>
		<idno type="DOI">10.1186/2193-1801-3-757</idno>
		<ptr target="https://doi.org/10.1186/2193-1801-3-757" />
	</analytic>
	<monogr>
		<title level="j" coord="15,408.20,196.39,51.41,7.86">SpringerPlus</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,218.30,337.63,7.86;15,151.52,229.26,329.07,7.86;15,151.52,240.22,152.64,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,373.87,218.30,56.57,7.86">Mask R-CNN</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.322</idno>
		<ptr target="https://doi.org/10.1109/iccv.2017.322" />
	</analytic>
	<monogr>
		<title level="m" coord="15,151.52,229.26,261.22,7.86">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,251.18,337.64,7.86;15,151.52,262.14,329.07,7.86;15,151.52,273.10,329.07,7.86;15,151.52,284.06,329.07,7.86;15,151.52,295.02,329.07,7.86;15,151.52,305.98,329.07,7.86;15,151.52,316.93,329.07,7.86;15,151.52,327.89,329.07,7.86;15,151.52,338.85,329.07,7.86;15,151.52,349.81,267.15,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,295.04,305.98,185.55,7.86;15,151.52,316.93,276.77,7.86">Overview of the ImageCLEF 2020: Multimedia Retrieval in Lifelogging, Medical, Nature, and Internet Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,450.74,316.93,29.85,7.86;15,151.52,327.89,329.07,7.86;15,151.52,338.85,306.29,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="15,180.20,349.81,168.93,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="15,142.96,360.77,337.64,7.86;15,151.52,371.73,329.07,7.86;15,151.52,382.69,329.07,7.86;15,151.52,393.65,171.84,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,218.57,371.73,196.90,7.86">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10602-1" />
	</analytic>
	<monogr>
		<title level="m" coord="15,440.12,371.73,40.47,7.86;15,151.52,382.69,88.97,7.86">Computer Vision -ECCV 2014</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,404.61,337.63,7.86;15,151.52,415.54,310.97,7.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,205.54,404.61,270.57,7.86">Simple Features for R: Standardized Support for Spatial Vector Data</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pebesma</surname></persName>
		</author>
		<idno type="DOI">10.32614/RJ-2018-009</idno>
		<ptr target="https://doi.org/10.32614/RJ-2018-009" />
	</analytic>
	<monogr>
		<title level="j" coord="15,151.52,415.56,59.15,7.86">The R Journal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="439" to="446" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,426.52,337.97,7.86;15,151.52,437.46,329.07,7.89;15,151.52,448.44,219.45,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,260.82,426.52,219.77,7.86;15,151.52,437.48,105.15,7.86">Underwater Image Restoration Based on Image Blurriness and Light Absorption</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">T</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">C</forename><surname>Cosman</surname></persName>
		</author>
		<idno type="DOI">10.1109/tip.2017.2663846</idno>
		<ptr target="https://doi.org/10.1109/tip.2017.2663846" />
	</analytic>
	<monogr>
		<title level="j" coord="15,264.09,437.48,162.17,7.86">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1579" to="1594" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,459.40,337.97,7.86;15,151.52,470.33,280.71,7.89" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,271.61,459.40,208.98,7.86;15,151.52,470.36,11.14,7.86">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,169.44,470.36,170.78,7.86">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,481.32,337.98,7.86;15,151.52,492.28,329.07,7.86;15,151.52,503.24,17.43,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,209.02,481.32,232.25,7.86">R: A Language and Environment for Statistical Computing</title>
		<author>
			<persName coords=""><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<ptr target="https://www.R-project.org/" />
	</analytic>
	<monogr>
		<title level="m" coord="15,448.34,481.32,32.26,7.86;15,151.52,492.28,128.00,7.86">R Foundation for Statistical Computing</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,514.19,337.98,7.86;15,151.52,525.15,329.07,7.86;15,151.52,536.11,148.76,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,307.15,514.19,173.44,7.86;15,151.52,525.15,173.41,7.86">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,349.43,525.15,131.16,7.86;15,151.52,536.11,73.89,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,547.07,337.98,7.86;15,151.52,558.03,210.97,7.86" xml:id="b14">
	<monogr>
		<author>
			<orgName type="collaboration" coords="15,151.52,547.07,57.03,7.86">RStudio Team</orgName>
		</author>
		<ptr target="https://www.rstudio.com/" />
		<title level="m" coord="15,217.77,547.07,218.44,7.86">RStudio: Integrated Development Environment for R</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>RStudio, Inc</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,568.99,337.98,7.86;15,151.52,579.95,329.07,7.86;15,151.52,590.91,329.07,7.86;15,151.52,601.87,266.22,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="15,277.98,568.99,202.62,7.86;15,151.52,579.95,123.34,7.86">Object classification in underwater images using adaptive fuzzy neural network</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Srividhya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Ramya</surname></persName>
		</author>
		<idno type="DOI">10.1109/fskd.2017.8392973</idno>
		<ptr target="https://doi.org/10.1109/fskd.2017.8392973" />
	</analytic>
	<monogr>
		<title level="m" coord="15,298.65,579.95,181.94,7.86;15,151.52,590.91,307.46,7.86">2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="142" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,612.82,337.98,7.86;15,151.52,623.78,329.07,7.86;15,151.52,634.72,329.07,7.89;15,151.52,645.70,179.62,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="15,468.57,612.82,12.03,7.86;15,151.52,623.78,329.07,7.86;15,151.52,634.74,144.89,7.86">An Experimental-Based Review of Image Enhancement and Image Restoration Methods for Underwater Imaging</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Fortino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Liotta</surname></persName>
		</author>
		<idno type="DOI">10.1109/access.2019.2932130</idno>
		<ptr target="https://doi.org/10.1109/access.2019.2932130" />
	</analytic>
	<monogr>
		<title level="j" coord="15,307.84,634.74,55.66,7.86">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="140233" to="140251" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
