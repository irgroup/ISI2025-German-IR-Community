<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.37,115.90,322.62,12.90">AUEB NLP Group at ImageCLEFmed Caption 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,170.21,155.00,57.82,8.64"><forename type="first">Basil</forename><surname>Karatzas</surname></persName>
							<email>karatzas.basil@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.48,155.00,70.23,8.64"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Systems Sciences</orgName>
								<orgName type="institution">Stockholm University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,414.71,155.00,30.45,8.64;1,188.80,166.95,28.98,8.64"><forename type="first">Vasiliki</forename><surname>Kougia</surname></persName>
							<email>kouyiav@aueb.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Systems Sciences</orgName>
								<orgName type="institution">Stockholm University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.68,166.95,84.40,8.64"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.37,115.90,322.62,12.90">AUEB NLP Group at ImageCLEFmed Caption 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BCF77F9C2EFFDD21F30961F2720DFDF4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Images</term>
					<term>Concept Detection</term>
					<term>Image Retrieval</term>
					<term>Image Captioning</term>
					<term>Multi-label Classification</term>
					<term>Multimodal</term>
					<term>Ensemble</term>
					<term>Convolutional Neural Network (CNN)</term>
					<term>Machine Learning</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article concerns the participation of AUEB's NLP Group in the ImageCLEFmed Caption task of 2020. The goal of the task was to identify medical terms that best describe each image, in order to accelerate and improve the interpretation of medical images by experts and systems. The systems we implemented extend our previous work <ref type="bibr" coords="1,297.54,303.44,10.24,7.77" target="#b6">[7,</ref><ref type="bibr" coords="1,307.78,303.44,6.83,7.77" target="#b7">8,</ref><ref type="bibr" coords="1,314.61,303.44,6.83,7.77" target="#b8">9]</ref> on models that employ CNN image encoders combined with an image retrieval method or a feed-forward neural network. Our systems were ranked 1st, 2nd and 6th.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF <ref type="bibr" coords="1,186.66,436.80,11.62,8.64" target="#b3">[4]</ref> is an evaluation campaign held annually since 2003 as part of CLEF 3 , and revolves around image analysis and retrieval tasks. ImageCLEFmedical <ref type="bibr" coords="1,446.35,448.75,16.60,8.64" target="#b10">[11]</ref> is a collection of ImageCLEF tasks that are associated with the study of medical images. In 2020, it consisted of 3 tasks: VQA-Med, Caption and Tuberculosis. 4 The Image-CLEFmed Caption task concerns the automatic assignment of medical terms (called concepts) to medical images. The dataset of ImageCLEFmed Caption 2020 consisted of medical images, which were split to 7 categories according to their radiology modality (see Table <ref type="table" coords="1,192.17,520.48,3.60,8.64">1</ref>). Writing a diagnostic report for a medical image is a demanding and very time-consuming task that needs to be handled by medical experts <ref type="bibr" coords="1,423.05,532.44,11.38,8.64" target="#b1">[2,</ref><ref type="bibr" coords="1,434.43,532.44,11.38,8.64" target="#b13">14]</ref>. One of the main goals of the ImageCLEFmed Caption task is to assist the development of efficient, multi-label, medical-image tagging models, which could be used to assist the medical experts and reduce the time needed for the diagnosis as well as to reduce potential medical errors.</p><p>Table <ref type="table" coords="2,158.52,115.83,3.36,8.06">1</ref>. The names of the seven categories in the dataset (first column), as provided by the organisers, and the description of each category (second column), drawn from <ref type="bibr" coords="2,414.08,127.13,13.74,7.77" target="#b9">[10]</ref>. We also give the Concept Unique Identifier (CUI) of concepts that appear in every image of the respective category (third column) and their corresponding description from the UMLS Metathesaurus (fourth column). In this paper we describe the medical image tagging systems of the AUEB NLP Group that were submitted to ImageCLEFmed Caption 2020. Following our last year's success <ref type="bibr" coords="2,167.13,334.28,10.58,8.64" target="#b7">[8]</ref>, our 3 submissions were ranked 1st, 2nd and 6th. <ref type="foot" coords="2,375.67,332.61,3.49,6.05" target="#foot_0">5</ref>Overall, our submissions were based on two methods. The first method was based on the Mean@k-NN system of <ref type="bibr" coords="2,263.83,358.19,11.62,8.64" target="#b8">[9]</ref> that assigns concepts to each test image using the k nearest neighbors from the training dataset. The second method extends the Con-ceptCXN system of <ref type="bibr" coords="2,216.14,382.10,11.62,8.64" target="#b8">[9]</ref> and uses a DenseNet-121 CNN to encode any test image and a Feed Forward Neural Network classifier on top. The remaining of the paper describes the data, methods, submitted systems and our results, followed by conclusions and future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>Initially, the ImageCLEFmed Caption datasets comprised a broad variety of clinical images, which were extracted from figures of scientific articles found in the open-access biomedical literature database PubMed Central. <ref type="foot" coords="2,330.98,493.66,3.49,6.05" target="#foot_1">6</ref> Each image was assigned medical terms from the Unified Medical Language System (UMLS) <ref type="bibr" coords="2,383.26,507.29,10.58,8.64" target="#b0">[1]</ref>. These terms, called concepts, were extracted from the processed text of the respective figure caption. Since 2019, in order to discard compound or non-radiology images from the initial datasets, the organisers applied filters and also performed a manual revision of their data. A subset of the resulting dataset, which is called the extended Radiology Objects in COntext (ROCO) <ref type="bibr" coords="2,170.85,567.06,15.27,8.64" target="#b9">[10]</ref>, was chosen to be used as the dataset of the competition this year (see Fig. <ref type="figure" coords="2,134.77,579.02,3.60,8.64" target="#fig_1">1</ref>). Additionally, this year, images were classified into 7 mutually exclusive categories, as shown in Table <ref type="table" coords="2,208.43,590.97,3.74,8.64">1</ref>, depending on the type of the radiology exam.</p><p>The number of possible concepts was reduced compared to previous years, by removing concepts with few occurrences, since the large number of concepts in the previous years resulted in the task being difficult for models <ref type="bibr" coords="2,375.50,626.84,15.27,8.64" target="#b14">[15]</ref>. There were 111,156 possible concepts in 2018 <ref type="bibr" coords="3,240.08,373.60,15.27,8.64" target="#b15">[16]</ref>, 5,528 in 2019 <ref type="bibr" coords="3,319.39,373.60,11.62,8.64" target="#b7">[8]</ref> and 3,047 in 2020. We also observed that there were concepts that appeared in every single image of a specific category, but rarely or never appeared in other categories. The Concept Unique Identifiers (CUIs) of these concepts are shown in Table <ref type="table" coords="3,269.80,409.47,3.74,8.64">1</ref>. <ref type="foot" coords="3,277.27,407.80,3.49,6.05" target="#foot_2">7</ref> These concepts rather describe the modality of the respective category. For example, DRPE images are always assigned with C0032743, whose UMLS term is POSITRON-EMISSION TOMOGRAPHY.</p><p>The dataset was split by the organisers to a training set of 64,753 images, a validation set of 15,970 images, and a test set of 3,534 images. For our experiments, we merged the provided training and validation sets and used 10% of the merged data as a development set. We will refer to the remaining 90% of the merged dataset as the training set for the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>This section describes the systems that were used in our submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System 1: 2xCNN+FFNN</head><p>CNN+FNNN (a.k.a. ConceptCXN or DenseNet121+FFNN) <ref type="bibr" coords="3,376.38,602.02,11.45,8.64" target="#b8">[9,</ref><ref type="bibr" coords="3,387.83,602.02,7.64,8.64" target="#b7">8]</ref> is the system that we submitted last year (and was ranked 1st) for the same task. It is a variation of CheXNet <ref type="bibr" coords="3,134.77,625.93,16.60,8.64" target="#b12">[13]</ref> that uses DenseNet-121 <ref type="bibr" coords="3,256.41,625.93,10.58,8.64" target="#b2">[3]</ref>, which is a stack of 120 CNN layers, followed by a feed-forward Neural Network (FFNN) that acts as a classifier layer on top. In the ImageCLEFmed Caption task of 2019, we changed the original FFNN to comprise 5,528 outputs (instead of 14), one per available concept. For the task of 2020, which also comprises different image categories (see Table <ref type="table" coords="4,326.44,395.17,3.60,8.64">1</ref>), we followed the same approach per model (i.e., we employed one model per category). For example, the respective FFNN for the model of category C generates N C outputs, which is the number of all possible concepts in category C. <ref type="foot" coords="4,230.71,429.36,3.49,6.05" target="#foot_3">8</ref> The red bars of Fig. <ref type="figure" coords="4,321.30,431.03,4.98,8.64" target="#fig_2">2</ref>  We trained the model by minimizing the binary cross entropy loss. We used Adam <ref type="bibr" coords="4,134.77,469.58,11.62,8.64" target="#b5">[6]</ref> as our optimizer and decreased the learning rate by a factor of 10 when the loss showed no improvement, following the work of <ref type="bibr" coords="4,333.60,481.54,10.58,8.64" target="#b7">[8]</ref>. We used a batch size of 16 and early stopping with a patience of 3 epochs. For each CNN+FFNN of a specific category (i.e., for each category, we fine-tuned a CNN and an FFNN on top), a classification threshold for all the concepts of the respective category was tuned by optimising the F1 score. Any concepts for which the respective output values exceeded that threshold were assigned to the corresponding image.</p><p>Two of our 2020 submissions consisted of ensembles of CNN+FFNN models, constructed in the following way. We trained 5 models per category, and kept the 2 best performing ones, according to their F1 score. We then created two ensembles, using the UNION and the INTERSECTION of the concepts returned by these two models. Hereafter, these two ensembles will be called 2xCNN+FFNN@U and 2xCNN+FFNN@I, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">System 2: CNN+kNN</head><p>Following our previous work <ref type="bibr" coords="5,257.38,429.97,10.79,8.64" target="#b7">[8,</ref><ref type="bibr" coords="5,268.17,429.97,7.19,8.64" target="#b6">7]</ref>, the goal of our CNN+k-NN model for each test image was to retrieve similar images from the training set. The encoder of this model stemmed from our fine-tuned CNN+FFNN system, hence it is a CNN per category. We employed the output of the last average pooling layer of the CNN to represent each encoded image. <ref type="foot" coords="5,198.02,476.12,3.49,6.05" target="#foot_4">9</ref> The encoded test image was then compared to all the images in the training set (encoded offline), using cosine similarity, and the k nearest images were returned.</p><p>After retrieving the k nearest images, CNN+k-NN returned the r concepts that were most frequently assigned to the k images. We tuned k and r for each category, investigating values from 1 to 200 for k, and values from 1 to 10 for r. During tuning, we also considered two other functions for r. First, we used the average number of concepts in the k images: Second, we used a weighting based on cosine similarity to weigh the concepts:</p><formula xml:id="formula_0" coords="5,281.20,594.85,199.39,30.32">r = 1 k k i=1 n i<label>(1)</label></formula><formula xml:id="formula_1" coords="6,244.89,324.32,235.71,30.32">r = k i=1 cos(g, g i ) k j=1 cos(g, g j )) * n i<label>(2)</label></formula><p>where n i is the number of concepts of the i-th retrieved image, g is the test image, g i is the i-th closest training image, and cos(g, g i ) the cosine similarity between g and g i .</p><p>As with CNN+FFNN, our DenseNet-121 CNN was pretrained on ImageNet and fine-tuned on the ImageCLEFmed Caption dataset. However, we experimented also with adding an attention layer 10 [12] to our CNN. We call this model CNN+kNN@att. We also experimented with fine-tuning our CNN on a large dataset of radiography images called MIMIC-CXR <ref type="bibr" coords="6,237.84,434.70,11.62,8.64" target="#b4">[5]</ref> (before fine-tuning it further on the ImageCLEFmed Caption dataset), but we did not obtain any improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submissions and Results</head><p>In order to decide what models to use for the final submissions, we evaluated all models on our development set. Since images were separated into seven categories, each submission consisted of a model per category, resulting in seven models per submission. Two out of our three submissions employed different instances of the same system (2xCNN+FFNN@U &amp; 2xCNN+FFNN@I), each for a different category, while the third one (called BEST@CATEGORY) combined results from different types of systems for each category.</p><p>The official measure of the competition was F1, macro-averaged over the images, without taking into account the different categories. To generate the predictions for the test set, we merged the training with the development set. We used a held-out set (20% of the merged data) to tune the hyper-parameters of the CNN+FFNN and CNN+k-NN models (see Table <ref type="table" coords="6,208.69,636.50,4.98,8.64">4</ref> and Table <ref type="table" coords="6,257.16,636.50,4.98,8.64">4</ref> for the final values). As shown in Fig. <ref type="figure" coords="6,416.66,636.50,3.74,8.64" target="#fig_6">5</ref>, the best score    <ref type="table" coords="7,174.43,560.21,4.98,8.64">4</ref> presents the scores of our systems on the development and the official test set, along with the official rankings. 2xCNN+FFNN@I was the best. On the other hand, 2xCNN+FFNN@U, which returns the union (instead of the intersection) of the predicted concepts of the models in the ensemble, was ranked much lower. It is worth mentioning that a baseline, which simply returns the concepts always shown per category, achieves very high F1 on the development set. The submission that combined the best model per category (see Table <ref type="table" coords="7,275.47,631.94,4.15,8.64">4</ref>) was ranked 2nd.</p><p>We noticed that models tended to predict only the concepts that always appear in each category, thus we also show statistics regarding the diversity of our submissions Fig. <ref type="figure" coords="8,150.82,316.24,3.36,8.06">6</ref>. The diversity (number of distinct concepts predicted / number of all possible concepts) for each of our submitted models per category.</p><p>for the test set (Fig. <ref type="figure" coords="8,215.71,364.96,3.60,8.64">6</ref>). We define diversity as the total number of distinct concepts the models predicted for a specific category divided by the total number of concepts found in the training set of that category. Observing the output of our models, we noticed that the ones with very low diversities only predicted the concepts that appeared in every image (as shown in Table <ref type="table" coords="8,238.59,412.78,4.15,8.64">1</ref>) of the category they were trained on.</p><p>Table <ref type="table" coords="8,158.22,447.57,3.36,8.06">4</ref>. The best models for each category according to the development scores, along with the hyper-parameter values used for the final submissions. t1 and t2 are the classification thresholds of the two models included in 2xCNN+FFNN, k is the number of the nearest images and r is the number of concepts or the function that defines the number of concepts. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,317.09,345.83,8.12;3,134.77,328.40,345.82,7.77;3,134.77,339.36,71.95,7.77;3,134.77,115.83,345.84,186.52"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Two images from ImageCLEFmed Caption 2020, with their gold CUIs. On the left is an image from a Computer Tomography (CT) and on the right is an image from a Positron Emission Tomography (PET).</figDesc><graphic coords="3,134.77,115.83,345.84,186.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,172.10,319.65,271.15,8.12;4,134.77,115.84,345.85,189.08"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Statistics regarding the number of images and concepts per category.</figDesc><graphic coords="4,134.77,115.84,345.85,189.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,329.24,430.71,151.35,9.65;4,134.77,442.99,115.94,8.64"><head></head><label></label><figDesc>depict the N C values per category C, computed on the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,147.98,357.75,319.41,8.12;5,134.77,115.83,345.83,227.19"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Boxplots illustrating the number of gold concepts for the images of each category.</figDesc><graphic coords="5,134.77,115.83,345.83,227.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,139.60,268.14,336.17,8.12;6,134.77,115.83,345.82,137.57"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of CNN+k-NN [8] for the DRAN category (we use DRAN as an example).</figDesc><graphic coords="6,134.77,115.83,345.82,137.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,162.87,290.31,289.62,8.12;7,168.23,115.84,276.67,171.08"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The top F1 score achieved each year in the ImageCLEFmed Caption task.</figDesc><graphic coords="7,168.23,115.84,276.67,171.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,154.75,496.00,33.11,6.96;8,215.29,496.31,21.93,6.71;8,253.56,496.31,19.79,6.71;8,302.62,496.31,21.51,6.71;8,349.18,496.31,85.63,6.71;8,451.08,496.31,21.51,6.71;8,151.41,505.81,39.78,6.96;8,214.37,506.11,125.86,6.71;8,355.96,506.11,38.84,6.71;8,414.26,506.11,57.95,6.71;8,139.41,520.39,63.78,6.96;8,212.14,515.67,28.24,6.96;8,214.07,525.13,24.37,6.96;8,251.26,515.67,24.37,6.96;8,249.33,525.13,28.24,6.96;8,303.45,515.92,19.85,6.71;8,296.07,525.38,34.62,6.71;8,351.51,515.92,15.98,6.71;8,352.16,525.38,14.69,6.71;8,382.18,515.92,15.98,6.71;8,382.83,525.38,14.69,6.71;8,410.52,515.67,28.24,6.96;8,410.52,525.13,28.24,6.96;8,447.71,515.67,28.25,6.96;8,447.71,525.13,28.25,6.96;8,134.77,571.39,345.82,8.12;8,134.77,582.70,329.24,7.77;8,144.21,597.58,36.89,7.75;8,198.84,597.91,24.44,7.48;8,241.47,597.91,22.05,7.48;8,281.95,597.91,23.96,7.48;8,323.87,597.91,23.01,7.48;8,364.10,597.91,25.40,7.48;8,406.91,597.91,22.66,7.48;8,447.70,597.91,23.96,7.48;8,139.94,613.82,45.42,7.75;8,195.33,608.56,31.47,7.75;8,197.48,619.11,27.15,7.75;8,238.92,608.56,27.16,7.75;8,236.76,619.11,31.47,7.75;8,278.20,608.56,31.47,7.75;8,278.20,619.11,31.47,7.75;8,319.64,608.56,31.47,7.75;8,321.79,619.11,27.16,7.75;8,361.07,608.56,31.47,7.75;8,361.07,619.11,31.47,7.75;8,402.51,608.56,31.47,7.75;8,402.51,619.11,31.47,7.75;8,443.95,608.56,31.47,7.75;8,443.95,619.11,31.47,7.75"><head>Table 5 .</head><label>5</label><figDesc>The thresholds used in our 2xCNN+FFNN ensembles, one for each CNN+FFNN. The two CNN+FFNN models and their corresponding thresholds are different for each category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.77,311.56,345.82,110.56"><head>Table 2 .</head><label>2</label><figDesc>The F1 scores of our submitted models, measured on the development set.</figDesc><table coords="7,134.77,326.80,345.82,95.31"><row><cell></cell><cell>DRAN DRPE DRCO DRCT DRMR DRUS DRXR</cell><cell>ANY</cell></row><row><cell cols="2">BEST@CATEGORY 0.3012 0.2485 0.1650 0.4456 0.3413 0.3093 0.3656</cell><cell>0.3715</cell></row><row><cell>2xCNN+FFNN@U</cell><cell>0.3012 0.2485 0.1554 0.4455 0.3388 0.3063 0.3651</cell><cell>0.3704</cell></row><row><cell>2xCNN+FFNN@I</cell><cell>0.2995 0.2388 0.1560 0.4456 0.3400 0.3093 0.3656</cell><cell>0.3710</cell></row><row><cell cols="3">improves every year. This is probably because the task was indeed simplified each year</cell></row><row><cell>(see Section 2).</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,445.58,345.83,98.03"><head>Table 3 .</head><label>3</label><figDesc>The results and rankings of our systems on the development and test set. The baseline of the last row only predicts the concepts that always appear in the images of the category.</figDesc><table coords="7,198.73,477.51,215.15,66.10"><row><cell>Approach</cell><cell>F1 Score Development</cell><cell>Test</cell><cell>Ranking</cell></row><row><cell>BEST@CATEGORY</cell><cell>0.3715</cell><cell>0.3933</cell><cell>2</cell></row><row><cell>2XCNN+FFNN@U</cell><cell>0.3704</cell><cell>0.3870</cell><cell>6</cell></row><row><cell>2XCNN+FFNN@I</cell><cell>0.3710</cell><cell>0.3940</cell><cell>1</cell></row><row><cell>BASELINE</cell><cell>0.3666</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,149.71,560.21,21.89,8.64"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0" coords="2,152.70,645.91,298.45,7.77"><p>Our best performing system will become available in the bioCaption PyPi package.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1" coords="2,152.70,657.93,177.53,6.31"><p>https://www.ncbi.nlm.nih.gov/pmc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2" coords="3,152.70,646.13,327.89,7.77;3,134.77,657.08,29.39,7.77"><p>We used UMLS Metathesaurus (uts.nlm.nih.gov/home.html) to map each CUI to its term.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_3" coords="4,152.70,646.13,327.89,7.77;4,134.77,657.08,135.72,7.77"><p>We did not use image augmentation this year due to time restrictions, because of the large number of models we needed to train.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_4" coords="5,152.70,646.13,327.89,7.77;5,134.77,657.08,37.60,7.77"><p>Each image is rescaled to 224x224 and normalised with the mean and standard deviation of ImageNet.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.61,272.35,337.98,7.77;9,150.95,282.96,231.23,8.12" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,212.63,272.35,267.96,7.77;9,150.95,283.31,22.77,7.77">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,179.76,283.31,80.66,7.77">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2004-01">Jan 2004</date>
		</imprint>
	</monogr>
	<note>Database issue</note>
</biblStruct>

<biblStruct coords="9,142.61,294.40,337.98,7.77;9,150.95,305.36,329.64,7.77;9,150.95,315.97,329.64,8.12;9,150.95,327.28,57.53,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,447.72,294.40,32.87,7.77;9,150.95,305.36,329.64,7.77;9,150.95,316.32,134.00,7.77">Diagnostic radiology resident and fellow workloads: a 12-year longitudinal trend analysis using national medicare aggregate claims data</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">H</forename><surname>Chokshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Mullins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">D</forename><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,290.61,316.32,166.09,7.77">Journal of the American College of Radiology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="664" to="669" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,338.36,337.98,7.77;9,150.95,349.32,329.64,7.77;9,150.95,360.28,202.98,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,372.80,338.36,107.79,7.77;9,150.95,349.32,56.23,7.77">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,227.06,349.32,253.53,7.77;9,150.95,360.28,42.24,7.77">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,371.37,337.98,7.77;9,150.95,382.33,329.64,7.77;9,150.95,393.29,329.64,7.77;9,150.95,404.25,329.64,7.77;9,150.95,415.21,329.64,7.77;9,150.95,426.17,329.64,7.77;9,150.95,437.13,329.64,7.77;9,150.95,448.08,329.64,7.77;9,150.95,459.04,329.64,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,395.71,415.21,84.88,7.77;9,150.95,426.17,326.13,7.77">Overview of the Image-CLEF 2020: Multimedia retrieval in medical, lifelogging, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,163.78,437.13,316.82,7.77;9,150.95,448.08,258.75,7.77">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="9,457.68,448.08,22.92,7.77;9,150.95,459.04,125.10,7.77">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="9,142.61,470.13,337.98,7.77;9,150.95,481.09,329.64,7.77;9,150.95,492.05,160.10,7.77" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">R</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ying Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Horng</surname></persName>
		</author>
		<title level="m" coord="9,329.53,481.09,151.07,7.77;9,150.95,492.05,133.96,7.77">Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,503.14,337.98,7.77" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,227.17,503.14,161.18,7.77">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,514.23,337.98,7.77;9,150.95,525.19,329.64,7.77;9,150.95,536.15,329.64,7.77;9,150.95,547.10,112.32,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,331.33,514.23,149.26,7.77;9,150.95,525.19,10.28,7.77">A Survey on Biomedical Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,180.51,525.19,300.09,7.77;9,150.95,536.15,284.53,7.77">Workshop on Shortcomings in Vision and Language of the Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="26" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,558.19,337.98,7.77;9,150.95,569.15,329.64,7.77;9,150.95,580.11,100.81,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,336.37,558.19,144.21,7.77;9,150.95,569.15,28.40,7.77">AUEB NLP Group at ImageCLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,221.17,569.15,216.41,7.77">CLEF2019 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,591.20,337.98,7.77;9,150.95,602.16,329.64,7.77;9,150.95,613.12,329.64,7.77;9,150.95,624.08,127.24,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,332.94,591.20,147.65,7.77;9,150.95,602.16,61.84,7.77">Medical Image Tagging by Deep Learning and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,232.45,602.16,248.15,7.77;9,150.95,613.12,329.64,7.77;9,150.95,624.08,19.30,7.77">Experimental IR Meets Multilinguality, Multimodality, and Interaction Proceedings of the Eleventh International Conference of the CLEF Association (CLEF 2020)</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.24,635.17,338.35,7.77;9,150.95,646.13,329.64,7.77;9,150.95,657.08,304.07,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,371.50,635.17,109.09,7.77;9,150.95,646.13,140.26,7.77">Radiology Objects in COntext (ROCO): A Multimodal Image Dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,310.08,646.13,170.51,7.77;9,150.95,657.08,167.43,7.77">MICCAI Workshop on Large-scale Annotation of Biomedical data and Expert Label Synthesis</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,119.96,338.35,7.77;10,150.95,130.92,329.64,7.77;10,150.95,141.88,329.64,7.77;10,150.95,152.84,87.65,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,404.26,119.96,76.33,7.77;10,150.95,130.92,270.13,7.77">Overview of the Im-ageCLEFmed 2020 concept prediction task: Medical image understanding</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,440.74,130.92,39.86,7.77;10,150.95,141.88,233.85,7.77">CLEF2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,163.80,338.35,7.77;10,150.95,174.76,91.65,7.77" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,244.36,163.80,236.23,7.77;10,150.95,174.76,65.50,7.77">Feed-forward networks with attention can solve some long-term memory problems</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,185.71,338.35,7.77;10,150.95,196.67,307.83,7.77" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,376.58,185.71,104.01,7.77;10,150.95,196.67,210.32,7.77">CheXNet: Radiologist-Level Pneumonia Detection on Chest X-rays with Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mehta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05225</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,207.63,338.35,7.77;10,150.95,218.24,99.86,8.12" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,200.06,207.63,249.31,7.77">Radiologist shortage leaves patient care at risk, warns royal college</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,456.17,207.63,24.42,7.77;10,150.95,218.59,58.02,7.77">British Medical Journal</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,229.55,338.35,7.77;10,150.95,240.51,329.64,7.77;10,150.95,251.47,210.41,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,297.68,229.55,182.91,7.77;10,150.95,240.51,184.50,7.77">Biomedical Concept Detection in Medical Images: MQ-CSIRO at 2019 ImageCLEFmed Caption Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">H S</forename><surname>Hamey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,354.53,240.51,95.63,7.77">CLEF2019 Working Notes</title>
		<title level="s" coord="10,456.68,240.51,23.91,7.77;10,150.95,251.47,81.11,7.77">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,262.43,338.35,7.77;10,150.95,273.39,329.64,7.77;10,150.95,284.34,105.18,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,287.37,262.43,193.22,7.77;10,150.95,273.39,113.10,7.77">ImageSem at ImageCLEF 2018 Caption Task: Image Retrieval and Transfer Learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,282.47,273.39,94.92,7.77">CLEF2018 Working Notes</title>
		<title level="s" coord="10,383.57,273.39,97.03,7.77;10,150.95,284.34,13.75,7.77">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
