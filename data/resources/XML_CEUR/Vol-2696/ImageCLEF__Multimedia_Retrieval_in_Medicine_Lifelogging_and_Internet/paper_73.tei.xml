<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,190.76,115.96,233.83,12.62;1,148.45,133.89,318.45,12.62;1,192.73,151.82,229.90,12.62">Shengyan at VQA-Med 2020: An Encoder-Decoder Model for Medical Domain Visual Question Answering Task</title>
				<funder ref="#_T4NkXA3">
					<orgName type="full">NSF of Yunnan Province</orgName>
				</funder>
				<funder ref="#_dEJkGUE">
					<orgName type="full">Natural Science Foundations of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,200.23,189.66,57.80,8.74"><forename type="first">Shengyan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650091</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.68,189.66,52.98,8.74"><forename type="first">Haiyan</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650091</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,346.07,188.09,65.84,10.31"><forename type="first">Xiaobing</forename><surname>Zhou</surname></persName>
							<email>zhouxb@ynu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650091</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,190.76,115.96,233.83,12.62;1,148.45,133.89,318.45,12.62;1,192.73,151.82,229.90,12.62">Shengyan at VQA-Med 2020: An Encoder-Decoder Model for Medical Domain Visual Question Answering Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8381E0737B3929E42A1F02D0234AE7C4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>VQA-Med</term>
					<term>VGG16</term>
					<term>Seq2seq</term>
					<term>GRU</term>
					<term>Attention Mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Intelligent learning and understanding of image and text information are important research directions for the successful application of deep learning in computer vision (CV) and natural language processing (NLP). This paper takes medical images and questions as the research objects, by extracting the feature information contained in the medical images and questions and combining with the attention mechanism makes the computer system to more accurately obtain the information expressed by the images. Then, the model predicts the answers to the questions about the images. This paper proposes a novel model for the ImageCLEF VQA-Med 2020 task <ref type="bibr" coords="1,298.79,373.91,9.22,7.86" target="#b0">[1]</ref>. In this model, we use the improved pre-trained VGG16 to extract image features, and GRU module to extract text features of the questions. Then the structure of Seq2seq, including encoding and decoding parts, is applied to obtain the predicted answers. Our team gets the seventh rank in the ImageCLEF VQA-Med 2020 challenge, and our model achieves accuracy score and BLEU score of 0.376 and 0.412 respectively, in the competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid development of CV and NLP, visual question answering (VQA) has become one of the increasingly popular research areas in deep learning <ref type="bibr" coords="1,150.81,555.19,9.96,8.74" target="#b1">[2]</ref>. VQA technology is a comprehensive technology that combines CV, natural language understanding, knowledge representation and reasoning. Compared with specific artificial intelligence technologies such as image processing, text processing, and NLP, VQA is a frontier for general artificial intelligence research explore <ref type="bibr" coords="1,169.21,603.01,9.96,8.74" target="#b2">[3]</ref>. Because it includes two parts of content about artificial intelligence, i.e., image processing and NLP. In the field of NLP, language-based question answering has been extensively studied and great achievements have been made. However, question answering systems involving vision is rarely known. VQA is an interdisciplinary research direction. Its main purpose is to automatically answer natural language questions based on relevant visual content (pictures or videos). It is one of the key research directions in the field of artificial intelligence in the future. Most of the VQA technology is applied to some random scenes containing objects or people, and then generates questions related to the image based on the image content, finally, the VQA system gives the answers to the questions. In general, applying the attention mechanism or target detection (similar to Fast-RCNN <ref type="bibr" coords="2,276.06,226.59,10.79,8.74" target="#b3">[4]</ref>) method in the above-mentioned scenarios is more effective, but because the medical datasets lack labels and artificially divided candidate bounding boxes, there are no special pre-trained models on large medical datasets, so VQA tasks in the medical field are still difficult challenges. This paper describes the implementation of VQA in the medical field. The model we propose in this paper is a multi-classification one. The answers in the training set are extracted as candidate answer sets. The answers are divided into a simple yes/no, one word, and sentences composed of multiple words. The model uses CNN and RNN to extract the image and text features respectively, and uses these two parts of features as input to the next step, where the image is not the original one but a pre-processed one when it is input to the network. The purpose is to reduce noise in the image. The structure of this paper is organized as below.</p><p>The next chapter briefly describes the relevant work and summarizes the methods used in this model. Chapter 3 describes our proposed method and dataset. Chapter 4 introduces our model in detail. Chapter 5 describes the experimental results and model evaluation results, and Chapter 6 is the summary of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>After reading a lot of literature, we found that the implementation of VQA technology is generally based on deep learning, and deep neural networks are also the most effective method to achieve VQA tasks. VQA tasks are roughly divided into two aspects. First of all, CNN is generally used to extract image features, such as VGGNet, Resnet, Inception, Googlenet, and so on. Pre-trained deep learning networks by ImageNet <ref type="bibr" coords="2,302.70,548.53,10.52,8.74" target="#b4">[5]</ref> have obtained good results on many traditional VQA datasets, such as COCO-QA <ref type="bibr" coords="2,333.65,560.48,9.96,8.74" target="#b5">[6]</ref>, Visual7W <ref type="bibr" coords="2,394.85,560.48,9.96,8.74" target="#b6">[7]</ref>. While we do not have a deep learning model pre-trained on large medical datasets, so we can only use the ImageNet pre-trained model and improve it. Secondly, RNN is used to extract the feature of the text processed through embedding layer. In this paper, the structure of seq2seq and the attention mechanism are applied. The attention mechanism first appeared in the Deepmind team using it to help classify images on the RNN model <ref type="bibr" coords="2,223.28,632.21,9.96,8.74" target="#b7">[8]</ref>, and achieved good results. Subsequently, Bahdanau et al. <ref type="bibr" coords="2,148.64,644.17,10.52,8.74" target="#b8">[9]</ref> proposed the use of attention mechanism in machine translation tasks to complete machine translation and alignment work, which also achieved a huge breakthrough. The sequence to sequence (Seq2seq) <ref type="bibr" coords="3,364.66,119.00,15.50,8.74" target="#b9">[10]</ref> method was proposed by the Google team in 2014. The basic idea of seq2seq is to use two RNNs, one RNN as the encoder, and the other as the decoder. We proposed an Xception-GRU model in the ImageCLEF VQA-Med 2019 task <ref type="bibr" coords="3,365.85,154.86,15.50,8.74" target="#b10">[11]</ref> last year, in which the Xception network was applied to the image feature extraction part, and the GRU model was applied to the text feature extraction part, these two parts of features were respectively passed through the attention module and the feature fusion module, and finally predicted answers after softmax layer. The model achieved accuracy and BLEU scores of 0.21 and 0.393 at ImageCLEF VQA-Med 2019 task and got the fifteen rank last year. Based on the new data set, we have made a little progress and gets the seventh rank in this year's ImageCLEF VQA-Med 2020 task. We will introduce this new data set, ImageCLEF VQA-Med 2020 dataset, in the following chapter 3 dataset description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Description</head><p>The dataset in this paper is from the ImageCLEF VQA-Med 2020 task, which is divided into three parts, training set, validation set and test set as shown in Table <ref type="table" coords="3,162.75,379.90,3.87,8.74" target="#tab_0">1</ref>. Compared to last year's data set, the pattern for this year's data set is one image for multiple questions instead of one image for one question last year, and this year's data set is not divided into four categories last year's data set, but the type of this year's questions is closer to last year's abnormality class questions. It is also the hardest class of questions to deal with, because the answers to the corresponding questions are not very regular. In continuation of the two previous editions, this year's task on VQA-Med consists in answering natural language questions from the visual content of associated radiology images, it focuses particularly on questions about abnormalities <ref type="bibr" coords="3,134.76,616.48,14.61,8.74" target="#b11">[12]</ref>.</p><p>There are two examples of medical images and associated questions and answers from the training set of ImageCLEF VQA-Med 2020, as shown in Figure <ref type="figure" coords="3,134.76,656.12,3.87,8.74" target="#fig_0">1</ref>: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model prediction</head><p>This paper proposes an Encoder-Decoder model. The answers from the training set are extracted to form a candidate answer set. There are a total of 333 candidate answers. All we have to do is to let the model assign a predicted probability value to each answer word in this candidate answer set. The output module consists of GRU network that takes the thought vector which includes question and image features as initial state. &lt; SOS &gt; token is taken as input in the first time step, then the GRU network tries to predict the answer using softmax layer. This method can be expressed as a mathematical formula:</p><formula xml:id="formula_0" coords="4,255.30,510.82,221.05,8.74">y = argmax P (a|q, i, m), (<label>1</label></formula><formula xml:id="formula_1" coords="4,476.35,510.82,4.24,8.74">)</formula><p>where y is the candidate answer word option with the highest probability predicted by the model, q is the answer to the question, i is the image corresponding to the question, m provides all parameters of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sequence to sequence</head><p>The model we propose in this paper uses the sequence to sequence method. The general structure of this method is composed of an encoding module and a decoding module, as shown in the Figure <ref type="figure" coords="4,317.14,656.12,3.87,8.74" target="#fig_1">2</ref>: The encoder is responsible for compressing the input sequence into a vector of a specified length. This vector can be regarded as the semantics of the sequence. This process is called encoding. As shown in the Figure <ref type="figure" coords="5,383.24,299.98,3.87,8.74" target="#fig_1">2</ref>, the simplest way to obtain the semantic vector is to directly use the hidden state of the last input as semantic vector C. It can perform a transformation on the last hidden state to obtain a semantic vector, and also perform a transformation on all the hidden states of the input sequence to obtain a semantic vector. The calculation formula is:</p><formula xml:id="formula_2" coords="5,247.10,390.34,233.49,9.65">C = q(h 1 , h 2 , h 3 , h tx ) = h tx ,<label>(2)</label></formula><p>where h i represents the output of each hidden layer, C is the state of the last input h tx . The decoder is responsible for generating the specific sequence based on the semantic vector. This process is called decoding. As shown in the Figure <ref type="figure" coords="5,455.58,450.81,3.87,8.74" target="#fig_1">2</ref>, the simplest way is to input the semantic variables obtained by the encoder as the initial state into the decoder's RNN to obtain the output sequence. It can be seen that the output of the previous moment will be used as the input of the current moment, and the semantic vector C only participates in the operation as the initial state, and the subsequent operations are independent of the semantic vector C. The calculation formula is:</p><formula xml:id="formula_3" coords="5,267.05,549.40,213.54,14.33">y i = g(y i-1 , h ′ i , C),<label>(3)</label></formula><p>where y i-1 is the output of the previous step, h ′ i is the output of the hidden layer, and g represents the nonlinear activation function.</p><p>The following symbols are represented as inputs at the decoding stage: &lt; P AD &gt;: Complete characters. &lt; EOS &gt;: End-of-sentence identifier on the decoder side. &lt; U N K &gt;: Low-frequency words or some words have not encountered so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; SOS &gt;:</head><p>The start identifier of the sentence on the decoder side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Encoder In terms of image feature extraction in the encoding module, we use an improved VGG16 model <ref type="bibr" coords="6,256.41,147.11,15.50,8.74" target="#b12">[13]</ref> to extract image features, as shown in Figure <ref type="figure" coords="6,472.86,147.11,3.87,8.74" target="#fig_2">3</ref>: Extracting image features refers to inputting an image preprocessed in form of pixels into a feature vector with high-level semantic information. Convolutional neural networks as feature extractors are all standard models proposed in ImageNet image recognition tasks, and CNN models can be used to indirectly used a large amount of training data on ImageNet to perform better feature extraction on images. This paper uses the pre-training VGG-16 model as the visual feature extractor of the images. Since the last two layers have entered the classification step, we need the complete output image features, so the last two layers are removed, and the 4096-dimensional features are extracted from the fully connected layer, and then the output feature vector passes through an attention module <ref type="bibr" coords="6,228.03,488.75,14.61,8.74" target="#b13">[14]</ref>. Because the mapping relationship between the global features of the image and the sentences is not enough, it will bring a lot of noise signals. We need to extract the local features of the image, which requires us to use the attention mechanism to find the relationship between local image features. The basic unit of sentences can better complete the task from images to sentences so that images can be better combined with text features at the semantic level.</p><p>In terms of text feature extraction, we input the question text into RNN after Glove Embedding <ref type="bibr" coords="6,215.89,584.39,14.61,8.74" target="#b14">[15]</ref>, and then summarize the output of each hidden layer to generate a semantic vector. GRU <ref type="bibr" coords="6,284.02,596.35,15.50,8.74" target="#b15">[16]</ref> is a variant of LSTM <ref type="bibr" coords="6,399.43,596.35,14.61,8.74" target="#b16">[17]</ref>, which cancels the cell state in LSTM and only uses Hidden state, and use the update gate to replace the input gates and forget gate in the LSTM, cancel the output gate in the LSTM, and add the reset gate. The advantage of this structure is that under the premise of achieving similar effect of LSTM, the calculation on training is smaller, and the training speed is faster. Figure <ref type="figure" coords="6,340.16,656.12,4.98,8.74" target="#fig_3">4</ref> is the structure of GRU model. The forward propagation formula of GRU is as follows:</p><formula xml:id="formula_4" coords="7,245.24,290.61,235.35,59.39">z t = σ(W z • [h t-1 , x t ]) r t = σ(W r • [h t-1 , x t ]) h t = tanh(W • [r t * h t-1 , x t ]) h t = (1 -z t ) * h t-1 + z t * h t (4)</formula><p>where z t is the update gate, which is the logic gate when updating activation, r t is the reset gate, whether to give up the previous activation h t when deciding on candidate activation, h t is candidate activation, receive [x t ,h t-1 ], h t is activate gate, which is the hidden layer of GRU, receive [h t-1 , h t ].</p><p>The following Figure <ref type="figure" coords="7,243.69,413.78,4.98,8.74" target="#fig_4">5</ref> is the model structure of encoding part we used. Decoder The semantic variables obtained by the encoder are input into the GRU of the decoder as the initial state to obtain the output sequence. The output of the previous moment will be used as the input of the current moment, and finally the predicted answer will be output.</p><p>The following is the model structure of decoding part we used. As shown in Figure <ref type="figure" coords="8,236.06,392.75,3.87,8.74" target="#fig_5">6</ref>, in this stacked GRU network, the red curve represents the hidden state information of the previous moment as the input of the next moment, the model input &lt; SOS &gt; represents the start of decoding, the model prediction output &lt; EOS &gt; represents the end of prediction. This is the decoder part of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation and Result</head><p>ImageCLEF VQA-Med 2020 competition implements two evaluation methods, Accuracy (Strict) and BLEU <ref type="bibr" coords="8,281.06,512.27,14.61,8.74" target="#b17">[18]</ref>. It uses an adapted version of the accuracy metric from the general domain VQA task that considers exact matching of a participant provided answer and the ground truth answer and uses the BLEU metric to capture the similarity between a system-generated answer and the ground truth answer.</p><p>The implementation of the BLEU method is to calculate the N-grams model of the candidate sentence and the reference sentence <ref type="bibr" coords="8,390.10,584.39,14.61,8.74" target="#b18">[19]</ref>. Each answer is pre-processed in the following way: The caption is converted to lower-case; All punctuation is removed an the caption is tokenized into its individual words; Stopwords are removed using NLTK's "english" stopword list; Stemming is applied using NLTK's Snowball stemmer. The answer is always considered as a single sentence, even if it actually contains several sentences. <ref type="bibr" coords="8,399.28,644.17,10.52,8.74" target="#b0">[1]</ref> And then count the number of matches to calculate. This method has nothing to do with the word order. Based on the model and method mentioned above, we submitted five results in the competition. The results of the competition have been shown in Table <ref type="table" coords="9,173.78,142.91,3.87,8.74" target="#tab_1">2</ref>. Our team ID is "Shengyan". As shown in the Table <ref type="table" coords="9,255.30,372.71,3.87,8.74" target="#tab_2">3</ref>, the Xception+GRU model was proposed in last year's competition by our team. The traditional CNN and RNN models were used in image processing and text processing respectively, which did not perform very well in this year's dataset. This year, we mainly introduced the encoding and decoding structure of seq2seq and made ablation experiments based on last year's model. We can see that the model with seq2seq construct achieves better accuracy. The VGG16+GRU+seq2seq model proposed in this paper is improved based on the traditional CNN model to reduce the number of parameters and improve the accuracy. The hyperparameters of this model are set as follows: we set the learning rate to 0.0001 in ADAM optimizer, with dropout = 0.5, epoch = 80 and batchsize = 64. The following is a comparison of the results of all the experiments we performed. It can be seen that the VGG16-seq2seq model is the best in this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper describes the model we use in the ImageCLEF VQA-Med 2020 competition. We use the seq2seq framework to input feature and predict answers. The image feature extraction part uses the improved VGG16 model. The text feature extraction uses the GRU model, and finally achieves the accuracy score of 0.376, and BLEU score of 0.412. We will improve the model and combine the attention mechanism in the Seq2seq structure to continuously improve the accuracy. Besides, our future work includes: (1) Image and natural language are signals of two modalities. How to fully integrate these two modalities belongs to the task of multi-modality fusion, which requires us to design a type that can fully learn the relationship between different modalities. (2) If the image visual features and the text features of the questions are directly fused, there will be a semantic level mismatch, so we will design a model to handle this question and improve the accuracy of VQA system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.76,271.82,345.82,7.89;4,134.76,282.81,193.21,7.86;4,169.35,115.89,276.57,141.16"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Two examples of medical images and associated questions and answers from the training set of ImageCLEF VQA-Med 2020.</figDesc><graphic coords="4,169.35,115.89,276.57,141.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,200.09,235.34,215.17,7.89;5,169.35,115.86,276.58,104.71"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The basic structure of encoding and decoding</figDesc><graphic coords="5,169.35,115.86,276.58,104.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,213.46,335.36,188.44,7.89;6,186.64,178.24,242.00,142.35"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The basic structure of VGG16 network</figDesc><graphic coords="6,186.64,178.24,242.00,142.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,233.69,228.95,147.97,7.89;7,238.51,115.85,138.30,98.33"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The structure of GRU model</figDesc><graphic coords="7,238.51,115.85,138.30,98.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,236.44,655.03,142.46,7.89;7,141.68,448.06,332.01,192.20"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Encoding part of the model</figDesc><graphic coords="7,141.68,448.06,332.01,192.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,236.57,355.84,142.21,7.89;8,160.70,200.41,293.96,140.66"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Decoding part of the model</figDesc><graphic coords="8,160.70,200.41,293.96,140.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,169.99,478.72,275.38,62.73"><head>Table 1 .</head><label>1</label><figDesc>Statistics of VQA-Med data.</figDesc><table coords="3,169.99,499.52,275.38,41.93"><row><cell></cell><cell>Training</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>Images</cell><cell>3000</cell><cell>500</cell><cell>500</cell></row><row><cell>Questions</cell><cell>3000</cell><cell>500</cell><cell>500</cell></row><row><cell>Answers</cell><cell>3000</cell><cell>500</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,185.51,182.99,244.34,146.44"><head>Table 2 .</head><label>2</label><figDesc>Official results of ImageCLEF VQA-Med 2020.</figDesc><table coords="9,185.51,206.98,244.34,122.45"><row><cell>Participants</cell><cell>Accuracy</cell><cell>BLEU</cell></row><row><cell>z liao</cell><cell>0.496</cell><cell>0.542</cell></row><row><cell>TheInceptionTea</cell><cell>0.480</cell><cell>0.511</cell></row><row><cell>bumjun jung</cell><cell>0.466</cell><cell>0.502</cell></row><row><cell>going</cell><cell>0.426</cell><cell>0.462</cell></row><row><cell>NLM</cell><cell>0.400</cell><cell>0.441</cell></row><row><cell>harendrakv</cell><cell>0.378</cell><cell>0.439</cell></row><row><cell>Shengyan</cell><cell>0.376</cell><cell>0.412</cell></row><row><cell>kdevqa</cell><cell>0.314</cell><cell>0.350</cell></row><row><cell>sheerin</cell><cell>0.282</cell><cell>0.330</cell></row><row><cell>umassmednlp</cell><cell>0.220</cell><cell>0.340</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,170.10,556.26,275.16,102.60"><head>Table 3 .</head><label>3</label><figDesc>Results of our experiments on test set</figDesc><table coords="9,170.10,580.25,275.16,78.61"><row><cell>model</cell><cell>Accuracy</cell><cell>BLEU</cell></row><row><cell>VGG16+GRU</cell><cell>0.28</cell><cell>0.35</cell></row><row><cell>Xception+GRU</cell><cell>0.21</cell><cell>0.39</cell></row><row><cell>Xception+GRU+seq2seq</cell><cell>0.30</cell><cell>0.40</cell></row><row><cell>GoogleNet+GRU+seq2seq</cell><cell>0.26</cell><cell>0.36</cell></row><row><cell>VGG16+LSTM+seq2seq</cell><cell>0.34</cell><cell>0.41</cell></row><row><cell>VGG16+GRU+seq2seq</cell><cell>0.376</cell><cell>0.412</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">Natural Science Foundations of China</rs> under Grants <rs type="grantNumber">61463050</rs>, the <rs type="funder">NSF of Yunnan Province</rs> under Grant <rs type="grantNumber">2015FB113</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dEJkGUE">
					<idno type="grant-number">61463050</idno>
				</org>
				<org type="funding" xml:id="_T4NkXA3">
					<idno type="grant-number">2015FB113</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.95,414.86,337.63,7.86;10,151.52,425.82,329.07,7.86;10,151.52,436.78,329.06,7.86;10,151.52,447.74,329.06,7.86;10,151.52,458.70,329.06,7.86;10,151.52,469.66,329.06,7.86;10,151.52,480.62,329.06,7.86;10,151.52,491.58,329.07,7.86;10,151.52,502.53,329.07,7.86;10,151.52,513.49,329.06,7.86;10,151.52,524.45,329.06,7.86;10,151.52,535.41,329.06,7.86;10,151.52,546.37,36.39,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,259.13,491.58,221.46,7.86;10,151.52,502.53,220.44,7.86">Overview of the ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Van-Tu</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tu-Khiem</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liting</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pål</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Triet</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dimitri</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raul</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liviu</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Daniel S ¸tefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Constantin</forename><surname>Gabriel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,389.68,502.53,90.91,7.86;10,151.52,513.49,195.09,7.86;10,423.52,513.49,57.07,7.86;10,151.52,524.45,291.45,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="10,307.10,535.41,169.52,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="10,142.95,557.54,337.63,7.86;10,151.52,568.50,329.06,7.86;10,151.52,579.46,104.04,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,151.52,568.50,329.06,7.86;10,151.52,579.46,74.75,7.86">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,590.63,337.64,7.86;10,151.52,601.59,318.65,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,315.65,601.59,125.22,7.86">Vqa: Visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,612.76,217.34,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,213.30,612.76,39.79,7.86">Fast r-cnn</title>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,623.93,337.63,7.86;10,151.52,634.88,329.07,7.86;10,151.52,645.84,329.06,7.86;10,151.52,656.80,113.02,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,405.71,623.93,74.87,7.86;10,151.52,634.88,134.93,7.86">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><forename type="middle">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,310.35,634.88,170.24,7.86;10,151.52,645.84,248.69,7.86">2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009)</title>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06-25">20-25 June 2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,119.68,337.63,7.86;11,151.52,130.64,130.67,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="11,352.52,119.68,128.06,7.86;11,151.52,130.64,101.37,7.86">Exploring models and data for image question answering</title>
		<author>
			<persName coords=""><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,141.60,337.63,7.86;11,151.52,152.55,145.05,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,397.95,141.60,82.63,7.86;11,151.52,152.55,115.73,7.86">Visual7w: Grounded question answering in images</title>
		<author>
			<persName coords=""><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,163.51,337.63,7.86;11,151.52,174.47,275.64,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,356.14,163.51,124.45,7.86;11,151.52,174.47,145.46,7.86">Recurrent models of visual coattention for person re-identification</title>
		<author>
			<persName coords=""><forename type="first">Lan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,304.72,174.47,49.76,7.86">IEEE Access</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,185.43,337.63,7.86;11,151.52,196.39,296.13,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,390.83,185.43,89.76,7.86;11,151.52,196.39,189.05,7.86">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName coords=""><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,348.46,196.39,71.10,7.86">Computer Science</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,207.35,337.97,7.86;11,151.52,218.31,301.43,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,339.21,207.35,141.37,7.86;11,151.52,218.31,62.15,7.86">Sequence to sequence learning with neural networks</title>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,222.01,218.31,202.73,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,229.27,337.97,7.86;11,151.52,240.23,329.06,7.86;11,151.52,251.19,329.08,7.86;11,151.52,262.14,329.06,7.86;11,151.52,273.10,94.04,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,277.46,240.23,203.11,7.86;11,151.52,251.19,133.37,7.86">VQA-Med: Overview of the medical visual question answering task at imageclef 2019</title>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,307.24,251.19,173.36,7.86;11,151.52,262.14,69.63,7.86">CLEF 2019 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,284.06,337.97,7.86;11,151.52,295.02,329.05,7.86;11,151.52,305.98,329.07,7.86;11,151.52,316.94,329.06,7.86;11,151.52,327.90,62.22,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,224.35,295.02,256.23,7.86;11,151.52,305.98,219.25,7.86">Overview of the vqa-med task at imageclef 2020: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="11,394.64,305.98,85.95,7.86;11,151.52,316.94,145.58,7.86">CLEF 2020 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,338.86,337.97,7.86;11,151.52,349.82,221.65,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,327.38,338.86,153.20,7.86;11,151.52,349.82,114.44,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,273.98,349.82,71.10,7.86">Computer Science</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,360.77,337.97,7.86;11,151.52,371.73,329.06,7.86;11,151.52,382.69,329.07,7.86;11,151.52,393.65,45.05,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,361.93,371.73,118.65,7.86;11,151.52,382.69,190.60,7.86">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,352.19,382.69,71.73,7.86">Computer Science</title>
		<imprint>
			<biblScope unit="page" from="2048" to="2057" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,404.61,337.98,7.86;11,151.52,415.57,329.06,7.86;11,151.52,426.53,234.11,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,407.38,404.61,73.21,7.86;11,151.52,415.57,111.48,7.86">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,283.00,415.57,197.58,7.86;11,151.52,426.53,204.61,7.86">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,437.49,337.97,7.86;11,151.52,448.45,329.06,7.86;11,151.52,459.40,329.07,7.86;11,151.52,470.36,56.26,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,375.38,448.45,105.20,7.86;11,151.52,459.40,279.41,7.86">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,440.95,459.40,39.64,7.86;11,151.52,470.36,28.17,7.86">Computer Science</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,481.32,337.98,7.86;11,151.52,492.28,87.03,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,289.43,481.32,98.08,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,396.35,481.32,79.69,7.86">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,503.24,337.97,7.86;11,151.52,514.20,329.06,7.86;11,151.52,525.16,329.06,7.86;11,151.52,536.12,147.43,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,418.31,503.24,62.27,7.86;11,151.52,514.20,189.04,7.86">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,359.08,514.20,121.50,7.86;11,151.52,525.16,211.09,7.86">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,547.08,337.97,7.86;11,151.52,558.03,329.06,7.86;11,151.52,568.99,329.06,7.86;11,151.52,579.95,102.15,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,428.60,547.08,51.98,7.86;11,151.52,558.03,213.29,7.86">Using google n-grams to expand word-emotion association lexicon</title>
		<author>
			<persName coords=""><forename type="first">Jessica</forename><surname>Perrie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aminul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Milios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vlado</forename><surname>Keselj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,386.54,558.03,94.04,7.86;11,151.52,568.99,329.06,7.86;11,151.52,579.95,27.76,7.86">Proceedings of the 14th international conference on Computational Linguistics and Intelligent Text Processing</title>
		<meeting>the 14th international conference on Computational Linguistics and Intelligent Text Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
