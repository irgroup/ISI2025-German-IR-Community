<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.66,115.96,334.04,12.62;1,140.37,133.89,334.61,12.62">Overview of the ImageCLEFmed 2020 Concept Prediction Task: Medical Image Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,195.87,171.56,61.02,8.74"><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
							<email>obioma.pelka@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Diagnostic and Interventional Radiology and Neuroradiology</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff16">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">SSN College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.01,171.56,52.53,8.74;1,135.57,183.51,38.66,8.74"><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
							<email>christoph.friedrich@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.35,183.51,106.98,8.74"><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Essex</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,234.29,195.47,68.10,8.74"><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<email>henning.mueller@hevs.ch</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="department">School of computer Science and Electronic Engineering</orgName>
								<orgName type="institution">University of Essex</orgName>
								<address>
									<settlement>Essex</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff12">
								<orgName type="institution">TUC MC [</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="4,136.23,396.25,46.09,7.86"><forename type="first">Morgan</forename><surname>Cs</surname></persName>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">PwC Healthcare</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">PricewaterhouseCoopers US Advisory</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<address>
									<settlement>Essex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="laboratory">IML DFKI [</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="department">German Research Center for Artificial Intelligence (DFKI)</orgName>
								<orgName type="laboratory">Interactive Machine Learning Group</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="institution">Technische Unversität Chemnitz</orgName>
								<address>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff14">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Morgan State University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
									<country>United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff15">
								<orgName type="department">CSE SSN</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.66,115.96,334.04,12.62;1,140.37,133.89,334.61,12.62">Overview of the ImageCLEFmed 2020 Concept Prediction Task: Medical Image Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B1EEBA1FCD4D28069A002C91F3301642</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Concept Detection</term>
					<term>Computer Vision</term>
					<term>ImageCLEF 2020</term>
					<term>Image Understanding</term>
					<term>Image Modality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the ImageCLEFmed 2020 Concept Detection Task. After first being proposed at ImageCLEF 2017, the medical task is in its 4th edition this year, as the automatic detection from medical images still remains a challenging task. In 2020, the format remained the same as in 2019, with a single sub-task. The concept detection task is part of the medical tasks, alongside the tuberculosis and visual question and answering tasks. Similar to the 2019 edition, the data set focuses on radiology images rather than biomedical images, however with an increased number of images. The distributed images were extracted from the biomedical open access literature (PubMed Central). The development data consists of 65,753 training and 15,970 validation images. Each image has corresponding Unified Medical Language System (UMLS R ) concepts, that were extracted from the original article image captions. In this edition, additional imaging acquisition technique labels were included in the distributed data, which were adopted for pre-filtering steps, concept selection and ensemble algorithms. Most applied approaches for the automatic detection of concepts were deep learning based architectures. Long short-term memory (LSTM) recurrent neural networks (RNN), adversarial auto-encoder, convolutional neural networks (CNN) image encoders and transfer learning-based multi-label classification models were adopted. The performances of the submitted models (best score 0.3940) were evaluated using F1-scores computed per image and averaged across all 3,534 test images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, the approaches for the detection of Unified Medical Language System (UMLS R ) concepts present in radiology images are presented. The task is part of the ImageCLEF 1 bench-marking campaign, that is part of the Cross Language Evaluation Forum 2 (CLEF). Since 2003, the ImageCLEF bench-marking campaign has been proposing several image understanding tasks from different domains every year <ref type="bibr" coords="2,222.95,252.21,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="2,235.12,252.21,12.73,8.74" target="#b14">15,</ref><ref type="bibr" coords="2,249.52,252.21,11.62,8.74" target="#b10">11]</ref>. Detailed information on other proposed tasks at the ImageCLEF 2020 can be found in Ionescu et al. <ref type="bibr" coords="2,364.10,264.16,9.96,8.74" target="#b8">[9]</ref>.</p><p>The concept detection task in this year is the fourth edition. At Image-CLEFmed Caption 2017 <ref type="bibr" coords="2,243.38,288.60,10.52,8.74" target="#b2">[3]</ref> and ImageCLEFmed Caption 2018 <ref type="bibr" coords="2,410.75,288.60,9.96,8.74" target="#b6">[7]</ref>, the task was comprised of two (2) sub-tasks: concept detection and caption prediction. The format changed in ImageCLEFmed Caption 2019 <ref type="bibr" coords="2,359.56,312.51,15.50,8.74" target="#b15">[16]</ref> with the single task of concept detection and remained that way this year at ImageCLEFmed Caption 2020. New in this edition is that the imaging modality is given for each image both in the development and evaluation sets.</p><p>As there is an increasing number of medical images available without metadata, for example in the scientific literature, there is an essential need to create systems that can automatically generate such information, hence making the content of these data sets more useful. The purpose of the ImageCLEFmed 2020 concept detection task was to create a platform for the evaluation of systems capable of automatically creating UMLS R concepts of a given radiology image. These predicted information is applicable for data sets that either not labeled or structured, but also for medical data sets lacking textual metadata, as multi-modal approaches prove to obtain better results regarding several image classification tasks <ref type="bibr" coords="2,218.29,468.45,15.50,8.74" target="#b17">[18,</ref><ref type="bibr" coords="2,235.44,468.45,11.62,8.74" target="#b18">19]</ref>.</p><p>The manual interpretation and generation of knowledge from medical images is not only time-consuming and prone to error, but also impractical. Therefore, the modeling systems that can automatically map visual content present in the images to concise textual representations is a necessity, in regards to efficient information retrieval and image classification.</p><p>For development data, both the development and test sets from the Image-CLEFmed Caption 2019 <ref type="bibr" coords="2,246.65,553.19,15.50,8.74" target="#b15">[16]</ref> was distributed. This data set is a subset of the Radiology Object in COntext data set (ROCO) <ref type="bibr" coords="2,339.54,565.15,15.50,8.74" target="#b16">[17]</ref> and contains solely radiology images that originate from the PubMed Central (PMC) Open Access Subset 3 <ref type="bibr" coords="2,134.77,589.06,14.61,8.74" target="#b19">[20]</ref>. Several UMLS R Concept Unique Identifiers (CUIs) are included to each image. The test set used for official evaluation was created in the same manner as proposed in Pelka et al. <ref type="bibr" coords="2,253.21,612.97,14.61,8.74" target="#b16">[17]</ref>, for generalization purposes. This paper presents an overview of the ImageCLEFmed 2020 Concept Detection Task. Section 2 contains the task description and lists the participating teams. An explorative analysis computed on the distributed development and test data sets is described in Section 3. The framework used to evaluate the submission runs is explained in Section 4. Section 5 displays the modeling approaches applied by the participating teams and the obtained scores, and is followed by discussion and conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task and Participation</head><p>Similar to the ImageCLEF caption task in 2019 <ref type="bibr" coords="3,353.53,243.95,14.61,8.74" target="#b15">[16]</ref>, in ImageCLEF Caption 2020 the focus is on the automatic detection of concepts in a large corpus of radiology images. The proposed task aims to interpret and summarise insights gained from medical images and therefore provide tools for radiology image understanding. The distributed images in both development and evaluation data sets originate from biomedical articles extracted from the PubMed Central (PMC) Open Access Subset <ref type="bibr" coords="3,220.31,315.68,17.13,8.74" target="#b19">[20]</ref>. To each radiology image in the distributed data sets, UMLS R CUIs are included. These concepts are generated from the the original image captions found in the articles. Figure <ref type="figure" coords="3,329.75,339.59,4.98,8.74" target="#fig_0">1</ref> displays an example of an image in the distributed data sets. In comparison to the previous tasks, the following improvements were made:</p><p>-The imaging modality was included.</p><p>-The focus remained on radiology images as in ImageCLEF 2019 .</p><p>-The number of concepts was decreased by preprocessing the captions prior to concept extraction. The automatic detection of concepts present in images is a fundamental step towards scene understanding and hence image captioning, as the presence of applicable biomedical concepts can be detected and located. As the usage of multi-modal representations (visual and textual) for image classification tasks helps to achieve good performance <ref type="bibr" coords="4,292.92,142.90,14.61,8.74" target="#b18">[19]</ref>, the automatically generated concepts can be adopted for this purpose. In addition, the concepts can also be used for context-based image analysis, as well as for information retrieval. The detected concepts are evaluated image-wise with precision and recall scores from the ground truth, which is described in Section 4.</p><p>Table <ref type="table" coords="4,162.78,221.63,4.13,7.89">1</ref>. Participating groups of the ImageCLEF 2020 Concept Detection Task. Teams with previous participation in 2019 are marked with an asterisk.</p><p>Team Institution Runs AUEB NLP Group* <ref type="bibr" coords="4,136.23,275.71,14.34,7.86" target="#b11">[12]</ref> Department of Informatics, Athens University of Economics and Business, Athens, Greece</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Set</head><p>As in previous editions, the data set distributed for the task originates from biomedical articles of the PMC Open Access subset <ref type="bibr" coords="4,361.84,632.21,14.61,8.74" target="#b19">[20]</ref>. The development data set contains training and validation sets with 65,753 and 15,970 images, respectively. These images are subsets of the multi-modal image data set Radiology Objects in COntext (ROCO), which is presented in Pelka et al. <ref type="bibr" coords="5,428.05,118.99,14.61,8.74" target="#b16">[17]</ref>. ROCO has two classes: Radiology and Out-Of-Class. The first contains 81,825 radiology images and was adopted for the proposed task. It includes several medical imaging modalities such as, Computed Tomography (CT), Ultrasound, X-Ray, Fluoroscopy, Positron Emission Tomography (PET), Mammography, Magnetic Resonance Imaging (MRI), Angiography and PET-CT.</p><p>The development data of the 2020 task includes the ImageCLEF caption 2019 development data set (archiving date: until 31.01.2018) and the official evaluation set (archiving date: 01.02.2018 -01.02.2019). To avoid an overlap with images distributed in previous ImageCLEF medical tasks, the test set for ImageCLEF 2020 was created with a subset of PMC Open Access (archiving date: 01.02.2019 -01.02.2020). The same procedures applied for the creation of the ROCO data set were applied for the test set as well. An analysis of the distributed data can be seen in Table <ref type="table" coords="5,300.95,274.95,3.87,8.74" target="#tab_0">2</ref>. From the PMC Open Access subset <ref type="bibr" coords="5,306.73,464.30,14.61,8.74" target="#b19">[20]</ref>, a total of 6,031,814 image -caption pairs were extracted in January 2018. Compound figures, which are images with more than one subfigure, were removed using deep learning as proposed in Koitka et al. <ref type="bibr" coords="5,159.66,500.16,14.61,8.74" target="#b12">[13]</ref>. The non-compound images were further split into radiology and nonradiology, as the focus was on radiology. Semantic knowledge of object interplay present in the images were extracted in the form of UMLS R Concepts using the QuickUMLS library <ref type="bibr" coords="5,223.13,536.03,14.61,8.74" target="#b22">[23]</ref>. The image captions from the biomedical articles served as basis for the extraction of the concepts. The text pre-processing steps applied are described in Pelka et al. <ref type="bibr" coords="5,264.04,559.94,14.61,8.74" target="#b16">[17]</ref>. Using deep learning systems as proposed in Koitka et al. <ref type="bibr" coords="5,191.94,571.90,14.61,8.74" target="#b12">[13]</ref>, the radiology images were further split into seven <ref type="bibr" coords="5,430.44,571.90,12.73,8.74" target="#b6">(7)</ref> imaging modality classes. This information can be used for filtering steps prior to model training, as well as for model fine-tuning.</p><p>An additional UMLS R CUI denoting the imaging technique modality was added to each image. Figure <ref type="figure" coords="5,268.51,620.25,4.98,8.74" target="#fig_1">2</ref> shows example images from the development data set, according to image modality and additional UMLS R CUI. Similarly to the caption task in 2019 <ref type="bibr" coords="5,245.64,644.16,14.61,8.74" target="#b15">[16]</ref>, concepts with very high frequency (&gt;13,000), as well as redundant synonyms were removed. This lead to a reduction of concepts per image in comparison to the previous years, from 5,528 in 2019 <ref type="bibr" coords="6,426.92,118.99,15.50,8.74" target="#b15">[16]</ref> to 3,047 in 2020. Not all concepts in the ground truth can be visually seen, for example the concept 'Hole Finding' in Fig. <ref type="figure" coords="6,285.10,142.90,4.98,8.74" target="#fig_1">2</ref> can not be detected from the image. Images in the training, validation and test sets have ,  and [1-95] concepts, respectively. All concepts in the validation and test sets also exist in the training set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Methodology</head><p>For all 3,534 radiology images distributed in the test set, UMLS R CUIs have to be predicted by the participating teams automatically. As in the previous years <ref type="bibr" coords="7,134.77,596.34,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="7,146.95,596.34,7.75,8.74" target="#b6">7,</ref><ref type="bibr" coords="7,156.35,596.34,11.62,8.74" target="#b15">16]</ref>, the model performance was measured using the balanced precision and recall trade-off in terms of F1-score. The default implementation of the Python scikit-learn (v0.17.1-2) library was applied to compute the F-scores per image and average them across all test images. The maximum number of concepts allowed per image was set to 150. This limitation was chosen as the training, validation and test set contain a maximum of 140, 142 and 95 concepts per image. Each group could have a maximum of 15 submission, with 10 valid and 5 faulty. Faulty submissions may include:</p><p>-Same image id more than once -Wrong image id -Too many concepts -Same concept more than once -Not all test images included All submission runs were uploaded by the participating teams and evaluated with AICrowd<ref type="foot" coords="8,219.45,232.04,3.97,6.12" target="#foot_0">4</ref> . The source code of the evaluation tool is available on the ImageCLEF web page<ref type="foot" coords="8,231.21,244.00,3.97,6.12" target="#foot_1">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The overall performance achieved by the concepts detection models submitted by the 7 participating teams are listed and discussed in this section. In Table <ref type="table" coords="8,472.84,316.08,3.87,8.74" target="#tab_2">4</ref>, the submission run with best performance per team is shown. An additional evaluation regarding the imaging modality was done internally, after the official concept detection evaluation process. The accuracy (%) across all images in the test set was computed and is listed in Table <ref type="table" coords="8,334.00,363.90,3.87,8.74">6</ref>. Compared to the previous editions, there is an improvement regarding the F1-Score of the submitted concept detection models, from 0.1583 in ImageCLEF 2017 <ref type="bibr" coords="8,365.85,387.81,9.96,8.74" target="#b2">[3]</ref>, 0.1108 in ImageCLEF 2018 <ref type="bibr" coords="8,158.01,399.77,10.52,8.74" target="#b6">[7]</ref> and 0.2823 in ImageCLEF 2019 <ref type="bibr" coords="8,313.40,399.77,15.50,8.74" target="#b15">[16]</ref> to 0.3940 in 2020.</p><p>The AUEB NLP Group <ref type="bibr" coords="8,252.48,412.00,15.50,8.74" target="#b11">[12]</ref> from the Athens University of Economics achieved the overall highest F1-Score of 0.3940 for the detection of concepts for the images in the official evaluation test set. Their three (3) submission runs ranked 1st, 2nd and 6th of all 47 submitted runs. The submitted systems are a variation of CheXNet <ref type="bibr" coords="8,218.74,459.82,15.50,8.74" target="#b25">[26]</ref> with DenseNet-121 <ref type="bibr" coords="8,327.00,459.82,10.52,8.74" target="#b7">[8]</ref> and followed by a feed-forward Neural Network (FFNN), which acts as the classifier layer on the top <ref type="bibr" coords="8,441.96,471.78,14.61,8.74" target="#b11">[12]</ref>. The system was first pre-trained on the ImageNet data set <ref type="bibr" coords="8,375.40,483.73,15.50,8.74" target="#b20">[21]</ref> and then fine-tuned using the ImageCLEF 2020 concept detection development data set. Several ensemble methods such as the intersection and union of predicted concepts were experimented. The system with the intersection of concepts achieved the overall highest F1-Score.</p><p>The overall 2nd ranked participating team is PwC Healthcare group from PricewaterhouseCoopers with a total number of nine (9) submitted runs. The adopted approaches range from Convolutional Neural Network (CNN) architectures, to Natural Language Processing techniques, as well as clustering algorithms <ref type="bibr" coords="8,167.01,591.61,14.61,8.74" target="#b23">[24]</ref>. The group's three (3) best systems ranked 3rd, 4th and 5th. Several pre-processing approaches such as range and intensity normalization and data augmentation were adopted prior to training the models <ref type="bibr" coords="9,405.22,118.99,14.61,8.74" target="#b23">[24]</ref>. Multi-modal approaches were experimented to incorporate the concept imbalanced distribution and a novel approach of band classification was applied. This classification method first clusters the vocabulary of concepts into bands and then creates for each band a classification architecture <ref type="bibr" coords="9,303.74,166.81,14.61,8.74" target="#b23">[24]</ref>. The third best participating team was from the University of Essex, with an overall F1-Score of 0.381. The proposed approach adopts pre-trained DenseNet models <ref type="bibr" coords="9,169.43,512.55,10.52,8.74" target="#b7">[8]</ref> for the extraction of relevant features. The additional information on the imaging modality was used for fine-tuning by adding a fully connected layer to the DenseNet-121 model and thereby transforming it into a multi-label classification model <ref type="bibr" coords="9,221.26,548.41,9.96,8.74" target="#b5">[6]</ref>. Several concept selection strategies, such as distance and ranked based methods, were applied to a given query image from the test set. The group's five best runs of the nine submitted runs ranked 6th to 10th among all submissions.</p><p>Five runs were submitted by the IML group from the German Research Center for Artificial Intelligence, with the best F1-Score of 0.3745, and the 4th best team. Multiple deep learning systems such as VGG16 <ref type="bibr" coords="9,401.82,620.25,14.61,8.74" target="#b21">[22]</ref>, ResNet50 <ref type="bibr" coords="9,470.08,620.25,10.52,8.74" target="#b4">[5]</ref> and DenseNet169 <ref type="bibr" coords="9,215.88,632.21,9.96,8.74" target="#b7">[8]</ref>, which were pre-trained on the ImageNet data set, were applied for modeling the concept detection systems. The task was addressed as a multi one-hot encoding with a final prediction layer of 3,047 sigmoidal activation units and several fine-tuning steps, such as data augmentation, hyper-parameter settings, were undertaken <ref type="bibr" coords="10,249.22,130.95,14.61,8.74" target="#b9">[10]</ref>. TUC MC, a media computing group from the Chemnitz University of Technology ranked 5th best participating team. The highest F1-Score from the ten submitted runs was 0.3745. The adopted deep learning model was based on the Xception architecture <ref type="bibr" coords="11,231.92,335.71,10.52,8.74" target="#b0">[1]</ref> with weights pre-trained on ImageNet. The submitted runs use the same model base structure, however the hyper-parameters are varied in regards to last layer threshold and max-pooling in the highest layers <ref type="bibr" coords="11,448.17,359.62,14.61,8.74" target="#b24">[25]</ref>.</p><p>Ten runs were submitted by Morgan CS, a group from the computer science department at the Morgan State University. The best achieved F1-Score was 0.1673, by approaching the concept detection task as a multi-label classification problem <ref type="bibr" coords="11,173.02,414.61,14.61,8.74" target="#b13">[14]</ref>. Classifiers were trained with deep features extracted with the deep learning system DenseNet169 and ResNet50 and pre-trained on ImageNet. Other methods experimented include a recurrent concept sequence generator that was modelled using a multimodal technique of fusing text and image features for recurrent sequence prediction. CSE SSN from the department of computer science of the SSN College of Engineering Chennai submitted one (1) run for official evaluation and achieved the average F1-Score of 0.1347 on all images in the test set. Similar to several participating teams, the concept detection task was addressed as a convolution neural network multi-label classification problem <ref type="bibr" coords="11,318.24,529.39,9.96,8.74" target="#b1">[2]</ref>. The imaging modality distributed was applied for pre-processing and model fine-tuning steps.</p><p>An ex-post evaluation was computed on all submitted runs. The aim was to compute the performance on correctly predicting the imaging modality. All images in the development and test set were assigned concepts that denote the acquisition technique, as shown in Figure <ref type="figure" coords="11,312.80,596.34,3.87,8.74" target="#fig_1">2</ref>. The images belonging to the imaging modality 'DRCO: Combined modalities in one image' were not considered for evaluation. For all images in the test set, we computed the presence of these concepts in the submission runs using this additional information. The best performance grouped per team is listed in Table <ref type="table" coords="11,348.88,644.16,4.98,8.74">6</ref> and the complete evaluation in Table <ref type="table" coords="11,173.78,656.12,3.87,8.74" target="#tab_4">7</ref>.</p><p>Table <ref type="table" coords="12,165.05,115.91,4.13,7.89">6</ref>. Performance of the participating teams in the ImageCLEF 2020 concept detection task on correctly predicting the imaging modality of the images in the test set. The best run per team is selected. Teams with a previous participation in 2019 are marked with an asterisk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>Institution Accuracy (%) PwC Healthcare <ref type="bibr" coords="12,205.48,179.20,14.34,7.86" target="#b23">[24]</ref> PricewaterhouseCoopers </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presents an overview of applied approaches and their performance, as well as the task description, participation and distributed data set for the ImageCLEF 2020 concept detection task. Similar to the 2019 edition, the results this year show that there is an improvement in the achieved F1-scores (best score 0.3940). In this edition, not only does the dataset contain an increased number of images, the number of concepts were reduced to be more precise and additional modality information was distributed. In the previous editions, the overall best F1-Scores were 0.2823 in Image-med Caption 2019, 0.1108 in ImageCLEFmed Caption 2018 and 0.1583 in ImageCLEFmed Caption 2017. Almost all participating groups were new to the task, with only one team that participated in ImageCLEF caption 2019. The seven participating teams are affiliated to institutions from 5 countries, which shows the continuing research interest to this challenging task. Most of the submitted runs are based on deep learning architectures. The pre-trained models DenseNet-121, ResNet50 and VGG16 on the ImageNet and CheXNet were used to extract relevant visual representation for the images. Multiple pre-processing steps such as concept filtering, data augmentation and image enhancement were applied to optimize the input for the predicting systems. Long short-term memory (LSTM) recurrent neural networks (RNN), adversarial auto-encoders, CNN image encoders and transfer learning-based multi-label classification models were the frequently used approaches.</p><p>As the focus in the caption task 2019 was reduced from biomedical images to solely radiology images, a reduction of the extracted concepts from 111,155 to 5,528 was observed. We added this year an additional label denoting the imaging modality of the images. This extra information was used by several teams for pre-filtering steps prior to training the models, concept selection and for ensemble algorithms. The class imbalance in the distributed data set proved to be challenging for several teams. However, medical data and diseases are also usually unbalanced with a few conditions happening very frequently and most being very rare.</p><p>In future work, an extensive review of the clinical relevance for the concepts in the development data should be explored. As the concepts originate from the natural language captions, not all concepts have high clinical utility. Medical journals also have very different policies in terms of checking figure captions. We believe this will assist in creating more efficient systems for automated medical data analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,611.02,345.83,7.89;3,211.05,454.52,193.27,141.73"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of a radiology image with the corresponding extracted UMLS R CUIs.</figDesc><graphic coords="3,211.05,454.52,193.27,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,134.77,633.11,345.82,7.89;6,134.77,644.10,345.83,7.86;6,134.77,655.05,126.03,7.86;6,165.95,261.46,283.45,356.88"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of radiology images distributed at the ImageCLEF 2020 concept detection task, showing the seven imaging modalities. All images were randomly selected from the development data set.</figDesc><graphic coords="6,165.95,261.46,283.45,356.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,306.57,345.83,126.34"><head>Table 2 .</head><label>2</label><figDesc>Analysis on data distribution for ImageCLEFmed 2020 Concept Detection Task.</figDesc><table coords="5,159.10,336.59,297.16,96.33"><row><cell>Imaging Technique</cell><cell cols="4">Train Validation Test Sum</cell></row><row><cell>DRAN: Angiography</cell><cell>4,713</cell><cell>1,132</cell><cell cols="2">325 6,170</cell></row><row><cell cols="2">DRCO: Combined modalities in one image 487</cell><cell>73</cell><cell>49</cell><cell>609</cell></row><row><cell>DRCT: Computerized Tomography</cell><cell cols="4">20,031 4,992 1,140 26,163</cell></row><row><cell>DRMR: Magnetic Resonance</cell><cell cols="2">11,447 2,848</cell><cell cols="2">562 14,857</cell></row><row><cell>DRPE: Positron emission tomography</cell><cell>502</cell><cell>74</cell><cell>38</cell><cell>614</cell></row><row><cell>DRUS: Ultrasound</cell><cell>8,629</cell><cell>2,134</cell><cell cols="2">502 11,265</cell></row><row><cell>DRXR: X-Ray, 2D radiography</cell><cell cols="2">18,944 4,717</cell><cell cols="2">918 24,579</cell></row><row><cell>Sum</cell><cell cols="4">65,753 15,970 3534 84,257</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,116.41,345.83,401.26"><head>Table 3 .</head><label>3</label><figDesc>UMLS R (An excerpt of Unified Medical Language System R ) Concept Unique Identifiers (CUIs) distributed for the task with their respective occurrences. The concepts were randomly chosen in a descending order. All listed concepts were distributed in the training set.</figDesc><table coords="7,151.82,169.69,305.80,347.99"><row><cell>CUI</cell><cell>Concept</cell><cell>Occurrence</cell></row><row><cell>C0040398</cell><cell>Tomography</cell><cell>20,031</cell></row><row><cell>C0040405</cell><cell>X-Ray Computed Tomography</cell><cell>20,031</cell></row><row><cell>C0043299</cell><cell>Diagnostic radiologic examination</cell><cell>18,944</cell></row><row><cell>C0024485</cell><cell>Magnetic Resonance Imaging</cell><cell>11,447</cell></row><row><cell>C0041618</cell><cell>Ultrasound</cell><cell>8,629</cell></row><row><cell>C0441633</cell><cell>Scanning</cell><cell>6733</cell></row><row><cell>C0043299</cell><cell>Diagnostic radiologic examination</cell><cell>6321</cell></row><row><cell>C1962945</cell><cell>Radiographic imaging procedure</cell><cell>6318</cell></row><row><cell>C0040395</cell><cell>Tomography</cell><cell>6235</cell></row><row><cell>C0034579</cell><cell>Panoramic Radiography</cell><cell>6127</cell></row><row><cell>C0817096</cell><cell>Chest</cell><cell>5981</cell></row><row><cell>C0040405</cell><cell>X-Ray Computed Tomography</cell><cell>5801</cell></row><row><cell>C1548003</cell><cell>Diagnostic Service Section ID -Radiograph</cell><cell>5159</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>C0000726</cell><cell>Abdomen</cell><cell>2297</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>C2985765</cell><cell>Enhancement Description</cell><cell>1084</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>C0228391</cell><cell>Structure of habenulopeduncular tract</cell><cell>672</cell></row><row><cell>C0729233</cell><cell>Dissecting aneurysm of the thoracic aorta</cell><cell>652</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>C0771711</cell><cell>Pancreas extract</cell><cell>456</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>C1704302</cell><cell>Permanent premolar tooth</cell><cell>177</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>C0042070</cell><cell>Urography</cell><cell>67</cell></row><row><cell>C0085632</cell><cell>Apathy</cell><cell>67</cell></row><row><cell>C0267716</cell><cell>Incisional hernia</cell><cell>67</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>C0081923</cell><cell>Cardiocrome</cell><cell>1</cell></row><row><cell>C0193959</cell><cell>Tonsillectomy and adenoidectomy</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.77,200.08,349.97,257.45"><head>Table 4 .</head><label>4</label><figDesc>Performance of the participating teams in the ImageCLEF 2020 concept detection task in regards to correctly predicting concepts of the images in the test set. The best run per team is selected. Teams with previous participation in 2019 are marked with an asterisk.</figDesc><table coords="9,136.16,252.01,348.57,205.52"><row><cell>Team</cell><cell>Institution</cell><cell>F1-Score</cell></row><row><cell cols="2">AUEB NLP Group* [12] Department of Informatics, Athens University of</cell><cell>0.3940</cell></row><row><cell></cell><cell>Economics and Business, Athens, Greece</cell><cell></cell></row><row><cell>PwC Healthcare [24]</cell><cell>PricewaterhouseCoopers US Advisory,</cell><cell>0.3924</cell></row><row><cell></cell><cell>Mumbai, India</cell><cell></cell></row><row><cell>Essex [6]</cell><cell>School of computer Science and Electronic</cell><cell>0.3808</cell></row><row><cell></cell><cell>Engineering, University of Essex,</cell><cell></cell></row><row><cell></cell><cell>Essex, United Kingdom</cell><cell></cell></row><row><cell>IML DFKI [10]</cell><cell>Interactive Machine Learning Group, German</cell><cell>0.3745</cell></row><row><cell></cell><cell>Research Center for Artificial Intelligence</cell><cell></cell></row><row><cell></cell><cell>(DFKI), Saarbrücken, Germany</cell><cell></cell></row><row><cell>TUC MC [25]</cell><cell>Technische Unversität Chemnitz,</cell><cell>0.3512</cell></row><row><cell></cell><cell>Chemnitz, Germany</cell><cell></cell></row><row><cell>Morgan CS [14]</cell><cell>Computer Science Department,</cell><cell>0.1673</cell></row><row><cell></cell><cell>Morgan State University, Baltimore,</cell><cell></cell></row><row><cell></cell><cell>Maryland, United States of America</cell><cell></cell></row><row><cell>CSE SSN [2]</cell><cell>Department of Computer Science and</cell><cell>0.1347</cell></row><row><cell></cell><cell>Engineering, SSN College of Engineering,</cell><cell></cell></row><row><cell></cell><cell>Chennai, India</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,154.65,153.76,298.78,509.60"><head>Table 5 :</head><label>5</label><figDesc>Concept detection performance in terms of all submitted runs for the ImageCLEF 2020 Concept Detection Task</figDesc><table coords="10,154.65,187.98,298.78,475.39"><row><cell>Group Name</cell><cell>Submission Run</cell><cell>F1-Score</cell></row><row><cell cols="2">AUEB NLP Group InterceptCheXNetCheckpoints.csv</cell><cell>0.3940</cell></row><row><cell cols="2">AUEB NLP Group BestOf.csv</cell><cell>0.3933</cell></row><row><cell>PwC Healtcare</cell><cell cols="2">folderwise KNN resnet101 test pred.csv0.3924</cell></row><row><cell>PwC Healtcare</cell><cell>combined test pred v1.csv</cell><cell>0.3889</cell></row><row><cell>PwC Healtcare</cell><cell>folder wise test pred v1.csv</cell><cell>0.3889</cell></row><row><cell cols="2">AUEB NLP Group UnionCheXNetCheckpoints.csv</cell><cell>0.3870</cell></row><row><cell>Essex</cell><cell>submit run3.csv</cell><cell>0.3808</cell></row><row><cell>Essex</cell><cell>submit run5.csv</cell><cell>0.3805</cell></row><row><cell>Essex</cell><cell>submit run1.csv</cell><cell>0.3797</cell></row><row><cell>Essex</cell><cell>cp99 all modified.txt</cell><cell>0.3785</cell></row><row><cell>Essex</cell><cell>c99 all man.txt</cell><cell>0.3777</cell></row><row><cell>IML DFKI</cell><cell>imageclefmed2020-test-vgg16-f1-bce-</cell><cell>0.3745</cell></row><row><cell></cell><cell>nomissing-iml.txt</cell><cell></cell></row><row><cell>IML DFKI</cell><cell>imageclefmed2020-test-vgg16-f1-bce-</cell><cell>0.3744</cell></row><row><cell></cell><cell>iml.txt</cell><cell></cell></row><row><cell>PwC Healtcare</cell><cell>combined test pred new.csv</cell><cell>0.3681</cell></row><row><cell>PwC Healtcare</cell><cell>NLP clusters test pred.csv</cell><cell>0.3668</cell></row><row><cell>PwC Healtcare</cell><cell>knn t117 test pred.csv</cell><cell>0.3666</cell></row><row><cell>IML DFKI</cell><cell cols="2">imageclefmed2020-test-resnet50-iml.txt 0.3652</cell></row><row><cell>IML DFKI</cell><cell cols="2">imageclefmed2020-test-vgg16-iml.txt 0.3631</cell></row><row><cell>IML DFKI</cell><cell>imageclefmed2020-test-densenet169-</cell><cell>0.3602</cell></row><row><cell></cell><cell>iml.txt</cell><cell></cell></row><row><cell>TUC MC</cell><cell>model thr0 18.csv</cell><cell>0.3512</cell></row><row><cell>TUC MC</cell><cell>streamlined1 thr0 25.csv</cell><cell>0.3486</cell></row><row><cell>TUC MC</cell><cell>streamlined1 thr0 20.csv</cell><cell>0.3486</cell></row><row><cell>TUC MC</cell><cell>2streamlined1.csv</cell><cell>0.3486</cell></row><row><cell>TUC MC</cell><cell>basemodel thr0 20.csv</cell><cell>0.3474</cell></row><row><cell>TUC MC</cell><cell>model low lr thr0 20.csv</cell><cell>0.3455</cell></row><row><cell>Essex</cell><cell>submit run2.csv</cell><cell>0.3449</cell></row><row><cell>TUC MC</cell><cell>streamlined1 nomax.csv</cell><cell>0.3448</cell></row><row><cell>TUC MC</cell><cell>basemodel.csv</cell><cell>0.3435</cell></row><row><cell>TUC MC</cell><cell>streamlined1 thr0 12.csv</cell><cell>0.3423</cell></row><row><cell>PwC Healtcare</cell><cell>f1 band test t025 pred.csv</cell><cell>0.3379</cell></row><row><cell>Essex</cell><cell>cp98 all.txt</cell><cell>0.3370</cell></row><row><cell>TUC MC</cell><cell>model weighting.csv</cell><cell>0.3325</cell></row><row><cell>PwC Healtcare</cell><cell>NLP test pred fixed.csv</cell><cell>0.3163</cell></row><row><cell>Essex</cell><cell>canberra all modified.txt</cell><cell>0.2804</cell></row><row><cell>PwC Healtcare</cell><cell>combined wo folder test.csv</cell><cell>0.2655</cell></row><row><cell>Essex</cell><cell>cp95 all.txt</cell><cell>0.2459</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,136.23,179.20,342.90,491.85"><head>Table 7 :</head><label>7</label><figDesc>Modality classification performance in terms of all submitted runs for the ImageCLEF 2020 Concept Detection Task</figDesc><table coords="12,136.23,179.20,342.90,491.85"><row><cell></cell><cell>US Advisory,</cell><cell></cell><cell>62.08</cell></row><row><cell></cell><cell>Mumbai, India</cell><cell></cell></row><row><cell>AUEB NLP Group*</cell><cell cols="2">Department of Informatics, Athens University</cell><cell>59.73</cell></row><row><cell>[12]</cell><cell>of Economics and Business, Athens, Greece</cell><cell></cell></row><row><cell>Essex [6]</cell><cell>School of computer Science and Electronic</cell><cell></cell><cell>56.34</cell></row><row><cell></cell><cell>Engineering, University of Essex,</cell><cell></cell></row><row><cell></cell><cell>Essex, United Kingdom</cell><cell></cell></row><row><cell>TUC MC [25]</cell><cell>Technische Unversität Chemnitz,</cell><cell></cell><cell>50.08</cell></row><row><cell></cell><cell>Chemnitz, Germany</cell><cell></cell></row><row><cell>IML DFKI [10]</cell><cell>Interactive Machine Learning Group,</cell><cell></cell><cell>47.06</cell></row><row><cell></cell><cell>German Research Center for Artificial</cell><cell></cell></row><row><cell></cell><cell>Intelligence (DFKI), Saarbrücken, Germany</cell><cell></cell></row><row><cell>Morgan CS [14]</cell><cell>Computer Science Department,</cell><cell></cell><cell>02.06</cell></row><row><cell></cell><cell>Morgan State University, Baltimore,</cell><cell></cell></row><row><cell></cell><cell>Maryland, United States of America</cell><cell></cell></row><row><cell>CSE SSN [2]</cell><cell>Department of Computer Science and</cell><cell></cell><cell>01.39</cell></row><row><cell></cell><cell>Engineering, SSN College of Engineering,</cell><cell></cell></row><row><cell></cell><cell>Chennai, India</cell><cell></cell></row><row><cell>Group Name</cell><cell>Submission Run</cell><cell>Acc(%)</cell></row><row><cell>PwC Healtcare</cell><cell>NLP clusters test pred.csv</cell><cell>62.08</cell></row><row><cell cols="2">AUEB NLP Group InterceptCheXNetCheckpoints.csv</cell><cell>59.73</cell></row><row><cell cols="2">AUEB NLP Group BestOf.csv</cell><cell>59.48</cell></row><row><cell>essexgp2020</cell><cell>cp99 all modified.txt</cell><cell>56.34</cell></row><row><cell>essexgp2020</cell><cell>c99 all man.txt</cell><cell>55.69</cell></row><row><cell cols="2">AUEB NLP Group UnionCheXNetCheckpoints.csv</cell><cell>55.23</cell></row><row><cell>PwC Healtcare</cell><cell cols="2">folderwise KNN resnet101 test pred.csv54.70</cell></row><row><cell>PwC Healtcare</cell><cell>folder wise test pred v1.csv</cell><cell>52.43</cell></row><row><cell>PwC Healtcare</cell><cell>combined test pred v1.csv</cell><cell>52.43</cell></row><row><cell>essexgp2020</cell><cell>submit run3.csv</cell><cell>50.93</cell></row><row><cell>TUC MC</cell><cell>streamlined1 thr0 25.csv</cell><cell>50.08</cell></row><row><cell>essexgp2020</cell><cell>submit run1.csv</cell><cell>49.29</cell></row><row><cell>essexgp2020</cell><cell>submit run5.csv</cell><cell>48.84</cell></row><row><cell>TUC MC</cell><cell>model low lr thr0 20.csv</cell><cell>48.22</cell></row><row><cell>iml</cell><cell>imageclefmed2020-test-densenet169-</cell><cell>47.06</cell></row><row><cell></cell><cell>iml.txt</cell><cell></cell></row><row><cell>iml</cell><cell>imageclefmed2020-test-vgg16-f1-bce-</cell><cell>46.94</cell></row><row><cell></cell><cell>iml.txt</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="8,144.73,623.92,319.61,7.86;8,144.73,634.88,103.58,7.86"><p>https://www.aicrowd.com/challenges/imageclef-2020-caption-concept-detection [last accessed: 26.07.2020]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="8,144.73,645.84,291.56,7.86;8,144.73,656.80,164.37,7.86"><p>https://www.imageclef.org/system/files/ImageCLEF-ConceptDetection-Evaluation.zip [last accessed: 26.07.2020]</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="14,142.96,479.55,337.63,7.86;14,151.52,490.51,329.07,7.86;14,151.52,501.47,329.07,7.86;14,151.52,512.42,163.64,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,200.42,479.55,261.91,7.86">Xception: Deep Learning with Depthwise Separable Convolutions</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.195</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.195" />
	</analytic>
	<monogr>
		<title level="m" coord="14,151.52,490.51,329.07,7.86;14,151.52,501.47,22.38,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017-07-25">July 22-25, 2017. 07 2017</date>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,523.86,337.63,7.86;14,151.52,534.82,329.07,7.86;14,151.52,545.78,285.99,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,212.88,523.86,267.72,7.86;14,151.52,534.82,104.85,7.86">ImageCLEF 2020: Image Caption Prediction using Multilabel Convolutional Neural Network</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Devi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,278.11,534.82,202.48,7.86;14,151.52,545.78,96.00,7.86">CLEF2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,557.21,337.64,7.86;14,151.52,568.17,329.07,7.86;14,151.52,579.13,329.07,7.86;14,151.52,590.09,329.07,7.86;14,151.52,601.05,100.60,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,382.67,557.21,97.93,7.86;14,151.52,568.17,329.07,7.86;14,151.52,579.13,26.16,7.86">Overview of ImageCLE-Fcaption 2017 -Image Caption Prediction and Concept Detection for Biomedical Images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1866/invitedpaper7.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="14,198.45,579.13,282.14,7.86;14,151.52,590.09,24.01,7.86">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,612.49,337.63,7.86;14,151.52,623.44,329.07,7.86;14,151.52,634.40,37.89,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="14,247.20,612.49,233.39,7.86;14,151.52,623.44,329.07,7.86">Information Retrieval Evaluation in a Changing World Lessons Learned from 20 Years of CLEF: Lessons Learned from 20 Years of CLEF</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,645.84,337.63,7.86;14,151.52,656.80,329.07,7.86;15,151.52,119.67,329.07,7.86;15,151.52,130.63,159.04,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,292.70,645.84,183.70,7.86">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m" coord="14,165.71,656.80,314.88,7.86;15,151.52,119.67,22.38,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-nition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recog-nition<address><addrLine>Las Vegas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016-06">June 26 -July 1, 2016. 06 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,141.61,337.64,7.86;15,151.52,152.56,329.07,7.86;15,151.52,163.52,268.57,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,415.71,141.61,64.89,7.86;15,151.52,152.56,92.96,7.86">Essex at Image-CLEFcaption 2020 task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">P</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Compean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,264.06,152.56,216.52,7.86;15,151.52,163.52,78.59,7.86">CLEF2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,174.50,337.64,7.86;15,151.52,185.45,329.07,7.86;15,151.52,196.41,329.07,7.86;15,151.52,207.37,251.47,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,414.37,174.50,66.22,7.86;15,151.52,185.45,177.24,7.86">Overview of the ImageCLEF 2018 Caption Prediction Tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2125/invitedpaper4.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="15,351.41,185.45,129.18,7.86;15,151.52,196.41,181.56,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,218.35,337.63,7.86;15,151.52,229.30,329.07,7.86;15,151.52,240.26,329.07,7.86;15,151.52,251.22,259.76,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,402.82,218.35,77.77,7.86;15,151.52,229.30,95.74,7.86">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.243" />
	</analytic>
	<monogr>
		<title level="m" coord="15,270.95,229.30,209.65,7.86;15,151.52,240.26,129.85,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017-07-25">July 22-25, 2017. July 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,262.19,337.64,7.86;15,151.52,273.15,329.07,7.86;15,151.52,284.11,329.07,7.86;15,151.52,295.07,329.07,7.86;15,151.52,306.03,329.07,7.86;15,151.52,316.99,329.07,7.86;15,151.52,327.95,329.07,7.86;15,151.52,338.91,329.07,7.86;15,151.52,349.87,329.07,7.86;15,151.52,360.82,329.07,7.86;15,151.52,371.78,95.77,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,295.04,316.99,185.55,7.86;15,151.52,327.95,276.77,7.86">Overview of the ImageCLEF 2020: Multimedia Retrieval in Medical, Lifelogging, Nature, and Internet Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,450.74,327.95,29.85,7.86;15,151.52,338.91,329.07,7.86;15,151.52,349.87,306.29,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="15,180.42,360.82,170.09,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="15,142.62,382.76,337.98,7.86;15,151.52,393.72,329.07,7.86;15,151.52,404.67,329.07,7.86;15,151.52,415.63,46.58,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,317.42,382.76,163.17,7.86;15,151.52,393.72,200.52,7.86">A Competitive Deep Neural Network Approach for the ImageCLEFmed Caption 2020 Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kalimuthu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nunnari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sonntag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,372.68,393.72,107.91,7.86;15,151.52,404.67,186.26,7.86">CLEF2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,426.61,337.97,7.86;15,151.52,437.56,329.07,7.86;15,151.52,448.52,329.07,7.86;15,151.52,459.48,329.07,7.86;15,151.52,470.42,13.17,7.89;15,181.40,470.44,23.04,7.86;15,221.14,470.44,28.16,7.86;15,266.00,470.44,214.59,7.86;15,151.52,481.40,212.03,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,323.31,437.56,157.28,7.86;15,151.52,448.52,329.07,7.86;15,151.52,459.48,170.67,7.86">Evaluating performance of biomedical image retrieval systems -An overview of the medical image retrieval task at ImageCLEF 2004-2013</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compmedimag.2014.03.004</idno>
		<ptr target="https://doi.org/10.1016/j.compmedimag.2014.03.004" />
	</analytic>
	<monogr>
		<title level="j" coord="15,334.72,459.48,145.87,7.86">Comp. Med. Imag. and Graph</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="55" to="61" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,492.37,337.98,7.86;15,151.52,503.33,329.07,7.86;15,151.52,514.29,300.72,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,403.93,492.37,76.67,7.86;15,151.52,503.33,111.08,7.86">AUEB NLP Group at ImageCLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,301.69,503.33,178.91,7.86;15,151.52,514.29,110.74,7.86">CLEF2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09-22">2020. September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,525.26,337.97,7.86;15,151.52,536.22,329.07,7.86;15,151.52,547.18,329.07,7.86;15,151.52,558.14,329.07,7.86;15,151.52,569.10,329.07,7.86;15,151.52,580.06,329.07,7.86;15,151.52,591.02,238.13,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,266.08,525.26,214.51,7.86;15,151.52,536.22,139.57,7.86">Optimized Convolutional Neural Network Ensembles for Medical Subfigure Classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-65813-15</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-65813-15" />
	</analytic>
	<monogr>
		<title level="m" coord="15,387.83,547.18,92.76,7.86;15,151.52,558.14,329.07,7.86;15,151.52,569.10,99.67,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction at the 8th International Conference of the CLEF Association</title>
		<title level="s" coord="15,424.04,569.10,56.55,7.86;15,151.52,580.06,111.95,7.86">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lawless</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Kelly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Dublin, Ireland; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
			<biblScope unit="volume">10456</biblScope>
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,601.99,337.98,7.86;15,151.52,612.95,329.07,7.86;15,151.52,623.91,322.28,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,249.25,601.99,231.34,7.86;15,151.52,612.95,140.04,7.86">Concept Detection in Biomedical Images with Deep Learning Based Multilabel Classification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Lyode</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,313.05,612.95,167.54,7.86;15,151.52,623.91,132.29,7.86">CLEF2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,634.88,337.97,7.86;15,151.52,645.84,329.08,7.86;15,151.52,656.80,171.84,7.86" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="15,429.01,634.88,51.57,7.86;15,151.52,645.84,248.91,7.86">ImageCLEF, Experimental Evaluation in Visual Information Retrieval</title>
		<idno type="DOI">10.1007/978-3-642-15181-1</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-15181-1" />
		<editor>Müller, H., Clough, P.D., Deselaers, T., Caputo, B.</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,119.67,337.98,7.86;16,151.52,130.63,329.07,7.86;16,151.52,141.59,329.07,7.86;16,151.52,152.55,329.07,7.86;16,151.52,163.51,329.07,7.86;16,151.52,174.47,78.85,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,386.96,119.67,93.63,7.86;16,151.52,130.63,165.56,7.86">Overview of the Image-CLEFmed 2019 Concept Detection Task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/paper245.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="16,252.93,141.59,227.66,7.86;16,151.52,152.55,102.62,7.86">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="16,453.97,152.55,26.62,7.86;16,151.52,163.51,89.92,7.86">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">September 9-12, 2019. 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,185.43,337.97,7.86;16,151.52,196.39,329.07,7.86;16,151.52,207.35,329.07,7.86;16,151.52,218.30,329.07,7.86;16,151.52,229.26,329.07,7.86;16,151.52,240.22,329.07,7.86;16,151.52,251.18,329.07,7.86;16,151.52,262.14,184.37,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,420.01,185.43,60.58,7.86;16,151.52,196.39,249.16,7.86;16,428.28,196.39,52.31,7.86;16,151.52,207.35,329.07,7.86;16,151.52,218.30,329.07,7.86;16,151.52,229.26,16.83,7.86">Intravascular Imaging and Computer Assisted Stenting -and -Large-Scale Annotation of Biomedical Data and Expert Label Synthesis -7th Joint International Workshop</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01364-620</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01364-620" />
	</analytic>
	<monogr>
		<title level="m" coord="16,178.08,229.26,229.38,7.86;16,151.52,240.22,166.42,7.86">CVII-STENT 2018 and Third International Workshop</title>
		<meeting><address><addrLine>LABELS; Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-16">2018. September 16, 2018. 2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Held in Conjunction with MICCAI 2018</note>
</biblStruct>

<biblStruct coords="16,142.62,273.10,337.98,7.86;16,151.52,284.06,329.07,7.86;16,151.52,295.02,329.07,7.86;16,151.52,305.98,329.07,7.86;16,151.52,316.93,306.99,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="16,302.21,273.10,178.38,7.86;16,151.52,284.06,201.39,7.86">Adopting Semantic Information of Grayscale Radiographs for Image Classification and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.5220/0006732301790187</idno>
		<ptr target="https://doi.org/10.5220/0006732301790187" />
	</analytic>
	<monogr>
		<title level="m" coord="16,372.81,284.06,107.78,7.86;16,151.52,295.02,329.07,7.86;16,151.52,305.98,69.45,7.86">Proceedings of the 11th International Joint Conference on Biomedical Engineering Systems and Technologies (BIOSTEC 2018)</title>
		<meeting>the 11th International Joint Conference on Biomedical Engineering Systems and Technologies (BIOSTEC 2018)<address><addrLine>Funchal, Madeira, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>BIOIMAGING</publisher>
			<date type="published" when="2018">January 19-21, 2018. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,327.89,337.98,7.86;16,151.52,338.85,329.07,7.86;16,151.52,349.81,329.07,7.86;16,151.52,360.77,329.07,7.86;16,151.52,371.73,183.35,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="16,310.02,327.89,170.58,7.86;16,151.52,338.85,189.02,7.86">Variations on Branding with Text Occurrence for Optimized Body Parts Classification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1109/EMBC.2019.8857478</idno>
		<ptr target="https://doi.org/10.1109/EMBC.2019.8857478" />
	</analytic>
	<monogr>
		<title level="m" coord="16,363.27,338.85,117.32,7.86;16,151.52,349.81,329.07,7.86;16,151.52,360.77,97.48,7.86">Proceedings of the 41th Annual International Conference of the IEEE Engineering in Medicine and Biology Society EMBC 2019</title>
		<meeting>the 41th Annual International Conference of the IEEE Engineering in Medicine and Biology Society EMBC 2019<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">July 23-27, 2019. 2019</date>
			<biblScope unit="page" from="890" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,382.69,337.98,7.86;16,151.52,393.65,329.07,7.86;16,151.52,404.58,265.56,7.89" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="16,212.82,382.69,242.65,7.86">PubMed Central: The GenBank of the published literature</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.98.2.381</idno>
		<ptr target="https://doi.org/10.1073/pnas.98.2.381" />
	</analytic>
	<monogr>
		<title level="j" coord="16,463.04,382.69,17.56,7.86;16,151.52,393.65,329.07,7.86">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="381" to="382" />
			<date type="published" when="2001-01">Jan 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,415.56,337.97,7.86;16,151.52,426.52,329.07,7.86;16,151.52,437.46,329.07,7.89;16,151.52,448.44,216.17,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="16,414.93,426.52,65.66,7.86;16,151.52,437.48,138.63,7.86">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j" coord="16,296.75,437.48,165.51,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">09</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,459.40,337.98,7.86;16,151.52,470.36,186.76,7.86" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="16,274.05,459.40,206.54,7.86;16,151.52,470.36,73.50,7.86">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv 1409.1556</idno>
		<imprint>
			<date type="published" when="2014-09">09 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,481.32,337.98,7.86;16,151.52,492.28,225.42,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="16,259.84,481.32,220.75,7.86;16,151.52,492.28,73.14,7.86">QuickUMLS: a fast, unsupervised approach for medical concept extraction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,245.75,492.28,102.52,7.86">MedIR Workshop, SIGIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,503.24,337.98,7.86;16,151.52,514.19,329.07,7.86;16,151.52,525.15,322.28,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="16,350.91,503.24,129.69,7.86;16,151.52,514.19,142.94,7.86">Techniques for Medical Concept Detection from Multi-Modal Images</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sonker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pattnaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,315.40,514.19,165.19,7.86;16,151.52,525.15,132.29,7.86">CLEF2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,536.11,337.97,7.86;16,151.52,547.07,329.07,7.86;16,151.52,558.03,322.28,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="16,286.51,536.11,194.08,7.86;16,151.52,547.07,145.97,7.86">TUC MC group at ImageCLEFmed 2020 concept detection task using Xception models</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Udas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Beuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,317.41,547.07,163.18,7.86;16,151.52,558.03,132.29,7.86">CLEF2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,568.99,337.98,7.86;16,151.52,579.95,329.07,7.86;16,151.52,590.91,329.07,7.86;16,151.52,601.87,329.07,7.86;16,151.52,612.82,207.73,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="16,424.35,568.99,56.24,7.86;16,151.52,579.95,329.07,7.86;16,151.52,590.91,246.49,7.86">ChestX-Ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,421.07,590.91,59.52,7.86;16,151.52,601.87,270.08,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017">July 22-25, 2017. 2017</date>
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
