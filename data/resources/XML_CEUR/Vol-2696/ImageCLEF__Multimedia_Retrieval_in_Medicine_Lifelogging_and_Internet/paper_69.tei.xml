<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,166.07,115.96,283.22,12.62;1,145.04,133.89,325.27,12.62;1,222.61,151.82,170.14,12.62">The Inception Team at VQA-Med 2020: Pretrained VGG with Data Augmentation for Medical VQA and VQG</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,174.82,189.86,58.76,8.74"><forename type="first">Aisha</forename><surname>Al-Sadi</surname></persName>
							<email>asalsadi16@cit.just.edu.jo</email>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.29,189.86,77.28,8.74"><forename type="first">Hana</forename><surname>Al-Theiabat</surname></persName>
							<email>haaltheiabat13@cit.just.edu.jo</email>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.89,189.86,94.65,8.74"><forename type="first">Mahmoud</forename><surname>Al-Ayyoub</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,166.07,115.96,283.22,12.62;1,145.04,133.89,325.27,12.62;1,222.61,151.82,170.14,12.62">The Inception Team at VQA-Med 2020: Pretrained VGG with Data Augmentation for Medical VQA and VQG</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3A1BE41B31E011AB185A2F29372FCD56</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF 2020</term>
					<term>VQA-Med</term>
					<term>Visual Question Answering</term>
					<term>Visual Question Generation</term>
					<term>Medical Image Interpretation</term>
					<term>Medical Questions and Answers</term>
					<term>Transfer Learning</term>
					<term>VGG Network</term>
					<term>Augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the methodology of The Inception team participation at ImageCLEF Medical 2020 tasks: Visual Question Answering (VQA) and Visual Question Generation (VQG). Based on the data type and structure of the dataset, both tasks are treated as image classification tasks and are handled by using the VGG16 pre-trained model along with a data augmentation technique. In both tasks, our best approach achieves the second place with an accuracy of 48% in the VQA task and a BLEU score of 33.9% in the VQG task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, enormous research efforts have been invested in merging or fusing techniques and natural language processing, signal processing and computer vision with the aim of improving the interaction between humans and intelligent systems, which in turn gives rise to tasks like Visual Question Answering (VQA), Visual Question Generation (VQG), Multimodal Machine Translation, Video Captioning and Scene Understanding, etc. Such tasks require learning across multimodal features from both visual and language data <ref type="bibr" coords="1,384.55,541.51,15.50,8.74" target="#b17">[18,</ref><ref type="bibr" coords="1,401.71,541.51,12.73,8.74" target="#b22">23,</ref><ref type="bibr" coords="1,416.10,541.51,12.73,8.74" target="#b24">25,</ref><ref type="bibr" coords="1,430.49,541.51,7.75,8.74" target="#b4">5,</ref><ref type="bibr" coords="1,439.90,541.51,11.62,8.74" target="#b10">11]</ref>.</p><p>VQA and VQG are closely related tasks with significant importance <ref type="bibr" coords="1,442.37,553.83,9.96,8.74" target="#b2">[3]</ref>. VQA aims to answer a given question based on a given image. On the other hand, given an input image, VQG is concerned with generating relevant questions on this image along with their answers while relying only on the image content. Both tasks have attracted the attention of several researchers who proposed interesting models with promising results <ref type="bibr" coords="1,268.71,613.61,9.96,8.74" target="#b2">[3]</ref>.</p><p>Among the most notable efforts on VQA is a set of challenges or shared tasks held every year addressing different versions of VQA. Starting from 2016, a challenge on general VQA, known as the VQA Challenge,<ref type="foot" coords="2,396.01,141.33,3.97,6.12" target="#foot_0">1</ref> is held every year to answer questions of various types (multiple-choice questions, yes/no questions and questions with open-ended answers).</p><p>For the medical domain, a challenge known as the VQA-Med challenge has been held since in 2018. In its first version <ref type="bibr" coords="2,329.67,190.72,10.52,8.74" target="#b5">[6]</ref> only five teams participated in the task which focused on answering questions about abnormalities in medical images. For the second version, VQA-Med 2019 <ref type="bibr" coords="2,346.19,214.64,9.96,8.74" target="#b3">[4]</ref>, the challenge included four question categories on each medical image: the plane, the modality, the organ system and the abnormality shown in the image.</p><p>In this paper, we propose models to solve the medical VQA-Med tasks <ref type="bibr" coords="2,470.08,250.50,10.52,8.74" target="#b2">[3]</ref> (VQA and VQG) organized by ImageCLEF 2020 <ref type="bibr" coords="2,347.67,262.46,9.96,8.74" target="#b7">[8]</ref>. All of our models are based on the pre-trained convolutional neural networks (CNN), VGG16 <ref type="bibr" coords="2,420.78,274.41,14.61,8.74" target="#b21">[22]</ref>, and data augmentation technique. In both tasks, our best approach achieves the second place with an accuracy of 48% in the VQA task and a BLEU score of 33.9% in the VQG task.</p><p>The rest of this paper is organized as follows. The following section briefly surveys the literature on VQA and VQG. Section 3 presents the description of the both VQA-Med tasks (VQA and VQG) along with a detailed analysis of the dataset for each task. In Section 4, we present the proposed models for each task. Submission results and discussion are presented in Section 5. Finally, the paper is concluded in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>For the general VQA task, several researchers merge both visual and text information using encoder-decoder networks <ref type="bibr" coords="2,320.61,446.96,14.61,8.74" target="#b13">[14]</ref>, while others introduce attention over images to highlight the most important regions in the image to answer questions <ref type="bibr" coords="2,177.93,470.87,14.61,8.74" target="#b19">[20]</ref>. In <ref type="bibr" coords="2,211.19,470.87,14.61,8.74" target="#b12">[13]</ref>, the authors proposed a co-attention mechanism to jointly apply attention on both images and questions.</p><p>For medical VQA, the task is challenging as it requires a dedicated medical dataset and expert doctors to understand the data. VQA-Med <ref type="bibr" coords="2,415.47,506.73,10.52,8.74" target="#b6">[7]</ref> is a competition launched first in 2018, where it provides each year a medical dataset for the VQA task. Team UMass <ref type="bibr" coords="2,267.27,530.65,14.61,8.74" target="#b16">[17]</ref>, which achieved the highest BLEU score in 2018, proposed a co-attention method between images features, which are extracted from ResNet-152, and text features from pre-trained word embedding. Team JUST <ref type="bibr" coords="2,190.23,566.51,15.50,8.74" target="#b23">[24]</ref> used a simple encoder-decoder model based on Recurrent Neural Networks (RNN) with Long Short-Term Memory (LSTM) where the image features were extracted by a VGG16 model.</p><p>For the second version of the VQA-Med challenge in 2019, the team of Zhejiang University <ref type="bibr" coords="2,208.24,614.33,15.50,8.74" target="#b25">[26]</ref> was ranked first among 17 teams. This team adopted the co-attention mechanism to merge both visual and text features. The image features were extracted using the VGG16 network with Global Average Pooling strategy, while the Bidirectional Encoder Representations from Transformers (BERT) model was used to extract question features. The Multimodal factorized bilinear pooling (MFB) and multi-modal factorized high-order pooling (MFH) methods were used for features concatenation. In the same year. Our team, Team JUST <ref type="bibr" coords="3,192.42,166.81,9.96,8.74" target="#b0">[1]</ref>, was ranked among the top five teams by proposing a hierarchical model composed of multiple deep learning sub-models to handle different question types. All sub-models were based on a pre-trained VGG16 network as image classification, without considering questions as input features for the sub-models.</p><p>Visual Question Generation (VQG) is a task of developing visual understanding from images to generate reasonable questions with constraints (conditional VQG) <ref type="bibr" coords="3,165.17,251.11,15.50,8.74" target="#b11">[12]</ref> using labeled answers, or without constraints (unconditional) using only the image itself. Annotating multiple questions with each image as a VQG dataset was first collected by <ref type="bibr" coords="3,264.19,275.02,14.61,8.74" target="#b14">[15]</ref>.</p><p>Various works have adopted the multimodal context of the natural language of questions/answers and visual understanding of the image. The authors in <ref type="bibr" coords="3,465.10,299.54,15.50,8.74" target="#b26">[27]</ref> proposed an approach to understanding the semantics in the image by simultaneously training VQG and VQA models as it is viewed in <ref type="bibr" coords="3,396.38,323.45,15.50,8.74" target="#b11">[12]</ref> as a dual-task. In <ref type="bibr" coords="3,147.46,335.41,14.61,8.74" target="#b26">[27]</ref>, the VQG model used both RNN and CNN to let the model learn both natural language and vision aspects. Extending this in <ref type="bibr" coords="3,382.65,347.36,9.96,8.74" target="#b8">[9]</ref>, where the authors used Variational Autoencoder (VAE) with LSTM to generate several questions per image. On the other hand, a deep Bayesian multimodal network was proposed by <ref type="bibr" coords="3,176.61,383.23,15.50,8.74" target="#b15">[16]</ref> to generate a set of questions for each image.</p><p>Regarding VQG in the medical field, Sarrouti et al. <ref type="bibr" coords="3,368.80,395.79,15.50,8.74" target="#b18">[19]</ref> proposed an approach for VQG about radiology images called VQGR. The approach relied on VAE. To increase the dataset size, the same authors <ref type="bibr" coords="3,343.17,419.70,15.50,8.74" target="#b18">[19]</ref> applied data augmentation on both images and questions on the <ref type="bibr" coords="3,298.76,431.66,15.50,8.74" target="#b9">[10]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tasks and Dataset Description</head><p>In this section, we discuss the details and datasets of VQA-Med 2020's two tasks: VQA and VQG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">VQA Task and Dataset Description</head><p>The dataset of VQA-Med consists of 4,000 training medical images and 500 validation medical images, where each image is associated with a Question-Answer (QA) pair. Additionally, there are 500 medical test images with their questions.</p><p>The questions are from two types:</p><p>-Type 1: Questions asking about abnormalities in the image. For example, "what is the abnormality in this image". This type represents the majority of abnormality questions (98.5% of the training data, and 94.4% of the validation data).</p><p>-Type 2: Questions with yes/no answers that ask if the image is normal or not. For example, "is this image normal" or "is this image abnormal" (1.5% of the training data, and 5.6% of the validation data).</p><p>It is worth to mention some brief statistics about the dataset:</p><p>-There are only 38 unique questions in the 4,000 training questions.</p><p>-There are only 26 unique questions in the 500 validation questions.</p><p>-There are only 332 unique answers in the 4,000 training answers.</p><p>-There are only 232 unique answers from the 500 validation answers.</p><p>-Most unique answers are each associated with 5-20 questions/images, while the most repetitive answer is associated with 80 questions/images, and the least repetitive answer is associated with 3 questions/images.</p><p>Samples of the datasets are provided in Table <ref type="table" coords="4,356.36,277.18,3.87,8.74" target="#tab_0">1</ref>. The required in the VQA task is to answer the question given for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">VQG Task and Dataset Description</head><p>For the second task, VQG, the dataset consists of 780 images with 2,156 questions for training data, and 141 images with 164 questions for validation data. Each image is associated with one or more questions (up to 12 questions). For the test data, there are 80 images. The answers are given for the training and validation datasets but not for the test dataset. The questions on the images are diverse and are not necessarily related to the VQA dataset (i.e., questions are not limited to asking about abnormality in images).</p><p>It is worth to mention some brief statistics about the dataset:</p><p>-There are 1,942 unique questions in the 2,156 training questions.</p><p>-There are 161 unique questions in the 164 validation questions.</p><p>-Most questions occurred once, but some questions are associated with more than one image (up to 8 images). -Some questions in the validation data are not in the training data.</p><p>Samples of the datasets are provided in Table <ref type="table" coords="4,346.10,514.46,3.87,8.74">2</ref>. The requirement in the VQG task is to generate at least one question and a maximum of seven questions for each test image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this section, first, the description of the basic models is provided, followed by the details of each task submission.</p><p>We explore different approaches for the tasks at hand. However, most of these approaches are based on our experiments from last year's edition of the VQA-Med task <ref type="bibr" coords="4,178.17,644.16,9.96,8.74" target="#b0">[1]</ref>. We call this the basic model for any reference later. Following is a description of this model. The model is for image classification that uses the pre-trained model VGG16 with the last layer (the Softmax layer) removed and all layers (except the last four) are frozen. The output from this part is passed into two fully-connected layers with 1,024 hidden nodes, followed by a normalization layer. Finally, the output is passed into a Softmax layer with the required number of classes based on the task. Figure <ref type="figure" coords="5,221.33,559.67,4.98,8.74">1</ref> shows the architecture of the basic model.</p><p>The other pillar in our models is the augmentation technique. Although deep learning models have a remarkably excellent performance in several fields such as computer vision, they suffer from overfitting when the learners fit the training data perfectly. Usually, to avoid overfitting, the networks have to access more data. However, many applications lack the access to big data, such as medical image applications. One solution to increase the amount of the training data is data augmentation, which aims at increasing the diversity of the data without the need to collect new samples. For images, the augmentation done by applying one of the geometric transformations, flipping, padding or random erasing on the existing images to produce new images <ref type="bibr" coords="6,324.80,495.47,14.61,8.74" target="#b20">[21]</ref>.</p><p>For the VQA-Med tasks, we apply data augmentation on images to improve the performance of our models. We experiment with various ways of augmentation such as rotation change, width/ height shift, rescale, zoom and ZCA whitening.</p><p>ZCA whitening <ref type="bibr" coords="6,222.36,555.78,10.52,8.74" target="#b1">[2]</ref> is a whitening transformation used to decorrelate features in the data. It is widely used for images as it removes redundancy and highlight the structure of features. Not like PCA whitening, ZCA preserves the arrangement of the origin which makes it a good option for CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">VQA Task</head><p>A common way of dealing with VQA tasks is by using sequence-to-sequence (SeqtoSeq) models that merge image features and question features to predict the answer. However, due to the dataset nature and the repetitiveness of the questions, the contents of the questions are not expected to play a major role in answering the questions (aside from determining the questions type, etc.). On the other hand, the image features play a more significant role. Hence, we treat this task as an image classification task.</p><p>The core for our models is the basic model which is illustrated previously (see Figure <ref type="figure" coords="7,187.26,191.47,4.43,8.74">1</ref>) with additional modifications. Specifically, we build two models as follows.</p><p>-A model for classifying image into normal/abnormal for yes/no answers.</p><p>-A model for answering abnormalities questions.</p><p>For the first model (normal/abnormal), we use the basic model by passing the output to a Softmax layer with 2 classes (normal/abnormal). Images with their QA pairs used in this model are the ones associated with questions that start with "What".</p><p>For answering abnormalities questions by the second model, we use the same previous model's architecture, but the output is passed to a Softmax layer with 330 classes (the different 330 abnormalities in the training dataset).</p><p>For this task, we make five submissions. Table <ref type="table" coords="7,358.59,346.34,4.98,8.74" target="#tab_1">3</ref> summarizes the difference between these submissions, which are all based on the previous model description. For Submission 2-5, we apply data augmentation on images to improve the performance of the second model (the abnormalities model). We experiment with various ways of augmentation such as rotation change, width/ height shift, rescale, zoom, and ZCA whitening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">VQG Task</head><p>For the VQG task, based on the data description, answers are available only for training and validation data. So, we decide to build our model based on the images only. The first idea to apply in such tasks is the image captioning where the question is considered as the caption. This model is used in our first submission out of the five ones we made for this task.</p><p>The image captioning model is a Seq2Seq model. In each time step, the image features are concatenated with the current question word to extract the features in that time step. These features are passed to an LSTM layer, then to a dense layer followed by a Softmax layer to predict the next word. For the first step, the image with a predefined start word is used to predict the first word of the answer, and the image with the first word of the answer is used to predict the second word of the answer, and so on.</p><p>Due to the poor performance we get from this approach, we resort to treating this task as an image classification task in Submissions 2 and 3. We use the same basic image classification model, except that the output is passed to a Softmax layer with 2,072 classes (the different 2,072 unique questions in both the training and validation datasets). For Submissions 4 and 5, we use the augmentation technique with ZCA whitening beside the basic image classification model. Table <ref type="table" coords="8,403.66,391.11,4.98,8.74" target="#tab_3">4</ref> summarizes the difference between the five submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">VQA Task Results</head><p>Our team, The Inception Team, got the second place in the leaderboard, as shown in Figure <ref type="figure" coords="8,206.67,483.47,3.87,8.74" target="#fig_1">2</ref>, among the eleven participating teams with our best accuracy reaching 48%. This is lower than the first place by only 1.6%. Our best result is obvious in Submission 4, which uses our basic model and ZCA whitening augmentation and 50 epochs of training (see Table <ref type="table" coords="8,359.06,519.34,3.87,8.74" target="#tab_1">3</ref>).    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">VQG Task Results</head><p>In the VQG task, we also got second place in the leaderboard, as shown in Figure <ref type="figure" coords="9,167.24,495.00,3.87,8.74" target="#fig_2">3</ref>, among the three participating teams with our best accuracy 33.9% which is less than the first place by 0.9%. Our best result is via Submission 4, which uses our basic model and ZCA whitening augmentation (see Table <ref type="table" coords="9,468.97,518.91,3.87,8.74" target="#tab_3">4</ref>), predicting 7 questions for each image, and 100 epochs of training. Table <ref type="table" coords="9,177.11,543.15,4.98,8.74" target="#tab_5">6</ref> provides the BLEU scores of each of our submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>Several observations can be obtained from our experimentation. For example, the positive effect of using the augmentation technique is clear in both tasks. The models with augmented images results are better than the ones without augmentation, especially in the VQA task. For both tasks, after using ZCA whitening augmentation, the accuracy of the models has increased. The reason why other augmentation methods such as rotation and shifting have worsened the  model performance is due to the nature of radiology images which are sensitive to these changes as they might affect the purpose of images. It worth to mention that both of feature wise std normalization and feature center techniques have not outperformed the ZCA whitening. The distribution of data will be Gaussian distribution in the feature wise std normalization, as it divides each image by the std standard deviation of all images in the dataset. On the other hand, the mean of each image is set to zero for the feature center method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we describe a set of models we submitted for the ImageCLEF 2020 VQA-Med tasks. We showed that our intuition (based on our analysis of the dataset) of treating the tasks as image classification tasks is more useful than including a natural language processing (NLP) component as one would expect in VQA/VQG tasks. Our best model is based on VGG16 and augments the data using the ZCA whitening technique. We achieved 48% accuracy in the VQA task, and 33.9% Bleu score in the VQG task. These scores gave our team the second place in the two tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,240.76,449.24,130.77,7.89;6,134.77,318.82,345.82,115.66"><head>Table 2 .Fig. 1 .</head><label>21</label><figDesc>Fig. 1. Basic model architecture</figDesc><graphic coords="6,134.77,318.82,345.82,115.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,246.71,624.10,121.93,7.89;8,134.77,550.10,345.82,59.22"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. VQA task leaderboard</figDesc><graphic coords="8,134.77,550.10,345.82,59.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,246.55,189.83,122.25,7.89;10,134.77,115.84,345.82,59.22"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. VQG task leaderboard</figDesc><graphic coords="10,134.77,115.84,345.82,59.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,207.74,115.91,237.66,327.44"><head>Table 1 .</head><label>1</label><figDesc>VQA task dataset samples</figDesc><table coords="5,207.74,136.68,237.66,306.67"><row><cell>Image</cell><cell>Question</cell><cell>Answer</cell></row><row><cell></cell><cell>what is the primary abnormality in this image?</cell><cell>neurofibromatosis-1, nf1, nfr</cell></row><row><cell></cell><cell></cell><cell>incarcerated</cell></row><row><cell></cell><cell>what is abnormal in the x-ray?</cell><cell>diaphragmatic hernia presenting as</cell></row><row><cell></cell><cell></cell><cell>colonic obstruction.</cell></row><row><cell></cell><cell>is this image normal?</cell><cell>yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,168.48,115.91,278.39,235.88"><head>Table 3 .</head><label>3</label><figDesc>Description of VQA submissions</figDesc><table coords="8,168.48,137.23,278.39,214.56"><row><cell cols="2">Submission# Method</cell></row><row><cell>1</cell><cell>The basic model without any data augmentation, 50 epochs</cell></row><row><cell></cell><cell>The basic model +</cell></row><row><cell>2</cell><cell>data augmentation using featurewise center and</cell></row><row><cell></cell><cell>featurewise std normalization, 50 epochs</cell></row><row><cell></cell><cell>The basic model +</cell></row><row><cell>3</cell><cell>data augmentation using</cell></row><row><cell></cell><cell>ZCA whitening, 300 epochs</cell></row><row><cell></cell><cell>The basic model +</cell></row><row><cell>4</cell><cell>data augmentation using</cell></row><row><cell></cell><cell>ZCA whitening, 50 epochs</cell></row><row><cell></cell><cell>The basic model +</cell></row><row><cell></cell><cell>use the weights of the highest</cell></row><row><cell>5</cell><cell>accuracy model so far (Submission 4)</cell></row><row><cell></cell><cell>as pre-training +</cell></row><row><cell></cell><cell>data augmentation using ZCA whitening, 150 epochs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,149.71,656.12,311.09,8.74"><head>Table 5 ,</head><label>5</label><figDesc>provides accuracy and BLEU score of each of our submissions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,193.21,115.91,228.93,216.96"><head>Table 4 .</head><label>4</label><figDesc>Description of VQG submissions</figDesc><table coords="9,193.21,136.68,228.93,196.18"><row><cell>Submission#</cell><cell>Method</cell></row><row><cell></cell><cell>Image captioning model</cell></row><row><cell>1</cell><cell>50 epochs</cell></row><row><cell></cell><cell>predict one question for each image.</cell></row><row><cell></cell><cell>Image classification model</cell></row><row><cell>2</cell><cell>25 epochs</cell></row><row><cell></cell><cell>predict 7 questions for each image.</cell></row><row><cell></cell><cell>Image classification model</cell></row><row><cell>3</cell><cell>50 epochs</cell></row><row><cell></cell><cell>predict 7 questions for each image</cell></row><row><cell></cell><cell>Image classification model</cell></row><row><cell>4</cell><cell>data augmentation using ZCA whitening 100 epochs</cell></row><row><cell></cell><cell>predict 7 questions for each image.</cell></row><row><cell></cell><cell>Image classification model</cell></row><row><cell>5</cell><cell>data augmentation using ZCA whitening 50 epochs</cell></row><row><cell></cell><cell>predict 7 questions for each image.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,186.13,347.52,243.09,85.45"><head>Table 5 .</head><label>5</label><figDesc>The Inception team VQA task submissions results</figDesc><table coords="9,223.55,368.29,168.25,64.68"><row><cell cols="3">Submission# Accuracy BLEU Score</cell></row><row><cell>1</cell><cell>0.454</cell><cell>0.486</cell></row><row><cell>2</cell><cell>0.458</cell><cell>0.495</cell></row><row><cell>3</cell><cell>0.444</cell><cell>0.479</cell></row><row><cell>4</cell><cell>0.48</cell><cell>0.511</cell></row><row><cell>5</cell><cell>0.44</cell><cell>0.476</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,185.97,210.63,243.41,85.45"><head>Table 6 .</head><label>6</label><figDesc>The Inception team VQG task submissions results</figDesc><table coords="10,216.36,231.40,182.64,64.68"><row><cell cols="3">Submission# BLEU Score BLEU Score</cell></row><row><cell>1</cell><cell>0.031</cell><cell>0.486</cell></row><row><cell>2</cell><cell>0.314</cell><cell>0.495</cell></row><row><cell>3</cell><cell>0.319</cell><cell>0.479</cell></row><row><cell>4</cell><cell>0.339</cell><cell>0.511</cell></row><row><cell>5</cell><cell>0.331</cell><cell>0.476</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,656.80,128.38,7.86"><p>https://visualqa.org/index.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,612.53,337.63,7.86;10,151.52,623.49,329.07,7.86;10,151.52,634.44,25.60,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,414.02,612.53,66.57,7.86;10,151.52,623.49,213.51,7.86">Just at imageclef 2019 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Al-Sadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Talafha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jararweh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Costen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,386.24,623.49,94.35,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,645.84,337.64,7.86;10,151.52,656.77,216.14,7.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,264.03,645.84,216.57,7.86;10,151.52,656.80,43.01,7.86">The &quot;independent components&quot; of natural scenes are edge filters</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,200.75,656.80,61.08,7.86">Vision research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="3327" to="3338" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,119.67,337.63,7.86;11,151.52,130.63,329.07,7.86;11,151.52,141.59,329.07,7.86;11,151.52,152.55,322.28,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,151.52,130.63,329.07,7.86;11,151.52,141.59,134.88,7.86">Overview of the vqa-med task at imageclef 2020: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,309.01,141.59,171.58,7.86;11,151.52,152.55,132.29,7.86">CLEF 2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,163.52,337.64,7.86;11,151.52,174.48,329.07,7.86;11,151.52,185.44,161.72,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,166.36,174.48,314.24,7.86">Vqa-med: Overview of the medical visual question answering task at imageclef</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,189.66,185.44,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,196.41,337.63,7.86;11,151.52,207.37,329.07,7.86;11,151.52,218.33,329.07,7.86;11,151.52,229.29,104.56,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,330.28,196.41,150.31,7.86;11,151.52,207.37,207.56,7.86">Determine bipolar disorder level from patient interviews using bi-lstm and feature fusion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ebrahim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alsmirat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,404.26,207.37,76.33,7.86;11,151.52,218.33,323.79,7.86">Fifth International Conference on Social Networks Analysis, Management and Security (SNAMS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="182" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,240.26,337.63,7.86;11,151.52,251.22,329.07,7.86;11,151.52,262.17,46.58,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="11,431.40,240.26,49.18,7.86;11,151.52,251.22,279.75,7.86">Overview of the ImageCLEF 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,273.14,337.64,7.86;11,151.52,284.10,329.07,7.86;11,151.52,295.06,55.09,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,432.03,273.14,48.57,7.86;11,151.52,284.10,244.73,7.86">Overview of imageclef 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,415.87,284.10,64.72,7.86;11,151.52,295.06,26.42,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,306.03,337.64,7.86;11,151.52,316.99,329.07,7.86;11,151.52,327.95,329.07,7.86;11,151.52,338.91,329.07,7.86;11,151.52,349.87,329.07,7.86;11,151.52,360.83,329.07,7.86;11,151.52,371.79,329.07,7.86;11,151.52,382.74,329.07,7.86;11,151.52,393.70,329.07,7.86;11,151.52,404.66,329.07,7.86;11,151.52,415.62,34.31,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,295.04,360.83,185.55,7.86;11,151.52,371.79,255.37,7.86">Overview of the ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,426.43,371.79,54.16,7.86;11,151.52,382.74,329.07,7.86;11,151.52,393.70,253.34,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="11,456.14,393.70,24.45,7.86;11,151.52,404.66,138.63,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="11,142.96,426.59,337.63,7.86;11,151.52,437.55,329.07,7.86;11,151.52,448.51,219.45,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,295.48,426.59,185.11,7.86;11,151.52,437.55,97.77,7.86">Creativity: Generating diverse questions using variational autoencoders</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,272.33,437.55,208.27,7.86;11,151.52,448.51,126.18,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6485" to="6494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,459.48,337.98,7.86;11,151.52,470.44,329.07,7.86;11,151.52,481.37,69.81,7.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,393.93,459.48,86.66,7.86;11,151.52,470.44,262.04,7.86">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,422.20,470.44,58.40,7.86">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,492.37,337.98,7.86;11,151.52,503.33,329.07,7.86;11,151.52,514.29,88.49,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,347.82,492.37,132.77,7.86;11,151.52,503.33,71.63,7.86">A survey on multimodal medical data visualization</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lawonn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">N</forename><surname>Smit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bühler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Preim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,246.86,503.33,109.37,7.86">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="413" to="438" />
			<date type="published" when="2018">2018</date>
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,525.26,337.98,7.86;11,151.52,536.22,329.07,7.86;11,151.52,547.17,329.07,7.86;11,151.52,558.13,25.60,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,455.20,525.26,25.39,7.86;11,151.52,536.22,247.98,7.86">Visual question generation as dual task of visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,421.65,536.22,58.95,7.86;11,151.52,547.17,265.77,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6116" to="6124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,569.10,337.98,7.86;11,151.52,580.06,329.07,7.86;11,151.52,591.02,76.80,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,305.72,569.10,174.87,7.86;11,151.52,580.06,100.83,7.86">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,273.42,580.06,202.96,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="289" to="297" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,601.99,337.97,7.86;11,151.52,612.95,329.07,7.86;11,151.52,623.91,212.06,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,324.84,601.99,155.75,7.86;11,151.52,612.95,174.95,7.86">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,347.36,612.95,133.22,7.86;11,151.52,623.91,146.92,7.86">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,634.88,337.97,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,25.60,7.86" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06059</idno>
		<title level="m" coord="11,151.52,645.84,187.50,7.86">Generating natural questions about an image</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,119.67,337.98,7.86;12,151.52,130.63,318.96,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="12,387.03,119.67,93.57,7.86;12,151.52,130.63,152.79,7.86">Multimodal differential network for visual question generation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">N</forename><surname>Patro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">K</forename><surname>Kurmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03986</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,141.59,337.98,7.86;12,151.52,152.55,259.13,7.86" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Rosen</surname></persName>
		</author>
		<title level="m" coord="12,284.94,141.59,195.65,7.86;12,151.52,152.55,114.54,7.86;12,287.06,152.55,94.91,7.86">Umass at imageclef medical visual question answering (med-vqa) 2018 task</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>CLEF (Working Notes)</note>
</biblStruct>

<biblStruct coords="12,142.62,163.51,337.98,7.86;12,151.52,174.44,311.39,7.89" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,294.42,163.51,186.17,7.86;12,151.52,174.47,80.39,7.86">Deep multimodal learning: A survey on recent advances and trends</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramachandram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,238.94,174.47,136.56,7.86">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="96" to="108" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,185.43,337.98,7.86;12,151.52,196.39,329.07,7.86;12,151.52,207.34,196.98,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,372.35,185.43,108.25,7.86;12,151.52,196.39,90.28,7.86">Visual question generation from radiology images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,266.00,196.39,214.59,7.86;12,151.52,207.34,122.07,7.86">Proceedings of the First Workshop on Advances in Language and Vision Research</title>
		<meeting>the First Workshop on Advances in Language and Vision Research</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,218.30,337.98,7.86;12,151.52,229.26,329.07,7.86;12,151.52,240.22,136.20,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,285.91,218.30,194.69,7.86;12,151.52,229.26,38.08,7.86">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,209.93,229.26,270.67,7.86;12,151.52,240.22,43.20,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4613" to="4621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,251.18,337.97,7.86;12,151.52,262.11,185.18,7.89" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,289.44,251.18,191.15,7.86;12,151.52,262.14,30.97,7.86">A survey on image data augmentation for deep learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,189.43,262.14,80.53,7.86">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,273.10,337.97,7.86;12,151.52,284.06,231.27,7.86" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="12,278.92,273.10,201.67,7.86;12,151.52,284.06,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,295.02,337.97,7.86;12,151.52,305.98,329.07,7.86;12,151.52,316.93,97.80,7.86" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="12,383.12,295.02,97.47,7.86;12,151.52,305.98,259.01,7.86">Visual question answering using deep learning: A survey and performance analysis</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01860</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,327.89,337.98,7.86;12,151.52,338.85,95.81,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,272.62,327.89,159.38,7.86">Just at vqa-med: A vgg-seq2seq model</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Talafha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,455.89,327.89,24.70,7.86;12,151.52,338.85,67.14,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,349.81,337.97,7.86;12,151.52,360.77,329.07,7.86;12,151.52,371.73,147.82,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,381.32,349.81,99.27,7.86;12,151.52,360.77,77.65,7.86">Dual learning for visual question generation</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,251.31,360.77,229.28,7.86;12,151.52,371.73,53.70,7.86">2018 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,382.69,337.97,7.86;12,151.52,393.65,312.89,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="12,309.14,382.69,171.45,7.86;12,151.52,393.65,167.55,7.86">Zhejiang university at imageclef 2019 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,340.83,393.65,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,404.61,337.98,7.86;12,151.52,415.56,329.07,7.86;12,151.52,426.52,25.60,7.86" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="12,339.60,404.61,140.99,7.86;12,151.52,415.56,187.70,7.86">Neural self talk: Image understanding via continuous questioning and answering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fermuller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03460</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
