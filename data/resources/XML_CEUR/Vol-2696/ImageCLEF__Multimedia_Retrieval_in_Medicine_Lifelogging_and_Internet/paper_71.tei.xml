<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.89,115.96,339.57,12.62;1,235.43,133.89,144.50,12.62">Techniques for Medical Concept Detection from Multi-Modal Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,166.29,171.77,55.35,8.74"><forename type="first">Rohit</forename><surname>Sonker</surname></persName>
							<email>rohit.sonker@pwc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">PricewaterhouseCoopers US Advisory</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.56,171.77,59.35,8.74"><forename type="first">Ayush</forename><surname>Mishra</surname></persName>
							<email>ayush.mishra@pwc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">PricewaterhouseCoopers US Advisory</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,296.94,171.77,64.54,8.74"><forename type="first">Palvika</forename><surname>Bansal</surname></persName>
							<email>palvika.lnu@pwc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">PricewaterhouseCoopers US Advisory</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,384.17,171.77,64.89,8.74"><forename type="first">Anup</forename><surname>Pattnaik</surname></persName>
							<email>anup.a.pattnaik@pwc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">PricewaterhouseCoopers US Advisory</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.89,115.96,339.57,12.62;1,235.43,133.89,144.50,12.62">Techniques for Medical Concept Detection from Multi-Modal Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A5744D91F9A8E0534C4CA7842D3C5B63</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Captioning</term>
					<term>Medical Imaging Modalities</term>
					<term>Deep Learning</term>
					<term>Machine Learning</term>
					<term>Concept Detection</term>
					<term>Information Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the increasing availability of medical images coming from different modalities (X-Ray, CT, PET, MRI, Ultrasound, etc.), the task of automatic medical image captioning is emerging as a key component in medical research. ImageCLEF 2020 is dedicated to extracting relevant concepts from a large corpus of radiology medical images with different image modalities by learning the visual contents of the images. The variability between modalities and expertise required in interpreting radiology images often represents a bottleneck in clinical diagnosis pipelines. Therefore, we propose a reliable automatic classification method which is highly desired as assistance for human radiologists in producing reports more accurately and efficiently. Throughout the experiment, we leveraged CNN Architectures, NLP, and clustering techniques to come up with our best system. In this paper, we introduce a novel technique of band classification, where we first cluster the vocabulary of concepts into bands and then build customized classification architectures for each of the band. Predictions of one band are given as input to subsequent bands to aid the learning of associated concepts. Also, we systematically explored several pre-processing approaches to handle variations in contrasts, intensities across images of different modalities. In the final evaluation of ImageCLEF 2020, we submitted 9 runs out of which our best systems ranked 3rd, 4th, 5th. Overall our team ranked 2nd among 41 participants globally.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The healthcare industry has been witnessing an increasing shift towards digitization across the world. With more and more hospitals now saving their patient data and medical images electronically, the platform is set to leverage AI capabilities to assist doctors in their diagnosis and enhance the entire healthcare ecosystem. Medical Images, ranging from MRI scans, CT scans, PET scans, X-Ray are used for diagnosis and treatment of many diseases such as cancer, pneumonia, and pneumothorax. Medical domain experts go through the medical scans of the patient and subsequently write a condensed textual report, which is a time-consuming process and also leads to an increase in the cost of such treatment. The reading and interpretation of medical images, like all other human processes, are prone to error.</p><p>The above stated problems and the abundance of medical images in the current scenario have motivated us to use AI techniques to semi-automate the report generation process. We intend to build a pipeline that will take a medical image and caption out the keywords stating the abnormalities and the type of machine used to produce the image. The output caption will not be a free flowing text in Natural Language which would make sense but just a list of keywords next to each other. We hope this system will be an efficient secondary check for the medical experts and will also speed up the report writing process.</p><p>ImageCLEF <ref type="bibr" coords="2,205.96,299.21,10.52,8.74" target="#b3">[4]</ref> hosted the 4th edition of its Medical Image Captioning task where we were provided with a subset of Radiology Objects in Context (ROCO) dataset <ref type="bibr" coords="2,170.64,323.12,9.96,8.74" target="#b7">[8]</ref>. We have used multiple approaches to tackle the problem leveraging deep learning architectures and NLP techniques. Before passing the image into the deep neural network models, they are first passed through certain preprocessing steps, which are further explained in section 3.</p><p>We have used the following methods to extract the keywords from a given patient medical image - We will explain all the above mentioned methods in detail in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>ImageCLEF 2020 MedCaption task <ref type="bibr" coords="2,294.26,548.52,10.52,8.74" target="#b6">[7]</ref> focused on extracting information from radiology images. The dataset provided as part of the challenge is a subset of the extended ROCO dataset, with additional imaging modality information. A total of 6,031,814 image-caption pairs were extracted. To focus on radiology images and non-compound figures, automatic filtering with deep learning systems as well as manual revisions were applied. Post the filtering, a total of 80,183 images were provided to us, out of which 64,753 images were part of the training set and 15,970 images were part of the validation set. There were 3047 unique concepts in the training set. For a given image, the number of concepts were in the range of 1-140. The average number of concepts per image was close to 6. All the images were in jpeg format. The number of channels within the images was not consistent.</p><p>There were several challenges within the dataset. There was significant noise in most of the images. The noises ranged from doctor signatures, patient ID and other random numbers inscribed on top of the radiology images. While most of the images were restricted to a given organ of the body (fig. <ref type="figure" coords="3,414.91,178.90,3.60,8.74" target="#fig_0">1</ref>), some of the images had the entire human body and a zoomed-in scan of a particular organ. Extracting captions from these types of images would be quite difficult as the AI system has to focus only on the zoomed-in part to understand the abnormalities. The zoomed scan in such images occupied very limited space making it more difficult to focus on the relevant part. Many images also had arrows, straight lines, and the watermark of the hospital or organization where the scan was taken, as shown in fig. <ref type="figure" coords="3,232.75,262.59,3.51,8.74" target="#fig_1">2</ref>.  The data was quite skewed in terms of the number of images in which a particular concept occurred. The range varied from 34 to 20031 which depicts the level of skewness. As part of exploration, we also extracted the text descriptions of the concept IDs that were provided to us. The caption was processed using QuickUMLS <ref type="bibr" coords="4,192.60,130.95,10.52,8.74" target="#b8">[9]</ref> to produce the gold UMLS concept unique identifiers (CUIs). We have further used the text extracted in one of the techniques. However, the text description for 12 concepts were not available. Table <ref type="table" coords="4,176.85,434.90,4.98,8.74" target="#tab_1">1</ref> shows list of the top 10 most frequently occurring concept IDs along with their descriptions: 3 Data Pre-processing</p><p>We explored different pre-processing approaches for different systems of models.</p><p>In general the CLAHE technique was used in most of the systems.</p><p>Contrast Limited Adaptive Histogram Equalization (CLAHE) : Images of different modalities have varying brightness, intensity, contrasts, etc, To handle these modalities, and to enhance feature detection, we used the CLAHE <ref type="bibr" coords="5,134.77,219.97,15.50,8.74" target="#b10">[11]</ref> technique as the first step. CLAHE equalizes brightness and contrast among images. An image is divided into regions and each region is histogram equalized.</p><p>To limit noise amplification, contrast limiting is applied. It strengthens feature extraction from the edges of each region in the image. In this, each pixel is transformed based on the histogram surrounding the pixel. CLAHE limits the amplification by clipping the histogram at a predefined value called a clip limit.</p><p>During our experiments, we used the clip limit of 2.5, as we got the best results from this value. Example is shown in fig. <ref type="figure" coords="5,314.09,303.66,3.65,8.74" target="#fig_3">4</ref> (a) Before (b) After  </p><formula xml:id="formula_0" coords="6,246.00,323.65,234.60,22.31">I norm = ( I -shif t scale ) × 2 -I<label>(1)</label></formula><p>The effect of range normalization is shown in fig. <ref type="figure" coords="6,363.98,357.34,3.51,8.74" target="#fig_5">6</ref>. The input images were first all transformed into 3 channel RGB images and then resized to 224 × 224 × 3 which is the input requirement shape for ResNet architectures <ref type="bibr" coords="7,193.25,163.45,9.96,8.74" target="#b2">[3]</ref>. Then the images were passed through a series of pre-processing steps for noise removal and enhancement of brightness and contrast. We have used Keras ImageDataGenerator in all of our techniques to load the images on the go and avoiding the need to load all the training images at once, which will require a lot of disk space. We also performed data augmentation using Keras DataGenerator. The images in the training as well as the validation set were given in 7 different folders, each representing the scan type of the images.</p><p>Since each image could have multiple CUIs tagged to it, we create a CUI index dictionary mapping concept IDs to integers ranging from 0 to 3046. We created target vectors for each image as a 3047 long vector. The vector will have 1 when the concept ID corresponding to the index of the vector is present in the caption of the image. All the operations were done using Python 3.6.2 and Keras framework. The Deep Learning models were trained on a virtual Ubuntu server machine equipped with 2 NVIDIA Tesla P4 GPU accelerators. The GPUs were accessed using the Google Cloud Platform. We now describe each technique variant in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">System 1: ResNet18 on All Data</head><p>Training Images varied a lot from each other across scan types as well as within the same type of scan. Despite using multiple pre-processing steps to remove noise, performing a multilabel classification with 3047 images was a daunting task. A convolutional neural network with enough depth was the obvious choice to learn the features and patterns and perform the above mentioned task. We chose a pre-trained ResNet18 as our baseline model. In this particular approach, we keep all the layers trainable. We start-off with weights trained on ImageNet data <ref type="bibr" coords="7,157.20,469.56,9.96,8.74" target="#b1">[2]</ref>. These weights are trained on images that are completely different from medical images, but they are still a better place to start with as compared to random weights, assuming that the model will be able to extract the high-level features of the image and learn the custom features when trained on our medical images. We took the output of the penultimate layer of ResNet18 and then passed that tensor through a convolutional layer and maxpool layer and finally through 3 fully connected layers. We used sigmoid function as the activation for the last layer to enable multi label outputs. We used binary cross entropy as the loss function and Adam optimizer <ref type="bibr" coords="7,286.53,565.20,10.52,8.74" target="#b4">[5]</ref> with a learning rate of 0.0001. The batch size was fixed at 32. The model was trained for 50 epochs and we observed the validation F1 score saturated post 42 epochs. Training each epoch took close to 840 seconds. The model architecture is described in fig. <ref type="figure" coords="7,376.85,601.07,3.50,8.74" target="#fig_6">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">System 2: ResNet18 on Scan Type</head><p>The given dataset had seven types of scanned medical images in their respective folders. A type of scanned image might have different features than another one  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">System 3: Band Classification</head><p>In this approach, we aim to separately handle the concepts which are predicted correctly versus the concepts which are not being predicted well. The idea behind this approach is to use different network parameters for different sets of labels and to allow correlated labels to enhance performance. The bands consist of different sets of labels categorized by the prediction performance on the complete ResNet18 network. The images are preprocessed using CLAHE and the training set is augmented. Hence, we first filter out the concepts which are being predicted well by our overall ResNet18 model. We first calculate the F1 score corresponding to each concept and filter out concepts that have an F1 score &gt; 0.2. This forms our first band of target concepts. Subsequently, the remaining concepts are considered as band 2. Based on our threshold of 0.2, band 1 contains 25 concepts and band 2 contains 3022 concepts. Further decomposition to more bands is also possible by repeating the process however, we notice that in band 2, there is no wide separation as to which concepts perform well, hence, no further decomposition is considered.</p><p>We train two separate neural networks to predict the concepts in band 1 and band 2, for all images. The images are first passed through band 1 network and their predictions are generated. In the second band, the predictions of the first band are added as an auxiliary input to the network. This is done to include associative information between concepts which may aid in learning concepts that were not being predicted well in a combined network. We use a ResNet 18 model with additional layers for both the band networks. The networks vary slightly in their architecture, due to the addition of more layers and an auxiliary input in band 2.</p><p>In band 1, the ResNet18 architecture (without top layer) is followed by a 2D Convolution layer (128,3,3), max pooling (2,2), and two fully connected layers of 512 and 256 neurons respectively. The fully connected layers have ReLU activation. All additional layers have a dropout of 0.2 while training. Finally, we have an output layer (25 neurons) with sigmoidal activation.</p><p>In band 2, we have a similar structure with ResNet18 (without top layer) followed by a 2D Convolution layer (128,3,3) with max pooling (2,2) and fully connected (FC) layers of 1024 neurons each. We also have an auxiliary input which is the prediction result of band 1. The input is passed through a layer of 256 neurons and concatenated to the FC layer as mentioned above. Finally, the combined input is passed through an FC layer with 1024 neurons and attached to a sigmoid output layer of 3022 neurons. The architecture is shown in fig. <ref type="figure" coords="9,464.60,593.64,3.51,8.74" target="#fig_8">9</ref>.</p><p>The outputs from both are probabilities of each label. To convert this to a one-hot encoded vector we set a threshold based on the maximum F1 score on the validation set. The outputs from both bands are combined to get the complete prediction for 3047 concepts. This process happens in both bands. The value of the threshold used in band 1 and band 2 was 0.3 and 0.25 respectively. In this approach, we use K-Nearest Neighbour algorithm <ref type="bibr" coords="10,386.69,377.61,10.52,8.74" target="#b0">[1]</ref> on ResNet101 embeddings <ref type="bibr" coords="10,176.86,389.57,9.96,8.74" target="#b2">[3]</ref>. For each test image, the K-most similar images from the training set are retrieved and their labels are used to predict labels of the test image. This approach is implemented independently for each modality.</p><p>The training images are first converted to embeddings using a ResNet101 encoder. We use ResNet101 pre-trained on ImageNet data, without further training of the network. The input images are preprocessed using CLAHE. No augmentation of the training set is performed here. The embeddings of dimension <ref type="bibr" coords="10,442.40,462.48,12.73,8.74" target="#b6">(7,</ref><ref type="bibr" coords="10,455.13,462.48,8.49,8.74" target="#b6">7,</ref><ref type="bibr" coords="10,463.62,462.48,16.97,8.74">512)</ref> are extracted from ResNet101 and then flattened.</p><p>These training embeddings are added as a new layer to the encoder network, such that for each input image, we get a similarity value corresponding to each training image. This architecture allows us to compute the cosine similarity of each input with respect to training images. The architecture is depicted in fig. <ref type="figure" coords="10,469.44,523.43,7.43,8.74" target="#fig_9">10</ref>. Hence, each test image is first encoded and then its cosine similarity with respect to each training image embedding is computed by the network.</p><p>To compute the output labels the following approach is used. The labels of these K images are taken as one-hot encoded vectors and first added then normalized. Hence, get a vector with values ranging from 0-1 for all labels. We then select a threshold value t to convert this combined vector to a one-hot encoded label vector. Note, that this process is done separately for each modality. Hence, we have a total of 7 models each with their own values of K and t. K is chosen in proportion to the size of the training set. The threshold value is chosen by analyzing the best performance on the validation set. The details of K,t chosen are in the table 2.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">System 5: KNN on ResNet18 Embeddings with weighted label combination</head><p>In this approach, we use a K-Nearest Neighbour algorithm on ResNet18 embeddings. A single ResNet18 network is used here for all modalities. The images are preprocessed using CLAHE and the training set is augmented using the methodology described earlier. The ResNet18 network is first trained on the training set as described in the section on ResNet18 (all data) and then the top layer is removed. This forms the encoder network.</p><p>The training images are converted into embeddings using the encoder and these embeddings are used as layers on top of the encoder network. This leads to architecture similar to the previous approach however all modalities are taken together here. A test image is first encoded and its cosine similarity to all training images is computed. We select a K of 200 and all label vectors of these closest training images are retrieved.</p><p>In this approach, we define a new method to combine these K one-hot label vectors. We use a method similar to term frequency-inverse document frequency (TF-IDF) to combine values for each label. The combined value is proportional to the occurrence of a label in the K images and inversely proportional to their occurrence in the complete training set. This allows rare concepts to be predicted more. The formula is described below. Let L be the combined vector of the sum of K one hot encoded label vectors of the nearest neighbours for a particular test point. Let R be the resulting combined label vector for our test point. The for each label i the value R i is defined as -</p><formula xml:id="formula_1" coords="12,179.28,137.40,59.60,22.31">R i = L i max(L)</formula><p>× log( size of training set f requency of label i in training set )</p><p>Once the vector R has been computed, we must convert this vector of values to a one-hot encoded vector to predict labels. For this, we set a threshold value which gives the best performance on the validation set, hence threshold t = 1.17.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">System 6: Concept Clustering based data segregation</head><p>The Concept Unique identifier (alphanumeric code for concepts) associated with the image must have some relation with each other, for example, an aneurism would be more closely related with blood clot than a fractured bone. In this approach, we tried to group together such concepts that were closely related to each other. However, this relationship would be hard to determine using CUIs which were assigned to a given image, hence we converted the unique identifiers into human readable comments using UMLS conversion <ref type="bibr" coords="12,387.09,314.30,9.96,8.74" target="#b8">[9]</ref>. Once we had the UMLS converted concepts, we extracted embeddings for each using BioWordVec <ref type="bibr" coords="12,134.77,338.21,15.50,8.74" target="#b9">[10]</ref> which is a word to vector converter trained on vocabulary frequently used in the medical field. We then calculated the closeness of a concept with respect to others using cosine similarity and then grouped them into 6 clusters using the k-means algorithm <ref type="bibr" coords="12,219.26,374.08,9.96,8.74" target="#b5">[6]</ref>.</p><p>Once we have a data table with similarity scores of concept with each other, it is divided into 5 clusters using k-means clustering. Each cluster had between 30,000 to 42,000 images and 400 to 600 concepts associated with it, except for the 6th cluster which had all the images with concepts whose embedding was not found using BioWordVec which had approximately 26000 images and 170 concepts. The process is described in fig. <ref type="figure" coords="12,313.79,445.81,7.43,8.74" target="#fig_10">11</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We submitted our concept predictions on the test set in a txt file, in which each row corresponds to the image ID followed by the predicted Concept IDs of that image via our models. The predictions were evaluated using F1 score, by comparing the ground truth vector y true to the predicted concept vector y pred and then averaging across all test images. Both the vectors were 3047 in length, which is equal to the number of unique classes present. The KNN clustering using ResNet-101 embeddings gave the best F1 score of 0.392. All results are shown in table <ref type="table" coords="13,201.51,504.82,3.87,8.74" target="#tab_3">3</ref>. Throughout the challenge, we experimented with visual features of images as well as the UMLS embeddings and tried to benefit from the concept grouping through the clustering mechanism. The setup is motivated by the variabilities in modalities of radiology images and stimulus to extract value from all the data that is provided. Our best model KNN with ResNet101 embeddings achieved an F1 score of 0.39238 and ranked 3rd. The band classification approach that we introduced in this paper allows the usage of different network architectures for different sets of labels. Incorporating predictions from a set of bands in making predictions for other bands promises an increase in performance in scenarios where the concepts in bands are associated with each other.</p><p>In future work, we aim to experiment with the attention mechanism to focus on important features of images to improve concept detection and interpretability of the models. We also aim to improve the performance of the band classification approach. Exploring skewness in data availability for different concepts could be an interesting extension to our models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,197.91,415.50,219.53,8.74;3,169.35,294.27,103.75,92.76"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Images from ImageCLEFmed Caption 2020</figDesc><graphic coords="3,169.35,294.27,103.75,92.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,197.91,595.66,219.53,8.74;3,169.35,471.65,103.75,95.55"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Images from ImageCLEFmed Caption 2020</figDesc><graphic coords="3,169.35,471.65,103.75,95.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,180.85,344.45,253.66,8.74;4,186.64,198.48,242.08,134.44"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Frequency of concept occurrence in training images</figDesc><graphic coords="4,186.64,198.48,242.08,134.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,181.08,453.26,253.20,8.74;5,197.02,334.92,83.00,89.89"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: CLAHE processing on Image ROCO2 CLEF 57065</figDesc><graphic coords="5,197.02,334.92,83.00,89.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,171.26,246.27,272.84,8.74;6,197.02,115.84,83.00,101.97"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Intensity Normalisation on Image ROCO2 CLEF 31504</figDesc><graphic coords="6,197.02,115.84,83.00,101.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,176.75,508.88,261.86,8.74;6,197.02,391.87,83.00,88.55"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Range Normalisation on Image ROCO2 CLEF 06408</figDesc><graphic coords="6,197.02,391.87,83.00,88.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,171.01,223.41,273.33,8.74;8,169.35,115.84,276.67,96.05"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Model Architecture for ResNet18 trained on entire data</figDesc><graphic coords="8,169.35,115.84,276.67,96.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,171.58,630.17,272.20,8.74;8,152.06,443.58,311.23,175.07"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Model Architecture for ResNet18 trained on Scan Type</figDesc><graphic coords="8,152.06,443.58,311.23,175.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="10,189.50,301.26,236.36,8.74;10,169.35,115.84,276.66,173.90"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Model architecture in band classification model</figDesc><graphic coords="10,169.35,115.84,276.66,173.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="11,165.20,210.90,284.96,8.74;11,152.06,115.83,311.24,83.54"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Model architecture for KNN with ResNet101 embeddings</figDesc><graphic coords="11,152.06,115.83,311.24,83.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="12,211.64,572.40,192.08,8.74;12,160.70,476.84,293.96,84.03"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Process to generate concept clusters</figDesc><graphic coords="12,160.70,476.84,293.96,84.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,182.61,502.01,250.14,130.19"><head>Table 1 : Top 10 concept in training set by frequency</head><label>1</label><figDesc></figDesc><table coords="4,182.61,514.34,250.14,117.85"><row><cell>Concept ID</cell><cell>UMLS Term Description</cell><cell>No. of Images</cell></row><row><cell cols="2">C0040398 Tomography, Emission-Computed</cell><cell>20031</cell></row><row><cell>C0040405</cell><cell>X-Ray Computed Tomography</cell><cell>20031</cell></row><row><cell cols="2">C0043299 Diagnostic Radiologic Examination</cell><cell>18944</cell></row><row><cell>C0024485</cell><cell>Magnetic Resonance Imaging</cell><cell>11447</cell></row><row><cell>C0041618</cell><cell>Ultrasonography</cell><cell>8629</cell></row><row><cell>C0002978</cell><cell>Angiogram</cell><cell>4713</cell></row><row><cell>C0018792</cell><cell>Heart Atrium</cell><cell>1262</cell></row><row><cell>C0021853</cell><cell>Intestines</cell><cell>1219</cell></row><row><cell>C0025066</cell><cell>Mediastinum</cell><cell>1205</cell></row><row><cell>C0227665</cell><cell>Both kidneys</cell><cell>1186</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,146.07,249.72,323.22,97.31"><head>Table 2 :</head><label>2</label><figDesc>Number of neighbours K and threshold value t for each modality</figDesc><table coords="11,220.98,262.06,173.39,84.97"><row><cell cols="3">Modality No. of Neighbours K Threshold t</cell></row><row><cell>DRMR</cell><cell>200</cell><cell>0.20</cell></row><row><cell>DRXR</cell><cell>200</cell><cell>0.26</cell></row><row><cell>DRAN</cell><cell>50</cell><cell>0.20</cell></row><row><cell>DRUS</cell><cell>100</cell><cell>0.22</cell></row><row><cell>DRCT</cell><cell>200</cell><cell>0.20</cell></row><row><cell>DRCO</cell><cell>25</cell><cell>0.35</cell></row><row><cell>DRPE</cell><cell>25</cell><cell>0.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="13,140.77,549.91,333.82,86.35"><head>Table 3 :</head><label>3</label><figDesc>Results of different methods on test data</figDesc><table coords="13,140.77,562.24,333.82,74.02"><row><cell>System</cell><cell>Technique</cell><cell>F1 Score</cell></row><row><cell>1</cell><cell>ResNet18 across all data</cell><cell>0.368</cell></row><row><cell>2</cell><cell>ResNet18 on Scan Type</cell><cell>0.389</cell></row><row><cell>3</cell><cell>Band Classification</cell><cell>0.338</cell></row><row><cell>4</cell><cell>KNN on ResNet101 Embeddings Modality wise</cell><cell>0.392</cell></row><row><cell>5</cell><cell cols="2">KNN on ResNet18 Embeddings with Weighted label combination 0.366</cell></row><row><cell>6</cell><cell>Concept Clustering based data segregation</cell><cell>0.316</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="14,142.96,370.20,337.63,7.86;14,151.52,381.13,162.58,7.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,232.33,370.20,153.33,7.86">Nearest neighbor pattern classification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,392.03,370.20,88.56,7.86;14,151.52,381.16,79.65,7.86">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,392.39,337.64,7.86;14,151.52,403.35,223.70,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,399.39,392.39,81.21,7.86;14,151.52,403.35,137.62,7.86">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,310.72,403.35,35.84,7.86">CVPR09</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,414.59,337.64,7.86;14,151.52,425.55,329.08,7.86;14,151.52,436.51,76.80,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,299.05,414.59,177.61,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,165.33,425.55,310.07,7.86">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,447.74,337.64,7.86;14,151.52,458.70,329.07,7.86;14,151.52,469.66,329.07,7.86;14,151.52,480.62,329.07,7.86;14,151.52,491.58,329.07,7.86;14,151.52,502.54,329.07,7.86;14,151.52,513.50,329.07,7.86;14,151.52,524.46,329.07,7.86;14,151.52,535.42,329.07,7.86;14,151.52,546.37,329.07,7.86;14,151.52,557.33,34.31,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,295.04,502.54,185.55,7.86;14,151.52,513.50,255.37,7.86">Overview of the ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,426.43,513.50,54.16,7.86;14,151.52,524.46,329.07,7.86;14,151.52,535.42,253.34,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="14,456.14,535.42,24.45,7.86;14,151.52,546.37,138.63,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="14,142.96,568.57,337.63,7.86;14,151.52,579.53,93.19,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="14,239.72,568.57,176.61,7.86">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.96,590.77,337.64,7.86;14,151.52,601.73,329.07,7.86;14,151.52,612.68,293.27,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,239.02,590.77,241.57,7.86;14,151.52,601.73,48.09,7.86">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,222.82,601.73,257.78,7.86;14,151.52,612.68,98.49,7.86">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,623.92,337.63,7.86;14,151.52,634.88,329.07,7.86;14,151.52,645.84,329.07,7.86;14,151.52,656.80,162.22,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,417.53,623.92,63.06,7.86;14,151.52,634.88,310.25,7.86">Overview of the ImageCLEFmed 2020 concept prediction task: Medical image understanding</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,151.52,645.84,297.85,7.86">CLEF2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,119.67,337.63,7.86;15,151.52,130.63,329.07,7.86;15,151.52,141.59,329.07,7.86;15,151.52,152.55,215.72,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,408.09,119.67,72.50,7.86;15,151.52,130.63,196.11,7.86">Radiology objects in context (roco): A multimodal image dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,371.76,130.63,108.83,7.86;15,151.52,141.59,329.07,7.86;15,151.52,152.55,92.38,7.86">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,163.51,337.64,7.86;15,151.52,174.47,44.54,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="15,202.63,163.51,277.96,7.86;15,151.52,174.47,15.87,7.86">Quickumls: a fast, unsupervised approach for medical concept extraction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,185.43,337.97,7.86;15,151.52,196.36,329.07,7.89;15,151.52,207.34,172.40,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,340.98,185.43,139.61,7.86;15,151.52,196.39,213.34,7.86">Biowordvec, improving biomedical word embeddings with subword information and mesh</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yijia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-019-0055-0</idno>
		<ptr target="https://doi.org/10.1038/s41597-019-0055-0" />
	</analytic>
	<monogr>
		<title level="j" coord="15,371.98,196.39,58.43,7.86">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,218.30,337.97,7.86;15,151.52,229.26,76.80,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,215.68,218.30,198.21,7.86">Contrast limited adaptive histogram equalization</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zuiderveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,421.20,218.30,59.39,7.86">Graphics gems</title>
		<imprint>
			<biblScope unit="page" from="474" to="485" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
