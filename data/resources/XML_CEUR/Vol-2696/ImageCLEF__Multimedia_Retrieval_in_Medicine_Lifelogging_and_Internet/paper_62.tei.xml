<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.06,150.87,320.93,12.64;1,134.18,166.95,326.44,12.64">HARENDRAKV at VQA-Med 2020: Sequential VQA with Attention for Medical Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,194.45,192.18,79.15,8.96"><forename type="first">Harendra</forename><forename type="middle">K</forename><surname>Verma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vadict Innovations Pvt. Ltd</orgName>
								<address>
									<settlement>Vadodara</settlement>
									<region>Gujarat</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,296.45,192.18,90.57,8.96"><forename type="first">Sindhu</forename><surname>Ramachandran</surname></persName>
							<email>sindhu.ramachandran@quest-global.com</email>
							<affiliation key="aff1">
								<orgName type="department">Quest Global</orgName>
								<orgName type="institution">Trivandram. Kerala</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.06,150.87,320.93,12.64;1,134.18,166.95,326.44,12.64">HARENDRAKV at VQA-Med 2020: Sequential VQA with Attention for Medical Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F618C5467718796982C32388B4AD141B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>Sequential VQA Model</term>
					<term>Attention</term>
					<term>Radiology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our approach for Medical Visual Question Answering (VQA-Med 2020) Task of ImageCLEF 2020. We used an encoder-decoder architecture for generating answers given the question and image. The encoder takes two inputs: the first is a feature vectors of image obtained from VGG16, and the second is a vector representation for each question using BERT. The question features are self-attended to get attention features. The question attention features, and image features are fused using multi-modal factorized bilinear pooling (MFB). The fused features are further self-attended to get fuse attention features. The thought vectors are obtained by concatenation of fuse attention and encoder LSTM hidden states. The decoder generates answer word by word for the input question and the image. The decoder consists of LSTM layer, Bahdanau Attention, and dense layer of answer vocabulary size with SoftMax. The answers are embedded using GLOVE word vectors before being passed to the decoder LSTM. Our best model achieves 37.8% accuracy and BLEU score of 0.439.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual question answering requires understanding of Computer Vision and NLP simultaneously. VQA has gained lot of attention from academicians and researchers due to introduction of new language models like BERT, new feature fusion techniques based on bilinear pooling and attention mechanisms. The recent advancements in artificial intelligence have encouraged the healthcare sector in storing large number of health records electronically. VQA can be used for automatic interpretation of radiology images, thereby helping make clinical decisions In this paper, we present our method to build a deep learning model for ImageCLEF VQA-Med 2020 Task <ref type="bibr" coords="2,215.81,162.42,10.60,8.96" target="#b0">[1]</ref>. ImageCLEF conducts many tasks related to multimedia retrieval in many domains such as medicine, security, lifelogging, and nature <ref type="bibr" coords="2,437.28,174.42,10.93,8.96" target="#b1">[2]</ref>. Our model is based on encoder-decoder system where encoder takes question and radiology image as input and decoder generates the answer word by word. The Image and question features were extracted using pre-trained VGG16 and 12-Layer BERT Language model respectively. The extracted features for image and questions were fused using Multi-Modal Factorized Bilinear Pooling.</p><p>This paper is organized in the following manner: Section 2 provides a brief information regarding similar works done on VQA-Med which inspired our work. Section 3 presents a brief description of dataset used. Section 4 describes our model architecture based on encoder-decoder system. Section 5 describes our model evaluation and results achieved on test set. Section 6 presents conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset Description</head><p>The dataset used here is VQA-Med-VQA 2020 <ref type="bibr" coords="2,327.19,350.97,11.72,8.96" target="#b0">[1]</ref> used in ImageCLEF VQA-Med-VQA competition 2020.</p><p>-The training set: 4,000 radiology images with 4,000 associated Question-Answer (QA) pairs.</p><p>-The validation set: 500 radiology images with 500 QA pairs.</p><p>-The VQA test set: 500 radiology images with 500 associated questions.</p><p>Additional data for training was used from abnormality section of VQA-Med 2019 dataset which constitutes 3817 images and 3817 Question-Answer (QA) pairs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In past few years, many interesting approaches are reported on VQA and most of these approaches consider VQA as a classification problem. However, the generative models are the only better options if the answers are long. In particular, the best performing VQA models reported in VQA-Med 2019 task were classification based <ref type="bibr" coords="3,447.37,213.42,23.31,8.96;3,124.70,225.42,24.22,8.96">[3,4,5 and 6]</ref>.</p><p>In this paper, we aim to explain our generative approach for VQA on the VQA-Med 2020 task. We have used VGG16 <ref type="bibr" coords="3,259.49,249.45,11.72,8.96" target="#b6">[7]</ref> for image feature extraction and 12-Layer BERT-Base <ref type="bibr" coords="3,146.18,261.45,11.71,8.96" target="#b7">[8]</ref> for question features extraction. The answers are decoded using 300d GLOVE <ref type="bibr" coords="3,124.70,273.45,11.72,8.96" target="#b8">[9]</ref> Word Embeddings. The glimpse attention mechanism is employed to get question image fuse self-attention while Bahdanau Attention [10] is used for decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>Model Description The encoder has two main components. The first component is a Pretrained VGG16 <ref type="bibr" coords="3,160.34,597.14,11.72,8.96" target="#b6">[7]</ref> network which takes the radiology image as input and extracts feature vectors, while the second component is a Pretrained BERT <ref type="bibr" coords="3,365.23,608.66,11.72,8.96" target="#b7">[8]</ref> language model which encodes the question text into a vector representation. The output from encoder are thought vectors and encoder LSTM sequence. The input to the encoder LSTM is obtained by concatenation of question image fuse attention and question feature vectors from BERT.</p><p>The decoder consists of LSTM network and attention network that takes the thought vector as initial state and encoder LSTM sequence output and try to predict the answer word by word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoder</head><p>The encoder takes image and the question as input and returns thought vectors and sequence output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Feature Extraction</head><p>The image features are extracted using VGG16 Pretrained model from last pooling layer with size of 7*7*512. This image feature vector is then passed to the MLP block having two fully connected layers with 2048 and 768 hidden nodes respectively along with dropout. The main purpose of this MLP block is to decrease the feature vector dimension to half of the LSTM output vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Feature Extraction</head><p>The semantic meanings of the questions are extracted using 12-layer, 768 hidden BERT Pretrained model. For each question, we will get a feature vector of size t*768, where t is the max sequence length of the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Image Fuse Attention</head><p>It has been reported that the attention mechanism allows the VQA models to effectively learn which regions of the image are important for given question. It's always better to employ an effective attention mechanism to improve the performance of VQA models. Our model uses question image fuse attention based on glimpse attention networks. We have used two glimpses each for question and image as an optimal choice based on Fukui et. al. 2016 <ref type="bibr" coords="4,297.05,484.55,15.33,8.96" target="#b10">[11]</ref>.</p><p>For question self-attention, the output from Pretrained BERT (t*768) is passed to question attention layer giving output attention feature vector of size 768. The image features from MLP block (1*768) and question attention features are fused using multimodal bilinear factorize pooling (MFB) <ref type="bibr" coords="4,317.47,536.51,16.64,8.96" target="#b11">[12]</ref> giving an output vector of size 1*768. Finally, the MFB output is passed to image attention layer with two glimpses giving an output of size 1*768.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder LSTM</head><p>Long Short-Term Memory networks or LSTM are a special type of RNN that are designed to avoid long range dependency problem of vanilla RNN networks. The LSTM cell carries an extra memory state (other than hidden state h) to store the context information. An LSTM has three different gates (i.e. Input gate, forget gate and output gate) which decide flow of information across time steps.</p><p>At each time step, LSTM cell has three inputs viz. current word (xt), previous hidden state (ht-1) and previous memory state (ct-1), and three outputs viz. output hidden state (ht), output memory state (ct) and encoded sequence.</p><p>In our model, the input to encoder LSTM are obtained by concatenation of question features extracted from BERT and tiled fuse attention from MFB. The Encoder LSTM has hidden nodes of 768. The outputs are hidden state h (768), memory state c (768) and encoded sequence of size t*768.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decoder</head><p>The decoder generates answer word by word for the input question and the image. The decoder consists of LSTM layer, Bahdanau Attention Layer and dense layer of answer vocabulary size with SoftMax. The answers are embedded using GLOVE <ref type="bibr" coords="5,459.10,257.01,11.60,8.96" target="#b8">[9]</ref> word vectors before being passed to the decoder LSTM.</p><p>At first time step, decoder takes four inputs viz. the start of sequence &lt;SOS&gt;, hidden state of the encoder, memory state of encoder and encoder sequence output. The output will be the first word of the answer which is obtained with highest probability using SoftMax layer. This word will be the input to LSTM at second time step to predict the next word of the answer. This process keeps on going until the decoder predicts end of sequence &lt;EOS&gt; token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results</head><p>A total of 5 runs were submitted to ImageCLEF VQA-Med-VQA 2020. The training was done on NVIDIA GeForce 940 MX GPU Device. The CUDA implementation of LSTM was employed from Tensorflow 2.0. Evaluation of the model was conducted using two different metrics: BLEU and Strict Accuracy, based on VQA-Med-VQA 2020 competition <ref type="bibr" coords="5,244.25,457.55,10.59,8.96" target="#b0">[1]</ref>. There are few pre-processing steps applied on each answer before running the evaluation metrics: lower-case, remove all punctuations and remove stopwords using NLTK. Table <ref type="table" coords="5,284.33,480.37,4.98,9.05">1</ref> shows model description and their accuracy scores of all the runs submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1 Results Comparison</head><p>There are two main models: model1 has hidden size of 1024 with approx. size of 36M and model2 with hidden size of 768 and approx. size of 26M. Model1 is trained with Adam optimizer while model2 is trained with RMSprop optimizer. Experiments were also done with different dropouts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we describe the model we submitted in ImageCLEF 2020 VQA-Med-VQA task. Our proposed model VGG+BERT+MFB+GLOVE employs an encoder-decoder architecture with an advanced feature fusion technique MFB with question image fuse attention. The image and question features were extracted using VGG16 and BERT Base networks respectively. The answers were encoded using GLOVE word embeddings. Our model achieved the accuracy of 37.8% and BLEU score of 0.439 on the test set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,146.30,477.85,263.28,9.06;2,149.15,510.39,296.87,155.60"><head>Fig 1</head><label>1</label><figDesc>Fig 1 shows few examples of questions and answers from dataset.</figDesc><graphic coords="2,149.15,510.39,296.87,155.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,214.73,681.43,165.70,8.10"><head>Fig 1</head><label>1</label><figDesc>Fig 1 Examples from VQA-Med-2020 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,257.69,534.04,79.80,8.10;3,146.30,556.45,324.32,9.06;3,124.70,568.10,223.36,8.96;3,124.70,350.90,345.90,174.10"><head>Fig 2</head><label>2</label><figDesc>Fig 2 Model Structure Fig 2 shows the proposed model based on encoder-decoder system. Different parts of this model are described in the following paragraphs.</figDesc><graphic coords="3,124.70,350.90,345.90,174.10" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,132.67,355.82,338.04,8.10;6,141.74,366.86,328.73,8.10;6,141.74,377.90,328.76,8.10;6,141.74,388.82,299.21,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,410.23,355.82,60.47,8.10;6,141.74,366.86,328.73,8.10;6,141.74,377.90,62.48,8.10">Overview of the VQA-Med Task at ImageCLEF 2020: Visual Question Answering and Generation in the Medical Domain</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<ptr target="://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="6,231.01,377.90,239.49,8.10;6,141.74,388.82,72.32,8.10">CLEF 2020 Working Notes. CEUR Workshop Proceedings, CEURWS.org &lt;http</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,132.67,399.88,338.10,8.10;6,141.74,410.92,328.62,8.10;6,141.74,421.84,328.75,8.10;6,141.74,432.88,328.71,8.10;6,141.74,443.92,328.99,8.10;6,141.74,454.84,329.00,8.10;6,141.74,465.88,328.64,8.10;6,141.74,476.92,329.02,8.10;6,141.74,487.84,328.76,8.10;6,141.74,498.88,60.29,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,430.87,443.92,39.86,8.10;6,141.74,454.84,329.00,8.10;6,141.74,465.88,58.75,8.10">{Overview of the ImageCLEF 2020}: Multimedia Retrieval in Lifelogging, Medical, Nature, and Internet Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tu-Khiem</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">I</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,220.40,465.88,249.98,8.10;6,141.74,476.92,329.02,8.10">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the 11th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="6,167.16,487.84,151.43,8.10">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-09-22">2020. September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,132.67,509.92,337.78,8.10;6,141.74,520.84,288.27,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,288.61,509.92,181.84,8.10;6,141.74,520.84,133.69,8.10">Zhejiang university at imageclef 2019 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,294.11,520.84,109.59,8.10">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,132.67,531.88,337.79,8.10;6,141.74,542.92,328.76,8.10;6,141.74,553.84,101.54,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,308.28,531.88,162.18,8.10;6,141.74,542.92,278.80,8.10">Ensemble of streamlined bilinear visual question answering models for the imageclef 2019 challenge in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nyholm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lfstedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,438.54,542.92,31.96,8.10;6,141.74,553.84,75.24,8.10">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,132.67,564.91,338.04,8.10;6,141.74,575.95,270.46,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,244.22,564.91,226.49,8.10;6,141.74,575.95,116.72,8.10">Tua1 at imageclef 2019 vqa-med: A classication and generation model based on transfer learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,276.51,575.95,109.40,8.10">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,132.67,586.87,337.76,8.10;6,141.74,597.91,164.10,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,247.09,586.87,223.34,8.10;6,141.74,597.91,10.41,8.10">Deep multimodal learning for medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,170.02,597.91,109.53,8.10">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,132.67,608.95,338.07,8.10;6,141.74,619.87,328.59,8.10;6,141.74,630.91,71.86,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,312.75,608.95,157.99,8.10;6,141.74,619.87,84.96,8.10">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,243.03,619.87,227.30,8.10;6,141.74,630.91,71.86,8.10">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,132.67,641.95,338.02,8.10;6,141.74,652.87,328.50,8.10;6,141.74,663.91,161.47,8.10" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,307.75,652.87,162.50,8.10;6,141.74,663.91,67.64,8.10">Attention is all you need. neural information processing systems</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">5998. 2017</date>
			<biblScope unit="page">6008</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,132.67,674.95,338.01,8.10;6,141.74,685.87,175.62,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,304.86,674.95,162.26,8.10">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,151.56,685.87,27.26,8.10">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,132.40,150.35,337.87,8.10;7,141.74,161.39,192.15,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,359.41,150.35,110.85,8.10;7,141.74,161.39,131.60,8.10">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName coords=""><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,288.71,161.39,18.20,8.10">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,132.40,172.31,338.19,8.10;7,141.74,183.35,328.67,8.10;7,141.74,194.39,151.14,8.10" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="7,428.16,172.31,42.44,8.10;7,141.74,183.35,271.97,8.10">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2008">2016. 4, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,132.40,205.31,338.17,8.10;7,141.74,216.35,328.81,8.10;7,141.74,227.39,224.19,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,327.27,205.31,143.30,8.10;7,141.74,216.35,202.44,8.10">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,381.70,216.35,88.85,8.10;7,141.74,227.39,129.94,8.10">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1839" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
