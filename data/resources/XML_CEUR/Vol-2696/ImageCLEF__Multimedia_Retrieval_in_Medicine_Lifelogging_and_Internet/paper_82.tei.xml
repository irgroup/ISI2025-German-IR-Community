<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,163.64,114.32,288.07,14.35;1,169.35,132.25,276.67,14.35">Sketch2Code: Automatic hand-drawn UI elements detection with Faster R-CNN</title>
				<funder ref="#_7BKCFdn">
					<orgName type="full">Ministry of Education, Youth and Sports of the Czech Republic</orgName>
				</funder>
				<funder ref="#_YgzjCZW">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,187.96,170.41,39.61,9.96"><forename type="first">Aleš</forename><surname>Zita</surname></persName>
						</author>
						<author>
							<persName coords="1,254.44,170.41,52.91,9.96"><surname>Lukáš Picek</surname></persName>
							<idno type="ORCID">0000-0001-8119-3147</idno>
						</author>
						<author>
							<persName coords="1,353.58,170.41,59.38,9.96"><forename type="first">Antonín</forename><surname>Říha</surname></persName>
							<idno type="ORCID">0000-0002-6166-9176</idno>
						</author>
						<author>
							<affiliation>
								<orgName>1 Czech Academy of Sciences, Institute of Information Theory and Automation </orgName>
								<address><addrLine>5 PiVa AI</addrLine></address>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 2 Faculty of Mathematics and Physics, Charles University </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 3 Dept. of Cybernetics, Faculty of Applied Sciences, University of West Bohemia </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 4 Faculty of Information Technology, Czech Technical University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,163.64,114.32,288.07,14.35;1,169.35,132.25,276.67,14.35">Sketch2Code: Automatic hand-drawn UI elements detection with Faster R-CNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3354AC4B17C4932E43E4EA47AAC9E4B2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Web Design</term>
					<term>Object Detection</term>
					<term>Convolutional Neural Networks</term>
					<term>Machine Learning</term>
					<term>Computer Vision</term>
					<term>User Interface</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transcription of User Interface (UI) elements hand drawings to the computer code is a tedious and repetitive task. Therefore, a need arose to create a system capable of automating such process. This paper describes a deep learning-based method for hand-drawn user interface elements detection and localization. The proposed method scored 1st place in the ImageCLEFdrawnUI competition while achieving an overall precision of 0.9708. The final method is based on Faster R-CNN object detector framework with ResNet-50 backbone architecture trained with advanced regularization techniques. The code has been made available at: https://github.com/picekl/ImageCLEF2020-DrawnUI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEFdrawnUI <ref type="bibr" coords="1,251.60,453.52,10.53,9.96" target="#b2">[3]</ref> challenge was organized in connection with the ImageCLEF 2020 evaluation campaign <ref type="bibr" coords="1,310.88,465.47,10.53,9.96" target="#b6">[7]</ref> at the Conference and Labs of the Evaluation Forum (CLEF). The Main goal of this competition was to create an algorithm or system which can automatically recognise and localize UI elements on high resolution pictures of their drawings. The desired outcome of the detection process are localized bounding boxes with corresponding classes assignments of the UI elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivation</head><p>The main motivation for this task is to simplify the process of websites creation by enabling people to create websites by drawing UI elements on a whiteboard or on a piece of paper to make the web page building process more accessible. In this context, the detection and recognition of hand drawn UI elements task addresses the problem of automatically transcribing the UI to computer code. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Dataset</head><p>The complete dataset consists of 1,000 hand drawn templates captured multiple times with different cameras, resulting in 2,950 high-resolution images. These data were further randomly split into 2,363 training and 587 test images. The training part includes 65,993 UI elements belonging to 21 classes. All images were annotated with bounding boxes and class labels by human experts. More detailed class distribution description is listed in Table <ref type="table" coords="2,380.03,434.69,3.88,9.96" target="#tab_0">1</ref>. Example images are depicted in Figure <ref type="figure" coords="2,216.03,446.65,3.88,9.96" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Solution</head><p>The proposed solution is based on utilization of a standard object detection network architecture and coherent data preparation and augmentation. In particular, the Faster R-CNN <ref type="bibr" coords="2,250.59,534.72,15.51,9.96" target="#b9">[10]</ref> framework with the ResNet-50 <ref type="bibr" coords="2,403.99,534.72,10.52,9.96" target="#b4">[5]</ref> feature extractor was used. The system was implemented and fine-tuned using TensorFlow Object Detection API<ref type="foot" coords="2,231.17,557.42,3.97,6.97" target="#foot_0">1</ref>  <ref type="bibr" coords="2,239.29,558.63,10.52,9.96" target="#b5">[6]</ref> from publicly available checkpoints. All networks in our experiments shared the optimizer settings -RMSProp <ref type="bibr" coords="2,390.30,570.58,15.51,9.96" target="#b12">[13]</ref> with momentum of 0.9. The initial architecture was based on our work <ref type="bibr" coords="2,376.55,582.54,10.53,9.96" target="#b7">[8]</ref> submitted to Image-CLEFcoral competition <ref type="bibr" coords="2,241.84,594.49,9.97,9.96" target="#b1">[2]</ref>. This included for instance the data augmentation methods or Accumulated Gradient Normalization technique <ref type="bibr" coords="2,398.62,606.45,9.97,9.96" target="#b3">[4]</ref>. During our followup research, we considered and tested several approaches including new data synthesis, different network architectures as well as network ensemble variants. The experiment performed with ResNet-50 backbone and grayscale data with 1000 × 1000 input size was evaluated over the validation set and showed interesting improvement in all measured scores on RGB images. Specifically, mean average precision with Intersection over Union (IoU) greater than 0.5 (mAP0.5) by 0.0081, mAP by 0.0222, and by Recall@100 (Recall calculated using best 100 detections) 0.0315. Although we were able to flatten the UI elements distribution curve, the overall performance of the original network was marginally better on grayscale images.  Data preprocessing -While testing, we experimented with three different approaches to input preprocessing. First, images without any augmentations. Second, images were converted to grayscale. Third, where grayscale images without borders around captured drawing and with dilated lines were used. This effect was achieved by finding contours in the image using OpenCV <ref type="bibr" coords="5,399.62,238.73,9.98,9.96" target="#b0">[1]</ref>. These contours were then sorted by area, and the largest of them was used to crop the image.</p><p>In some cases, this approach did not work correctly, especially where parts of the image were covered with a shadow, so this crop was only applied when the area of the contour was at least 70% of the image. After the crop, we utilized CLAHE <ref type="bibr" coords="5,173.97,298.50,10.52,9.96" target="#b8">[9]</ref> for histogram equalization and applied topological erosion to pronounce lines on paper. This approach is described as Gray++ in our results. Refer to Table <ref type="table" coords="5,202.05,322.41,3.88,9.96" target="#tab_1">2</ref>, where it can be seen that Gray++ was worse in our testing, so our submissions mainly used grayscale images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Object Detection Networks</head><p>There are several network architectures that were taken into the consideration for this task, in particular the Faster R-CNN <ref type="bibr" coords="5,343.91,380.77,15.51,9.96" target="#b9">[10]</ref> and EfficientDet <ref type="bibr" coords="5,440.67,380.77,14.62,9.96" target="#b11">[12]</ref>. The initial performance test for each particular network architecture was to train these networks with recommended configuration. These tests revealed the overall architecture suitability for the task at hand. The best performance was achieved with the Faster R-CNN architecture that used comparable backbones. Refer to Table <ref type="table" coords="5,162.16,440.55,3.88,9.96" target="#tab_3">4</ref>. Next, we needed to decide on the object detection network backbone. We have tested several widely used backbone architectures, namely the ResNet-50 <ref type="bibr" coords="5,148.04,476.72,9.97,9.96" target="#b4">[5]</ref>, ResNet-101 <ref type="bibr" coords="5,217.67,476.72,9.98,9.96" target="#b4">[5]</ref>, Inception V2 and Inception-ResNet-V2 <ref type="bibr" coords="5,407.98,476.72,15.51,9.96" target="#b10">[11]</ref> (Table <ref type="table" coords="5,458.08,476.72,3.88,9.96" target="#tab_2">3</ref>). and <ref type="bibr" coords="7,156.00,218.12,9.97,9.96" target="#b7">[8]</ref>, respectively. The resulting predictions were filtered with confidence threshold of 0.95 to maximize the official metric of mAP.</p><p>In our opinion, the winning submission is not the best of our submissions. According to the widely accepted performance metrics (mAP@0.5 and Recall@0.5), our Submission 5 (run ID 68003), which scored 3rd place overall, is superior to the winning submission. It diminishes ImageCLEF Overall Precision only by 0.0144, while it increases mAP@0.5 by 0.111 and Recall@0.5 by 0.074.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have presented a system for automatic hand-drawn UI element detection and localization. To achieve this goal, we had to gain a deep understanding of the provided dataset and perform many experiments to craft the best data preprocessing and augmentation methods, as well as objectively adjust the network parameters.</p><p>The final methods were based on the Faster R-CNN detection network with ResNet-50 used as a backbone architecture. The presented method scored first place in ImageCLEFdrawnUI competition, with an overall precision of 0.9708. Fig. <ref type="figure" coords="7,154.13,642.89,4.13,8.97">3</ref>. Results for all runs submitted by the competition participants. Including additional metrics e.g. mAP@IoU0.5 and Recall@IoU0.5.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,300.30,345.83,8.97;2,134.77,311.26,47.16,8.97;2,136.79,115.84,214.41,160.81"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example images with annotations from ImageCLEFdrawnUI competition training dataset.</figDesc><graphic coords="2,136.79,115.84,214.41,160.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,642.89,345.82,8.97;4,134.77,653.85,337.65,8.97;4,152.28,451.99,172.91,172.91"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Synthetic data example images. For classes with very few instances we injected the training set with artificial data containing augmented instances of these classes.</figDesc><graphic coords="4,152.28,451.99,172.91,172.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,134.77,465.01,345.83,161.31"><head></head><label></label><figDesc></figDesc><graphic coords="7,134.77,465.01,345.83,161.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,114.93,345.83,549.92"><head>Table 1 .</head><label>1</label><figDesc>Class distribution description including number of UI elements and their number in training and validation set. To better understand the problem at hand, we have performed a frequency analysis on UI element type distribution and concluded, that some of the element types are represented by very few occurrences in the training dataset, namely the 'stepper input', 'text area' or 'table' (See Table1). Reviewing the training dataset further revealed that it contains multiple images of the same drawings. This is caused by the fact that the whole dataset (training and testing) consists of 2,950 images of only 1,000 templates, i.e., the templates were each captured by several different cameras. Following the random splitting of the dataset to the training and testing part caused some rarer elements to go to the training set multiple times and others not at all. This worsens the uneven distribution of the UI element classes in such a way that, for example, the rarest element is contained only on two templates (6 images) in the training dataset. For the deep network to learn to recognize such an element, a much higher number of examples is needed. To compensate for the uneven distribution of the UI element types, we decided to expand the training dataset with synthetic data containing such elements. The data were generated using augmentations of seg-</figDesc><table coords="3,134.77,145.98,334.98,277.30"><row><cell cols="3">Dataset distribution</cell><cell cols="2">Train. / Val. split</cell><cell></cell></row><row><cell cols="3">Class Name # Boxes Fraction[%]</cell><cell cols="3">Train. Boxes Val. Boxes Fraction[%]</cell></row><row><cell>button</cell><cell>18,704</cell><cell>28.34</cell><cell>16,841</cell><cell>1,863</cell><cell>9.96%</cell></row><row><cell>paragraph</cell><cell>10,367</cell><cell>15.71</cell><cell>9,342</cell><cell>1,025</cell><cell>9.89%</cell></row><row><cell>image</cell><cell>7,683</cell><cell>11.64</cell><cell>7,020</cell><cell>663</cell><cell>8.63%</cell></row><row><cell>link</cell><cell>6,809</cell><cell>10.32</cell><cell>6,140</cell><cell>669</cell><cell>9.83%</cell></row><row><cell>linebreak</cell><cell>5,798</cell><cell>8.786</cell><cell>5,267</cell><cell>531</cell><cell>9.16%</cell></row><row><cell>container</cell><cell>4,678</cell><cell>7.089</cell><cell>4,233</cell><cell>445</cell><cell>9.51%</cell></row><row><cell>header</cell><cell>4,356</cell><cell>6.601</cell><cell>3,947</cell><cell>409</cell><cell>9.39%</cell></row><row><cell>textinput</cell><cell>1,732</cell><cell>2.624</cell><cell>1,577</cell><cell>155</cell><cell>8.95%</cell></row><row><cell>label</cell><cell>1,691</cell><cell>2.562</cell><cell>1,539</cell><cell>152</cell><cell>8.99%</cell></row><row><cell>dropdown</cell><cell>1,472</cell><cell>2.231</cell><cell>1,350</cell><cell>122</cell><cell>8.29%</cell></row><row><cell>list</cell><cell>798</cell><cell>1.209</cell><cell>702</cell><cell>96</cell><cell>12.03%</cell></row><row><cell>checkbox</cell><cell>758</cell><cell>1.148</cell><cell>694</cell><cell>64</cell><cell>8.44%</cell></row><row><cell>video</cell><cell>360</cell><cell>0.545</cell><cell>323</cell><cell>37</cell><cell>10.28%</cell></row><row><cell>radiobutton</cell><cell>279</cell><cell>0.422</cell><cell>246</cell><cell>33</cell><cell>11.83%</cell></row><row><cell>toggle</cell><cell>178</cell><cell>0.249</cell><cell>159</cell><cell>19</cell><cell>10.67%</cell></row><row><cell>datepicker</cell><cell>91</cell><cell>0.138</cell><cell>83</cell><cell>8</cell><cell>8.79%</cell></row><row><cell>rating</cell><cell>75</cell><cell>0.114</cell><cell>62</cell><cell>13</cell><cell>17.33%</cell></row><row><cell>slider</cell><cell>75</cell><cell>0.114</cell><cell>65</cell><cell>10</cell><cell>13.33%</cell></row><row><cell>textarea</cell><cell>47</cell><cell>0.071</cell><cell>42</cell><cell>5</cell><cell>10.64%</cell></row><row><cell>table</cell><cell>29</cell><cell>0.043</cell><cell>25</cell><cell>4</cell><cell>13.79%</cell></row><row><cell>stepperinput</cell><cell>13</cell><cell>0.019</cell><cell>10</cell><cell>3</cell><cell>23.08%</cell></row><row><cell cols="2">2 Methodology</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="3,134.77,437.13,175.96,9.96;3,134.77,457.34,345.83,9.96;3,134.77,469.30,345.82,9.96;3,134.77,481.25,345.82,9.96;3,134.77,493.21,345.83,9.96;3,134.77,505.17,345.82,9.96;3,134.77,517.12,345.83,9.96;3,134.77,529.08,345.82,9.96;3,134.77,541.03,310.68,9.96;3,134.77,571.21,94.47,9.96;4,134.77,256.12,345.82,9.96;4,134.77,268.08,345.82,9.96;4,134.77,280.03,120.34,9.96;4,275.93,280.03,204.66,9.96;4,134.77,291.99,345.82,9.96;4,134.77,303.94,345.83,9.96;4,134.77,315.90,168.00,9.96"><p><p><p><p>2.1 Data analysis and preparation</p>Dataset splitting for validation -To create a set for continuous network performance evaluation, the provided dataset needed to be split into training and validation sets. After careful examination of the content, it became apparent that a random split of the dataset could cause discrepancies between the validation and training sets performances. The reason being, that less frequent classes could end up not having comparable representations in both the training and validation sets. Therefore the split had to be carefully engineered and resulted in the final approximate ratio of 11:1 for training and validation sets, respectively. Data distribution -mented UI elements, which were consequently pasted on random size paper of very light random color. The augmentation consisted mainly of constrained random affine transformations. have added 500 synthetically generated images with the least frequent classes. Examples of the synthetic data are depicted in Figure</p>2</p>. UI element classes which were artificially added are: datepicker, rating, slider, textarea, table and stepperinput.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,93.67,345.83,73.68"><head>Table 2 .</head><label>2</label><figDesc>Results of input format experiment. Trained for 50 epochs on 1000 × 1000 grayscale images with Faster R-CNN</figDesc><table coords="5,199.00,125.11,214.27,42.24"><row><cell>Input format</cell><cell>mAP0.5</cell><cell>mAP</cell><cell>Recall@100</cell></row><row><cell>RGB</cell><cell>0.9151</cell><cell>0.5814</cell><cell>0.6594</cell></row><row><cell>Grayscale</cell><cell>0.9151</cell><cell>0.5855</cell><cell>0.6689</cell></row><row><cell>Gray ++</cell><cell>0.9087</cell><cell>0.5788</cell><cell>0.6705</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,134.77,502.54,345.82,84.64"><head>Table 3 .</head><label>3</label><figDesc>Results of backbone architecture experiment. Trained for 50 epochs on 1000 × 1000 grayscale images with Faster R-CNN.</figDesc><table coords="5,182.92,533.97,246.44,53.20"><row><cell>Backbone</cell><cell>mAP0.5</cell><cell>mAP</cell><cell>Recall@100</cell></row><row><cell>Inception-V2</cell><cell>0.8974</cell><cell>0.5850</cell><cell>0.6689</cell></row><row><cell>ResNet-50</cell><cell>0.9151</cell><cell>0.5855</cell><cell>0.6689</cell></row><row><cell>ResNet-101</cell><cell>0.9035</cell><cell>0.6035</cell><cell>0.6835</cell></row><row><cell>Inception-ResNet-V2</cell><cell>0.9176</cell><cell>0.6095</cell><cell>0.6904</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,134.77,598.66,345.82,62.72"><head>Table 4 .</head><label>4</label><figDesc>Results of detection framework experiment. Trained for 50 epochs on 1000 × 1000 RGB images.</figDesc><table coords="5,173.80,630.09,264.68,31.28"><row><cell>Approach</cell><cell>mAP0.5</cell><cell>mAP</cell><cell>Recall@100</cell></row><row><cell>EfficientDet-B3</cell><cell>0.583</cell><cell>0.416</cell><cell>0.458</cell></row><row><cell>Faster R-CNN ResNet-50</cell><cell>0.9151</cell><cell>0.5814</cell><cell>0.6594</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,143.00,114.93,329.35,75.22"><head>Table 6 .</head><label>6</label><figDesc>Submission results achieved over test set.</figDesc><table coords="7,143.00,134.16,329.35,55.99"><row><cell>Submission</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell>Overall Precision</cell><cell>0.939</cell><cell>0.956</cell><cell>0.971</cell><cell>0.944</cell><cell>0.956</cell><cell>0.956</cell><cell>0.942</cell></row><row><cell>mAP@0.5</cell><cell>0.688</cell><cell>0.676</cell><cell>0.583</cell><cell>0.647</cell><cell>0.695</cell><cell>0.694</cell><cell>0.755</cell></row><row><cell>Recall@0.5</cell><cell>0.536</cell><cell>0.517</cell><cell>0.445</cell><cell>0.472</cell><cell>0.520</cell><cell>0.519</cell><cell>0.555</cell></row><row><cell>Run ID</cell><cell>67733</cell><cell>67814</cell><cell>67816</cell><cell>67991</cell><cell>68003</cell><cell>68014</cell><cell>68015</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,655.70,320.13,8.97"><p>https://github.com/tensorflow/models/blob/master/research/object_detection</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.73,655.70,105.55,8.97"><p>https://www.aicrowd.com</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p><rs type="person">Lukáš Picek</rs> was supported by the <rs type="funder">Ministry of Education, Youth and Sports of the Czech Republic</rs> project No. <rs type="grantNumber">LO1506</rs>, and by the grant of the <rs type="projectName">UWB</rs> project No. <rs type="grantNumber">SGS-2019-027</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_7BKCFdn">
					<idno type="grant-number">LO1506</idno>
					<orgName type="project" subtype="full">UWB</orgName>
				</org>
				<org type="funding" xml:id="_YgzjCZW">
					<idno type="grant-number">SGS-2019-027</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submissions</head><p>In this competition, the AICrowd platform 2 was used to evaluate participants submissions. Each participating team was allowed to submit up to 10 text files with detection bounding-boxes in a specific format for each image. We have created 7 submission using configurations listed below.</p><p>Baseline configuration -As a baseline for all our experiments we used Faster R-CNN with ResNet-50 as a backbone. For training we used parameters and augmentations described in Table <ref type="table" coords="6,301.03,348.56,4.98,9.96">5</ref> and <ref type="bibr" coords="6,328.44,348.56,9.97,9.96" target="#b7">[8]</ref>, respectively. Finally, thresholding was used to select only detection with high confidence. Submission 1 -Baseline experiment trained on RGB images. Tested on originalsize RGB images. Detection confidence threshold was set to 0.8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Competition Results and Discusion</head><p>The official ImageCLEFdrawnUI competition results are displayed in Figure <ref type="figure" coords="6,472.84,575.21,3.88,9.96">3</ref>.</p><p>The proposed system achieved the best Overall Precision score of 0.9709 and outperformed 2 other participating teams as well as the baseline solution proposed by organizers. The best scoring submission was produced by Mask R-CNN model with ResNet-50 backbone architecture and input resolution of 1000×1000 trained for 80 epochs with parameters and augmentations described in Table <ref type="table" coords="6,475.61,634.98,4.98,9.96">5</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,212.44,332.17,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,203.52,212.44,85.29,8.97">The OpenCV Library</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,295.86,212.44,150.58,8.97">Dr. Dobb&apos;s Journal of Software Tools</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,222.72,337.63,8.97;8,151.53,233.68,329.06,8.97;8,151.53,244.64,329.06,8.97;8,151.53,255.60,180.36,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,204.96,233.68,275.63,8.97;8,151.53,244.64,68.99,8.97">Overview of the ImageCLEFcoral 2020 task: Automated coral reef image annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">G</forename><surname>Clift</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="8,242.80,244.64,105.44,8.97">CLEF2020 Working Notes</title>
		<title level="s" coord="8,356.01,244.64,124.58,8.97;8,151.53,255.60,21.70,8.97">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,265.89,337.63,8.97;8,151.53,276.85,329.06,8.97;8,151.53,287.80,329.07,8.97;8,151.53,298.76,329.05,8.97;8,151.53,309.72,95.78,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,202.63,276.85,277.96,8.97;8,151.53,287.80,147.21,8.97">Overview of ImageCLEFdrawnUI 2020: The Detection and Recognition of Hand Drawn Website UIs Task</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="8,318.21,287.80,102.52,8.97">CLEF2020 Working Notes</title>
		<title level="s" coord="8,427.05,287.80,53.55,8.97;8,151.53,298.76,124.09,8.97">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,320.01,337.64,8.97;8,151.53,330.97,329.06,8.97;8,151.53,341.93,183.19,8.97" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m" coord="8,257.48,330.97,223.10,8.97;8,151.53,341.93,16.81,8.97">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.96,352.21,337.63,8.97;8,151.53,363.17,329.06,8.97;8,151.53,374.13,47.75,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,299.05,352.21,177.60,8.97">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,165.85,363.17,314.74,8.97">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,384.42,337.63,8.97;8,151.53,395.37,329.06,8.97;8,151.53,406.33,329.06,8.97;8,151.53,417.29,213.41,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,329.87,395.37,150.72,8.97;8,151.53,406.33,117.42,8.97">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,288.08,406.33,192.51,8.97;8,151.53,417.29,120.36,8.97">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7310" to="7311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,427.58,337.63,8.97;8,151.53,438.54,329.06,8.97;8,151.53,449.50,329.06,8.97;8,151.53,460.45,329.06,8.97;8,151.53,471.41,329.06,8.97;8,151.53,482.37,329.06,8.97;8,151.53,493.33,329.07,8.97;8,151.53,504.29,329.06,8.97;8,151.53,515.25,329.06,8.97;8,151.53,526.21,329.06,8.97;8,151.53,537.17,34.31,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,295.07,482.37,185.51,8.97;8,151.53,493.33,255.38,8.97">Overview of the ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,426.42,493.33,54.18,8.97;8,151.53,504.29,329.06,8.97;8,151.53,515.25,253.34,8.97">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="8,456.14,515.25,24.44,8.97;8,151.53,526.21,138.61,8.97">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,142.96,547.45,337.63,8.97;8,151.53,558.41,329.06,8.97;8,151.53,569.37,309.38,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,265.97,547.45,214.62,8.97;8,151.53,558.41,176.68,8.97">Coral reef annotation, localisation and pixel-wise classification using mask-rcnn and bag of tricks</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Říha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zita</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="8,349.47,558.41,131.12,8.97;8,151.53,569.37,14.99,8.97">CLEF (Working Notes). CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,579.66,337.63,8.97;8,151.53,590.62,329.06,8.97;8,151.53,601.57,329.06,8.97;8,151.53,612.53,88.98,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,400.62,590.62,79.96,8.97;8,151.53,601.57,121.77,8.97">Adaptive histogram equalization and its variations</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">P</forename><surname>Amburn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cromartie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geselowitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Greer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ter Haar Romeny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zuiderveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,280.66,601.57,199.92,8.97">Computer vision, graphics, and image processing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="368" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,622.82,337.97,8.97;8,151.53,633.78,329.06,8.97;8,151.53,644.74,329.06,8.97;8,151.53,655.70,78.40,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,308.96,622.82,171.63,8.97;8,151.53,633.78,150.91,8.97">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,287.20,644.74,193.39,8.97;8,151.53,655.70,29.91,8.97">Advances in Neural Information Processing Systems 28</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,118.57,337.97,8.97;9,151.53,129.53,329.06,8.97;9,151.53,140.49,143.19,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,360.34,118.57,120.25,8.97;9,151.53,129.53,204.40,8.97">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,377.38,129.53,103.20,8.97;9,151.53,140.49,114.51,8.97">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,151.45,337.97,8.97;9,151.53,162.41,329.07,8.97;9,151.53,173.37,148.65,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,272.55,151.45,204.09,8.97">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,166.34,162.41,314.25,8.97;9,151.53,173.37,46.13,8.97">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10781" to="10790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,184.33,337.97,8.97;9,151.53,195.28,329.06,8.97;9,151.53,206.24,89.82,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,258.89,184.33,221.69,8.97;9,151.53,195.28,123.13,8.97">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,281.94,195.28,198.65,8.97;9,151.53,206.24,12.29,8.97">COURSERA: Neural networks for machine learning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="26" to="31" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
