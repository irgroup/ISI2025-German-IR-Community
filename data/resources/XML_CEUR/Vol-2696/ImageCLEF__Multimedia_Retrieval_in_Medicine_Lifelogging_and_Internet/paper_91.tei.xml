<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.07,115.96,311.22,12.62;1,158.09,133.89,299.17,12.62">UA.PT Bioinformatics at ImageCLEF 2020: Lifelog Moment Retrieval Web based Tool</title>
				<funder ref="#_yhPfAaP">
					<orgName type="full">of SR&amp;TD SOCA</orgName>
				</funder>
				<funder>
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_J9KDuqm">
					<orgName type="full">Portugal 2020</orgName>
				</funder>
				<funder>
					<orgName type="full">European Regional Development Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,164.56,173.11,67.66,8.74"><forename type="first">Ricardo</forename><surname>Ribeiro</surname></persName>
							<email>rfribeiro@ua.pt</email>
							<affiliation key="aff0">
								<orgName type="department">IEETA/DETI</orgName>
								<orgName type="institution">University of Aveiro</orgName>
								<address>
									<postCode>3810-193</postCode>
									<settlement>Aveiro</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.89,173.11,44.10,8.74"><forename type="first">Júlio</forename><surname>Silva</surname></persName>
							<email>silva.julio@ua.pt</email>
							<affiliation key="aff0">
								<orgName type="department">IEETA/DETI</orgName>
								<orgName type="institution">University of Aveiro</orgName>
								<address>
									<postCode>3810-193</postCode>
									<settlement>Aveiro</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.22,173.11,52.01,8.74"><forename type="first">Alina</forename><surname>Trifan</surname></persName>
							<email>alina.trifan@ua.pt</email>
							<affiliation key="aff0">
								<orgName type="department">IEETA/DETI</orgName>
								<orgName type="institution">University of Aveiro</orgName>
								<address>
									<postCode>3810-193</postCode>
									<settlement>Aveiro</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.75,173.11,76.55,8.74"><forename type="first">José</forename><forename type="middle">Luis</forename><surname>Oliveira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IEETA/DETI</orgName>
								<orgName type="institution">University of Aveiro</orgName>
								<address>
									<postCode>3810-193</postCode>
									<settlement>Aveiro</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.65,185.06,88.05,8.74"><forename type="first">António</forename><forename type="middle">J R</forename><surname>Neves</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IEETA/DETI</orgName>
								<orgName type="institution">University of Aveiro</orgName>
								<address>
									<postCode>3810-193</postCode>
									<settlement>Aveiro</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.07,115.96,311.22,12.62;1,158.09,133.89,299.17,12.62">UA.PT Bioinformatics at ImageCLEF 2020: Lifelog Moment Retrieval Web based Tool</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6921AB56D2C460095604FCF8250A9381</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>lifelog</term>
					<term>moment retrieval</term>
					<term>image processing</term>
					<term>web application</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the Bioinformatics group of the Institute of Electronics and Engineering Informatics of University of Aveiro in the ImageCLEF lifelog task, more specifically in the Lifelog Moment Retrieval (LMRT) sub-task. In our first participation last year we tackled the LMRT challenge with an automatic approach. Following the same steps, we improved our results, while introducing a new interactive approach. For the automatic approach, two submissions were made. We started by processing all images in the lifelog dataset using object detection and scene recognition algorithms. Afterwards, we processed the query topics with Natural Language Processing (NLP) algorithms in order to extract relevant words related to the desired moment. Finally, we compared the visual concepts of the image with the textual concepts of the query topic with the goal of computing a confidence score that relates the image to the topic. For the interactive approach, we developed a web application in order to visualize and provide an interactive tool to the users. The application is divided in three stages. In the first one, the user uploads the images from the dataset, as well the textual data annotations. In the second stage, the user interacts with the application assigning the extracted words to the several topics. Consequently, the application retrieves the image associated to the topic with a certain confidence. In the last stage, we provide a visual environment with two different views, in the form of a image gallery or data tables organized into timestamp clusters. Similarly to our previous participation, the results of the automatic approach are still far from being competitive. We conclude that an automatic approach might not be the best solution for the LMRT task since the currently available stateof-the-art technology is still not able to wield better results. However, our interactive approach with relevance feedback obtained better and competitive results, achieving a F1-measure@10 score of 0.52.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The number of workshops and tasks for research has increased over the last few years and among them are the main fields of ImageCLEF 2020 lab <ref type="bibr" coords="2,467.30,154.00,9.96,8.74" target="#b2">[3]</ref>: multimedia retrieval in lifelogging, medical, mature, and internet applications. The multimedia retrieval in lifelogging has received significant attention from both research and commercial communities. The increasing number of mobile and wearable devices is dramatically changing the way we collect data about a person's life.</p><p>Lifelogging is defined as a form of pervasive computing consisting of a unified digital record of the totality of an individual's experiences, captured multimodally through digital sensors and stored permanently as a personal multimedia archive. In a simple way, lifelogging is the process of tracking and recording personal data created through our activities and behaviour <ref type="bibr" coords="2,395.43,273.55,9.96,8.74" target="#b0">[1]</ref>.</p><p>Personal lifelogs have a great potential in numerous applications, including memory and moments retrieval, daily living understanding, diet monitoring, or disease diagnosis, as well as other emerging application areas <ref type="bibr" coords="2,398.00,309.42,9.96,8.74" target="#b8">[9]</ref>. For example: in Alzheimer's disease, people with memory problems can use a lifelog application to help a specialist follow the progress of the disease, or to remember certain moments from the last days or months.</p><p>One of the greatest challenges of lifelog applications is the large amount of lifelog data that a person can generate. The lifelog datasets, for example the ImageCLEFlifelog dataset <ref type="bibr" coords="2,254.93,381.15,9.96,8.74" target="#b4">[5]</ref>, are rich multimodal datasets which consist in one or more months of data from multiple lifeloggers. Therefore, an important aspect is the lifelog data organization in the interest of improving the search and retrieval of information. In order to organize the lifelog data, useful information has to be extracted from it. Other important aspects are the visualization and user interface of the application.</p><p>With the purpose of improving the results obtained in the previous year's challenge <ref type="bibr" coords="2,176.97,464.84,9.96,8.74" target="#b6">[7]</ref>, we developed a first version of a web application to provide a visual and interactive environment to the user. In last year's work <ref type="bibr" coords="2,405.64,476.79,9.96,8.74" target="#b6">[7]</ref>, the approach was fully automatic using an exhaustive method to retrieve data and there was no tool for visualization and interaction with the user. However this year, a significant improvement has been made with regard to the data retrieval using a dynamic and faster method. Initially, only the data provided by the organization is used and stored in the database to further use in the retrieval stage in our application. We divided this approach into 3 different stages, such as upload, retrieval and visualization. At each stage, there is an interaction with the user, which is encouraged by the organizers of the ImageCLEFlifelog <ref type="bibr" coords="2,424.34,572.43,9.96,8.74" target="#b4">[5]</ref>. The web application is still in an early stage but is the baseline of our current work.</p><p>This paper starts with an introductory section and it is organized as follows. Section 2 provides a brief introduction to the ImageCLEF lifelog and the subtask Lifelog Moment Retrieval. The proposed methods are described in Section 3. In Section 4, the results of all submitted runs obtained in the LMRT sub-task are described. Finally, a summary of the work presented in this paper, concluding remarks, and future work can be read in Section 5.</p><p>The ImageCLEFlifelog 2020 task <ref type="bibr" coords="3,282.31,143.11,10.52,8.74" target="#b4">[5]</ref> is divided into two different sub-tasks: the Lifelog moment retrieval (LMRT) and Sport Performance Lifelog (SPLL) subtask. In this work, as in the previous year's challenge <ref type="bibr" coords="3,368.59,167.02,9.96,8.74" target="#b6">[7]</ref>, we only addressed the LMRT sub-task, as a continuous research work that we intend to develop with the aim of giving our contribution to real problems that exist around the world that can benefit from this technology.</p><p>In the LMRT subtask, the main objective is to create a system capable of retrieving a number of predefined moments in a lifelogger's day-to-day life from a set of images. Moments can be defined as semantic events or activities that happen at any given time during the day. For example, given the query "Find the moment(s) when the lifelogger was having an icecream on the beach" the participants should return the corresponding relevant images that show the moments of the lifelogger having icecream at the beach. Like last year, particular attention should be paid to the diversification of the selected moments with respect to the target scenario.</p><p>ImageCLEFlifelog dataset is a new rich multimodal dataset which consists of 4.5 months of data from three lifeloggers, namely: images (1,500-2,500 per day), visual concepts (automatically extracted visual concepts with varying rates of accuracy), semantic content (locations and activities) based on sensor readings on mobile devices (via the Moves App), biometrics information (heart rate, galvanic skin response, calories burn, steps, continual blood glucose, etc.), music listening history and computer usage <ref type="bibr" coords="3,298.69,394.26,9.96,8.74" target="#b4">[5]</ref>. However, in this work we only use the images, the visual concepts and the semantic content of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>We submitted a total of 3 runs in the LMRT sub-task. The work made this year had a significant improvement comparing with our previous work <ref type="bibr" coords="3,448.12,474.38,9.96,8.74" target="#b6">[7]</ref>, due to the interactive and visual approach with the user that we choose to apply. In this section, we present the proposed approach of our submissions. The first two runs follow the same approach as last year <ref type="bibr" coords="3,338.10,510.24,9.96,8.74" target="#b6">[7]</ref>, where we aimed at building a fully automatic process for image retrieval. However, the improvement is in our last submission, in which a web application was developed providing visual and interactive environment to the user. This web application is a first prototype, far from a final version, but we consider it as a baseline of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Automatic approach (Run 1 and 2)</head><p>Initially, the images of the dataset were processed using algorithms for label detection, such as objects and scenes. The information provided by the organizers, such as locations, activities and local time, are also used. In both runs, for scene recognition we used a pretrained model provided by Zhou et al. <ref type="bibr" coords="3,465.10,644.16,15.50,8.74" target="#b9">[10]</ref> trained on the Places365 standard dataset. For the first run, the method used to extract objects from the images is a combination of ResNeXt-101 and Feature Pyramid Network architectures in a basic Faster Region-based Convolutional Network (Faster R-CNN) pretrained on the COCO dataset that was proposed by Mahajan et al. <ref type="bibr" coords="4,216.13,154.86,9.96,8.74" target="#b3">[4]</ref>.</p><p>In the second run, the object detection algorithm used is the YoloV3 <ref type="bibr" coords="4,441.47,167.03,10.52,8.74" target="#b5">[6]</ref> model pretrained in the COCO dataset. Subsequently, we proceed to the extraction of relevant words from the query topics and the computation of the semantic similarity between word vectors done with a Natural Language Processing library called SpaCy <ref type="bibr" coords="4,194.52,214.85,9.96,8.74" target="#b1">[2]</ref>. From the topic title, description and narrative, relevant words were extracted and organized into different categories, such as relevant things, negative things, activities, dates, locations and environment.</p><p>Using topic 1 as an example :</p><p>-Title : "Praying Rite." -Description : "Find the moment when u1 was attending a praying rite with other people in the church." -Narrative : "To be relevant,the moment must show u1 is currently inside the church, attending a praying rite with other people. The moments that u1 is outside with the church visible or inside the church but is not attending the praying rite are not considered relevant."</p><p>The extracted textual data is as follows:</p><p>relevant things -"rite" , people".</p><p>activities -"praying", "praying rite", "attending".</p><p>locations -"church" dates -empty.</p><p>user inside -"true".</p><p>user outside -"false".</p><p>negative relevant thing -"church visible".</p><p>negative locations: empty.</p><p>negative activities : empty.</p><p>negative dates: empty.</p><p>Afterwards, a confidence score is computed for each image in the dataset. The score is obtained through the comparison of the extracted words from the topic and the extracted labels from the images. This score is influenced by the scores of the image concepts obtained through the object detection phase and the different weights assigned to each category. The weight for each category is obtained through two different factors, a factor of importance and a computed factor.</p><p>In Run 1, the importance factor for all categories is the same. This means that each category has the same weight for the computation of the confidence score.</p><p>For Run 2, we decided to define the importance factor differently for each category. We give a bigger importance to specific categories like "relevant things" in order to improve results, since we compute the similarity of this textual category with our object detection extracted image label concepts. Categories like "activities" and "locations" get a lesser importance factor since they are being compared to the organizers label data which is limiting and lesser accurate. The sum of all importance factors of all categories is equal to 1, which represents 100%.</p><p>The computed factor is obtained from the distribution of the factor of importance from empty categories to all other categories. If we don't extract any textual data from a query topic for the category "activities", this category will be empty, therefore, we apportioned the importance factor of the "activities" category to all other categories, increasing their importance factor, in order to maintain the sum of 1. This value is not the same for each category, we maintain the ratio of the distribution the same as the distribution of the importance factor between all categories. To make it clearly, if the importance factor for "relevant things" is 0.5, which is half of the sum of all importance factors, and if the "activities" category is worth 0.2 and has no extracted textual data, then half of 0.2 is distributed to "relevant things", which increases the importance to 0.6 and the remainder 0.1 will be distributed the same way to other categories ensuring that the sum of all importance factors is 1.</p><p>The negative categories works the same way, but instead of contributing for the confidence score, it decreases the value of the confidence.</p><p>A general threshold was previously defined in order to remove images of low concept scores or low confidence score, images above the threshold are selected for the query topic. The threshold was implemented through some trial and error during the test phases, and it merely serves the purpose of saving some computational time.</p><p>Run 2 differs from Run 1 not only in the image processing step, where different image processing algorithms were used, but also in the retrieval step, where all factors of importance were altered in order to give more importance to some categories than others, as previously discussed. Another difference is the negative category which was discarded from the calculation of the confidence score in Run 2.</p><p>Finally, a script runs through all the selected confidence scores for a given query topic and stores the fifty highest on the csv file. As expected by the previous year results, this automatic and exhaustive approach is not the most suitable for a lifelog application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Web application (Run 3)</head><p>To improve our results in this challenge, we develop a web application in order to visualize and provide an interactive tool for our lifelog system. As encouraged by the organizers, in this run, we used a method that allows interaction with users. As a first approach, we are only considering the data provided by the challenge organizers. We divided the web application into three stages, respectively:</p><p>-Upload: the user uploads the images from the lifelog dataset into the application. The textual data annotations provided by the organizers are au-tomatically uploaded and organized in the application database associated with uploaded images. -Retrieval: the user introduces the inputs words extracted from the query topic into several words categories, date and time. The retrieval process starts comparing these inputs with the app database information. Finally, a confidence to each image retrieved is assigned for the query topic. -Visualization: the user visualizes the retrieved images and scores, in form of image gallery or data tables, divided into timestamp clusters. The user choose manually the relevant clusters for the query topic.</p><p>Figure <ref type="figure" coords="6,181.51,240.31,4.98,8.74" target="#fig_0">1</ref> shows a general representation of our lifelog application. In a first stage, the user has to upload the images into the application, which are stored in the database together with the data provided by the organizers for each image from the lifelog dataset. Afterwards, the user requests the image retrieval for the query topic by introducing relevant words manually in the application, the stage of retrieval begins. These relevant words are divided into several categories, such as objects, locations, activities, irrelevant words, date and time, and they are compared with the labels stored in the database. This comparison is made using the similarity of word vectors. Images with labels similar to the topic relevant words are selected. Subsequently, the confidence for the corresponding image is computed through the similarity value of the labels and the score of each similar label in the database. In order to reduce the amount of images retrieved by the system, images with low confidence are excluded from the output images. At the end, the retrieved images are clustered by timestamp intervals and the user can visualize the images in the form of image gallery or data tables.</p><p>A more detailed explanation is provided in the following sections for each stage of our lifelog application. Upload In an initial stage, the user uploads the images dataset into the lifelog application that are organized and stored in the database associated with some of the data provided by the organizers, such as visual concepts and metadata. The data is organized in our database into different tables/models, such as images, concepts, locations, activities, scenes, attributes, among others. In our application, each model maps to a single database table. Figure <ref type="figure" coords="7,387.55,178.77,4.98,8.74" target="#fig_1">2</ref> shows a diagram of these data models in the database. The relationship between models makes our system faster and more efficient compared to an exhaustive approach.</p><p>The image model has a many-to-many relationship with the models concept, location, category, activity and attribute. For example: an image can contain several concepts, and a concept can be found in several images. The tag field of the label model is the labels name extracted from the visual concepts and metadata, which has a one-to-many relationship with the other models, in other words, one label may be connected to several images and this label can be associated to several models, such as concept and category models, depending on the type of label and the number of times that appear in the image. Usually, the name of the labels are in their base form or dictionary form, called the word's lemma, however labels in other forms are transformed to the basic form for further use. This transformation is called lemmatizer. Retrieval Unlike the exhaustive approach of run 1 and 2, that compute the confidence of each image, this approach (run 3) only computes the confidence of some images that are selected in a first step for the specific topic by using the similarity of word vectors, which makes this retrieval method more efficient and using less processing time.</p><p>The topics are manually analysed by the user, which extracts relevant words from them. By introducing these words divided into several categories, such as objects, locations, activities and irrelevant words in the application, the retrieval step begins. If a topic contains time ranges, years or days of the week, the user can also insert that data in our application to further filter the retrieved images. Figure <ref type="figure" coords="8,166.20,239.02,4.98,8.74" target="#fig_2">3</ref> shows the retrieval view of the web application.</p><p>In the retrieval stage, the input arguments are: objects that appear on the images; activities that the user was practicing; locations or places where the user was; negatives or irrelevant things, activities or locations that should not appear in the images; time ranges, years and days of the week (Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, and Sunday). The SpaCy library <ref type="bibr" coords="8,232.93,572.43,10.52,8.74" target="#b1">[2]</ref> is used for two different tasks: to assign the base forms words (lemmatizer) and to compare word vectors (cosine similarity). As in the upload stage, the input words are processed to their lemma, which improves and facilitate the comparison between word vectors. Afterward, the similarity between the processed input words and the labels stored the database is computed. Images that contains labels that are similar or equal to the words entered by the user are selected to compute the confidence. If the user enters negative words in our applications, images with labels similar or equal to these negative words are automatically excluded. In order to improve the processing time of the retrieval stage, the similarity of word pairs are stored in the database so that it is not necessary to compute the similarity of the same word pair more than once.</p><p>The confidence of the selected images is computed using the similarity calculated previously and the score of the labels. For labels without score field, it is only used the similarity to calculate the confidence. As last filtering on the retrieval stage, the images are selected based on the confidence threshold.</p><p>Visualization The selected images are organized into different clusters based on images timestamps provided by the organizers. The retrieved images were visualized in our application organized into the timestamp clusters. The application provides an easy way for users to visualize and identify the clusters that are associated to the specific topic. Figure <ref type="figure" coords="9,318.52,280.39,4.98,8.74" target="#fig_3">4</ref> shows the user view of the clustered images in form of images gallery. We provided another way of visualization in form of a data table as shown in Figure <ref type="figure" coords="9,311.08,304.30,3.87,8.74" target="#fig_4">5</ref>.</p><p>In order to improve the results, the user can exclude several irrelevant images from the selected clusters. To improve the cluster recall of the run, the user can change the confidence of a relevant image of each selected timestamp clusters to the maximum confidence that consequently increases the f1 measure of this run.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We submitted a total of 3 runs on the LMRT sub-task. In this task, an arithmetic mean of all query topics results is calculated as the final score. The ranking metrics was the F1-measure@10, which gives equal importance to diversity (via CR@10) and relevance (via P@10), Cluster Recall and Precision at top 10 results, respectively.</p><p>We described the three submissions in Section 3. The first two submissions follows an automatic manner as in our previous work <ref type="bibr" coords="10,372.60,452.39,9.96,8.74" target="#b6">[7]</ref>. Due to the results of this automatic approach, we take into consideration the development of a system that allows interaction with real users, as emphasized by the organizers.</p><p>Comparing the automatic with the interactive approach, a significant improvement can be seen. This improvement is due to not only to the new retrieval approach, but also to the interactive and visual approach. We consider the visualization and user interaction one of the most important tools in a lifelog application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">UA.PT Bioinformatics Results</head><p>The results obtained are shown in Table <ref type="table" coords="10,308.33,584.39,3.87,8.74" target="#tab_0">1</ref>, along with the best result in this task, for comparison. The results of all of the participating teams can be found in <ref type="bibr" coords="10,467.31,596.34,9.96,8.74" target="#b4">[5]</ref>. We can observe that our last submission (run 3) is still not the best on this task, but we made a considerable improvement compared with the automatic approach from this year and the previous year <ref type="bibr" coords="10,343.91,632.21,9.96,8.74" target="#b6">[7]</ref>, and we are also closing the gap between the best ones, such as HCMUS team with the best F1-measure@10 on the LMRT task, with the ambition to obtain much better results. Considering the results shown in Table <ref type="table" coords="11,327.71,235.42,4.98,8.74" target="#tab_0">1</ref> we are convinced that the interactive approach is a better suited method for the LMRT challenge, the user visualization and interaction with the application allows for much more accurate results. Creating a fully automatic system is complicated, this is because it requires a lot of processing power, every image has to be fully processed in order to extract labels. However, considering that computing time is not a problem, a few ways that we could improve the results of our automatic approach in the future would be implementing activity recognition algorithms, color recognition algorithms and better scene recognition algorithms.</p><p>As an initial lifelog application, the results shows that we are in a good path to solve some of the problems that exist in these challenges, which could help to improve the daily lives of many people. Considering the previous work problems <ref type="bibr" coords="11,134.77,379.36,9.96,8.74" target="#b6">[7]</ref>, we solve some of them in this work, such as the identification of bigrams, trigrams or n-grams, which allows to compute the similarity between n-grams or sentences.</p><p>In our application, we only use the information provided by the organizers, which leaves us somewhat limited as to the visual concepts in the lifelog images. We believe that using the most recent state-of-art algorithms, a more rich description of the images can be obtained, resulting in a performance increase. In the future, we intend to integrate in our application features that have already been developed in previous work, such as selecting images in upload stage based on low level properties <ref type="bibr" coords="11,236.75,487.44,9.96,8.74" target="#b7">[8]</ref>. However, we think that using more of the metadata provided by the organizers can also improve the result. For example, make use of the GPS coordinates (latitude and longitude) to trace the lifelogger routes, such as the way home to work and vice-versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>The Lifelog Moment Retrieval (LMRT) sub-task of ImageCLEF lifelog 2020 was the baseline for a new web application that aims to help people to improve their quality of life.</p><p>We obtained the same exact results for the automatic approach (run 1 and run 2) even when using different state-of-the-art object detection algorithms and different weights for each category. Some of the reasons for this to occur is because much of the used information used was provided by the organizers, like activities and locations. Not only that, but the obtained scene recognition labels were not accurate enough.</p><p>In our interactive approach, using the application developed we were able to obtain a F1-measure@10 score of 0.52, which is till date our best. This makes us believe that an approach with visualization and user interaction is a more suitable method for a lifelog application. Although the results are already better compared to the previous work, our application is a baseline version which still requires improvements and new tools.</p><p>For future improvements in our approaches, we pretend to implement better scene recognition, object detection, activity and color detection algorithms, since color was a relevant element in some of the topics in the LMRT task. We will also use other data provided by the organizers, such as GPS coordinates and integrate features that have already been implemented in previous work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,134.77,644.07,345.83,7.89;6,134.77,655.05,311.11,7.86;6,152.06,464.89,311.24,164.41"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. General representation of the developed web application. The user interacts with the three stages of the application: Upload, Retrieval and Visualization.</figDesc><graphic coords="6,152.06,464.89,311.24,164.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,150.19,627.57,314.99,7.89;7,169.35,378.02,276.66,234.78"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Diagram of the proposed database tables used by the web application.</figDesc><graphic coords="7,169.35,378.02,276.66,234.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,229.46,538.24,156.45,7.89;8,134.77,332.21,345.82,191.26"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Web application retrieval view.</figDesc><graphic coords="8,134.77,332.21,345.82,191.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,175.44,579.41,264.47,7.89;9,134.77,383.54,345.83,181.10"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. User view of the image clusters in form of image galleries.</figDesc><graphic coords="9,134.77,383.54,345.83,181.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,182.62,324.53,250.11,7.89;10,134.77,115.83,345.83,193.92"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. User view of the image clusters in form of data tables.</figDesc><graphic coords="10,134.77,115.83,345.83,193.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,134.77,115.91,345.83,90.91"><head>Table 1 .</head><label>1</label><figDesc>F1-measure@10 of each run submitted by us and the best team run in the LMRT task.</figDesc><table coords="11,222.84,145.78,169.69,61.04"><row><cell cols="3">Team Run Name F1-measure@10</cell></row><row><cell></cell><cell>Run 1</cell><cell>0.03</cell></row><row><cell>Our</cell><cell>Run 2</cell><cell>0.03</cell></row><row><cell></cell><cell>Run 3</cell><cell>0.52</cell></row><row><cell cols="2">HCMUS Run 10</cell><cell>0.81</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgments</head><p>Supported by the <rs type="programName">Integrated Programme</rs> <rs type="funder">of SR&amp;TD SOCA</rs> (Ref. <rs type="grantNumber">CENTRO-01-0145-FEDER-000010</rs>), co-funded by <rs type="programName">Centro 2020 program</rs>, <rs type="funder">Portugal 2020</rs>, <rs type="funder">European Union</rs>, through the <rs type="funder">European Regional Development Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_yhPfAaP">
					<idno type="grant-number">CENTRO-01-0145-FEDER-000010</idno>
					<orgName type="program" subtype="full">Integrated Programme</orgName>
				</org>
				<org type="funding" xml:id="_J9KDuqm">
					<orgName type="program" subtype="full">Centro 2020 program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,385.65,337.64,7.86;12,151.52,396.60,329.07,7.86;12,151.52,407.54,117.15,7.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,252.44,385.65,228.16,7.86;12,151.52,396.60,141.56,7.86">outlines of a world coming into existence&apos;: pervasive computing and the ethics of forgetting</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kitchin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,300.74,396.60,179.85,7.86;12,151.52,407.56,25.14,7.86">Environment and planning B: planning and design</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="445" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,417.82,337.63,7.86;12,151.52,428.78,329.07,7.86;12,151.52,439.74,17.69,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="12,261.69,417.82,218.90,7.86;12,151.52,428.78,271.60,7.86">spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Montani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="12,142.96,449.99,337.64,7.86;12,151.52,460.95,329.07,7.86;12,151.52,471.91,329.07,7.86;12,151.52,482.87,329.07,7.86;12,151.52,493.82,329.07,7.86;12,151.52,504.78,329.07,7.86;12,151.52,515.74,329.07,7.86;12,151.52,526.70,329.07,7.86;12,151.52,537.66,329.07,7.86;12,151.52,548.62,329.07,7.86;12,151.52,559.58,34.31,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,295.04,504.78,185.55,7.86;12,151.52,515.74,255.37,7.86">Overview of the ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,426.43,515.74,54.16,7.86;12,151.52,526.70,329.07,7.86;12,151.52,537.66,253.34,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="12,456.14,537.66,24.45,7.86;12,151.52,548.62,138.63,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="12,142.96,569.83,337.64,7.86;12,151.52,580.79,329.07,7.86;12,151.52,591.75,329.07,7.86;12,151.52,602.71,60.92,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,255.93,580.79,220.73,7.86">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,166.73,591.75,291.68,7.86">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,612.96,337.64,7.86;12,151.52,623.92,329.07,7.86;12,151.52,634.88,329.07,7.86;12,151.52,645.84,329.07,7.86;12,151.52,656.80,220.19,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,349.61,623.92,130.98,7.86;12,151.52,634.88,261.15,7.86">Overview of ImageCLEF Lifelog 2020:Lifelog Moment Retrieval and Sport Performance Lifelog</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="12,437.46,634.88,43.13,7.86;12,151.52,645.84,61.29,7.86">CLEF2020 Working Notes</title>
		<title level="s" coord="12,223.10,645.84,179.07,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,119.67,337.64,7.86;13,151.52,130.63,97.80,7.86" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m" coord="13,260.26,119.67,151.93,7.86">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.96,141.59,337.63,7.86;13,151.52,152.55,329.07,7.86;13,151.52,163.51,155.58,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,317.93,141.59,162.66,7.86;13,151.52,152.55,329.07,7.86;13,151.52,163.51,11.14,7.86">Ua.pt bioinformatics at imageclef 2019: Lifelog moment retrieval based on image annotation and natural language processing</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,183.52,163.51,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,174.47,337.63,7.86;13,151.52,185.43,329.07,7.86;13,151.52,196.39,329.07,7.86;13,151.52,207.34,68.27,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,315.26,174.47,165.33,7.86;13,151.52,185.43,125.26,7.86">Image selection based on low level properties for lifelog moment retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,297.32,185.43,183.27,7.86;13,151.52,196.39,25.39,7.86">Twelfth International Conference on Machine Vision</title>
		<meeting><address><addrLine>ICMV</addrLine></address></meeting>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2019">2019. 2020</date>
			<biblScope unit="volume">11433</biblScope>
			<biblScope unit="page">1143303</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,218.30,337.63,7.86;13,151.52,229.26,329.07,7.86;13,151.52,240.22,243.90,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,379.15,218.30,101.44,7.86;13,151.52,229.26,268.74,7.86">Computer vision for lifelogging: Characterizing everyday activities based on visual semantics</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,440.12,229.26,40.47,7.86;13,151.52,240.22,122.54,7.86">Computer Vision for Assistive Healthcare</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="249" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,251.18,337.97,7.86;13,151.52,262.14,329.07,7.86;13,151.52,273.07,182.13,7.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,399.52,251.18,81.07,7.86;13,151.52,262.14,147.87,7.86">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,306.73,262.14,173.86,7.86;13,151.52,273.10,80.90,7.86">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
