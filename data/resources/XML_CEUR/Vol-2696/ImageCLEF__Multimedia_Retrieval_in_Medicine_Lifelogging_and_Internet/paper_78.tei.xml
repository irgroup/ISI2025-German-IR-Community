<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.72,115.96,335.92,12.62;1,168.05,133.89,279.27,12.62;1,141.40,151.82,332.57,12.62;1,270.58,169.76,74.21,12.62">AIML at VQA-Med 2020: Knowledge Inference via a Skeleton-based Sentence Mapping Approach for Medical Domain Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,218.87,211.35,50.50,8.74"><forename type="first">Zhibin</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">South Australian Health and Medical Research Institute</orgName>
								<address>
									<settlement>Adelaide</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,286.26,211.35,28.78,8.74"><forename type="first">Qi</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.60,211.35,63.66,8.74"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,207.64,223.30,97.42,8.74"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,334.98,223.30,61.93,8.74"><forename type="first">Johan</forename><surname>Verjans</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">South Australian Health and Medical Research Institute</orgName>
								<address>
									<settlement>Adelaide</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.72,115.96,335.92,12.62;1,168.05,133.89,279.27,12.62;1,141.40,151.82,332.57,12.62;1,270.58,169.76,74.21,12.62">AIML at VQA-Med 2020: Knowledge Inference via a Skeleton-based Sentence Mapping Approach for Medical Domain Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B4F5DDB1A44A7BA4786C69DC4657EAF0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>Visual Question Generation</term>
					<term>Knowledge Inference</term>
					<term>Deep Neural Networks</term>
					<term>Skeleton-based Sentence Mapping</term>
					<term>Class-wise and Task-wise Normalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our contribution to the 2020 Im-ageCLEF Medical Domain Visual Question Answering (VQA-Med) challenge. Our submissions scored first place on the VQA challenge leaderboard, and also the first place on the associated Visual Question Generation (VQG) challenge leaderboard. Our VQA approach was developed using a knowledge inference methodology called Skeleton-based Sentence Mapping (SSM). Using all the questions and answers, we derived a set of classifiable tasks and inferred the corresponding labels. As a result, we were able to transform the VQA task into a multi-task image classification problem which allowed us to focus on the image modelling aspect. We further propose a class-wise and task-wise normalization facilitating optimization of multiple tasks in a single network. This enabled us to apply a multi-scale and multi-architecture ensemble strategy for robust prediction. Lastly, we positioned the VQG task as a transfer learning problem using the VGA task trained models. The VQG task was also solved using classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual question answering (VQA) <ref type="bibr" coords="1,282.67,557.25,12.01,8.74" target="#b3">[4,</ref><ref type="bibr" coords="1,294.68,557.25,12.01,8.74" target="#b19">20]</ref> is a challenging new task which requires a broad knowledge of image processing, natural language processing (NLP), and multi-modal learning. In the medical domain, VQA is an attractive topic showing great potential in automated medical image interpretation and machine supported diagnoses, with potential to benefit both medical practitioners and patients. Nevertheless, medical VQA remains an unsolved problem. The Image-CLEF association <ref type="bibr" coords="2,215.48,130.95,15.50,8.74" target="#b14">[15]</ref> has been hosting the Medical Domain VQA (VQA-Med) challenges for three consequent years since 2018 <ref type="bibr" coords="2,346.07,142.90,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,357.83,142.90,7.75,8.74" target="#b4">5,</ref><ref type="bibr" coords="2,366.81,142.90,11.62,8.74" target="#b9">10]</ref>. In the 2018 challenge, the images were extracted from PubMed Central articles with the questions and answers automatically generated from image captions before checked manually by human annotators. In addition to the clarity issues of the machine generated questions as reported by <ref type="bibr" coords="2,247.81,190.72,9.96,8.74" target="#b0">[1]</ref>, it is also noticeable that both the questions and ground-truth answers are in variable-length and free-form, both of which add difficulties to the answer generation task. The 2019 challenge <ref type="bibr" coords="2,387.73,214.64,10.52,8.74" target="#b1">[2]</ref> advanced from the previous challenge by narrowing the task scope: 1) using only radiology images; and 2) asking questions in four topics (i.e., image modality, imaging plane, visualized organ systems, and abnormality detectable from an image). As noticed by many participated teams, the 2019 challenge is solvable in a classification manner, i.e., there are 36 unique answers for the modality questions, 16 for the plane questions, 10 for the organ questions, with an exception of over a thousand possible answers for the abnormality category. A post-challenge question category-wise accuracy analysis <ref type="bibr" coords="2,274.39,310.28,10.52,8.74" target="#b1">[2]</ref> suggests that the modality, plane, and organ categories possess much better accuracy compared to the abnormality category.</p><p>In the 2020 VQA challenge, our AIML team participated in, the dataset <ref type="bibr" coords="2,470.08,341.87,10.52,8.74" target="#b4">[5]</ref> was curated with only questions in abnormality category. While analyzing the questions, we found that questions come in two major forms: 1) yes/no questions, e.g., "is this image normal/abnormal?", and 2) wh-questions e.g., "what is abnormal in the image?". In comparison to the last year's challenge, we noticed the unique question phrasings were reduced from 253 to 52 and the unique answer phrasings from 1,749 to 332, while having a 25% increase of images (from 3,200 to 4,000 in the training set; validation and test sets are equal), resulting in a much richer data support for the VQA task.</p><p>Our initial attempt at the 2020 VQA-Med challenge was to fine-tuning of the Pythia <ref type="bibr" coords="2,167.90,469.11,15.50,8.74" target="#b26">[27]</ref> model. However, this did not yield a desirable performance, hence after which we conducted an analysis of the predicted answers. The analysis led to the development of a novel knowledge inference method, namely Skeletonbased Sentence Mapping (SSM) that helped reverse engineer a set of question backbones. SSM helped us to determine the question categories and infer corresponding labels, reducing the VQA problem to a pure multi-task image classification problem. As a result, we were able to focus on the imaging modality. In particular, we developed a class-wise and task-wise normalization method to give balanced weighting to presented classes and tasks in a mini-batch. This helps to jointly optimize multiple tasks in a single network. At last, we applied multi-scale and multi-architecture ensemble learning. Our best submission scored 0.496 in accuracy and 0.542 in BLEU score which won the first place at the 2020 VQA challenge.</p><p>For the associated Medical Domain Visual Question Generation (VQG-Med) challenge, we considered the task as a transfer learning problem, where we applied the VQA-Med data trained models as non-trainable feature extractors. The answer generation is also formed as a classification task. Our best submission scored 0.348 in BLEU score which won the first place at the VQG challenge.</p><p>In the rest of the paper, we give explanations on our VQA and VQG approaches. Each approach is a self-contained section to avoid cluttering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">VQA-Med Challenge Participation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Literature Review</head><p>We will first introduce the general domain VQA methods followed by an introduction to the methods that have been applied specifically in the medical domain VQA.</p><p>General domain VQA: the goal of a VQA method is to produce an answer from a given image-question pair. Early VQA works <ref type="bibr" coords="3,366.76,298.31,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="3,378.67,298.31,7.75,8.74" target="#b8">9,</ref><ref type="bibr" coords="3,387.82,298.31,12.73,8.74" target="#b19">20,</ref><ref type="bibr" coords="3,401.95,298.31,12.73,8.74" target="#b23">24]</ref> used a general CNN-RNN framework. In brief, the CNN-RNN approach is carried out using a Convolutional Neural Network (CNN) model (e.g., VGG-Net <ref type="bibr" coords="3,399.29,322.22,15.50,8.74" target="#b25">[26]</ref>) to process the input image and a Recurrent Neural Network (RNN) Encoder-Decoder <ref type="bibr" coords="3,442.10,334.18,10.52,8.74" target="#b6">[7]</ref> (more specifically, LSTM <ref type="bibr" coords="3,218.02,346.13,15.50,8.74" target="#b11">[12]</ref>) to handle the language modelling. While the vision and language information fusion component can also be handled by the RNN language model altogether, or just by concatenation, there are also more advanced options such as the Multi-modal Factorized Bilinear (MFB) pooling and Highorder pooling (MFH) <ref type="bibr" coords="3,229.21,393.95,15.50,8.74" target="#b37">[38]</ref> and MUTAN <ref type="bibr" coords="3,307.40,393.95,9.96,8.74" target="#b5">[6]</ref>. Attention is also a frequently visited topic in VQA, e.g., question-guided visual attention methods <ref type="bibr" coords="3,401.07,405.91,16.65,8.74" target="#b34">[35,</ref><ref type="bibr" coords="3,417.71,405.91,12.49,8.74" target="#b36">37]</ref> and visionlanguage co-attention methods <ref type="bibr" coords="3,274.13,417.86,15.50,8.74" target="#b18">[19,</ref><ref type="bibr" coords="3,291.33,417.86,11.62,8.74" target="#b37">38]</ref>. Finally, semantic image representation (e.g., attribute-based image representation <ref type="bibr" coords="3,325.04,429.82,14.76,8.74" target="#b30">[31]</ref>), pretrained language representation (e.g., BERT <ref type="bibr" coords="3,221.64,441.77,10.30,8.74" target="#b7">[8]</ref>), external knowledge and common sense knowledge <ref type="bibr" coords="3,465.09,441.77,15.50,8.74" target="#b31">[32]</ref> could all be beneficial towards solving VQA.</p><p>Medical domain VQA: a noticeable difference between the medical domain and general domain VQA is the size of the dataset. The general domain VQA can accumulate a sizable dataset due to the fact that a common-sense knowledge is sufficient for generating question and answers. On the other hand, the necessity of clinical expertise imposes a huge difficulty in the medical domain VQA data collection.</p><p>In the 2018 VQA-Med challenge, the leading<ref type="foot" coords="3,347.04,554.34,3.97,6.12" target="#foot_0">3</ref> three participating teams <ref type="bibr" coords="3,470.08,555.91,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="3,134.77,567.87,12.73,8.74" target="#b22">23,</ref><ref type="bibr" coords="3,148.50,567.87,12.73,8.74" target="#b38">39]</ref> differentiate in image modelling (i.e., ResNet-152 <ref type="bibr" coords="3,380.53,567.87,14.61,8.74" target="#b10">[11]</ref>, Inception-ResNet-v2 <ref type="bibr" coords="3,148.52,579.82,14.61,8.74" target="#b27">[28]</ref>, VGG-16), language modelling (i.e., LSTM, Bi-LSTM), vision-language fusion (i.e., MFB/MFH <ref type="bibr" coords="3,241.55,591.78,14.60,8.74" target="#b37">[38]</ref>, SAN <ref type="bibr" coords="3,286.83,591.78,14.76,8.74" target="#b36">[37]</ref>), attention models (i.e., question guided attention <ref type="bibr" coords="3,178.36,603.74,14.61,8.74" target="#b34">[35]</ref>, co-attention <ref type="bibr" coords="3,256.98,603.74,14.76,8.74" target="#b37">[38]</ref>), and word embeddings (i.e., word2vec <ref type="bibr" coords="3,452.19,603.74,15.50,8.74" target="#b20">[21]</ref> or medical article pretrained embedding <ref type="bibr" coords="4,299.63,118.99,14.76,8.74" target="#b22">[23]</ref>). Considering the component-wise diversity and minor performance gaps, it is difficult to find out which component is favourable. However, we notice that all three teams treated the VQA task as a classification problem whereas the rest two teams treated the problem as a generation task <ref type="bibr" coords="4,206.27,166.81,15.50,8.74" target="#b28">[29]</ref> or still a classification task but not fine-tuning the image model <ref type="bibr" coords="4,164.37,178.77,9.96,8.74" target="#b2">[3]</ref>.</p><p>In the 2019 VQA-Med challenge, the top three teams <ref type="bibr" coords="4,397.82,190.74,15.50,8.74" target="#b29">[30,</ref><ref type="bibr" coords="4,415.43,190.74,12.73,8.74" target="#b35">36,</ref><ref type="bibr" coords="4,430.27,190.74,12.73,8.74" target="#b39">40]</ref> (with a working notes paper) all used BERT <ref type="bibr" coords="4,304.39,202.70,10.52,8.74" target="#b7">[8]</ref> for language processing. Apart from that, we point to some of the unique techniques from the top three teams. The winning team Hanlin <ref type="bibr" coords="4,227.47,226.61,15.50,8.74" target="#b35">[36]</ref> has adopted Global Average Pooling (GAP) <ref type="bibr" coords="4,436.97,226.61,15.50,8.74" target="#b17">[18]</ref> shortcuts. This differs from the conventional position of GAP which connects the last convolution layer and the classification layer. The Hanlin team placed multiple GAPs that each links to a low-level convolution layer and forwards the pooled low-level features to be concatenated with the final image representation. The second-place team minhvu <ref type="bibr" coords="4,254.85,286.39,15.50,8.74" target="#b29">[30]</ref> adopted an ensemble learning approach with a variation of VQA components. The third-place team TUA1 <ref type="bibr" coords="4,395.26,298.34,15.50,8.74" target="#b39">[40]</ref> used a question classifier to figure out the question category and then choose answers from a set of modality, plane, and organ classifiers and a generative model for abnormality answers. Note that the question classification strategy was also employed by several other participated teams; therefore we speculate the use of BERT could have been the delimiting factor that caused a noticeable gap of 0.04 (in both accuracy and BLEU) between the third place <ref type="bibr" coords="4,342.09,370.07,15.50,8.74" target="#b29">[30]</ref> and fourth place <ref type="bibr" coords="4,439.34,370.07,15.50,8.74" target="#b24">[25]</ref> (who also used question classification and sub answer models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset</head><p>The VQA-Med 2020 dataset has a composition of 4,000 radiology images for training, 500 for validation, and 500 for testing. Each image has exactly one Question-Answer (QA) pair from the abnormality question category.</p><p>We followed the official suggestion to use the VQA-Med 2019 dataset<ref type="foot" coords="4,462.80,466.34,3.97,6.12" target="#foot_1">4</ref> as additional training data. The VQA-Med 2019 dataset has 3,200 medical images for training, 500 for validation, and another 500 for testing. For training and validation sets, there are 12,792 and 2,000 QA pairs, giving most images exactly one QA pair in each question category (i.e., imaging modality, imaging plane, organ systems, and abnormality). For the test set, each question category has 125 images. In addition, the yes-no questions appear only in the imaging modality and abnormality question categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Skeleton-based Sentence Mapping</head><p>As mentioned in Sec. 1, Pythia <ref type="bibr" coords="4,281.97,601.60,15.50,8.74" target="#b26">[27]</ref> was our initial attempt, from which we observed a proportion of the yes-no questions answered by categorical abnormality answers and vice versa. This could be a sign of insufficient question variations. To address this issue, we tried to develop a question generator to populate training questions while keeping the meaning unchanged. The Skeletonbased Sentence Mapping (SSM) method was developed to summarize questions with similar sentence structures into a unified backbone. An example of the derived sentence backbones are shown in Table <ref type="table" coords="5,336.77,154.86,3.87,8.74">1</ref>. Taking the question backbone "is ${this pronoun alts} ${ct alts} ${normal alts}? " as an example, we call the swap-able parts the skeleton variables and write in the Shell variable style "${. . . }". An example can be found in Table <ref type="table" coords="5,333.19,190.72,3.87,8.74" target="#tab_1">2</ref> Before applying SSM, we first removed the duplicated questions in the dataset, resulting in 266 unique questions. After then, we applied word-level edit distance (i.e., levenshtein distance) to pairs of questions, finding groups of questions with 1-distance and 2-distance. For example, in Table <ref type="table" coords="5,348.60,589.08,3.87,8.74">1</ref>, the corresponding questions of each question backbone mostly have either 1-distance or 2-distance within the group, and the highest 4-distance is between "what is shown in the x-ray?" and "what is seen in this ct scan?". The grouped questions were manually checked to see if the dissimilar parts can be described by a unified skeleton variable. If so, the generated backbone would replace the group of question and enter the next iteration of edit distance computation. The first iteration was able to detect most of the easy question groups, leave the later iterations with a small number of questions.</p><p>The process was ran until all questions were skeletonized, resulting in 68 question backbones. We labeled the question backbones in the four aforementioned question categories, partially based on the corresponding answers. In addition, we also determined two sub categories under the imaging modality category, namely the MR modality category and the contrast imaging type category. Next, we compared our own question category annotation with the official question category annotation for the VQA-Med 2019 test set (only available in this set), which is equivalent. The SSM was able to populate dynamic question variations (with some rule based restrictions, e.g., changing "ct scan" in "is the ct scan normal?" to other candidates except "ct" and "image" results in a fallacious judgement of the image modality, hence is not allowed) and the same Pythia model trained with the augmented questions was able to rectify the yes-no and wh-question cross answering errors. Nevertheless, we found the SSM method rendered language modelling trivial. With its help, we can solve the VQA task as an image classification task.</p><p>Label inference from question backbones: based on the question category annotation, we were able to record the paired answer annotation as the label for each mapped task. In addition, we could also extract labels from the skeleton variables. For example, for the first question "is the ct scan normal?" in Table <ref type="table" coords="6,472.85,424.89,3.87,8.74">1</ref>, "ct" is capturable by ${ct alts} and "normal" is capturable by ${normal alts}; hence producing a coarse modality label "ct", and also produce a binary abnormality label "normal" if the answer is a "yes". We found the same can also be generalized to infer task labels from the wh-questions.</p><p>An issue with the question backbone derived modality labels is that the detailed modality (e.g., ct with contrast or not) is unknown. To address this issue, we treat the coarse modality labels as an independent task. The answer derived modality labels were mapped back to the coarse labels following the information provided in <ref type="bibr" coords="6,187.92,532.66,9.96,8.74" target="#b1">[2]</ref>. Next, we treated all abnormality wh-questions to have an "abnormal" label to add to the yes-no question derived binary abnormality labels.</p><p>At the end of the process, we were able to produce six classification tasks: 1) fine imaging modalities; 2) coarse imaging modalities; 3) imaging plane; 4) organ systems; 5) binary abnormality, and 6) categorical abnormality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multi-task Image Classification</head><p>The schematic of an exemplar image classification network we used is illustrated in Fig. <ref type="figure" coords="6,168.95,644.16,3.87,8.74" target="#fig_0">1</ref>, sketched with the knowledge inference process. The two important tasks are the binary and categorical abnormality classification tasks while the rest four can be thought as regularization tasks. We believe that all the tasks should have strong correlation to each other, i.e., the correct imaging modality and organ judgements should be strong prior knowledge for correct recognition of abnormality.  Class-wise and task-wise normalization: since only the 2019 challenge images have (almost) complete four QA pairs per image, a large number of images in the joint 2019 and 2020 dataset do not have a complete label set (mainly the 2020 images). Hence when all six tasks are jointly optimized via a mini-batch gradient method, a conventional normalization by the batch size effectively assigns a lower weight to a less populated task, e.g., for a batch with 12 images, a task that has 3 labeled images effectively has 0.25 weighting. In addition to the incomplete label problem, we also observed imbalanced class distributions within the tasks. For example in the categorical abnormality question category, the number of samples per abnormality class ranges from 4 to 104. We propose to solve both issues together by a class-wise and task-wise normalization in order to jointly optimize all six tasks together. Assume that t ∈ {coarse modality, . . .} represents a task, for a set of images X and the label set Y t , the mini-batch training loss L is computed as:</p><formula xml:id="formula_0" coords="7,134.77,626.65,351.37,38.20">L = t 1 ct 1(c t ∈ Y t ) ct 1 yt∈Yt 1(y t = c t ) x∈X,yt∈Yt 1(y t = c t )• t (x, y t ) ,<label>(1)</label></formula><p>where x ∈ X and y t ∈ Y t represent individual image and label, 1(.) denotes an indicator function, and c t denotes a candidate class of t (e.g., c t ∈ {ct, . . . , x-ray}, if t = coarse modality).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Multi-scale and multi-architecture ensemble</head><p>We adopted a multi-scale learning technique, using 128, 256, 384, and 512 as candidate image resize options. After applying the resize operation, we randomly crop the network input image at a ratio of 87.5% along both dimensions from a resized image. Random affine transformations and horizontal flip were used. The initial learning rate is set to 1e-3, linearly reduced 1e-6 after 100 epochs using Adam optimizer.</p><p>On the other hand, ResNets <ref type="bibr" coords="8,270.37,262.03,14.61,8.74" target="#b10">[11]</ref>, DenseNets <ref type="bibr" coords="8,338.70,262.03,14.61,8.74" target="#b13">[14]</ref>, ResNexts <ref type="bibr" coords="8,402.04,262.03,14.61,8.74" target="#b33">[34]</ref>, MobileNet <ref type="bibr" coords="8,470.12,262.03,14.60,8.74" target="#b12">[13]</ref>, and VGG nets <ref type="bibr" coords="8,200.63,273.98,15.50,8.74" target="#b25">[26]</ref> were selected as the image backbone candidates. We put the backbone and input scale options as training script hyper-parameters, which helped us to disperse the training over several GPU stations and gradually expand the number of ensemble members.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Experiment Results</head><p>We show the validation results from all trained models in Table <ref type="table" coords="8,425.64,357.24,3.87,8.74" target="#tab_3">3</ref>, the corresponding training volume includes 2019-{train, val, test} and 2020-train. Based on these results, we made decisions of which models to be trained for test evaluation. Note that the training volume was changed to all of the 2019-{train, val, test} and 2020-{train, val, test} sets for training the testing-use models. We included the 2020-test set because some amount of partial coarse imaging modality labels (i.e., from ${ct alts}) and binary abnormality labels (i.e., only the abnormal ones from wh-question abnormality) were extractable by SSM from only the questions, which served as a form of weak regularization for the test images. Finally, for the categorical abnormality type questions, we only select a top prediction from the VQA-Med 2020 subset of the abnormality classes as the predictions.</p><p>Our submissions on the 2020 validation set are shown in Table <ref type="table" coords="8,431.93,500.70,3.87,8.74" target="#tab_4">4</ref>. Our second submission was purposed to determine the exact category type of the last question backbone in Table <ref type="table" coords="8,260.15,524.61,4.98,8.74">1</ref> as the five instances all appear in the 2020 test set. Although all other 2020 questions were in the abnormality question category (aligned with the official statement), we found the five questions could also be interpreted as asking which organ is present. We treated the 5 questions as categorical abnormality questions in the first submission and as organ questions in the second submission. Given the accuracy dropped, the ground truth should be the abnormality category.</p><p>From a post-challenge point of view, our third submission secured the leading position in the leaderboard. Our fourth submission was purposed to include more DenseNet-121 instances in the ensemble as the DenseNet-121-only multiscale ensemble showed the highest 0.6 accuracy in Table <ref type="table" coords="8,381.18,644.16,3.87,8.74" target="#tab_3">3</ref>. Our fifth submission added the two VGG multi-scale groups, presenting the final ensemble result of all trained models. Nevertheless, these final attempts only pushed up the performance marginally, suggesting a performance saturation in our approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VQG-Med Challenge Participation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Challenge Overview</head><p>The VQG-Med challenge dataset is a much smaller dataset compared to the VQA-Med datasets. The training set contains 780 radiology images with 2,156 associated QA pairs. The validation set has 141 images with 164 QA pairs. The test set has only 80 images. The goal of the VQG challenge is to generate between 1 to 7 answers for each test image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Methodology</head><p>The VQG challenge describes a question generation task which in concept is close to image captioning but our proposed solution continued as a classification approach. The main reason is that we found there were more than one ground truth questions tied to each image. Unlike a VQA task, a question can be considered as a prior knowledge on which the corresponding answer is conditionally dependent. Generating multiple questions while lacking such prior knowledge could be resolved by sampling approaches, but it can be difficult to associate a random state to a specific ground truth question. Hence, we instead treated all observed questions for an image as its attributes and modelled the question generation task as again an image attributes classification task. A downside of the classification approach is not able to produce novel questions.</p><p>Our VQG approach was built upon our VQA-Med solution with the following settings.</p><p>-Solving the question generation task as a classification task leads to a total of 2,073 classes each as an unique observed question from the joint training and validation sets. -We were concerned about finetuning the entire image model by the limited amount of data and the large number of class, which may end up over-fitting in a much faster rate, hence we did not choose to fine-tune the backbones. However, as a compensation of non-linear capacity, we added a 2-layer batch-normalized and fully-connected (FC) (512 units each, ReLU activation) multiple-level perceptron (MLP) model before the softmax layer.</p><p>The MLP model also avoided a direct mapping from the image features (e.g., 2048 dimensional features) to the 2,073 classes which would result in a computational expensive matrix multiplication and a large memory usage. -At the training hyper-parameter level, we kept the initial learning rate as 1e-3 but adjusted the final learning rate to 1e-5. Finally, we shortened the number of epochs to 40.</p><p>-Each training image could be associated with more than one question, resulting a multi-label problem. We used the Stochastic Ground Truth method in <ref type="bibr" coords="10,162.98,452.47,15.50,8.74" target="#b15">[16]</ref> which treats each image with multiple observed questions as multiple one-question-for-one-image samples, converting the multi-label problem to a single-label problem. -The multi-scale and multi-architecture ensemble were continued in the VQG approach.</p><p>These settings helped us to reuse most of the VQA-Med code base and models to develop a tangible solution within a very short time frame. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiment Results</head><p>Similar to the VQA-Med 2020 result presentation, we show the VQG-Med 2020 validation and test results separately in Table <ref type="table" coords="11,340.69,151.54,4.98,8.74" target="#tab_5">5</ref> and<ref type="table" coords="11,369.16,151.54,3.87,8.74" target="#tab_6">6</ref>, respectively. While the official evaluation only has BLEU score, in our local evaluation, we used top-7 accuracy to evaluate the validation performance. For official testing, each of our submission generates seven questions according to the highest probabilities for each image. The first two submissions tested whether the large input size models should be continued. Given the lower top-7 accuracy on the validation set and the same BLEU value on the test set, we decided to not continue the 512 input size training. In the third submission, we tried to utilize the ground truth answer annotations by introducing the answer classification as an additional regularization task, but the result dropped by 0.009. In addition, the results from the first three submissions suggested a low correlation between the validation top-7 accuracy and the test BLEU scores. Hence in our forth submission we made two decisions in order to push for a much larger margin on the local evaluation: 1) forgoing the low accuracy models from the ensemble (validation accuracy &lt; 0.079); 2) including the DenseNet-121 architecture given its good performance in the VQA-Med challenge. The fourth submission scored 0.11 for the validation accuracy and 0.348 for the test BLEU score, secured our leading position in the VQG-Med challenge. Finally, in the fifth submission, we further added the DenseNet-161 multi-scale models as a last-minute attempt. Given the local evaluation dropped by 0.012, the test performance drop was expected as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>In this paper, we described our participation at the 2020 VQA-Med challenge and the associated VQG-Med challenge. The center of our approach is a knowledge inference method which we named Skeleton-based Sentence Mapping (SSM). In the VQA-Med challenge, the SSM method was useful on multiple fronts: 1) it mapped questions to a set of backbones which were useful to populate dynamic question instances; 2) it replaced the need of the language modelling and was able to provide the direct selection to the corresponding answer predictor; and 3) it was used to infer six image classification tasks and corresponding training labels. Bypassing the development of language modelling allowed us to focus on tweaking the image classification model so that we devoted more time and resource on the multi-scale and multi-architecture ensemble learning. At last, we developed a class-wise and task-wise normalization technique for balancing the class and task populations, allowing the tasks with incomplete labels to be jointly optimized in one network.</p><p>The main inspiration of SSM came from <ref type="bibr" coords="12,330.42,214.80,14.61,8.74" target="#b16">[17]</ref>, where we back-translated the questions via a number of foreign languages for augmentation purpose, resulting from a group of sentences with a small wording variation; hence a sentence backbone could be inferred. Nevertheless, whether the augmented questions carry the same meaning needs to be manually checked. The idea of reverse-engineering the sentence backbone was extended during our participation at the VQA-Med challenge and led to the proposal of SSM.</p><p>We are aware of the fact that SSM is not fully automated which requires further development. In addition, we understand SSM is a form of explicit reasoning model and its efficiency highly depends on the question regularity and dataset size which may not generalize well for VQA datasets containing free-form questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,134.77,373.06,345.83,7.89;7,134.77,384.04,188.58,7.86;7,135.17,268.85,68.07,68.07"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The schematic of an image classification network we used and the label inference result produced by the proposed SSM method.</figDesc><graphic coords="7,135.17,268.85,68.07,68.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,189.15,345.83,175.55"><head>. 5 Table 1 .</head><label>51</label><figDesc>An example of the question backbones derived from the VQA-Med 2019 and 2020 datasets. The last six columns present the respective number of question instances in each set.</figDesc><table coords="5,135.84,264.62,341.32,100.08"><row><cell>Dataset Questions</cell><cell>Question Backbones</cell><cell cols="5">VQA-Med 2020 VQA-Med 2019 train val test train val test</cell></row><row><cell>is the ct scan normal?</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>3</cell><cell>1</cell></row><row><cell>is the mri normal?</cell><cell>is ${this alts} ${ct alts}</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>6</cell></row><row><cell>is the ultrasound normal?</cell><cell>${normal alts}?</cell><cell>1</cell><cell>1</cell><cell></cell><cell></cell><cell>1</cell></row><row><cell>is the x-ray normal?</cell><cell></cell><cell>2</cell><cell>4</cell><cell>1</cell><cell>3</cell></row><row><cell cols="7">what abnormality is seen in the image? what abnormality is ${imaged alts} 1001 105 127 776 133 20</cell></row><row><cell>what abnormality is seen in this x-ray?</cell><cell>in ${this alts} ${ct alts}?</cell><cell></cell><cell></cell><cell>2</cell><cell></cell></row><row><cell>what is seen in the image?</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell></row><row><cell>what is seen in the x-ray?</cell><cell>what ${is being alts} ${imaged alts}</cell><cell></cell><cell></cell><cell>2</cell><cell></cell></row><row><cell>what is seen in this ct scan?</cell><cell>in ${this alts} ${ct alts}?</cell><cell></cell><cell></cell><cell>1</cell><cell></cell></row><row><cell>what is shown in the x-ray?</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,414.60,345.83,109.94"><head>Table 2 .</head><label>2</label><figDesc>Corresponding candidates for the skeleton variables appeared in Table1. The candidate elements were extracted from the real VQA-Med 2019 and 2020 dataset questions and added with improvised ones.</figDesc><table coords="5,186.55,457.39,234.41,67.15"><row><cell cols="2">Skeleton variables Candidates</cell></row><row><cell>this alts</cell><cell>this, the</cell></row><row><cell>ct alts</cell><cell>ct, ct scan, mri, pet, x-ray, image, . . .</cell></row><row><cell>normal alts</cell><cell>normal, abnormal</cell></row><row><cell>imaged alts</cell><cell>imaged, displayed, seen, shown, . . .</cell></row><row><cell>is being alts</cell><cell>is, is being</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,136.12,115.91,341.82,145.83"><head>Table 3 .</head><label>3</label><figDesc>The accuracy evaluation on the VQA-Med 2020 validation set.</figDesc><table coords="9,136.12,136.65,341.82,125.09"><row><cell>Architecture</cell><cell>128</cell><cell cols="2">Network Input Size 256 384</cell><cell cols="8">Ensemble 512 Multi-scale Multi-scale &amp; Arch.</cell></row><row><cell cols="2">ResNet-50 ResNet-101 ResNet-152 ResNext-50 32x4d 0.510 0.510 0.486 0.486 ResNext-101 32x8d 0.522 DenseNet-121 0.548</cell><cell>0.508 0.530 0.522 0.538 0.520 0.562</cell><cell>0.478 0.508 0.486 0.492 -0.536</cell><cell>0.492 0.460 0.386 0.456 -0.504</cell><cell>0.558 0.566 0.548 0.566 0.538 0.600</cell><cell>0.570</cell><cell>0.580</cell><cell>0.596</cell><cell>0.596</cell><cell>0.590</cell><cell>0.584</cell></row><row><cell>DenseNet-161</cell><cell>0.526</cell><cell>0.520</cell><cell>0.518</cell><cell>-</cell><cell>0.564</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNet v2</cell><cell>0.512</cell><cell>0.512</cell><cell>0.428</cell><cell>-</cell><cell>0.538</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">VGG-16 with BN 0.478</cell><cell>0.482</cell><cell>0.426</cell><cell>0.486</cell><cell>0.530</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">VGG-19 with BN 0.444</cell><cell>0.474</cell><cell>0.442</cell><cell>-</cell><cell>0.502</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,134.77,333.30,345.82,111.16"><head>Table 4 .</head><label>4</label><figDesc>The officially evaluated accuracy and BLEU scores on the VQA-Med 2020 test set. The numbers in the brackets, e.g., 256x2, indicates the use of 256 as the network input size and repeated 2 times (with different initial seeds).</figDesc><table coords="9,136.02,376.32,343.32,68.14"><row><cell>ID</cell><cell>Ensemble Members</cell><cell cols="2">2020-val 2020-test Accu. Accu. BLEU</cell></row><row><cell cols="4">67598 ResNet-50 (256x2, 384) + ResNet-101 (256) + ResNet-152 (256) 0.552 0.446 0.486</cell></row><row><cell>67737</cell><cell>Same as 67598</cell><cell cols="2">0.552 0.442 0.482</cell></row><row><cell cols="4">67915 All Resnets + All ResNexts + All Densenets + All Mobilenet V2 0.596 0.494 0.539</cell></row><row><cell>68012</cell><cell>67915 + extra DenseNet-121 (128x2, 256x2, 384x2, 512)</cell><cell>-</cell><cell>0.496 0.540</cell></row><row><cell>68017</cell><cell>68012 + VGG-16/19</cell><cell>-</cell><cell>0.496 0.542</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,159.50,565.70,293.28,95.31"><head>Table 5 .</head><label>5</label><figDesc>The accuracy evaluation on the VQG-Med 2020 validation set.</figDesc><table coords="10,186.48,586.47,239.34,74.54"><row><cell>Architecture</cell><cell>128</cell><cell cols="2">Network Input Size 256 384</cell><cell>512</cell><cell>Multi-scale Ensemble</cell></row><row><cell>ResNet-50</cell><cell>0.067</cell><cell>0.091</cell><cell>0.098</cell><cell>0.067</cell><cell>0.091</cell></row><row><cell cols="2">ResNet-101 0.055</cell><cell>0.098</cell><cell>0.080</cell><cell>0.061</cell><cell>0.067</cell></row><row><cell cols="2">ResNet-152 0.091</cell><cell>0.067</cell><cell>0.067</cell><cell>0.073</cell><cell>0.067</cell></row><row><cell>DenseNet-121</cell><cell>-</cell><cell>0.085</cell><cell>0.079</cell><cell>-</cell><cell>0.091</cell></row><row><cell>DenseNet-161</cell><cell>-</cell><cell>0.079</cell><cell>0.079</cell><cell>-</cell><cell>0.073</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,134.77,231.12,345.83,99.87"><head>Table 6 .</head><label>6</label><figDesc>The VQG-Med 2020 submitted results. The number in a bracket indicates the network input scale of the respective member model.</figDesc><table coords="11,136.02,262.69,343.32,68.30"><row><cell>ID</cell><cell>Ensemble Members</cell><cell>val Accu. BLEU test</cell></row><row><cell>67984</cell><cell>ResNets-50/101/152 (no 512)</cell><cell>0.085 0.335</cell></row><row><cell>67995</cell><cell>ResNets-50/101/152 (all scales)</cell><cell>0.073 0.335</cell></row><row><cell>67996</cell><cell>67995 + ResNets-50/101/152 (no 512 + answer prediction)</cell><cell>0.091 0.326</cell></row><row><cell cols="3">68006 ResNet-50/101 (256, 384) + ResNet-152 (128) + DenseNet-121 (256, 384) 0.110 0.348</cell></row><row><cell>68018</cell><cell>68006 + DenseNet-161 (256, 384)</cell><cell>0.098 0.338</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="3,144.73,623.92,335.87,7.86;3,144.73,634.88,335.87,7.86;3,144.73,645.84,335.87,7.86;3,144.73,656.80,165.11,7.86"><p>The 2018 VQA-Med challenge employed three measurements: BLEU<ref type="bibr" coords="3,434.30,623.92,13.52,7.86" target="#b21">[22]</ref>, Wordbased Semantic Similarity (WBSS)<ref type="bibr" coords="3,295.31,634.88,13.52,7.86" target="#b32">[33]</ref>, and Concept-based Semantic Similarity (CBSS). The leading teams are referred to the BLEU and WBSS<ref type="bibr" coords="3,404.52,645.84,14.34,7.86" target="#b32">[33]</ref> score rankings. The CBSS can result a different ranking.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="4,144.73,656.80,182.05,7.86"><p>https://github.com/abachaa/VQA-Med-2019</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="5,144.73,634.88,335.87,7.86;5,144.73,645.84,335.87,7.86;5,144.73,656.80,73.04,7.86"><p>The naming was determined by choosing the a representative candidate from candidates for each skeleton variable; by ignoring the "alts" suffix, a question backbone becomes readable.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,403.59,337.64,7.86;12,151.52,414.55,329.07,7.86;12,151.52,425.51,95.81,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,463.44,403.59,17.15,7.86;12,151.52,414.55,279.76,7.86">Nlm at imageclef 2018 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,455.89,414.55,24.70,7.86;12,151.52,425.51,67.14,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,436.63,337.63,7.86;12,151.52,447.59,329.07,7.86;12,151.52,458.55,161.72,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,166.36,447.59,314.24,7.86">Vqa-med: Overview of the medical visual question answering task at imageclef</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,189.66,458.55,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,469.67,337.64,7.86;12,151.52,480.63,329.07,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,265.25,469.67,215.34,7.86;12,151.52,480.63,188.38,7.86">Deep neural networks and decision tree classifier for visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Allaouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,359.80,480.63,93.05,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,491.76,337.64,7.86;12,151.52,502.72,329.07,7.86;12,151.52,513.67,213.60,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,168.06,502.72,129.13,7.86">Vqa: Visual question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,321.17,502.72,159.42,7.86;12,151.52,513.67,120.81,7.86">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,524.80,337.63,7.86;12,151.52,535.76,329.07,7.86;12,151.52,546.72,329.07,7.86;12,151.52,557.67,322.28,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,151.52,535.76,329.07,7.86;12,151.52,546.72,134.88,7.86">Overview of the vqa-med task at imageclef 2020: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,309.01,546.72,171.58,7.86;12,151.52,557.67,132.29,7.86">CLEF 2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,568.80,337.64,7.86;12,151.52,579.76,329.07,7.86;12,151.52,590.72,213.60,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,370.70,568.80,109.90,7.86;12,151.52,579.76,146.71,7.86">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,321.87,579.76,158.72,7.86;12,151.52,590.72,120.81,7.86">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2612" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,601.84,337.63,7.86;12,151.52,612.80,329.07,7.86;12,151.52,623.76,281.85,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,219.48,612.80,261.11,7.86;12,151.52,623.76,120.48,7.86">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.96,634.88,337.63,7.86;12,151.52,645.84,329.07,7.86;12,151.52,656.80,25.60,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,346.99,634.88,133.60,7.86;12,151.52,645.84,188.12,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.96,119.67,337.64,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,245.31,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,395.92,119.67,84.67,7.86;13,151.52,130.63,257.10,7.86">Are you talking to a machine? dataset and methods for multilingual image question</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,431.19,130.63,49.40,7.86;13,151.52,141.59,152.01,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2296" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,152.56,337.97,7.86;13,151.52,163.52,329.07,7.86;13,151.52,174.48,55.09,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,432.03,152.56,48.56,7.86;13,151.52,163.52,244.73,7.86">Overview of imageclef 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,415.87,163.52,64.72,7.86;13,151.52,174.48,26.42,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,185.45,337.97,7.86;13,151.52,196.41,329.07,7.86;13,151.52,207.37,76.80,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,290.95,185.45,172.55,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,151.52,196.41,325.14,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,218.34,337.98,7.86;13,151.52,229.27,92.85,7.89" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,288.76,218.34,100.72,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,398.70,218.34,81.90,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,240.26,337.98,7.86;13,151.52,251.22,329.07,7.86;13,151.52,262.18,268.97,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="13,253.25,251.22,227.35,7.86;13,151.52,262.18,102.95,7.86">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.62,273.15,337.98,7.86;13,151.52,284.11,329.07,7.86;13,151.52,295.07,186.91,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,405.15,273.15,75.45,7.86;13,151.52,284.11,89.79,7.86">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,261.20,284.11,219.39,7.86;13,151.52,295.07,93.91,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,306.04,337.98,7.86;13,151.52,317.00,329.07,7.86;13,151.52,327.96,329.07,7.86;13,151.52,338.92,329.07,7.86;13,151.52,349.87,329.07,7.86;13,151.52,360.83,329.07,7.86;13,151.52,371.79,329.07,7.86;13,151.52,382.75,329.07,7.86;13,151.52,393.71,329.07,7.86;13,151.52,404.67,329.07,7.86;13,151.52,415.63,34.31,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,295.04,360.83,185.55,7.86;13,151.52,371.79,255.37,7.86">Overview of the ImageCLEF 2020: Multimedia retrieval in medical, lifelogging, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,426.43,371.79,54.16,7.86;13,151.52,382.75,329.07,7.86;13,151.52,393.71,253.34,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="13,456.14,393.71,24.45,7.86;13,151.52,404.67,138.63,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="13,142.62,426.60,337.98,7.86;13,151.52,437.56,329.07,7.86;13,151.52,448.52,329.07,7.86;13,151.52,459.45,329.07,7.89;13,151.52,470.43,25.60,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,289.91,437.56,190.68,7.86;13,151.52,448.52,329.07,7.86;13,151.52,459.47,90.56,7.86">On modelling label uncertainty in deep neural networks: Automatic estimation of intra-observer variability in 2d echocardiography quality assessment</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Girgis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Vaseli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rohling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Abolmaesumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,249.24,459.47,158.85,7.86">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1868" to="1883" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,481.40,337.97,7.86;13,151.52,492.36,329.07,7.86;13,151.52,503.32,326.30,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,151.52,492.36,218.77,7.86">Medical data inquiry using a question answering model</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verjans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,412.48,492.36,68.11,7.86;13,151.52,503.32,205.44,7.86">IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1490" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,514.29,337.98,7.86;13,151.52,525.25,108.03,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,258.78,514.29,76.15,7.86">Network in network</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,341.45,514.29,139.15,7.86;13,151.52,525.25,79.36,7.86">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,536.22,337.98,7.86;13,151.52,547.18,329.07,7.86;13,151.52,558.14,76.80,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,305.72,536.22,174.87,7.86;13,151.52,547.18,100.83,7.86">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,273.42,547.18,202.96,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="289" to="297" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,569.11,337.97,7.86;13,151.52,580.06,329.07,7.86;13,151.52,591.02,212.06,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,324.84,569.11,155.75,7.86;13,151.52,580.06,174.95,7.86">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,347.36,580.06,133.22,7.86;13,151.52,591.02,146.92,7.86">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,601.99,337.97,7.86;13,151.52,612.95,329.07,7.86;13,151.52,623.91,217.12,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,407.41,601.99,73.17,7.86;13,151.52,612.95,232.55,7.86">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,404.48,612.95,76.12,7.86;13,151.52,623.91,123.83,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="3111" to="3119" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,634.88,337.98,7.86;13,151.52,645.84,329.07,7.86;13,151.52,656.80,268.37,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="13,355.50,634.88,125.09,7.86;13,151.52,645.84,134.32,7.86">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,307.60,645.84,172.99,7.86;13,151.52,656.80,184.68,7.86">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,119.67,337.98,7.86;14,151.52,130.63,259.13,7.86" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Rosen</surname></persName>
		</author>
		<title level="m" coord="14,284.94,119.67,195.65,7.86;14,151.52,130.63,114.54,7.86;14,287.06,130.63,94.91,7.86">Umass at imageclef medical visual question answering (med-vqa) 2018 task</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>CLEF (Working Notes)</note>
</biblStruct>

<biblStruct coords="14,142.62,141.59,337.98,7.86;14,151.52,152.55,329.07,7.86;14,151.52,163.51,25.60,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,285.30,141.59,195.29,7.86;14,151.52,152.55,38.08,7.86">Exploring models and data for image question answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,211.26,152.55,204.44,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2953" to="2961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,174.47,337.98,7.86;14,151.52,185.43,183.04,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="14,268.26,174.47,212.34,7.86;14,151.52,185.43,38.08,7.86">Deep multimodal learning for medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,210.98,185.43,94.92,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,196.39,337.97,7.86;14,151.52,207.34,231.27,7.86" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="14,278.92,196.39,201.67,7.86;14,151.52,207.34,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.62,218.30,337.98,7.86;14,151.52,229.26,329.07,7.86;14,151.52,240.22,323.18,7.86" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="14,214.13,229.26,141.81,7.86">Towards vqa models that can read</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,378.63,229.26,101.96,7.86;14,151.52,240.22,229.91,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8317" to="8326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,251.18,337.97,7.86;14,151.52,262.14,329.07,7.86;14,151.52,273.10,143.13,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="14,360.37,251.18,120.22,7.86;14,151.52,262.14,204.41,7.86">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,377.39,262.14,103.21,7.86;14,151.52,273.10,114.46,7.86">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,284.06,337.98,7.86;14,151.52,295.02,95.81,7.86" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="14,272.62,284.06,159.38,7.86">Just at vqa-med: A vgg-seq2seq model</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Talafha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,455.89,284.06,24.70,7.86;14,151.52,295.02,67.14,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,305.98,337.98,7.86;14,151.52,316.93,329.07,7.86;14,151.52,327.89,165.75,7.86" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="14,348.92,305.98,131.67,7.86;14,151.52,316.93,329.07,7.86;14,151.52,327.89,27.65,7.86">Ensemble of streamlined bilinear visual question answering models for the imageclef 2019 challenge in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nyholm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Löfstedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,200.92,327.89,44.57,7.86">CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,338.85,337.98,7.86;14,151.52,349.81,329.07,7.86;14,151.52,360.77,327.94,7.86" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="14,387.90,338.85,92.70,7.86;14,151.52,349.81,234.24,7.86">What value do explicit high level concepts have in vision to language problems?</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,404.53,349.81,76.06,7.86;14,151.52,360.77,244.14,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,371.73,337.97,7.86;14,151.52,382.69,329.07,7.86;14,151.52,393.65,329.07,7.86;14,151.52,404.61,86.01,7.86" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="14,407.19,371.73,73.40,7.86;14,151.52,382.69,311.53,7.86">Ask me anything: Free-form visual question answering based on knowledge from external sources</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,151.52,393.65,325.14,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4622" to="4630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,415.56,337.98,7.86;14,151.52,426.52,72.70,7.86" xml:id="b32">
	<monogr>
		<title level="m" type="main" coord="14,240.56,415.56,148.36,7.86">Verb semantics and lexical selection</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<idno>arXiv preprint cmp- lg/9406033</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,437.48,337.98,7.86;14,151.52,448.44,329.07,7.86;14,151.52,459.40,213.33,7.86" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="14,335.74,437.48,144.85,7.86;14,151.52,448.44,100.94,7.86">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,275.97,448.44,204.62,7.86;14,151.52,459.40,120.32,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,470.36,337.97,7.86;14,151.52,481.32,329.07,7.86;14,151.52,492.28,147.29,7.86" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="14,237.63,470.36,242.96,7.86;14,151.52,481.32,159.48,7.86">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,334.66,481.32,145.93,7.86;14,151.52,492.28,23.96,7.86">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="451" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,503.24,337.97,7.86;14,151.52,514.19,312.89,7.86" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="14,309.14,503.24,171.45,7.86;14,151.52,514.19,167.55,7.86">Zhejiang university at imageclef 2019 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,340.83,514.19,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,525.15,337.97,7.86;14,151.52,536.11,329.07,7.86;14,151.52,547.07,194.90,7.86" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="14,354.07,525.15,126.52,7.86;14,151.52,536.11,102.98,7.86">Stacked attention networks for image question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,277.49,536.11,203.10,7.86;14,151.52,547.07,120.32,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,558.03,337.97,7.86;14,151.52,568.99,329.07,7.86;14,151.52,579.95,260.72,7.86" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="14,284.45,558.03,196.14,7.86;14,151.52,568.99,192.07,7.86">Multi-modal factorized bilinear pooling with coattention learning for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,365.69,568.99,114.91,7.86;14,151.52,579.95,167.94,7.86">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1821" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,590.91,337.97,7.86;14,151.52,601.87,278.89,7.86" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="14,265.28,590.91,215.31,7.86;14,151.52,601.87,133.93,7.86">Employing inception-resnet-v2 and bi-lstm for medical domain visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,306.83,601.87,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,612.82,337.98,7.86;14,151.52,623.78,320.64,7.86" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="14,269.52,612.82,211.07,7.86;14,151.52,623.78,176.03,7.86">Tua1 at imageclef 2019 vqa-med: a classification and generation model based on transfer learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,348.58,623.78,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
