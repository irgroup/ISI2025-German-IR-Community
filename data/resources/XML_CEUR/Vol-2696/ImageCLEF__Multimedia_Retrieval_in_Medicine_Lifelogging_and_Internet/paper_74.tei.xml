<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.89,115.96,329.58,12.62;1,152.59,133.89,310.18,12.62;1,270.58,151.82,74.21,12.62">HCP-MIC at VQA-Med 2020: Effective Visual Representation for Medical Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,208.08,189.49,55.58,8.74"><forename type="first">Guanqi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.08,189.49,52.97,8.74"><forename type="first">Haifan</forename><surname>Gong</surname></persName>
							<email>haifangong@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.95,189.49,49.75,8.74"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
							<email>liguanbin@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.89,115.96,329.58,12.62;1,152.59,133.89,310.18,12.62;1,270.58,151.82,74.21,12.62">HCP-MIC at VQA-Med 2020: Effective Visual Representation for Medical Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DB02392C90E2883B80362FC8CD95A747</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our submission for the Medical Domain Visual Question Answering Task of ImageCLEF 2020. We desert complex cross-modal fusion strategies and concentrate on how to capture the effective visual representation, due to the information inequality between images and questions in this task. Based on the observation of long-tailed distribution in the training set, we utilize the bilateral-branch network with a cumulative learning strategy to tackle this issue. Besides, to alleviate the issue of limited training data, we design an approach to extend the training set by Kullback-Leibler divergence. Our proposed method achieved the score with 0.426 in accuracy and 0.462 in BLEU, which ranked 4th in the competition. Our code is publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual Question Answering (VQA) aims at answering questions according to the content of corresponding images. In recent years, researchers have made great progress in the VQA task with many effective methods and large-scale datasets. With the purpose of supporting clinical decision making and improving patient engagement, the VQA task is introduced into the medical field. To promote the development of medical VQA, ImageCLEF <ref type="bibr" coords="1,342.42,498.26,15.50,8.74" target="#b9">[10]</ref> organizes 3rd edition of the Medical Domain Visual Question Answering Task <ref type="bibr" coords="1,351.88,510.22,10.52,8.74">[3]</ref> (see examples in Figure <ref type="figure" coords="1,468.97,510.22,3.87,8.74">1</ref>). Compared to general VQA, the valid medical data for training is limited in ImageCLEF 2020 VQA-Med task. Besides, it focuses particularly on questions about abnormalities, which is different from previous editions of the VQA-Med task. We argue that the semantic information from questions is finite due to the single theme of the ImageCLEF 2020 VQA-Med task. However, there are many kinds of abnormal medical images, which need effective visual representation to distinguish them. In this paper, we describe the method we developed to deal with the above concerns. Based on the observation of the questions, we divide them into three groups, and utilize a pre-trained BioBERT <ref type="bibr" coords="2,328.10,560.48,15.50,8.74" target="#b11">[12]</ref> to classify them. As for visual representation, we map abnormalities to medical images, and discover the phenomenon of the long-tailed distribution in the training set (as shown in Figure <ref type="figure" coords="2,151.93,596.34,3.87,8.74">2</ref>). Thus, we apply the bilateral-branch network with a cumulative learning strategy <ref type="bibr" coords="2,174.08,608.30,15.50,8.74" target="#b18">[19]</ref> to obtain effective visual representation. In addition, we propose a retrieval-based candidate answer selection algorithm to further improve the performance. Last but not least, to alleviate the issue of limited training data, we design an approach to expand the training set by Kullback-Leibler (KL) divergence.</p><p>The common framework for VQA systems is composed of four parts: an image encoder, a question encoder, a cross-modal fusion strategy, and an answer predictor. Many researchers highlight and explore the cross-modal fusion strategy for a better combination of visual and linguistic information. Some works <ref type="bibr" coords="3,427.65,186.72,11.92,8.74" target="#b5">[6,</ref><ref type="bibr" coords="3,439.57,186.72,11.92,8.74" target="#b10">11]</ref> utilize compact bilinear pooling methods to capture the joint representation between images and questions. Yang et al. <ref type="bibr" coords="3,289.05,210.63,14.61,8.74" target="#b15">[16]</ref>, <ref type="bibr" coords="3,311.46,210.63,58.37,8.74">Cao et al. [4]</ref>, and Anderson et al. <ref type="bibr" coords="3,470.07,210.63,10.52,8.74" target="#b1">[2]</ref> exploited the question information to attend the corresponding sub-region of the image. <ref type="bibr" coords="3,167.10,234.54,15.50,8.74" target="#b16">[17]</ref> proposed a co-attention mechanism between images and questions to obtain better multi-modal alignment and representation. However, based on the observation that the ImageCLEF 2020 VQA-Med task focuses particularly on questions about abnormalities, we argue that the abnormalities rely on the information from images rather than questions. Thus, we desert the complex cross-modal fusion strategy due to the information inequality between images and questions. And we concentrate on how to obtain an effective visual representation.</p><p>As for visual representation in VQA systems, the bottom-up feature representation <ref type="bibr" coords="3,177.60,343.73,10.52,8.74" target="#b1">[2]</ref> based on deep CNNs is adopted by many works. <ref type="bibr" coords="3,404.77,343.73,10.52,8.74" target="#b1">[2]</ref> utilized Faster R-CNN <ref type="bibr" coords="3,171.83,355.68,15.50,8.74" target="#b14">[15]</ref> to capture region-specific features in a bottom-up attention way, which boosted the performance of VQA and image captioning tasks. However, since not all radiology images contain object-level annotations, medical VQA systems usually apply a CNN to extract grid-like feature maps as visual representations. In the ImageCLEF 2020 VQA-Med task, we discover that there exists a long-tailed distribution phenomenon in the training set. Therefore, we adopt the bilateral-branch network with a cumulative learning strategy to obtain effective visual representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>In ImageCLEF 2020 VQA-Med task, the dataset includes a training set of 4000 radiology images with 4000 question-answer (QA) pairs, a validation set of 500 radiology images with 500 QA pairs, and a test set of 500 radiology images with 500 QA pairs. The questions mainly focus on the abnormalities of medical images, and they can be divided into two forms. One is making inquiries about the existence of abnormalities in the picture, and another is making inquiries about the abnormal type. Figure <ref type="figure" coords="3,252.85,582.80,4.98,8.74">1</ref> shows three examples in the VQA-Med dataset.</p><p>The VQA-Med-2019 dataset <ref type="bibr" coords="3,281.49,596.34,10.52,8.74" target="#b0">[1]</ref> can be used as additional training data, whose training set contains 3200 medical images associated with 12792 QA pairs. However, different from the VQA-Med-2020 dataset, it focuses on four main categories of questions: Modality, Plane, Organ system, and Abnormality. In this paper, we only leverage its Abnormality subset to extend the VQA-Med-2020 training set . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>As shown in Figure <ref type="figure" coords="4,220.74,292.37,3.87,8.74" target="#fig_1">3</ref>, our medical VQA framework consists of three parts: question semantic classification, vision-based candidate answer classification, and retrieval-based candidate answer selection. We train the first two parts separately and then connect all the components to predict the final answer in the inference phase. Besides, we design a distribution-based algorithm to expand the training set for further improving the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Question semantic classification</head><p>According to different answer forms, we divide all the questions into two categories: open-ended questions (e.g., Figure <ref type="figure" coords="4,314.26,414.44,15.87,8.74">1(c)</ref>), and closed-ended questions (e.g., Figure <ref type="figure" coords="4,165.66,426.40,4.18,8.74">1</ref> In all, we need to classify the question sentence into three categories: open-ended questions, closed-ended abnormal questions, and closed-ended normal questions.</p><p>For question semantic classification, a pre-trained BioBERT is adopted to classify the questions. Unlike the conventional BERT <ref type="bibr" coords="4,380.12,510.14,9.96,8.74" target="#b4">[5]</ref>, the BioBERT is a domain-specific language representation model pre-trained on large-scale biomedical corpora. Based on BioBert, we send the 768-dimensional vector, the output of BioBERT, into a 2-layer MLP to obtain the classification score of the input question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Vision-based candidate answer classification</head><p>In this part, we need to classify the answer according to the radiology image and the category of question. For the closed-ended questions, we apply the ResNet-34 <ref type="bibr" coords="4,147.19,632.21,10.52,8.74" target="#b6">[7]</ref> to distinguish normal and abnormal medical images. Then, combining with the fine-grained category information of closed-ended questions, we can simply choose the answer from "yes" or "no".</p><p>As for the open-ended questions, we need to predict the specific abnormalities of the input images in a classification way. Due to the long-tailed distribution among the candidate answers in the training set, we apply the bilateral-branch network (BBN) with a cumulative learning strategy to deal with this problem. In BBN, there are two branches, one is called "conventional learning branch", and another is called "re-balancing branch". The conventional learning branch is for representation learning while the re-balancing is for classifier learning. In the meanwhile, a novel cumulative learning strategy is proposed for adjusting bilateral learning. It is worth noting that, inspired by the attention mechanism, many advanced residual networks have been proposed, such as SE-Net <ref type="bibr" coords="5,447.21,226.59,9.96,8.74" target="#b8">[9]</ref>, SK-Net <ref type="bibr" coords="5,154.61,238.55,14.61,8.74" target="#b12">[13]</ref>, NLCE-Net <ref type="bibr" coords="5,227.80,238.55,9.96,8.74" target="#b7">[8]</ref>. In this work, we replace the original ResNet in BBN with ResNeSt <ref type="bibr" coords="5,197.78,250.50,14.61,8.74" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Retrieval-based candidate answer selection</head><p>As for the open-ended questions, we discover that the top-5 score is about 10% higher than the top-1 score for the open-ended questions the training procedure. To alleviate this issue, we apply the retrieval-based top-5 answer selection to further improve the performance. The schedule is designed into three steps. The first step is to create a feature dictionary of each class based on the training set. It is worth noting that those features are extracted from the BBN. The second one is to calculate the feature-level cosine similarity between the input sample and all the training samples belong to the top-5 categories. Then, we treat the answer of the most similar training sample as the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Expanding the training set by Kullback-Leibler divergence</head><p>Since the valid medical data for training is limited in ImageCLEF 2020 VQA-Med task and external datasets are allowed to use, we expand the training set with the data from the VQA-Med-2019 dataset. Before extending the training set, we define the distribution of the VQA-Med-2020 training set as P tr , which is obtained by:</p><formula xml:id="formula_0" coords="5,274.61,489.78,205.98,26.56">P tr = n k C j=1 n j<label>(1)</label></formula><p>where k and j are the indexes of category, C denotes the number of catergories, and n represents the number of samples with same category. And we exploit the same way to calculate the distribution of the validation set P v . The KL divergence between P tr and P v is defined as:  5 Experiments</p><formula xml:id="formula_1" coords="5,229.28,574.15,251.31,26.88">D KL (P v ||P tr ) = k P v (k) log P v (k) P tr (k)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation details</head><p>As for training data, we leverage the whole VQA-Med-2020 training set with 4000 questions to train the BioBERT for question semantic classification. We leverage the extended dataset to train the vision-based model. Among them, 303 images are used to train a ResNet-34 to determine whether the images are abnormal or not, and 4039 images are used to train a BBN to recognize the abnormalities. Besides, a center cropping operation is applied to the input image. We train those models which are mentioned above separately with corresponding cross-entropy losses. And the optimizer we used is SGD with momentum which is set to 0.9. The initial learning rate is set to 0.08, and the weight decay is 4e-4. We select the best model based on the performance on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation</head><p>The VQA-Med competition uses accuracy and BLEU <ref type="bibr" coords="6,366.95,538.57,15.50,8.74">[14]</ref> as the evaluation metrics. Accuracy is calculated as the number of correct predicted answers over total answers. BLEU measures the similarity between the predicted answers and ground truth answers. As shown in Table <ref type="table" coords="6,322.21,574.43,3.87,8.74" target="#tab_0">1</ref>, we achieved an accuracy of 0.426 and a BLEU score of 0.462 in the VQA-Med-2020 test set, which won the 4th place in the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Abaltion study</head><p>In this section, we study some contributions of our proposed method on the VQA-Med-2020 validation set, which is shown in Table <ref type="table" coords="6,351.29,656.12,3.87,8.74" target="#tab_1">2</ref>. The baseline represents the method that contains a BioBERT for question semantic classification and two ResNet-34 models for vision-based candidate answer classification. And we train the baseline with the original VQA-Med-2020 training set.</p><p>Firstly, we replace a ResNet-34 with a BBN-ResNet-34 to better recognize the abnormalities, which surpasses the baseline by 14.4%. We expand the training set by KL divergence, which brings an improvement of 3.0%. The performance is further boosted by 1.0%, using a powerful ResNeSt-50 backbone. Then, we apply a center cropping operation to the input image for reducing noise, which leads to 1.6% improvement. The strategy of retrieval-based candidate answer selection brings a performance gain of 0.6%. Finally, we achieve 57.2% accuracy on the VQA-Med-2020 validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we describe the method we submitted in ImageCLEF 2020 VQA-Med task. Considering the information inequality between images and questions in this task, we desert complex cross-modal fusion strategies. We adopt the bilateral-branch network with a cumulative learning strategy to handle the longtailed problem for effective visual representation. Besides, to alleviate the issue of limited training data, we design an approach to extend the training set by Kullback-Leibler divergence. In addition, we propose a retrieval-based candidate answer selection module to further boost the performance. Our proposed method achieves great results with an accuracy of 0.426 and a BLEU score of 0.462.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,268.99,345.82,7.89;2,134.77,279.98,155.84,7.86;2,147.18,116.01,93.70,93.70"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Three examples of image and corresponding question-answer pair in the Im-ageCLEF 2020 VQA-Med training set.</figDesc><graphic coords="2,147.18,116.01,93.70,93.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,187.81,234.36,239.74,7.89;4,141.11,154.97,84.92,58.17"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of the proposed medical VQA framework.</figDesc><graphic coords="4,141.11,154.97,84.92,58.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,169.84,426.40,310.74,8.74;4,134.77,438.35,345.83,8.74;4,134.77,450.31,345.83,8.74;4,134.77,462.26,345.82,8.74"><head></head><label></label><figDesc>(a)(b)). Based on different semantic information, the closed-ended questions can be further separated into two classes: closed-ended abnormal questions (e.g., Figure1(b)) representing whether the image is abnormal, and closed-ended normal questions (e.g., Figure1(a)) denoting whether the image looks normal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,134.77,608.30,345.83,8.74;5,134.77,620.25,345.83,8.74;5,134.77,632.21,345.82,8.74;5,134.77,641.64,345.82,12.17;5,134.77,653.60,345.83,12.17"><head></head><label></label><figDesc>Then we expand the training set by the following steps. For each sample in the the Abnormality training subset of the VQA-Med-2019 dataset, we assume that it is added to the VQA-Med-2020 training set. Then, we calculate the distribution of new training set Ptr and the KL divergence D KL (P v || Ptr ). Lastly, we extend the training set with the sample if D KL (P v || Ptr ) is lower than D KL (P v ||P tr ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,175.16,115.91,265.03,83.85"><head>Table 1 .</head><label>1</label><figDesc>Official results of the ImageCLEF 2020 VQA-Med task.</figDesc><table coords="6,210.11,136.71,198.34,63.06"><row><cell>Teams</cell><cell>Accuracy</cell><cell>BLEU</cell></row><row><cell>z liao</cell><cell>0.496</cell><cell>0.542</cell></row><row><cell>TheInceptionTeam</cell><cell>0.480</cell><cell>0.511</cell></row><row><cell>bumjun jung</cell><cell>0.466</cell><cell>0.502</cell></row><row><cell>Ours</cell><cell>0.426</cell><cell>0.462</cell></row><row><cell>NLM</cell><cell>0.400</cell><cell>0.441</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,156.86,212.75,312.71,94.81"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on the VQA-Med-2020 validation set.</figDesc><table coords="6,156.86,233.55,312.71,74.02"><row><cell>Methods</cell><cell>Accuracy</cell><cell>Boost</cell></row><row><cell>Baseline</cell><cell>36.6%</cell><cell>-</cell></row><row><cell>+BBN-ResNet-34</cell><cell>51.0%</cell><cell>+14.4%</cell></row><row><cell>+Training Set Expansion by KL Divergence</cell><cell>54.0%</cell><cell>+3.0%</cell></row><row><cell>+BBN-ResNeSt-50</cell><cell>55.0%</cell><cell>+1.0%</cell></row><row><cell>+Image Center Cropping</cell><cell>56.6%</cell><cell>+1.6%</cell></row><row><cell>+Retrieval-based Candidate Answer Selection</cell><cell>57.2%</cell><cell>+0.6%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.96,447.98,337.64,7.86;7,151.52,458.94,329.07,7.86;7,151.52,469.90,100.20,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,151.52,458.94,329.07,7.86">Vqa-med: Overview of the medical visual question answering task at imageclef</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,189.66,469.90,24.70,7.86">CLEF</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,480.97,337.64,7.86;7,151.52,491.93,329.07,7.86;7,151.52,502.89,329.07,7.86;7,151.52,513.85,136.20,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,166.09,491.93,314.50,7.86;7,151.52,502.89,38.08,7.86">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,209.93,502.89,270.67,7.86;7,151.52,513.85,43.20,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,524.93,337.63,7.86;7,151.52,535.89,329.07,7.86;7,151.52,546.85,329.07,7.86;7,151.52,557.81,329.07,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,151.52,535.89,329.07,7.86;7,151.52,546.85,134.88,7.86">Overview of the vqa-med task at imageclef 2020: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,309.01,546.85,171.58,7.86;7,151.52,557.81,134.24,7.86">CLEF 2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,579.85,337.63,7.86;7,151.52,590.80,329.07,7.86;7,151.52,601.76,119.92,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,319.33,579.85,161.26,7.86;7,151.52,590.80,55.12,7.86">Visual question reasoning on general dependency tree</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,226.98,590.80,253.61,7.86;7,151.52,601.76,60.41,7.86">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,612.84,337.63,7.86;7,151.52,623.80,320.25,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,354.85,612.84,125.74,7.86;7,151.52,623.80,199.77,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,372.72,623.80,54.52,7.86">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,634.88,337.64,7.86;7,151.52,645.84,329.07,7.86;7,151.52,656.80,167.75,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,455.25,634.88,25.34,7.86;7,151.52,645.84,324.77,7.86">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.96,119.67,337.63,7.86;8,151.52,130.63,329.07,7.86;8,151.52,141.59,92.66,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,290.95,119.67,172.55,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,151.52,130.63,325.14,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<biblScope unit="page" from="4" to="6" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,152.55,337.64,7.86;8,151.52,163.51,329.07,7.86;8,151.52,174.47,11.78,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,356.19,152.55,124.40,7.86;8,151.52,163.51,255.26,7.86">Non-local context encoder: Robust biomedical image segmentation against adversarial attacks</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,427.85,163.51,24.06,7.86">AAAI</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,185.43,337.64,7.86;8,151.52,196.39,291.65,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,259.30,185.43,129.50,7.86">Squeeze-and-excitation networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,410.78,185.43,69.82,7.86;8,151.52,196.39,182.52,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,207.34,337.98,7.86;8,151.52,218.30,329.07,7.86;8,151.52,229.26,329.07,7.86;8,151.52,240.22,329.07,7.86;8,151.52,251.18,329.07,7.86;8,151.52,262.14,329.07,7.86;8,151.52,273.10,329.07,7.86;8,151.52,284.06,329.07,7.86;8,151.52,295.02,329.07,7.86;8,151.52,305.98,329.07,7.86;8,151.52,316.93,42.99,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,295.04,262.14,185.55,7.86;8,151.52,273.10,255.37,7.86">Overview of the ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,426.43,273.10,54.16,7.86;8,151.52,284.06,329.07,7.86;8,151.52,295.02,253.34,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="8,456.14,295.02,24.45,7.86;8,151.52,305.98,138.63,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,142.62,327.89,337.98,7.86;8,151.52,338.85,243.06,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,282.51,327.89,107.48,7.86">Bilinear attention networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,410.51,327.89,70.09,7.86;8,151.52,338.85,140.89,7.86">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,349.81,337.98,7.86;8,151.52,360.77,329.07,7.86;8,151.52,371.73,301.88,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,430.36,349.81,50.23,7.86;8,151.52,360.77,324.76,7.86">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btz6821" />
	</analytic>
	<monogr>
		<title level="j" coord="8,151.52,371.73,58.56,7.86">Bioinformatics</title>
		<imprint>
			<date type="published" when="2019-09">09 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,382.69,337.98,7.86;8,151.52,393.65,228.68,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,293.74,382.69,97.79,7.86">Selective kernel networks</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,411.69,382.69,68.91,7.86;8,151.52,393.65,184.15,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,404.61,337.98,7.86;8,151.52,415.56,329.07,7.86;8,151.52,426.52,293.06,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,338.48,404.61,142.12,7.86;8,151.52,415.56,117.78,7.86">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,290.44,415.56,190.15,7.86;8,151.52,426.52,168.81,7.86">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,437.48,337.98,7.86;8,151.52,448.44,329.07,7.86;8,151.52,459.40,113.03,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,301.55,437.48,179.05,7.86;8,151.52,448.44,137.84,7.86">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,310.29,448.44,170.29,7.86;8,151.52,459.40,29.48,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,470.36,337.97,7.86;8,151.52,481.32,329.07,7.86;8,151.52,492.28,203.60,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,354.07,470.36,126.52,7.86;8,151.52,481.32,102.98,7.86">Stacked attention networks for image question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,277.49,481.32,203.10,7.86;8,151.52,492.28,120.32,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,503.24,337.98,7.86;8,151.52,514.19,329.07,7.86;8,151.52,525.15,177.80,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,331.02,503.24,149.57,7.86;8,151.52,514.19,115.60,7.86">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,288.72,514.19,191.87,7.86;8,151.52,525.15,118.29,7.86">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,536.11,337.98,7.86;8,151.52,547.07,329.07,7.86;8,151.52,558.00,115.77,7.89" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="8,314.94,547.07,133.08,7.86">Resnest: Split-attention networks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno>4.2</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,568.99,337.97,7.86;8,151.52,579.95,329.07,7.86;8,151.52,590.91,280.52,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,322.01,568.99,158.58,7.86;8,151.52,579.95,198.42,7.86">Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,369.47,579.95,111.12,7.86;8,151.52,590.91,221.01,7.86">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
