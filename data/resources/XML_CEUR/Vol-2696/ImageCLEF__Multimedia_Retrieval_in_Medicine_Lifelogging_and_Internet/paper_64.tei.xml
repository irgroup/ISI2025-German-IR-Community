<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,169.85,115.96,275.65,12.62;1,141.52,133.89,332.31,12.62;1,135.50,151.82,344.35,12.62;1,269.90,169.76,75.57,12.62">Ensemble of Deep Learning Models for Automatic Tuberculosis Diagnosis Using Chest CT Scans: Contribution to the ImageCLEF-2020 Challenges</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.17,207.98,74.83,8.74"><forename type="first">Abdela</forename><forename type="middle">A</forename><surname>Mossa</surname></persName>
							<email>amossa@student.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">C ¸ukurova University</orgName>
								<address>
									<settlement>Adana</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.78,207.98,42.56,8.74"><forename type="first">Halit</forename><surname>Eriş</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical-Electronics Engineering</orgName>
								<orgName type="institution">C ¸ukurova University</orgName>
								<address>
									<settlement>Adana</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.86,219.93,47.93,8.74"><forename type="first">Ulus</forename><forename type="middle">C</forename><surname>¸evik</surname></persName>
							<email>ucevik@cu.edu.tr</email>
						</author>
						<title level="a" type="main" coord="1,169.85,115.96,275.65,12.62;1,141.52,133.89,332.31,12.62;1,135.50,151.82,344.35,12.62;1,269.90,169.76,75.57,12.62">Ensemble of Deep Learning Models for Automatic Tuberculosis Diagnosis Using Chest CT Scans: Contribution to the ImageCLEF-2020 Challenges</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1F7605BE224F4AC407A22A9F95B002E3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Automatic CT Report Generation</term>
					<term>Deep Learning</term>
					<term>Convolutional Neural Network</term>
					<term>Tuberculosis Diagnosis</term>
					<term>3D Medical Image Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tuberculosis (TB) is a bacterial infection that mainly affects the lungs. It is a potentially serious disease killing around 2 million people a year. Nevertheless, it can be cured if treated with the right antibiotics. However, manual diagnosing of TB can be difficult, and several tests are usually conducted by clinicians. Consequently, automated diagnosis of TB based on chest Computed Tomography (CT) images for rapid and accurate diagnosis are currently of great interest. Recently, deep learning algorithms, and in particular convolutional neural network (CNN), due to the ability to learn low-and high-level discriminative features directly from images in an end-to-end architecture, have been shown to be the state-of-the-art in automatic medical image analysis. In this work, we developed a deep learning model for automated TB diagnosis using an ensemble of different CNN architectures trained on 2D images sliced from volumetric chest CT scans. The CNN-based methods proposed in this study includes Multi-View and Triplanar CNN architectures using pre-trained AlexNet, VGG11, VGG19 and GoogLeNet feature extraction layers as a backend. Using five-fold cross validation, the average AUC, Accuracy, Sensitivity and Specificity of the proposed ensemble method were 0.799, 77.1, 0.57 and 0.824, respectively, for multi-label binary classification on the ImageCLEFtuberculosis 2020 training dataset of the lung-based automated CT report generation task, which is a wellbenchmarked public dataset running every year since 2017. The result shows the strength of our model trained in a small dataset with highly unbalanced label distributions, leading to 4 th place on the Leaderboard, with a mean AUC of 0.767 on the test dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tuberculosis (TB) is a highly contagious disease that typically attacks the lungs. Every year, approximately ten million people become infected with TB, with around one and half million deaths, thereby making the disease a global health problem <ref type="bibr" coords="2,173.37,180.66,9.96,8.74" target="#b0">[1]</ref>. Even though many researches have been done to reduce the spread of TB in the society, the report by the World Health Organization (WHO) in 2019 <ref type="bibr" coords="2,157.56,204.57,10.52,8.74">[2]</ref> indicates that TB still remains at the top ten causes of death worldwide and epidemic in 202 countries and territories (see Table <ref type="table" coords="2,380.79,216.52,3.87,8.74" target="#tab_0">1</ref>). Computed Tomography (CT) is one of the most commonly used non-invasive medical imaging techniques in the diagnosis and management of patients with TB <ref type="bibr" coords="2,151.72,416.26,9.96,8.74" target="#b1">[3]</ref>. A volumetric chest CT scan of people with suspected TB is obtained and examined either for abnormalities suggestive of TB or for detection of any kind of TB abnormality. It aids physicians to visualize lesions with specific manifestations in the altering lung tissues caused by tuberculosis <ref type="bibr" coords="2,377.86,452.13,9.96,8.74" target="#b2">[4]</ref>. However, CT comes at the cost of generating thousands of images per patient, which makes it timeconsuming, subjective, and even impossible to achieve high performance level in the absence of expert radiologists <ref type="bibr" coords="2,280.65,487.99,9.96,8.74" target="#b3">[5]</ref>. Hence, the development of computer-aided diagnosis (CAD) techniques to assist physicians in tuberculosis detection and diagnosis have been attracted much attention from researchers at the intersection of medicine and artificial intelligence <ref type="bibr" coords="2,298.07,523.86,7.75,8.74" target="#b4">[6]</ref><ref type="bibr" coords="2,305.82,523.86,3.87,8.74" target="#b5">[7]</ref><ref type="bibr" coords="2,305.82,523.86,3.87,8.74" target="#b6">[8]</ref><ref type="bibr" coords="2,309.69,523.86,7.75,8.74" target="#b7">[9]</ref>.</p><p>Deep learning (DL) <ref type="bibr" coords="2,236.85,536.19,15.50,8.74" target="#b8">[10]</ref> based CAD algorithms especially convolutional neural networks (CNNs) <ref type="bibr" coords="2,230.56,548.14,15.50,8.74" target="#b9">[11]</ref> that learn visual patterns directly from images with minimal pre-processing and without the intermediate step of experts have recently been effective in the medical imaging and other computer vision applications <ref type="bibr" coords="2,169.72,584.01,12.45,8.74" target="#b10">[12]</ref><ref type="bibr" coords="2,182.18,584.01,4.15,8.74" target="#b11">[13]</ref><ref type="bibr" coords="2,186.33,584.01,12.45,8.74" target="#b12">[14]</ref>. Along these lines, as part of CLEF (Conference and Labs of the Evaluation Forum) -a series of campaigns that have been carried out in the information retrieval domain since 2000, ImageCLEF 2020 has presented an evaluation campaign that offers researchers around the world to participate in the ImageCLEFtuberculosis task that runs for fourth consecutive year <ref type="bibr" coords="2,444.99,631.83,15.50,8.74" target="#b13">[15,</ref><ref type="bibr" coords="2,461.78,631.83,11.62,8.74" target="#b14">16]</ref>.</p><p>The task provided by ImageCLEFtuberculosis organizers varies from year to year. Last year the tasks were Severity Score Prediction (SVR) and CT -based automatic CT report generation (CTR) based on volumetric chest CT scans and clinical information of patients <ref type="bibr" coords="3,268.91,130.95,14.61,8.74" target="#b15">[17]</ref>. However, this year's challenge (ImageCLEFtuberculosis 2020) was a lung-based automatic CT report generation solely on CT images <ref type="bibr" coords="3,187.11,154.86,14.61,8.74" target="#b16">[18]</ref>. In last year's tuberculosis challenge, even though we participated for the first time, our Multi-View CNN based approach achieved rank 4 th with mean AUC of 0.707 <ref type="bibr" coords="3,252.55,178.77,14.61,8.74" target="#b17">[19]</ref>. Hence, since our last year approach produced competitive result, we decided to improve and adapt it to the requirements of this year challenge. Therefore, in this study, we developed a novel CAD based system for automated TB diagnosis by using different Multi-View and Triplanar CNN architectures with the ensemble method on chest CT images. We developed the CNN architectures using pre-trained AlexNet <ref type="bibr" coords="3,385.55,238.55,14.61,8.74" target="#b18">[20]</ref>, GoogLeNet <ref type="bibr" coords="3,462.33,238.55,14.61,8.74" target="#b19">[21]</ref>, and VGG <ref type="bibr" coords="3,180.29,250.50,15.50,8.74" target="#b20">[22]</ref> feature extraction layers as a backend.</p><p>This paper has the following structure: in section 2, we present the dataset, image pre-processing, CNN architectures and ensemble methods used in this work. Results and discussions are reported in section 3. Finally, section 4 points future works, and concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset and Image Pre-processing</head><p>The training and test datasets provided by the ImageCLEFtuberculosis 2020 task organizers consist of 403 studies of people with TB where the organizers divided the dataset into 283 training and 120 test studies. Each study contains patients' volumetric chest CT scans stored in NIFTI file format, automatically extracted masks of the lungs obtained with the algorithm discussed in <ref type="bibr" coords="3,443.06,417.01,14.61,8.74" target="#b21">[23]</ref>, and a lung-based six diagnosis labels, which are:</p><p>(i) LeftLungAffected (LL) -binary label for presence of any TB lesions in the left lung;</p><p>(ii) RightLungAffected (RL) -binary label for presence of any TB lesions in the right lung;</p><p>(iii) CavernsLeft (CL) -binary label for presence of caverns in the left lung;</p><p>(iv) CavernsRight (CR) -binary label for presence of caverns in the right lung;</p><p>(v) PleurisyLeft (PL) -binary label for presence of pleurisy in the left lung;</p><p>(vi) PleurisyRight (PR) -binary label for presence of pleurisy in the right lung.</p><p>The provided training dataset by the task organizers is highly imbalanced in which there are more positive cases than negative cases in LL and RL labels, and few positive cases than negative cases in the other diagnosis labels. Moreover, the PL label has the largest unbalanced distribution in the dataset where the proportion of positive training cases being about 2.5%. Even the CVR label, which has a relatively better balanced distribution than the other labels, has only 27.9% of the training cases labelled positive. Fig. <ref type="figure" coords="3,375.46,632.21,4.40,8.74" target="#fig_0">1</ref> depicts the number of positive and negative patients for each of the six diagnosis labels of the training dataset. More details about the datasets can also be found in <ref type="bibr" coords="3,405.72,656.12,14.61,8.74" target="#b15">[17]</ref>.</p><p>The sizes of all the volumetric chest CT scans are 512 × 512 × k, where image length and width are 512 and k indicates number of slices in the axial plane varying from 47 to 264 and 101 to 258 for training and testing datasets, respectively. We used the training dataset to develop a model that can generate multi-class binary classification prediction results related to the three labeled diagnosis conditions of each lung. In other words, our model simultaneously predicts whether a certain condition is present (i.e. 'positive or the numerical equivalent of 1') or absent (i.e. 'negative or the numerical equivalent of 0') for each of the three diagnosis labels of each lung. As we planned to leverage 2D CNN models pre-trained on natural images of a fixed image resolution, we reformatted each 3D chest CT scan to a group of 2D stacked slices in the axial, coronal and sagittal views, respectively. Each axial slice is then cropped to a fixed size of 256 × 256 pixels around the left and right lung regions, respectively. Similarly, we cropped each sagittal and coronal slices to a fixed size of 128 × 256 pixels around the left and right lung regions, respectively. The rectangular bounding box locations around each lung were selected through visual inspection of few mid-level slices using the provided segmented masks. To avoid processing the background which does not contain any lung tissue and process the scans under the memory constraints of the GPU, only 30 axials, 60 coronal and 60 sagittal mid-level slices from each volumetric chest CT exams were selected. In addition, to avoid the effect of image enlarging on the models classification performance, two consecutive sagittal slices and two consecutive coronal slices, respectively, were concatenated and reshaped to 256×256 pixel sizes. Then, we rescaled the intensity values of the slices to (0,255) range, convert them to PNG format, and normalized to have zero mean and unit variance. Then, all the sliced axial, sagittal and coronal PNG images were stacked together, and saved in serialized form with pickle toolbox, respectively. Therefore, our input shape turned to be (30, <ref type="bibr" coords="5,339.74,130.95,7.75,8.74" target="#b1">3,</ref><ref type="bibr" coords="5,351.67,130.95,17.71,8.74">256,</ref><ref type="bibr" coords="5,373.56,130.95,17.27,8.74">256)</ref>. The values can be interpreted such that first value holds for the number of axial, coronal or sagittal slices after pre-processing. The last two values for width and height of images and 3 represents the number of color channels. The sketch map of image preprocessing steps is shown in Fig.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Development</head><p>Convolutional neural network (CNNs), also known as deep learners are machine learning methods designed to process image data via convolutional, pooling and fully connected layers. Convolution and pooling layers occur in an alternative fashion to extract high-level features, and fully connected layers are used to perform classification. In this paper, we aimed to develop a DL model that simultaneously predicts lung-based TB diagnosis labels by using different CNN architectures with the ensemble method on chest CT images. We address it as a multi-class binary classification problem. Moreover, we repeated training the proposed architectures two times, one for each lung related diagnosis labels report generation task.</p><p>Considering the training dataset being very small and heavily imbalanced, we proposed five CNN architectures (3 Multi-View CNN architectures: AlexNet M V , GoogLeNet M V and VGG19 M V , and 2 Triplanar-CNN architectures: AlexNet T P and VGG11 T P ) using pre-trained AlexNet, GoogLeNet, VGG11, and VGG19 feature extraction layers as a backend. All of the five CNN architectures were trained using Adam optimization with backpropagation algorithms as they are successfully applied in many deep learning models. In addition, all the models were optimized using weighted binary cross-entropy loss function to account for the unbalanced class sizes. The parameters tuning were experimentally determined individually for each proposed architecture. Moreover, when the validation loss did not decrease for 20 epochs, we early-stopped the parameter optimization and training process to avoid the overfitting problem. Then, the model with the lowest average loss on the validation dataset were selected as our final model candidate. All the models were developed by using a desktop computer with NVIDIA GeForce RTX 2070 GPU and the widely used deep learning framework Pytorch with backend libraries of Tensorflow <ref type="bibr" coords="6,333.27,438.20,14.61,8.74" target="#b22">[24]</ref>.</p><p>The individual classification performance of the five CNNs on the training and testing datasets were compared. Then, in order to get a better and more comprehensive generalized model <ref type="bibr" coords="6,279.76,474.13,14.60,8.74" target="#b23">[25]</ref>, and motivated by the idea of "two or more heads are better than one", the probability predictions by the four CNNs that performed better were fused using different strategies: average, majority voting and stacking (Naïve Bayes). The probability predictions by GoogLeNet M V was relatively not good compared to the other architectures. Hence, we used the other four CNN models as our base learners in the ensemble approach we used. Details of each model architecture and results are discussed in the following parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Architectures</head><p>Multi-View CNNs. The Multi-View CNN architectures proposed in this work are an extension of our prior work for last year year's TB challenge <ref type="bibr" coords="6,432.58,620.25,14.61,8.74" target="#b17">[19]</ref>. In the paper, coronal and sagittal slices were concatenated before fed to the AlexNet based multi-view CNN model, and axial slices were not used. However, in this work's proposed Multi-View CNN , in addition to AlexNet, we used pre-trained VGG19 and GoogLeNet feature extraction layers as a backend. Moreover, in addition to coronal and sagittal slices, axial slices is also used in this work to train the proposed models.</p><p>The basic concept of the proposed Multi-View CNN architecture is that during the training process we provide the model a serious of 2D axial images sliced from 3D CT scan as input and similar sagittal and coronal images as data augmentation techniques, and generates a classification prediction results for each lung related labels. As depicted in Fig. <ref type="figure" coords="7,343.95,206.20,4.13,8.74" target="#fig_4">3</ref>, the overall Multi-View CNN architecture consists of three core parts:  Triplanar-CNN. The overview of the proposed Triplanar-CNN architecture is depicted in Fig. <ref type="figure" coords="8,213.26,130.95,4.13,8.74" target="#fig_5">4</ref>. A 2D images sliced from the volumetric chest CT scans in the axial, coronal and sagittal planes were fed into the three parallel channels of the Multi-View CNN architecture, respectively. Generated features from the three channels were consolidated into a fixed size feature map to form a single combined feature representation. Then, the classification is performed using a fully connected layer and a sigmoid activation function on top of it. More details on the Triplanar-CNN architecture is available in our prior work developed for automated brain tumor grading <ref type="bibr" coords="8,276.54,214.64,14.61,8.74" target="#b24">[26]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head><p>As previously mentioned in Section 2.1, the ImageCLEFtuberculosis 2020 dataset was provided with training and test set partitions. The training dataset is highly imbalanced in each diagnostic labels. Thus, we used five-fold stratified crossvalidation upon the training dataset to reduce overfitting and avoid bias during the overall system evaluation in the test dataset. That is, for each validation fold in the training dataset, the remaining other folds were used to train the models. Indeed, this procedure ensures that every CT scan in the training dataset gets to be in the validation set exactly once. The independent testing dataset was not used during training and internal validation. In fact, diagnosis labels of the patients in the test dataset were not visible to the challenge participants. Participants of the challenge were required to submit the probability prediction for each diagnostic labels and ranking was based on the average and minimum AUC over the six diagnostic labels of both the left and right lungs. However, to quantitatively evaluate the capability of the proposed deep learning based approach on both the provided training and testing datasets, the performance measures averaged over all the five folds of the training dataset are reported in this paper, including the area under the receiver operating characteristic curve (AUC), precision (PRE), specificity (SPE), and sensitivity (SEN) evaluation metrics. Accuracy is not significant for evaluating the performance of the proposed approach as the dataset for each diagnostic labels are highly unbalanced. Performance of our proposed system on both the training and test dataset is explained in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Performance of the Five Multi-View and Triplanar-CNN Models</head><p>Table <ref type="table" coords="9,162.69,260.77,4.98,8.74" target="#tab_1">2</ref> and 3 reports multi-class binary classification performance of the proposed Multi-View CNN and Triplanar-CNN models, respectively, for both the left and right lung related diagnosis labels, on the training dataset using the fivefold cross validation. The results show that the AlexNet M V classifier achieved better classification performance compared to the other classifiers in terms of mean AUC. Moreover, the AlexNet M V methods outperformed its corresponding Triplanar-CNN model, i.e. AlexNet T P , with a marginal increment of 1.6% in terms of mean AUC. Meanwhile, the AlexNet M V classifier outperformed the VGG19 M V and the VGG11 T P models with improvement rates of 3.2% and 2.6% in terms of the mean AUC, respectively. GoogLeNet M V suffers with the overfitting problem and it performance (mean AUC of 0.506) was relatively poor compared to the other models. This may be due to the architecture is deeper than the AlexNet and VGG architectures, and due to the scarcity of the available training dataset. In addition to axial slices, Multi-View CNN classifiers were trained using coronal and sagittal slices as data augmentation techniques. However, validation and testing were performed using axial slices only. Triplanar-CNN models were trained and evaluated using axial, coronal and sagittal slices without using any data augmentation techniques. Yet due to the strong performance of the the proposed models, as reported in Table <ref type="table" coords="9,475.61,476.79,4.98,8.74" target="#tab_1">2</ref> and 3, for the multi-class binary classification problems across the multiple tasks, we are confident that our models will perform better if we were to incorporate extensive data augmentation techniques. In addition, though we used weighted cross-entropy loss to account for the imbalanced class sizes, the performances of the proposed models on some tasks are highly biased towards the majority class. For instance, as shown in Fig. <ref type="figure" coords="9,263.56,548.52,4.13,8.74" target="#fig_0">1</ref>, out of 283 patients of the training dataset, only 7 (2.5%) of them were PL positive, whereas the remaining 276 (97.5%) were PL negative. Hence, performance of all the models in terms of SEN for the PL binary classification is very poor, whereas the PRE is obviously very high. Similarly, only 4.9% of the training dataset were PR positive, the remaining 95.1% were PR negative. However, unlike that of the PL task, classification performance of all the models in terms of SEN for PR was not highly affected. This shows that the weighted loss computation we used during training the models for tackling imbalanced class size problems worked well for some tasks. Hence weighted loss computation along with some renowned resampling techniques might be further investigated in order to balance of the classes distribution and avoid bias on classification performance of deep learning models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results Comparison on the Training and Test Datasets</head><p>In this challenge, participants were required to come up with an approach that generate an automatic lung-based report generation based on the volumetric CT image. For this, the organizers provided training and test datasets. Labels of the training dataset were given to the participants. However, test dataset labels were not visible to the participants. Participants were allowed to submit up to 10 runs to the system arranged by the organizers. The organizers do evaluation of the results, and ranking participants algorithms based on the results. The results of our proposed approaches on the test dataset obtained from the organizers website is depicted in Table <ref type="table" coords="11,290.37,572.43,3.87,8.74" target="#tab_4">5</ref>. From our proposed individual classifiers, AlexNetMV performed best on the test dataset with average and min AUC of 0.757 and 0.713, respectively. From the proposed ensemble approaches, average fusion strategy outperforms all the models with mean and min AUC of 0.767 and 0.733, respectively. When best runs of each participant are compared using mean AUC on the test dataset, our result ranked 4 th . Detailed results of each participant algorithm on the test dataset using multiple performance metrics can be obtained at <ref type="bibr" coords="11,200.45,656.12,14.61,8.74" target="#b15">[17]</ref>. In addition, as shown in Fig. <ref type="figure" coords="11,345.12,656.12,4.13,8.74" target="#fig_6">5</ref>, performance of our proposed DL models in both the training and test datasets is nearly the same, indicating the robustness of our model. This also provides insight on how the proposed DL system will be generalized to an unknown dataset at the real test time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In conclusion, we propose a robust CAD system for automated tuberculosis diagnosis using ensemble of different CNN architectures trained on volumetric chest CT scans of less than 300 tuberculosis patients. The proposed CNN architectures includes novel Multi-View and Triplanar-CNN architectures using pre-trained feature extraction layers of state-of-the-art deep learning models as a backend. Our experiment result that completes the top four in the challenge demonstrates that the proposed deep learning model has the ability to generate competitive performance on automated lung-based CT report generation solely based on volumetric CT images of patients with tuberculosis. There are still some rooms for improvement within our proposed CAD system to improve the performance. To crop the left and right lungs regions from the chest CT images, we used a fixed bounding box location for all the images through visual inspection of some random mid-level slices that could result in missing some abnormal regions of the lungs, as different CT devices produce images in different orientation. In the literature, transfer learning with different data augmentation techniques have been used to improve the performance of deep learning models in datasets with limited size. However, we only used transfer learning to increase the performance of our deep learning models on the available limited amount of training data. We did not use data augmentation techniques. Moreover, though the provided datasets were highly imbalanced, various class imbalance techniques and ensemble learner with multiple deep learning base classifiers were not investigated very well due to the limited time constraints. Ultimately, we would like to address these issues in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgments</head><p>This work was supported by the research fund of C ¸ukurova University Project Number: 10683</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,394.06,345.83,7.89;4,134.77,405.05,345.83,7.86;4,134.77,416.01,345.82,7.86;4,134.77,426.97,62.23,7.86;4,172.87,248.79,269.63,130.50"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Distribution of the positive and negative cases across the different diagnosis labels. For LL and RL, the majority of cases are "Positive" compared to the minority of "Negative" cases. However, for CVL, CVR, PL and PR labels the majority of cases are "Negative".</figDesc><graphic coords="4,172.87,248.79,269.63,130.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,277.26,178.77,4.40,8.74"><head></head><label></label><figDesc>2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,604.79,345.83,7.89;5,134.77,615.78,345.83,7.86;5,134.77,626.74,321.57,7.86;5,210.20,221.47,194.95,368.55"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of 3D chest CT scan of patient ID CTR-TRN-051 pre-processing stages. From top to bottom: 2D sliced from the 3D scan and then cropped around the left and right lung regions in the axial, coronal, and sagittal views, respectively.</figDesc><graphic coords="5,210.20,221.47,194.95,368.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,149.71,233.64,330.88,8.74;7,134.77,245.59,163.37,8.74;7,149.71,261.07,330.89,8.74;7,134.77,273.03,345.82,8.74;7,134.77,284.98,16.05,8.74;7,149.71,300.46,330.88,8.74;7,134.77,312.41,345.83,8.74;7,134.77,324.37,345.83,8.74;7,134.77,336.32,155.50,8.74"><head></head><label></label><figDesc>(i) The feature extraction layers of pre-trained state-of-the-art CNN model (i.e VGG19, AlexNet or GoogLeNet).(ii) Global average pooling and max pooling layers on top of the feature extraction layers applied across the spatial dimensions to reduce feature maps, and (iii) Dense layer. The dense layer was fed the resulted feature maps after pooling operations. Then, the sigmoid function applied to the output of the dense layer to obtain the final probability binary prediction score for each of the three diagnosis labels of each lungs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,134.77,556.40,345.82,7.89;7,134.77,567.38,345.83,7.86;7,134.77,578.34,345.83,7.86;7,134.77,589.30,345.83,7.86;7,134.77,600.26,345.83,7.86;7,134.77,611.22,345.83,7.86;7,134.77,622.18,343.78,7.86;7,134.77,633.14,345.82,7.86;7,134.77,644.10,345.83,7.86;7,134.77,655.05,152.27,7.86;7,134.18,374.79,347.01,166.84"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Multi-View CNN architecture: VGG19MV . VGG19MV is an automatic TB diagnosis Multi-View CNN architecture using VGG19 feature extraction layers as a backend. The architecture takes as input a series of CT slices and outputs a multi-class binary classification predictions of the CT scan. Global average and max-pooling operation were used to combine features from each slice obtained using the VGG19 feature extraction layers. The resulted feature maps were then fed to a fully connected layer to generate a probability score of each the three diagnosis labels. We trained VGG19MV two times, one for each lung related report generations. Using similar architecture and training, we developed AlexNetMV and GoogLeNetMV with AlexNet and GoogLeNet feature extraction layers, respectively.</figDesc><graphic coords="7,134.18,374.79,347.01,166.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,134.77,355.19,345.83,8.31;8,134.77,366.17,345.83,7.86;8,134.77,377.13,345.83,7.86;8,134.77,388.09,345.83,8.28;8,134.77,399.05,345.83,8.28;8,134.77,410.01,186.54,7.86;8,138.83,246.92,337.70,93.50"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Triplanar-CNN: VGG19T P . A 3D chest CT scan is first decomposed into 2D axial, coronal and sagittal cross-sectional slices then passed to each of the three column VGG19MV Multi-View CNN feature extraction layers, respectively. We trained the VGG19T P two times, one for each lung related report generation. Using similar architecture and training procedure, we developed AlexNetT P using AlexNetMV feature extraction layers in each of the three columns.</figDesc><graphic coords="8,138.83,246.92,337.70,93.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="12,134.77,492.82,345.83,7.89;12,134.77,503.81,35.43,7.86;12,193.60,340.58,228.15,137.48"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Performance comparisons of the proposed models on the training and test datasets.</figDesc><graphic coords="12,193.60,340.58,228.15,137.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,134.77,249.32,345.82,114.99"><head>Table 1 .</head><label>1</label><figDesc>Number of countries and territories that reported the TB incidents to WHO in 2019.</figDesc><table coords="2,240.99,279.34,133.37,84.97"><row><cell>Regions</cell><cell>Numbers</cell></row><row><cell>Africa</cell><cell>46</cell></row><row><cell>European</cell><cell>45</cell></row><row><cell cols="2">Region of the Americas 43</cell></row><row><cell>Western Pacific</cell><cell>35</cell></row><row><cell cols="2">Eastern Mediterranean 22</cell></row><row><cell>South-East Asia</cell><cell>11</cell></row><row><cell>Global</cell><cell>202</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,134.77,162.26,345.83,183.28"><head>Table 2 .</head><label>2</label><figDesc>Performance evaluation of the three Multi-View CNN models. Bold indicates our best results averaged across the six labels.</figDesc><table coords="10,165.45,194.01,284.45,151.53"><row><cell>Models</cell><cell>Left Lung LL CVL PL</cell><cell>Right Lung RL CVR PR</cell><cell>Avg.</cell></row><row><cell></cell><cell cols="3">AUC 0.744 0.775 0.82 0.736 0.717 0.88 0.779</cell></row><row><cell>AlexNetMV</cell><cell cols="3">SEN 0.609 0.663 0.278 0.644 0.594 0.806 0.599 SPE 0.836 0.799 0.932 0.779 0.746 0.925 0.836</cell></row><row><cell></cell><cell cols="3">PRE 0.943 0.561 0.194 0.936 0.482 0.387 0.584</cell></row><row><cell></cell><cell cols="3">AUC 0.76 0.751 0.74 0.71 0.65 0.869 0.747</cell></row><row><cell>VGG19MV</cell><cell cols="3">SEN 0.632 0.535 0.17 0.656 0.57 0.611 0.529 SPE 0.71 0.728 0.932 0.714 0.598 0.93 0.769</cell></row><row><cell></cell><cell cols="3">PRE 0.923 0.587 0.111 0.92 0.369 0.375 0.548</cell></row><row><cell></cell><cell cols="3">AUC 0.566 0.526 0.456 0.454 0.548 0.486 0.506</cell></row><row><cell>GoogLeNetMV</cell><cell cols="3">SEN 0.591 0.594 0.433 0.79 0.742 0.306 0.576 SPE 0 0.371 0.383 0 0.46 0.563 0.296</cell></row><row><cell></cell><cell cols="3">PRE 0.782 0.265 0.031 0.824 0.35 0.063 0.386</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,134.77,392.54,345.83,272.32"><head>Table 3 .</head><label>3</label><figDesc>Performance evaluation of the two Triplanar-CNN models. Bold indicates our best results averaged across the six labels. SEN, SPE, and PRE of the Naïve Bayes fusion strategy were 0.759, 0.573, 0.801, and 0.829, respectively. Average fusion strategy has the highest mean AUC and SPE values, and Naïve Bayes has the lowest in both evaluation metrics. However, Naïve Bayes has the highest mean PRE values than average and voting fusion approaches with improvement rates of more than 25%. The SEN and SPE of the three ensemble methods are almost the same with less than 0.5% and 2.5% difference, respectively.</figDesc><table coords="10,134.77,424.30,345.83,240.56"><row><cell>Models</cell><cell>Left Lung LL CVL PL</cell><cell>Right Lung RL CVR PR</cell><cell>Avg.</cell></row><row><cell></cell><cell cols="3">AUC 0.706 0.789 0.745 0.697 0.698 0.944 0.763</cell></row><row><cell>AlexNetTP</cell><cell cols="3">SEN 0.668 0.646 0 SPE 0.553 0.892 0.833 0.698 0.737 0.931 0.774 0.665 0.619 0.806 0.567</cell></row><row><cell></cell><cell cols="3">PRE 0.85 0.677 0.083 0.914 0.475 0.446 0.574</cell></row><row><cell></cell><cell>AUC 0.745 0.79 0.69</cell><cell cols="2">0.681 0.656 0.957 0.753</cell></row><row><cell>VGG11TP</cell><cell cols="3">SEN 0.617 0.729 0.0389 0.615 0.553 0.72 0.546 SPE 0.684 0.741 0.483 0.695 0.714 0.956 0.712</cell></row><row><cell></cell><cell cols="3">PRE 0.891 0.582 0.059 0.906 0.45 0.595 0.581</cell></row><row><cell cols="4">3.2 Performance of Ensemble Multi-View and Triplanar-CNN</cell></row><row><cell>Models</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">With regard to the AUC, SEN, SPE and PRE, the classification results achieved</cell></row><row><cell cols="4">by each of the three ensemble methods used in our work are reported in Table 4.</cell></row><row><cell cols="4">The mean AUC, SEN, SPE, and PRE of the average fusion strategy were 0.799,</cell></row><row><cell cols="4">0.571, 0.824, and 0.576, respectively. The mean AUC, SEN, SPE, and PRE of</cell></row><row><cell cols="4">the voting fusion strategy were 0.777, 0.574, 0.821, and 0.574, respectively. The</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,134.77,223.08,345.83,183.28"><head>Table 4 .</head><label>4</label><figDesc>Performance of the proposed system for three different fusion strategies. Bold indicates our best results averaged across the six labels.</figDesc><table coords="11,173.01,254.84,269.34,151.53"><row><cell>Models</cell><cell cols="2">Left Lung LL CVL PL</cell><cell>Right Lung RL CVR PR</cell><cell>Avg.</cell></row><row><cell></cell><cell cols="4">AUC 0.788 0.815 0.841 0.73 0.689 0.93 0.799</cell></row><row><cell>Average</cell><cell cols="4">SEN 0.639 0.614 0.286 0.643 0.574 0.667 0.571 SPE 0.778 0.832 0.932 0.724 0.721 0.956 0.824</cell></row><row><cell></cell><cell cols="4">PRE 0.914 0.562 0.154 0.918 0.443 0.462 0.576</cell></row><row><cell></cell><cell>AUC 0.779 0.8</cell><cell cols="3">0.736 0.73 0.682 0.936 0.777</cell></row><row><cell>Voting</cell><cell cols="4">SEN 0.632 0.614 0.286 0.65 0.596 0.667 0.574 SPE 0.75 0.84 0.938 0.724 0.721 0.95 0.821</cell></row><row><cell></cell><cell cols="4">PRE 0.903 0.574 0.167 0.919 0.452 0.429 0.574</cell></row><row><cell></cell><cell cols="4">AUC 0.78 0.805 0.68 0.722 0.681 0.886 0.759</cell></row><row><cell>NaiveBayes</cell><cell cols="4">SEN 0.677 0.614 0.143 0.778 0.447 0.778 0.573 SPE 0.75 0.856 0.938 0.552 0.811 0.931 0.801</cell></row><row><cell></cell><cell cols="4">PRE 0.798 0.794 0.926 0.799 0.704 0.955 0.829</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,135.77,175.26,343.82,116.73"><head>Table 5 .</head><label>5</label><figDesc>Mean and minimum AUC of the each proposed models on the test dataset.</figDesc><table coords="12,221.24,196.06,172.89,95.93"><row><cell>Models</cell><cell cols="2">Mean AUC Minimum AUC</cell></row><row><cell>AlexNetMV</cell><cell>0.757</cell><cell>0.713</cell></row><row><cell>AlexNetTP</cell><cell>0.755</cell><cell>0.707</cell></row><row><cell>VGG19MV</cell><cell>0.756</cell><cell>0.724</cell></row><row><cell>VGG11TP</cell><cell>0.731</cell><cell>0.722</cell></row><row><cell>GoogLeNet</cell><cell>0.427</cell><cell>0.36</cell></row><row><cell>Average</cell><cell>0.767</cell><cell>0.733</cell></row><row><cell>Voting</cell><cell>0.757</cell><cell>0.727</cell></row><row><cell>NaiveBayes</cell><cell>0.759</cell><cell>0.714</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.96,417.23,337.64,7.86;13,151.52,428.18,329.07,7.86;13,151.52,439.14,203.57,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,359.07,417.23,121.52,7.86;13,151.52,428.18,164.57,7.86">Chest tuberculosis: Radiological review and imaging recommendations</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Bhalla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Guleria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="DOI">10.4103/0971-3026.161431</idno>
		<ptr target="https://doi.org/10.4103/0971-3026.161431" />
	</analytic>
	<monogr>
		<title level="j" coord="13,323.79,428.18,106.47,7.86">Indian J. Radiol. Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="213" to="225" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,471.51,337.64,7.86;13,151.52,482.47,329.07,7.86;13,151.52,493.43,183.74,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,407.50,471.51,73.10,7.86;13,151.52,482.47,38.04,7.86">Imaging in tuberculosis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Bomanji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Das</surname></persName>
		</author>
		<idno type="DOI">10.1101/cshperspect.a017814</idno>
		<ptr target="https://doi.org/10.1101/cshperspect.a017814" />
	</analytic>
	<monogr>
		<title level="j" coord="13,206.91,482.47,179.32,7.86">Cold Spring Harb. Perspect. Med</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,504.14,337.63,7.86;13,151.52,515.09,329.07,7.86;13,151.52,526.05,329.07,7.86;13,151.52,537.01,329.07,7.86;13,151.52,547.97,216.90,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,303.30,504.14,177.29,7.86;13,151.52,515.09,221.31,7.86">A framework of predicting drug resistance of lung tuberculosis by utilizing radiological images</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">.org/10.1109/ICACI.2018.8377474</idno>
		<ptr target="https://doi.org/doi.org/10.1109/ICACI.2018.8377474" />
	</analytic>
	<monogr>
		<title level="m" coord="13,399.55,515.09,81.04,7.86;13,151.52,526.05,294.42,7.86">Proceedings -2018 10th International Conference on Advanced Computational Intelligence</title>
		<meeting>-2018 10th International Conference on Advanced Computational Intelligence<address><addrLine>ICACI</addrLine></address></meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="308" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,558.68,337.64,7.86;13,151.52,569.63,329.07,7.86;13,151.52,580.59,329.07,7.86;13,151.52,591.55,227.38,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,272.20,569.63,208.39,7.86;13,151.52,580.59,195.96,7.86">High sensitivity of chest radiograph reading by clinical officers in a tuberculosis prevalence survey</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Van't Hoog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">K</forename><surname>Meme</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Deutekom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Mithika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Olunga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Onyino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Borgdorff</surname></persName>
		</author>
		<idno type="DOI">10.5588/ijtld.11.0004</idno>
		<ptr target="https://doi.org/10.5588/ijtld.11.0004" />
	</analytic>
	<monogr>
		<title level="j" coord="13,356.20,580.59,107.85,7.86">Int. J. Tuberc. Lung Dis</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1308" to="1314" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,602.26,337.64,7.86;13,151.52,613.22,329.07,7.86;13,151.52,624.18,316.27,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,217.72,613.22,233.78,7.86">Smart spotting of pulmonary TB cavities using CT images</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">E</forename><surname>Swanly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Selvam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Renjith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Arunachalam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Shunmuganathan</surname></persName>
		</author>
		<idno type="DOI">10.1155/2013/864854</idno>
		<ptr target="https://doi.org/10.1155/2013/864854" />
	</analytic>
	<monogr>
		<title level="j" coord="13,458.58,613.22,22.02,7.86;13,151.52,624.18,105.01,7.86">Comput. Math. Methods Med</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,634.88,337.64,7.86;13,151.52,645.84,329.07,7.86;13,151.52,656.80,262.99,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,151.52,645.84,329.07,7.86;13,151.52,656.80,19.71,7.86">Computer-aided detection and quantification of cavitary tuberculosis from CT scans</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Bagci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kubler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Bishai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Mollura</surname></persName>
		</author>
		<idno type="DOI">10.1118/1.4824979</idno>
		<ptr target="https://doi.org/10.1118/1.4824979" />
	</analytic>
	<monogr>
		<title level="j" coord="13,178.25,656.80,45.48,7.86">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,119.67,337.64,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,329.07,7.86;14,151.52,152.55,329.07,7.86;14,151.52,163.51,187.22,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,348.49,130.63,132.11,7.86;14,151.52,141.59,329.07,7.86;14,151.52,152.55,184.73,7.86">A systematic review of the diagnostic accuracy of artificial intelligence-based computer programs to analyze chest x-rays for pulmonary tuberculosis</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jeagal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Menzies</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Korobitsyn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Nathavitharana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ahmad Khan</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0221339</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0221339" />
	</analytic>
	<monogr>
		<title level="j" coord="14,344.83,152.55,40.66,7.86">PLoS One</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">221339</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,173.48,337.63,7.86;14,151.52,184.44,329.07,7.86;14,151.52,195.40,329.07,7.86;14,151.52,206.36,222.21,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,329.78,184.44,150.81,7.86;14,151.52,195.40,324.95,7.86">An automated tuberculosis screening strategy combining X-ray-based computer-aided detection and clinical information</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Melendez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">H H M</forename><surname>Philipsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Maduskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Theron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Dheda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<idno type="DOI">10.1038/srep25265</idno>
		<ptr target="https://doi.org/10.1038/srep25265" />
	</analytic>
	<monogr>
		<title level="j" coord="14,151.52,206.36,35.96,7.86">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,216.34,337.98,7.86;14,151.52,227.30,151.38,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,298.50,216.34,55.25,7.86">Deep learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
		<ptr target="https://doi.org/10.1038/nature14539" />
	</analytic>
	<monogr>
		<title level="j" coord="14,361.54,216.34,30.49,7.86">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,237.27,337.97,7.86;14,151.52,248.23,329.07,7.86;14,151.52,259.19,137.02,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,350.11,237.27,130.48,7.86;14,151.52,248.23,100.39,7.86">Gradient-based learning applied to document recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
		<ptr target="https://doi.org/10.1109/5.726791" />
	</analytic>
	<monogr>
		<title level="m" coord="14,276.42,248.23,102.43,7.86">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="2278" to="2323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,269.16,337.97,7.86;14,151.52,280.12,329.08,7.86;14,151.52,291.08,209.47,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,412.40,269.16,68.19,7.86;14,151.52,280.12,180.25,7.86">Concept attribution: Explaining CNN decisions to physicians</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Graziani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marchand-Maillet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compbiomed.2020.103865</idno>
		<ptr target="https://doi.org/10.1016/j.compbiomed.2020.103865" />
	</analytic>
	<monogr>
		<title level="j" coord="14,338.68,280.12,80.24,7.86">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="page">103865</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,301.06,337.97,7.86;14,151.52,312.02,329.07,7.86;14,151.52,322.97,329.07,7.86;14,151.52,333.93,329.07,7.86;14,151.52,344.89,181.82,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,247.70,301.06,232.89,7.86;14,151.52,312.02,190.79,7.86">Implementation of Target Tracking Methods on Images Taken from Unmanned Aerial Vehicles</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Eriş</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename></persName>
		</author>
		<idno type="DOI">10.1109/SAMI.2019.8782768</idno>
		<ptr target="https://doi.org/10.1109/SAMI.2019.8782768" />
	</analytic>
	<monogr>
		<title level="m" coord="14,369.87,312.02,110.72,7.86;14,151.52,322.97,284.96,7.86">SAMI 2019 -IEEE 17th World Symposium on Applied Machine Intelligence and Informatics</title>
		<title level="s" coord="14,445.36,322.97,35.23,7.86;14,151.52,333.93,14.79,7.86">Proceedings</title>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="311" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,354.87,337.98,7.86;14,151.52,365.83,329.07,7.86;14,151.52,376.79,329.07,7.86;14,151.52,387.74,180.80,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,437.05,354.87,43.54,7.86;14,151.52,365.83,329.07,7.86;14,151.52,376.79,88.46,7.86">Computeraided diagnosis of breast ultrasound images using ensemble learning from convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">K</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cmpb.2020.105361</idno>
		<ptr target="https://doi.org/10.1016/j.cmpb.2020.105361" />
	</analytic>
	<monogr>
		<title level="j" coord="14,247.40,376.79,151.48,7.86">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="page">105361</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,397.72,337.98,7.86;14,151.52,408.68,329.07,7.86;14,151.52,419.64,329.07,7.86;14,151.52,430.60,329.07,7.86;14,151.52,441.55,329.07,7.86;14,151.52,452.51,329.07,7.86;14,151.52,463.47,329.07,7.86;14,151.52,474.43,329.07,7.86;14,151.52,485.39,329.07,7.86;14,151.52,496.35,45.57,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,267.52,452.51,213.07,7.86;14,151.52,463.47,166.55,7.86">ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-45442-5-69</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-45442-5-69" />
	</analytic>
	<monogr>
		<title level="s" coord="14,338.67,463.47,141.92,7.86;14,151.52,474.43,329.07,7.86;14,151.52,485.39,56.62,7.86">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="533" to="541" />
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,506.32,337.97,7.86;14,151.52,517.28,329.07,7.86;14,151.52,528.24,329.07,7.86;14,151.52,539.20,329.07,7.86;14,151.52,550.16,114.11,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,385.78,517.28,94.81,7.86;14,151.52,528.24,185.83,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Theodora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Névéol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,356.68,528.24,123.91,7.86;14,151.52,539.20,244.11,7.86">Proceedings of the Eleventh International Conference of the CLEF Association (CLEF 2020)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename></persName>
		</editor>
		<meeting>the Eleventh International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,560.14,337.97,7.86;14,151.52,571.09,329.07,7.86;14,151.52,582.05,328.48,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,151.52,571.09,329.07,7.86;14,151.52,582.05,169.53,7.86">Overview of ImageCLEFtuberculosis 2019 -Automatic CT-based Report Generation and Tuberculosis Severity Assessment</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,342.59,582.05,106.18,7.86">CLEF2019 Working Notes</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,592.03,337.98,7.86;14,151.52,602.99,329.07,7.86;14,151.52,613.95,329.07,7.86;14,151.52,624.90,28.16,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,166.23,602.99,314.36,7.86;14,151.52,613.95,26.90,7.86">Overview of ImageCLEFtuberculosis 2020 -Automatic CT-based Report Generation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org" />
	</analytic>
	<monogr>
		<title level="m" coord="14,201.21,613.95,106.29,7.86">CLEF2020 Working Notes</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,634.88,337.97,7.86;14,151.52,645.84,329.07,7.86;14,151.52,656.80,172.55,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,307.08,634.88,173.50,7.86;14,151.52,645.84,280.29,7.86">Multi-view CNN with MLP for diagnosing tuberculosis patients using CT scans and clinically relevant metadata</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Mossa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Yibre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="14,453.97,645.84,26.62,7.86;14,151.52,656.80,141.32,7.86">CEUR Workshop Proceedings. CEUR-WS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,119.67,337.98,7.86;15,151.52,130.63,329.08,7.86;15,151.52,141.59,134.46,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="15,357.50,119.67,123.09,7.86;15,151.52,130.63,156.27,7.86">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
		<ptr target="https://doi.org/10.1145/3065386" />
	</analytic>
	<monogr>
		<title level="j" coord="15,320.46,130.63,71.90,7.86">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,152.55,337.98,7.86;15,151.52,163.51,329.07,7.86;15,151.52,174.47,329.07,7.86;15,151.52,185.43,248.62,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="15,262.95,163.51,124.37,7.86">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298594</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298594" />
	</analytic>
	<monogr>
		<title level="m" coord="15,407.32,163.51,73.27,7.86;15,151.52,174.47,324.88,7.86">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,196.39,337.97,7.86;15,151.52,207.34,329.07,7.86;15,151.52,218.30,329.07,7.86;15,151.52,229.26,123.38,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="15,278.92,196.39,201.67,7.86;15,151.52,207.34,69.96,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,242.86,207.34,237.73,7.86;15,151.52,218.30,329.07,7.86;15,151.52,229.26,62.40,7.86">3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings. International Conference on Learning Representations</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,240.22,337.97,7.86;15,151.52,251.18,329.07,7.86;15,151.52,262.14,329.07,7.86;15,151.52,273.10,329.07,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="15,407.31,240.22,73.28,7.86;15,151.52,251.18,202.84,7.86">Efficient and fully automatic segmentation of the lungs in CT volumes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A</forename><surname>Del Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,349.48,262.14,131.11,7.86;15,151.52,273.10,204.56,7.86">Proceedings of the (VISCERAL) Anatomy Grand Challenge at the 2015 (IEEE ISBI)</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Del Toro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Foncubierta-Rodriguez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<meeting>the (VISCERAL) Anatomy Grand Challenge at the 2015 (IEEE ISBI)</meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,284.06,337.98,7.86;15,151.52,295.02,329.07,7.86;15,151.52,305.98,329.07,7.86;15,151.52,316.93,72.70,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="15,391.12,295.02,89.46,7.86;15,151.52,305.98,62.61,7.86">Automatic differentiation in PyTorch</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">D</forename><surname>Facebook</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">I</forename><surname>Research</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Srl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,236.62,305.98,212.50,7.86">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,327.89,337.98,7.86;15,151.52,338.85,270.14,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="15,220.73,327.89,168.96,7.86">Ensemble learning via negative correlation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0893-6080(99)00073-8</idno>
		<ptr target="https://doi.org/10.1016/S0893-6080(99)00073-8" />
	</analytic>
	<monogr>
		<title level="j" coord="15,396.29,327.89,65.27,7.86">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1399" to="1404" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,349.81,337.97,7.86;15,151.52,360.77,329.07,7.86;15,151.52,371.73,329.07,7.86;15,151.52,382.69,314.20,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="15,248.17,349.81,232.42,7.86;15,151.52,360.77,149.66,7.86">Triplanar-CNN for Automated Grading of Gliomas Using Preoperative Multi-modal MR Images</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Mossa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename></persName>
		</author>
		<idno type="DOI">10.15224/978-1-63248-188-7-05</idno>
		<ptr target="https://doi.org/10.15224/978-1-63248-188-7-05" />
	</analytic>
	<monogr>
		<title level="m" coord="15,321.20,360.77,159.39,7.86;15,151.52,371.73,263.04,7.86">Proc. Of the International E-Conference onAdvances in Engineering,Technology and Management -ICETM</title>
		<meeting>Of the International E-Conference onAdvances in Engineering,Technology and Management -ICETM</meeting>
		<imprint>
			<publisher>SEEK Digital Library</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
