<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.18,115.90,331.00,12.68;1,159.10,133.83,297.16,12.68">A Competitive Deep Neural Network Approach for the ImageCLEFmed Caption 2020 Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,172.21,171.59,96.64,8.80"><forename type="first">Marimuthu</forename><surname>Kalimuthu</surname></persName>
							<email>marimuthu.kalimuthu@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="department">German Research Center for Artificial Intelligence (DFKI</orgName>
								<orgName type="institution">Saarland Informatics Campus</orgName>
								<address>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.10,171.59,72.02,8.80"><forename type="first">Fabrizio</forename><surname>Nunnari</surname></persName>
							<email>fabrizio.nunnari@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="department">German Research Center for Artificial Intelligence (DFKI</orgName>
								<orgName type="institution">Saarland Informatics Campus</orgName>
								<address>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,376.61,171.59,66.54,8.80"><forename type="first">Daniel</forename><surname>Sonntag</surname></persName>
							<email>daniel.sonntag@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="department">German Research Center for Artificial Intelligence (DFKI</orgName>
								<orgName type="institution">Saarland Informatics Campus</orgName>
								<address>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.18,115.90,331.00,12.68;1,159.10,133.83,297.16,12.68">A Competitive Deep Neural Network Approach for the ImageCLEFmed Caption 2020 Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7279DF9FFFC3C4AB390E893B8C1F24C6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Imaging</term>
					<term>Concept Detection</term>
					<term>Image Labeling</term>
					<term>Multi-Label Classification</term>
					<term>Deep Convolutional Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The aim of ImageCLEFmed Caption task is to develop a system that automatically labels radiology images with relevant medical concepts. We describe our Deep Neural Network (DNN) based approach for tackling this problem. On the challenge test set of 3,534 radiology images, our system achieves an F1 score of 0.375 and ranks high, 12th among all systems that were successfully submitted to the challenge, whereby we only rely on the provided data sources and do not use any external medical knowledge or ontologies, or pretrained models from other medical image repositories or application domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF organises 4 main tasks for the 2020 edition with a global objective of promoting the evaluation of technologies for annotation, indexing, and retrieval of visual data with the aim of providing information access to large collections of images in various usage scenarios and application domains, including medicine <ref type="bibr" coords="1,176.26,482.96,9.96,8.80" target="#b3">[4]</ref>.</p><p>Interpreting and summarizing the insights gained from medical images is a time-consuming task that involves highly trained experts and often represents a bottleneck in clinical diagnosis pipelines. Consequently, there is a considerable need for automatic methods that can approximate this mapping from visual information to condensed textual descriptions. The more image characteristics are known, the more structured are the radiology scans and hence, the more efficient are the radiologists regarding interpretation, see https://www.imageclef.org/ 2020/medical/caption/ and <ref type="bibr" coords="1,267.26,578.69,14.61,8.80" target="#b11">[11]</ref>.</p><p>Recent years have witnessed tremendous advances in deep neural networks in terms of architectures, optimization algorithms, tooling and techniques for training large networks, and handling multiple modalities (e.g., text, images, videos, speech, etc.). In particular, deep convolutional neural networks have proved to be extremely successful image encoders and have thus become the de facto standard for visual recognition <ref type="bibr" coords="2,253.75,142.84,11.25,8.80" target="#b1">[2,</ref><ref type="bibr" coords="2,265.00,142.84,7.50,8.80" target="#b2">3,</ref><ref type="bibr" coords="2,272.50,142.84,11.25,8.80" target="#b13">13]</ref>. We have been working on machine learning problems in several medical application domains <ref type="bibr" coords="2,344.08,154.80,16.52,8.80" target="#b14">[14,</ref><ref type="bibr" coords="2,360.59,154.80,12.39,8.80" target="#b15">15,</ref><ref type="bibr" coords="2,372.98,154.80,12.39,8.80" target="#b16">16,</ref><ref type="bibr" coords="2,385.37,154.80,12.39,8.80" target="#b17">17]</ref> in our projects, see https://ai-in-medicine.dfki.de/. In this paper, we describe how we built a competitive deep neural network approach based on these projects.</p><p>The rest of the paper is organized as follows. In Section 2, we formally describe the challenge and its goal. In Section 3, we present some statistics on the dataset and explain the approach we adopted to tackle the challenge. In Section 4, we describe the experiments that we conducted with different architectures and introduce a new loss function that addresses the sparsity problem in ground truth labels. In Section 5, results are presented followed by a short discussion. Finally, we summarize our work in Section 6 and discuss some future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Challenge</head><p>The overarching goal of ImageCLEFmed Caption challenge is to assist medical experts such as radiologists in interpreting and summarising information contained in medical images. As a first step towards this goal, a simpler task would be to detect as many key concepts as possible, with the goal that these concepts can then be composed into comprehensible sentences, and eventually into medical reports. For full details about the challenge, we refer the reader to Pelka et al. <ref type="bibr" coords="2,148.60,406.08,14.61,8.80" target="#b11">[11]</ref>.</p><p>The challenge has evolved over the years, since its first edition in 2017, to focus only on radiology images in this year's version, and incorporating the lessons learned from previous years. The aim of 2020 ImageCLEFmed Caption challenge is to develop a system that would automatically assign medical concepts to radiology images that were sorted into 7 different categories (see Table <ref type="table" coords="2,442.76,466.24,3.87,8.80" target="#tab_0">1</ref>). More concretely, given an image (I), the objective is to learn a function F that maps I to a set of concepts (C 1 , C 2 , ..., C v I ) where v I is the number of concepts associated with I. A peculiarity of this challenge is that v k, where k is the total number of unique labels.</p><formula xml:id="formula_0" coords="2,256.96,536.74,223.64,10.33">F : I → (C 1 , C 2 , ..., C v I )<label>(1)</label></formula><p>On the challenge data, the ground truth concepts are not known for images in the test set. The only known constraint is that predictions of the test set must be submitted with a maximum of 100 non-repeating concepts per image.</p><p>The performance of submissions to the challenge are evaluated on a withheld test set of 3,534 radiology images using the F 1 score evaluation metric, which is defined as the harmonic mean of precision and recall values.</p><formula xml:id="formula_1" coords="2,240.33,640.61,240.26,22.31">F 1 = 2 * precision * recall precision + recall<label>(2)</label></formula><p>Firstly, the instance level F1 scores are computed using the predicted concepts for images in the test set. Later, an average F1 score is computed over all images in the test set using scikit-learn<ref type="foot" coords="3,269.39,141.29,3.97,6.16" target="#foot_0">1</ref> library's default binary averaging method. This yields the final F1 score for an accepted submission.</p><p>All registered teams are allowed for a maximum of 10 submissions. Successful submissions and teams are then ranked based on the achieved F1 scores and results are made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We provide information about the dataset and some analysis that we performed on it during the exploratory phase. Then, we discuss our learning approach and a suitable data preparation strategy for training our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>All participants are provided with ImageCLEFmed Caption dataset which is a subset of the ROCO dataset <ref type="bibr" coords="3,265.15,317.58,14.61,8.80" target="#b12">[12]</ref>. It is a multi-modal, medical images dataset containing radiology images that are each labelled with a set of medical concepts, called as Concept Unique Identifiers (CUIs) in the literature. As common in many challenges, the provided images are already split into train, validation, and test sets. Images of the first two sets come with ground truth labels, while the test set contains only images. Moreover, images in each of the splits are sorted into one of the 7 categories as described in tables 1 and 2; such information can be inferred from the name of the sub-directory containing the images. As we can observe from Table <ref type="table" coords="3,455.95,568.01,3.87,8.80" target="#tab_1">2</ref>, the images are not equally distributed across categories, indicating an imbalance in the dataset. The DRCT category, which contains images captured using Computerized Tomography (CT), has the highest representation, followed by X-ray images (DRXR), while the least number of images (approximately 1 40 th of the images in DRCT) are seen in DRCO category. Despite having these category labels as a meta-information, we could not leverage them during model training due to time constraints. Thus, the relationship between CUIs and image category labels is, for us, still uninvestigated. Possibly, in a follow-up work we will investigate on how to exploit such metainformation to improve classification performance of predictive models, for example by using one of the metadata fusion strategies tested by the authors in <ref type="bibr" coords="4,467.31,301.34,9.96,8.80" target="#b9">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Analysis</head><p>Here, we outline some insights that we gained after performing analysis on the CUIs and the category labels meta-information. Furthermore, this section sheds light on the imbalance in the dataset, which is a common problem in many research domains.</p><p>Figure <ref type="figure" coords="4,180.44,398.50,4.98,8.80" target="#fig_0">1</ref> provides a conceptual representation of the input, viz. images paired with relevant concepts. In this case, both images are labelled with four CUIs, the descriptions of which are provided by Unified Medical Language System (UMLS)<ref type="foot" coords="4,170.87,432.81,3.97,6.16" target="#foot_1">2</ref> terms. These terms are depicted in Figure <ref type="figure" coords="4,370.68,434.36,4.98,8.80" target="#fig_0">1</ref> merely for the purpose of understanding since the mapping of CUIs to UMLS terms is not part of the provided dataset.</p><p>Figure <ref type="figure" coords="4,180.37,470.23,4.98,8.80" target="#fig_1">2</ref> shows the frequencies of top 30 CUIs on the training split. Two CUIs (C0040398, C0040405 ), which both occur around 20k times in the training set, dominate this list and represent the concept "computer assisted tomography". Similar behavior is observed on the validation set (see Figure <ref type="figure" coords="4,408.25,506.10,3.87,8.80" target="#fig_2">3</ref>). However, the frequencies of top 2 CUIs in this case are only around 4.6k.</p><p>Figure <ref type="figure" coords="4,182.27,530.01,4.98,8.80" target="#fig_3">4</ref> depicts a histogram representation of the number of images and the CUI counts on the training set. For instance, there are around 5,200 images that have exactly two CUIs as ground truth labels. On the contrary, there are only around 100 images that have exactly 50 CUIs as ground truth labels in the provided training set. This histogram is truncated at CUI count 50 (x-axis) for clarity and uncluttered representation.</p><p>In a similar manner, Figure <ref type="figure" coords="4,273.13,601.74,4.98,8.80" target="#fig_4">5</ref> shows a histogram representation of the number of images and the CUI counts on the validation set. For instance, there are around 1,300 images that have exactly two CUIs as ground truth labels. On the contrary, there are only around 30 images that have exactly 50 CUIs as ground truth labels in the validation set. This histogram is truncated at CUI count 50 (x-axis) for clarity and uncluttered representation.</p><p>On the combined training and validation set, there are 80,723 images and 907,718 non-unique CUIs. Among them, we counted 3,047 unique CUIs, which were used to build the label space in our training objective (see Table <ref type="table" coords="5,440.85,467.80,3.87,8.80" target="#tab_2">3</ref>).</p><p>Following Tsoumakas et al. <ref type="bibr" coords="5,271.84,480.05,14.61,8.80" target="#b18">[18]</ref>, we compute the Label Cardinality (LC) on the combined training and validation set, denoted as D, using the formula:</p><formula xml:id="formula_2" coords="5,261.76,513.81,218.83,31.18">LC(D) = 1 |D| |D| i=1 |Y i |<label>(3)</label></formula><p>In a similar manner, we compute the Label Density (LD) using the following formula:</p><formula xml:id="formula_3" coords="5,260.22,588.89,216.13,31.18">LD(D) = 1 |D| |D| i=1 |Y i | |L| (<label>4</label></formula><formula xml:id="formula_4" coords="5,476.35,600.10,4.24,8.80">)</formula><p>where |L| is the number of unique labels in our multi-label classification objective. The LC and LD scores on the combined training and validation sets are 11.24 and 0.0037 respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data preparation</head><p>As a first step in formatting the labels, we convert CUIs associated with images to a format that is suitable as input for neural network learning. Since our objective here is multi-label classification, we cannot use simple one-hot encoding, as usually done in classification tasks, hence we apply a multi one-hot encoding. An illustration of this representation can be found in Table <ref type="table" coords="6,402.99,535.15,3.87,8.80" target="#tab_2">3</ref>. Specifically, we sort the list of CUIs from the unique label set (k) in alphabetical order and use the positions of CUIs in the sorted list to mark as 1 if a specific CUI is associated with the image in question, else as 0. After this conversion step, the label set for each image (I) is represented as a single multi one-hot vector of fixed size k, which is equal to 3,047. We used only the images from ImageCLEFmed Caption dataset and did not use pre-training on external datasets, or utilize other modalities such as text during model training.</p><p>For our experiments, we divided the validation set via random sampling into two equally sized subsets, namely val1 and val2. We conducted our internal   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning Approach</head><p>The prediction problem for this challenge lays in the category of multi-label classification <ref type="bibr" coords="8,195.11,452.63,14.61,8.80" target="#b18">[18]</ref>. It differs from most common classification problems in the fact that each sample of the dataset is simultaneously associated with more than one class from the ground truth label pool. Technically, when addressing such a problem with deep neural networks, it means that the final classification layer relies on multiple sigmoidal units rather than a single softmax probability distribution. The last layer of the network contains one sigmoidal unit for each of the target classes, and the association with a true/false result is performed by thresholding the final sigmoid activation value (usually at 0.5).</p><p>To address the challenge, we followed a classical transfer learning approach starting from a Convolutional Neural Network (CNN) model pre-trained on an image classification problem, namely ImageNet, because the pre-trained network already offers the ability to detect basic image features, viz. edges, borders, and corners. Then, the final classification stage of the network (i.e., all the layers after the last convolutional layer) is substituted with randomly initialized fully connected layers. Finally, the network is fitted for the new target training set.</p><p>In detail, we used VGG16 <ref type="bibr" coords="8,271.85,644.10,14.61,8.80" target="#b13">[13]</ref>, ResNet50 <ref type="bibr" coords="8,340.47,644.10,9.96,8.80" target="#b1">[2]</ref>, and DenseNet169 <ref type="bibr" coords="8,439.84,644.10,9.96,8.80" target="#b2">[3]</ref>, all of them pre-trained on ImageNet data used for ILSVRC <ref type="bibr" coords="8,373.71,656.06,9.96,8.80" target="#b5">[6]</ref>. An example configu- Remaining layers, from flatten to predictions, are newly instantiated and initialized randomly (where applicable). The final predictions layer is a dense layer with 3,047 sigmoidal activation units (one per target class).</p><p>The system was developed in a Python environment using Keras deep learning framework <ref type="bibr" coords="9,200.88,495.93,10.51,8.80" target="#b0">[1]</ref> with TensorFlow <ref type="bibr" coords="9,291.77,495.93,10.51,8.80" target="#b7">[7]</ref> as the backend. For our experiments we used a desktop machine equipped with an 8-core 9 th -gen i7 CPU, 64GB RAM, and NVIDIA RTX TITAN 24GB GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe our experimental procedure, model configuration, and a variety of deep CNN architectures that we tried for achieving the multi-label classification task.</p><p>Table <ref type="table" coords="9,177.45,620.19,4.98,8.80" target="#tab_3">4</ref> reports the results of the experiment we conducted throughout the challenge. Because of time constraints, rather than running a grid search for the best hyper-parameter values, we started from a reference configuration, already successfully used in other past works <ref type="bibr" coords="9,298.39,656.06,11.15,8.80" target="#b9">[9,</ref><ref type="bibr" coords="9,309.53,656.06,11.15,8.80" target="#b10">10]</ref>.  </p><formula xml:id="formula_5" coords="10,126.39,122.93,296.55,13.78">1 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 2 Layer (</formula><formula xml:id="formula_6" coords="10,123.01,146.84,299.94,197.09">0 5 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 6 block1_conv1 ( Conv2D ) ( None , 227 , 227 , 64) 1792 7 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 8 block1_conv2 ( Conv2D ) ( None , 227 , 227 , 64) 36928 9 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 10 block1_pool ( MaxPooling2D ) ( None , 113 , 113 , 64) 0 11 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 12 [... 13 more layers ...] 13 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 14 block5_conv3 ( Conv2D ) ( None , 14 , 14 , 512) 2359808 15 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 16 block5_pool ( MaxPooling2D ) ( None , 7 , 7 , 512) 0 17 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 18 flatten ( Flatten ) ( None , 25088) 0 19 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 20 fc1 ( Dense ) ( None , 4096) 102764544 21 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 22 dropout_1 ( Dropout ) ( None , 4096) 0 23 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 24 fc2 ( Dense ) ( None , 4096) 16781312 25 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 26 dropout_2 ( Dropout ) ( None , 4096) 0 27 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 28 predictions ( Dense ) ( None ,</formula><formula xml:id="formula_7" coords="10,123.01,377.98,299.94,5.81">33 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</formula><p>Listing 1.1: An excerpt of the VGG16 architecture used for the multi-label classification task.</p><p>Together with the base architecture used for convolution (CNN arch) we report: (res) the resolution in pixels of the input images of equal height and width, (aug) the data augmentation strategy, (fc layers) the configuration of final fully-connected stage of the CNN architectures, (do) the dropout value after each fully connected layer, (bs) the batch size used for training, (loss func) the loss function used for optimization, (lr-red) the learning rate reduction strategy (reduction factor/patience/monitored metric). Additionally, we report the best training epoch, based on an early stopping criteria by monitoring the F1 score on the validation set. The last three columns report the F1 scores achieved on the two internal cross-validation sets and finally on the AIcrowd<ref type="foot" coords="10,396.60,564.05,3.97,6.16" target="#foot_2">3</ref> online submission platform.</p><p>Other training parameters, common to all configurations are: NAdam optimizer, learning rate = 1e-5, and schedule decay 0.9. All images were scaled to the input resolution of the CNN using nearest filtering, without any cropping.</p><p>In the following, we report on the evolution of our tests and obtained results. Experiments 1-4: VGG16 baseline We started with (1) a VGG16 architecture, pretrained on ImageNet, with the last two fully connected (FC) layers configured with n=2048 nodes, each followed by a dropout layer with the dropout probability p set to 0.5.</p><p>(2) We observed an increase in performance by increasing the size of the FC layers to 4096 nodes.</p><p>From the first two experiments, it was evident how the loss value (based on binary cross-entropy) could not be effectively used to monitor the validation. The ground truth of each sample, a vector of size 3,047, contains on average about 11 concepts per image (see Section 3.2), and only a few images contain more than 50 concepts. Hence, the ground truth matrix is very sparse. As a consequence, the loss function quickly stabilizes into a plateau, as does the accuracy, which saturates to values above 0.9966 after the first epoch. Hence, to better handle early stopping, we implemented a training-time computation of the F1 score.</p><p>(3) A further improvement was observed by applying a 2X data augmentation of the input dataset. Each image is provided to the training procedure both as-it-is and flipped horizontally. At the same time, learning rate reduction was applied by monitoring F1 scores on the validation set, rather than the loss values. However, increasing the learning rate happens only after an overfitting occurs. This configuration led to an online evaluation score of 0.363. Figure <ref type="figure" coords="11,430.43,529.36,4.98,8.80" target="#fig_7">6</ref> shows the evolution of loss values and F1 scores over epochs during model training.</p><p>(4) We tried to improve the performance by increasing the size of input images to 450x450 pixels, which forced a reduction of the batch size to 24. We could not observe any significant improvement in the accuracy, suggesting that higher image resolutions do not provide useful details for label selection in our case.</p><p>Experiments 5-7: more powerful CNN architectures By using deeper CNN architectures, we could observe a slight improvement in the test accuracy. Indeed, the ResNet50 architecture (5) led to an F1 score of 0.365 in the online evaluation. The DenseNet169 architecture (6-7) led to higher test values, but the online evaluation was slightly lower (0.360) than the ResNet50 version.</p><p>Experiment 8: more layers In order to increase the overall performance, we tried to increase the number of FC layers to 3x4k <ref type="bibr" coords="12,376.97,351.92,11.62,8.80" target="#b8">(8)</ref>. However, taking as reference the performance of configuration (3), we could not observe a significant improvement by introducing an additional 4k FC layer to the classification stage.</p><p>Experiments 9-10: a new loss function To further improve performance, we decided to directly optimize for the F1 score evaluation metric. Notice that the F1 score used in the ImageCLEF challenge is computed as an average F1 over the samples (and not over the labels, as more often found in online code repositories 4,5 ). We implemented a loss function F 1 = 1 -sF 1, where sF 1 is called the "soft F1 score". The sF 1 is a differentiable version of the F1 function that computes true positives, false positives, and false negatives as continuous sum of likelihood values, without applying any thresholding to round the probabilities to 0 or 1. The implementation of F 1 is shown in listing 1.2.</p><p>Experiments using the F 1 loss function could not converge. Likely, the problem is due to the fact that the F 1 loss lies in the range [0, 1]. As such, the gradient search space can be abstractly seen as a huge plateau, just below 1.0, with a solitary hole in the middle that quickly converges to the global minimum. At the same level of abstraction, we can visualize the binary cross-entropy bce search space as a wide bag, with a large flat surface, just above 0. It is easy to reach the bottom of the bag, i.e., reach a very high binary accuracy due to the sparsity of the labeling, but then we see a very mild slope towards the global import keras . backend as K 4 import tensorflow as tf   Our intuition is that by combining (multiplying or adding) F 1 and bce results in a search space where F 1 does not affect the identification of inside of the bag, and at the same time helps with the identification of the F 1's and bce's common global minimum. The implementation of the F 1 * bce loss function is straightforward and is presented in listing 1.3.</p><p>(9) An experiment using VGG16 confirms that the loss function F 1 * bce leads to better results with 0.3604/0.3606 on our tests. This is the configuration that performed best in the online submission (0.374).</p><p>Further experiments, e.g., using ResNet50, could not be submitted to the challenge due to time constraints. However, (10) an internal test using VGG16 in combination with the F 1 + bce loss function, led to the best performance in our internal evaluation (0.3636/0.3632).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>The results of our experiments for the ImageCLEFmedical 2020 challenge can be summarized as follows.</p><p>The task of concept detection can be modeled as a multi-labeling problem and solved by a transfer learning approach where deep CNNs pretrained on real-world images can be fine-tuned on the target dataset. The multi-labeling is technically addressed by using a sigmoid activation function on the output layer and a label selection by thresholding. A good configuration consists of a VGG16 deep CNN architecture followed by two fully connected layers of 4096 nodes, each followed by a dropout layer with probability p set to 0.5. Augmenting the training set with horizontally flipped images increases accuracy and also reduces the number of epochs needed for training. Increasing the resolution of input images does not prove to be useful, while better results are achieved by substituting the convolution stage with a deeper CNN architecture (ResNet50). We noticed that the learning rate reduction has never helped in improving the results.</p><p>Using the standard binary cross-entropy loss function leads to competitive results, which significantly increases when it is combined with a soft-F1 score computation. It is worth noticing that when using solely the soft-F1 score as a loss function, the network could not converge and this problem needs further investigation.</p><p>In total, we made five online submissions to the challenge. Table <ref type="table" coords="14,436.79,584.33,4.98,8.80" target="#tab_4">5</ref> presents the F1 scores achieved on the withheld test set and the overall ranking of our team (iml ) out of 47 successful submissions, as reported by the challenge organizers. In addition, it is worth mentioning that the difference in F1 scores between our best submission and the system that achieved the highest score in the challenge is 0.0195. What percentage of test set images on which our model still needs to achieve correct labels to bridge this gap needs further investigation.</p><p>In this work we have proposed a deep convolutional neural network based approach for concept detection in radiology images. Our best performance (12 th position) is achieved by implementing a new loss function whereby we combined the widely used binary cross-entropy loss together with a differentiable version of the F1 score evaluation metric.</p><p>Still several aspects could be investigated to improve the achieved results. For instance, as we can observe from the CUI distribution plots, there is an imbalance in the dataset. Consequently the model is biased towards predicting the concepts associated with over-represented samples. Our future work will focus on the approaches to combat such type of biases. A straightforward approach would be to undersample the over-represented samples using a query strategy that maximizes informativeness of chosen samples. Such a strategy showed promising results for incremental domain adaptation task in neural machine translation <ref type="bibr" coords="15,467.31,283.60,9.96,8.80" target="#b4">[5]</ref>.</p><p>Furthermore, we did not make use of the existing categorization of the images in 7 sub-sets. A straightforward idea would be to train 7 different models, one per category, and rely on their ensemble for a final global classification result. Alternatively, the class identifier might be used as additional metadata information, concatenated to the images' internal features representation in the CNN, and fed to a further shallow neural network for improved classification (see <ref type="bibr" coords="15,463.44,355.33,10.29,8.80" target="#b9">[9]</ref>). Another promising direction to look would be to consider further trends in the integration of vision and language research <ref type="bibr" coords="15,324.75,379.24,9.96,8.80" target="#b8">[8]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,185.25,370.93,244.87,8.80"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Sample images &amp; their CUIs from the train split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,189.23,426.45,236.89,8.80"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Histogram of top 30 CUIs on the training split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,185.03,424.16,245.30,8.80"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Histogram of top 30 CUIs on the validation split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,134.77,372.11,345.83,8.80;8,134.77,384.06,143.96,8.80"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Histogram of CUI counts vs. Images Count on the training set after combining all of the 7 categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,134.77,373.65,345.83,8.80;9,134.77,385.60,143.96,8.80"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Histogram of CUI counts vs. Images Count on the validation set after combining all of the 7 categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,166.47,130.90,21.26,5.81;10,264.30,130.90,52.08,5.81;10,379.84,130.90,30.12,5.81;10,126.39,138.87,296.55,5.81;10,126.39,146.84,96.89,5.81;10,264.03,146.84,83.59,5.81"><head></head><label></label><figDesc>= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 4 input_1 ( InputLayer ) ( None , 227 , 227 , 3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="10,295.39,338.13,20.99,5.81;10,379.88,338.13,34.25,5.81;10,123.01,346.10,299.94,5.81;10,123.01,354.07,122.29,5.81;10,123.01,362.04,140.07,5.81;10,123.01,370.01,113.62,5.81"><head></head><label></label><figDesc>3047) 12483559 29 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 30 Total params : 146 ,744 ,103 31 Trainable params : 146 ,744 ,103 32 Non -trainable params : 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="12,134.77,242.92,345.83,8.80;12,134.77,254.88,345.83,8.80;12,134.77,266.83,345.83,8.80"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Plots of the metrics computed on the validation set during training: (left) binary cross-entropy and (right) F1 score. Here, epochs are counted from 0. The best validation results are achieved at epoch 9, then the model starts overfitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="13,126.39,155.64,3.39,4.40;13,126.39,163.61,3.39,4.40;13,152.91,162.78,168.16,5.81"><head>5 6#</head><label>5</label><figDesc>The following is not d iffere ntiabl e .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="13,126.39,171.58,3.39,4.40;13,152.91,170.75,212.61,5.81;13,126.39,179.55,3.39,4.40;13,152.91,178.72,114.82,5.81;13,126.39,187.52,3.39,4.40;13,152.91,186.69,234.83,5.81"><head>7 # 8 # 9 #</head><label>789</label><figDesc>Round the prediction to 0 or 1 (0.5 threshold ) y_pred = K . round ( y_pred ) By commenting , we implement what is called soft -F1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="13,123.01,195.49,6.78,4.40;13,123.01,203.46,6.78,4.40;13,152.91,202.63,137.05,5.81;13,123.01,211.43,6.78,4.40;13,153.04,210.60,234.58,5.81;13,123.01,219.40,6.78,4.40;13,152.91,218.57,296.93,5.81;13,123.01,227.37,6.78,4.40;13,153.04,226.54,261.25,5.81;13,123.01,235.34,6.78,4.40;13,153.04,234.51,261.25,5.81;13,123.01,243.31,6.78,4.40;13,123.01,251.28,6.78,4.40;13,152.91,250.45,141.49,5.81;13,123.01,259.25,6.78,4.40;13,152.91,258.42,141.49,5.81;13,123.01,267.22,6.78,4.40;13,123.01,275.19,6.78,4.40;13,152.91,274.36,145.94,5.81;13,123.01,283.16,6.78,4.40;13,153.04,282.33,168.04,5.81;13,123.01,291.13,6.78,4.40;13,153.04,290.30,225.82,5.81;13,123.01,299.10,6.78,4.40;13,153.18,298.27,92.33,5.81;13,134.77,320.78,187.69,8.80;13,362.71,320.78,117.88,8.80;13,134.77,332.73,199.81,8.80;13,126.39,376.32,172.46,5.81;13,126.39,385.12,3.39,4.40;13,153.18,384.29,110.11,5.81;13,126.39,393.09,3.39,4.40;13,126.39,401.06,3.39,4.40;13,153.19,400.23,181.22,5.81;13,126.39,409.03,3.39,4.40;13,153.10,408.20,310.21,5.81;13,155.31,416.17,25.68,5.81;13,153.18,432.11,87.71,5.81;13,134.77,454.61,345.83,8.80;13,134.77,466.57,113.36,8.80;13,134.77,516.88,99.68,8.80"><head>10 11 # 13 # 17 p 3 4</head><label>101113173</label><figDesc>Compute precision and recall . 12 tp = K . sum ( K . cast ( y_true * y_pred , ' float ') , axis = -1) tn = K . sum ( K . cast ((1 -y_true ) * (1 -y_pred ) , ' float ') , axis = -1) 14 fp = K . sum ( K . cast ((1 -y_true ) * y_pred , ' float ') , axis = -1) 15 fn = K . sum ( K . cast ( y_true * (1 -y_pred ) , ' float ') , axis = -1) 16 = tp / ( tp + fp + K . epsilon () ) 18 r = tp / ( tp + fn + K . epsilon () ) 19 20 # Compute F1 and return the loss . 21 f1 = 2 * p * r / ( p + r + K . epsilon () ) 22 f1 = tf . where ( tf . is_nan ( f1 ) , tf . zeros_like ( f1 ) , f1 ) 23 return 1 -K . mean ( f1 ) Listing 1.2: Python implementation of the based loss function for the Keras environment with TensorFlow backend. 1 def l o s s _ 1 m f 1 _ b y _ b c e ( y_true , y_pred ) : 2 import keras . backend as K loss_f1 = l o ss _1 _ mi nu s _f 1 ( y_true , y_pred ) 5 bce = K . b i n a r y _ c r o s s e n t r o p y ( target = y_true , output = y_pred , from_logits = False ) return loss_f1 * bce Listing 1.3: Python implementation of the loss function combining soft-F1 score with binary cross-entropy. minimum at its center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,200.35,408.53,214.67,108.01"><head>Table 1 :</head><label>1</label><figDesc>Terminology used in "ROCO" dataset.</figDesc><table coords="3,200.35,427.69,214.67,88.84"><row><cell>Abbreviation</cell><cell>Full form</cell></row><row><cell>DRAN</cell><cell>DR Angiography</cell></row><row><cell>DRCO</cell><cell>DR Combined modalities in One image</cell></row><row><cell>DRCT</cell><cell>DR Computerized Tomography</cell></row><row><cell>DRMR</cell><cell>DR Magenetic Resonance</cell></row><row><cell>DRPE</cell><cell>DR Positron Emission Tomography</cell></row><row><cell>DRUS</cell><cell>DR Ultrasound</cell></row><row><cell>DRXR</cell><cell>DR X-Ray, 2D Tomography</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,163.60,127.30,288.15,85.17"><head>Table 2 :</head><label>2</label><figDesc>Splits and statistics of ImageCLEFmed Caption dataset.</figDesc><table coords="4,177.24,148.31,260.87,64.16"><row><cell cols="7">Total Number of Images in the Category of Split DRAN DRCO DRCT DRMR DRPE DRUS DRXR</cell><cell>Total</cell></row><row><cell cols="8">Train 4,713 487 20,031 11,447 502 8,629 18,944 64,753</cell></row><row><cell>Val</cell><cell>1,132</cell><cell>73</cell><cell>4,992 2,848</cell><cell>74</cell><cell cols="3">2,134 4,717 15,970</cell></row><row><cell>Test</cell><cell>325</cell><cell>49</cell><cell>1,140 562</cell><cell>38</cell><cell>502</cell><cell cols="2">918 3,354</cell></row><row><cell cols="8">Total 6,170 609 26,163 14,857 614 11,265 24,579 84,077</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,134.77,467.54,345.83,197.31"><head>Table 3 :</head><label>3</label><figDesc>Encoding CUIs using multi one-hot representation.evaluations by always training pairs of models, first using val1 for validation and val2 for testing, and then vice-versa. When results were promising, we then submitted our predictions on the test set images by using the model trained with first configuration. Training a third model, based on the validation on the full development set would have been the ideal solution. However, this could not be applied in our case because of time constraints.</figDesc><table coords="8,157.36,347.29,317.73,9.65"><row><cell>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Count of Concept Unique Identifiers (CUIs)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,138.52,127.30,335.64,139.62"><head>Table 4 :</head><label>4</label><figDesc>The list of experiments conducted for the challenge.</figDesc><table coords="11,138.52,151.32,335.64,115.60"><row><cell cols="3"># CNN arch res aug. fc</cell><cell>do bs loss</cell><cell>lr-red.</cell><cell>test1</cell><cell>test2</cell><cell>score</cell></row><row><cell></cell><cell>(px)</cell><cell>layers</cell><cell>func</cell><cell></cell><cell>ep F1</cell><cell>ep F1</cell><cell>F1</cell></row><row><cell>1 VGG16</cell><cell cols="3">227 none 2x2k 0.5 32 bce</cell><cell>0.2/3/loss</cell><cell>12 0.333</cell><cell>22 0.335</cell></row><row><cell>2 VGG16</cell><cell cols="3">227 none 2x4k 0.5 32 bce</cell><cell>0.2/3/loss</cell><cell>25 0.346</cell><cell>26 0.336</cell></row><row><cell>3 VGG16</cell><cell cols="3">227 hflip 2x4k 0.5 32 bce</cell><cell>0.2/5/f1</cell><cell>9 0.3475</cell><cell>9 0.3455 0.363</cell></row><row><cell>4 VGG16</cell><cell cols="3">450 hflip 2x4k 0.5 24 bce</cell><cell>0.2/5/f1</cell><cell>n/a crash</cell><cell>6 0.3417</cell></row><row><cell>5 ResNet50</cell><cell cols="3">224 hflip 2x4k 0.5 16 bce</cell><cell>nothing</cell><cell>3 0.3484</cell><cell>3 0.3487 0.365</cell></row><row><cell cols="4">6 DenseNet169 224 none 2x4k 0.5 32 bce</cell><cell>nothing</cell><cell cols="2">8 0.3495 10 0.3450</cell></row><row><cell cols="4">7 DenseNet169 224 hflip 2x4k 0.5 32 bce</cell><cell>nothing</cell><cell>5 0.3500</cell><cell>4 0.3463 0.360</cell></row><row><cell>8 VGG16</cell><cell cols="3">227 hflip 3x4k 0.5 32 bce</cell><cell>0.2/5/f1</cell><cell cols="2">13 0.3465 14 0.3433</cell></row><row><cell>9 VGG16</cell><cell cols="4">227 hflip 2x4k 0.5 48 F 1  *  bce 0.2/5/f1</cell><cell>11 0.3604</cell><cell>7 0.3606 0.374</cell></row><row><cell>10 VGG16</cell><cell cols="4">227 hflip 2x4k 0.5 48 F 1 + bce 0.2/5/f1</cell><cell cols="2">14 0.3636 12 0.3632</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,147.21,127.29,320.95,87.94"><head>Table 5 :</head><label>5</label><figDesc>F1 scores for submissions by our team 'iml' .</figDesc><table coords="14,147.21,148.31,320.95,66.92"><row><cell>AIcrowd Submission Run</cell><cell>F1 Score</cell><cell>Rank</cell></row><row><cell cols="3">imageclefmed2020-test-vgg16-f1-bce-nomissing-iml.txt 0.374525478882926 12</cell></row><row><cell>imageclefmed2020-test-vgg16-f1-bce-iml.txt</cell><cell cols="2">0.374402134956526 13</cell></row><row><cell>imageclefmed2020-test-resnet50-iml.txt</cell><cell cols="2">0.365168555515581 17</cell></row><row><cell>imageclefmed2020-test-vgg16-iml.txt</cell><cell cols="2">0.363067945861981 18</cell></row><row><cell>imageclefmed2020-test-densenet169-iml.txt</cell><cell cols="2">0.360156086299303 19</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,152.70,646.48,325.23,7.47;3,134.77,657.44,47.06,7.47"><p>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_ score.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,152.70,657.44,178.84,7.47"><p>https://www.nlm.nih.gov/research/umls/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="10,152.70,646.48,312.11,7.47;10,134.77,657.44,42.36,7.47"><p>https://www.aicrowd.com/challenges/imageclef-2020-caption-conceptdetection</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="12,152.70,624.57,323.01,7.47;12,134.77,635.53,218.97,7.47"><p>https://towardsdatascience.com/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="12,148.55,644.10,3.65,5.22;12,152.70,646.48,303.69,7.47;12,134.77,657.44,28.24,7.47;13,126.39,122.93,168.01,5.81"><p><ref type="bibr" coords="12,148.55,644.10,3.65,5.22" target="#b4">5</ref> https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-scoremetric 1 def l os s _1 _m i nu s_ f 1 ( y_true , y_pred ) :</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="15,142.95,428.09,244.72,8.97" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Keras</forename><surname>Special</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Interest</forename><surname>Group</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Keras</surname></persName>
		</author>
		<ptr target="https://keras.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.95,438.37,337.64,8.97;15,151.52,449.33,329.07,8.97;15,151.52,460.29,329.08,8.97;15,151.52,471.25,123.51,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,391.24,438.37,89.35,8.97;15,151.52,449.33,83.29,8.97">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,253.54,449.88,227.05,7.92;15,151.52,460.84,98.50,7.92">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.95,481.53,337.64,8.97;15,151.52,492.49,329.07,8.97;15,151.52,504.00,329.08,7.92;15,151.52,514.41,220.18,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,151.52,492.49,165.79,8.97">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,335.21,493.04,145.37,7.92;15,151.52,504.00,186.53,7.92">2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">July 21-26, 2017. 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.95,524.69,337.63,8.97;15,151.52,535.65,329.07,8.97;15,151.52,546.61,329.07,8.97;15,151.52,557.57,329.07,8.97;15,151.52,568.53,329.07,8.97;15,151.52,579.49,329.07,8.97;15,151.52,590.44,329.07,8.97;15,151.52,601.40,329.07,8.97;15,151.52,612.36,329.07,8.97;15,151.52,623.32,329.07,8.97;15,151.52,634.28,329.07,8.97;15,151.52,645.24,329.07,8.97;15,151.52,656.20,36.38,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,248.46,601.40,232.13,8.97;15,151.52,612.36,215.23,8.97">Overview of the ImageCLEF 2020: Multimedia retrieval in medical, lifelogging, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Van-Tu</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tu-Khiem</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liting</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pål</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Triet</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dimitri</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raul</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liviu</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Daniel Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Constantin</forename><surname>Gabriel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,387.43,612.91,93.15,7.92;15,151.52,623.87,190.31,7.92;15,422.64,623.87,57.95,7.92;15,151.52,634.83,291.45,7.92">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="15,307.10,645.24,169.52,8.97">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="16,142.95,119.07,337.63,8.97;16,151.52,130.03,329.07,8.97;16,151.52,140.99,329.08,8.97;16,151.52,151.95,329.08,8.97;16,151.52,162.91,329.08,8.97;16,151.52,173.87,133.31,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,399.53,119.07,81.06,8.97;16,151.52,130.03,264.18,8.97">Incremental domain adaptation for neural machine translation in low-resource settings</title>
		<author>
			<persName coords=""><forename type="first">Marimuthu</forename><surname>Kalimuthu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Barz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Sonntag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,184.44,152.50,296.15,7.92;16,151.52,163.46,80.67,7.92">Proceedings of the Fourth Arabic Natural Language Processing Workshop, WANLP@ACL 2019</title>
		<editor>
			<persName><forename type="first">Wassim</forename><surname>El-Hajj</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lamia</forename><surname>Hadrich Belguith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Imed</forename><surname>Zitouni</surname></persName>
		</editor>
		<meeting>the Fourth Arabic Natural Language Processing Workshop, WANLP@ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-01">August 1, 2019. 2019</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.95,185.84,337.63,8.97;16,151.52,196.79,329.07,8.97;16,151.52,207.75,329.08,8.97;16,151.52,218.71,119.47,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,385.12,185.84,95.47,8.97;16,151.52,196.79,168.10,8.97">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,303.14,208.30,177.46,7.92;16,151.52,219.26,31.98,7.92">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,274.05,218.71,120.74,8.97" xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Inc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.95,230.68,337.64,8.97;16,151.52,241.64,329.07,8.97;16,151.52,252.60,329.07,8.97;16,151.52,263.56,329.07,8.97;16,151.52,274.52,329.07,8.97;16,151.52,285.47,329.07,8.97;16,151.52,296.43,329.08,8.97;16,151.52,307.39,329.07,8.97;16,151.52,318.35,257.33,8.97" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="16,430.96,307.94,49.63,7.92;16,151.52,318.90,227.94,7.92">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</title>
		<author>
			<persName coords=""><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dandelion</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.95,330.32,337.64,8.97;16,151.52,341.28,329.07,8.97;16,151.52,352.24,118.95,8.97" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="16,419.70,330.32,60.90,8.97;16,151.52,341.28,324.49,8.97">Trends in integration of vision and language research: A survey of tasks, datasets, and methods</title>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Mogadala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marimuthu</forename><surname>Kalimuthu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<idno>CoRR, abs/1907.09358</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.95,364.20,337.63,8.97;16,151.52,375.16,329.07,8.97;16,151.52,386.12,329.07,8.97;16,151.52,397.08,329.07,8.97;16,151.52,408.04,20.99,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,217.07,375.16,263.52,8.97;16,151.52,386.12,131.23,8.97">A study on the fusion of pixels and patient metadata in CNN-based classification of skin lesion images</title>
		<author>
			<persName coords=""><forename type="first">Fabrizio</forename><surname>Nunnari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chirag</forename><surname>Bhuvaneshwara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abraham</forename><surname>Obinwanne Ezema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Sonntag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,300.29,386.67,180.30,7.92;16,151.52,397.63,251.28,7.92">International IFIP Cross Domain Conference for Machine Learning and Knowledge Extraction (CD-MAKE)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08">August 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,420.01,337.97,8.97;16,151.52,430.97,246.51,8.97" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Fabrizio</forename><surname>Nunnari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Sonntag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08187</idno>
		<idno>arXiv: 1908.08187</idno>
		<title level="m" coord="16,303.99,420.01,173.05,8.97">A CNN toolbox for skin cancer classification</title>
		<imprint>
			<date type="published" when="2019-08">August 2019</date>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct coords="16,142.61,442.93,337.98,8.97;16,151.52,453.89,329.07,8.97;16,151.52,464.85,329.07,8.97;16,151.52,475.81,244.91,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,186.75,453.89,293.84,8.97;16,151.52,464.85,81.30,8.97">Overview of the ImageCLEFmed 2020 concept prediction task: Medical image understanding</title>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="16,250.06,464.85,226.32,8.97">CLEF2020 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,487.78,337.97,8.97;16,151.52,498.74,329.07,8.97;16,151.52,510.24,329.08,7.92;16,151.52,520.66,317.39,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,195.67,498.74,266.66,8.97">Radiology objects in context (roco): A multimodal image dataset</title>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felix</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,151.52,510.24,329.08,7.92;16,151.52,521.20,186.89,7.92">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,532.62,337.98,8.97;16,151.52,543.58,329.07,8.97;16,151.52,555.09,329.07,7.92;16,151.52,565.50,254.81,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,327.40,532.62,153.19,8.97;16,151.52,543.58,116.51,8.97">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,467.77,544.13,12.81,7.92;16,151.52,555.09,275.27,7.92">3rd International Conference on Learning Representations, ICLR 2015</title>
		<title level="s" coord="16,260.48,566.05,117.76,7.92">Conference Track Proceedings</title>
		<editor>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,577.47,337.98,8.97;16,151.52,588.43,148.84,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,224.09,577.47,256.50,8.97;16,151.52,588.43,41.48,8.97">AI in Medicine, Covid-19 and Springer Nature&apos;s Open Access Agreement</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Sonntag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,201.70,588.97,8.79,7.92">KI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="125" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,600.39,337.98,8.97;16,151.52,611.35,329.07,8.97;16,151.52,622.31,245.67,8.97" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="16,425.66,600.39,54.94,8.97;16,151.52,611.35,329.07,8.97;16,151.52,622.31,44.42,8.97">The Skincare project, an interactive deep learning system for differential diagnosis of malignant skin lesions</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Sonntag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabrizio</forename><surname>Nunnari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hans-Jürgen</forename><surname>Profitlich</surname></persName>
		</author>
		<idno>CoRR, abs/2005.09448</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="16,142.61,634.28,337.98,8.97;16,151.52,645.24,329.08,8.97;16,151.52,656.20,235.53,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,333.32,634.28,147.27,8.97;16,151.52,645.24,329.08,8.97;16,151.52,656.20,25.57,8.97">An architecture of open-source tools to combine textual information extraction, faceted search and information visualisation</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Sonntag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hans-Jürgen</forename><surname>Profitlich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,184.83,656.74,133.44,7.92">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="13" to="28" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,119.07,337.97,8.97;17,151.52,130.03,329.07,8.97;17,151.52,140.99,329.07,8.97;17,151.52,151.95,329.08,8.97;17,151.52,162.91,308.34,8.97" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="17,363.12,151.95,117.48,8.97;17,151.52,162.91,129.69,8.97">The clinical data intelligence project -A smart data initiative</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Sonntag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sonja</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Zillner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">André</forename><surname>Hammon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Fasching</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Sedlmayr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hans-Ulrich</forename><surname>Ganslandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klemens</forename><surname>Prokosch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Budde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carl</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Hinrichs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Wittenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patricia</forename><forename type="middle">G</forename><surname>Daumke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Oppelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,288.78,163.46,81.17,7.92">Informatik Spektrum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="290" to="300" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,173.87,337.98,8.97;17,151.52,184.83,329.07,8.97;17,151.52,195.79,20.99,8.97" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="17,351.58,173.87,129.01,8.97;17,151.52,184.83,35.74,8.97">Multi-Label Classification: An Overview</title>
		<author>
			<persName coords=""><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,194.88,185.37,219.07,7.92">International Journal of Data Warehousing and Mining</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007-07">July 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
