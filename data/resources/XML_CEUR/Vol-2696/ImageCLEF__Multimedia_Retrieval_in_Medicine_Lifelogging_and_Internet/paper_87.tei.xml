<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.34,115.96,320.67,12.62;1,149.69,133.89,315.98,12.62;1,259.43,151.82,96.50,12.62">bumjun jung at VQA-Med 2020: VQA model based on feature extraction and multi-modal feature fusion</title>
				<funder ref="#_9p7WA7h">
					<orgName type="full">JSPS KAKENHI</orgName>
				</funder>
				<funder ref="#_ReTRhWE">
					<orgName type="full">JST</orgName>
				</funder>
				<funder ref="#_c6PdAzY #_rnPyHwh">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,198.60,189.65,59.50,8.74"><forename type="first">Bumjun</forename><surname>Jung</surname></persName>
							<email>jung@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.65,189.65,31.21,8.74"><forename type="first">Lin</forename><surname>Gu</surname></persName>
							<email>lingu@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">RIKEN AIP</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.13,189.65,69.82,8.74"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<email>harada@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">RIKEN AIP</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.34,115.96,320.67,12.62;1,149.69,133.89,315.98,12.62;1,259.43,151.82,96.50,12.62">bumjun jung at VQA-Med 2020: VQA model based on feature extraction and multi-modal feature fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">15E1B7CD2F3F12D1EEC6E534A6C99A8F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>Medical Imagery</term>
					<term>Global Average Pooling</term>
					<term>bioBERT</term>
					<term>Multi-modal Factorized High-order Pooling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the submission of University of Tokyo for Medical Domain Visual Question Answering (VQA-Med) task [3] at ImageCLEF 2020 <ref type="bibr" coords="1,250.19,297.13,13.52,7.86" target="#b10">[11]</ref>. The data set for the task mostly consists of Medical Images and Question Answer pair considering the abnormality appeared in the images. We extract visual features by VGG16 network <ref type="bibr" coords="1,163.11,330.01,14.34,7.86" target="#b15">[16]</ref> with Global Average Pooling (GAP) <ref type="bibr" coords="1,334.13,330.01,13.52,7.86" target="#b13">[14]</ref>. Compared to the model <ref type="bibr" coords="1,163.11,340.97,14.34,7.86" target="#b17">[18]</ref> that ranked first in last year's competition that used BERT [6] model to encode semantic features of questions, we used bioBERT model <ref type="bibr" coords="1,435.35,351.93,13.52,7.86" target="#b12">[13]</ref>, which is a BERT model pre-trained by biomedical textual data. We also apply multi-modal Factorized High-order (MFH) Pooling <ref type="bibr" coords="1,400.75,373.85,14.34,7.86" target="#b19">[20]</ref> with coattention which shows higher performance than Multi-modal Factorized Bilinear (MFB) Pooling [19]  used in <ref type="bibr" coords="1,310.78,395.76,13.52,7.86" target="#b17">[18]</ref>, to fuse two feature modalities. The fused features are then fed to a decoder to predict the answer in a manner of classification. The score of our model is 0.466 in accuracy, 0.502 in BLEU score, and ranked 3rd among all the participating teams in the VQA-Med task [3] at ImageCLEF 2020 <ref type="bibr" coords="1,351.70,439.60,13.52,7.86" target="#b10">[11]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With many achievements and rapid progress in the field of Artificial Intelligence (AI) related to Computer Vision (CV) and Natural Language Processing (NLP), recently the AI technology is applied in the medical domain to analyze the pathological images and medical reports. To be specific, it is used to detect abnormalities or symptoms shown in the images or to generate explanations regarding the medical images.</p><p>Visual Question Answering (VQA) task involves both CV and NLP techniques to process the data. VQA data set is comprised of both Images and Question Answer (QA) pairs about the medical images. The images and questions become the inputs to VQA system, whose goal is to predict the answers for the given questions.</p><p>Large-scale data sets of VQA for general domain <ref type="bibr" coords="2,378.58,154.86,9.96,8.74" target="#b1">[2]</ref>, <ref type="bibr" coords="2,396.79,154.86,10.52,8.74" target="#b7">[8]</ref> exist and there are many advanced models and techniques that effectively solve the task. With increasing interest in applying AI technology in the medical field, VQA in medical domain is drawing attention due to the importance of supporting the doctors' clinical decision and enhancing the patients' understanding of their conditions from the medical images especially in patient-centered medical care.</p><p>VQA for medical domain is a challenging task compared to that of general domain. First, since the cost of collecting valid data is high, valid medical data for training are limited compared to those in general domain such as <ref type="bibr" coords="2,452.27,250.50,9.96,8.74" target="#b1">[2]</ref>, <ref type="bibr" coords="2,470.08,250.50,10.52,8.74" target="#b7">[8]</ref> where hundreds of thousands of images and QA pairs are available. Second, the vocabulary used in QA pairs or medical reports is quite distinct from the language used in daily life.</p><p>VQA-Med data set provided by ImageCLEF 2020 consists of 4,000 training set with radiology images and QA pairs, 500 validation set, and 500 test set only with questions without answers. As illustrated in Fig. <ref type="figure" coords="2,364.75,322.23,4.13,8.74" target="#fig_0">1</ref>, VQA-Med 2020 data set generally asks questions related to the abnormalities shown in the images. The proposed framework in this paper is shown in Fig. <ref type="figure" coords="2,383.75,596.34,4.40,8.74" target="#fig_1">2</ref> and can be described as following steps with Fig2: 1. VGG16 network <ref type="bibr" coords="2,349.54,608.30,15.50,8.74" target="#b15">[16]</ref> with GAP <ref type="bibr" coords="2,417.31,608.30,15.50,8.74" target="#b13">[14]</ref> (Green) is used to extract image features from input image 2. bioBERT model <ref type="bibr" coords="2,425.68,620.25,15.50,8.74" target="#b12">[13]</ref> (Blue) is used to capture the semantic of questions and encode it into textual features. 3. Visual features and textual features are fused by fusion mechanism called MFH Pooling <ref type="bibr" coords="2,172.16,656.12,15.50,8.74" target="#b19">[20]</ref> (Purple) 4. Co-attention mechanism (Purple) is applied to both visual and textual features to focus on particular image regions based on the question features and vice versa. 5. Finally, the features, fused by MFH Pooling <ref type="bibr" coords="3,134.77,142.90,15.50,8.74" target="#b19">[20]</ref> with co-attention, are fed to the decoder to predict the answer in a manner of classification. The improvements and contributions we made compared to the method <ref type="bibr" coords="3,465.09,404.89,15.50,8.74" target="#b17">[18]</ref> comprises of three points: First, for the visual feature extraction, the dimension of extracted feature is reduced to 1472 from 1984 to avoid over-fitting problem while maintaining the quantity of information in extracted features. Second, bioBERT model <ref type="bibr" coords="3,164.08,452.71,15.50,8.74" target="#b12">[13]</ref> is used to extract textual features instead of BERT <ref type="bibr" coords="3,406.98,452.71,10.52,8.74" target="#b5">[6]</ref> model used in <ref type="bibr" coords="3,134.77,464.67,14.61,8.74" target="#b17">[18]</ref>. While bioBERT <ref type="bibr" coords="3,230.09,464.67,15.50,8.74" target="#b12">[13]</ref> model has the same network structure as BERT <ref type="bibr" coords="3,463.81,464.67,10.52,8.74" target="#b5">[6]</ref> , the data used in pre-training of bioBERT <ref type="bibr" coords="3,313.58,476.62,15.50,8.74" target="#b12">[13]</ref> model were biomedical texts which are different from the text data of general domain used in pre-training of BERT <ref type="bibr" coords="3,134.77,500.53,10.52,8.74" target="#b5">[6]</ref> . Third, MFH Pooling <ref type="bibr" coords="3,253.63,500.53,15.50,8.74" target="#b19">[20]</ref> is used to fuse the visual and textual features which is the advanced version of MFB Pooling <ref type="bibr" coords="3,344.53,512.49,15.50,8.74" target="#b18">[19]</ref> used in <ref type="bibr" coords="3,398.95,512.49,14.61,8.74" target="#b17">[18]</ref>. MFH Pooling <ref type="bibr" coords="3,134.77,524.44,15.50,8.74" target="#b19">[20]</ref> method achieves higher performance than MFB Pooling <ref type="bibr" coords="3,401.26,524.44,15.50,8.74" target="#b18">[19]</ref> method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>There has been much of developments in methods and models used in open domain VQA <ref type="bibr" coords="3,196.96,596.34,9.96,8.74" target="#b1">[2]</ref>, <ref type="bibr" coords="3,214.09,596.34,9.96,8.74" target="#b7">[8]</ref>. For these tasks, deep learning models for image processing based on deep Convolution Neural Networks (CNNs) such as VGGNet <ref type="bibr" coords="3,462.32,608.30,14.61,8.74" target="#b15">[16]</ref>, ResNet <ref type="bibr" coords="3,170.70,620.25,10.52,8.74" target="#b8">[9]</ref> are frequently used to extract image features after pre-trained by large-scale data set in the general domain such as Image net data set <ref type="bibr" coords="3,448.23,632.21,9.96,8.74" target="#b4">[5]</ref>. Regarding the question information processing, models for NLP, which are based on Recurrent Neural Networks (RNNs) such as long short-term memory (LSTM) <ref type="bibr" coords="4,134.77,118.99,15.50,8.74" target="#b9">[10]</ref> and grated recurrent units (GRU) <ref type="bibr" coords="4,308.64,118.99,9.96,8.74" target="#b3">[4]</ref>, are frequently used not only to encode the textual features but also to generate answers as output. Similarlly, NLP models such as BERT <ref type="bibr" coords="4,234.59,142.90,10.52,8.74" target="#b5">[6]</ref> pre-trained by large-scale data are applied to extract semantic features from text data.</p><p>Attention mechanism and multi-modal feature fusion methods are the important factors of VQA system since VQA is a multidisciplinary task that involves both CV and NLP approaches. Attention mechanisms have been successfully employed in image captioning <ref type="bibr" coords="4,267.31,202.68,15.50,8.74" target="#b16">[17]</ref> and NLP models such as BERT <ref type="bibr" coords="4,427.41,202.68,10.52,8.74" target="#b5">[6]</ref> also have adopted self-attention transformers in the network structure. Multi-modal feature fusion is essential for VQA task since it combines the information from both modalities to predict the right answer. Fusion techniques have evolved starting from hierarchical co-attention model (Hie+CoAtt) <ref type="bibr" coords="4,379.60,250.50,15.50,8.74" target="#b14">[15]</ref> which employs coattention mechanism using element-wise summations, concatenation, and fully connected layers. Multimodal Compact Bilinear (MCB) pooling <ref type="bibr" coords="4,424.56,274.41,10.52,8.74" target="#b6">[7]</ref> computes the outer product between two features to represent every information from the features and this also reduces the computational cost compared to simple outer product calculation. Also, Multi-modal Low-rank Bilinear (MLB) pooling <ref type="bibr" coords="4,134.77,322.23,15.50,8.74" target="#b11">[12]</ref> generate output features with lower dimensions and models with fewer parameters compared to MCB Pooling. MFB Pooling <ref type="bibr" coords="4,364.04,334.19,15.50,8.74" target="#b18">[19]</ref> method fixed the slow convergence rate of MLB Pooling as well as the part that it is sensitive to the hyper-parameters. MFH Pooling <ref type="bibr" coords="4,279.00,358.10,15.50,8.74" target="#b19">[20]</ref> extends the MFB Pooling to a generalized high-order setting to fuse the multi-modal features more effectively.</p><p>[18] describes the first rank method of VQA-Med challenge at ImageCLEF 2019 <ref type="bibr" coords="4,158.39,393.96,10.52,8.74" target="#b0">[1]</ref> which uses VGG16 network with GAP to extract visual features from input images and BERT model to extract textual features from questions. Those extracted features are fused by MFB Pooling with co-attention method and the fused features are used to predict the answer in a manner of classification.</p><p>In addition, bioBERT <ref type="bibr" coords="4,251.90,441.78,15.50,8.74" target="#b12">[13]</ref> model is a pre-trained language representation model for the biomedical domain which shares the same network structure with BERT <ref type="bibr" coords="4,166.81,465.69,9.96,8.74" target="#b5">[6]</ref>. bioBERT is pre-trained on biomedical domain corpora and it can capture semantic features of biomedical texts such as medical reports more effectively than BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>This section describes the whole pipeline of our VQA model submitted for Im-ageCLEF 2020 VQA-Med task <ref type="bibr" coords="4,269.28,551.94,9.96,8.74" target="#b2">[3]</ref>. As shown in Fig. <ref type="figure" coords="4,356.29,551.94,4.13,8.74" target="#fig_1">2</ref>, first, from the input image and question the image features and question features are extracted by Image feature extractor and Question encoder. The extracted features are then fused with feature fusion method with co-attention to a classification network for the answer selecting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image feature extractor</head><p>In our VQA framework, VGG16 network pre-trained by ImageNet data set <ref type="bibr" coords="4,470.08,644.16,10.52,8.74" target="#b4">[5]</ref> is used to extract image features. GAP <ref type="bibr" coords="4,305.04,656.12,15.50,8.74" target="#b13">[14]</ref> strategy is applied with VGG16 net-work to prevent over-fitting problem. The GAP method take the average of last convolution outputs of each layers that have different number of channels. If the input image shape is 224x224x3 as in our model, the output shapes of VGG16 network layers are as follows: 224x224x64, 112x112x128, 56x56x256, 28x28x512, 14x14x512, 7x7x512. The last number of each output shape is the channel size of the convolution layer outputs. After taking the average of the outputs by channel, the extracted features' dimension become the channel size of the layers. Those features are concatenated to form a 1472-dimensional (64+128+256+512+512=1472) <ref type="foot" coords="5,499.70,201.11,3.97,6.12" target="#foot_0">3</ref>vector and it is used as image features and fed to the next network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question encoder</head><p>bioBERT <ref type="bibr" coords="5,179.77,262.56,15.50,8.74" target="#b12">[13]</ref> is used to extract the semantic features of the given questions. bioBERT is pre-trained with biomedical text and has the same network structure as BERT <ref type="bibr" coords="5,200.98,286.47,9.96,8.74" target="#b5">[6]</ref>. bioBERT largely outperforms BERT and previous state-ofthe-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. To extract the textual features that can represent the question sentences, we average the last layer of bioBERT-base model to obtain a 768-dimensional question feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature fusion with co-attention</head><p>Fusing multi-modal features is essential and important technique to improve the performance of VQA model. As mentioned in section 2, Multi-modal Factorized High-order (MFH) Pooling <ref type="bibr" coords="5,274.90,406.12,15.50,8.74" target="#b19">[20]</ref> method can fuse multi-modal features with less computational cost and improved performance. Co-attention mechanism can help the model to learn the importance of each part in both visual and textual features. It can use the relative information from both modalities to learn which parts of the features are important and to ignore the irrelevant information. We therefore employ the MFH Pooling with co-attention to fuse visual and textual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>Our model is trained for 990 epochs on one Quadro GV100 for about 4 hours. This section describes the detailed process and parameters used in the actual training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Train data extension</head><p>Besides the data set provided in the ImageCLEF 2020 VQA-Med task <ref type="bibr" coords="5,451.96,603.59,9.96,8.74" target="#b2">[3]</ref>, we also took advantage of VQA-Med data set of ImageCLEF 2019 <ref type="bibr" coords="5,422.16,615.54,9.96,8.74" target="#b0">[1]</ref>. From the data set in <ref type="bibr" coords="6,185.39,118.99,9.96,8.74" target="#b0">[1]</ref>, only the data comprised of QA pair existing in VQA-Med 2020 data set is used to train the model. 978 pairs in training set and 143 pairs in validation set from <ref type="bibr" coords="6,220.08,142.90,10.52,8.74" target="#b0">[1]</ref> are used to extend the VQA-Med 2020 data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyper parameters</head><p>Hyper parameters are set according to the performance on the validation data set. We used Binary cross-entropy loss as loss function, ADAM optimizer with initial learning rate of 3e-5 and the L1 regularization with co-efficient of 5e-11. MFH Pooling <ref type="bibr" coords="6,198.26,223.57,15.50,8.74" target="#b19">[20]</ref> is used with default parameters explained in <ref type="bibr" coords="6,417.62,223.57,15.50,8.74" target="#b19">[20]</ref> except for the dropout co-efficient which is set to 0.85 to prevent the over-fitting problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>Two evaluation methods were adopted to VQA-Med 2020 competition, accuracy (strict) and BLEU score. The accuracy measures the ratio of correct prediction and the BLEU score measures the similarity between the real answer and predicted answer. The max validation accuracy of our model was 0.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper describes the model submitted in ImageCLEF 2020 VQA-Med challenge. Our model ranked 3rd place and achieved accuracy of 0.466 and BLEU score of 0.502 on test data set. We applied bioBERT <ref type="bibr" coords="7,358.74,428.78,15.50,8.74" target="#b12">[13]</ref> model to extract textual features which has stronger performance on encoding biomedical texts compared BERT <ref type="bibr" coords="7,165.62,452.69,9.96,8.74" target="#b5">[6]</ref>. Also MFH Pooling <ref type="bibr" coords="7,267.25,452.69,15.50,8.74" target="#b19">[20]</ref> is used to fuse the multi-modal features that extends the MFB Pooling <ref type="bibr" coords="7,254.71,464.64,15.50,8.74" target="#b18">[19]</ref> to a generalized high-order setting to perform better. For the future work, we will continue to improve the current network and apply it to other data set or tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,158.60,563.72,298.17,7.89;2,183.43,364.45,248.50,184.50"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. One example of VQA-Med data set provided by ImageCLEF 2020</figDesc><graphic coords="2,183.43,364.45,248.50,184.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,203.77,368.34,207.81,7.89"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. General pipeline of the proposed framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,428.24,322.16,52.35,8.74;6,134.77,334.12,345.83,8.74;6,134.77,346.08,345.83,8.74;6,134.77,358.03,180.38,8.74"><head></head><label></label><figDesc>612 and the accuracy transition by training epoch is shown in Fig.3. For the actual training of submitted model, the validation accuracy became 1.0 since the validation data set was also included during the training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,215.47,575.77,184.41,7.89;6,192.48,388.20,230.40,172.80"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Accuracy transition by training epoch</figDesc><graphic coords="6,192.48,388.20,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,154.80,347.61,305.75,7.89;7,164.43,115.84,286.50,217.00"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Leader-board page of the competition. Our team ID is bumjun jung</figDesc><graphic coords="7,164.43,115.84,286.50,217.00" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="5,144.73,634.88,335.86,7.86;5,144.73,645.84,335.87,7.86;5,144.73,656.80,158.60,7.86"><p>The last convolution layer output that shaped 7x7x512 is precluded when extracting features because it is only the output of Max pooling layer that represents the same information as the former layer output.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was supported by <rs type="funder">JSPS KAKENHI</rs> Grant Number <rs type="grantNumber">JP20H05556</rs>, <rs type="funder">JST</rs> <rs type="grantName">AIP Acceleration Research Grant</rs> Number <rs type="grantNumber">JPMJCR20U3</rs> and <rs type="programName">JST ACT</rs><rs type="grantNumber">-X</rs> Grant Number <rs type="grantNumber">JPMJAX190D</rs>. We would like to thank <rs type="person">Kohei Uehara</rs>, <rs type="person">Ryohei Shimizu</rs>, <rs type="person">Dr. Hiroaki Yamane</rs>, and <rs type="person">Dr. Yusuke Kurose</rs> for helpful discussion.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9p7WA7h">
					<idno type="grant-number">JP20H05556</idno>
				</org>
				<org type="funding" xml:id="_ReTRhWE">
					<idno type="grant-number">JPMJCR20U3</idno>
					<orgName type="grant-name">AIP Acceleration Research Grant</orgName>
					<orgName type="program" subtype="full">JST ACT</orgName>
				</org>
				<org type="funding" xml:id="_c6PdAzY">
					<idno type="grant-number">-X</idno>
				</org>
				<org type="funding" xml:id="_rnPyHwh">
					<idno type="grant-number">JPMJAX190D</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.96,634.88,337.63,7.86;7,151.52,645.84,329.07,7.86;7,151.52,656.80,161.72,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,166.36,645.84,314.24,7.86">Vqa-med: Overview of the medical visual question answering task at imageclef</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,189.66,656.80,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,119.67,337.64,7.86;8,151.52,130.63,329.07,7.86;8,151.52,141.59,213.60,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,168.06,130.63,129.13,7.86">Vqa: Visual question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,321.17,130.63,159.42,7.86;8,151.52,141.59,120.81,7.86">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,151.78,337.63,7.86;8,151.52,162.74,329.07,7.86;8,151.52,173.70,329.07,7.86;8,151.52,184.65,322.28,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,151.52,162.74,329.07,7.86;8,151.52,173.70,134.88,7.86">Overview of the vqa-med task at imageclef 2020: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,309.01,173.70,171.58,7.86;8,151.52,184.65,132.29,7.86">CLEF 2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,194.84,337.63,7.86;8,151.52,205.80,329.07,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,336.15,194.84,144.43,7.86;8,151.52,205.80,169.31,7.86">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.96,215.99,337.63,7.86;8,151.52,226.94,329.07,7.86;8,151.52,237.90,198.96,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,403.66,215.99,76.93,7.86;8,151.52,226.94,134.02,7.86">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,330.56,226.94,150.03,7.86;8,151.52,237.90,93.91,7.86">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,248.09,337.63,7.86;8,151.52,259.05,329.07,7.86;8,151.52,270.01,25.60,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,346.99,248.09,133.60,7.86;8,151.52,259.05,189.89,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.96,280.19,337.64,7.86;8,151.52,291.15,329.07,7.86;8,151.52,302.11,159.05,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,455.25,280.19,25.34,7.86;8,151.52,291.15,324.77,7.86">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.96,312.30,337.63,7.86;8,151.52,323.26,329.07,7.86;8,151.52,334.21,329.07,7.86;8,151.52,345.17,86.01,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,400.59,312.30,80.00,7.86;8,151.52,323.26,311.32,7.86">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,151.52,334.21,324.88,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,355.36,337.63,7.86;8,151.52,366.32,329.07,7.86;8,151.52,377.28,76.80,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,290.95,355.36,172.55,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,151.52,366.32,325.14,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,387.46,337.97,7.86;8,151.52,398.40,92.85,7.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,288.76,387.46,100.72,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,398.70,387.46,81.89,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,408.61,337.98,7.86;8,151.52,419.57,329.07,7.86;8,151.52,430.53,329.07,7.86;8,151.52,441.48,329.07,7.86;8,151.52,452.44,329.07,7.86;8,151.52,463.40,329.07,7.86;8,151.52,474.36,329.07,7.86;8,151.52,485.32,329.07,7.86;8,151.52,496.28,329.07,7.86;8,151.52,507.24,329.07,7.86;8,151.52,518.20,34.31,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,295.04,463.40,185.55,7.86;8,151.52,474.36,255.37,7.86">Overview of the ImageCLEF 2020: Multimedia retrieval in medical, lifelogging, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,426.43,474.36,54.16,7.86;8,151.52,485.32,329.07,7.86;8,151.52,496.28,253.34,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="8,456.14,496.28,24.45,7.86;8,151.52,507.24,138.63,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,142.61,528.38,337.98,7.86;8,151.52,539.34,329.07,7.86;8,151.52,550.30,157.97,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,151.52,539.34,174.76,7.86">Multimodal residual learning for visual qa</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,349.98,539.34,130.61,7.86;8,151.52,550.30,73.89,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="361" to="369" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,560.49,337.97,7.86;8,151.52,571.45,329.07,7.86;8,151.52,582.38,159.79,7.89" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,438.58,560.49,42.01,7.86;8,151.52,571.45,324.76,7.86">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,151.52,582.40,58.56,7.86">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,592.59,337.98,7.86;8,151.52,603.55,25.60,7.86" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m" coord="8,267.03,592.59,78.89,7.86">Network in network</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.61,613.74,337.98,7.86;8,151.52,624.69,329.07,7.86;8,151.52,635.65,76.80,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,305.72,613.74,174.87,7.86;8,151.52,624.69,100.83,7.86">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,273.42,624.69,202.96,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="289" to="297" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,645.84,337.97,7.86;8,151.52,656.80,231.27,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="8,278.92,645.84,201.67,7.86;8,151.52,656.80,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.62,119.67,337.98,7.86;9,151.52,130.63,329.07,7.86;9,151.52,141.59,329.07,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,201.52,130.63,279.07,7.86;9,151.52,141.59,35.25,7.86">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,207.79,141.59,179.92,7.86">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,152.55,337.97,7.86;9,151.52,163.51,312.89,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,309.14,152.55,171.45,7.86;9,151.52,163.51,167.55,7.86">Zhejiang university at imageclef 2019 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,340.83,163.51,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,174.47,337.97,7.86;9,151.52,185.43,329.07,7.86;9,151.52,196.39,260.72,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,284.45,174.47,196.14,7.86;9,151.52,185.43,192.07,7.86">Multi-modal factorized bilinear pooling with coattention learning for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,365.69,185.43,114.91,7.86;9,151.52,196.39,167.94,7.86">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1821" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,207.34,337.97,7.86;9,151.52,218.30,329.07,7.86;9,151.52,229.24,257.15,7.89" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,317.68,207.34,162.91,7.86;9,151.52,218.30,234.96,7.86">Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,393.79,218.30,86.80,7.86;9,151.52,229.26,151.31,7.86">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5947" to="5959" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
