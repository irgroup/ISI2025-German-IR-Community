<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,159.97,115.96,295.42,12.62;1,158.78,133.89,297.18,12.62;1,280.09,151.82,55.19,12.62">NLM at VQA-Med 2020: Visual Question Answering and Generation in the Medical Domain</title>
				<funder>
					<orgName type="full">U.S. National Library of Medicine, National Institutes of Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,271.25,189.49,72.86,8.74"><forename type="first">Mourad</forename><surname>Sarrouti</surname></persName>
							<email>mourad.sarrouti@nih.gov</email>
							<affiliation key="aff0">
								<orgName type="department">National Library of Medicine</orgName>
								<orgName type="institution">National Institutes of Health</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,159.97,115.96,295.42,12.62;1,158.78,133.89,297.18,12.62;1,280.09,151.82,55.19,12.62">NLM at VQA-Med 2020: Visual Question Answering and Generation in the Medical Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4047F9B2A51B966D41BE9CA9D1F23B54</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>Visual Question Generation</term>
					<term>Deep Learning</term>
					<term>Variational Autoencoders</term>
					<term>Natural Language Processing</term>
					<term>Computer Vision</term>
					<term>ImageCLEF 2020</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the U.S. National Library of Medicine (NLM) in Visual Question Answering (VQA) and Visual Question Generation (VQG) tasks of the VQA-Med challenge at ImageCLEF 2020. In the VQA task, I proposed a variational autoencoders model that takes as input a medical question-image pair and generates a natural language answer as output. The encoder consists of a pre-trained CNN model and LSTM to encode the dense vectors of the images and the questions into a latent space, respectively. The decoder network uses LSTM to decode questions from the latent space. I also presented a multi-class image classification-based method for VQA that takes as input an image and returns an answer as output. I used the pre-trained model ResNet-50 with the last layer (the Softmax layer) removed, and added a Softmax layer with different answers as classes. I used the VQA-Med 2019 and VQA-Med 2020 training datasets to train my models. In the VQG task, I presented a variational autoencoders model that takes as input an image and generates a question as output. I also generated new training data from the existing VQA-Med 2020 VQG dataset, based on contextual word embeddings and image augmentation techniques. My best VQA and VQG models achieve 44.1% and 11.6% respectively in terms of BLEU score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual Question Answering (VQA) and Visual Question Generation (VQG) from images is a rising research topic in both fields of natural language processing <ref type="bibr" coords="1,134.77,592.07,16.47,8.74" target="#b11">[12,</ref><ref type="bibr" coords="1,151.24,592.07,12.35,8.74" target="#b13">14,</ref><ref type="bibr" coords="1,163.59,592.07,12.35,8.74" target="#b12">13]</ref> and computer vision <ref type="bibr" coords="1,277.95,592.07,16.00,8.74" target="#b10">[11,</ref><ref type="bibr" coords="1,293.95,592.07,12.00,8.74" target="#b16">17,</ref><ref type="bibr" coords="1,305.96,592.07,12.00,8.74" target="#b14">15]</ref>. A VQA system takes as input an image and a natural language question about the image, and produces a natural language answer as the output. Whereas a VQG system aims at generating natural language questions from the images. Both VQA and VQG combine natural language processing that provide an understanding of the question and the ability to produce the answer, and computer vision techniques that provide an understanding of the content of the image.</p><p>In contrast to answering and generating visual questions from the content of the image in the open domain, answering and generating visual questions has received little attention in the medical domain. A few recent works have attempted to answer and generate questions about medical images <ref type="bibr" coords="2,428.66,203.15,11.15,8.74" target="#b4">[5,</ref><ref type="bibr" coords="2,439.81,203.15,11.15,8.74" target="#b14">15]</ref>. This paper presents the participation of the U.S. National Library of Medicine (NLM) in VQA and VQG tasks of the VQA-Med challenge <ref type="bibr" coords="2,386.72,227.52,9.84,8.74" target="#b3">[4]</ref>, which is organized by ImageCLEF 2020 <ref type="bibr" coords="2,236.09,239.48,10.09,8.74" target="#b8">[9]</ref>. The VQA-Med challenge aims at answering and generating questions about medical images. For the VQA task, I proposed a variational autoencoders model that is tasked with answering a natural language question when shown a medical image. I also presented another VQA model based on multi-class image classification approach. The questions format are repetitive so they might not contribute in answer predictions and only the image can determine the answer. For the VQG task, I introduced and used our recent VQG system based on variational autoencoders that takes as input a medical image and generates a question as output <ref type="bibr" coords="2,374.39,335.12,14.74,8.74" target="#b14">[15]</ref>. All my models use the pre-trained convolutional neural networks (CNN), ResNet50 <ref type="bibr" coords="2,422.64,347.07,10.08,8.74" target="#b7">[8]</ref>, for visual features extractions.</p><p>The rest of paper is organized as follows. Section 2 presents the most relevant work. Section 2 describes datasets used in the 2020 VQA-Med challenge. Section 4 presents my proposed models for answering and generating visual questions from medical images. Official results for all models are presented in Section 5. Finally, the paper is concluded in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Open-domain VQA and VQG research areas has received much attention from the research community in recent years. They benefited from the open-domain VQA challenge<ref type="foot" coords="2,200.82,502.05,3.97,6.12" target="#foot_0">1</ref> , which takes place regularly every year since 2015. Otherwise, VQA and VQG has been a challenge from the past few years in the medical domain. There has been no significant progress toward this, because of the lack of labelled data and the difficulty of creating such data. Medical images such as radiology images are highly domain-specific, which can only be interpreted by well-educated medical professionals. Since the launch of the VQA-Med challenge at ImageCLEF <ref type="bibr" coords="2,206.87,575.35,10.63,8.74" target="#b6">[7,</ref><ref type="bibr" coords="2,217.49,575.35,7.09,8.74" target="#b5">6]</ref>, methods in medical VQA continue to evolve to better meet the needs of users visual questions. Many participants systems follow a traditional supervised maximum likelihood estimation (MLE) paradigm that typically relies on a convolutional neural network (CNN) + recurrent neural network (RNN) encoder-decoder formulation (e.g., <ref type="bibr" coords="2,365.01,623.17,10.44,8.74" target="#b2">[3]</ref>). They leveraged CNN like VGGNet <ref type="bibr" coords="2,196.57,635.13,15.61,8.74">[16]</ref> or ResNet <ref type="bibr" coords="2,264.10,635.13,10.63,8.74" target="#b7">[8]</ref> with a variety of pooling strategies to encode image features and RNN to extract question features. Some other participants formulated VQA as multi-class image classification task <ref type="bibr" coords="3,386.07,130.95,10.09,8.74" target="#b1">[2]</ref>. In addition, some teams have improved VQA performance using advanced techniques such as the stacked attention networks and multimodal compact bilinear (MCB) pooling <ref type="bibr" coords="3,469.26,154.86,9.89,8.74" target="#b0">[1]</ref>.</p><p>In contrast to VQA, VQG has received little interest so far in the medical domain. More recently, the task of VQG in the medical domain has been studied and explored in <ref type="bibr" coords="3,204.64,190.86,14.48,8.74" target="#b14">[15]</ref>. The authors introduced VQGR, a VQG system that is able to generate natural language questions when shown radiology images. They have used variational autoencoders as a neural network and introduced a text data augmentation technique to create more training data.</p><p>3 Data Description</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">VQA</head><p>Given an image and a question expressed in natural language, the VQA task consists in providing an answer based on the image content. The dataset used in VQA-Med 2020 consists of 4,000 radiology images with 4,000 Question-Answer (QA) pairs as training data, 500 radiology images with 500 QA pairs as validation data, and 500 radiology images with 500 questions as test data. Figure <ref type="figure" coords="3,447.00,352.36,4.98,8.74" target="#fig_0">1</ref> shows examples from the VQA-Med 2020 data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">VQG</head><p>Given a radiology image, the VQG task consists in generating a natural language question based on the content of the image. The dataset used in VQA-Med 2020 consists of 780 radiology images with 2,156 associated questions as training data, 141 radiology images with 164 questions as validation data, and 80 radiology images as test data. Figure <ref type="figure" coords="3,256.03,656.12,4.98,8.74" target="#fig_1">2</ref> shows examples from VQA-Med 2020 VQG data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>In this section, I present in details the proposed methods for my participation in VQA and VQG task of the 2020 VQA-Med challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual Question Answering</head><p>Variational autoencoders-based method for VQA: To address the VQA task of the VQA-Med challenge at ImageCLEF 2020, I proposed a VQA model based on the variational autoencoders approach [10] that takes as input a medical question-image pair and generates a natural language answer as output.</p><p>As shown in Figure <ref type="figure" coords="4,240.98,417.99,3.95,8.74" target="#fig_2">3</ref>, the proposed model consists of two neural network modules, encoder, and decoder, for learning the probability distributions of data p(x). First, the encoder creates a latent variable z from the image v and the question q, and encodes the dense vectors h v and h q into a latent space z -space. A CNN is used to obtain the image feature map v, and an LSTM is used to generate the embedded question features q. Then, the model reconstructs the input features L v , L q from the z-space using a simple Multi Layer Perceptron (MLP) which is a neural network with fully connected layers. I optimize the model by minimizing the following l 2 loss:</p><p>(1)</p><formula xml:id="formula_0" coords="4,231.04,531.13,152.77,12.28">L v = ||h v -ĥv || 2 , L q = ||h q -ĥq || 2</formula><p>Finally, it uses LSTM decoder to generate the answer â from the z-space. The decoder takes a sample from the latent dimension z-space, and uses that as an input to output the answer â. It receives a "start" symbol and proceeds to output an answer word by word until it produces an "end" symbol. Cross Entropy loss function have been used to evaluate the quality of the neural network and to minimize the error L g between the generated answer â and the reference answer a.</p><p>The final loss of the proposed VQA model is as follows:</p><p>(2) where KL is Kullback-Leibler divergence, λ 1 , λ 2 , λ 3 , λ 4 are hyper-parameters that control the variational loss, the question generation loss, the image reconstruction loss, the question reconstruction loss, respectively.</p><formula xml:id="formula_1" coords="4,227.42,656.12,159.78,9.65">Loss = λ 1 L g + λ 2 KL + λ 3 L v + λ 4 L q</formula><p>Multi-class image classification-based method for VQA: I introduced another VQA system based on multi-class image classification approach to solve the VQA task of the VQA-Med challenge at ImageCLEF 2020. The questions are repetitive and have almost the same format and meaning even if they use some different words. So, the questions would not contribute in answer predictions and only the image can determine the answer. Moreover, the majority of questions have a fixed number of candidate answers (332 answers in the whole data) and therefore they can be answered by multi-way classification. Consequently, the VQA task can be equivalently formulated as multi-class classification problems with 332 classes. To do so, I use the pre-trained model ResNet50 with the last layer (the Softmax layer) removed. The output from this part is fed into a Softmax layer with 332 classes (candidate answers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visual Question Generation</head><p>To address the VQG task of the VQA-Med challenge at ImageCLEF 2020, I used our recent VQG model based on the variational autoencoders approach <ref type="bibr" coords="5,444.22,608.30,15.39,8.74" target="#b14">[15]</ref> that takes as input a medical image and generates a natural language question as output. This model first uses a CNN for obtaining the image feature map v and encoding the dense vectors h v into a latent (hidden) representation z-space. It then reconstructs the inputs from the z-space using a simple MLP. Finally, it uses a decoder LSTM to generate the question q from the z-space. The decoder takes a sample from the latent dimension z-space, and uses that as an input to output the question q. I trained this model on the augmented data obtained using contextual word embeddings and image augmentation techniques. I first make use of VQA-Med image-question pairs to generate a heavily augmented dataset for training the question generation model. Each question is tagged with part-of-speech and each candidate word replaced by its most cosine-similar neighbor in a word embedding space based on vocabulary from English Wikipedia, PubMed and PubMedCentral to generate a new augmented question. Each image is also augmented with shifts, flips, rotations, and blurs. More details of this method appairs in <ref type="bibr" coords="6,217.60,238.55,14.61,8.74" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>In this section, I report my official results in the 2020 VQA-Med challenge. The evaluation metrics are accuracy and BLEU for the VQA task, and BLEU for the VQG task. For all models, all images are resized to 224*224, adam optimiser with a learning rate of 0.0001 and a batch size of 32 is used. All models are trained for 20 epochs and the best validation results are used as final results. I implemented these models using PyTorch. The source code are available at https: //github.com/sarrouti/vqa and https://github.com/sarrouti/vqa-mcc. Table <ref type="table" coords="6,176.59,572.43,4.88,8.74" target="#tab_0">1</ref> shows my official results in the VQA task of the VQA-Med challenge. Run 5 which deals with VQA as a multi-class image classification problem has the best accuracy score and best BLEU score among my submissions. The multi-class image classification approach significantly outperforms the variational autoencoders-based method for VQA. This is likely because the questions were repetitive and all questions have almost the same format and meaning even if they use different words. So, it is expected that the questions would not contribute well in answer generation and only the image can determine the answer. Moreover, in the VQA-Med 2020 data, the majority of questions have a fixed number of candidate answers and hence can be answered by multi-class classification.   On the other hand, I submitted 3 automatic runs to the VQG task at ImageCLEF 2020 VQA-Med:</p><p>-Run 1: This run used the varational autoencoders. The VQG model was trained on the augmented data, and without inputs reconstruction. The output length is 10. -Run 2: This run used the varational autoencoders. The VQG model was trained on the augmented data, and with inputs reconstruction. The output length is 20. -Run 3: This run used the varational autoencoders. The VQG model was trained on the augmented data, and with inputs reconstruction. The output length is 30.</p><p>Table <ref type="table" coords="7,175.79,620.25,4.88,8.74" target="#tab_3">3</ref> shows my official results in the VQG task of the VQA-Med challenge at ImageCLEF 2020. Run 2 has the best BLEU score (0.116) among my submissions. Table <ref type="table" coords="7,162.39,644.16,5.08,8.74" target="#tab_4">4</ref> presents the results of the participating teams. Although many teams have participated in the VQA task of the VQA-Med challenge, only 3 teams have submitted runs for the VQG task. The results obtained by my VQG system compared with other systems are encouraging and I hope to make improvements in the future. VQG is a challenging task, especially in the medical domain as the available dataset is too small for training efficient VQG models. Small data might require models that have low complexity. Whereas the variational autoencoders model requires a large amount of training data as it tries to learn deeply the underlying data distribution of the input to output new sequences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, I described my participation in the VQA and VQG tasks at ImageCLEF VQA-Med 2020. I introduced a variational autoencoders-based method and a multi-class image classification-based method for VQA. My VQA model's best accuracy is 0.40 with 0.441 BLEU score. I also presented a variational autoencoders-based method for VQG. The VQG model achieved 0.116 in terms of BLEU score. In the future, I plan to use the generated questions to advance VQA in the medical domain. I also plan to improve the performance of both VQA and VQG by using the attention mechanism that allows to pay more attention to specific regions that better represent the question instead of the whole image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,524.16,345.83,7.89;3,134.77,535.15,224.93,7.86;3,150.24,396.01,311.81,113.38"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of radiology images and the associated questions and answers from the VQA validation set of ImageCLEF 2020 VQA-Med.</figDesc><graphic coords="3,150.24,396.01,311.81,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,243.99,345.83,7.89;4,134.77,254.98,177.34,7.86;4,150.24,115.84,311.81,113.38"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of radiology images and the associated questions from the VQG training set of ImageCLEF 2020 VQA-Med.</figDesc><graphic coords="4,150.24,115.84,311.81,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,311.64,346.01,7.89;5,134.77,322.63,345.83,7.86;5,134.77,333.59,23.04,7.86;5,134.77,115.84,354.33,170.08"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of the proposed VQA system based on variational autoencoders for radiology images. Input questions are encoded by an LSTM. Images are encoded by a CNN.</figDesc><graphic coords="5,134.77,115.84,354.33,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,149.71,376.34,331.13,8.74;6,134.39,388.29,47.60,8.74;6,140.99,408.92,339.60,8.77;6,151.70,420.90,329.16,8.74;6,151.70,432.86,48.21,8.74;6,140.99,444.57,339.60,8.77;6,151.70,456.56,328.89,8.74;6,151.70,468.51,169.53,8.74;6,140.99,480.23,339.60,8.77;6,151.70,492.21,328.89,8.74;6,151.70,504.17,169.53,8.74;6,140.99,515.88,339.60,8.77;6,151.70,527.87,328.89,8.74;6,151.70,539.82,174.51,8.74;6,140.99,551.54,341.53,8.77"><head></head><label></label><figDesc>I submitted five automatic runs to the VQA task at ImageCLEF 2020 VQA-Med: -Run 1: This run used variational autoencoders. The VQA model was trained on the 2020 VQA-Med data, and without inputs reconstruction. The output length is 3. -Run 2: This run used variational autoencoders. The VQA model was trained on the 2020 VQA-Med and 2019 VQA-Med training data, and without inputs reconstruction. The output length is 3. -Run 3: This run used variational autoencoders. The VQA model was trained on the 2020 VQA-Med and 2019 VQA-Med training data, and with inputs reconstruction. The output length is 4. -Run 4: This run used variational autoencoders. The VQA model was trained on the 2020 VQA-Med and 2019 VQA-Med training data, and with inputs reconstruction. The output length is 10. -Run 5: This run used multi-class image classification-based method for VQA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.39,162.94,347.98,83.85"><head>Table 1 .</head><label>1</label><figDesc>Official results of ImageCLEF 2020 VQA-Med: NLM runs for the VQA task.</figDesc><table coords="7,191.50,183.71,201.13,63.08"><row><cell>NLM Runs</cell><cell>Accuracy</cell><cell>BLEU</cell></row><row><cell>Run 1</cell><cell>0.232</cell><cell>0.299</cell></row><row><cell>Run 2</cell><cell>0.256</cell><cell>0.323</cell></row><row><cell>Run 3</cell><cell>0.278</cell><cell>0.321</cell></row><row><cell>Run 4</cell><cell>0.286</cell><cell>0.335</cell></row><row><cell>Run 5</cell><cell>0.400</cell><cell>0.441</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,275.12,347.21,44.60"><head>Table 2</head><label>2</label><figDesc>presents the results of the best five participating teams in the VQA task of the VQA-Med challenge. My best overall result was obtained by Run 5, achieving the fifth best BLEU score of 0.441 and the fifth best accuracy of 0.40 in the VQA-Med challenge.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,134.39,342.97,346.20,94.81"><head>Table 2 .</head><label>2</label><figDesc>Official results of the VQA-Med challenge at ImageCLEF 2020: The top five participating teams in the VQA task.</figDesc><table coords="7,177.33,374.70,229.47,63.08"><row><cell>Participants</cell><cell>Accuracy</cell><cell>BLEU</cell></row><row><cell>z liao</cell><cell>0.496</cell><cell>0.542</cell></row><row><cell>TheInceptionTeam</cell><cell>0.48</cell><cell>0.511</cell></row><row><cell>bumjun jung</cell><cell>0.466</cell><cell>0.502</cell></row><row><cell>going</cell><cell>0.426</cell><cell>0.462</cell></row><row><cell>NLM</cell><cell>0.40</cell><cell>0.441</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.39,223.50,347.99,61.94"><head>Table 3 .</head><label>3</label><figDesc>Official results of ImageCLEF 2020 VQA-Med: NLM runs for the VQG task.</figDesc><table coords="8,235.42,244.27,113.30,41.17"><row><cell>NLM Runs</cell><cell>BLEU</cell></row><row><cell>Run 1</cell><cell>0.104</cell></row><row><cell>Run 2</cell><cell>0.116</cell></row><row><cell>Run 3</cell><cell>0.115</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,134.39,334.73,346.20,72.90"><head>Table 4 .</head><label>4</label><figDesc>Official results of the VQA-Med challenge at ImageCLEF 2020: The participating teams in the VQG task.</figDesc><table coords="8,221.24,366.46,141.64,41.16"><row><cell>Participants</cell><cell>BLEU</cell></row><row><cell>z liao</cell><cell>0.348</cell></row><row><cell>TheInceptionTeam</cell><cell>0.339</cell></row><row><cell>NLM</cell><cell>0.116</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,657.44,188.29,7.47"><p>https://visualqa.org/challenge_2016.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the intramural research program at the <rs type="funder">U.S. National Library of Medicine, National Institutes of Health</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,142.34,337.64,7.86;9,151.52,153.29,329.37,7.86;9,150.44,164.25,95.81,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,463.10,142.34,17.49,7.86;9,151.52,153.29,280.22,7.86">Nlm at imageclef 2018 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,455.70,153.29,25.19,7.86;9,150.44,164.25,67.14,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,175.16,337.64,7.86;9,151.29,186.12,330.38,7.86;9,150.44,197.08,25.60,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,414.26,175.16,66.34,7.86;9,151.29,186.12,214.33,7.86">Just at imageclef 2019 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Al-Sadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Talafha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jararweh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Costen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,387.29,186.12,94.38,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,207.99,337.63,7.86;9,151.52,218.95,312.89,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,328.59,207.99,152.00,7.86;9,151.52,218.95,167.55,7.86">An encoder-decoder model for visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Allaouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Benamrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,340.83,218.95,94.91,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,229.86,338.91,7.86;9,151.52,240.81,329.07,7.86;9,151.52,251.77,329.07,7.86;9,151.52,262.73,300.72,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,151.52,240.81,329.07,7.86;9,151.52,251.77,129.09,7.86">Overview of the vqa-med task at imageclef 2020: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,301.23,251.77,179.36,7.86;9,151.52,262.73,110.74,7.86">CLEF 2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,273.64,338.91,7.86;9,151.52,284.60,329.07,7.86;9,151.29,295.56,329.30,7.86;9,151.52,306.52,330.35,7.86;9,151.52,317.48,329.07,7.86;9,150.44,328.43,225.23,8.11" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,166.50,284.60,314.09,7.86;9,151.29,295.56,16.46,7.86">Vqa-med: Overview of the medical visual question answering task at imageclef 2019</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/paper_272.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,412.45,295.56,68.14,7.86;9,151.52,306.52,238.04,7.86">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,246.53,317.48,122.49,7.86">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">September 9-12, 2019. 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,339.34,338.91,7.86;9,151.52,350.30,329.07,7.86;9,151.29,361.26,329.30,7.86;9,151.52,372.22,271.36,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,166.50,350.30,314.09,7.86">Vqa-med: Overview of the medical visual question answering task at imageclef</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="9,187.94,361.26,276.71,7.86">CLEF 2019 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,383.13,337.64,7.86;9,151.52,394.09,329.07,7.86;9,151.52,405.05,55.09,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,431.59,383.13,49.00,7.86;9,151.52,394.09,244.45,7.86">Overview of imageclef 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,416.51,394.09,64.08,7.86;9,151.52,405.05,26.42,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,415.95,337.64,7.86;9,150.44,426.91,25.60,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,299.88,415.95,180.71,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,437.82,338.92,7.86;9,151.52,448.78,330.35,7.86;9,151.52,459.74,330.35,7.86;9,151.52,470.70,330.35,7.86;9,151.52,481.66,330.35,7.86;9,151.52,492.62,329.07,7.86;9,151.52,503.58,262.03,7.86;9,134.77,514.48,11.78,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,295.51,492.62,185.08,7.86;9,151.52,503.58,262.03,7.86">Overview of the ImageCLEF 2020: Multimedia retrieval in medical, lifelogging, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,151.52,514.48,329.32,7.86;9,151.52,525.44,93.19,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,275.25,514.48,134.82,7.86">Auto-encoding variational bayes</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.62,536.35,339.26,7.86;9,151.52,547.31,329.07,7.86;9,151.18,558.27,329.41,7.86;9,151.52,569.23,329.41,7.86;9,150.44,580.19,331.09,8.12;9,151.52,591.79,85.23,7.47" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,166.72,547.31,189.71,7.86">Generating natural questions about an image</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1170</idno>
		<ptr target="https://www.aclweb.org/anthology/P16-1170" />
	</analytic>
	<monogr>
		<title level="m" coord="9,380.32,547.31,100.27,7.86;9,151.18,558.27,260.18,7.86">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">Aug 2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1802" to="1813" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="9,142.62,602.05,337.98,7.86;9,151.52,613.01,329.07,7.86;9,151.52,623.95,330.01,8.14;9,151.29,635.58,104.56,7.47" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,270.85,602.05,209.75,7.86;9,151.52,613.01,180.44,7.86">A machine learning-based method for question type classification in biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">O E</forename><surname>Alaoui</surname></persName>
		</author>
		<idno type="DOI">10.3414/me16-01-0116</idno>
		<ptr target="https://doi.org/10.3414%2Fme16-01-0116" />
	</analytic>
	<monogr>
		<title level="j" coord="9,338.79,613.01,141.80,7.86">Methods of Information in Medicine</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="209" to="216" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,645.84,337.98,7.86;9,151.52,656.80,329.07,7.86;10,151.52,119.65,329.26,7.89;10,151.29,130.63,332.60,8.11;10,151.52,142.24,70.61,7.47" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,297.89,645.84,182.70,7.86;9,151.52,656.80,329.07,7.86;10,151.52,119.67,80.96,7.86">A passage retrieval method based on probabilistic information retrieval model and UMLS concepts in biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">O E</forename><surname>Alaoui</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2017.03.001</idno>
		<ptr target="https://doi.org/10.1016%2Fj.jbi.2017.03.001" />
	</analytic>
	<monogr>
		<title level="j" coord="10,244.64,119.67,153.92,7.86">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="96" to="103" />
			<date type="published" when="2017-04">apr 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,152.55,337.98,7.86;10,151.52,163.51,329.07,7.86;10,151.52,174.44,330.86,7.89;10,151.52,185.43,249.75,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,285.14,152.55,195.45,7.86;10,151.52,163.51,329.07,7.86;10,151.52,174.47,78.98,7.86">Sembionlqa: A semantic biomedical question answering system for retrieving exact and ideal answers to natural language questions</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">O E</forename><surname>Alaoui</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artmed.2019.101767</idno>
		<ptr target="https://doi.org/https://doi.org/10.1016/j.artmed.2019.101767" />
	</analytic>
	<monogr>
		<title level="j" coord="10,240.97,174.47,146.71,7.86">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">101767</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,196.39,337.98,7.86;10,151.52,207.34,329.07,7.86;10,151.52,218.30,329.07,7.86;10,151.52,229.26,332.36,8.11" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,371.76,196.39,108.84,7.86;10,151.52,207.34,93.24,7.86">Visual question generation from radiology images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.alvr-1" />
	</analytic>
	<monogr>
		<title level="m" coord="10,270.61,207.34,209.98,7.86;10,151.52,218.30,141.90,7.86">Proceedings of the First Workshop on Advances in Language and Vision Research</title>
		<meeting>the First Workshop on Advances in Language and Vision Research<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="page" from="12" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,251.18,337.97,7.86;10,151.52,262.14,99.86,7.86" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m" coord="10,278.54,251.18,202.05,7.86;10,151.52,262.14,71.19,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,273.10,337.97,7.86;10,151.28,284.06,329.31,7.86;10,151.52,295.02,236.17,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,343.58,273.10,137.01,7.86;10,151.28,284.06,60.38,7.86">Automatic generation of grounded visual questions</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,231.18,284.06,249.41,7.86;10,151.52,295.02,142.82,7.86">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17)</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4235" to="4243" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
