<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.76,115.96,293.83,12.62;1,256.15,133.89,103.06,12.62">Deep Learning for UI Element Detection: DrawnUI 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,136.49,171.84,81.94,8.74"><forename type="first">Naveen</forename><surname>Narayanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sri Sivasubramaniya Nadar College Of Engineering</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,229.09,171.84,73.61,8.74"><forename type="first">Nitin</forename><surname>Nikamanth</surname></persName>
							<email>nitinnikamanth17099@cse.ssn.edu.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Sri Sivasubramaniya Nadar College Of Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.03,171.84,61.30,8.74"><forename type="first">Appiah</forename><surname>Balaji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sri Sivasubramaniya Nadar College Of Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,397.37,171.84,81.49,8.74"><forename type="first">Kavya</forename><surname>Jaganathan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sri Sivasubramaniya Nadar College Of Engineering</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.76,115.96,293.83,12.62;1,256.15,133.89,103.06,12.62">Deep Learning for UI Element Detection: DrawnUI 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BD791A844106C5D8FB92A70BB0A958BD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>DrawnUI</term>
					<term>YOLOv4</term>
					<term>Cascade RCNN</term>
					<term>Object Detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As the field of software development is rapidly growing, it becomes vital to show steady progress in the application development sector to maintain the pace. Constantly tweaking the front-end of an application becomes cumbersome for developers. Detection tools that help design UI from hand-drawn sketches can aid developers and nondevelopers in building applications with great ease. The recent advancements in deep learning techniques and the availability of computational power facilitates training efficient models for object detection. Two such models, the YOLOv4 and Cascade RCNN are implemented for detecting UI elements from hand-drawn sketches and are detailed in this paper. These models are observed to have an average mAP@IoU 0.5 improvement of 31.85% over the baseline Faster RCNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With recent needs in the software development industry for tools to increase efficiency of application development, aiding tools like recommendation systems, generative models help in reducing the time taken to build quality systems. With the difficulty in iterative software engineering setups, it becomes cumbersome for UI designers and front-end developers to make changes and fix issues. So the requirement of an interactive tool for designers to work on, for generating previews or even generate a completely finished front-end application becomes convenient.</p><p>As the field of computer vision and deep learning have attained maturity, various network architectures have been built for numerous downstream applications like object detection, instance segmentation, semantic segmentation and panoptic segmentation. The rise of CNN's and its huge success in image classification tasks have encouraged further research on CNN based models for object detection.</p><p>In this paper we discuss about various object detection models and their performance comparison for UI element detection.The rest of the paper is divided into 5 sections. Section 2 gives an idea of the evolution of object detection models and the motivation behind the chosen networks. Section 3 gives an overview of the dataset used. Section 4 explains the chosen architectures and Section 5 compares the performance of the proposed methods and Section 6 draws the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Object Detection using deep neural networks(DNN) has been an active area of research for over a decade. R-CNN, proposed by Ross Girshick et al. <ref type="bibr" coords="2,441.65,272.18,10.52,8.74" target="#b4">[5]</ref> had a two stage architecture, which combined a proposal detector and a region-wise classifier that became predominant in the recent past due to its success. Subsequently SPP net, proposed by Kaiming He et al. <ref type="bibr" coords="2,350.08,308.05,10.52,8.74" target="#b5">[6]</ref> improved over RCNN with correct estimation and detection efficiency in testing as analysed by <ref type="bibr" coords="2,425.33,320.00,14.61,8.74" target="#b12">[13]</ref>. Regardless of region proposal generation used in all the above networks, the training of all network layers can be processed in a single-stage with a multi-task loss. This was implemented in Fast RCNN <ref type="bibr" coords="2,281.15,355.87,26.01,8.74">[13][4]</ref> which saved the additional expense on storage space,and improved both accuracy and efficiency with more reasonable training schemes. It was later found that the Faster RCNN <ref type="bibr" coords="2,388.40,379.78,15.50,8.74" target="#b10">[11]</ref> can overcome the region proposal computational cost by implementing a RPN module becoming a significant improvement over Fast RCNN. Cascade RCNN <ref type="bibr" coords="2,393.28,403.69,9.96,8.74" target="#b1">[2]</ref>, which is a multi stage extension of Faster RCNN, achieves high quality object detection by effectively rejecting close false positives. This is achieved by combining a cascaded bounding box regression and cascaded detection which simultaneously increases both, the quality of hypotheses and the detector.</p><p>YOLO, developed by Joseph Redmon et al. <ref type="bibr" coords="2,336.13,464.84,9.96,8.74" target="#b7">[8]</ref>, had a different approach when compared to R-CNN, where-in a single network predicted both, the bounding boxes and the class probabilities for these boxes. The YOLOv1 divided input images into Size x Size grid cells and in each grid, certain number of bounding boxes were taken. Bounding boxes are then selected if their class probabilities are more than a particular threshold value. But YOLOv1 had difficulties in detecting small objects that appear in groups and in detecting objects having unusual aspect ratios. Combating this issue, YOLOv2 as described by <ref type="bibr" coords="2,370.31,548.52,9.96,8.74" target="#b8">[9]</ref>, used batch normalisation, a high resolution classifier, anchor boxes and dimension clusters to improve the performance. It also bounds the location using logistic activation overcoming the instability of YOLOv1 in early iterations. YOLOv2 also uses Darknet-19 thus obtaining a good balance between accuracy and model complexity. The next proposed YOLOv3 had few incremental improvements on YOLOv2. It had a better feature extractor, DarkNet-53 with shortcut connections as well as a better object detector with feature map upsampling and concatenation as mentioned in <ref type="bibr" coords="2,173.73,644.16,15.18,8.74" target="#b9">[10]</ref>. The latest YOLOv4 has inbuilt Data Augmentation for a more robust training as part of its 'Bag of Freebies' and it improves accuracy of ob-ject detection using its 'Bag of Specials' as explained in <ref type="bibr" coords="3,377.11,118.99,10.52,8.74" target="#b0">[1]</ref> on the backbone and Detector.</p><p>In our work, we thus implement Cascade RCNN and Yolov4 in detecting UI elements because of their significant advantages and state-of-the-art performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Description</head><p>The dataset provided by <ref type="bibr" coords="3,245.10,223.24,23.83,8.74">[3] [7]</ref>, consists of hand drawn images of internet websites, mobile application interfaces from 1000 different templates. The dataset consisted of 21 classes which included a variety of small elements like check boxes and buttons all the way to large elements such as images and containers. In order to increase the sample images and to rectify the class imbalance, data augmentation techniques such as random scaling and flipping were implemented. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>The two different architectures that was employed are YOLOv4 and Cascade RCNN with Resnest Backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cascade RCNN</head><p>Cascade RCNN <ref type="bibr" coords="3,207.83,524.55,10.52,8.74" target="#b1">[2]</ref> is a multi-stage extension of the two stage architecture of Faster-RCNN. Proposal sub-network is the first stage of the Faster RCNN architecture, in which the entire image is processed by a backbone network after which preliminary detection hypotheses, also called object proposals is produced by applying a proposal head("H0"). The second stage consists of processing the hypotheses by using a region-of-interest detection sub-network ("H1"), denoted as a detection head. Every hypotheses is then assigned a classification score("C") and a bounding box("B"). This architecture also consists of a cascaded bounding box regression and cascaded detection which simultaneously increases both, the quality of hypotheses and the detector. The Backbone used is ResNeSt over the traditional ResNet.This is because, according to <ref type="bibr" coords="3,193.24,656.12,14.61,8.74" target="#b11">[12]</ref>, architectures originally designed for image classification like </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">YOLO</head><p>Unlike algorithms which re-purposes classifiers for object detection, YOLO takes a different approach. Instead of repeated forward passes, as in the case of sliding window or region proposal models, the image is divided into S*S grids, and a single convolutional network pass outputs the results for all the S*S grids. The output of the network is of the shape S*S*(B * 5 + C), where B is the number of anchor boxes and C is the number of object classes <ref type="bibr" coords="4,367.78,500.46,9.96,8.74" target="#b7">[8]</ref>. It generates B number of possible detections for each grid. So instead of separately regressing for region proposals, YOLO considers an end-to-end regression task for detection.</p><p>The architecture as mentioned by <ref type="bibr" coords="4,302.20,536.44,9.96,8.74" target="#b0">[1]</ref>, consists of CSPDarknet53 backbone, SPP additional module, PANet path-aggregation neck, and YOLOv3 head. YOLOv4 also uses techniques such as 'Bag of Freebies' and 'Bag of Special' to introduce data augmentation, mish activation, cross-stage partial connection and multi-input weighted residual connections in the network .</p><p>With YOLOv4, augmentation techniques such as CutMix, Mosaic, class label smoothing and self-adversarial training were trialed to improve the performance. By CutMix, parts of an image is replaced by another image, with annotations from both image parts. Similarly in mosaic augmentation, 4 different images are combined in various ratios, which thereby improves the recognition of objects of different scales. To reduce model overfitting on the classification output, the con-fidence scores are intentionally reduced by label smoothing technique. Finally by Self-Adversarial training (SAT), the images are passed normally in the forward step. Instead of back propagating, the loss output is used to distort the image in a way it harms the model. These augmented images are then used to train the model in a generic fashion. With these additional improvements YOLOv4 performs more accurately as compared to its earlier versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In the ImageClef DrawnUI 2020 Challenge, our team CudaMemError1 achieved a top submission rank of 2, with Overall Precision of 0.9504. Two additional metrics namely, mAP@IoU 0.5 and recall@IoU 0.5 were further used to evaluate the models. We can observe that, Cascade RCNN (67972) scored better than YOLOv4 (67706) by 1.7% in terms of Overall Precision but, YOLOv4 outperformed Cascade RCNN by 10.9% and 7.5% in mAP@IoU 0.5 and recall@IoU 0.5 respectively. Comparing with the baseline Faster RCNN model, a 38.7% improvement in mAP@IoU 0.5 and 48.3% improvement in recall@IoU 0.5 was achieved with YOLOv4 model and a 25% improvement in mAP@IoU 0.5 and 37.9% in recall@IoU 0.5 is achieved with the Cascade RCNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Two different architectures namely YOLOv4 and Cascade RCNN were implemented for detecting UI elements from hand drawn sketches. YOLOv4 performed significantly better than the baseline and Cascade RCNN in terms of mAP@IoU 0.5 and recall@IoU 0.5 with a score of 79.36 and 59.8 respectively, while Cascade RCNN had an Overall Precision score of 0.9504. Front end development can be streamlined and made easy with these detection systems. Automatic code generation can be further added to this pipeline in order to make the whole process of front end development simple and convenient. These detection systems will be an asset for developers to be able to keep up with ever-growing software engineering world, and also for non-developers to get into the digital space without learning how to code.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,245.57,345.83,8.74;4,134.77,257.52,345.82,8.74;4,134.77,269.48,345.83,8.74;4,134.77,122.80,126.30,96.30"><head>Fig. 1 :</head><label>1</label><figDesc>Fig.1: The architectures of different frameworks. "I" is input image, "conv" backbone convolutions, "pool" region-wise feature extraction, "H" network head, "B" bounding box, and "C" classification. "B0" is proposals in all architectures.</figDesc><graphic coords="4,134.77,122.80,126.30,96.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,203.60,326.07,208.17,53.87"><head>Table 1 :</head><label>1</label><figDesc>Development and testing distributions.</figDesc><table coords="3,235.17,338.38,145.01,41.56"><row><cell cols="2">Data Type Number of samples</cell></row><row><cell>Development</cell><cell>2,363</cell></row><row><cell>Testing</cell><cell>587</cell></row><row><cell>Total</cell><cell>2,950</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,136.16,311.77,354.20,133.87"><head>Table 2 :</head><label>2</label><figDesc>Scores on test images</figDesc><table coords="5,136.16,324.08,354.20,121.56"><row><cell>Run ID</cell><cell>Model</cell><cell cols="3">Overall precision mAP@IoU 0.5 recall@IoU 0.5</cell></row><row><cell cols="2">67413 baseline Faster RCNN</cell><cell>0.94789</cell><cell>57.2</cell><cell>40.3</cell></row><row><cell>67833</cell><cell>Cascade RCNN</cell><cell>0.95035</cell><cell>68.16</cell><cell>53.3</cell></row><row><cell>67710</cell><cell>Cascade RCNN</cell><cell>0.94909</cell><cell>64.92</cell><cell>50.5</cell></row><row><cell>67722</cell><cell>Cascade RCNN</cell><cell>0.93463</cell><cell>72.33</cell><cell>58.5</cell></row><row><cell>67829</cell><cell>YOLOv4</cell><cell>0.93300</cell><cell>73.82</cell><cell>55.6</cell></row><row><cell>67707</cell><cell>YOLOv4</cell><cell>0.93125</cell><cell>79.24</cell><cell>59.4</cell></row><row><cell>67831</cell><cell>YOLOv4</cell><cell>0.92987</cell><cell>79.11</cell><cell>60</cell></row><row><cell>67972</cell><cell>Cascade RCNN</cell><cell>0.95044</cell><cell>71.53</cell><cell>55.6</cell></row><row><cell>67706</cell><cell>YOLOv4</cell><cell>0.93437</cell><cell>79.36</cell><cell>59.8</cell></row><row><cell></cell><cell cols="3">Run 67972: 10000 iterations; Run 67706: 7000 iterations</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,210.33,337.64,7.86;6,151.52,221.29,103.94,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,333.16,210.33,147.43,7.86;6,151.52,221.29,75.27,7.86">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">Y M</forename><surname>Liao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,232.25,337.63,7.86;6,151.52,243.21,81.72,7.86" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<title level="m" coord="6,250.37,232.25,230.22,7.86;6,151.52,243.21,53.04,7.86">Cascade r-cnn: High quality object detection and instance segmentation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,254.17,337.64,7.86;6,151.52,265.13,329.07,7.86;6,151.52,276.09,329.07,7.86;6,151.52,287.05,329.07,7.86;6,151.52,298.01,95.77,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,202.45,265.13,278.14,7.86;6,151.52,276.09,147.22,7.86">Overview of ImageCLEFdrawnUI 2020: The Detection and Recognition of Hand Drawn Website UIs Task</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="6,318.22,276.09,102.52,7.86">CLEF2020 Working Notes</title>
		<title level="s" coord="6,427.05,276.09,53.55,7.86;6,151.52,287.05,124.11,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,308.96,132.42,7.86" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m" coord="6,205.54,308.96,41.16,7.86">Fast r-cnn</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,319.92,337.63,7.86;6,151.52,330.88,224.65,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,348.04,319.92,132.55,7.86;6,151.52,330.88,195.97,7.86">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,341.84,337.63,7.86;6,151.52,352.80,152.48,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,295.90,341.84,184.69,7.86;6,151.52,352.80,123.81,7.86">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,363.76,337.64,7.86;6,151.52,374.72,329.07,7.86;6,151.52,385.68,329.07,7.86;6,151.52,396.64,329.07,7.86;6,151.52,407.59,329.07,7.86;6,151.52,418.55,329.07,7.86;6,151.52,429.51,329.07,7.86;6,151.52,440.47,329.07,7.86;6,151.52,451.43,329.07,7.86;6,151.52,462.39,329.07,7.86;6,151.52,473.35,34.31,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,295.04,418.55,185.55,7.86;6,151.52,429.51,255.37,7.86">Overview of the ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,426.43,429.51,54.16,7.86;6,151.52,440.47,329.07,7.86;6,151.52,451.43,253.34,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="6,456.14,451.43,24.45,7.86;6,151.52,462.39,138.63,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="6,142.96,484.31,337.63,7.86;6,151.52,495.27,132.38,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,363.82,484.31,116.76,7.86;6,151.52,495.27,103.71,7.86">You only look once: Unified, real-time object detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,506.22,274.75,7.86" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m" coord="6,255.09,506.22,133.95,7.86">Yolo9000: Better, faster, stronger</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,517.18,291.20,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="6,255.09,517.18,150.06,7.86">Yolov3: An incremental improvement</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,528.14,337.97,7.86;6,151.52,539.10,329.07,7.86;6,151.52,550.06,104.35,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,301.55,528.14,179.04,7.86;6,151.52,539.10,137.84,7.86">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,310.29,539.10,170.29,7.86;6,151.52,550.06,29.48,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,561.02,337.98,7.86;6,151.52,571.98,315.81,7.86" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<title level="m" coord="6,304.72,571.98,133.94,7.86">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,582.94,337.98,7.86;6,151.52,593.90,54.55,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="6,323.97,582.94,156.62,7.86;6,151.52,593.90,25.88,7.86">Object detection with deep learning: A review</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
