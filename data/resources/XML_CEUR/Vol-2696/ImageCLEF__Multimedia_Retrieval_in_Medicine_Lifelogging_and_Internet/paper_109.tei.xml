<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.31,115.96,308.75,12.62;1,242.36,133.89,130.63,12.62;1,185.07,151.82,245.21,12.62">Concept Detection in Medical Images using Xception Models -TUC MC at ImageCLEFmed 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,174.89,194.87,55.46,8.74"><forename type="first">Nisnab</forename><surname>Udas</surname></persName>
							<email>nisnab.udas@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,240.90,194.87,65.64,8.74"><forename type="first">Frederik</forename><surname>Beuth</surname></persName>
							<email>beuth@cs.tu-chemnitz.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.43,194.87,69.63,8.74"><forename type="first">Danny</forename><surname>Kowerko</surname></persName>
							<email>danny.kowerko@cs.tu-chemnitz.de</email>
							<affiliation key="aff2">
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.31,115.96,308.75,12.62;1,242.36,133.89,130.63,12.62;1,185.07,151.82,245.21,12.62">Concept Detection in Medical Images using Xception Models -TUC MC at ImageCLEFmed 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F0348BE70F20FCB2BBC5F6E215920B42</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper summarizes the approach and the results of the submission of the Media Computing group from the Chemnitz University of Technology (TUC MC) at ImageCLEFmed Caption task, launched by ImageCLEF 2020. In the task, contents of medical images have to be detected, for the goal of a better diagnosis of medical diseases and conditions. In the context of a master thesis by Nisnab Udas, Xception model, which slightly outperformed InceptionV3 on the ImageNet dataset in 2017, was adapted to this caption task. Out-of-the-box, his approach achieved an F1 score of 35.1% compared to the best contribution with 39.4%, which places our team in the top-5. Part of his strategy was to optimize the confidence threshold, and to bring in a max pooling in the last layer which reduced the number of parameters making the model less prone to overfitting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computer science challenges have been established in the last decades to advance diverse problems in text, audio and video processing <ref type="bibr" coords="1,366.93,508.44,9.96,8.74" target="#b3">[4]</ref>. In this tradition, challenges are organized within the established ImageCLEF or LifeCLEF lab since 2003 and 2014, respectively. Since 2003, medical (retrieval) tasks have been part of the challenge and been continuously developed into 3 subtasks, where one is called medical concept detection since 2017 <ref type="bibr" coords="1,337.10,556.26,11.62,8.74" target="#b1">[2,</ref><ref type="bibr" coords="1,348.72,556.26,11.62,8.74" target="#b13">13,</ref><ref type="bibr" coords="1,360.35,556.26,11.62,8.74" target="#b10">10,</ref><ref type="bibr" coords="1,371.97,556.26,7.75,8.74" target="#b5">6]</ref>. It contains automatic image captioning and scene understanding to identify the presence and location of relevant concepts in a large corpus of medical images. The latter stem from the PubMed Open Access subset containing 1,828,575 archives. A total number of 6,031,814 image -caption pairs were extracted. A combination of automatic filtering with deep learning systems and manual revisions was applied to focus merely on radiology images and non-compound figures. The origin of the biomedical images distributed in this challenge is a subset from the extended ROCO (Radiology Objects in COntext) dataset <ref type="bibr" coords="2,317.51,282.07,14.61,8.74" target="#b11">[11]</ref>. In ImageCLEF 2020, additional information regarding the modalities of all 80,747 images was distributed <ref type="bibr" coords="2,457.22,294.02,9.96,8.74" target="#b5">[6]</ref>.</p><p>Evaluation is conducted in terms of set coverage metrics such as precision, recall, and combinations thereof. Leaderboards utilize the F1 metric summarized in Table <ref type="table" coords="2,174.75,329.89,3.87,8.74" target="#tab_0">1</ref>. The results prove that the task remains challenging even though a continuous improvement from year to year is to be noted. The results of this year bring the top-3 group closer together for the first time. In the caption task of 2019 <ref type="bibr" coords="2,192.55,365.76,14.60,8.74" target="#b10">[10]</ref>, Kougia et al. won the competition by combining their CNN (Convolutional Neural Network) image encoders with an image retrieval method or a feed-forward neural network and achieved an F1 score of 28.2% <ref type="bibr" coords="2,427.33,389.67,9.96,8.74" target="#b6">[7]</ref>. Xu et al. applied a multilabel classification model based on ResNet <ref type="bibr" coords="2,383.40,401.62,10.52,8.74" target="#b4">[5]</ref> and achieved 26.6% <ref type="bibr" coords="2,134.77,413.58,14.61,8.74" target="#b14">[14]</ref>. Guo et al. achieved 22.4% F1 score with a two-stage concept including the medical image pre-classification based on body parts with AlexNet ( <ref type="bibr" coords="2,430.12,425.53,10.96,8.74" target="#b7">[8]</ref>) and the transfer learning-based multi-label classification model based on Inception V3 and Resnet152 <ref type="bibr" coords="2,201.93,449.44,9.96,8.74" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Analysis</head><p>The amount of images has increased from 2019 to 2020. The concept detection task this year, contains training and validation images in 7 separate folders. In total, there are 64,753 training images, 14,970 validation images and 3,534 test images, respectively. Concept frequency was reduced from 5,528 last year to 3,047 in 2020 as low occuring concepts were removed by the organizers. Top-20 concepts in our training images are shown in Fig. <ref type="figure" coords="2,357.94,560.48,3.87,8.74" target="#fig_0">1</ref>. Concepts 'C0040405' and 'C0040398' both occur 25022 images in training images. The figure clearly shows how our concepts are imbalanced in the dataset.</p><p>In Fig. <ref type="figure" coords="2,170.48,608.30,3.87,8.74" target="#fig_1">2</ref>, we show distribution of concept length in training dataset. Maximum number of images, 5,248, to be specific, have only 2 concepts. The second and third largest group of images have 3 and 4 concepts per image, respectively. The highest number of concepts occurring in an image is 140 which occurs one time.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed System</head><p>Our deep learning based architecture was based on Xception architecture <ref type="bibr" coords="4,451.34,428.97,10.52,8.74" target="#b0">[1]</ref> and is shown in Fig. <ref type="figure" coords="4,209.93,440.92,3.87,8.74" target="#fig_2">3</ref>. The Xception model slightly outperforms Inception V3 on the ImageNet dataset in 2017, and was chosen due to this performance and as our preliminary test found it well working on our medical detection task. For fine-tuning concerning model, we utilize transfer learning and use weights pretrained on ImageNet Dataset <ref type="bibr" coords="4,264.41,488.75,14.61,8.74" target="#b12">[12]</ref>. We then eliminated the top classifier layer as is required in transfer learning. We froze the entire Xception model and made only the last six layers trainable. Generally, as in transfer learning, before adding classifier to a pre-trained model, the layer is flattened. Flattening transforms a 2D matrix of features to the vector, which can be provided to a fully-connected layer (FC layer). In our case, we used a max pooling layer of window size (2,2) followed by the dropout layer to reduce the number of free parameters and facilitate object-size dependent pooling as a special trick. Afterwards, the usual flattening layer is added.</p><p>Subsequently to adding the flattening layer, we used the ReLU activation function, followed by the dropout layer. The data contains 3,047 concepts in total, thus our final FC layer contains 3,047 units and sigmoid as our activation function because we are dealing with a multi-label problem. The white rings in the FC layer represent neurons (Fig. <ref type="figure" coords="4,310.63,644.16,3.87,8.74" target="#fig_2">3</ref>). The top lambda layer extracts top-100 highest probabilities. These probabilities are compared against a threshold value, e.g. t=0.12, which generates boolean values for these 100 probabilities. The lower lambda layer gives the index of these individual neurons/concepts. In data processing, these indices are used to locate only the neurons with 'True' boolean value. In the data processing part, results are reformatted into the competition format. Table <ref type="table" coords="5,199.55,377.32,4.98,8.74" target="#tab_1">2</ref> shows how feature maps change shape after passing through each layer.</p><p>For training our network, a proper optimization is necessary and we conduct the following optimization methods. The major contribution to a satisfying F1 score had the optimization of the confidence threshold, along with the maxpooling, as shown in the next sections. Besides these two methods, we deploy other minor approaches to raise the performance. These include the tuning of the drop-out level and the data augmentation level. Drop-out is a well-suitable technique to avoid overfitting and the values were optimized for both neuronal layers of drop-out (Table <ref type="table" coords="5,249.58,484.92,4.43,8.74" target="#tab_1">2</ref>) via conducting a cross-test of 25 different combinations. The best configuration has a drop-out value of 0.2 for the first layer and 0.5 for the second layer. Additionally, data augmentation was tuned, also increasing the F1 score value by 0.01-0.02 depending on the configuration. Each of the methods raise the F1 score by 0.01 -0.02 only, but in total, these effects add up to an elevate of the F1 score by a level of 0.03 -0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>One of the ideas for improving the original Xception model <ref type="bibr" coords="5,404.11,596.34,10.52,8.74" target="#b0">[1]</ref> was the introduction of an additional max-pooling operation before the highest layer. It is shown in Table <ref type="table" coords="5,207.29,620.25,4.98,8.74" target="#tab_1">2</ref> in the second entry. This particular max-pooling operation reduces, on the application set, the spatial resolution, inducing a reduction of the free parameters in the next layer. In our dataset, the layer before the maxpooling operation had a 5 × 5 resolution, which is reduced by a 2 × 2 pooling to a 2 × 2 layer resolution. This operation reduces the free trainable parameters from 160, 758, 247 parameters to 29, 712, 871 parameters in total, which fabricates a more robust and stable model. As a second argument, the operation allows the recognition of concepts in the image more independently from the position. In the original ImageNet data set, objects are larger on average than in our medical image data set. To compensate for this difference in size, we increase the pooling as the objects in the original dataset cover large portions of the image, while our concepts are typically appearing in a smaller region. The pooling operation allows both, a recognition independent of the concept's place, and smaller object sensitive filters facilitating the recognition of smaller objects. The difference in F1 score and performance, i.e. free trainable parameters, is shown in Table <ref type="table" coords="6,464.64,487.90,3.87,8.74" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Confidence threshold optimization</head><p>Fig. <ref type="figure" coords="6,155.22,537.72,4.98,8.74">4</ref> shows the threshold variation against accuracy and F1. A classical accuracy metric is not optimal for training a model in this challenge as we have a large class imbalance. Therefore, the F1 score metric is used.</p><p>Confidence threshold selection plays a crucial role in the multi-label problem. The threshold determines over which predicted probability a concept is mapped to our image or not. When a class is predicted, the network outputs a probability and only probabilities exceeding a certain threshold are counted as that this concept is in that image. Given that our model is well trained, an unoptimized threshold may still have a substantial effect on our result. And determining the optimum threshold can often be tricky. Therefore, we varied the threshold systematically and tuned the value of the threshold on the validation set, shown in Fig. <ref type="figure" coords="7,345.39,130.95,3.87,8.74">4</ref>. The maximum performance with respect to the confidence threshold is identified to range around 0.1 to 0.25. Hence, we submitted several runs with different threshold values between θ = 0.12 and θ = 0.25 (see Table <ref type="table" coords="7,278.73,166.81,3.87,8.74" target="#tab_3">4</ref>). As expected from the Fig. <ref type="figure" coords="7,409.37,166.81,3.87,8.74">4</ref>, improvements of F1 are within a large amount.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Optimization techniques</head><p>There are plenty of ways of managing limited data volume and imbalanced datasets such as eliminating outliers, expanding the data set, augmentation, etc. In the medical image domain, few type of disease or conditions occurs less frequently to humans resulting in less sample numbers. Thus, to tackle these problems, we decided to use image data augmentation. Available methods are for example in Keras the following, which we employ as parameterized below:</p><p>-Rotation is performed by randomly rotating an image around its center of up to 5 • . -Vertical and horizontal flip. Flipping images is one of the most widely implemented techniques popularized by <ref type="bibr" coords="7,316.17,345.57,9.96,8.74" target="#b7">[8]</ref>. -Height and width shift range: The images are randomly shifted horizontally or vertically up to 5% of the total height and width respectively. -Zoom: Objects in images are randomly zoomed in a range of ±5%.</p><p>-Brightness shift: The image is randomly darkening or brightening in range of 80 -120% of the initial brightness. -Samplewise center: To eliminate the problem of vanishing gradients or saturating values, data are normalized in such a way that the mean value of each data sample becomes 0. -Samplewise standard normalization: This pre-processing method approaches the same concept as sample-wise centering, but rather it fixes the standard deviation to 1.</p><p>The enabling of data augmentation increases the F1 score and contributes to a more robust working of the system.</p><p>The competition requires only 100 concepts per image. Therefore, to ensure that, probabilities were sorted in descending order and Top-100 probabilities were selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Run description</head><p>We submitted ten runs (Table <ref type="table" coords="7,266.22,608.30,3.87,8.74" target="#tab_3">4</ref>). The runs often utilize the same base-structure, an Xception model, and all use transfer learning from ImageNet. The runs vary in meta-parameters as we tested different ones. We vary primarily: (i) the threshold in the last layer, (ii) slightly different base-models, and (iii) with and without max-pooling in the highest layers. Run ID 1/68024 We deploy an Xception model and utilize transfer learning from ImageNet. We ran our model for N=100 epochs and set the learning rate to 1e-3. The model uses the confidence threshold theta in the last layer to map concepts probabilities to true/false for the concepts. We tuned the threshold to 0.15 from the validation set, selected the top 100 concepts and submitted our results. Run ID 2/68029 This run again uses the Xception model and generally the configuration of Run ID 1. It optimizes the threshold further, setting it to 0.20. Run ID 3/68034 We again deploy an Xception model and utilize transfer learning from ImageNet. This submission has a more streamed-lined source code structure and explores different meta-parameters: We ran our model for N=30 epochs and set the learning rate to 1e-2. We tuned the threshold to 0.15 from the validation set. Run ID 4/68045 We again deploy an Xception model and utilize the configuration of run 1. This submission explores different meta-parameters: We ran our model for N=50 epochs and set the learning rate to 1e-4. We tuned the threshold to 0.20 from the validation set. Run ID 5/68067 This run again uses the Xception model and generally the configuration of Run ID 3, while exploring which effect has the max-pooling layer before the highest layer. The max-pooling was removed here to show the effect. Run ID 6/68073 This run uses the more streamed-lined source code structure and explores again different meta-parameters: We ran our model for N=30 epochs and set the learning rate to 1e-2. We tuned the threshold to 0.12 from the validation set. Run ID 7/68074 This run again uses the Xception model and general the configuration of Run ID 6, while tuning the threshold to 0.25. Run ID 8/68076 We again deploy the standard configuration of Run ID 1.</p><p>This submission focuses on an experimental normalizing of the dataset, but was not very successfully.</p><p>Run ID 9/68077 We again deploy an Xception model and utilize transfer learning from ImageNet. This submission explores an early stop strategy: the best, i.e. lowest loss, was used over a run period of N=30 epochs. The learning rate was 1e-3 and the threshold was tuned to 0.18. Run ID 10/68078 This run deploys the more streamed-lined source code structure and explores a different threshold: 0.20.</p><p>In Table <ref type="table" coords="9,189.60,200.69,3.87,8.74" target="#tab_4">5</ref>, we listed the top teams with their best F1 score in percent. Our team, TUC MC occupied 5th position in terms of team ranking with F1 score of 0.3512. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Outlook</head><p>Our approach of adapting an Xception model for the medical caption task 2020 achieves an F1 score of 35.1% which is better than the 2019 results and close to the best contributions of 2020 which achieved 39.4%. Our strategies to rely on a modern Xception neural network proved to be successful. It also shows that transfer learning, with weights pre-learned on ImageNet, is very usable on an indeed different image material such as medical images. The introduction of a max pooling in the last layer, and to optimize the confidence threshold, have boosted the performance of our Xception model. Further investigation could lead in the direction of optimization learning through entropy-based analysis concepts of neural networks. Moreover, a more in-depth analysis of certain concept classes might be carried out in order to better understand the errors in the present classification task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,205.79,370.74,203.78,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Frequency distribution of Top-20 concepts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,199.26,635.20,216.84,7.89"><head>O c c u r r e n c e N u m b e r o f c o n c e p t s p e r i m a g e T r a i n i n g i m a g e sFig. 2 .</head><label>2</label><figDesc>Fig. 2. Frequency distribution of the concepts length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,134.77,362.82,345.83,7.89;4,134.77,373.81,97.81,7.86;4,152.06,115.83,311.24,236.95"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Model architecture describing the mechanism of concept detection from pretrained Xception model.</figDesc><graphic coords="4,152.06,115.83,311.24,236.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,160.29,334.62,294.77,7.89"><head>8 A c c u r a c y F 1 EFig. 4 .</head><label>814</label><figDesc>Fig. 4. Variation in F1 score and accuracy with respect to the threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,134.77,116.41,345.83,116.23"><head>Table 1 .</head><label>1</label><figDesc>Columns 2 to 4 show the F1 metric results in % for the best submission run of the top 3 teams at medical caption subtask since 2017. Note, that the number of registrations is usually considerably larger than the number of teams who submit results (last column).</figDesc><table coords="2,224.84,169.59,168.74,63.06"><row><cell>Year</cell><cell>1st</cell><cell>2nd</cell><cell>3rd</cell><cell>No. of teams</cell></row><row><cell>2017</cell><cell>12.1</cell><cell>9.6</cell><cell>5.0</cell><cell>4</cell></row><row><cell>2018</cell><cell>25.0</cell><cell>18.0</cell><cell>17.3</cell><cell>5</cell></row><row><cell>2019</cell><cell>28.2</cell><cell>26.6</cell><cell>22.4</cell><cell>11</cell></row><row><cell>2020</cell><cell>39.4</cell><cell>39.2</cell><cell>38.1</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,115.91,345.83,115.74"><head>Table 2 .</head><label>2</label><figDesc>Model summary with layers and feature maps changing shapes as it passes through these layers.</figDesc><table coords="5,219.71,146.67,163.63,84.97"><row><cell>Layer</cell><cell>Output shape</cell></row><row><cell>Xception</cell><cell>16 × 5 × 5×2048</cell></row><row><cell>Max pooling</cell><cell>16 × 2 × 2×2048</cell></row><row><cell>Dropout</cell><cell>16 × 2 × 2×2048</cell></row><row><cell>Flatten</cell><cell>16 × 8192</cell></row><row><cell>Activation</cell><cell>16 × 8192</cell></row><row><cell>Dropout</cell><cell>16 × 8192</cell></row><row><cell>Dense</cell><cell>16 × 3047</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,162.16,253.81,299.37,49.98"><head>Table 3 .</head><label>3</label><figDesc>Influence of the high-level max-pooling.</figDesc><table coords="5,162.16,273.61,299.37,30.18"><row><cell>Method</cell><cell>Mean F1 score</cell><cell>Free trainable parameters</cell></row><row><cell>Base model</cell><cell>0.349</cell><cell>29,712,871</cell></row><row><cell>Without max-pooling</cell><cell>0.345</cell><cell>160,758,247</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,162.99,115.91,289.38,135.91"><head>Table 4 .</head><label>4</label><figDesc>Test results of our 10 submitted runs. For details see text.</figDesc><table coords="8,162.99,133.97,289.38,117.85"><row><cell>Run id</cell><cell>Method</cell><cell>Name</cell><cell cols="2">Mean F1 score Rank</cell></row><row><cell cols="2">68077 Early stopping</cell><cell>model thr0 18.csv</cell><cell>0.351</cell><cell>20</cell></row><row><cell cols="3">68078 CNN2, θ = 0.25 streamlined1 thr0 25.csv</cell><cell>0.349</cell><cell>21</cell></row><row><cell cols="3">68034 CNN2, θ = 0.20 streamlined1 thr0 20.csv</cell><cell>0.349</cell><cell>22</cell></row><row><cell cols="2">68074 CNN2, θ = 0.15</cell><cell>streamlined1.csv</cell><cell>0.349</cell><cell>23</cell></row><row><cell cols="3">68029 CNN1, θ = 0.20 basemodel thr0 20.csv</cell><cell>0.347</cell><cell>24</cell></row><row><cell>68045</cell><cell cols="2">Slow learning model low lr thr0 20.csv</cell><cell>0.345</cell><cell>25</cell></row><row><cell cols="3">68067 No max-pooling streamlined1 nomax.csv</cell><cell>0.345</cell><cell>27</cell></row><row><cell cols="2">68024 CNN1, θ = 0.15</cell><cell>basemodel.csv</cell><cell>0.343</cell><cell>28</cell></row><row><cell cols="3">68073 CNN2, θ = 0.12 streamlined1 thr0 12.csv</cell><cell>0.342</cell><cell>29</cell></row><row><cell cols="3">68076 Exp. Normalizing model weighting.csv</cell><cell>0.332</cell><cell>32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,134.77,254.16,345.82,116.23"><head>Table 5 .</head><label>5</label><figDesc>Top-7 team performance in concept detection problem 2020. F1 metrics are given in percent.<ref type="bibr" coords="9,204.94,265.14,9.73,7.86" target="#b9">[9]</ref> </figDesc><table coords="9,229.12,285.41,142.57,84.97"><row><cell>Group Name</cell><cell>F1 score</cell></row><row><cell>AUEB NLP Group</cell><cell>39.4</cell></row><row><cell>PwC MedCaption 2020</cell><cell>39.2</cell></row><row><cell>essexgp2020</cell><cell>38.1</cell></row><row><cell>iml</cell><cell>37.5</cell></row><row><cell>TUC MC</cell><cell>35.1</cell></row><row><cell>Morgan CS</cell><cell>16.7</cell></row><row><cell>saradadevi</cell><cell>13.5</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,142.28,337.64,7.86;10,151.52,153.24,329.07,7.86;10,151.52,164.20,329.07,7.86;10,151.52,175.16,166.20,7.86;10,353.49,175.80,127.10,7.47;10,151.52,186.76,80.03,7.47" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,212.83,142.28,267.76,7.86;10,151.52,153.24,35.06,7.86">Xception: Deep Learning with Depthwise Separable Convolutions</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.195</idno>
		<ptr target="http://ieeexplore.ieee.org/document/8099678/" />
	</analytic>
	<monogr>
		<title level="m" coord="10,217.75,153.24,262.84,7.86;10,151.52,164.20,88.08,7.86">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">Jul 2017</date>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,197.01,337.63,7.86;10,151.52,207.97,329.07,7.86;10,151.52,218.91,329.07,7.89;10,151.52,229.89,160.45,8.12" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,296.25,197.01,184.34,7.86;10,151.52,207.97,262.16,7.86">Overview of ImageCLEFcaption 2017 -Image Caption Prediction and Concept Detection for Biomedical Images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1866/" />
	</analytic>
	<monogr>
		<title level="m" coord="10,420.86,207.97,59.73,7.86;10,151.52,218.91,280.71,7.89">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation Forum 1866</title>
		<imprint>
			<date type="published" when="2017-09">Sep 2017</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,240.79,337.64,7.86;10,151.52,251.75,329.07,7.86;10,151.52,262.68,329.07,7.89;10,151.52,274.31,189.29,7.47" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,302.99,240.79,177.60,7.86;10,151.52,251.75,223.00,7.86">ImageSem at ImageCLEFmed Caption 2019 Task: a Two-stage Medical Concept Detection Strategy</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/paper_80.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,381.84,251.75,98.75,7.86;10,151.52,262.68,249.74,7.89">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum 2380</title>
		<imprint>
			<date type="published" when="2019-09">Sep 2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,284.56,337.63,7.86;10,151.52,295.52,329.07,7.86;10,151.52,306.48,329.07,7.86;10,151.52,317.44,248.76,8.11" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,218.86,306.48,186.11,7.86">Evaluation-as-a-Service: Overview and Outlook</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.07454[cs]pp.1-28</idno>
		<ptr target="http://arxiv.org/abs/1512.07454" />
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,328.34,337.64,7.86;10,151.52,339.30,329.08,7.86;10,151.52,350.26,76.80,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,299.05,328.34,177.61,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,165.33,339.30,310.07,7.86">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,361.15,337.64,7.86;10,151.52,372.11,329.07,7.86;10,151.52,383.07,329.07,7.86;10,151.52,394.03,329.07,7.86;10,151.52,404.99,329.07,7.86;10,151.52,415.95,329.07,7.86;10,151.52,426.91,329.07,7.86;10,151.52,437.87,329.07,7.86;10,151.52,448.82,329.07,7.86;10,151.52,459.78,329.07,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,319.80,415.95,160.79,7.86;10,151.52,426.91,213.76,7.86">Imageclef 2020: Multimedia retrieval in medical, lifelogging, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,386.32,426.91,94.27,7.86;10,151.52,437.87,329.07,7.86;10,151.52,448.82,221.86,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="10,423.70,448.82,56.90,7.86;10,151.52,459.78,105.60,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="10,142.96,470.68,337.64,7.86;10,151.52,481.64,329.07,7.86;10,151.52,492.60,329.07,7.86;10,151.52,504.20,194.00,7.47" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,358.30,470.68,122.29,7.86;10,151.52,481.64,76.72,7.86">AUEB NLP Group at Image-CLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/paper_136.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,268.84,481.64,211.75,7.86;10,151.52,492.60,101.30,7.86">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">2019. Sep 2019</date>
			<biblScope unit="volume">2380</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,514.45,337.63,7.86;10,151.52,525.41,329.07,7.86;10,151.52,536.37,329.07,7.86;10,151.52,547.33,60.87,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,337.07,514.45,143.52,7.86;10,151.52,525.41,130.28,7.86">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,252.34,536.37,212.94,7.86">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,216.45,547.33,264.15,8.12;10,151.52,558.94,328.29,7.47;10,151.52,569.90,14.12,7.47" xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Inc</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,580.15,337.63,7.86;10,151.52,591.11,329.07,7.86;10,151.52,602.06,329.07,7.86;10,151.52,613.02,162.22,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,417.53,580.15,63.06,7.86;10,151.52,591.11,310.25,7.86">Overview of the ImageCLEFmed 2020 concept prediction task: Medical image understanding</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,602.06,297.85,7.86">CLEF2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,623.92,337.98,7.86;10,151.52,634.88,329.07,7.86;10,151.52,645.84,329.07,7.86;10,151.52,656.80,292.83,8.12" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,414.60,623.92,65.99,7.86;10,151.52,634.88,186.72,7.86">Overview of the ImageCLEFmed 2019 Concept Detection Task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/paper_245.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,360.08,634.88,120.51,7.86;10,151.52,645.84,200.36,7.86">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">Sep 2019</date>
			<biblScope unit="volume">2380</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,119.67,337.97,7.86;11,151.52,130.63,329.07,7.86;11,151.52,141.59,329.07,7.86;11,151.52,152.55,329.07,7.86;11,151.52,163.51,329.07,7.86;11,151.52,174.47,329.07,7.86;11,151.52,185.43,329.07,7.86;11,151.52,196.39,329.07,7.86;11,151.52,207.34,329.07,8.12;11,151.52,218.30,128.87,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,420.01,119.67,60.58,7.86;11,151.52,130.63,245.51,7.86;11,400.57,163.51,80.02,7.86;11,151.52,174.47,329.07,7.86;11,151.52,185.43,154.44,7.86">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Balocco</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01364-6_20</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-030-01364-6_20,seriesTitle:Lec-ture" />
	</analytic>
	<monogr>
		<title level="s" coord="11,171.00,218.30,109.39,7.86">Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Martel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Duong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Zahnd</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Demirci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Moriconi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Cheplygina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Mateus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Trucco</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Granger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11043</biblScope>
			<biblScope unit="page" from="180" to="189" />
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Radiology Objects in COntext (ROCO): A Multimodal Image Dataset</note>
</biblStruct>

<biblStruct coords="11,142.62,229.26,337.97,7.86;11,151.52,240.22,329.07,7.86;11,151.52,251.15,329.07,7.89;11,151.52,262.14,329.07,8.12;11,151.52,273.74,138.00,7.47" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,441.68,240.22,38.92,7.86;11,151.52,251.18,172.44,7.86">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="http://link.springer.com/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j" coord="11,332.76,251.18,76.15,7.86">Int J Comput Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015-12">Dec 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,284.06,337.97,7.86;11,151.52,295.02,329.07,7.86;11,151.52,305.98,329.07,7.86;11,151.52,316.93,300.73,8.11" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,431.28,284.06,49.31,7.86;11,151.52,295.02,189.34,7.86">Overview of the ImageCLEF 2018 Caption Prediction Tasks</title>
		<author>
			<persName coords=""><forename type="first">Seco</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2125/invited_paper_4.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,361.76,295.02,118.83,7.86;11,151.52,305.98,198.05,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">Sep 2018</date>
			<biblScope unit="volume">2125</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,327.89,337.98,7.86;11,151.52,338.85,329.07,7.86;11,151.52,349.81,329.07,7.86;11,151.52,360.77,329.07,8.12;11,151.52,372.37,170.46,7.47" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,407.10,327.89,73.50,7.86;11,151.52,338.85,329.07,7.86;11,151.52,349.81,49.02,7.86">Concept detection based on multi-label classification and image captioning approach -DAMO at ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/paper_141.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,241.54,349.81,239.05,7.86;11,151.52,360.77,71.63,7.86">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">2019. Sep 2019</date>
			<biblScope unit="volume">2380</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
