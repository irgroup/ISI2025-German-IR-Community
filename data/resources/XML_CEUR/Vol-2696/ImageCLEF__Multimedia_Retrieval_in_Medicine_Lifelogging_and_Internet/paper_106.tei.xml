<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.34,115.96,338.69,12.62;1,175.07,133.89,265.21,12.62;1,184.18,151.82,247.00,12.62">Overview of the VQA-Med Task at ImageCLEF 2020: Visual Question Answering and Generation in the Medical Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,173.83,189.57,81.00,8.74"><forename type="first">Asma</forename><forename type="middle">Ben</forename><surname>Abacha</surname></persName>
							<email>asma.benabacha@nih.gov</email>
							<affiliation key="aff0">
								<orgName type="department">Lister Hill Center</orgName>
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.38,189.57,66.00,8.74"><forename type="first">Vivek</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
							<email>vivek.datla@philips.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Philips Research Cambridge</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,341.94,189.57,68.14,8.74"><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
							<email>sadidhasan@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">CVS Health</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,420.63,189.57,20.89,8.74;1,217.92,201.53,77.02,8.74"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Lister Hill Center</orgName>
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.87,201.53,68.10,8.74"><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<email>henning.mueller@hevs.ch</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Applied Sciences Western Switzerland</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.34,115.96,338.69,12.62;1,175.07,133.89,265.21,12.62;1,184.18,151.82,247.00,12.62">Overview of the VQA-Med Task at ImageCLEF 2020: Visual Question Answering and Generation in the Medical Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DD1CC964D1A28C340CEFA22C843225EB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>Visual Question Generation</term>
					<term>Data Creation</term>
					<term>Radiology Images</term>
					<term>Medical Questions and Answers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an overview of the Medical Visual Question Answering (VQA-Med) task at ImageCLEF 2020. This third edition of VQA-Med included two tasks: (i) Visual Question Answering (VQA), where participants were tasked with answering abnormality questions from the visual content of radiology images and (ii) Visual Question Generation (VQG), consisting of generating relevant questions about radiology images based on their visual content. In VQA-Med 2020, 11 teams participated in at least one of the two tasks and submitted a total of 62 runs. The best team achieved a BLEU score of 0.542 in the VQA task and 0.348 in the VQG task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the increasing interest in artificial intelligence technologies to support clinical decision making and improve patient engagement, opportunities to generate and leverage algorithms for automated medical image interpretation are being explored at a faster pace. The clinicians' confidence in interpreting complex medical images can be enhanced by a "second opinion" provided by an automated system. Also, since patients may now access structured and unstructured data related to their health via patient portals, such access motivates the need to help them better understand their conditions regarding their available data, including medical images.</p><p>To offer more training data and evaluation benchmarks, we organized the first visual question answering (VQA) task in the medical domain in 2018 <ref type="bibr" coords="2,447.25,130.95,9.96,8.74" target="#b3">[4]</ref>, and continued the task in 2019 <ref type="bibr" coords="2,249.92,142.90,10.52,8.74" target="#b1">[2]</ref> as part of the ImageCLEF initiatives <ref type="bibr" coords="2,423.34,142.90,9.96,8.74" target="#b5">[6]</ref>. Following the strong engagement from the research community in both editions of VQA in the medical domain (VQA-Med) and the ongoing interests from both the computer vision and the medical informatics communities, we continued the task this year (VQA-Med 2020) within the scope of ImageCLEF-2020 initiatives <ref type="bibr" coords="2,470.08,190.72,10.52,8.74" target="#b4">[5]</ref> by putting an enhanced focus on answering questions about abnormalities from the visual content of associated radiology images. Furthermore, we introduced an additional task this year, visual question generation (VQG), consisting of generating relevant questions about radiology images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>For the visual question answering task, similar to 2019, given a radiology medical image accompanied by a clinically relevant question, participating systems were tasked with answering the question based on the visual image content. In VQA-Med 2020, we specifically focused on questions about abnormality (e.g., "what is most alarming about this ultrasound image?"), which can be answered from the image content without requiring additional medical knowledge or domainspecific inference. Additionally, the visual question generation (VQG) task was introduced for the first time in this third edition of the VQA-Med challenge. This task required participants to generate relevant natural language questions about radiology images using their visual content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Creation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">VQA Data</head><p>For the visual question answering task, we automatically constructed the training, validation, and test sets by: (i) applying several filters to select relevant images and associated annotations, and, (ii) creating patterns to generate the questions and their answers. We selected relevant medical images from the Med-Pix 5 database with filters based on their captions, localities, and diagnosis methods. We selected only the cases where the diagnosis was made based on the image. Examples of the selected diagnosis methods include: CT/MRI imaging, angiography, characteristic imaging appearance, radiographs, imaging features, ultrasound, and diagnostic radiology.</p><p>Finally, we selected the list of abnormalities to be used to create the questionanswer pairs. The final list covers 330 medical problems; each problem occurs at least 10 times in the created VQA data.</p><p>Examples of medical problems (and their frequency) in the VQA data:</p><p>pulmonary embolism (114),</p><p>acute appendicitis (109), angiomyolipoma (68), osteochondroma (63), adenocarcinoma of the lung (60), sarcoidosis (58).</p><p>The VQA training set includes 4,000 radiology images with 4,000 Question-Answer (QA) pairs. The validation set consists of 500 radiology images with 500 QA pairs. The test set includes 500 radiology images and 500 questions. To further ensure the quality of the data, the test set was manually validated by a medical doctor. Figure <ref type="figure" coords="3,244.77,235.32,4.98,8.74" target="#fig_0">1</ref> presents examples from the VQA-Med-2020 test set. The participants were also encouraged to utilize the VQA-Med-2019 dataset as additional training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">VQG Data</head><p>For the visual question generation task, we automatically constructed the training, validation, and test sets in a similar fashion by using a separate collection of radiology images and their associated captions. We semi-automatically generated questions from the image captions first by using a rule-based sentenceto-question generation approach<ref type="foot" coords="3,274.67,356.39,3.97,6.12" target="#foot_0">6</ref> , and then, three annotators manually curated the list of question-answer pairs by removing or editing the noises related to grammatical inconsistencies. The final curated corpus for the VQG task was comprised of 780 radiology images with 2,156 associated questions (and answers) for training, 141 radiology images with 164 questions for validation, and 80 radiology images for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted Runs</head><p>Out of 47 online registrations, 30 participants submitted signed end user agreement forms. Finally, 11 groups submitted a total of 49 successful runs for the VQA task<ref type="foot" coords="3,178.25,496.96,3.97,6.12" target="#foot_1">7</ref> (cf. Figure <ref type="figure" coords="3,234.04,498.53,3.87,8.74" target="#fig_1">2</ref>), while 3 groups submitted a total of 13 successful runs for the VQG task<ref type="foot" coords="3,214.25,508.91,3.97,6.12" target="#foot_2">8</ref> , indicating a notable interest in the VQA-Med 2020 challenge. Table <ref type="table" coords="3,190.64,522.44,4.98,8.74" target="#tab_0">1</ref> and Table <ref type="table" coords="3,246.07,522.44,4.98,8.74">2</ref> give an overview of all participants and the number of submitted runs (please note that were allowed only 5 runs per team).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Similar to the evaluation setup of the VQA-Med 2019 challenge <ref type="bibr" coords="3,416.05,591.28,9.96,8.74" target="#b1">[2]</ref>, the evaluation of the participant systems for the VQA task in the VQA-Med 2020 challenge is also conducted based on two primary metrics: accuracy and BLEU. We used  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>Institution (s) # Runs bumjun jung <ref type="bibr" coords="5,191.19,150.66,9.73,7.86" target="#b6">[7]</ref> University of Tokyo and RIKEN AIP (Japan) 5 dhruv sharma Virginia Tech (USA) 1 HCP-MIC <ref type="bibr" coords="5,180.56,172.58,9.73,7.86" target="#b2">[3]</ref> School of Data and Computer Science, Sun Yat-Sen University (China)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>HARENDRAKV <ref type="bibr" coords="5,207.82,194.49,14.34,7.86" target="#b13">[14]</ref> Vadict Innovations and Quest Global (India) 5 kdevqa <ref type="bibr" coords="5,167.65,205.45,14.34,7.86" target="#b12">[13]</ref> Toyohashi University of Technology (Japan) 4 NLM <ref type="bibr" coords="5,160.35,216.41,14.34,7.86" target="#b10">[11]</ref> U.S. National Library of Medicine (USA) 5 sheerin Individual participation (India) 5 Shengyan <ref type="bibr" coords="5,177.63,238.33,9.73,7.86" target="#b8">[9]</ref> School of Information Science and Engineering, Yunnan University (China)</p><p>5</p><p>TheInceptionTeam <ref type="bibr" coords="5,215.01,260.25,9.73,7.86" target="#b0">[1]</ref> Jordan University of Science and Technology (Jordan) 5 umassmednlp University of Massachusetts Medical School (USA) 4 AIML <ref type="bibr" coords="5,163.68,282.16,9.73,7.86" target="#b7">[8]</ref> The Australian Institute for Machine Learning, University of Adelaide and South Australian Health and Medical Research Institute (Australia) 5</p><p>Table <ref type="table" coords="5,196.10,337.14,3.87,8.74">2</ref>: Participating groups in the VQA-Med 2020 VQG task.</p><p>Team Institution (s) # Runs NLM <ref type="bibr" coords="5,160.35,360.44,14.34,7.86" target="#b10">[11]</ref> U.S. National Library of Medicine (USA) 3 TheInceptionTeam <ref type="bibr" coords="5,215.01,371.40,9.73,7.86" target="#b0">[1]</ref> Jordan University of Science and Technology (Jordan) 5 AIML <ref type="bibr" coords="5,163.68,382.36,9.73,7.86" target="#b7">[8]</ref> The Australian Institute for Machine Learning, University of Adelaide and South Australian Health and Medical Research Institute (Australia)</p><formula xml:id="formula_0" coords="5,465.84,382.36,4.61,7.86">5</formula><p>an adapted version of accuracy from the general domain VQA<ref type="foot" coords="5,400.82,437.88,3.97,6.12" target="#foot_3">9</ref> task that strictly considers exact matching of a participant provided answer and the ground truth answer. To compensate for the strictness of the accuracy metric, BLEU <ref type="bibr" coords="5,454.65,463.36,15.50,8.74" target="#b9">[10]</ref> is used to capture the word overlap-based similarity between a system-generated answer and the ground truth answer. The overall methodology and resources for the BLEU metric are essentially similar to last year's VQA task <ref type="bibr" coords="5,416.18,499.23,9.96,8.74" target="#b1">[2]</ref>. The BLEU metric is also used to evaluate the submissions for the VQG task, where we essentially compute the word overlap-based average similarity score between the system-generated questions and the ground truth question for each given test image. The overall results of the participating systems are presented in Table <ref type="table" coords="5,475.61,547.05,4.98,8.74" target="#tab_1">3</ref> and Table <ref type="table" coords="5,183.53,559.01,4.98,8.74" target="#tab_2">4</ref> in a descending order of the accuracy and average BLEU scores respectively (the higher the better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Similar to the last two years, participants continued to use state-of-the-art deep learning techniques to build their VQA-Med systems for both VQA and VQG A variety of pooling strategies were explored, e.g., global average pooling to encode image features and transformer-based architectures like BERT or recurrent neural networks (RNN) to extract question features (for the VQA task). Various types of attention mechanisms are also used coupled with different pooling strategies such as multimodal factorized bilinear (MFB) pooling or multi-modal factorized high-order pooling (MFH) in order to combine multimodal features followed by bilinear transformations to finally predict the possible answers in the VQA task and generate possible question words in the VQG task. Additionally, the top performing systems first classified the questions into two types: yes/no, and abnormality, then added another multi-class classification framework for abnormality-related question answering, while using the same backbone architecture along with utilizing additional training data, leading to better results. Analyses of the results in Table <ref type="table" coords="6,290.92,572.43,4.98,8.74" target="#tab_1">3</ref> suggest that in general, participating systems performed well for the VQA task and achieved better accuracy relatively compared to last year's results for answering abnormality-related questions <ref type="bibr" coords="6,467.30,596.34,9.96,8.74" target="#b1">[2]</ref>. They obtained slightly lower BLEU scores as we focused on only abnormality questions this year that are generally complex than modality, plane, or organ category questions given in the last year. Overall, the VQA task results obtained this year entail the robustness of the provided dataset compared to last year's task due to the enhanced focus on the abnormality-related questions for corpus  <ref type="table" coords="7,334.74,416.34,4.98,8.74" target="#tab_2">4</ref> suggest that the task was comparatively more challenging than the VQA task as the systems achieved lower BLEU scores. As BLEU is not the ideal metric to semantically compare the generated questions with the ground-truth questions, this could also urge the necessity of an embedding-based similarity metric to be explored in the future edition of this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,192.87,741.08,229.61,8.74;4,167.83,586.40,141.73,106.30"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Examples from the Test Set of the VQA Task</figDesc><graphic coords="4,167.83,586.40,141.73,106.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,180.54,372.01,254.27,8.74;7,143.34,118.89,332.42,238.55"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Results of the VQA Task on the AICrowd platform</figDesc><graphic coords="7,143.34,118.89,332.42,238.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,168.88,127.36,277.60,8.74"><head>Table 1 :</head><label>1</label><figDesc>Participating groups in the VQA-Med 2020 VQA task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,127.36,345.83,152.70"><head>Table 3 :</head><label>3</label><figDesc>Maximum Accuracy and Maximum BLEU Scores for VQA Task (out of each team's submitted runs).</figDesc><table coords="6,235.15,151.26,143.63,128.81"><row><cell>Team</cell><cell cols="2">Accuracy BLEU</cell></row><row><cell>AIML</cell><cell>0.496</cell><cell>0.542</cell></row><row><cell cols="2">TheInceptionTeam 0.480</cell><cell>0.511</cell></row><row><cell>bumjun jung</cell><cell>0.466</cell><cell>0.502</cell></row><row><cell>HCP-MIC</cell><cell>0.426</cell><cell>0.462</cell></row><row><cell>NLM</cell><cell>0.400</cell><cell>0.441</cell></row><row><cell>HARENDRAKV</cell><cell>0.378</cell><cell>0.439</cell></row><row><cell>Shengyan</cell><cell>0.376</cell><cell>0.412</cell></row><row><cell>kdevqa</cell><cell>0.314</cell><cell>0.350</cell></row><row><cell>sheerin</cell><cell>0.282</cell><cell>0.330</cell></row><row><cell>umassmednlp</cell><cell>0.220</cell><cell>0.340</cell></row><row><cell>dhruv sharma</cell><cell>0.142</cell><cell>0.177</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,308.11,345.83,116.13"><head>Table 4 :</head><label>4</label><figDesc>Maximum Average BLEU Scores for VQG Task (out of each team's submitted runs).</figDesc><table coords="6,134.77,332.01,345.83,92.23"><row><cell>Team</cell><cell>Average BLEU</cell></row><row><cell>AIML</cell><cell>0.348</cell></row><row><cell>TheInceptionTeam</cell><cell>0.339</cell></row><row><cell>NLM</cell><cell>0.116</cell></row><row><cell cols="2">tasks [4, 2]. In particular, most systems leveraged encoder-decoder architectures</cell></row><row><cell cols="2">with, e.g., deep convolutional neural networks (CNNs) like VGGNet or ResNet.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_0" coords="3,144.73,635.53,216.54,9.21"><p>http://www.cs.cmu.edu/ ~ark/mheilman/questions/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_1" coords="3,144.73,646.48,269.81,7.47"><p>https://www.aicrowd.com/challenges/imageclef-2020-vqa-med</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_2" coords="3,144.73,657.44,289.14,7.47"><p>https://www.aicrowd.com/challenges/imageclef-2020-vqa-med-vqg</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_3" coords="5,144.73,657.44,169.47,7.47"><p>https://visualqa.org/evaluation.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Conclusion</head><p>In this paper, we presented the VQA-Med 2020 tasks, datasets, and official results. We created new datasets for the visual question generation and visual question answering tasks with a focus on questions about abnormality. In the VQA task, the best team achieved 0.542 BLEU score and 0.496 accuracy. The VQG task was more challenging, with a best BLEU score of 0.348. In the future editions of VQA-Med, we will focus on expanding the VQG dataset with more images and questions [12] to enable effective development of deep learning models and on designing new evaluation metrics for both tasks.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,142.26,337.63,7.86;8,151.52,153.22,329.07,7.86;8,151.52,164.18,329.07,7.86;8,151.52,175.13,126.07,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,347.01,142.26,133.58,7.86;8,151.52,153.22,282.98,7.86">The inception team at vqa-med 2020: Pretrained vgg with data augmentation for medical vqa and vqg</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Al-Sadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Al-Theiabat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,455.89,153.22,24.70,7.86;8,151.52,164.18,268.61,7.86">CLEF 2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,186.03,337.64,7.86;8,151.52,196.99,329.07,7.86;8,151.52,207.95,329.07,7.86;8,151.52,218.90,329.07,7.86;8,151.52,229.86,129.80,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,166.36,196.99,314.24,7.86">Vqa-med: Overview of the medical visual question answering task at imageclef</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="8,190.39,207.95,290.21,7.86;8,151.52,218.90,24.01,7.86">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="8,358.66,218.90,117.72,7.86">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-09">2019. September 9-12, 2019. 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,240.76,337.63,7.86;8,151.52,251.71,329.07,7.86;8,151.52,262.67,329.07,7.86;8,151.52,273.63,46.58,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,274.37,240.76,206.22,7.86;8,151.52,251.71,192.33,7.86">Hcp-mic at vqa-med 2020: Effective visual representation for medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,366.66,251.71,113.93,7.86;8,151.52,262.67,186.26,7.86">CLEF 2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,284.53,337.63,7.86;8,151.52,295.48,329.07,7.86;8,151.52,306.44,329.07,7.86;8,151.52,317.40,122.39,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,431.40,284.53,49.18,7.86;8,151.52,295.48,248.54,7.86">Overview of imageclef 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,420.76,295.48,59.83,7.86;8,151.52,306.44,252.08,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,328.29,337.64,7.86;8,151.52,339.25,329.07,7.86;8,151.52,350.21,329.07,7.86;8,151.52,361.17,329.07,7.86;8,151.52,372.13,329.07,7.86;8,151.52,383.09,329.07,7.86;8,151.52,394.05,329.07,7.86;8,151.52,405.01,329.07,7.86;8,151.52,415.97,329.07,7.86;8,151.52,426.92,329.07,7.86;8,151.52,437.88,34.31,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,295.04,383.09,185.55,7.86;8,151.52,394.05,255.37,7.86">Overview of the ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,426.43,394.05,54.16,7.86;8,151.52,405.01,329.07,7.86;8,151.52,415.97,253.34,7.86">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="8,456.14,415.97,24.45,7.86;8,151.52,426.92,138.63,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,142.96,448.78,337.63,7.86;8,151.52,459.74,329.07,7.86;8,151.52,470.69,329.07,7.86;8,151.52,481.65,329.07,7.86;8,151.52,492.61,329.07,7.86;8,151.52,503.57,329.07,7.86;8,151.52,514.53,329.07,7.86;8,151.52,525.49,329.07,7.86;8,151.52,536.45,329.07,7.86;8,151.52,547.41,216.27,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,268.29,503.57,212.30,7.86;8,151.52,514.53,124.23,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,297.67,514.53,182.92,7.86;8,151.52,525.49,329.07,7.86;8,151.52,536.45,122.46,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="8,305.55,536.45,171.07,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,142.96,558.30,337.63,7.86;8,151.52,569.26,329.07,7.86;8,151.52,580.22,329.07,7.86;8,151.52,591.18,46.58,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,272.09,558.30,208.50,7.86;8,151.52,569.26,197.46,7.86">bumjun jung at vqa-med 2020: Vqa model based on feature extraction and multi-modal feature fusion</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,369.46,569.26,111.13,7.86;8,151.52,580.22,186.26,7.86">CLEF 2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,602.07,337.64,7.86;8,151.52,613.03,329.07,7.86;8,151.52,623.99,329.07,7.86;8,151.52,634.95,322.28,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,389.45,602.07,91.14,7.86;8,151.52,613.03,329.07,7.86;8,151.52,623.99,135.23,7.86">Aiml at vqa-med 2020: Knowledge inference via a skeleton-based sentence mapping approach for medical domain visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verjans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,309.00,623.99,171.60,7.86;8,151.52,634.95,132.29,7.86">CLEF 2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,645.84,337.63,7.86;8,151.52,656.80,329.07,7.86;9,151.52,119.67,329.07,7.86;9,151.52,130.63,46.58,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,262.01,645.84,218.58,7.86;8,151.52,656.80,198.85,7.86">Shengyan at vqa-med 2020: An encoder-decoder model for medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,370.34,656.80,110.25,7.86;9,151.52,119.67,186.26,7.86">CLEF 2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,141.59,337.98,7.86;9,151.52,152.55,329.07,7.86;9,151.52,163.51,329.07,7.86;9,151.52,174.47,98.02,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,350.73,141.59,129.86,7.86;9,151.52,152.55,133.70,7.86">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,306.56,152.55,174.03,7.86;9,151.52,163.51,162.47,7.86">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,185.43,337.98,7.86;9,151.52,196.39,329.07,7.86;9,151.52,207.34,247.01,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,207.04,185.43,273.55,7.86;9,151.52,196.39,75.61,7.86">Nlm at vqa-med 2020: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,247.18,196.39,233.42,7.86;9,151.52,207.34,57.04,7.86">CLEF 2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,218.30,337.97,7.86;9,151.52,229.26,329.07,7.86;9,151.52,240.22,329.07,7.86;9,151.52,251.18,329.07,8.12;9,151.52,262.79,160.05,7.47" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,382.30,218.30,98.29,7.86;9,151.52,229.26,109.96,7.86">Visual question generation from radiology images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<ptr target="https://alvr-workshop.github.io/proceedings/ALVR_2020_15_Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,285.24,229.26,195.35,7.86;9,151.52,240.22,329.07,7.86;9,151.52,251.18,28.76,7.86">Proceedings of the first workshop on Advances in Language and Vision Research (ALVR). Association for Computational Linguistics</title>
		<meeting>the first workshop on Advances in Language and Vision Research (ALVR). Association for Computational Linguistics<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,273.10,337.98,7.86;9,151.52,284.06,329.07,7.86;9,151.52,295.02,217.32,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,249.32,273.10,231.28,7.86;9,151.52,284.06,23.26,7.86">kdevqa at vqa-med 2020: focusing on glu-based classification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Umada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,198.80,284.06,281.79,7.86;9,151.52,295.02,27.88,7.86">CLEF 2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,305.98,337.97,7.86;9,151.52,316.93,329.07,7.86;9,151.52,327.89,322.28,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,241.82,305.98,238.77,7.86;9,151.52,316.93,145.13,7.86">Harendrakv at vqa-med 2020: Sequential vqa with attention for medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">K</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,316.08,316.93,164.51,7.86;9,151.52,327.89,132.29,7.86">CLEF 2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
