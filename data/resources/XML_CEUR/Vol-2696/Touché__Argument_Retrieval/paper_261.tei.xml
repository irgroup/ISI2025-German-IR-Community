<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,165.10,116.36,285.16,12.90">Overview of Touché 2020: Argument Retrieval</title>
				<funder ref="#_hPDzd5S #_vS3fxdr">
					<orgName type="full">DFG</orgName>
				</funder>
				<funder ref="#_7kghJZK">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,155.96,154.40,90.28,8.64"><forename type="first">Alexander</forename><surname>Bondarenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Martin-Luther-Universität Halle-Wittenberg</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.84,154.40,44.97,8.64"><forename type="first">Maik</forename><surname>Fröbe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Martin-Luther-Universität Halle-Wittenberg</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,314.05,154.40,66.74,8.64"><forename type="first">Meriem</forename><surname>Beloucif</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universität Hamburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,391.85,154.40,58.55,8.64"><forename type="first">Lukas</forename><surname>Gienapp</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Leipzig University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,163.91,166.36,54.36,8.64"><forename type="first">Yamen</forename><surname>Ajjour</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Martin-Luther-Universität Halle-Wittenberg</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.24,166.36,85.20,8.64"><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.99,166.36,57.74,8.64"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universität Hamburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,395.50,166.36,47.65,8.64"><forename type="first">Benno</forename><surname>Stein</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,183.22,178.32,81.16,8.64"><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">University</orgName>
								<address>
									<settlement>Paderborn</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,276.26,178.32,60.36,8.64"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Leipzig University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.31,178.32,63.36,8.64"><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Martin-Luther-Universität Halle-Wittenberg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,165.10,116.36,285.16,12.90">Overview of Touché 2020: Argument Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AF314C75CF34C5D4845127D9CA2341AD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Argumentation is essential for opinion formation when it comes to debating on socially important topics as well as when making everyday personal decisions. The web provides an enormous source of argumentative texts, where well-reasoned argumentations are mixed with biased, faked, and populist ones. The research direction of developing argument retrieval technologies thus focuses not only retrieving relevant arguments for some argumentative information need, but also on retrieving arguments of a high quality. In this overview of the first shared task on argument retrieval at the CLEF 2020 Touché lab, we survey and evaluate 41 approaches submitted by 17 participating teams for two tasks: (1) retrieval of arguments on socially important topics, and (2) retrieval of arguments on everyday personal decisions. The most effective approaches submitted share some common techniques, such as query expansion, and taking argument quality into account. Still, the evaluation results show that only few of the submitted approaches (slightly) improve upon relatively simple argumentation-agnostic baselines-indicating that argument retrieval is in its infancy and meriting further research into this direction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Decision making and opinion formation processes are routine tasks for most of us. Often, such opinion formation relates to a decision between two sides based on previous experience and knowledge, but it may also require accumulating new knowledge. With the widespread access to every kind of information on the web, everyone has the chance to acquire new knowledge and to form an informed opinion about a given topic. In the process, be it on the level of socially important topics or "just" personal decisions, one of the at least two sides (i.e., decision options) will challenge the other with an appeal to justify its stance. In the simplest form, a justification might be simple facts or opinions, but more complex justifications often are based on argumentation: a complex relational aggregation of evidence and opinions, where one element is supported by the other.</p><p>Web resources, such as blogs, community question answering sites, and social platforms, contain an immense variety of opinions and argumentative texts-including many that can be considered to be of a biased, faked, or populist nature. This motivates research on the task of argument retrieval, which concerns specific informations needs that current web search engines, as far as we can tell, do not treat any differently than "standard" ad hoc search. 1,2 One of the first argument search engines has been args.me <ref type="bibr" coords="2,168.39,191.04,15.27,8.64" target="#b44">[45]</ref>, which retrieves relevant arguments on a given (controversial) query from a focused collection of arguments crawled from a selection of debate portals <ref type="bibr" coords="2,436.68,203.00,15.27,8.64" target="#b44">[45]</ref>. Other argument retrieval systems, such as ArgumenText <ref type="bibr" coords="2,334.23,214.95,16.60,8.64" target="#b39">[40]</ref> and TARGER <ref type="bibr" coords="2,411.06,214.95,10.58,8.64" target="#b8">[9]</ref>, use the larger Common Crawl, requiring additionally also argument mining components as part of their retrieval pipelines. The comparative argumentation machine CAM <ref type="bibr" coords="2,423.77,238.86,16.60,8.64" target="#b36">[37]</ref> also aims at supporting decision making in comparison scenarios, based on billions of sentences from the Common Crawl, however, it still lacks a proper ranking of diverse arguments. Moreover, argument retrieval is not just limited to search engines serving more argumentative results for corresponding information needs: it will also be an integral part of any open-domain conversational agent capable of "discussing" controversial topics with their human users-as showcased by IBM's Project Debater <ref type="bibr" coords="2,397.10,310.60,10.79,8.64" target="#b4">[5,</ref><ref type="bibr" coords="2,410.38,310.60,11.83,8.64" target="#b22">23]</ref>. <ref type="foot" coords="2,426.15,308.93,3.49,6.05" target="#foot_2">3</ref>To foster research on argument retrieval, we organize the Touché lab at CLEF 2020 and the first shared task on argument retrieval. <ref type="foot" coords="2,328.65,332.84,3.49,6.05" target="#foot_3">4</ref> The lab is a collaborative platform to develop argument retrieval approaches for decision support on a societal (e.g.,"Is climate change real and what to do?") and also on a personal level (e.g.,"Should I buy real estate or rent, and why?"), featuring two tasks:</p><p>1. Given a focused collection of arguments and some socially important and controversial topic, retrieve arguments that could help an individual forming an opinion on the topic, or arguments that support/challenge their existing stance. 2. Given a generic web crawl and a comparative question relating to a personal decision, retrieve documents with arguments that could help an individual to arrive at a conclusion regarding their decision.</p><p>From the 28 teams registered for the Touché lab (10 teams for Task 1, 7 for Task 2, and 11 for both) 17 teams actively participated with a total of 41 different approaches (to allow for a wide diversity of ideas, the teams could submit multiple approaches). Additionally, we evaluated two retrieval baselines: the Lucene implementation of query likelihood with Dirichlet-smoothed language models (DirichletLM <ref type="bibr" coords="2,402.15,511.60,15.93,8.64" target="#b47">[48]</ref>) for Task 1 and the BM25F-based <ref type="bibr" coords="2,209.50,523.55,16.60,8.64" target="#b35">[36]</ref> search engine ChatNoir <ref type="bibr" coords="2,326.31,523.55,11.62,8.64" target="#b5">[6]</ref> for Task 2. Effectiveness was measured using nDCG@5 <ref type="bibr" coords="2,228.93,535.51,16.60,8.64" target="#b16">[17]</ref> based on the top-5 pools manually labeled for relevance by human assessors <ref type="bibr" coords="2,216.96,547.46,12.87,8.64" target="#b6">(7,</ref><ref type="bibr" coords="2,229.82,547.46,12.87,8.64">045</ref> judgments in total). The most effective approaches in both tasks use query expansion and take argument quality into account, but the evaluation results show that only few of the submitted approaches (slightly) improve upon relatively simple baselines. Further research on argument retrieval thus seems well-justified.</p><p>Examples of argument retrieval scenarios are opinion formation on controversial topics <ref type="bibr" coords="3,147.92,152.56,15.77,8.64" target="#b39">[40,</ref><ref type="bibr" coords="3,165.78,152.56,13.28,8.64" target="#b44">45]</ref> or on comparison options <ref type="bibr" coords="3,283.59,152.56,16.60,8.64" target="#b36">[37]</ref> but also finding counterarguments for a given argument <ref type="bibr" coords="3,175.21,164.52,15.27,8.64" target="#b46">[47]</ref>. In the Touché lab, we address the first two types of information needs in two separate tasks. Here, we briefly review the related work on argument retrieval in general and on retrieval in comparative scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Argument Retrieval</head><p>Usually, an argument is modeled as a conclusion with supporting or attacking premises <ref type="bibr" coords="3,149.74,244.96,15.27,8.64" target="#b44">[45]</ref>. While a conclusion is a statement that can be accepted or rejected, a premise is a more grounded statement (e.g., a statistical evidence). The development of an argument retrieval system comes with challenges that range from mining arguments from unstructured text to assessing their relevance and quality <ref type="bibr" coords="3,362.11,280.82,15.27,8.64" target="#b44">[45]</ref>.</p><p>Argument retrieval can follow different paradigms that start from different sources and employ argument mining and retrieval tasks in different orders <ref type="bibr" coords="3,411.11,304.73,10.58,8.64" target="#b1">[2]</ref>. For instance, the args.me search engine's approach <ref type="bibr" coords="3,290.30,316.69,16.60,8.64" target="#b44">[45]</ref> is based on a focused crawl of arguments from online debate portals that were gathered in an offline pre-processing using tailored heuristics. In the online query phase, the extracted arguments are then ranked using BM25F <ref type="bibr" coords="3,194.99,352.55,16.60,8.64" target="#b35">[36]</ref> giving conclusions more weight than premises. Also Levy et al. <ref type="bibr" coords="3,134.77,364.51,16.60,8.64" target="#b20">[21]</ref> use distant-supervision to mine arguments in an offline pre-processing for a set of topics from Wikipedia before ranking them. In contrast, the ArgumenText search engine <ref type="bibr" coords="3,164.96,388.42,16.60,8.64" target="#b39">[40]</ref> and the TARGER argument retrieval component <ref type="bibr" coords="3,386.92,388.42,11.62,8.64" target="#b8">[9]</ref> are based on Common Crawl<ref type="foot" coords="3,179.47,398.71,3.49,6.05" target="#foot_4">5</ref> web crawls without an offline pre-processing phase for argument mining. Both systems simply retrieve web documents and then use argument mining approaches in an online manner to extract arguments from the retrieved documents. The two tasks in the Touché lab address the two different paradigms of a focused argument crawl, similar to that of the args.me search engine, as the to-be-indexed dataset (Task 1), and of a general web crawl from which argumentative results are to be retrieved (Task 2).</p><p>Apart from an argument's topical relevance, argument retrieval might also need to take an argument's quality into account. What makes a good argument has been studied since the time of Aristotle <ref type="bibr" coords="3,239.32,496.02,10.58,8.64" target="#b3">[4]</ref>. Recently, Wachsmuth et al. <ref type="bibr" coords="3,365.38,496.02,16.60,8.64" target="#b42">[43]</ref> categorized the different aspects of argument quality into a taxonomy that covers three quality dimensions: logic, rhetoric, and dialectic quality. Logic quality refers to the local structure of an argument, i.e, the conclusion and the premises and their relations. Rhetoric quality covers the effectiveness of an argument in persuading an audience with its conclusion. Dialectic quality addresses the relations of an argument to other arguments on the topic. For example, an argument may be particularly vulnerable in a debate when many attacking arguments exist. Note that the topical relevance of an argument is part of the dialectic quality in Wachsmuth et al.'s categorization <ref type="bibr" coords="3,311.74,591.66,15.27,8.64" target="#b42">[43]</ref>.</p><p>In an evaluation of standard text-based retrieval models on an argument collection, Potthast et al. <ref type="bibr" coords="3,193.52,615.57,16.60,8.64" target="#b30">[31]</ref> incorporate argument quality evaluation with respect to the aforementioned dimensions. Based on a TREC-style pooling and human assessment of relevance as well as the three dimensions, DirichletLM turns out to be better suited for argument retrieval than BM25, DPH, and TF-IDF. Later, Gienapp et al. <ref type="bibr" coords="4,422.08,119.31,16.60,8.64" target="#b13">[14]</ref> suggested a strategy to annotate the argument quality dimensions in a pairwise manner, reducing annotation costs by 93%.</p><p>Apart from standard text-based retrieval models, also other ideas have been proposed for argument ranking. For example, Wachsmuth et al. <ref type="bibr" coords="4,379.94,167.13,16.60,8.64" target="#b45">[46]</ref> suggest to create argument graphs by connecting two arguments if one uses the other's conclusion as a premise, and to exploit this structure for argument ranking using PageRank <ref type="bibr" coords="4,442.93,191.04,15.27,8.64" target="#b26">[27]</ref>. Another example is Dumani et al.'s idea of a probabilistic framework that clusters semantically similar claims and premises <ref type="bibr" coords="4,280.98,214.95,15.27,8.64" target="#b10">[11]</ref>, taking into account potential support/attack relations between premise and claim clusters, and claim clusters and a query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieval in Comparative Scenarios</head><p>Comparative information needs arise when someone has to decide between different options. Such needs range from simple facts (e.g., "Does water or ethanol have the higher boiling point?") over comparing products (e.g., "What is the best phone for me?") to problems like which location or college to choose for undergraduate studies.</p><p>The earliest web-based comparison systems employed interfaces where users entered the to-be-compared objects into dedicated search boxes <ref type="bibr" coords="4,388.87,336.50,15.77,8.64" target="#b24">[25,</ref><ref type="bibr" coords="4,408.21,336.50,11.83,8.64" target="#b41">42]</ref>. In parallel to the early comparison system development, opinion mining from product reviews has been dealing with the identification of comparative sentences and their polarity (in favor or not) using various techniques <ref type="bibr" coords="4,284.60,372.36,15.77,8.64" target="#b17">[18,</ref><ref type="bibr" coords="4,303.56,372.36,12.45,8.64" target="#b18">19,</ref><ref type="bibr" coords="4,319.20,372.36,11.83,8.64" target="#b19">20]</ref>. Recently, the identification of preferences ("winning" object) from comparative sentences has been addressed in opendomain settings (not just for product reviews) by applying feature-based and neural classifiers <ref type="bibr" coords="4,176.37,408.23,15.77,8.64" target="#b21">[22,</ref><ref type="bibr" coords="4,194.44,408.23,11.83,8.64" target="#b28">29]</ref>. This preference classification forms the basis of the comparative argumentation machine CAM <ref type="bibr" coords="4,248.77,420.18,15.27,8.64" target="#b36">[37]</ref>, which accepts two to-be-compared objects and comparison aspects as input, retrieves comparative sentences in favor of one or the other object using BM25, and clusters the sentences to present a summary table. Improving retrieval models for systems like CAM is the objective of Touché's Task 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Lab Overview and Statistics</head><p>A total of 28 teams registered for the Touché lab, with a majority coming from Germany (17 teams from Germany, two from France, two from India, and one each from China, Italy, the Netherlands, Pakistan, Russia, Switzerland, and the US). For a nice team naming scheme, participants could choose as their team name a real or fictional fencer or swordsman (e.g., Zorro)-a tip of the hat to 'touché's other meaning.</p><p>From the 28 registered teams, 17 actively participated in the lab by submitting approaches/results. We asked the teams to use the TIRA evaluation platform <ref type="bibr" coords="4,445.07,583.57,15.27,8.64" target="#b31">[32]</ref>, enabling the submission of working software, in order to increase the overall result reproducibility. TIRA is an integrated cloud-based evaluation-as-a-service shared task platform where teams have full administrative access to a virtual machine. By default, the virtual machines operated Ubuntu 18.04 with one CPU core (Intel Xeon E5-2620), 4GB of RAM, and 16GB HDD, but we adjusted the resources to the participants' requirements when needed (e.g., one team asked for 24 GB of RAM, 5 CPU cores, and and Python) to simplify the deployment. After deployment, the teams could create result submissions via the web UI of TIRA, triggering the following standard pipeline. To create a run submission from a participating team's software, the respective virtual machine was shut down, disconnected from the internet, cloned, and the clone booted up again, this time mounting also the test datasets for the respective task. The interruption of the internet connection was meant to discourage the use of external web services, which may disappear or get incompatible in the future, harming reproducibility. However, two exceptions were made for all participants: the APIs of ChatNoir and args.me were available, even in the sandbox mode. Additionally, if participants requested it, other external web services based on the teams' requirements could be whitelisted; only one team asked to access the Web of Trust API. <ref type="foot" coords="5,330.73,249.15,3.49,6.05" target="#foot_5">6</ref> All virtual machines that the teams used for their submissions are archived such that they can be re-evaluated or applied to new datasets as long as data formats and external APIs remain stable.</p><p>To foster diverse approaches, the participating teams could submit several runs, giving priorities for evaluation when more than one was submitted. The actual output run files needed to follow the standard TREC-style format. <ref type="foot" coords="5,353.05,308.93,3.49,6.05" target="#foot_6">7</ref> Upon submission, we checked the validity of the run files and asked the participants to adjust and re-submit their runs in case of problems, also offering assistance. This resulted in 41 valid runs from 17 teams. From every team, at least the top five runs of highest priority were pooled for evaluation. Additionally, we evaluated two retrieval baselines: the Lucene implementation of query likelihood with Dirichlet-smoothed language models (DirichletLM <ref type="bibr" coords="5,460.67,370.37,15.93,8.64" target="#b47">[48]</ref>) for Task 1, and the BM25F-based <ref type="bibr" coords="5,270.93,382.33,16.60,8.64" target="#b35">[36]</ref> search engine ChatNoir <ref type="bibr" coords="5,386.58,382.33,11.62,8.64" target="#b5">[6]</ref> for Task 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Touché Task 1: Conversational Argument Retrieval</head><p>The goal of the Touché's first task is to provide assistance to users searching for good and relevant pro and con arguments on various societal topics (e.g., climate change, electric cars, etc.) that are, for instance, engaged in an argumentative conversation. A respective retrieval system may aid users in collecting evidence on issues of general societal interest and support them in forming their own opinion.</p><p>Several existing community question answering websites, such as Yahoo! Answers and Quora, as well as debate portals, such as debatewise.org and idebate.org, are designed to accumulate opinions and arguments and to engage users in dialogues. Generic web search engines lack an effective solution to retrieve relevant arguments from these and other platforms beyond things like returning entire discussion threads. One reason lies in their ignorance of the argumentative nature of the underlying discussions, which results in generic web search engines offering only limited support during conversations or debates. This motivates the development of robust and effective approaches specifically focused on conversational argument retrieval. As the evidence that the climate is changing rapidly mounts, a user questions the common belief that climate change is anthropogenic and desires to know whether humans are the primary cause, or whether there are other causes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Narrative</head><p>Highly relevant arguments include those that take a stance in favor of or opposed to climate change being anthropogenic and that offer valid reasons for either stance. Relevant arguments talk about human or non-human causes, but not about primary causes. Irrelevant arguments include ones that deny climate change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task Definition</head><p>The participants of Task 1 are asked to retrieve relevant arguments from a focused crawl of arguments originating from debate portals for a given query on some controversial topic. Given the amount of argumentative texts readily available on such portals, instead of having to develop own argument extraction technology, the participants could focus their attention on retrieving the previously extracted arguments from the portals, covering a wide range of popular debate topics. To ease access to the argument collection, we provide an openly accessible and flexible API at args.me,<ref type="foot" coords="6,362.10,362.80,3.49,6.05" target="#foot_7">8</ref> also allowing participants to participate in the lab without having to index the collection themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Description</head><p>Retrieval Topics. We have formulated 50 search scenarios on controversial issues in the form of TREC-style topics with a title (the query potentially issued by a user), a description (a short summary of the search context and information need), and a narrative (a definition of what constitutes relevant results for this topic, serving as a guideline for human assessors). An example topic is shown in Table <ref type="table" coords="6,357.07,472.25,3.74,8.64" target="#tab_0">1</ref>. As topics, we selected those issues that have the largest number of user-generated arguments on the debate portals, and thus can be assumed to be of high societal interest. Further, we ensured that, for each topic, at least some relevant arguments are present in the collection.</p><p>Document Collection. Task 1 is based on the args.me corpus <ref type="bibr" coords="6,382.92,531.11,10.58,8.64" target="#b1">[2]</ref>, which comes in two versions that are freely available for download, 9 as well as being accessible via the API of args.me. Version 1 of the corpus contains 387,606 arguments crawled from four debate portals in the middle of 2019 (debatewise.org, idebate.org, debatepedia.org, and debate.org). Each argument in the corpus consists of a conclusion (a claim that something is true or favorable) and a text covering one or more premises (reasons supporting or attacking the claim). Version 2 contains 387,740 arguments crawled from the same four debate portals, and 48 arguments from Canadian parliament discussions. In addition to the conclusions and premises, Version 2 also includes the texts surrounding them on the pages from which the arguments were extracted.</p><p>Table <ref type="table" coords="7,157.68,115.83,3.36,8.06">2</ref>. Overview of the participants' strategies for Task 1. Swordsman serves as a baseline using Lucene's DirichletLM retrieval model. Teams are ordered descending by nDCG@5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Survey of Submissions to Task 1</head><p>The submissions to Task 1 largely employ a similar general strategy, which is characterized by the following three components: (1) a retrieval strategy; (2) an augmentation component, where either the query set is expanded, or results are extended directly based on features of documents in an initially retrieved set; (3) a (re)ranking component based on a primary document feature, which weighs, boosts, or modifies the retrieval scores, or which is used directly to rank the results.</p><p>In Table <ref type="table" coords="7,184.95,398.32,3.74,8.64">2</ref>, we provide a high-level overview of the participants' systems. Each run is characterized with regard to the three aforementioned components. For their retrieval component, most teams opted for one of the four models also evaluated for argument search by Potthast et al. <ref type="bibr" coords="7,230.90,434.18,15.27,8.64" target="#b30">[31]</ref>. In line with the authors' results, approaches using Dirich-letLM or DPH seem to be far better than approaches relying on BM25 or TF-IDF. Two teams further rely on the cosine similarity, which yields favorable results as well.</p><p>Six teams opted to integrate query or result augmentation into their pipeline. Most notably, the three top-performing approaches use different ways of query augmentation, aimed either at generating synonymous queries, or at generating new queries from scratch using large language models. Others use result augmentation, opting for a cluster-based approach to group arguments based on text-inherent features, such as topic models or semantic clustering. In an effort to increase the topical coverage of results for a single query, arguments belonging to a cluster present in the initially retrieved results are returned as well-even though they may not be directly related to the query. Query augmentation seems to be more successful than direct result augmentation.</p><p>For re-ranking, we can differentiate two main types of features being chosen: three teams integrate the notion of argument quality into their ranking process; two other teams utilize sentiment analysis. The first choice operates under the hypothesis that an argument of higher quality will likely be of higher relevance as well. For the second choice, one team proposes that a neutral sentiment coincides with higher relevance, while the other argues that a high sentiment value hints at an emotionally involved author, possibly arguing more convincingly.</p><p>Different other text-based re-ranking features were proposed as well, such as premise prediction scores, readability, or the presence of named entities. However, they all have only limited effects on performance. One team integrates external information by calculating credibility scores for authors of arguments in the corpus. Approaches relying on trained models, i.e., quality prediction and premise prediction scoring, perform less favorable in general, which may be due to the limited amount of domain-specific training data. It remains to be seen whether the results improve in future iterations of the Touché lab. In what follows, we first summarize the systems presented in a submitted paper, to provide more in-depth insights into the participants' approaches. We also received runs from six teams who did not submit a respective paper. Based on a consultation with those teams, we describe their systems in the latter part of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submissions with working notebooks</head><p>Aragorn by Entezari and Völske <ref type="bibr" coords="8,270.66,292.66,16.60,8.64" target="#b12">[13]</ref> employs a distant-supervision approach to train seven deep-learning models, the idea being that the argument retrieval task is similar to the structure of arguments in the args.me corpus: Retrieving relevant arguments for a query is comparable to retrieving premises that support an argument's conclusion. Models receive the conclusion as a query during the training process and rank possible premises, with premise/conclusion labels derived from the corpus. To construct training samples, the premise text of each argument is marked as relevant, whereas the top 100 premises from other arguments ranked by BM25 are marked as irrelevant. Using that methodology, a distant-supervision dataset is produced comprising over 300,000 training and 4,800 validation queries to train the seven deep neural models. The training comprises multiple epochs per model, using only the best-performing candidate per model for the actual argument retrieval task, according to MAP@20 on the validation queries. All seven trained models are combined using linear regression optimized on the validation queries. A run was submitted for this combined approach (Run 3), with additional individual runs for the four most promising trained models (Runs 1, 2, 4, and 5). For all five runs, the retrieval pipeline used BM25 to retrieve 100 arguments and then re-ranked them by the predicted model score. Don Quixote by Dumani and Schenkel <ref type="bibr" coords="8,300.34,507.86,16.60,8.64" target="#b11">[12]</ref> follows a two-step approach: In an initial offline operation, the dataset is clustered based on Sentence-BERT (SBERT) embeddings of both the conclusions and the premises. Premises are further grouped by identical conclusions. This grouping allows to calculate the so-called dimension convincing frequencies (DCFs) of a premise, in comparison to other premises of the same conclusion. For the three quality dimensions cogency, reasonableness, and effectiveness, a logistic regression is applied to all possible conclusion-premise pairs. The DCFs are then obtained by counting how often a premise was better than other premises belonging to the same conclusion in a cross comparison. Conclusions and premises are indexed separately. At retrieval time, a set of premises is retrieved using a divergencefrom-randomness model. This set is then extended with all other premises belonging to the same group. For each entry in the extended set, a score is calculated using both the similarity of the query to the conclusion and the sum of the three DCFs per premise. For each group in this result set, the scores of its members are aggregated and a final ranking is obtained by choosing a single representative per group, and ranking representatives by group score. Representatives are chosen by text length, under the hypothesis that a longer premise is also more specific and therefore may be better suited.</p><p>Dread Pirate Roberts by Akiki and Potthast <ref type="bibr" coords="9,315.53,178.80,11.62,8.64" target="#b2">[3]</ref> employs transformer-based models as part of its argument retrieval pipeline, pursuing three independent approaches: (1) The initial query is expanded using GPT-2; by adding question-like suffixes to the query, the text generation is steered towards argumentative text. Thus, a set of queries is built from generated sentences, and for each, results are retrieved using a DirichletLM retrieval model. Finally, all results are combined and ranked by their respective absolute score (Run 1). ( <ref type="formula" coords="9,173.96,250.53,3.87,8.64">2</ref>) Similar to the first approach, query expansion is achieved by generating argumentative text. However, instead of generating sentences, single word predictions by BERT are used. Once again, question-like suffixes are employed to influence the nature of the generated text. Final results are obtained by composing a query out of the single generated terms and retrieving results with a DirichletLM model (Run 2). ( <ref type="formula" coords="9,440.94,298.35,3.87,8.64">3</ref>) Instead of focusing on query expansion, the third approach uses a transformer-based model to obtain document representations. Arguments are embedded in a vector space using Google's BERT-like Universal Sentence Encoder (USE). Retrieval is subsequently conducted using nearest-neighbor search with respect to the query (Runs 3, 4, and 5).</p><p>Oscar François de Jarjayes by Staudte and Lange <ref type="bibr" coords="9,341.49,369.80,16.60,8.64" target="#b40">[41]</ref> combines the traditional DPH retrieval model with document similarities based on a CBOW dual embedding space model. The intersection of the top-1000 result sets of both retrieval strategies is ranked descending by DPH score, producing a search result that ensures contextual relevance, as determined by the dual embedding space model, as well as query-specific accuracy, as given by the DPH scoring. Furthermore, a sentiment-based score weighting is proposed under the hypothesis that texts written by emotionally involved authors (high sentiment values according to the Google Cloud Natural Language API) are of higher quality and relevance to a query than neutral documents. An evaluation based on the retrieval performance of different sentiment weighting schemes seems to support this hypothesis.</p><p>Weiss Schnee by Bundesmann et al. <ref type="bibr" coords="9,290.47,512.97,16.60,8.64" target="#b23">[24]</ref> integrate a notion of argument quality as well as result heterogeneity into the argument retrieval process. First, quality ratings for all arguments in the args.me corpus are predicted using Support Vector Regression (SVR) based on 22 text features. For retrieval, the authors considered three different strategies of query expansion, one based on WordNet synonyms, the other two using embedding-based language modeling. Based on the augmented queries, an initial set of arguments is retrieved and scored by a DPH retrieval model, additionally weighted by their quality score. The top-8 results are then re-ranked, maximizing the weighted sum of semantic distance between consecutive entries in the result list, thus ensuring result heterogeneity, as well as each entries' argumentative quality. However, this reranking approach is considered an experimental feature and is not included in officially submitted runs, as the nDCG evaluation measure does not take diversity of results into account.</p><p>Zorro by Shahshahani and Kamps <ref type="bibr" coords="10,273.58,119.31,16.60,8.64" target="#b37">[38]</ref> was evaluated with only a single run. First, document encodings are constructed using BERT on the first 512 tokens of each argument in the args.me corpus. A ranking is then created in three steps. (1) An initial ranking is produced using BM25. (2) An SVM classifier trained on the Dagstuhl-15512 ArgQuality corpus <ref type="bibr" coords="10,177.72,167.13,16.60,8.64" target="#b43">[44]</ref> is used to predict scores for cogency, well-writtenness, reasonableness and overall quality of each argument in the initial ranking. (3) The final ranking is then produced using the learning-to-rank library RankLib. In addition to the scores predicted in the previous step, training incorporates the following features: the BERT encoding, two named entity-based features, and two binary features indicating the presence of numerical named entities (percent, quantity, money) and other entities (person, location, organization) in the argument. The assumption is that an argument exhibiting such entities is more likely to provide users with persuasive and effective information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submissions without working notebooks</head><p>Black Knight clusters the arguments in the corpus based on their debate titles. A query is then associated to a cluster, and all arguments belonging to the cluster are added to the result set. A ranking is obtained by calculating the cosine similarity between TF-IDF vectors of the arguments and the query. Moreover, a filtering pipeline is applied to ensure suitable length, readability, and stance of results. The team includes an extensive stance classification approach as part of their indexing.</p><p>Prince of Persia builds two separate indices for the premise and conclusion of each argument, the hypothesis being that due to the different lengths of both fields, using different retrieval models for each yields improved results overall. In five runs, different combinations of retrieval models are explored based on language models (Dirich-letLM), divergence from randomness (DLH) and term frequency (BM25) from the Terrier library <ref type="bibr" coords="10,179.65,448.08,15.27,8.64" target="#b25">[26]</ref>. Run 1 used DirichletLM for both indices, Run 2 used BM25. In Run 3, DLH and PL2 are combined, in Run 4 DLH and BM25, and in Run 5 DLH is used for both. Further a WordNet-based query augmentation is incorporated, and score weighting using sentiment analysis, boosting arguments with neutral sentiment in the ranking.</p><p>The Three Mouseketeers tackles the task of argument retrieval with a parameter optimization of Terrier's <ref type="bibr" coords="10,226.63,519.81,16.60,8.64" target="#b25">[26]</ref> implementation of the DirichletLM retrieval model, using pseudo-relevance feedback.</p><p>Boromir combines a word embedding space and a topic model to retrieve arguments that are both semantically related to the query as well as contextually related to each other. First, the cosine similarity is calculated on 300-dimensional word embeddings and used to retrieve an initial set of arguments. This set is extended with other arguments close to the initial results in the topic space, which is constructed using an LDA topic model with 80 topics. Furthermore, arguments are re-ranked based on author credibility: author statistics were crawled from the source page (if available) for each argument in the corpus. On this basis, an Elo rating was established for authors based on their debate winning statistics. Utena Tenjou creates two indices for the conclusion and premise and weights these two fields differently. In different runs, the weight of the premise is fixed at 1, adjusting the weight of the conclusion between 0.40 and 3.65. The same weighting mechanism is applied using two different retrieval models: BM25 and DirichletLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Task Evaluation</head><p>In this first edition of the lab, we evaluated only the relevance of the retrieved documents (not the quality of the comprised arguments), given that the collection of manual judgments is a rather complex and time-consuming task. We collected the participants' results as classical TREC-style runs where, for each topic, the document IDs are returned in a ranked list, ordered by descending relevance (i.e., the most relevant document should occur at Rank 1). The document pools for judgments were created with the trectools Python library <ref type="bibr" coords="11,268.04,482.16,15.27,8.64" target="#b27">[28]</ref>,<ref type="foot" coords="11,287.14,480.49,6.97,6.05" target="#foot_8">10</ref> using a top-5 pooling strategy that resulted in 5,291 unique retrieval results to be judged. The relevance judgments were collected on Amazon Mechanical Turk, following previously designed annotation guidelines <ref type="bibr" coords="11,310.68,518.02,15.77,8.64" target="#b13">[14,</ref><ref type="bibr" coords="11,330.38,518.02,11.83,8.64" target="#b30">31]</ref>. We tasked the crowdworkers to decide whether or not a given retrieved text is an argument, and to annotate the relevance of the item on a scale ranging from 1 (low) to 5 (high). Non-arguments were subsequently marked as spam and received a score of -2. Each retrieval result was separately annotated by five crowdworkers, using majority vote as a decision rule. To further ensure the annotation quality, we recruited only workers with an approval rate of at least 95%, and checked for occurrences of systematic spam.</p><p>In total, 2,964 relevance judgments were collected for Version 1 of the args.me corpus, and 2,298 relevance judgments were collected for Version 2. Due to an annotation error, no judgments were collected for topic 25, prompting us to omit this topic from further evaluation. Notice that this reduces the total number of retrieval results in pooling  from 5,291 to a total of 5,262. The participants' approaches were subsequently evaluated using nDCG <ref type="bibr" coords="12,208.90,491.26,16.60,8.64" target="#b16">[17]</ref> with an evaluation depth of 5 on remaining 49 topics. We used the nDCG implementation provided by the trec_eval library. <ref type="foot" coords="12,393.28,501.55,6.97,6.05" target="#foot_9">11</ref> Results are given in Table <ref type="table" coords="12,158.95,515.17,9.40,8.64" target="#tab_2">3a</ref> and b for the first and second version of the dataset, respectively. Additionally, in Figure <ref type="figure" coords="12,174.67,527.13,3.74,8.64" target="#fig_0">1</ref>, the plots display effectiveness and 95% confidence intervals for each run on the first and second version of the args.me dataset, respectively. Confidence intervals were obtained using bootstrapping (n = 10, 000).</p><p>As baseline run, the args.me corpus was indexed in Elasticsearch and results for each topic were retrieved using the Lucene implementation of the DirichletLM model <ref type="bibr" coords="12,163.07,586.90,15.27,8.64" target="#b47">[48]</ref>. <ref type="foot" coords="12,182.16,585.23,6.97,6.05" target="#foot_10">12</ref> This retrieval model has shown favorable performance in prior experiments on argument search <ref type="bibr" coords="12,241.04,598.86,15.27,8.64" target="#b30">[31]</ref>. No additional query augmentation or re-ranking strategies were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Touché Task 2: Comparative Argument Retrieval</head><p>The goal of the Touché's second task is to support individuals' personal decisions in everyday life that can be expressed as a comparative question (e.g., "Is X better than Y with respect to Z?") and that do not have a single "factual" answer. Such questions can, for instance, be found on community question answering (CQA) sites like Yahoo! Answers or Quora, or in discussions on Reddit, but are also submitted as queries to search engines. The search engines then often simply show content from CQA websites or some web document mentioning the query terms as a direct answer above the classic "ten blue links." However, a problem of such attempts at short direct answers is that CQA websites may not always provide a diverse and sufficient overview of all possible options with well-formulated arguments, nor will all underlying textual information be credible-a broader set of such issues also forms the dilemma of direct answers <ref type="bibr" coords="13,461.50,262.77,15.27,8.64" target="#b32">[33]</ref>. As a first step to work on technology to present several credible arguments and different angles in potential direct comparative answers, Task 2 deals with the scenario of retrieving comparative arguments from web-scale collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task Definition</head><p>The participants of Task 2 are asked to retrieve and rank documents from the Clue-Web12 <ref type="foot" coords="13,162.74,358.74,6.97,6.05" target="#foot_11">13</ref> that help to answer a comparative question. Ideally, the retrieved documents contain convincing arguments for or against some of the possible options for a given comparison. Similar to Task 1, participation is possible without indexing the document collection on the participants' side, since we provide easy access to the document collection through the BM25F-based ChatNoir search engine <ref type="bibr" coords="13,373.81,408.23,11.62,8.64" target="#b5">[6]</ref> 14 -via a web-interface and an API. To identify arguments in texts, the participants are not restricted to any system; they can use own technology or any existing argument tagger of their choice. To lower the entry barriers for participants new to argument mining, we offer support via the neural argument tagger TARGER <ref type="bibr" coords="13,300.22,456.05,10.58,8.64" target="#b8">[9]</ref>, hosted on our servers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Description</head><p>Retrieval Topics. We selected 50 comparative questions from questions submitted to commercial search engines or asked on question answering platforms <ref type="bibr" coords="13,423.54,517.82,10.58,8.64" target="#b6">[7]</ref>, each covering some personal decision from everyday life. For every question, we have formulated a respective TREC-style topic with the question as the title, a description of the searcher's possible context and information need, and a narrative describing what makes a result relevant (i.e., serving as a guideline for human assessors). An example topic is shown in Table <ref type="table" coords="13,197.01,577.59,3.74,8.64" target="#tab_4">4</ref>. In the topic creation, we ensured through manual spot checks that the ClueWeb12 collection actually contains possibly relevant documents for all topics. A person is planning to move out from their current small flat to start a family. Hoping that the new family will stay together in the new place for some longer time, the person is considering to even buy a new home and not just to rent it. However, this is kind of an important decision with many different angles to be considered: financial situation, the duties coming with owning a flat/house, potential happiness living in a property owned by someone else without any further (financial) responsibilities when major redos are needed, etc. ChatNoir API for a BM25F-based baseline retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Survey of Submissions to Task 2</head><p>Five teams submitted a total of eleven approaches to this task (ten of which plus the additional ChatNoir baseline are used to create the assessment pools). All approaches use the BM25F-based search engine ChatNoir <ref type="bibr" coords="14,321.91,432.09,11.62,8.64" target="#b5">[6]</ref> to retrieve candidate documents that are then re-ranked using machine learning models of different complexity in basically three steps: (1) Represent documents and queries using language models, (2) identify arguments and comparative structures in documents, and (3) assess argument quality. Only two approaches use query expansion techniques for retrieving the candidates before re-ranking. The characteristics of the teams' approaches are summarized in Table <ref type="table" coords="14,475.61,491.86,4.98,8.64" target="#tab_6">5</ref> and detailed below (teams ordered alphabetically).</p><p>Bilbo Baggins by Abye et al. <ref type="bibr" coords="14,256.07,527.73,11.62,8.64" target="#b0">[1]</ref> uses a two-stage retrieval pipeline: (1) Query expansion to increase the recall of the candidate retrieval, and (2) re-ranking the candidate documents using three feature types: relevance, credibility, and support features. Before querying ChatNoir, Bilbo Baggins expands topic titles with synonyms and antonyms from WordNet for entities (e.g., laptop and desktop) and comparison aspects (e.g., better, best) detected with Spacy. <ref type="foot" coords="14,255.97,585.84,6.97,6.05" target="#foot_13">16</ref> Then, ChatNoir is queried with four queries for each topic: (1) the original topic title, (2) all identified entities as one conjunctive ANDquery, (3) all entities and comparison aspects as one disjunctive OR-query, (4) all entities, aspects, their synonyms and antonyms as one disjunctive OR-query. The set of the top-30 results of each of these four queries are then re-ranked using "relevance features" (PageRank, number of comparative sentences as identified by an XGBoost classifier with InferSent embeddings <ref type="bibr" coords="15,288.57,272.96,15.27,8.64" target="#b29">[30]</ref>, argument ratio Reimers et al. <ref type="bibr" coords="15,436.62,272.96,14.94,8.64" target="#b33">[34]</ref>), document "credibility" (SpamRank, BlocklistedLinks), and "support" features (number of sentences that support claims <ref type="bibr" coords="15,252.38,296.87,14.94,8.64" target="#b34">[35]</ref>), where features are respective numerical scores. The final ranking is created over the sums of the scores multiplied with weighting values.</p><p>Frodo Baggins by Sievers <ref type="bibr" coords="15,247.86,332.31,16.60,8.64" target="#b38">[39]</ref> explores the hypothesis that large language models, given a search query as input, can generate prototypical candidate documents similar to relevant documents. This prototype document is generated using the GPT-2 model conditioned on the original query and a maximum of 1024 tokens for the generated text.</p><p>The TF-IDF-based cosine similarity between a ChatNoir search result document and the generated prototype document induces Frodo Baggins' re-ranking. As for query expansion, each term of the original query is augmented by its nearest neighbor according to the cosine similarity of GloVe embeddings.</p><p>Inigo Montoya by Huck <ref type="bibr" coords="15,235.18,439.48,16.60,8.64" target="#b15">[16]</ref> uses the topic titles as queries, retrieving ChatNoir's top-20 results. For each result, the TARGER argument tagger is used to extract the argumentative units (premises and claims) into a new document for each original result. These new documents are then BM25-indexed using the Whoosh python library 17 (BM25 parameters b=0.75 and k 1 =1.2, document body: the set of arguments of the original document, document title: document ID from the ClueWeb12). This index is then queried with the topic titles as a disjunctive OR-query.</p><p>Katana by Chekalina and Panchenko <ref type="bibr" coords="15,297.95,534.69,11.62,8.64" target="#b7">[8]</ref> comprises a total of seven different approaches (runs), which all re-rank ChatNoir's top-1000 results for the topic title as the query. The re-ranking is based on different language models to encode query-document pairs and different variations of similarity:</p><p>-Run 1 only removes "duplicate" results (based on an exact title match against higher-ranked results) from ChatNoir's original top-1000 results. -For Run 2 and other following Runs, Katana re-ranks Run 1 (basically, ChatNoir's results) by using as the sorting criterion a document's original ChatNoir relevance score multiplied with a "comparativeness" score. This comparativeness score is the number of comparative sentences in the document as identified by an XG-Boost classifier with InferSent embeddings <ref type="bibr" coords="16,331.08,131.27,15.27,8.64" target="#b29">[30]</ref>, which is additionally increased if a query's comparison objects, aspects, and predicates are found in a document (using a one-layer LSTM with 200 hidden units and BERT embeddings for the input, pre-trained on a dataset created by the authors). -For Run 3, the final ranking is based on the cosine similarity between a query and a document using the ULMFiT language model <ref type="bibr" coords="16,345.00,191.04,16.60,8.64" target="#b14">[15]</ref> multiplied with the comparativeness score also used in Run 2. -For Run 4, the final ranking is based on the cosine similarity between BERT encodings of the query and the document titles (document body neglected). -For Run 5, the final ranking is based on a similarity score calculated using a complex formula composed of the weights from selected BERT attention heads in a standard transformer (cf. the team's paper <ref type="bibr" coords="16,320.74,262.77,11.62,8.64" target="#b7">[8]</ref> for more details). -For Run 6, the final ranking is based on the cosine similarity between a query and a document using the ULMFiT language model (i.e., Run 3 without multiplication). -For Run 7, the similarity score is an average of the scores of the other runs multiplied with the comparativeness score. Note that this run is not part of the assessment pool due to our human assessors' workload.</p><p>Puss in Boots is the baseline for Task 2, which simply uses the results that Chat-Noir <ref type="bibr" coords="16,156.48,358.42,11.62,8.64" target="#b5">[6]</ref> returns for a topic's title. ChatNoir is an Elasticsearch-based search engine, indexing the complete ClueWeb12 (and also other web collections) by processing raw HTML documents using main content extraction, language detection, and extraction of metadata (keywords, headings, hostnames, etc.). During retrieval, ChatNoir combines BM25 scores of multiple fields (title, keywords, main content, and the full document) and uses the documents' SpamRank <ref type="bibr" coords="16,281.15,418.19,16.60,8.64" target="#b9">[10]</ref> as a threshold to remove spam.</p><p>Zorro by Shahshahani and Kamps <ref type="bibr" coords="16,279.07,442.10,16.60,8.64" target="#b37">[38]</ref>  To train the classifier, team Zorro sampled 3000 documents from the args.me corpus <ref type="bibr" coords="16,468.97,537.74,11.62,8.64" target="#b1">[2]</ref> as positive examples (argumentative) and 3000 documents from the ClueWeb12 as negative examples (not argumentative) by collecting the at most top-100 results from the args.me and ChatNoir APIs when queried with the 50 topics from Task 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Task Evaluation</head><p>Similar to Task 1, in the first lab year, we only evaluate the relevance of the retrieved documents but not any other argument quality dimensions. Using a top-5 pooling strategy of the submitted runs, including the baseline, a total of 1,783 unique results were  judged by human assessors. To this end, we recruited seven graduate and undergraduate student volunteers, all with a computer science background. We used a κ-test of five documents from five topics to calibrate the annotators' interpretations of the guidelines (i.e., the topics including the narratives) and the three relevance labels: 0 (not relevant), 1 (relevant), and 2 (highly relevant). The original Fleiss' κ of 0.46 indicates a moderate agreement such that a follow-up discussion among the annotators was invoked to adjust their individual interpretations and to emphasize that documents should not be judged as highly relevant when they do not provide well-formulated evidence support. After the κ test, each annotator judged the results for disjoint subsets of the topics (i.e., each topic judged by one annotator only). The achieved average nDCG@5 scores of the individual runs are given in Table <ref type="table" coords="17,473.11,516.89,3.74,8.64" target="#tab_7">6</ref>, while Figure <ref type="figure" coords="17,189.03,528.85,4.98,8.64" target="#fig_2">2</ref> also shows the 95% confidence intervals obtained using bootstrapping (n = 10, 000). Only Bilbo Baggins achieves a slightly better average nDCG@5 score than the ChatNoir baseline by using query expansion and taking credibility and argumentativeness into account in the re-ranking. A reason for the pretty strong baseline effectiveness of Puss in Boots might be that, during the topic selection, the topic titles were already checked against the ClueWeb12 collection-and to this end the topic creators actually did submit the topic titles to ChatNoir and checked whether some result snippets at least contain the comparison items. However, since all the teams' approaches submitted to Task 2 do use the ChatNoir results as their candidates for re-ranking, the potential "easiness" of the topics did not really favor any approach. Still, topic selection is an issue that we will treat differently in the future.</p><p>The top-5 approaches (including the baseline) all lie in the 0.55-0.58 range with their average nDCG@5 scores. Interestingly, the top-4 runs are classical feature engineering approaches, while four out of the six lower-ranked runs (right side of Table <ref type="table" coords="18,472.29,143.22,4.15,8.64" target="#tab_7">6</ref>) use deep learning-based language models. This observed difference in the effectiveness between the feature-based and the deep learning-based approaches might be caused by the fact that no training data was available, such that it will be interesting to observe whether any fine-tuning based on the now created ground truth might help in the future. That some more respective research effort is justified is also indicated by the fact that none of the approaches actually substantially improved upon the baseline ranking, even though there still is quite some headroom towards "perfect" effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary and Outlook</head><p>Touché and its two shared tasks have been designed with the goal to establish a collaborative platform for researchers in the field of argument retrieval. Starting from argument relevance and argument quality corpora, Touché is meant to provide tools for the submission and evaluation of retrieval approaches, and to organize collaboration events such as workshops. By providing argument retrieval baselines and APIs, also researchers new to the field may quickly start developing their own approaches.</p><p>The first edition of Touché featured two tasks: (1) conversational argument retrieval to support argumentation on socially important problems in dialogue or debate scenarios, and (2) comparative argument retrieval to support decision making on a personal level. In total, 17 teams submitted 41 different approaches that were evaluated with respect to relevance. Still, we find that relatively "simple" argumentation-agnostic baselines like DirichletLM-based or BM25F-based retrieval are not substantially worse then the best approaches, or they are even are on a par with them. The best approaches share some common techniques such as query expansion, argument quality assessment, and the identification of comparative textual features in documents, but there still seems to be a lot of room for improvement. Further research on argument retrieval thus seems well-justified.</p><p>In the future, the participants will be able to use this year's relevance judgments to develop and fine-tune new approaches. We also plan to have deeper judgment pools and to additionally evaluate argument quality dimensions, such as logical cogency and strength of support.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="12,134.77,440.80,345.83,8.12;12,134.77,452.11,169.59,7.77"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Mean nDCG@5 and 95% confidence intervals for runs submitted to Task 1, args.me corpus Version 1 (top), and Version 2 (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="16,298.81,442.10,181.78,8.64;16,134.77,454.06,345.82,8.64;16,134.77,466.01,345.82,8.64;16,134.77,477.97,345.82,8.64;16,134.77,489.92,345.82,8.64;16,134.77,501.88,345.82,8.64;16,134.77,513.83,345.82,8.64;16,134.77,525.79,345.82,8.64"><head></head><label></label><figDesc>re-ranks ChatNoir's top-1000 results in three consecutive steps: (1) Non-overlapping subsets of 10 documents are re-ranked by descending PageRank scores (i.e., original ranks 1-10, ranks 11-20, etc.). (2) In nonoverlapping subsets of 10 documents already re-ranked by the first step, documents from blogs and discussions (categorization based on the URL domain) are moved to the top (i.e., rank 20 after Step 1 might at most move up to rank 11). (3) In non-overlapping subsets of 20 documents re-ranked with Steps 1 and 2, documents are moved to the top that are more argumentative according to an SVM-based argumentativeness classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="17,153.93,369.91,307.49,8.12"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Mean nDCG@5 and 95% confidence intervals for runs submitted to Task 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,135.07,115.83,300.10,55.48"><head>Table 1 .</head><label>1</label><figDesc>Example topic for Task 1: Conversational Argument Retrieval</figDesc><table coords="6,135.07,137.24,300.10,34.07"><row><cell>Number</cell><cell>21</cell></row><row><cell>Title</cell><cell>Is human activity primarily responsible for global climate change?</cell></row><row><cell>Description</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,134.77,115.83,345.82,194.39"><head>Table 3 .</head><label>3</label><figDesc>Results for Task 1 on conversational argument retrieval for (a) args.me corpus Version 1, and (b) args.me corpus Version 2. The baseline approach is in bold.Arya Stark and Thongor uses the BM25 model and preprocesses the queries by trying different combinations of the tokens (shingling). Before generating combinations for a topic, stop words are removed and the tokens are stemmed.</figDesc><table coords="11,134.77,143.50,345.82,117.40"><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell>Team</cell><cell cols="2">nDCG@5 Team (continued)</cell><cell>nDCG@5</cell><cell>Team</cell><cell>nDCG@5</cell></row><row><cell>Weiss Schnee</cell><cell></cell><cell>0.804 Prince of Persia (Run 4)</cell><cell>0.641</cell><cell>Dread Pirate Roberts (Run 1)</cell><cell>0.808</cell></row><row><cell>Prince of Persia (Run 1)</cell><cell></cell><cell>0.791 Don Quixote</cell><cell>0.617</cell><cell>Swordsman (Baseline)</cell><cell>0.756</cell></row><row><cell>The Three Mouseketeers</cell><cell></cell><cell>0.789 Aragorn (Run 3)</cell><cell>0.593</cell><cell>Dread Pirate Roberts (Run 2)</cell><cell>0.755</cell></row><row><cell>Swordsman (Baseline)</cell><cell></cell><cell>0.769 Prince of Persia (Run 5)</cell><cell>0.555</cell><cell>Aragorn (Run 1)</cell><cell>0.684</cell></row><row><cell cols="2">Dread Pirate Roberts (Run 2)</cell><cell>0.743 Aragorn (Run 2)</cell><cell>0.331</cell><cell>Dread Pirate Roberts (Run 3)</cell><cell>0.598</cell></row><row><cell>Prince of Persia (Run 2)</cell><cell></cell><cell>0.724 Aragorn (Run 4)</cell><cell>0.319</cell><cell>Zorro</cell><cell>0.573</cell></row><row><cell>Thongor</cell><cell></cell><cell>0.717 Aragorn (Run 1)</cell><cell>0.288</cell><cell>Dread Pirate Roberts (Run 4)</cell><cell>0.527</cell></row><row><cell>Oscar François de Jarjayes</cell><cell></cell><cell>0.699 Aragorn (Run 5)</cell><cell>0.271</cell><cell>Dread Pirate Roberts (Run 5)</cell><cell>0.519</cell></row><row><cell>Black Knight</cell><cell></cell><cell>0.692 Boromir</cell><cell>0.152</cell><cell>Aragorn (Run 2)</cell><cell>0.404</cell></row><row><cell>Utena Tenjou</cell><cell></cell><cell>0.689</cell><cell></cell><cell>Aragorn (Run 3)</cell><cell>0.372</cell></row><row><cell>Arya Stark</cell><cell></cell><cell>0.662</cell><cell></cell><cell>Aragorn (Run 4)</cell><cell>0.371</cell></row><row><cell>Prince of Persia (Run 3)</cell><cell></cell><cell>0.642</cell><cell></cell><cell>Aragorn (Run 5)</cell><cell>0.319</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,135.07,115.83,297.61,59.52"><head>Table 4 .</head><label>4</label><figDesc>Example topic for Task 2 on comparative argument retrieval.</figDesc><table coords="14,135.07,141.27,137.79,34.07"><row><cell>Number</cell><cell>16</cell></row><row><cell>Title</cell><cell>Should I buy or rent?</cell></row><row><cell>Description</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,134.77,246.48,345.83,108.57"><head></head><label></label><figDesc>Narrative Highly relevant documents contain various pros and cons for buying or renting a home. Particularly interesting could be checklists of what to favor in what situations. Documents containing definitions and "smaller" comparisons of buying or renting a property are relevant. Documents without any personal opinion/recommendation or pros/cons are not relevant. Document Collection. Task 2 is based on the ClueWeb12 crawl 15 from between February and May 2012 (733 million English web pages; 27.3TB uncompressed). Participants of Task 2 could index the ClueWeb12 on their own or could use the Elasticsearch-based</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="15,134.77,115.83,345.82,110.27"><head>Table 5 .</head><label>5</label><figDesc>Overview of the participating teams' strategies for Task 2. All approaches use the BM25F-based search engine ChatNoir for an initial candidates retrieval (Puss in Boots as the baseline being the unchanged ChatNoir results).</figDesc><table coords="15,135.99,160.60,343.38,65.50"><row><cell>Team</cell><cell>Representation</cell><cell>Query processing</cell><cell>(Re-)Ranking features</cell></row><row><cell cols="2">Bilbo Baggins Bag of words</cell><cell cols="2">Named entities, comp. aspects Credibility, support</cell></row><row><cell cols="2">Frodo Baggins Bag of words</cell><cell>GloVe nearest neighbors</cell><cell>Simil. with gen. documents (GPT-2)</cell></row><row><cell cols="2">Inigo Montoya Bag of words</cell><cell>Tokens &amp; logic. OR</cell><cell>Argum. units (TARGER)</cell></row><row><cell>Katana</cell><cell cols="2">Diff. language models Diff. language models</cell><cell>Comparativeness score</cell></row><row><cell cols="2">Puss in Boots Bag of words</cell><cell>-</cell><cell>BM25F, SpamRank</cell></row><row><cell>Zorro</cell><cell>Bag of words</cell><cell>-</cell><cell>PageRank, argumentativeness</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="17,142.46,115.83,330.45,241.63"><head>Table 6 .</head><label>6</label><figDesc>Results for Task 2 on comparative argument retrieval. Baseline approach is in bold.</figDesc><table coords="17,157.34,139.39,297.99,218.07"><row><cell>Team</cell><cell></cell><cell>nDCG@5</cell><cell cols="2">Team (continued)</cell><cell>nDCG@5</cell></row><row><cell>Bilbo Baggins</cell><cell></cell><cell>0.580</cell><cell cols="2">Frodo Baggins</cell><cell>0.450</cell></row><row><cell>Puss in Boots (ChatNoir)</cell><cell></cell><cell>0.568</cell><cell>Zorro</cell><cell></cell><cell>0.446</cell></row><row><cell>Inigo Montoya</cell><cell></cell><cell>0.567</cell><cell cols="2">Katana (Run 4)</cell><cell>0.404</cell></row><row><cell>Katana (Run 1)</cell><cell></cell><cell>0.564</cell><cell cols="2">Katana (Run 5)</cell><cell>0.223</cell></row><row><cell>Katana (Run 2)</cell><cell></cell><cell>0.553</cell><cell cols="2">Katana (Run 6)</cell><cell>0.200</cell></row><row><cell>Katana (Run 3)</cell><cell></cell><cell>0.464</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Katana (Run 6) Katana (Run 5) Katana (Run 4) Zorro Frodo Baggins Katana (Run 3) Katana (Run 2) Katana (Run 1) Inigo Montoya Puss in Boots (ChatNoir) Bilbo Baggins</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,138.25,601.72,245.72,7.77"><p>A notable exception have been Bing's "Multi-Perspective Answers".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,138.25,612.87,319.72,7.77;2,138.25,623.83,146.06,7.77"><p>https://blogs.bing.com/search-quality-insights/february-2018/Toward-a-More-Intelligent-Search-Bing-Multi-Perspective-Answers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,134.77,633.11,2.99,5.18;2,138.25,634.98,243.61,7.77"><p>3 https://www.research.ibm.com/artificial-intelligence/project-debater</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,381.86,634.98,3.58,7.77;2,134.77,644.26,2.99,5.18;2,138.25,646.13,342.34,7.77;2,138.25,657.08,339.88,7.77"><p>/ 4 'touché' is commonly "used to acknowledge a hit in fencing or the success or appropriateness of an argument, an accusation, or a witty point." [https://merriam-webster.com/dictionary/touche]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,138.25,657.08,86.62,7.77"><p>http://commoncrawl.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,138.25,613.17,129.47,7.77"><p>https://www.mywot.com/developers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="5,138.25,624.16,208.20,7.77"><p>Also described on the lab website: https://touche.webis.de</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="6,138.25,657.89,118.05,7.77"><p>https://www.args.me/api-en.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8" coords="11,141.24,657.08,118.06,7.77"><p>https://pypi.org/project/trectools/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9" coords="12,141.24,630.45,106.51,7.77"><p>https://trec.nist.gov/trec_eval/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10" coords="12,141.24,641.60,288.04,7.77;12,141.24,652.55,99.30,7.77"><p>https://lucene.apache.org/core/8_6_0/core/org/apache/lucene/search/similarities/ LMDirichletSimilarity.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11" coords="13,141.24,609.18,128.37,7.77"><p>https://lemurproject.org/clueweb12/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_12" coords="14,141.24,643.00,128.37,7.77"><p>https://lemurproject.org/clueweb12/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_13" coords="14,141.24,654.15,56.33,7.77"><p>https://spacy.io/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We are very grateful to the CLEF 2020 organizers and the Touché participants, who allowed this lab to happen. We also want to thank our volunteer annotators who helped to create the relevance assessments.</p><p>This work was supported by the <rs type="funder">DFG</rs> through the project "<rs type="projectName">ACQuA: Answering Comparative Questions with Arguments"</rs> (grants <rs type="grantNumber">BI 1544/7-1</rs> and <rs type="grantNumber">HA 5851/2-1</rs>) as part of the priority program "<rs type="projectName">RATIO: Robust Argumentation Machines"</rs> (<rs type="grantNumber">SPP 1999</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_hPDzd5S">
					<idno type="grant-number">BI 1544/7-1</idno>
					<orgName type="project" subtype="full">ACQuA: Answering Comparative Questions with Arguments&quot;</orgName>
				</org>
				<org type="funded-project" xml:id="_vS3fxdr">
					<idno type="grant-number">HA 5851/2-1</idno>
					<orgName type="project" subtype="full">RATIO: Robust Argumentation Machines&quot;</orgName>
				</org>
				<org type="funding" xml:id="_7kghJZK">
					<idno type="grant-number">SPP 1999</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="19,154.69,140.09,312.27,7.77;19,154.68,151.05,316.08,7.77;19,154.68,162.01,284.58,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="19,275.45,140.09,191.51,7.77;19,154.68,151.05,316.08,7.77">An Open-Domain Web Search Engine for Answering Comparative Questions-Notebook for the Touché Lab on Argument Retrieval at CLEF</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Abye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,189.30,162.01,208.12,7.77">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<imprint>
			<date type="published" when="2020-09">2020. Sep 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,154.69,172.28,324.52,7.77;19,154.68,183.23,287.29,7.77;19,154.68,194.19,304.87,7.77;19,154.68,205.15,225.23,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="19,418.19,172.28,61.02,7.77;19,154.68,183.23,150.79,7.77">Data Acquisition for Argument Search: The args.me Corpus</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-30179-8_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-30179-8_4" />
	</analytic>
	<monogr>
		<title level="m" coord="19,323.94,183.23,118.03,7.77;19,154.68,194.19,51.37,7.77">Proceedings of the 42nd German Conference AI</title>
		<title level="s" coord="19,246.31,194.19,126.47,7.77">Lecture Notes in Computer Science</title>
		<meeting>the 42nd German Conference AI<address><addrLine>KI</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">11793</biblScope>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,154.69,215.42,320.34,7.77;19,154.68,226.38,312.85,7.77;19,154.68,237.33,144.48,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="19,240.62,215.42,234.41,7.77;19,154.68,226.38,172.75,7.77">Exploring Argument Retrieval with Transformers-Notebook for the Touché Lab on Argument Retrieval at CLEF</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,364.29,226.38,103.24,7.77;19,154.68,237.33,102.64,7.77">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<imprint>
			<date type="published" when="2020-09">2020. Sep 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,154.69,247.60,307.75,7.77;19,154.68,258.56,84.81,7.77" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="19,248.75,247.60,151.11,7.77">On Rhetoric: A Theory of Civic Discourse</title>
		<author>
			<persName coords=""><forename type="first">Kennedy</forename><surname>Aristotle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,154.69,268.82,322.20,7.77;19,154.68,279.78,310.43,7.77;19,154.68,290.74,316.89,7.77;19,154.68,301.70,303.74,7.77;19,154.68,312.66,260.88,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="19,299.41,279.78,165.71,7.77;19,154.68,290.74,147.04,7.77">From Surrogacy to Adoption; From Bitcoin to Cryptocurrency: Debate Topic Expansion</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Toledo-Ronen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Edelstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bilu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Halfon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Menczel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Aharonov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1094</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1094" />
	</analytic>
	<monogr>
		<title level="m" coord="19,320.18,290.74,151.40,7.77;19,154.68,301.70,193.12,7.77">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="977" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,154.69,322.92,324.16,7.77;19,154.68,333.88,325.37,7.77;19,154.68,344.84,304.64,7.77;19,154.68,355.80,229.72,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="19,336.16,322.92,142.69,7.77;19,154.68,333.88,119.42,7.77">Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-76941-7_83</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-76941-7_83" />
	</analytic>
	<monogr>
		<title level="m" coord="19,292.55,333.88,187.51,7.77;19,154.68,344.84,76.15,7.77">Proceedings of the 40th European Conference on IR Research, ECIR 2018</title>
		<title level="s" coord="19,237.11,344.84,126.47,7.77">Lecture Notes in Computer Science</title>
		<meeting>the 40th European Conference on IR Research, ECIR 2018</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10772</biblScope>
			<biblScope unit="page" from="820" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,154.69,366.06,320.35,7.77;19,154.68,377.02,321.93,7.77;19,154.68,387.98,325.91,7.77;19,154.68,398.94,177.93,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="19,245.85,377.02,127.59,7.77">Comparative Web Search Questions</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Braslavski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Völske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3336191.3371848</idno>
		<ptr target="https://doi.org/10.1145/3336191.3371848" />
	</analytic>
	<monogr>
		<title level="m" coord="19,391.69,377.02,84.92,7.77;19,154.68,387.98,259.27,7.77">Proceedings of the 13th International Conference on Web Search and Data Mining, WSDM 2020</title>
		<meeting>the 13th International Conference on Web Search and Data Mining, WSDM 2020</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,154.69,409.20,322.60,7.77;19,154.68,420.16,308.20,7.77;19,154.68,431.12,308.74,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="19,264.62,409.20,212.67,7.77;19,154.68,420.16,308.20,7.77;19,154.68,431.12,21.92,7.77">Retrieving Comparative Arguments using Deep Pre-trained Language Models and NLU-Notebook for the Touché Lab on Argument Retrieval at CLEF</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chekalina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,213.47,431.12,208.12,7.77">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<imprint>
			<date type="published" when="2020-09">2020. Sep 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,154.69,441.38,323.52,7.77;19,154.68,452.34,325.53,7.77;19,154.68,463.30,308.46,7.77;19,154.68,474.26,216.93,7.77;19,154.68,485.22,134.84,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="19,211.74,452.34,196.99,7.77">TARGER: Neural Argument Mining at Your Fingertips</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Chernodub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Oliynyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Heidenreich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-3031</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-3031" />
	</analytic>
	<monogr>
		<title level="m" coord="19,426.68,452.34,53.54,7.77;19,154.68,463.30,290.98,7.77">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,154.69,495.48,288.27,7.77;19,154.68,506.09,280.51,8.12" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="19,297.09,495.48,145.87,7.77;19,154.68,506.44,116.69,7.77">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,277.02,506.44,74.50,7.77">Information retrieval</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="441" to="465" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,154.69,516.70,324.97,7.77;19,154.68,527.66,324.69,7.77;19,154.68,538.62,321.31,7.77;19,154.68,549.58,279.03,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="19,306.27,516.70,173.39,7.77;19,154.68,527.66,173.64,7.77">A Framework for Argument Retrieval -Ranking Argument Clusters by Frequency and Specificity</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dumani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-45439-5_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-45439-5_29" />
	</analytic>
	<monogr>
		<title level="m" coord="19,355.86,527.66,123.51,7.77;19,154.68,538.62,142.13,7.77">Proceedings of the 42nd European Conference on IR Research, ECIR 2020</title>
		<title level="s" coord="19,303.10,538.62,126.47,7.77">Lecture Notes in Computer Science</title>
		<meeting>the 42nd European Conference on IR Research, ECIR 2020</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12035</biblScope>
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,154.69,559.84,299.38,7.77;19,154.68,570.80,318.27,7.77;19,154.68,581.76,308.74,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="19,250.48,559.84,203.59,7.77;19,154.68,570.80,318.27,7.77;19,154.68,581.76,21.92,7.77">Ranking Arguments by Combining Claim Similarity and Argument Quality Dimensions-Notebook for the Touché Lab on Argument Retrieval at CLEF</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dumani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,213.47,581.76,208.12,7.77">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<imprint>
			<date type="published" when="2020-09">2020. Sep 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,154.69,592.03,265.76,7.77;19,154.68,602.99,325.91,7.77;19,154.68,613.94,216.56,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="19,245.49,592.03,174.95,7.77;19,154.68,602.99,258.14,7.77">Argument Retrieval Using Deep Neural Ranking Models-Notebook for the Touché Lab on Argument Retrieval at CLEF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Völske</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,449.44,602.99,31.16,7.77;19,154.68,613.94,174.72,7.77">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<imprint>
			<date type="published" when="2020-09">2020. Sep 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,154.69,624.21,289.53,7.77;19,154.68,635.17,307.90,7.77;19,154.68,646.13,310.98,7.77;19,154.68,657.08,286.71,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="19,328.28,624.21,115.95,7.77;19,154.68,635.17,63.94,7.77">Efficient Pairwise Annotation of Argument Quality</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.511/" />
	</analytic>
	<monogr>
		<title level="m" coord="19,236.71,635.17,225.87,7.77;19,154.68,646.13,135.84,7.77">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5772" to="5781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,154.69,119.96,319.21,7.77;20,154.68,130.92,325.76,7.77;20,154.68,141.88,272.61,7.77;20,154.68,152.84,135.35,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="20,235.94,119.96,222.35,7.77">Universal Language Model Fine-tuning for Text Classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1031</idno>
		<ptr target="https://doi.org/10.18653/v1/P18-1031" />
	</analytic>
	<monogr>
		<title level="m" coord="20,154.68,130.92,325.76,7.77;20,154.68,141.88,35.95,7.77">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,154.69,163.80,314.53,7.77;20,154.68,174.76,325.54,7.77;20,154.68,185.71,144.48,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="20,189.05,163.80,280.17,7.77;20,154.68,174.76,185.44,7.77">Development of a Search Engine to Answer Comparative Queries-Notebook for the Touché Lab on Argument Retrieval at CLEF</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,376.99,174.76,103.24,7.77;20,154.68,185.71,102.64,7.77">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<imprint>
			<date type="published" when="2020-09">2020. Sep 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,154.69,196.67,313.20,7.77;20,154.68,207.28,285.95,8.12" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="20,256.17,196.67,185.50,7.77">Cumulated Gain-based Evaluation of IR Techniques</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
		<idno type="DOI">10.1145/582415.582418</idno>
		<ptr target="https://doi.org/10.1145/582415.582418" />
	</analytic>
	<monogr>
		<title level="j" coord="20,447.82,196.67,20.07,7.77;20,154.68,207.63,57.21,7.77">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,154.69,218.59,325.87,7.77;20,154.68,229.55,325.10,7.77;20,154.68,240.51,182.74,7.77;20,154.68,251.47,149.54,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="20,223.68,218.59,194.30,7.77">Identifying Comparative Sentences in Text Documents</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/1148170.1148215</idno>
		<ptr target="https://doi.org/10.1145/1148170.1148215" />
	</analytic>
	<monogr>
		<title level="m" coord="20,436.73,218.59,43.83,7.77;20,154.68,229.55,325.10,7.77;20,154.68,240.51,78.70,7.77">Proceedings of the 29th Annual International Conference on Research and Development in Information Retrieval, SIGIR 2006</title>
		<meeting>the 29th Annual International Conference on Research and Development in Information Retrieval, SIGIR 2006</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="244" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,154.69,262.43,316.42,7.77;20,154.68,273.39,325.91,7.77;20,154.68,284.34,322.48,7.77;20,154.68,295.30,204.20,7.77" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="20,223.68,262.43,162.64,7.77">Mining Comparative Sentences and Relations</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/Library/AAAI/2006/aaai06-209.php" />
	</analytic>
	<monogr>
		<title level="m" coord="20,404.38,262.43,66.74,7.77;20,154.68,273.39,325.91,7.77;20,154.68,284.34,165.79,7.77">Proceedings of the 21st National Conference on Artificial Intelligence and the 18th Innovative Applications of Artificial Intelligence Conference, AAAI 2006</title>
		<meeting>the 21st National Conference on Artificial Intelligence and the 18th Innovative Applications of Artificial Intelligence Conference, AAAI 2006</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1331" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,154.69,306.26,316.03,7.77;20,154.68,317.22,323.51,7.77;20,154.68,328.18,280.65,7.77;20,154.68,339.14,249.53,7.77" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="20,234.57,306.26,164.08,7.77">A Corpus of Comparisons in Product Reviews</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kuhn</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2014/summaries/1001.html" />
	</analytic>
	<monogr>
		<title level="m" coord="20,417.18,306.26,53.54,7.77;20,154.68,317.22,306.02,7.77">Proceedings of the 9th International Conference on Language Resources and Evaluation, LREC 2014</title>
		<meeting>the 9th International Conference on Language Resources and Evaluation, LREC 2014</meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2242" to="2248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,154.69,350.10,300.26,7.77;20,154.68,361.06,320.87,7.77;20,154.68,372.02,318.52,7.77;20,154.68,382.97,318.05,7.77;20,154.68,393.93,164.68,7.77" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="20,360.88,350.10,94.07,7.77;20,154.68,361.06,162.71,7.77">Towards an argumentative content search engine using weak supervision</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gretz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Aharonov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C18-1176/" />
	</analytic>
	<monogr>
		<title level="m" coord="20,185.56,372.02,287.64,7.77;20,154.68,382.97,52.25,7.77">Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Derczynski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Isabelle</surname></persName>
		</editor>
		<meeting>the 27th International Conference on Computational Linguistics, COLING 2018</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2066" to="2081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,154.69,404.89,318.09,7.77;20,154.68,415.85,321.54,7.77;20,154.68,426.81,294.28,7.77;20,154.68,437.77,246.07,7.77;20,154.68,448.73,196.05,7.77" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="20,307.26,404.89,165.52,7.77;20,154.68,415.85,218.77,7.77">Entity-Aware Dependency-Based Deep Graph Attention Network for Comparative Preference Classification</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.512/" />
	</analytic>
	<monogr>
		<title level="m" coord="20,391.31,415.85,84.92,7.77;20,154.68,426.81,276.79,7.77">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5782" to="5788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,154.69,459.69,317.82,7.77;20,154.68,470.65,320.16,7.77;20,154.68,481.60,319.49,7.77;20,154.68,492.56,303.19,7.77" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="20,168.13,470.65,203.33,7.77">Word Emphasis Prediction for Expressive Text to Speech</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mordechay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hoory</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">S</forename><surname>Shalom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Konopnicki</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2018-1159</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2018-1159" />
	</analytic>
	<monogr>
		<title level="m" coord="20,389.93,470.65,84.92,7.77;20,154.68,481.60,319.49,7.77;20,154.68,492.56,16.14,7.77">Proceedings of the 19th Annual Conference of the International Speech Communication Association, Interspeech 2018</title>
		<meeting>the 19th Annual Conference of the International Speech Communication Association, Interspeech 2018</meeting>
		<imprint>
			<publisher>ISCA</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2868" to="2872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,154.69,503.52,323.72,7.77;20,154.68,514.48,295.51,7.77;20,154.68,525.44,314.96,7.77" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="20,304.76,503.52,173.65,7.77;20,154.68,514.48,260.89,7.77">Creating an Argument Search Engine for Online Debates-Notebook for the Touché Lab on Argument Retrieval at CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bundesmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Richter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,154.68,525.44,208.12,7.77">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<imprint>
			<date type="published" when="2020-09">2020. Sep 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,154.69,536.40,297.57,7.77;20,154.68,547.36,295.68,7.77;20,154.68,558.32,193.63,7.77;20,154.68,569.28,140.58,7.77" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="20,253.32,536.40,198.94,7.77;20,154.68,547.36,80.23,7.77">A Comparative Web Browser (CWB) for Browsing and Comparing Web Pages</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nadamoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
		<idno type="DOI">10.1145/775152.775254</idno>
		<ptr target="https://doi.org/10.1145/775152.775254" />
	</analytic>
	<monogr>
		<title level="m" coord="20,253.20,547.36,197.17,7.77;20,154.68,558.32,89.58,7.77">Proceedings of the 12th International World Wide Web Conference, WWW 2003</title>
		<meeting>the 12th International World Wide Web Conference, WWW 2003</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="727" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,154.69,580.23,288.33,7.77;20,154.68,591.19,304.93,7.77;20,154.68,602.15,325.91,7.77;20,154.68,613.11,319.45,7.77" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="20,418.76,580.23,24.26,7.77;20,154.68,591.19,105.69,7.77">Terrier information retrieval platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-31865-1_37</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-31865-1_37" />
	</analytic>
	<monogr>
		<title level="m" coord="20,278.50,591.19,181.11,7.77;20,154.68,602.15,190.07,7.77">Advances in Information Retrieval, 27th European Conference on IR Research, ECIR 2005, Proceedings</title>
		<title level="s" coord="20,350.73,602.15,126.09,7.77">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3408</biblScope>
			<biblScope unit="page" from="517" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,154.69,624.07,320.15,7.77;20,154.68,635.03,194.36,7.77" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="20,322.05,624.07,152.80,7.77;20,154.68,635.03,59.41,7.77">The PageRank Citation Ranking: Bringing Order to the Web</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Stanford InfoLab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct coords="20,154.69,645.99,306.88,7.77;20,154.68,656.95,322.05,7.77;21,154.68,119.96,325.65,7.77;21,154.68,130.92,306.59,7.77" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="20,295.02,645.99,166.55,7.77;20,154.68,656.95,249.80,7.77">TrecTools: an Open-source Python Library for Information Retrieval Practitioners Involved in TREC-like Campaigns</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R M</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Scells</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331399</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331399" />
	</analytic>
	<monogr>
		<title level="m" coord="20,423.19,656.95,53.54,7.77;21,154.68,119.96,325.65,7.77;21,154.68,130.92,41.79,7.77">Proceedings of the 42nd International Conference on Research and Development in Information Retrieval, SIGIR 2019</title>
		<meeting>the 42nd International Conference on Research and Development in Information Retrieval, SIGIR 2019</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1325" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,154.69,140.98,304.86,7.77;21,154.68,151.94,300.60,7.77;21,154.68,162.89,321.06,7.77;21,154.68,173.85,136.84,7.77" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="21,413.36,140.98,46.19,7.77;21,154.68,151.94,82.49,7.77">Categorizing Comparative Sentences</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w19-4516</idno>
		<ptr target="https://doi.org/10.18653/v1/w19-4516" />
	</analytic>
	<monogr>
		<title level="m" coord="21,255.43,151.94,199.85,7.77;21,154.68,162.89,84.40,7.77">Proceedings of the 6th Workshop on Argument Mining, ArgMining@ACL 2019</title>
		<meeting>the 6th Workshop on Argument Mining, ArgMining@ACL 2019</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,154.69,183.91,304.86,7.77;21,154.68,194.87,316.32,7.77;21,154.68,205.83,325.91,7.77;21,154.68,216.79,184.84,7.77" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="21,413.36,183.91,46.19,7.77;21,154.68,194.87,82.49,7.77">Categorizing Comparative Sentences</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W19-4516" />
	</analytic>
	<monogr>
		<title level="m" coord="21,371.02,194.87,99.98,7.77;21,154.68,205.83,280.00,7.77">6th Workshop on Argument Mining (ArgMining 2019) at ACL, Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019-08">Aug 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,154.69,226.84,316.14,7.77;21,154.68,237.80,319.98,7.77;21,154.68,248.76,298.46,7.77;21,154.68,259.72,191.71,7.77;21,154.68,270.68,149.54,7.77" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="21,233.14,237.80,179.34,7.77">Argument Search: Assessing Argument Relevance</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Euchner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Heilenkötter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Weidmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331327</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331327" />
	</analytic>
	<monogr>
		<title level="m" coord="21,430.84,237.80,43.83,7.77;21,154.68,248.76,298.46,7.77;21,154.68,259.72,78.70,7.77">Proceedings of the 42nd International Conference on Research and Development in Information Retrieval, SIGIR 2019</title>
		<meeting>the 42nd International Conference on Research and Development in Information Retrieval, SIGIR 2019</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1117" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,154.69,280.74,325.90,7.77;21,154.68,291.70,306.41,7.77;21,154.68,302.66,318.49,7.77;21,154.68,313.61,163.73,7.77" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="21,337.03,280.74,143.56,7.77;21,154.68,291.70,306.41,7.77;21,154.68,302.66,51.30,7.77">TIRA Integrated Research Architecture. In: Information Retrieval Evaluation in a Changing World -Lessons Learned from 20 Years of CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-22948-1_5" />
	</analytic>
	<monogr>
		<title level="j" coord="21,213.06,302.66,116.63,7.77">The Information Retrieval Series</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="123" to="160" />
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,154.69,323.67,307.68,7.77;21,154.68,334.28,225.99,8.12" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="21,281.21,323.67,125.18,7.77">The Dilemma of the Direct Answer</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://sigir.org/forum/issues/june-2020/" />
	</analytic>
	<monogr>
		<title level="j" coord="21,412.94,323.67,49.43,7.77">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020-06">Jun 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,154.69,344.69,318.60,7.77;21,154.68,355.65,318.01,7.77;21,154.68,366.60,324.99,7.77;21,154.68,377.56,289.02,7.77;21,154.68,388.52,134.84,7.77" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="21,424.47,344.69,48.82,7.77;21,154.68,355.65,245.75,7.77">Classification and Clustering of Arguments with Contextualized Word Embeddings</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1054</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1054" />
	</analytic>
	<monogr>
		<title level="m" coord="21,419.16,355.65,53.54,7.77;21,154.68,366.60,290.98,7.77">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="567" to="578" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="21,154.69,398.58,308.60,7.77;21,154.68,409.54,310.65,7.77;21,154.68,420.50,288.86,7.77;21,154.68,431.46,316.56,7.77;21,154.68,442.42,163.23,7.77" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="21,428.89,398.58,34.40,7.77;21,154.68,409.54,294.74,7.77">Show Me Your Evidence -an Automatic Method for Context Dependent Evidence Detection</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dankin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">A</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1050</idno>
		<ptr target="https://doi.org/10.18653/v1/d15-1050" />
	</analytic>
	<monogr>
		<title level="m" coord="21,154.68,420.50,288.86,7.77;21,154.68,431.46,92.10,7.77">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<title level="s" coord="21,302.38,431.46,168.86,7.77">The Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
	<note>EMNLP 2015</note>
</biblStruct>

<biblStruct coords="21,154.69,452.47,323.30,7.77;21,154.68,463.43,289.73,7.77;21,154.68,474.39,297.97,7.77;21,154.68,485.35,324.67,7.77;21,154.68,496.31,177.93,7.77" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="21,314.48,452.47,163.51,7.77;21,154.68,463.43,18.58,7.77">Simple BM25 extension to multiple weighted fields</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="DOI">10.1145/1031171.1031181</idno>
		<ptr target="https://doi.org/10.1145/1031171.1031181" />
	</analytic>
	<monogr>
		<title level="m" coord="21,154.68,474.39,297.97,7.77;21,154.68,485.35,87.95,7.77">Proceedings of the 2004 ACM CIKM International Conference on Information and Knowledge Management</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Grossman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Gravano</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Herzog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</editor>
		<meeting>the 2004 ACM CIKM International Conference on Information and Knowledge Management<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">November 8-13, 2004. 2004</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,154.69,506.37,318.13,7.77;21,154.68,517.32,314.04,7.77;21,154.68,528.28,317.80,7.77;21,154.68,539.24,200.24,7.77" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="21,154.68,517.32,232.86,7.77">Answering Comparative Questions: Better than Ten-Blue-Links?</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schildwächter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zenker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<idno type="DOI">10.1145/3295750.3298916</idno>
		<ptr target="https://doi.org/10.1145/3295750.3298916" />
	</analytic>
	<monogr>
		<title level="m" coord="21,401.99,517.32,66.74,7.77;21,154.68,528.28,215.96,7.77">Proceedings of the Conference on Human Information Interaction and Retrieval</title>
		<meeting>the Conference on Human Information Interaction and Retrieval<address><addrLine>CHIIR</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="361" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,154.69,549.30,325.90,7.77;21,154.68,560.26,323.81,7.77;21,154.68,571.22,120.31,7.77" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="21,266.50,549.30,214.09,7.77;21,154.68,560.26,159.55,7.77">University of Amsterdam at CLEF 2020-Notebook for the Touché Lab on Argument Retrieval at CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Shahshahani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,351.09,560.26,127.40,7.77;21,154.68,571.22,78.47,7.77">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<imprint>
			<date type="published" when="2020-09">2020. Sep 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,154.69,581.27,317.47,7.77;21,154.68,592.23,312.85,7.77;21,154.68,603.19,144.48,7.77" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="21,198.16,581.27,274.00,7.77;21,154.68,592.23,172.75,7.77">Question Answering for Comparative Questions with GPT-2-Notebook for the Touché Lab on Argument Retrieval at CLEF</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sievers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,364.29,592.23,103.24,7.77;21,154.68,603.19,102.64,7.77">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<imprint>
			<date type="published" when="2020-09">2020. Sep 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,154.69,613.25,312.68,7.77;21,154.68,624.21,304.48,7.77;21,154.68,635.17,304.79,7.77;21,154.68,646.13,325.27,7.77;21,154.68,657.08,205.33,7.77" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="21,204.26,624.21,238.87,7.77">ArgumenText: Searching for Arguments in Heterogeneous Sources</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stahlhut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tauchmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-5005</idno>
		<ptr target="https://doi.org/10.18653/v1/n18-5005" />
	</analytic>
	<monogr>
		<title level="m" coord="21,154.68,635.17,304.79,7.77;21,154.68,646.13,168.07,7.77">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT 2018</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT 2018</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,154.69,119.96,323.60,7.77;22,154.68,130.92,307.98,7.77;22,154.68,141.88,249.96,7.77" xml:id="b40">
	<analytic>
		<title level="a" type="main" coord="22,238.11,119.96,240.17,7.77;22,154.68,130.92,273.36,7.77">SentArg: A Hybrid Doc2Vec/DPH Model with Sentiment Analysis Refinement-Notebook for the Touché Lab on Argument Retrieval at CLEF</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Staudte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,154.68,141.88,208.12,7.77">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<imprint>
			<date type="published" when="2020-09">2020. Sep 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,154.69,152.84,298.54,7.77;22,154.68,163.80,324.91,7.77;22,154.68,174.76,271.97,7.77" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="22,329.29,152.84,123.93,7.77;22,154.68,163.80,24.56,7.77">CWS: A Comparative Web Search System</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/1135777.1135846</idno>
		<ptr target="https://doi.org/10.1145/1135777.1135846" />
	</analytic>
	<monogr>
		<title level="m" coord="22,197.78,163.80,281.82,7.77;22,154.68,174.76,16.14,7.77">Proceedings of the 15th International Conference on World Wide Web, WWW 2006</title>
		<meeting>the 15th International Conference on World Wide Web, WWW 2006</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="467" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,154.69,185.71,299.46,7.77;22,154.68,196.67,303.66,7.77;22,154.68,207.63,294.28,7.77;22,154.68,218.59,216.93,7.77;22,154.68,229.55,135.35,7.77" xml:id="b42">
	<analytic>
		<title level="a" type="main" coord="22,154.68,196.67,200.84,7.77">Argumentation Quality Assessment: Theory vs. Practice</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Naderi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2039</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-2039" />
	</analytic>
	<monogr>
		<title level="m" coord="22,373.43,196.67,84.92,7.77;22,154.68,207.63,276.79,7.77">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="250" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,154.69,240.51,307.18,7.77;22,154.68,251.47,325.91,7.77;22,154.68,262.43,310.36,7.77;22,154.68,273.39,316.53,7.77;22,154.68,284.34,258.64,7.77;22,154.68,295.30,134.34,7.77" xml:id="b43">
	<analytic>
		<title level="a" type="main" coord="22,190.42,251.47,245.37,7.77">Computational argumentation quality assessment in natural language</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Naderi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bilu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Thijm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/e17-1017</idno>
		<ptr target="https://doi.org/10.18653/v1/e17-1017" />
	</analytic>
	<monogr>
		<title level="m" coord="22,277.05,262.43,188.00,7.77;22,154.68,273.39,233.75,7.77">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Koller</surname></persName>
		</editor>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="176" to="187" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="22,154.69,306.26,318.29,7.77;22,154.68,317.22,325.86,7.77;22,154.68,328.18,316.79,7.77;22,154.68,339.14,207.97,7.77;22,154.68,350.10,136.84,7.77" xml:id="b44">
	<analytic>
		<title level="a" type="main" coord="22,286.93,317.22,176.79,7.77">Building an Argument Search Engine for the Web</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Khatib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puschmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dorsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-5106</idno>
		<ptr target="https://doi.org/10.18653/v1/w17-5106" />
	</analytic>
	<monogr>
		<title level="m" coord="22,154.68,328.18,299.31,7.77">Proceedings of the 4th Workshop on Argument Mining, ArgMining@EMNLP 2017</title>
		<meeting>the 4th Workshop on Argument Mining, ArgMining@EMNLP 2017</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="49" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,154.69,361.06,283.50,7.77;22,154.68,372.02,298.81,7.77;22,154.68,382.97,316.46,7.77;22,154.68,393.93,204.83,7.77" xml:id="b45">
	<analytic>
		<title level="a" type="main" coord="22,294.28,361.06,127.79,7.77">PageRank&quot; for Argument Relevance</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/e17-1105</idno>
		<ptr target="https://doi.org/10.18653/v1/e17-1105" />
	</analytic>
	<monogr>
		<title level="m" coord="22,154.68,372.02,298.81,7.77;22,154.68,382.97,141.32,7.77">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1117" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,154.69,404.89,317.96,7.77;22,154.68,415.85,306.11,7.77;22,154.68,426.81,302.02,7.77;22,154.68,437.77,254.34,7.77" xml:id="b46">
	<analytic>
		<title level="a" type="main" coord="22,284.48,404.89,188.17,7.77;22,154.68,415.85,61.46,7.77">Retrieval of the Best Counterargument without Prior Topic Knowledge</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P18-1023/" />
	</analytic>
	<monogr>
		<title level="m" coord="22,234.92,415.85,225.87,7.77;22,154.68,426.81,135.84,7.77">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="241" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,154.69,448.73,320.21,7.77;22,154.68,459.69,308.49,7.77;22,154.68,470.65,319.18,7.77;22,154.68,481.60,193.51,7.77" xml:id="b47">
	<analytic>
		<title level="a" type="main" coord="22,240.53,448.73,234.36,7.77;22,154.68,459.69,106.65,7.77">A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<idno type="DOI">10.1145/383952.384019</idno>
		<ptr target="https://doi.org/10.1145/383952.384019" />
	</analytic>
	<monogr>
		<title level="m" coord="22,279.25,459.69,183.93,7.77;22,154.68,470.65,266.40,7.77">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
