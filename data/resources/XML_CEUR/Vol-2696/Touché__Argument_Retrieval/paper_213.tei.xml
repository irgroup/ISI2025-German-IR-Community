<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.56,115.90,294.24,12.90;1,272.27,133.83,70.81,12.90;1,216.89,153.68,181.57,10.75">Question Answering for Comparative Questions with GPT-2 Notebook for Touché at CLEF 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,276.45,190.08,57.98,8.64"><forename type="first">Bjarne</forename><surname>Sievers</surname></persName>
							<email>b.sievers@studserv.uni-leipzig.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Leipzig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.56,115.90,294.24,12.90;1,272.27,133.83,70.81,12.90;1,216.89,153.68,181.57,10.75">Question Answering for Comparative Questions with GPT-2 Notebook for Touché at CLEF 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0C692DFE8C220BC275EA5342D139D5AF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Finding the best answer for comparative questions is a difficult problem in the field of Information Retrieval. Since the best answer ideally covers not only both subjects of the query, but puts them into relation with each other, keyword based search alone has difficulties understanding the users intention correctly. Language models like GPT-2 on the other hand can distinguish fine nuances of intention in a query or sentence and generating a text conditioned on that phrase. We try to leverage this by generating substitute answer articles conditioned on the given query. Search results are then re-ranked by their textual similarity to the generated substitute answer articles.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, search engines are not only used for looking up pure facts, but also for opinions, discussions and arguments. Users now turn to search engines with questions like "Which is healthiest: coffee, green tea or black tea and why?" or "Do you prefer tampons or pads?". In these cases, the user is likely more interested in the arguments or personal testimonies than in the final answer. Platforms like Quora often list the exact question and several answers to it, but depending on the answer, little context might be given. Discussion forums or comments on private blogs might contain more interesting debates and replies.</p><p>However, search engines still struggle with this type of questions. They usually either return websites that list facts for parts of the query or websites where the exact same question was posted. If a question was phrased in a different way, most search engines have trouble returning good results.</p><p>Language models like GPT-2 on the other hand have proven to precisely capture the semantics of a given prompt. Conditioned on such a prompt, they can generate almost arbitrary lengths of coherent text while understanding what the prompt is about. This has been demonstrated to be useful for summarization, translation and question answering <ref type="bibr" coords="1,173.36,592.51,17.15,8.64" target="#b12">[13]</ref>.</p><p>The second task of the "1st Shared Task on Argument Retrieval" in Touché@CLEF 2020 <ref type="bibr" coords="1,156.58,616.42,11.62,8.64" target="#b3">[4]</ref> is about returning the best results for comparative questions from the ClueWeb12</p><p>Corpus <ref type="bibr" coords="2,166.67,119.31,10.58,8.64" target="#b0">[1]</ref>. For this task, we propose using a language model like GPT-2 to generate the hypothetically perfect answer to a question. We can then compare this answer with actual search results from a classical search engine. Re-ranking them by their similarity with the generated answer, we produce search results that are more useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Argument Retrieval</head><p>Argument Retrieval is a part of Information Retrieval that is focused on providing the best arguments, either for debates of for comparative questions. Most research to date is concerned with mining arguments from the web, identifying argument units <ref type="bibr" coords="2,468.97,253.19,11.62,8.64" target="#b5">[6]</ref> and making them searchable <ref type="bibr" coords="2,252.79,265.15,15.27,8.64" target="#b17">[18]</ref>. The Comparative Argumentative Machine <ref type="bibr" coords="2,447.22,265.15,16.60,8.64" target="#b14">[15]</ref> can search the web for comparisons between two entities and summarize the tendency of online publications towards either of them in percent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning</head><p>Since the success of Deep Learning with Convolutional Neural Networks <ref type="bibr" coords="2,437.88,337.79,16.60,8.64" target="#b9">[10]</ref> at the ImageNet competitions, Deep Learning has been used very successfully in many areas. In the field of speech processing, Recurrent Neural Networks (RNNS) are used, for example LSTMs <ref type="bibr" coords="2,194.39,373.66,10.58,8.64" target="#b8">[9]</ref>. Long dependencies that exist in natural language can be a challenge here: words can refer to past phrases or sentences that are even further back in the sequence. To counter this problem, approaches are often used that can dynamically decide on which previous tokens they focus their attention <ref type="bibr" coords="2,344.25,409.52,15.27,8.64" target="#b16">[17]</ref>. These transformers combine CNNs with a modification of attention, the so-called self-attention. Transformers thus form the basis of almost all current generative language models, for example GPT-2 <ref type="bibr" coords="2,134.77,445.39,16.60,8.64" target="#b12">[13]</ref> and T-NLG <ref type="bibr" coords="2,202.18,445.39,10.58,8.64" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Text Similarity</head><p>There are different approaches to capture the similarity between texts. <ref type="bibr" coords="2,422.03,494.13,11.62,8.64" target="#b7">[8]</ref> divides the similarity metrics into three main categories: Similarity at the character or word level, similarity by comparing with large amounts of text, and semantic similarity calculated by semantic word graphs. <ref type="bibr" coords="2,237.21,529.99,16.60,8.64" target="#b13">[14]</ref> is a Neuronal Network pretrained on NSLI <ref type="bibr" coords="2,429.57,529.99,11.62,8.64" target="#b4">[5]</ref> and Mul-tiSLI <ref type="bibr" coords="2,157.67,541.95,16.60,8.64" target="#b18">[19]</ref> on the basis of BERT <ref type="bibr" coords="2,263.99,541.95,10.58,8.64" target="#b6">[7]</ref>. It generates semantic vectors at sentence level that can be compared to each other, for example, via their cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>The architecture used in this approach consists of three steps. First, multiple queries are generated from the given query by replacing words with synonyms, one by one. Second, results for all queries are queried with ChatNoir <ref type="bibr" coords="2,367.79,632.53,11.62,8.64" target="#b2">[3]</ref> and accumulated. Third, the retrieved results are re-ranked by their similarity with GPT-2 generated texts conditioned on the original query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language Model</head><p>In order to best satisfy the information deficit of the users of our system, we have to provide an answer that is precisely tailored to the question. Lets suppose we understand exactly what the information deficit is and we have the information and knowledge to give the answer directly. And even if we do not have this knowledge, we would probably use very similar words and expressions in our answer, like a document representing the perfect answer. A system which formulates an answer for a given search query which at first glance does not differ significantly from a human answer, thus offers the possibility to reduce the problem finding the best answer to a classical information retrieval problem. The interpretation of which aspect of the question was meant by the user and what answer style they are expecting is already decided by this system. The generated mock document can now be compared with the corpus (here CW12 <ref type="bibr" coords="3,446.95,476.12,15.35,8.64" target="#b0">[1]</ref>) by classical search engines and existing documents can be used as an answer.</p><p>Such systems have become conceivable due to the success of Deep Learning (DL) in Natural Language Processing (NLP). GPT-2 <ref type="bibr" coords="3,324.98,518.46,16.60,8.64" target="#b12">[13]</ref> from OpenAI or T-NLG from Microsoft prove that DL systems are already able to distinguish fine nuances of meaning even in short text snippets and to generate and summarize suitable texts or even answer questions. Our approach uses the GPT-2 model to generate conditioned query mock responses on the query. These generated texts are then used to find matching, similar documents in the corpus. We show that by using suitable suffixes on the question, certain answer formats can be favored and thus influence the results.</p><p>In order to find personal opinions, field reports, subjective impressions of the question, one can, for example, add generic sentences to the question that one would read in forums or question-answer platforms. If, on the other hand, you are more interested in a scientific approach to the text or in journalistic texts, you can use appropriate phrases. A corresponding overview of tested suffixes can be found in 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation 1</head><p>Before querying ChatNoir with the topic, each query is duplicated and adapted several times. This is done by removing all stopwords from the query and generating n queries, where n is the number of remaining words. In each query, one of the words is now substituted by its closest neighbor in the GloVe-graph <ref type="bibr" coords="4,335.10,175.78,18.15,8.64" target="#b11">[12]</ref> that has a Levenshtein distance greater than three. The Levenshtein distance is used in order to retrieve words that are neither singular or plural forms nor spelling mistakes of the original word. ChatNoir is now queried with each query and the results are downloaded and accumulated.</p><p>For each original query, the system uses the GPT-2-M model to generate ten documents until a text length of greater than 700 characters has been reached. This is necessary because especially the smaller GPT-2 models sometimes break off after one sentence. The upper text length is bounded by the length of the trained context of GPT-2, which is 1024 tokens including the tokens of the prompt (here: the query). After weighing up runtime and performance, GPT-2-M with 345 million parameters is always used in this thesis. The generated documents are then compared with the accumulated search results of the ChatNoir queries. This similarity factor is later used as a score for the final ranking of the results. As possible similarity metrics, the cosine similarity on the tf-idf vectors as well as on S-Bert embeddings (bert-base-nli-stsb-mean-tokens) were evaluated (see chapter 3.4), the final run used tf-idf vectors due to the limited capability of the Tira submission system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word based approach</head><p>Another approach for using language models to assess the relevance of documents is based on the internal understanding of models of words and their contexts. GPT-2 and other models use large transformers <ref type="bibr" coords="4,282.89,430.31,16.60,8.64" target="#b16">[17]</ref> to assign a probability to each word in their vocabulary for being the next token in the sequence. If we now condition GPT-2 on the search query, we can compare the first word of our given document with the probability predicted by GPT-2 for that word. Now we condition GPT-2 on the search query with the first word of the document as the suffix and compare the probability with the second word, and so on. Assuming an appropriate normalization over the length of the text, the sum over all word probabilities could reflect the relevance of the document, since GPT-2 can remember the meaning of the search query even over longer passages <ref type="bibr" coords="4,433.15,514.00,15.27,8.64" target="#b12">[13]</ref>. GPT-2 thus assigns low probabilities to non-topic words that it does not expect, conditioned on the search query. This is observed when GPT-2 searches are made and documents coming from a different direction are evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation</head><p>In order to evaluate whether the generated texts fit the question at all, a pairwise comparison was done between each of them. The similarity of the pairs was computed by the cosine similarity on the tf-idf vectors as well as on semantic embeddings of a language model based on BERT. We also compared the paired similarity between the generated texts between different topics. Finally, we considered the similarity of the generated texts to the first ten search results per topic.</p><p>In order to get the best texts regardless of the matching topic, different types of conditioning were tested. Based on the single question as well as on the question with pre-and suffix, texts were generated and compared qualitatively and quantitatively.</p><p>Text Generation In a manual, qualitative evaluation of text quality, it is noticeable that almost all texts deal with the right topic over the entire length. Sometimes this is at the expense of a desirable variety in the text, repetitions of the same statement are frequent. Sometimes the model ends in an infinite loop that always repeats the same phrases alternately or continuously. Furthermore, some texts do not go beyond the length of a few sentences. Sample texts can be found in Appendix 5.1. When comparing texts which were generated by using different suffixes, a difference in writing style becomes obvious. Texts conditioned of suffixes containing "I" usually generate answers written in first-person perspective or personal testimonies. A quantitative analysis that corroborates this observation can be found in Figure <ref type="figure" coords="5,446.51,424.91,7.93,8.64">2a</ref>.</p><p>One can see that without adding a suffix to the query (No. 1 &amp; 2), the perspective of the query correlates with whether the answer is given in first-person perspective or not. When using a suffix, this correlation diminishes. Moreover, it can be suspected that more objective suffixes push the generation in a less personal direction (No. 4 &amp; 5). Since a complete manual check of the fit of all texts to the prompt is out of scope due to both by subjectivity and effort, the generated texts were compared with each other and with search results of the same query and other topics by means of cosine similarity (see Figure <ref type="figure" coords="6,225.28,155.18,7.89,8.64">3b</ref>). In each row (generated texts) and each column (ChatNoir documents), the highest value is found in the diagonal, i.e. in the matching topic.</p><p>If you examine the generated texts using sentence embeddings generated by <ref type="bibr" coords="6,461.50,179.40,15.27,8.64" target="#b13">[14]</ref>, you can see their semantic similarity to the search queries of the same topic as well as to the first documents of ChatNoir. In Figure <ref type="figure" coords="6,330.19,203.31,9.40,8.64">3a</ref> you can see the embeddings of all sentences for five topics, while the crosses mark the respective search query phrase embedding.</p><p>In a further comparison, Figure <ref type="figure" coords="6,282.86,239.48,4.98,8.64">4</ref> shows sentence embeddings of the documents together with the texts for five topics. The dark colors represent the sentences of the generated texts, the lighter colors represent the documents. One can see in both figures that almost every sentence in the generated texts has a semantic reference to the question. Figure <ref type="figure" coords="6,162.44,481.03,3.88,8.64">3</ref>: Left: S-Bert embeddings of all sentences of the generated documents, reduced in dimensionality by UMAP <ref type="bibr" coords="6,241.71,492.99,21.28,8.64" target="#b10">[11]</ref>. Embeddings of the queries are marked with a cross. Right: The average of the pairwise cosine-similarity of sentence-embeddings between the search result documents and the generated texts. First ten topics only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-2 Bias</head><p>A major problem with this approach, which has not yet been sufficiently evaluated, is the bias present in language models. Since these are usually trained on freely available texts on the Internet, they "learn" to adopt various implicit and explicit discriminations. If the output of such systems is now used to understand search queries, this bias is automatically passed on and ensures that the results are influenced. For example, if answers to the question "Do I clean tiles better with a mop or vacuum cleaner" are generated and all answers and recommendations come from women, existing stereotypes are consolidated. Furthermore, new perspectives on a topic may be What is the difference between sex and love? Which is better, laptop or desktop? Which is better, Canon or Nikon? What are the best dish detergents? What are the best cities to live?</p><p>Figure <ref type="figure" coords="7,162.44,245.20,3.88,8.64">4</ref>: S-Bert embeddings of all sentences of the generated texts in light colors and search result documents in dark colors, reduced in dimensionality by UMAP <ref type="bibr" coords="7,434.18,257.16,21.28,8.64" target="#b10">[11]</ref>. Embeddings of the queries are marked with a cross.</p><p>structurally prevented if these biased texts are seen as ideal for the documents to be delivered. Other perspectives are thus likely to find it harder to be heard. A separate evaluation of the biases of this model is outside the scope of this paper, but since an unchanged, pre-trained GPT-2 model was used, relevant literature can be found in <ref type="bibr" coords="7,461.50,341.32,15.27,8.64" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Models like GPT-2 already understand the intention of the user sufficiently well to generate suitable results. But using this knowledge to create a good ranking is not trivial. The approaches presented here show the basic possibility, but do not deliver the hopedfor results. While a semantic similarity to the research question can be recognized and thus documents with similar vocabulary can be sorted out, the ranking based on this similarity does not achieve good sorting. To recognize good arguments, other approaches seem to make more sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Examples of generated text</head><p>What is the difference between love and sex? I would love to have a conversation about it.</p><p>Is love a natural and universal emotion? Or is it something that we develop and learn over time?</p><p>Is it a part of the human condition? Or is it something that we can develop and learn about through practice?</p><p>Is it something that we are born with? Or does it come through our parents? I am curious about the nature of love and if it is a part of the human condition. My question is a matter of life and death and that is really what it boils down to. I think all of us will be alive in our lifetimes without knowing it. We will all be alive and we will all be affected by it and have it shape us for the rest of our lives. It may not be something we understand, but we all have it.</p><p>I am wondering if love is a natural emotion, or if it is something that we develop and learn through practice?</p><p>What is the difference between sex and love? Love, like sex, is an experience and an expression of our own desires.</p><p>Is it something that we develop and learn over time? Yes. I think the answer is that there is a huge range of sexual experiences that can be experienced. It really depends on your level of experience and your level of experience. I think it's an experience that you are capable of. Do we develop the same sense of love as we develop sex? I think that sex is much more developed in your brain than love. We develop that same kind of sexual drive, the same sexual experience, that is necessary for our capacity to become human beings. That same level of experience and that same level of understanding is necessary for our capacity to have that same kind of love for one another.</p><p>Is it really necessary to be a human being, to be a loving and affectionate human being, if we are not able to have sex?</p><p>If we were born with a certain kind of sex drive, would that affect our capacity to be a good human being? If we were born with a certain kind of sexual drive, would that affect our capacity to have that same kind of love for one another?</p><p>What is the difference between sex and love? I think sex is a natural and universal emotion. Are there certain things that we need to learn or experience before we can know whether we are a loving and affectionate human being?</p><p>What is better, laptop or desktop? What's better? A desktop or a laptop? What's better, laptop or desktop? I am really wondering what the answer is. Posted on Oct 18th 2017, 12:46 Reply #21 TheLionKnight "Killer4Life said: If I had to choose, I'd choose the desk, since I don't want to have to work with a mouse or keyboard in an environment where I could lose track of which direction to turn the mouse. The only reason I could even think of that desk is that the desk has a screen which looks really cool.</p><p>If there's no way to put a mouse on it, I might have to say no to my mouse. It's the only reason I'm choosing a desk. The only reason I could even think of that desk is that the desk has a screen which looks really cool.If there's no way to put a mouse on it, I might have to say no to my mouse. It's the only reason I'm choosing a desk. Posted on Oct 18th 2017, 13:13 Reply 22 kyren "TheLionKnight said: If I had to choose, I'd choose the desk, since I don't want to have to work with a mouse or keyboard in an environment where I could lose track of which direction to turn the mouse. The only reason I could even think of that desk is that the desk has a screen which looks really cool.</p><p>If there's no way to put a mouse on it, I might have to say no to my mouse. It's the only reason I'm choosing a desk. Yeah, it's a good one, considering how the screen is so small, so small that you could be standing there while wearing it. Also, yes, I'd go with the desk because it has a nice shape, the screen is awesome and the monitor is great.</p><p>I don't like how the mouse buttons are made and the size is also small. You can always make it bigger, though. I think the best way is to just buy the mouse, which is much cheaper, and make a stand so that you can sit on it, because it is so small. Also, the only other thing I would change is the button layout on the side so that it isn't all vertical.</p><p>"TheLionKnight said: Yeah, it's a good one, considering how the screen is so Which is better, Canon or Nikon? I have only used Canon lenses since I got my first camera back in 2004 and I never really cared about buying Nikon. I am using the D800E to try the new EOS 10D (and have shot some of the best shots with it so far) so I figured I would get some opinions on the differences.</p><p>I have used both cameras and I have not found any differences in my experiences. So I will start with the Canon EOS 30D and see if I can find any differences. Canon EOS 30D My first shot Canon EOS 30D at night Canon EOS 30D at sunrise Canon EOS 30D at sunset Canon EOS 30D at sunset with a flash Canon EOS 30D at night with a flash and with a tripod Canon EOS 30D at night with a flash and with a tripod with tripod and a flash Canon EOS 30D at sunrise with a flash and with a tripod Canon EOS 30D at sunset with a flash and with a tripod Canon EOS 30D at sunrise with a flash and with a tripod and a flash Canon EOS 30D at sunset with a flash and with a tripod and a flash Canon EOS 30D at sunrise with a flash and with a tripod and a flash Canon EOS 30D at sunset with a flash and with a flash Canon EOS 30D at sunrise with a flash and with a tripod and a flash Canon EOS 30D at sunset with a flash and with a flash Canon EOS 30D at sunrise with a flash and with a flash Canon EOS 30D at sunset with a flash and with a flash Canon EOS 30D at sunrise with a flash and with a flash Canon EOS 30D at sunset with a flash and with a flash Canon EOS 30D at sunrise with a flash and with a flash Canon EOS 30D at sunset with a flash and with a flash Canon EOS 30D at sunrise with a flash and with a flash Canon EOS 30D at sunset with a flash and with a flash Canon EOS 30D at sunrise with a flash and with a flash Canon EOS 30D a</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,210.84,255.09,193.67,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,216.25,460.06,9.95,7.77;6,349.08,329.86,3.71,9.70;6,367.84,329.86,3.71,9.70;6,386.62,329.86,3.71,9.70;6,405.38,329.86,24.46,9.70;6,327.21,353.60,3.71,9.70;6,327.21,372.28,3.71,9.70;6,327.21,391.14,3.71,9.70;6,327.21,409.92,3.71,9.70;6,323.64,428.68,7.46,9.70;6,450.72,433.39,9.46,9.70;6,450.72,414.53,9.46,9.70;6,450.72,395.85,9.46,9.70;6,450.72,377.06,9.46,9.70;6,450.72,358.31,9.46,9.70;6,450.72,339.44,9.46,9.70;6,309.17,321.96,170.03,9.70;6,312.28,422.91,9.70,19.95;6,312.28,382.06,9.70,38.26;6,312.28,368.17,9.70,11.42;6,312.28,359.80,9.70,5.90;6,312.28,338.68,9.70,18.59;6,334.34,441.00,99.95,9.70;6,388.91,460.06,10.45,7.77"><head></head><label></label><figDesc>Texts vs. Query Documents Query Documents, No. of Topic Generated Texts, No. of Topic (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,506.30,345.82,155.19"><head>table 1 .</head><label>1</label><figDesc>Right: Average readability in relation to the suffix used, see table 1.</figDesc><table coords="5,134.77,506.30,345.82,155.19"><row><cell>Count of 'I' in generated texts</cell><cell>1 Count of 'I' in generated texts in dependence of the suffix 2 3 4 5 Suffix index (refer to Table 1) 0.0 0.2 0.4 0.6 0.8 1.0 % 'I', if question contains 'I' % 'I', if question does no contain 'I'</cell><cell>0 10 20 30 40 50 Average readability of texts</cell><cell>1 Readability of texts in dependence of the suffix 2 3 4 Suffix index (refer to Table 1) Coleman Liau Index Gunnig Fog Index Occurence of syllable-rich words</cell><cell>5</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell></row><row><cell cols="5">Figure 2: Left: Occurence of 'I' in GPT-2-generated texts in relation to the suffix used,</cell></row><row><cell>see</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.73,657.08,262.37,7.77"><p>Code can be found on Github (github.com/bjrne/gpt-question-answering)</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.61,525.41,324.77,7.77;7,150.95,536.37,67.48,7.77" xml:id="b0">
	<monogr>
		<ptr target="https://lemurproject.org/clueweb12/index.php" />
		<title level="m" coord="7,150.96,525.41,71.83,7.77">Clueweb12 -dataset</title>
		<imprint>
			<date type="published" when="2020">3.2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,547.38,242.52,7.77;7,150.95,559.18,247.96,6.31;7,150.95,570.14,340.27,6.31;7,150.95,580.26,67.48,7.77" xml:id="b1">
	<monogr>
		<ptr target="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" />
		<title level="m" coord="7,150.96,547.38,230.51,7.77">Turing-nlg -a 17 billion parameter language model by microsoft</title>
		<imprint>
			<date type="published" when="2020-03-06">6.3.2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,591.28,325.52,7.77;7,150.95,602.23,314.93,7.77;7,150.95,613.19,92.89,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,332.43,591.28,135.71,7.77;7,150.95,602.23,112.26,7.77">Elastic chatnoir: search engine for the clueweb and the common crawl</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,281.33,602.23,167.65,7.77">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="820" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,624.21,317.60,7.77;7,150.95,635.17,315.97,7.77;7,150.95,646.13,327.78,7.77;7,150.95,657.08,302.89,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,243.63,635.17,167.58,7.77">Touché: First shared task on argument retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,434.39,646.13,44.35,7.77;7,150.95,657.08,76.27,7.77">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Castells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Martins</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="517" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,119.96,334.71,7.77;8,150.95,130.57,208.70,8.12" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,343.27,119.96,134.05,7.77;8,150.95,130.92,93.79,7.77">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>ArXiv abs/1508.05326</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,141.88,323.15,7.77;8,150.95,152.84,329.64,7.77;8,150.95,163.80,327.74,7.77;8,150.95,174.76,241.98,7.77;8,150.95,186.56,221.07,6.31" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,207.89,152.84,196.64,7.77">TARGER: Neural Argument Mining at Your Fingertips</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chernodub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Oliynyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Heidenreich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-3031" />
	</analytic>
	<monogr>
		<title level="m" coord="8,224.16,163.80,254.54,7.77;8,150.95,174.76,200.88,7.77">57th Annual Meeting of the Association for Computational Linguistics (ACL 2019). Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Costa-Jussà</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Alfonseca</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019-07">Jul 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,196.67,325.47,7.77;8,150.95,207.63,238.30,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,328.63,196.67,139.45,7.77;8,150.95,207.63,144.12,7.77">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,313.31,207.63,49.80,7.77">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,218.59,266.88,7.77" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H</forename><surname>Gomaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fahmy</surname></persName>
		</author>
		<title level="m" coord="8,246.53,218.59,136.81,7.77">A survey of text similarity approaches</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,229.55,329.11,7.77;8,150.95,240.16,292.22,8.12;8,150.95,251.47,66.49,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,416.93,229.55,54.78,7.77;8,150.95,240.51,48.77,7.77">Lstm: A search space odyssey</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,205.77,240.51,223.96,7.77">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,262.43,336.90,7.77;8,150.95,273.39,119.69,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,308.34,262.43,170.80,7.77;8,150.95,273.39,55.78,7.77">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,225.07,273.39,19.43,7.77">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,284.34,305.72,7.77;8,150.95,294.95,187.06,8.12" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="8,234.79,284.34,213.18,7.77;8,150.95,295.30,72.10,7.77">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<idno>ArXiv abs/1802.03426</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,306.26,331.86,7.77;8,150.95,317.22,323.50,7.77;8,150.95,329.02,215.69,6.31" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,306.76,306.26,163.80,7.77">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m" coord="8,163.16,317.22,222.04,7.77">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,339.14,328.46,7.77;8,150.95,350.10,139.71,7.77" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="8,394.01,339.14,76.69,7.77;8,150.95,350.10,113.56,7.77">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,361.06,290.13,7.77;8,150.95,372.02,305.46,7.77;8,150.95,382.97,307.78,7.77;8,150.95,393.93,277.55,7.77;8,150.95,405.74,204.93,6.31" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,248.09,361.06,184.28,7.77;8,150.95,372.02,48.24,7.77">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1410</idno>
		<ptr target="http://dx.doi.org/10.18653/v1/d19-1410" />
	</analytic>
	<monogr>
		<title level="m" coord="8,205.15,372.02,251.27,7.77;8,150.95,382.97,307.78,7.77;8,150.95,393.93,109.83,7.77">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,415.85,326.85,7.77;8,150.95,426.46,306.97,8.12;8,150.95,437.77,195.16,7.77" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="8,150.95,426.81,222.03,7.77">Answering comparative questions: Better than ten-blue-links?</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schildwächter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zenker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<idno>CoRR abs/1901.05041</idno>
		<ptr target="http://arxiv.org/abs/1901.05041" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,448.73,322.89,7.77;8,150.95,459.69,80.43,7.77" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Celis</surname></persName>
		</author>
		<title level="m" coord="8,232.01,448.73,233.12,7.77;8,150.95,459.69,54.28,7.77">Assessing social and intersectional biases in contextualized word representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,470.65,321.33,7.77;8,150.95,481.60,205.23,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,205.76,481.60,86.57,7.77">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,310.61,481.60,19.43,7.77">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,492.56,327.01,7.77;8,150.95,503.52,318.74,7.77;8,150.95,514.48,104.98,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,283.20,503.52,170.08,7.77">Building an argument search engine for the web</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Khatib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puschmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dorsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,150.95,514.48,78.83,7.77">ArgMining@EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,525.44,332.08,7.77;8,150.95,536.05,230.13,8.12" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="8,301.55,525.44,172.76,7.77;8,150.95,536.40,115.22,7.77">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>ArXiv abs/1704.05426</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
