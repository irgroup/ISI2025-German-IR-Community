<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.25,136.92,344.86,10.75">Notebook for the Touché Lab on Argument Retrieval at CLEF 2020</title>
				<funder ref="#_HKqnC6w">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,228.82,174.49,89.93,8.64"><forename type="first">Mahsa</forename><forename type="middle">S</forename><surname>Shahshahani</surname></persName>
							<email>m.shahshahani@uva.nl</email>
						</author>
						<author>
							<persName coords="1,338.12,174.49,48.42,8.64"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<email>kamps@uva.nl</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam at CLEF</orgName>
								<address>
									<postCode>2020</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.25,136.92,344.86,10.75">Notebook for the Touché Lab on Argument Retrieval at CLEF 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9D034CD0901C54112367074984B306A7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper documents the University of Amsterdam's participation in CLEF 2020 Touché Track. This is the first year this track has been introduced at CLEF, and we were attracted to participate in it due to its potentialities for Parliamentary debates we are currently working on. This track consists of two tasks: Conversational Argument Retrieval and Comparative Argument Retrieval. We submitted a run to both tasks. For the first task, we used a combination of the traditional BM25 model and learning to rank models. BM25 model helps to retrieve relevant arguments, and learning to rank model helps to re-rank the list and put stronger arguments on top of the list. For the second task, Comparative Argument Retrieval, we proposed a pipeline to re-rank documents retrieved from Clueweb using three features: PageRank scores, web domains, and argumentativeness. Preliminary results on 5 queries have shown that this heuristic pipeline may help to achieve a balance among three important dimensions: relevance, trustworthiness, and argumentativeness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We believe that we passed the era in which search engines were supposed to only give us a ranked list of documents or answers and they have more potentialities to help us in decision making process. Argument retrieval task has been defined to formulate this problem. Touché track at CLEF <ref type="bibr" coords="1,268.13,503.13,11.62,8.64" target="#b2">[3]</ref> offers an opportunity to work on this interesting problem having access to a debate corpus from two different points of view: Looking for different views about a problem in debates between opponents and supporters of a controversial issue, and looking for comparative opinions about different alternative. This track consists of a different task (two tasks in total) for each point of view and we submitted a run to both tasks.</p><p>In this paper, we cover both tasks; first, we give a high-level summary of the first task, followed by our detailed approach. Then, we cover the same for the second task. Finally, we will conclude the paper by mentioning our main contributions and findings.</p><p>We will add results section whenever the results would be out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Conversational Argument Retrieval</head><p>For detailed information about CLEF track's experimental setup, we refer to the overview paper <ref type="bibr" coords="2,160.07,152.38,11.62,8.64" target="#b3">[4]</ref> and to the track homepage. <ref type="foot" coords="2,285.24,150.71,3.49,6.05" target="#foot_0">1</ref> However, we provide a high-level summary to make this paper self-contained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task definition</head><p>The goal of this task is to retrieve relevant arguments from online debate portals, given a query on a controversial topic.</p><p>Corpus Args.me <ref type="bibr" coords="2,206.34,241.62,11.62,8.64" target="#b0">[1]</ref> has been created by crawling arguments from 7 debate websites. It includes 387,606 arguments taken from 59,637 debates. A search engine based on Elasticsearch has been set to make it easier to work with this corpus <ref type="bibr" coords="2,412.60,265.53,15.27,8.64" target="#b9">[10]</ref>. This search engine ranks arguments using BM25 ranking algorithm. Different approaches can be used later to re-rank these retrieved arguments.</p><p>Queries 50 controversial topics have been picked for this task. Each topic has both pro and con arguments in the corpus.</p><p>Quality Assessment Proposed approaches are supposed to retrieve "strong" arguments. An argument is consiagstuhl ered strong if it is topically relevant, logically cogent, rhetorically well-written, and useful to help in stance-building process. Here, we define these assessment dimensions taken from <ref type="bibr" coords="2,296.08,379.48,11.62,8.64" target="#b8">[9]</ref> and the annotation guidelines for Dagstuhl-15512 ArgQuality Corpus. <ref type="foot" coords="2,241.13,389.77,3.49,6.05" target="#foot_1">2</ref>Topical relevance: As every other ranking task, retrieved arguments should provide the user with relevant information about the query. Besides relevance, in general, there are three main dimensions for assessing the quality of arguments: logic, rhetoric, and dialectic.</p><p>Logical cogency: An argument with acceptable premises that are relevant and sufficient to the argument's conclusion is considered "cogent" <ref type="bibr" coords="2,344.60,481.48,10.58,8.64" target="#b6">[7]</ref>.</p><p>Rhetorical well-writtenness: An argument is called "rhetorically well-written" if it is effective and successful in persuading a target audience of a conclusion <ref type="bibr" coords="2,421.39,514.55,10.58,8.64" target="#b1">[2]</ref>.</p><p>Dialectic: An argument is considered reasonable if it contributes in the users' stancebuilding process regards a given issue in a way that is acceptable to everyone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Our Approach</head><p>We treated this task as a re-ranking problem. In the absence of training data, we have to use unsupervised approaches. However, we used an existing debate dataset created for studying argument quality assessment to train a classifier. First, we describe this corpus. Later, we explain our approach in three consecutive steps.</p><p>Corpus Dagstuhl-15512 ArgQuality <ref type="bibr" coords="3,283.98,119.31,11.62,8.64" target="#b8">[9]</ref> includes 20 arguments for each of 16 queries. Three annotators have annotated these 320 arguments on 15 dimensions with three labels. However, we only use four dimensions: cogency, effectiveness, reasonableness, and overall quality. The set of labels includes ordinal scores from 1 (low) to 3 (high) for all dimensions. We used majority voting technique to get the label for each dimension, and substituted the ones labeled as 3 with 2 as there are a very few samples with label "3" in the corpus .</p><p>Approach We created the final ranked list in three steps. Before explaining each step, we explain the way we represented arguments.</p><p>Argument Representation: We used pre-trained BERT-base <ref type="bibr" coords="3,379.04,256.10,11.62,8.64" target="#b5">[6]</ref> model from Hugging-Face Transformers framework in python to represent arguments. As BERT model imposes a limit on the length of documents after tokenization, we used the first 512 tokens in an argument if its length exceeds this limit.</p><p>First step-Ranking: BM25 is the traditional unsupervised ranking model which scores the relevancy of documents (here arguments) in regards to queries based on the frequency of common terms between the query and argument. We ranked arguments for each topic based on BM25 using args.me search engine.</p><p>Second step-Classification: We trained a classifier on Dagstuhl-15512 ArgQuality corpus to recognize and label cogency, well-writtenness, reasonableness and overall quality of each argument. Later, we applied this classifier to all retrieved arguments in the ranked list from the fist step.</p><p>We got the majority voting for each dimension; in all of the arguments the majority for all four dimensions are the same. It is aligned with the conclusion in the main paper <ref type="bibr" coords="3,134.77,446.69,11.62,8.64" target="#b8">[9]</ref> which indicates that cogency, effectiveness, and reasonableness correlate strongly with overall quality, and also much with each other.</p><p>We trained two classifiers on 90% of data: A decision tree classifier, and an SVM classifier. The accuracy on the remaining 10% of data has been shown in Table <ref type="table" coords="3,457.15,482.56,3.74,8.64" target="#tab_0">1</ref>. We used scikit-learn framework for python 3 to train both classifiers. For decision tree, we used "gini" criterion, and set minimum required samples to split to 2. For SVM, we used "rbf" kernel, and set regularization parameter to 1. We selected SVM classifier for the further step.</p><p>Third step-Re-ranking: In the third step, we re-ranked retrieved arguments from the first step using learning-to-rank models. We use the output of step 2 as a feature in step 3 to re-rank the arguments. We trained three different learning to rank models: Ranknet, RandomForests, and LambdaRank. We used RankLib 3 library with default sets of parameters to apply these models. In order to train learning to rank models, we used argument representations based on BERT model, the output of the second step, and two additional features based on named entities. We defined two binary features indicating the presence of numerical named entities (percent, quantity, money) and other entities (person, location, organization) in the argument. We showed the number of arguments with and without these entities in Figure <ref type="figure" coords="4,338.06,131.27,4.98,8.64">1</ref> and Figure <ref type="figure" coords="4,392.44,131.27,3.74,8.64">2</ref>. These figures show the difference in the distribution of each of these features in strong (label=2) and weak (label=1) arguments. Arguments using these kinds of entities are more likely providing users with persuasive and effective information to make their stance, and this can lead to a more probability to be labeled as "strong".</p><p>We trained learning to rank models on Dagstuhl dataset, and applied the best model (Ranknet) to re-rank retrieved arguments from args.me dataset for all 50 topics in the shared task. We trained models on 90% of data and reported accuracy on the remaining 10% of data in Table <ref type="table" coords="4,220.02,226.91,3.74,8.64" target="#tab_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results</head><p>In the final relevance judgments, 30 documents for each topic have been annotated using 6 labels: -2,1,2,3,4 and 5.</p><p>Classifier We ran our classifier that was trained on Dagstuhl-15512 ArgQuality corpus on arguments from judgments to see if the results are correlated with the final judgment. Unfortunately, we observed that the classifier does not work well and return label '1' for more than 90% of the judged arguments. This suggests that the classifier does not play a role in the final results of our learning to rank model. This is not surprising as the dataset we trained our SVM classifier on is very different from the test set. The arguments in the training set are very short and consist of only one to three sentences, while we are labeling a set of complete documents in the test set. However, as the BERT representation has a limit on the size of the input text, for the longer texts we only used the first 512 tokens of each argument. Using more advanced approaches such as averaging over sliding windows of 512 tokens might make the classifier useful.</p><p>Entities Similar to Figure <ref type="figure" coords="5,245.44,529.25,4.98,8.64">1</ref> and Figure <ref type="figure" coords="5,299.93,529.25,3.74,8.64">2</ref>, we looked into the distribution of numerical and other types of entities in relevant and non-relevant documents. We considered documents with label '-2' as non-relevant, and other documents in the judgment file as relevant. Results have been shown in Figure <ref type="figure" coords="5,313.78,565.11,4.98,8.64">3</ref> and<ref type="figure" coords="5,338.52,565.11,3.74,8.64">4</ref>. As it is obvious from the figures, the distribution of entities is the opposite of what we observed in the dataset we used at the time of developing our model. However, it still can be used as a feature as it shows a little difference between relevant and non-relevant documents.</p><p>Query Length We looked into per query results to get an insight into the way our model works. We used NDCG@5 metric as it has been the main metric for the shared task. Table <ref type="table" coords="6,266.28,538.14,3.36,8.06">3</ref>. Results-NDCG@10 metric Model NDCG@1 NDCG@5 NDCG@10 MAP UvATask1LTR 0.5214 0.5548 0.3709 0.1129</p><p>As we used the whole topic (without removing stop words), the model works better for shorter queries (Figure <ref type="figure" coords="6,242.11,656.44,3.60,8.64" target="#fig_1">5</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Comparative Argument Retrieval</head><p>Similar to the previous section, for detailed information about this task's experimental setup, we refer to the overview paper <ref type="bibr" coords="7,292.51,354.75,11.62,8.64" target="#b3">[4]</ref> and to the track homepage. <ref type="foot" coords="7,419.82,353.08,3.49,6.05" target="#foot_3">4</ref> However, we provide a high-level summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task definition</head><p>The goal of this task is to retrieve and rank documents from web that help to answer a comparative question from "everyday life".</p><p>Corpus Clueweb12 is a dataset created by crawling 733,019,372 web documents seeded with 2,820,500 urls from Clueweb09 <ref type="bibr" coords="7,282.23,466.82,10.58,8.64" target="#b4">[5]</ref>. We used a publicly available search engine <ref type="bibr" coords="7,468.97,466.82,11.62,8.64" target="#b7">[8]</ref> based on Elasticsearch to retrieve documents from Clueweb12 based on BM25 ranking model.</p><p>Queries 50 comparative topics from everyday life have been picked for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our approach</head><p>We treated this task as a re-ranking problem. In the absence of training data, we have to use unsupervised approaches. We labeled retrieved documents for 5 topics to have an insight of how our heuristic approach works.</p><p>In the first step, we used ChatNoir search engine to rank documents retrieved for each topic. In the second step, we used three different features to re-rank them. Here, we introduce the features we used, followed by our heuristic approach to combine them and create the final ranked list.</p><p>Argumentativeness We trained a simple SVM classifier based on data from args.me corpus and Clueweb to distinguish between argumentative and non-argumentative documents. Similar to the first task, we used BERT-based model from HuggingFace Transformers library for Python to represent documents, and we used the first 512 tokens in a document if its length exceeds the limit of BERT model.</p><p>To train the classifier, we used a small sample from each corpus. These samples are created by submitting all 50 controversial queries in the first task to both corpora and got up to 100 documents for each query. Then, we manually removed argumentative documents from the sample taken from Clueweb and considered the remaining documents as negative examples. All retrieved documents from args.me corpus have been considered as positive examples. The final training set consists of 3000 positive and 3000 negative examples. Then, We trained a simple SVM classifier on 80% of the data, and evaluated it on the remaining 20% of documents. It achieved 87% in terms of accuracy. All parameters for the argumentativeness classifier have been set to their default values in Scikit-learn<ref type="foot" coords="8,249.24,285.31,3.49,6.05" target="#foot_4">5</ref> library for Python.</p><p>Web domains Clueweb has been formed by crawling web documents with some postfilters. But, the goal of this task is to retrieve documents including personal opinions or suggestions. Thus, documents from particular domains like Wikipedia are not desirable. On the contrary, documents from discussion forums, debate websites, and blogs can be very helpful. Having this intuition in mind, we defined a binary feature that indicates if the source URL for a discussion contains 'forum' or 'blog' terms to give a bonus to web pages from discussion forums or blogs.</p><p>PageRank Although desired documents are those from discussion forums and personal blogs, they should also be trustworthy. To take trustworthiness into account, we used page rank scores to prioritize documents taken from more reliable sources.</p><p>In ChatNoir search engine, every returned document has been associated with a PageRank score. We directly used these returned scores.</p><p>Re-ranking We introduced three features, and our goal is to re-rank documents based on a combination of these features (argumentativeness, domain addresses , and PageRank scores).</p><p>To generate the final ranked list, we make a heuristic ranking pipeline in four steps:</p><p>-The 1st step: The initial ranked list is taken from ChatNoir search engine. ChatNoir retrieves documents from Clueweb and ranks them using traditional BM25 ranking model. We use the whole topic title as the query. We also examined submitting the query after removing stop words or just using the entities in the topic title. But, using the whole topic title worked better. -The 2nd step: Page-rank scores are used to re-rank the list from the first step in descending order. This may result in putting a document, initially ranked very low, on top of the list. In order to avoid this, moving documents in the ranked list is limited to a maximum of 10 positions. -The 3rd step: Web domain are used to re-rank ranked list from the second step. To do this, the documents with positive domain feature (which means the document is taken from a blog or discussion forum) are put on top of the list. This has been performed within every 10 documents in the list. We split the list into chunks of 10 documents and we keep their relative positions. -The 4th step:All retrieved documents in the ranked list from the third step are classified using the argumentativeness classifier we have trained. Documents classified as positive are put on top of the list. We keep their relative positions. This has been performed within every chunk of 10 documents in the list.</p><p>We put this limits on moving documents in the list with this intuition in mind that relevance should be prioritized in comparison with trustworthiness and argumentativeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Since the final judgment file does not consist of separate judgment lists for three different dimensions (relevance, trustworthiness, and argumentativeness), we include our preliminary results too.</p><p>Preliminary Results: To gain an insight into the effectiveness of our heuristic model, we manually labeled 10 retrieved documents for 5 queries. We labeled documents using three labels: 0 for non-relevant, non-argumentative, or untrustworthy; 1 for relevant, argumentative, or trustworthy, and 2 for highly-relevant, highly-argumentative, or highly trustworthy.</p><p>We evaluated the top 10 documents for each ranked list: BM25, re-ranked by PageRank scores, re-ranked by web domains, re-ranked by argumentativeness, and mixed model.</p><p>Evaluation results have been reported in Table <ref type="table" coords="9,340.12,608.62,3.74,8.64">3</ref>. The heuristic mixed model does not achieve the same performance as BM25 in terms of relevance, the same performance as the argumentative classifier model in terms of argumentativeness, and the same performance as the PageRank model in terms of trustworthiness. But, it seems that it struck a balance among all three dimensions. Final Results: The final results for every step of our model have been reported in Figure <ref type="figure" coords="10,163.50,254.65,3.74,8.64" target="#fig_1">5</ref>. Since three different aspects had been defined for evaluation of this task, We expected to receive three judgment sets. However, the judgment file issued only the overall relevance, which we sacrificed for the other two dimensions.</p><p>Initially, we processed the data and implemented our models based on the three different aspects which had been defined previously. Those include relevance, trustworthiness, and argumentativeness. Given the fact that we received the overall relevance file only, we extracted our results purely based on the relevance factor for the evaluation of this task. This categorically implies that the obtained results could have been more optimized if we had performed our preliminary assessment based on one aspect only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper documents our first participation in the Touché 2020 Track. We explained our approaches for both tasks in the track: Conversational Argument Retrieval, and Comparative Argument Retrieval.</p><p>For the Conversational Argument Retrieval task, we used an existing argument quality assessment dataset to train a classifier and re-rank arguments based on the output of this classifier. We showed that named entities are important features to distinguish between strong and weak arguments on the preliminary data. But, the final results show that neither classifier nor entities do not help. However, it is worth mentioning that the classifier had been trained on a completely different set of arguments. So, if we train it on the same data from args.me, it might be useful for the final results. This needs further investigations to be proved.</p><p>For the Comparative Argument Retrieval task, we introduced three features to rerank arguments taken from Clueweb. We proposed a pipeline to combine different aspects (relevance, trustworthiness, and argumentativeness) to create the final ranked list. Preliminary results have shown that this heuristic pipeline may successfully strike a balance between all three dimensions. However, the final evaluation has been done only on relevance. This caused our method to be sub-optimal.</p><p>We hope and expect that the valuable bench-marking data created at Touché track will be of great value to motivate, and greatly facilitate, further research into argument retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,167.76,277.95,279.85,8.12;6,186.64,115.83,242.07,147.38"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. Having/lacking numerical named entities (percent, money, quantity)</figDesc><graphic coords="6,186.64,115.83,242.07,147.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,180.14,283.44,255.07,8.12;7,186.64,115.83,242.08,152.87"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Average Query Length for the different ranges of NDCG@5)</figDesc><graphic coords="7,186.64,115.83,242.08,152.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,167.76,255.63,279.85,269.40"><head>Table 1 .</head><label>1</label><figDesc>classifiers</figDesc><table coords="4,263.86,274.72,87.63,30.09"><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell cols="2">Decision Tree 0.43</cell></row><row><cell>SVM</cell><cell>0.53</cell></row></table><note coords="4,167.76,516.91,279.85,8.12"><p>Figure 1. Having/lacking numerical named entities (percent, money, quantity)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,237.35,569.33,140.66,60.13"><head>Table 2 .</head><label>2</label><figDesc>LTR</figDesc><table coords="4,237.35,588.41,140.66,41.05"><row><cell>Model</cell><cell cols="2">NDCG@1 NDCG@5</cell></row><row><cell>RankNet</cell><cell>0.873</cell><cell>0.9587</cell></row><row><cell cols="2">Random Forests 0.7333</cell><cell>0.9155</cell></row><row><cell>LambdaRank</cell><cell>0.7778</cell><cell>0.9291</cell></row></table><note coords="5,184.19,278.10,246.98,8.12"><p>Figure 2. Having/lacking other named entities (person, organization)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,174.01,115.83,267.33,97.07"><head>Table 4 .</head><label>4</label><figDesc>Preliminary Results-NDCG@10 metric</figDesc><table coords="9,174.01,139.98,267.33,72.92"><row><cell>Model</cell><cell cols="3">Relevance Argumentativeness Trustworthiness</cell></row><row><cell>initial</cell><cell>0.87</cell><cell>0.71</cell><cell>0.81</cell></row><row><cell>pagerank</cell><cell>0.89</cell><cell>0.66</cell><cell>0.87</cell></row><row><cell>domain</cell><cell>0.84</cell><cell>0.72</cell><cell>0.80</cell></row><row><cell cols="2">argumentative classifier 0.80</cell><cell>0.84</cell><cell>0.79</cell></row><row><cell>mixed</cell><cell>0.84</cell><cell>0.78</cell><cell>0.82</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,199.98,115.83,215.40,95.24"><head>Table 5 .</head><label>5</label><figDesc>Final Results</figDesc><table coords="10,199.98,138.15,215.40,72.92"><row><cell>Model</cell><cell cols="3">NDCG@1 NDCG@5 NDCG@10</cell></row><row><cell>initial</cell><cell>0.5068</cell><cell>0.4480</cell><cell>0.4196</cell></row><row><cell>pagerank</cell><cell>0.4723</cell><cell>0.4256</cell><cell>0.4087</cell></row><row><cell>domain</cell><cell>0.4100</cell><cell>0.3845</cell><cell>0.4132</cell></row><row><cell cols="2">argumentative classifier 0.4281</cell><cell>0.4123</cell><cell>0.4023</cell></row><row><cell>UvATask2SVM</cell><cell>0.5000</cell><cell>0.4464</cell><cell>0.4185</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,645.94,189.16,7.77"><p>https://events.webis.de/touche-20/shared-task-1.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,657.08,88.41,7.77"><p>http://www.arguana.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,657.08,125.43,7.77"><p>https://github.com/codelibs/ranklib</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="7,144.73,657.08,189.16,7.77"><p>https://events.webis.de/touche-20/shared-task-2.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="8,144.73,657.08,271.30,7.77"><p>https://scikit-learn.org/stable/supervised_learning.html#supervised-learning</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported in part by the <rs type="funder">Netherlands Organization for Scientific Research (NWO</rs>, grant # CISC.<rs type="grantNumber">CC.016</rs>, <rs type="projectName">ACCESS</rs> project). Views expressed in this paper are not necessarily shared or endorsed by those funding the research.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_HKqnC6w">
					<idno type="grant-number">CC.016</idno>
					<orgName type="project" subtype="full">ACCESS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.61,218.68,330.38,7.77;11,150.95,229.64,326.44,7.77;11,150.95,240.60,299.84,7.77;11,150.95,251.56,319.85,7.77;11,150.95,262.52,294.73,7.77;11,150.95,273.48,163.73,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,414.46,218.68,58.52,7.77;11,150.95,229.64,145.09,7.77">Data acquisition for argument search: The args.me corpus</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-30179-8_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-30179-8_4" />
	</analytic>
	<monogr>
		<title level="m" coord="11,173.62,240.60,244.88,7.77">Advances in Artificial Intelligence -42nd German Conference on AI</title>
		<title level="s" coord="11,276.49,251.56,174.78,7.77">Proceedings. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Benzmüller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</editor>
		<meeting><address><addrLine>Kassel, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-23">2019. September 23-26, 2019. 2019</date>
			<biblScope unit="volume">11793</biblScope>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,284.44,318.39,7.77;11,150.95,295.40,206.92,7.77" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="11,192.19,284.44,227.35,7.77">Groundwork in the theory of argumentation: Selected papers of</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Blair</surname></persName>
		</author>
		<editor>J. Anthony Blair</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,306.36,337.98,7.77;11,150.95,317.32,294.82,7.77;11,150.95,328.28,314.34,7.77;11,150.95,339.24,20.92,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,349.99,317.32,95.79,7.77;11,150.95,328.28,69.64,7.77">Overview of Touché 2020: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,238.50,328.28,208.12,7.77">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<imprint>
			<date type="published" when="2020-09">Sep 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,350.21,317.60,7.77;11,150.95,361.16,315.98,7.77;11,150.95,372.12,327.78,7.77;11,150.95,383.08,317.30,7.77;11,150.95,394.04,325.13,7.77;11,150.95,405.00,308.17,7.77;11,150.95,415.96,168.21,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,243.63,361.16,167.58,7.77">Touché: First shared task on argument retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-45442-5_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-45442-5_67" />
	</analytic>
	<monogr>
		<title level="m" coord="11,434.39,372.12,44.35,7.77;11,150.95,383.08,283.88,7.77">Advances in Information Retrieval -42nd European Conference on IR Research, ECIR 2020</title>
		<title level="s" coord="11,254.77,394.04,201.79,7.77">Proceedings, Part II. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Castells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Martins</surname></persName>
		</editor>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">April 14-17, 2020. 2020</date>
			<biblScope unit="volume">12036</biblScope>
			<biblScope unit="page" from="517" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,426.92,316.46,7.77;11,150.95,437.88,302.44,7.77;11,150.95,448.84,313.66,7.77;11,150.95,459.80,121.86,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,304.04,426.92,139.46,7.77">Overview of the TREC 2009 web track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,288.51,437.88,164.89,7.77;11,150.95,448.84,87.11,7.77">Proceedings of The Eighteenth Text REtrieval Conference, TREC 2009</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>The Eighteenth Text REtrieval Conference, TREC 2009<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="2009">November 17-20, 2009</date>
			<biblScope unit="page" from="500" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,481.72,321.58,7.77;11,150.95,492.68,307.39,7.77;11,150.95,503.64,324.96,7.77;11,150.95,514.60,290.75,7.77;11,150.95,525.56,288.35,7.77;11,150.95,536.52,225.90,7.77;11,150.95,547.48,274.17,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,318.75,481.72,145.44,7.77;11,150.95,492.68,144.12,7.77">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="11,150.95,503.64,324.96,7.77;11,150.95,514.60,286.72,7.77">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="11,142.61,558.44,294.36,7.77;11,150.95,569.40,68.48,7.77" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,247.48,558.44,71.69,7.77">Logical self-defense</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">H</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Blair</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>International Debate Education Association</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,580.37,328.34,7.77;11,150.95,591.33,329.23,7.77;11,150.95,602.28,304.92,7.77;11,150.95,613.24,329.43,7.77;11,150.95,624.20,233.86,7.77;11,150.95,635.16,149.54,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,150.95,591.33,182.79,7.77">Chatnoir: a search engine for the clueweb09 corpus</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Graßegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Welsch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2348283.2348429</idno>
		<ptr target="https://doi.org/10.1145/2348283.2348429" />
	</analytic>
	<monogr>
		<title level="m" coord="11,227.65,602.28,228.22,7.77;11,150.95,613.24,175.70,7.77">The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR &apos;12</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<meeting><address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">August 12-16, 2012. 2012</date>
			<biblScope unit="page">1004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,646.13,337.98,7.77;11,150.95,657.08,320.86,7.77;12,150.95,119.96,308.29,7.77;12,150.95,130.92,312.16,7.77;12,150.95,141.88,198.60,7.77;12,150.95,152.84,134.34,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,163.91,657.08,245.79,7.77">Computational argumentation quality assessment in natural language</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Naderi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bilu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Thijm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/e17-1017</idno>
		<ptr target="https://doi.org/10.18653/v1/e17-1017" />
	</analytic>
	<monogr>
		<title level="m" coord="11,427.99,657.08,43.83,7.77;12,150.95,119.96,308.29,7.77;12,150.95,130.92,85.77,7.77">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">April 3-7, 2017. 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="176" to="187" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="12,142.24,163.80,327.01,7.77;12,150.95,174.76,318.74,7.77;12,150.95,185.71,317.80,7.77;12,150.95,196.67,318.31,7.77;12,150.95,207.63,304.71,7.77;12,150.95,218.59,207.97,7.77;12,150.95,229.55,278.16,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,283.20,174.76,170.08,7.77">Building an argument search engine for the web</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Khatib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puschmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dorsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-5106</idno>
		<ptr target="https://doi.org/10.18653/v1/w17-5106" />
	</analytic>
	<monogr>
		<title level="m" coord="12,300.30,196.67,168.96,7.77;12,150.95,207.63,128.10,7.77">Proceedings of the 4th Workshop on Argument Mining, ArgMining@EMNLP 2017</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Habernal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Ashley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Green</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Litman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Petasis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Reed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Slonim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Walker</surname></persName>
		</editor>
		<meeting>the 4th Workshop on Argument Mining, ArgMining@EMNLP 2017<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-08">September 8, 2017. 2017</date>
			<biblScope unit="page" from="49" to="59" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
