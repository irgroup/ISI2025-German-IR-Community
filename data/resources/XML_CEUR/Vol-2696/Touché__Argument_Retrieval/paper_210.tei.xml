<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,179.50,115.90,256.37,12.90;1,168.54,133.83,278.29,12.90;1,135.25,153.68,344.86,10.75">Retrieving Comparative Arguments using Deep Pre-trained Language Models and NLU Notebook for the Touché Lab on Argument Retrieval at CLEF 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,208.05,190.08,78.27,8.64;1,286.32,188.18,1.93,6.12"><forename type="first">Viktoriia</forename><surname>Chekalina</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Philips Research Lab RUS</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.85,190.08,87.30,8.64;1,403.16,188.18,1.83,6.12"><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,179.50,115.90,256.37,12.90;1,168.54,133.83,278.29,12.90;1,135.25,153.68,344.86,10.75">Retrieving Comparative Arguments using Deep Pre-trained Language Models and NLU Notebook for the Touché Lab on Argument Retrieval at CLEF 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F28753D64E4A42112FF1638D319964A7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present our submission to the CLEF-2020 shared task on Comparative Argument Retrieval. We propose several approaches based on state-of-the-art NLP techniques such as Seq2Seq, Transformer, and BERT embedding. In addition to these models, we use features that describe the comparative structures and comparability of text. For the set of given topics, we retrieve the corresponding responses and rank them using these approaches. Presented solutions could help to improve the performance of processing comparative queries in information retrieval and dialogue systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>People are faced with a multitude of choice problems on a daily basis. This type of questions can be related to products, e.g., which milk producer to trust, which fruit contains less sugar, or which laptop brand is more reliable. Another popular type of comparison is related to travel destinations, e.g., which cities or national parks to visit. The comparative questions can also involve more complex objects/matters of comparison, e.g., which country is safer to raise children, Germany, or the United States? Finally, the fuzziness of comparative questions can go even further into some philosophical questions with possibly no definitive answer, e.g., which political system is better to maximize the overall average happiness of a population. Therefore, the comparative information need is an omnipresent type of information need of users.</p><p>While for some categories of products, e.g., mobile phones and digital cameras, tools for side-to-side comparison of features are available, for many domains, e.g., programming languages or databases, this information is not well structured. On the other hand, the Web contains a vast number of opinions and objective arguments that can facilitate the comparative decision-making process. The goal of our work is to develop methods for the retrieval of such textual documents, which are highly relevant for fulfilling the various comparative information needs of the users. Recent research on this topic touched on some aspects of the comparative question answering, e.g., retrieval human-computer interaction interface for comparative queries <ref type="bibr" coords="1,419.22,604.55,15.27,8.64" target="#b17">[17]</ref>, classification of comparative questions <ref type="bibr" coords="1,255.69,616.51,11.62,8.64" target="#b2">[3]</ref> or extraction of objects and aspect from comparative texts <ref type="bibr" coords="2,156.64,119.31,10.58,8.64" target="#b0">[1]</ref>, inter alia. However, the quality of retrieval of comparative answers was not evaluated to date.</p><p>More specifically, this notebook contains a description of our approach used in the submission of the CLEF-2020 shared task on Comparative Argument Retrieval<ref type="foot" coords="2,446.25,153.51,3.49,6.05" target="#foot_0">1</ref> including all details necessary to reproduce our results. The source codes and data used in our submission are also available online. <ref type="foot" coords="2,280.70,177.42,3.49,6.05" target="#foot_1">2</ref> The contribution of our work is three-fold:</p><p>1. We are first to use various deep pre-trained language models, such as ULMFiT and Transformer-based, on the task of comparative argument retrieval. 2. We are first to experiment with features based on specialized sequence taggers of comparative structures (detection objects, predicates, and aspects of comparison) that implement a shallow Natural Language Understanding (NLU). 3. We are first to experiment with features based on the density of comparative sentences in a text (based on a pre-trained classifier of comparative sentences <ref type="bibr" coords="2,448.29,269.73,14.94,8.64" target="#b13">[13]</ref>).</p><p>The remainder of this paper is organized as following: Section 2 introduces the task, then Section 3 presents several variations of the method we proposed. In Section 4, the results of the experiments based on the manual evaluation are discussed. Finally, Section 5 summarizes the main findings and directions for future work.</p><p>This XML file does not appear to have any style information associated with it. The document tree is shown below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-&lt;topic&gt; &lt;number&gt;38&lt;/number&gt; -&lt;title&gt;</head><p>What are the differences between MySQL and PostgreSQL in performance? &lt;/title&gt; -&lt;description&gt;</p><p>Before starting a new DB-related project, a software developer wants to find some tips about when to use which database. Back in their studies some years ago, they had learnt that the choice of a database management system is important when starting a new project. Not having too much experience with database-related software development, the user just remembers that historically, MySQL had been a default choice for creating and maintaining databases. Even though the performance differences between MySQL and PostgreSQL have been largely eliminated in recent versions, there still are differences worth considering like usage of indices, default installation, what types of replication / clustering are available and so on. Getting to know these issues will help the developer make a choice for the project. &lt;/description&gt; -&lt;narrative&gt; Highly relevant documents discuss differences between MySQL and PostgreSQL focusing on their performance such as query throughput on what hardware or time to write large amounts of data, but also development and support effort, etc. Highly relevant documents should provide a conclusion on main differences between the two database management systems and about typical scenarios that would favor either option. Relevant documents may help to form an opinion on the performance of either database by for instance describing the usage of one of the systems in some specific scenario(s 2 Task: Retrieval of Comparative Arguments on the Web</p><p>The track Touché <ref type="bibr" coords="2,209.00,615.10,11.62,8.64" target="#b3">[4]</ref> suggests the following goal: given a set of topics, one needs to retrieve and rank documents according to their relevance. The relevant documents are those which are helpful in making the comparative decision, i.e., those which directly compare the target objects facing their pros and cons. The topic contains a question implying comparison of the two objects, i.e. "What is better, a laptop or a desktop?", "Which is better, Canon or Nikon?", "Should I buy or rent?". An example of the topic is presented on Figure <ref type="figure" coords="3,355.10,167.48,3.74,8.64">1</ref>. Each topic consists of a title, e.g., a short description similar to those in which a user could enter into an information retrieval engine but also contains two additional fields: description and narrative. These fields specify more closely the context and semantics of the topic, and these are actually used by human annotators to perform judgments of the retrieved documents. In our experiments, we only used the "title" field.</p><p>We use topic title as a query in ChatNoir search engine <ref type="bibr" coords="3,375.86,239.56,16.60,8.64" target="#b15">[15]</ref> <ref type="foot" coords="3,392.46,237.89,3.49,6.05" target="#foot_2">3</ref> , which extracts documents from ClueWeb12 corpus. <ref type="foot" coords="3,268.81,249.84,3.49,6.05" target="#foot_3">4</ref> In response to the query ChatNoir returns a set of documents which contains titles, body texts, documents identifiers and search engine's scores. We try to retrieve 1000 unique documents, but for some queries the system gives less.</p><p>The goal of our methods is to find documents that most reliably and completely answer the query question in this set of pre-retrieved candidates. In other words, the document should be relevant to the topic, be trustworthy, and give an entirely and reasonable comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The main objective of the experiments is to develop a method finding among retrieved documents that meet the comparative criteria most fully and reasonably.</p><p>In addition to the search system's scoring, we employ pre-trained state-of-the-art language models and methods for getting the rate of the document's comparability. This section contains short descriptions of approaches for the computation of the score of one document in the search engine's response. All of the approaches described below are run on TIRA system <ref type="bibr" coords="4,265.98,119.31,15.27,8.64" target="#b14">[14]</ref>. The computation of scores for the entire set of responses by a certain method is schematically shown in Figure <ref type="figure" coords="4,390.05,131.27,3.74,8.64" target="#fig_0">2</ref>. The ranking process is completed by sorting documents by these values. Ultimately, each of the presented methods computes a similarity score s ij between a topic t i and a candidate document d j from a candidate set:</p><formula xml:id="formula_0" coords="4,271.15,190.72,209.44,9.65">s ij = sim(t i , d j ).<label>(1)</label></formula><p>The goal of every presented method is to compute for each topic t i a vector of scores s i = (s i1 , ..., s iNi ), where N i is the number of candidate documents for the topic t i .</p><p>Overall, we submitted six various solutions to test topic titles. Since no training data were provided, it is not allowed to evaluate the performance of the suggested strategies during the development. Below, we describe all proposed approaches in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline based on an inverted index</head><p>In our experiments, we utilize ChatNoir system <ref type="bibr" coords="4,325.52,306.57,11.62,8.64" target="#b1">[2]</ref> as a candidate documents extractor, which was provided (as a baseline) by organizers. ChatNoir is an Elasticsearch-based <ref type="foot" coords="4,476.61,316.86,3.49,6.05" target="#foot_4">5</ref>engine providing access to nearly 3 billion web pages from ClueWeb and Common Crawl corpora. Query processing shared across several search node allows reaching response time compared to the commercial system. Text relevance estimation is based on custom BM25 scoring function, <ref type="foot" coords="4,273.24,364.68,3.49,6.05" target="#foot_5">6</ref> which ranks the set of texts depends on the query's tokens existing in each response documents.</p><p>The defined search system in response to question in the topic title returns documents, its titles and scores. We take scores given by it and create a document ranking based on it, so, similarity score is</p><formula xml:id="formula_1" coords="4,285.16,435.75,195.44,9.65">s ij = cn ij ,<label>(2)</label></formula><p>where cn ij -scores provided by ChatNoir for i-th title to j-th responded document.</p><p>It should be noted that the system issue may contain similar documents. We look through the response and remove the document with duplicated titles. We also clean documents' bodies from HTML tags and markups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Language model LSTM ULMFiT</head><p>The simplest way to estimate the relevance of documents is by mapping the query and response in the same vector space. The relevance is defined as the cosine similarity between retrieved objects.</p><p>We assume that the hidden state in the recurrent network implicitly contains information about all processed sequences. Providing topic title and a response document's body to LSTM as an input gives in the hidden state of the last step their compressed representations.</p><p>The modification of the hidden state at each step depends on the parameters of the model. We employ the weights from pre-trained Universal Language Model Finetuning (ULMFiT) <ref type="bibr" coords="5,209.93,143.22,15.27,8.64" target="#b10">[10]</ref>. A state-of-the-art language model AWD-LSTM <ref type="bibr" coords="5,428.67,143.22,16.60,8.64" target="#b11">[11]</ref> is tuned on Wikitext-103 <ref type="bibr" coords="5,202.27,155.18,15.27,8.64" target="#b12">[12]</ref>, which collects 28,595 Wikipedia articles and 103 million words. The model class consists of 3 LSTM layers, encoder-decoder, dropouts, and linear layer. We use the class definition from here <ref type="foot" coords="5,286.64,177.42,3.49,6.05" target="#foot_6">7</ref> , extract only LSTM layers, and apply them to texts.</p><p>We pass the query and the document body through these layers. The input tokens are transferred into vectors using bert-as-service library <ref type="foot" coords="5,363.86,213.32,3.49,6.05" target="#foot_7">8</ref> .</p><p>Similarity score for this method is computed as following:</p><formula xml:id="formula_2" coords="5,271.32,248.66,209.27,9.65">s ij = cos(h i , h j ),<label>(3)</label></formula><p>where h i is the hidden state of LSTM that was fed with the i-th topic's title and h j is the hidden state for j-th response's body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention of a Transformer-based language model</head><p>The Transformer is a neural-network encoder-decoder model, being used as an alternative to recurrent neural networks, such as LSTM, e.g., the ULMFiT model described in the previous section. The key innovation of the Transformer is the attention mechanism, which at each step calculates the importance of each word from the input sequence. Information from pre-trained attention layers can be used to analyze the closeness of the query and the response. A Transformer can deal with a pair of input sequences separated by a special character. The attention layer returns the mutual weights of every word of this pair. Since we are interested in the relation of the topic and retrieved document, the input pair will be composed of them. The entrance is "[CLS]" + query + "[SEP]" + response document's body + "[SEP]", where "[CLS]" and "[SEP]" are special symbols being used when processing the sentences through Transformer.</p><p>The appropriate Transformers' head selecting. The attention layer in the standard Transformer provides 12 outputs, named heads. Each of head describes its own, not predefined meaning. For every token in an encoded sequence, one head gives weights for all input tokens. If we encode the input to itself, we get a matrix of adjacent weights for each input word.</p><p>Using the obtained matrix we can build a map of attention. On Figure <ref type="figure" coords="5,427.75,542.62,4.98,8.64" target="#fig_1">3</ref> these structures are shown for input "[CLS]" + Which is better, a laptop or a desktop? + "[SEP]" + Laptop is preferable than desktop because it is more portable. + "[SEP]". The bright vertical stripe corresponds to the separation token and should be excluded from consideration. For interconnection estimation of words from different sentences, only the upper left and lower right corners of the map should be taken into account -the "nondiagonal" right upper and lower left parts describe the response of the one sentence in input pair to itself. To use the Transformer efficiently, we need to select those outputs that provide information relevant to response ranking. As it can be observed in Figure <ref type="figure" coords="6,435.87,455.38,3.74,8.64" target="#fig_2">4</ref>, the third head determines similar words in sequence's pair, so we take it for scoring.</p><p>To select other suitable heads, we design a sandbox experiment. We take a query "Which is better, a laptop or a desktop?" and make a set of 4 documents consisting of 1 sentence. Two of these documents are retrieved from top Google sites to query determined above and are marked as relevant. The other two are taken from "The Hunting of the Snark" by Lewis Carroll, and they are considered as unreasonable. The query and the obtained sentences are in Table <ref type="table" coords="6,279.42,539.61,3.74,8.64" target="#tab_1">1</ref>. The idea of the experiment is to process paired input to the Transformer attention layer and observe at which outputs the value of the sum for the relevant and irrelevant documents differs the most.</p><p>We apply the Transformer to query merged with one of four sentences. For each of 12 transformer's heads, we count the sum of weights from the upper left and lower right. The most significant variation appears in 4, 10, 11 heads (Table <ref type="table" coords="6,417.65,599.93,3.60,8.64">2</ref>). These heads are taken into consideration when a similarity score creates.</p><p>The counting score of similarity using attention layer. The response scoring consists of two steps: first, we concatenate query with enumerated document and process it by Transformer attention layers; second, we count the sum of the appropriate parts of maps for 3, 4, 10, 11 heads. Thereby, a similarity score for i-th topic title and j-th retrieved document is calculated as</p><formula xml:id="formula_3" coords="7,192.08,561.13,288.51,33.76">s ij = h=3,4,10,11   Q-1 l=1 Q+R-1 m=Q+1 w lm + Q+R-1 l=Q+1 Q-1 m=1 w lm   ,<label>(4)</label></formula><p>where Q -the length of query, which is i-th topic title, R -the length of j-th document body. W lm is an attention weight for l-th to m-th token in input, ranges of indices l and m describes the proper part of the attention map. The accounted attention heads are enumerated by h. The idea of this approach for the third layer is illustrated in Figure <ref type="figure" coords="7,473.11,644.48,3.74,8.64" target="#fig_2">4</ref>. Namely, the zones highlighted in red color zones correspond to similarity of words from query to these from a candidate documents. The other parts represent self-similarity of query and document and thus have somewhat trivial sparsity pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Which is better, a laptop or a desktop?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document 1</head><p>Laptop is preferable than desktop because it is more portable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document 2</head><p>If you need portability, the laptop is the best option than desktop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document 3</head><p>The crew was complete: it included boots, a maker of bonnets and hoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document 4</head><p>Just the pace for snark, the Bellman cried.  <ref type="table" coords="8,159.37,398.32,3.36,8.06">2</ref>. Sums of the attention heads' outputs for relevant and unrelated responses. In every cell, there is a value counted over upper left and lower right corners on the attention map. Bold columns have a high variation on close and random answers, which means that they are sensitive to the proximity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bidirectional encoder representations from Transformer (BERT)</head><p>The architecture of Bidirectional Encoder Representations from Transformer (BERT) <ref type="bibr" coords="8,134.77,521.02,11.62,8.64" target="#b9">[9]</ref> is based on Transformer and is fine-tuned on specific masked language tasks. The result is a bidirectional language model that can give distributed representations of words taking into account contextual information.</p><p>We employ bert-as-service library to provide word embedding from BERT. This library uses pre-trained weights from a large uncased model with 340M parameters and encodes every word in query and document title in ChatNoir's response. Vectors corresponding to query and title are averages between all word embedding in defined sequences. The similarity score between query and title is described as</p><formula xml:id="formula_4" coords="8,257.79,624.28,222.80,9.65">s ij = cos(e query , e title ),<label>(5)</label></formula><p>where e query -average between embedded query's tokens, e title -average between embedded tokens in responded document title.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Comparative feature extraction</head><p>The scores by the approaches described above estimate the relevance of the topic and response as the closeness of the texts; in other words, show how possible and appropriate the given response is. The closeness is calculated in the context of well-known models (BERT, ULMFiT), trained on a huge amount of texts of a natural language. Such a method allows us to select documents that are similar in meaning but do not evaluate the quality of the comparison explicitly.</p><p>In order to evaluate the document as an argumentation, we use the combination of one of the some recently used methods and the approach giving information about the document's argumentativeness. The resultant similarity score is a multiplication of the score provided by the chosen method and additive term r. This term is counted for one document and represents a composition of features relied on the density of comparative sentences and features derived from the number of comparative parameters existed in the text. Initially, r is equal to 1.</p><p>The comparative degree of the document depends on the number of existent comparative sentences. To detect comparative sentences, we use the method described in <ref type="bibr" coords="9,461.50,311.07,15.27,8.64" target="#b13">[13]</ref>. It encodes the sentence by the InferSent embedder <ref type="bibr" coords="9,344.28,323.03,10.58,8.64" target="#b8">[8]</ref>, then applies gradient boosted decision trees (XGBoost) <ref type="bibr" coords="9,238.31,334.98,11.62,8.64" target="#b6">[6]</ref> classifier to the resulting features. The XGBoost model is pre-trained on multi-domain Comparative Sentences Corpus 2019, formed by the 7,199 sentences. It determines the probability that considered sentences are being regular or comparative. If the comparative probability is greater than 0.2, the counter of comparative sentences is incremented.</p><p>After using the classifier of comparatives, r increases by the number of revealed sentences n:</p><formula xml:id="formula_5" coords="9,285.80,420.90,194.79,8.96">r = r + n,<label>(6)</label></formula><p>To precise if the document collates exactly to what the user wants to, we formalize the comparative parameters. We determine two comparison objects, predicates (comparison conditions, for example, "cheaper"), and comparison features -aspects ("for children", "for deep learning"). Tagging these parameters in a given sentence leads to the sequence labeling problem. State-of-the-art solutions provide low performance on comparative cases, and we created and trained our own sequence-labeling module to achieve acceptable quality.</p><p>Our model consists of a single layer LSTM with 200 hidden units from <ref type="bibr" coords="9,452.42,526.03,10.58,8.64" target="#b7">[7]</ref>. To the input of the recurrent network, we enter the BERT embedding of words. We train BERT and LSTM parts of the model together with a learning rate 0.00001 and 0.01, respectively. As a target, we use a custom dataset structurally similar to that by Arora <ref type="bibr" coords="9,134.77,573.85,11.62,8.64" target="#b0">[1]</ref> <ref type="foot" coords="9,146.38,572.18,3.49,6.05" target="#foot_8">9</ref> composed of 3,967 labeled comparative sentences from the different domains.</p><p>To count comparative parameters part of the additive term, first, we process the query by sequence labeling model described above. The model extracts objects, aspects, and predicates and formalize the user's answer. Then we combine the document's title and body, and by a simple search in text, try to detect extracted parameters in it.</p><p>The additional term changes according to the following law:</p><formula xml:id="formula_6" coords="10,232.49,130.63,48.75,15.53">r = r * 1.</formula><p>2 if we find one object r * 1.5 if we find two objects <ref type="bibr" coords="10,468.98,137.74,11.62,8.64" target="#b7">(7)</ref> Appearance in the text one of the predicates or aspects in case of objects' existence additionally adds 1 to r:</p><formula xml:id="formula_7" coords="10,287.21,185.89,193.38,8.96">r = r + l,<label>(8)</label></formula><p>where l -number of predicates or aspects founded in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Combination of Baseline, number of comparative sentences and comparative structure extraction</head><p>For every document in the response, we count additive term and multiply it with the engine's score. The resulting similarity score is</p><formula xml:id="formula_8" coords="10,274.70,290.76,205.90,9.65">s ij = cn ij * r ij ,<label>(9)</label></formula><p>where cn ij is a score issued by ChatNoir system, r ij -additive term for i-th title and j-th document, calculated as described above. For making answer, we rank documents by the resulting values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Combination of ULMFiT, number of comparative sentences and comparative structure extraction</head><p>In this method, we do the same as in the previous section, with the only difference that scores counted by method from the section 3.2 are used as the basic value. We also compute an additive term r ij for i-th title and j-th document and resulting scoring is the following:</p><formula xml:id="formula_9" coords="10,271.03,451.07,209.56,9.65">s ij = ulm ij * r ij ,<label>(10)</label></formula><p>where ulm ij is a score by an approach based on ULMFiT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation</head><p>Each retrieved document from each of the seven approaches tested by our team was manually evaluated on the scale 0-1-2, where 0 means no relevance, "1" means the document contains relevant information, e.g., characteristics of one of the object, and "2" means very relevant i.e., the document directly compares the objects mentioned in the topic in the required context. In addition to assessing the relevance, for every response, we estimate pieces of evidence provided in the document by support retrieval model <ref type="bibr" coords="10,381.81,608.62,10.58,8.64" target="#b4">[5]</ref>. Based on these judgments, the official NDCG score was computed for each submission. The results are discussed in the following section. The correspondence of the names of the methods described above and experiment run tags are in the first and the second columns of the Table <ref type="table" coords="10,159.14,656.44,3.74,8.64">3</ref>.  <ref type="table" coords="11,207.28,273.47,3.36,8.06">3</ref>. Results of ranking quality's measure for described methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discussion</head><p>The top-5 discounted cumulative gain(DCG@5) scores for the proposed approaches are in the Table <ref type="table" coords="11,184.05,354.66,3.74,8.64">3</ref>.</p><p>The Table <ref type="table" coords="11,191.66,367.34,4.98,8.64">3</ref> shows that approaches using only pre-trained language models give the smallest scores. It can be explained by the fact that the information stored in the SOTA linguistic model is sufficient to estimate the appropriateness of the text but not enough to assess how complete, persuasively, and supportive the document is. As in many other tasks, the Attention-based model has a better performance than ULMFit -0.223 against 0.200. This is due to the fact that the attention mechanism allows us to consider the meaningful context that is located at a distance from the current word, which makes the model more expressiveness. BERT-based model is a bidirectional expansion of the attention layer. Therefore, its application increases the performance to 0.405.</p><p>Overall, a combination of the approaches with comparative information shows better performance than the same method without comparative terms. Thus, consideration of comparative structures and sense improves the results for ULMFit from 0.200 to 0.464.</p><p>The best quality is provided by the baseline model being cleaned from document duplicates. Its scoring function is based on the BM25 ranking formula but uses a more efficient way of calculating term frequencies <ref type="bibr" coords="11,322.61,548.12,15.27,8.64" target="#b16">[16]</ref>. It provides the ability to consider information from all parts of the document -title and body, which gives superiority over methods that process only the title or only the document's body. It should be noticed that the baseline gives NDCG@5 0.565, baseline with CAM, and object extraction -0.554. One reason for decreasing quality when complementary information is added is choosing the weight with which we consider the CAM information and number of comparative structures.</p><p>The main take-aways are as follows. First, the methods for re-ranking of the candidate documents which do not rely on the original baseline score, but instead completely replace it with similarity scores based on the language models do not yield superior re-sults to the baseline; therefore, the original scores shall be used. Among all such, completely baseline-free methods BERT-based similarity yielded the best results. Second, a combination of the custom features based on the density of comparative structures in text combined with the baseline yield better results. Since no training data was provided in this version of the shared task, it was not obvious to test various combinations of the tested features, but given such supervised training data, a promising way to further improve the results is to combine various signals using a supervised machine learning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present our solution to the Argument retrieval shared task. Our main innovations are (i) the use of large pre-trained language models, (ii) the use of features based on natural language understanding of comparative sentences, and (iii) the use of features based on the density of comparative sentences. It should be noted that modern linguistic models meet the response relevance quite well, but to assess the comparability and argumentation of the answer, we need to add external features.</p><p>Overall, according to the experimental results, the baseline information retrieval model proved to be a hard baseline. In fact, among all 11 evaluated runs in the shared task only one outperformed the baseline by more than 0.5% which is a substantial difference. 10 The results suggest that considering the score taking into account claim support and evidence existing, models based on SOTA language models do not work as well as the models which combine comparative structure and comparative sentiment in sentences. We conclude that in future work, more combinations of methods based on a combination of baseline IR models with comparative features shall be investigated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,556.23,345.83,8.12;3,134.77,567.54,345.83,7.77;3,134.77,578.50,158.11,7.77"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. Overview of our methods: candidate documents are obtained using a traditional inverted index-based IR system and then re-ranked on the basis of topic-document similarity measures specific to each of the proposed approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,134.77,397.10,345.83,8.12;6,134.77,408.41,308.01,7.77;6,136.49,115.83,342.36,266.53"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Maps for 12 attention heads for input "[CLS]" + Which is better, a laptop or a desktop? + "[SEP]" + Laptop is preferable than desktop because it is more portable. + "[SEP]".</figDesc><graphic coords="6,136.49,115.83,342.36,266.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,134.77,440.89,345.82,8.12;7,134.77,452.20,345.82,7.77;7,134.77,463.16,345.82,7.77;7,134.77,474.12,345.82,7.77;7,134.77,485.08,134.29,7.77;7,152.06,115.83,311.25,310.32"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The third attention head for input "[CLS]" + Which is better, a laptop or a desktop? + "[SEP]" + Laptop is preferable than desktop because it is more portable. + "[SEP]". Biggest weights correspond to similar words. Areas highlighted in red describe the response of tokens from one sentence to another and are taken into account in calculating the score, corresponding to two respective sums in Equation 4.</figDesc><graphic coords="7,152.06,115.83,311.25,310.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,159.85,121.25,27.89,7.77;11,319.73,121.25,44.09,7.77;11,439.26,121.25,38.14,7.77;11,137.95,137.21,150.51,7.77;11,319.73,137.21,97.13,7.77;11,439.26,136.86,20.17,8.06;11,137.95,153.17,177.81,7.77;11,159.85,164.13,28.38,7.77;11,319.73,153.17,76.21,7.77;11,439.26,153.17,20.17,7.77;11,137.95,180.09,177.81,7.77;11,159.85,191.05,28.38,7.77;11,319.73,180.09,139.71,7.77;11,137.95,207.01,177.81,7.77;11,159.85,217.97,94.77,7.77;11,319.73,207.01,62.77,7.77;11,439.26,207.01,20.17,7.77;11,137.95,233.93,177.81,7.77;11,159.85,244.89,22.42,7.77;11,319.73,233.93,139.71,7.77;11,137.95,260.85,143.19,7.77;11,319.73,260.85,61.77,7.77;11,439.26,260.85,20.17,7.77;11,183.94,273.47,21.10,8.06"><head>3 . 1</head><label>31</label><figDesc>Baseline based on an inverted index MyBaselineFilterResponse 0.564 § 3.6 Combination of Baseline and comparative features Baseline_CAM_OBJ 0.553 § 3.7 Combination of ULMFiT and comparative features ULMFIT_LSTM_CAM_OBJ 0.464 § 3.4 Bidirectional Encoder Representations from Transformer (BERT) myBertSimilarity 0.404 § 3.3 Attention of a Transformer-based language model MethodAttentionFilterResponse 0.223 § 3.2 Language model LSTM ULMFiT ULMFIT_LSTM 0.200 Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,243.66,345.83,162.72"><head>Table 1 .</head><label>1</label><figDesc>Possible user request and similar in meaning and distant responses for sandbox experiment, where we selecting appropriate transformer's heads.</figDesc><table coords="8,134.77,321.50,342.73,84.87"><row><cell>Input</cell><cell>h0 h1 h2</cell><cell>h3 h4 h5 h6 h7 h8 h9 h10 h11</cell></row><row><cell>Query + Document1</cell><cell cols="2">1.62 3.70 0.001 5.54 7.79 2.04 4.58 1.38 3.46 6.01 6.05 4.38</cell></row><row><cell>Query + Document2</cell><cell cols="2">1.70 4.27 0.001 5.08 7.53 1.93 4.69 1.25 3.62 6.37 6.81 5.55</cell></row><row><cell>Query + Document3</cell><cell cols="2">1.80 4.20 0.001 4.87 4.97 1.72 3.34 1.35 3.09 4.97 3.73 1.11</cell></row><row><cell>Query + Document4</cell><cell cols="2">1.67 3.49 0.001 2.83 4.33 1.90 3.30 1.40 3.42 4.47 3.92 3.01</cell></row><row><cell>Table</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,645.94,189.16,7.77"><p>https://events.webis.de/touche-20/shared-task-2.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,657.08,138.29,7.77"><p>https://github.com/skoltech-nlp/touche</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,645.94,101.03,7.77"><p>https://www.chatnoir.eu/doc</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,144.73,657.08,125.88,7.77"><p>https://lemurproject.org/clueweb12</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,144.73,634.98,79.61,7.77"><p>https://www.elastic.co</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,144.73,646.13,283.65,7.77;4,144.73,657.08,52.48,7.77"><p>https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modulessimilarity.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="5,144.73,645.94,108.81,7.77"><p>https://github.com/fastai/fastai</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="5,144.73,657.08,151.31,7.77"><p>https://github.com/hanxiao/bert-as-service</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="9,144.73,657.08,129.41,7.77"><p>https://github.com/uhh-lt/comparely</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.61,472.75,337.98,7.77;12,150.95,483.55,329.64,7.93;12,150.95,494.51,327.48,7.93" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,322.96,472.75,157.63,7.77;12,150.95,483.71,75.41,7.77">Extracting entities of interest from comparative product reviews</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pathak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,244.04,483.55,236.55,7.73;12,150.95,494.51,129.92,7.93">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1975" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,505.78,337.98,7.77;12,150.95,516.73,329.64,7.77;12,150.95,527.53,329.64,7.93;12,150.95,538.49,329.64,7.93;12,150.95,549.61,55.04,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,348.02,505.78,132.57,7.77;12,150.95,516.73,137.23,7.77">Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,218.91,527.53,261.68,7.73;12,150.95,538.49,69.60,7.73">Advances in Information Retrieval. 40th European Conference on IR Research (ECIR 2018)</title>
		<title level="s" coord="12,226.38,538.65,125.40,7.77">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Pasi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Piwowarski</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,560.72,337.98,7.77;12,150.95,571.52,329.64,7.93;12,150.95,582.47,284.03,7.93" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,247.12,571.68,125.64,7.77">Comparative web search questions</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Braslavski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Völske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,392.96,571.52,87.63,7.73;12,150.95,582.47,208.03,7.73">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,593.74,337.98,7.77;12,150.95,604.70,329.64,7.77;12,150.95,615.50,300.27,7.93" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,345.80,604.70,134.79,7.77;12,150.95,615.66,31.20,7.77">Overview of Touché 2020: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,198.56,615.50,205.89,7.73">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<imprint>
			<date type="published" when="2020-09">Sept. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,626.77,337.98,7.77;12,150.95,637.72,329.64,7.77;12,136.01,655.22,5.98,5.18;12,144.73,657.08,191.73,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,383.51,626.77,97.08,7.77;12,150.95,637.72,146.15,7.77">Supporting human answers for advice-seeking questions in cqa sites</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Braunstain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kurland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shtok</surname></persName>
		</author>
		<ptr target="https://events.webis.de/touche-20/shared-task-2.html#" />
	</analytic>
	<monogr>
		<title level="j" coord="12,446.51,637.72,29.81,7.77">J. Mothe</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M.-F</forename><surname>Moens</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.46,657.08,25.32,7.77;13,150.95,119.80,329.63,7.93;13,150.95,130.76,265.85,7.93" xml:id="b5">
	<monogr>
		<title level="m" coord="13,390.57,119.80,90.02,7.73;13,150.95,130.76,31.48,7.73">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Hauff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Silvello</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="129" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,141.88,337.98,7.77;13,150.95,152.68,329.64,7.93;13,150.95,163.80,44.72,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,247.28,141.88,149.72,7.77">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,401.06,152.68,15.88,7.73">KDD</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,174.76,337.98,7.77;13,150.95,185.55,329.64,7.93;13,150.95,196.51,329.64,7.73;13,150.95,207.47,329.64,7.93" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,206.71,185.71,190.53,7.77">TARGER: Neural argument mining at your fingertips</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chernodub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Oliynyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Heidenreich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,413.66,185.55,66.93,7.73;13,150.95,196.51,329.64,7.73;13,150.95,207.47,16.40,7.73">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,218.59,337.98,7.77;13,150.95,229.39,329.64,7.93;13,150.95,240.35,329.64,7.93;13,150.95,251.47,302.85,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,397.16,218.59,83.43,7.77;13,150.95,229.55,257.72,7.77">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,426.59,229.39,54.00,7.73;13,150.95,240.35,281.39,7.73">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09">Sept. 2017</date>
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,262.43,337.98,7.77;13,150.95,273.22,329.64,7.93;13,150.95,284.18,329.64,7.73;13,150.95,295.14,329.64,7.93;13,150.95,306.26,238.35,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,348.20,262.43,132.39,7.77;13,150.95,273.39,168.81,7.77">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,338.60,273.22,141.99,7.73;13,150.95,284.18,329.64,7.73;13,150.95,295.14,70.35,7.73">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="13,142.24,317.22,338.35,7.77;13,150.95,328.02,329.64,7.73;13,150.95,338.98,329.64,7.93;13,150.95,350.10,97.64,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,247.29,317.22,216.66,7.77">Universal language model fine-tuning for text classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,150.95,328.02,329.64,7.73;13,186.42,338.98,52.97,7.73">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
	<note>: Long Papers)</note>
</biblStruct>

<biblStruct coords="13,142.24,361.06,338.35,7.77;13,150.95,371.85,148.17,7.93" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,294.95,361.06,181.62,7.77">Regularizing and optimizing lstm language models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,160.67,371.85,111.96,7.93">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,382.81,338.35,7.93;13,150.95,393.77,116.79,7.93" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,329.91,382.97,113.27,7.77">Pointer sentinel mixture models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,461.16,382.81,19.43,7.73;13,150.95,393.77,90.29,7.93">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,404.89,338.35,7.77;13,150.95,415.69,329.63,7.93;13,150.95,426.81,253.45,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,413.78,404.89,66.81,7.77;13,150.95,415.85,63.22,7.77">Categorizing comparative sentences</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,229.24,415.69,189.77,7.73">Proceedings of the 6th Workshop on Argument Mining</title>
		<meeting>the 6th Workshop on Argument Mining<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08">Aug. 2019</date>
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,437.77,338.35,7.77;13,150.95,448.57,329.64,7.93;13,150.95,459.69,197.92,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,337.73,437.77,139.24,7.77">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,278.17,448.57,202.42,7.73;13,150.95,459.69,116.63,7.77">Information Retrieval Evaluation in a Changing World, The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09">Sept. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,470.65,338.35,7.77;13,150.95,481.44,329.63,7.93;13,150.95,492.40,329.64,7.93;13,150.95,503.52,42.59,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,150.95,481.60,180.86,7.77">Chatnoir: a search engine for the clueweb09 corpus</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Graßegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Welsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,347.61,481.44,132.98,7.73;13,150.95,492.40,279.13,7.73">Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 35th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1004" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,514.48,338.35,7.77;13,150.95,525.44,105.84,7.77" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="13,314.93,514.48,165.66,7.77;13,150.95,525.44,18.58,7.77">Simple bm25 extension to multiple weighted fields</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-01">01 2004</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,536.40,338.35,7.77;13,150.95,547.20,329.64,7.93;13,150.95,558.16,301.73,7.93" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,150.95,547.36,225.16,7.77">Answering comparative questions: Better than ten-blue-links?</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schildwächter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zenker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,391.70,547.20,88.89,7.73;13,150.95,558.16,217.30,7.73">Proceedings of the 2019 Conference on Human Information Interaction and Retrieval</title>
		<meeting>the 2019 Conference on Human Information Interaction and Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="361" to="365" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
