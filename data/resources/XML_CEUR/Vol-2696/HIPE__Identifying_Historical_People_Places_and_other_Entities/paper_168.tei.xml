<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.96,115.96,345.44,12.62;1,232.26,133.89,150.84,12.62">Transfer Learning for Named Entity Recognition in Historical Corpora</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,208.18,177.86,87.81,8.74;1,295.99,176.29,3.80,6.12"><forename type="first">Konstantin</forename><surname>Todorov</surname></persName>
							<email>kztodorov@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,218.40,189.82,83.64,8.74;1,302.04,188.24,3.93,6.12"><forename type="first">Giovanni</forename><surname>Colavizza</surname></persName>
							<email>g.colavizza@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.96,115.96,345.44,12.62;1,232.26,133.89,150.84,12.62">Transfer Learning for Named Entity Recognition in Historical Corpora</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F2D386E80AFA5718F880360998D36BA0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>NERC</term>
					<term>BERT</term>
					<term>Bi-LSTM-CRF</term>
					<term>Transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report on our participation to the 2020 CLEF HIPE shared task as team Ehrmama, focusing on bundle 3: Named Entity Recognition and Classification (NERC) on coarse and fine-grained tags. Motivated by an interest to assess the added value of transfer learning for NERC on historical corpora, we propose an architecture made of two components: (i) a modular embedding layer where we combine newly trained and pre-trained embeddings, and (ii) a task-specific Bi-LSTM-CRF layer. We find that character-level embeddings, BERT, and a document-level data split are the most important factors in improving our results. We also find that using in-domain FastText embeddings and a single-task as opposed to multi-task approach yields minor gains. Our results confirm that pre-trained language models can be beneficial for NERC on low-resourced historical corpora.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The advent of contextual language models such as Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" coords="1,311.56,488.57,10.52,8.74" target="#b2">[3]</ref> has furthered the adoption of transfer learning in Natural Language Processing (NLP). Transfer learning aims at transferring knowledge from a general-purpose source task to a specialised target task <ref type="bibr" coords="1,156.89,524.43,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="1,172.39,524.43,11.62,8.74" target="#b12">13]</ref>. The specialised target task is often linguistically under-resourced (e.g., small data or lack of linguistic resources) <ref type="bibr" coords="1,352.23,536.39,9.96,8.74" target="#b1">[2]</ref>. Transfer learning further allows saving computation resources by training once and applying the same model widely with little or no further adaptation <ref type="bibr" coords="1,352.09,560.30,14.61,8.74" target="#b14">[15]</ref>.</p><p>The increasing abundance of historical text corpora offers a compelling opportunity to apply transfer learning. Historical texts pose a set of challenges to the NLP and Digital Humanities (DH) communities, of which the most general and pressing are <ref type="bibr" coords="1,192.30,614.42,14.87,8.74" target="#b11">[12,</ref><ref type="bibr" coords="1,207.16,614.42,7.43,8.74" target="#b3">4]</ref>: a) noisy inputs, for example due to Optical/Handwritten Character Recognition (OCR/HTR) errors; b) linguistic change over time; c) language variety, in the absence of mainstream languages such as English. These challenges are not unique to historical texts, but they come to the forefront when dealing with them.</p><p>We participated in the CLEF HIPE as team Ehrmama, focusing on bundle 3: NERC coarse and NERC fine-grained <ref type="bibr" coords="2,305.04,184.75,9.96,8.74" target="#b4">[5]</ref>, conducted over English, French and German languages. This task bundle focuses on the recognition of named entities in six different tag types, namely coarse-and fine-grained and their metonymic senses, as well as components and nested entities of depth one. Our general goal is to assess if and how transfer learning using modern-day language models can help with tasks on OCRed historical corpora. To this end, we propose a model composed of a general purpose embedding layer which allows to combine character, sub-word and word-level embeddings in a modular way, equipped with a state-of-the-art NERC-specific layer. We then explore the use of newly-trained and pre-trained embeddings in isolation and in combination. Our code is publicly available<ref type="foot" coords="2,172.40,302.72,3.97,6.12" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our proposed architecture is composed of two parts: an embedding layer and a task-specific layer, in this case for NERC. An illustration is given in Figure <ref type="figure" coords="2,466.05,368.58,3.87,8.74" target="#fig_0">1</ref>.</p><p>Several embeddings can be combined into a modular embedding layer which we use to represent input text. We broadly distinguish between (i) pre-trained (transferred) embeddings and (ii) newly trained embeddings. While pre-trained embeddings can either be fine-tuned or frozen during learning, newly trained representations are learned from scratch. Furthermore, embeddings can be applied at different input granularities, including: (i) character-, (ii) sub-word-and (iii) word-level. These are also modular, and can be used in combination.</p><p>The task-specific layer is a Bidirectional Long Short-Term Memory, using Conditional Random Field (Bi-LSTM-CRF) as proposed by <ref type="bibr" coords="2,380.82,488.13,9.96,8.74" target="#b8">[9]</ref>, with the additional removal of the tanh non-linearity after the LSTM. As a sanity test, we applied our model on the modern-day CoNLL-2003 dataset <ref type="bibr" coords="2,362.33,512.04,14.61,8.74" target="#b13">[14]</ref>, achieving results comparable to current state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Empirical setup</head><p>Embedding layer containing four different embedding modules.</p><p>Character embeddings consist of an embedding layer, followed by a bidirectional LSTM. The embedding layer's size and the LSTM hidden size are both hyperparameters with values ranging from 16 to 128 and 16 to 256 respectively. We use a character-level custom vocabularies for each language built from the training and validation data sets. BERT embeddings work on sub-word level. We use bert-base-multilingual-cased for French, bert-base-german-cased for German, and bert-base-cased for English, relying on the HuggingFace Transformers library <ref type="bibr" coords="3,354.98,511.40,14.61,8.74" target="#b15">[16]</ref>. This brings the specific limitation of only working with sequences of 512 tokens in maximum length. As our text sequences are usually longer, we implement a sliding-window splitting of input sequences before passing them through BERT. While splitting, we keep the first and last 5 tokens of each chunk as overlap among sequential chunks. After embedding each chunk, we then reconstruct the full input sequences by averaging the embeddings of the overlapping characters.</p><p>Newly trained embeddings work on sub-word level and their weights are randomly initialised and learned during training. We use the same vocabulary as with BERT. The size of these embeddings is a hyper-parameter and ranges between 64 and 512.</p><p>In-domain pre-trained embeddings provided by the task organisers are used for feature extraction only (frozen). These embeddings have size of 300 and work at the sub-word level. This model uses the FastText library <ref type="bibr" coords="4,388.62,142.90,9.96,8.74" target="#b6">[7]</ref>.</p><p>After testing different alternatives, we found that the simplest and fastest way to combine these embeddings is by concatenating them, resulting in concatenated sub-word embeddings of a size equal to the sum of the embedding sizes of all enabled modules.</p><p>Task-specific layer based on a Bi-LSTM-CRF <ref type="bibr" coords="4,349.62,227.24,9.96,8.74" target="#b8">[9]</ref>.</p><p>The Bi-LSTM-CRF uses the concatenated sub-word embeddings as its input, and then merges the output to word level by taking the mean. Finally, the resulting representation is pushed to a fully connected layer which then outputs tag probabilities for each token. We tested concatenating embeddings before or after the Bi-LSTM, or not merging at all, and found that our approach performs best, also in accordance with previous findings <ref type="bibr" coords="4,335.24,311.36,14.61,8.74" target="#b12">[13]</ref>. A Conditional Random Field (CRF) <ref type="bibr" coords="4,167.23,323.32,10.52,8.74" target="#b7">[8]</ref> is eventually used over the produced tag probabilities to decode the final tag predictions.</p><p>A multi-task approach is our primary setup. We introduce additional output heads, one for each of the different entity types that the task aims to predict. The final two layers of the model, namely the fully connected layer and CRF, are specific to each entity tag type, while the rest of the architecture is shared. The individual losses for each task are summed during backpropagation. We compare using single vs multi-task approach in what follows.</p><p>Additional resources We use the Annotated Corpus for Named Entity Recognition built on top of Groningen Meaning Bank (GMB) <ref type="bibr" coords="4,378.91,461.67,10.51,8.74" target="#b0">[1]</ref> <ref type="foot" coords="4,389.42,460.10,3.97,6.12" target="#foot_1">2</ref> . This dataset is annotated specifically for training NER classifiers, and contains most of the coarse grained tag types which occur in the English dataset provided by organisers. We consolidate some tags with the same meaning but different labels ourselves. The dataset contains in total 1,354,149 tokens of which 85% are labelled as O originally. We convert the tag types that are not part of this challenge to O as well, resulting in total of 94.92% tokens having O literal tags.</p><p>We used an NVidia GeForce 1080Ti GPU with 11GB GDDR5X memory for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model fitting</head><p>In this section, we discuss the remaining pre-processing or hyper-parameter choices which we assessed empirically.</p><p>Pre-processing The input data is organised into documents, and each document is split into multiple segments where usually one segment corresponds to one line in the original historical source. The input can thus be split into segments or into documents. Using segments leads to much faster convergence, while document splitting usually yields better results in our experiments. We further analyse the importance of splitting by introducing a multi-segment option which combines more than one consecutive segment. We perform a hard split and pick the maximum length of one multi-segment sequence to be the maximum length allowed by the HuggingFace Transformers library. We do this to avoid any unwanted noise. At document level we overcome this limitation by splitting documents using a sliding window approach where the first and last 5 tokens for each split are overlapping with the previous and next splits respectively. We perform the cutting before extracting features through BERT after which we concatenate the representations back. We take the average values for the overlapping tokens. Finally, we replace all numbers with zeros, including such that contain more than one digit. Besides, we do not lowercase, nor do we remove any punctuation or other characters.</p><p>Fine-tuning vs. freezing There are two possibilities when using pre-trained models: keep them frozen or fine-tune further more. Fine-tuning the model lets us introduce two additional configuration options. The first one is related to when to start fine-tuning. This is most often performed at the beginning of the training process and until convergence. We try a second approach where firstly the full model with frozen pre-trained weights converges. After, the pre-trained weights are fine-tuned. This is something that we also investigate but find no difference between the two approaches. We therefore fine-tune from the start in the reported experiments with fine-tuning enabled.</p><p>Manually crafted features Following previous work <ref type="bibr" coords="5,384.88,459.29,9.96,8.74" target="#b5">[6]</ref>, we assess the importance of manually crafted features. We use AllLower, AllUpper, IsTitle, FirstLetterUpper, FirstLetterNotUpper, IsNumeric and NoAlphaNumeric as extra morphological features. When including these features in the model, we do not get significant improvements.</p><p>Weighting As it is common with NERC tasks, most of the ground truth is composed of outside or O tags. In our case, these make up for approximately 94.92%, 95.95%, and 96.5% of the total tokens for English, French and German languages respectively. To counteract tag imbalance, we test a weighted loss which we plug into the CRF layer, giving more weight to tags predicted as outside ones but are in fact part of entities, and less on tokens which are predicted as inside an entity but are actually outside. This weighted loss does not prove to be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameters</head><p>We assess Adam and AdamW <ref type="bibr" coords="5,365.39,644.16,23.12,8.74" target="#b9">[10]</ref> optimizers. For the learning rate we see that higher values benefit the model more. We pick a de-fault value of 1e-8 for weight decay for all optimizers. The final hyper-parameter configurations that we use are summarised in Table <ref type="table" coords="6,363.11,130.95,3.87,8.74" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We report results for the three languages part of the task, namely French, German and English, using the official test set v1.3 <ref type="foot" coords="6,341.14,397.23,3.97,6.12" target="#foot_2">3</ref> . In addition to that, we report results using multi-segment and document split types for French and German and segment split type for English, since our English training data lacks the document level.</p><p>All results are reported in the two scoring approaches used in the challengefuzzy and strict. As a reminder, fuzzy scoring works in a relaxed way, allowing fuzzy boundary matching of entities. That is if an entity is only partially recognised, e.g., if 4 out of total of 6 tokens are recognised correctly, this is still considered a successful recognition. Conversely, strict matching requires all tokens to match with exact boundary matching -in previous example this would require 6 out of 6 total tokens to be predicted correctly. For each scoring approach, we provide precision (P), recall (R) and F-score (F), all reported as micro and calculated using the original scorer, used in the competition <ref type="foot" coords="6,410.45,546.83,3.97,6.12" target="#foot_3">4</ref> . We report the baseline model provided by organisers for reference, reminding the reader that the baseline model always uses a document level split. We also report the baseline model results on our English data.</p><p>We order the different configurations for all languages following our ablation studies, which primarily focus on assessing the impact of transfer learning. We start with the simplest Base model which is only using newly-trained sub-word embeddings and no pre-trained information of any type. Then we continue by adding Character Embeddings (CE) which use Bi-LSTM (+CE). Due to the significant improvements observed by adding character embeddings, we keep them enabled in all of our next reported setups. We further report results that were achieved by adding firstly the (frozen) FastText embeddings provided by organisers (+FT), then (frozen) BERT embeddings (+BERT), and finally both. Whenever BERT is enabled, we also report runs where we disable newly trained embeddings (-newly). Eventually, we report three different setups where we unfreeze BERT and fine-tune them on the task at hand. Due to the long sequence lengths when working on document level, we are unable to perform fine-tuning of BERT at the document level. We therefore report the results of fine-tuning BERT only using multi-segment split. All models use the multi-task approach, except for one single-task run, which has all available embeddings enabled (single).</p><p>We start reporting results for French, in Table <ref type="table" coords="7,339.96,280.39,3.87,8.74" target="#tab_1">2</ref>. Firstly, adding character-level embeddings and BERT consistently improves results. Better results overall are obtained with a single-task approach and using all available embeddings, including newly trained ones. A document level split, following this configuration, perform best across the board. We also see that most of our configurations struggle on tasks with sparser annotations such as Metonymic and Nested. Furthermore, fine-tuning BERT does not seem to improve results. Results for German, shown in Table <ref type="table" coords="7,172.64,364.07,3.87,8.74">3</ref>, are consistent with those for French. It is worth noting that our models struggle even more on the German Metonymic and Nested tasks. For nested tags, we are not able to be predictive at all, specifically on the multi-segment level.</p><p>For completeness, we report results for English in Table <ref type="table" coords="7,378.79,417.87,3.87,8.74">4</ref>, limited to the Literal coarse task. For a better comparison, we provide results from two baseline models: i) results from the organisers and ii) results training the baseline model on the English dataset we use. Our models are mostly not able to perform beyond the provided baseline. This is likely due to the training data we use.</p><p>We clarify that most of these results were obtained after the task submission deadline. For the deadline, and as reported in <ref type="bibr" coords="7,339.59,495.58,9.96,8.74" target="#b4">[5]</ref>, we submitted three different runs for German and French and two for English. For German and French we submitted one run using multi-task learning and document-level splitting; another run using multi-task learning and multi-segment splitting; for English we had one run using multi-task learning and segment splitting; finally, we submitted one run for all languages where we used literal tag types from two single-task learning runs. All of our submitted runs had all modules enabled. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,432.71,345.83,8.74;3,134.77,444.66,331.65,8.74;3,186.64,115.84,242.07,305.34"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: NERC multi-task model architecture. Our single-task architecture is identical and only contains a fully connected layer and CRF for one entity type.</figDesc><graphic coords="3,186.64,115.84,242.07,305.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.77,174.05,345.83,145.16"><head>Table 1 :</head><label>1</label><figDesc>Hyper-parameter configurations. Configuration I is used for Base. Configuration II is used for Base + CE + BERT and Base + CE + BERT -newly. Configuration III is used for all remaining setups.</figDesc><table coords="6,138.34,212.44,338.69,106.77"><row><cell>Hyper-parameters</cell><cell cols="3">Configuration I Configuration II Configuration III</cell></row><row><cell>RNN hidden size</cell><cell>512</cell><cell>256</cell><cell>512</cell></row><row><cell>RNN directionality</cell><cell>bi-directional</cell><cell>bi-directional</cell><cell>bi-directional</cell></row><row><cell>RNN dropout</cell><cell>0.5/0.8</cell><cell>0.5/0.8</cell><cell>0.5/0.8</cell></row><row><cell>Newly trained embeddings size</cell><cell>64</cell><cell>64</cell><cell>64</cell></row><row><cell>Character embeddings size</cell><cell>-</cell><cell>16</cell><cell>16</cell></row><row><cell>Character embeddings RNN hidden size</cell><cell>-</cell><cell>32</cell><cell>32</cell></row><row><cell>Replace numbers during pre-processing</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell></row><row><cell>Weighted loss usage</cell><cell>no</cell><cell>no</cell><cell>no</cell></row><row><cell>Optimizer</cell><cell>AdamW</cell><cell>AdamW</cell><cell>AdamW</cell></row><row><cell>Learning rate</cell><cell>1e-2</cell><cell>1e-2</cell><cell>1e-2</cell></row><row><cell>Fine-tune learning rate</cell><cell>1e-4</cell><cell>1e-4</cell><cell>1e-4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,77.39,127.36,460.58,627.09"><head>Table 2 :</head><label>2</label><figDesc>NERC, French. The best result per table and column is given in bold, the second best result is underlined CE + FT + BERTnewly .864 .848 .856 .765 .751 .758 .766 .321 .453 .766 .321 .453 Base + CE + FT + BERT (single) .872 .835 .853 .769 .737 .753 .036 .069 .000 .036 .069 .000 + Fine-tuning (unfreezing) BERT Base + CE + BERT .876 .824 .849 .775 .729 .751 .442 .375 .406 .432 .366 .396 Base + CE + BERTnewly .877 .804 .839 .775 .711 .742 .754 .384 .509 .754 .384 .509 Base + CE + FT + BERT .857 .836 .846 .759 .741 .75 .551 .482 .514 .541 .473 .505 Base + CE + FT + BERTnewly .845 .838 .842 .742 .737 .74 .659 .5 .569 .659 .5 .569 (b) multi-segment level, fine grained entity type CE + FT + BERTnewly .873 .82 .846 .672 .631 .651 .771 .241 .367 .743 .232 .354 .842 .546 .663 .774 .503 .61 .393 .067 .115 .286 .049 .083 Base + CE + FT + BERT (single) .868 .818 .842 .676 .636 .655 .538 .442 .485 .533 .438 .48 .752 .677 .713 .659 .594 .625 .000 .000 .000 .000 .000 .000 CE + FT + BERT .871 .814 .842 .687 .642 .664 .568 .411 .477 .543 .393 .456 .741 .672 .705 .648 .587 .616 .232 .159 .188 .179 .122 .145 Base + CE + FT + BERTnewly .852 .837 .845 .663 .652 .658 .681 .420 .519 .609 .375 .464 .785 .626 .697 .701 .559 .622 .333 .183 .236 .244 .134 .173 (c) document level, coarse grained entity type CE + FT + BERT .872 .828 .849 .772 .733 .752 .433 .696 .534 .428 .688 .527 Base + CE + FT + BERTnewly .869 .872 .871 .78 .782 .781 .755 .357 .485 .755 .357 .485 Base + CE + FT + BERT (single) .89 .856 .873 .807 .776 .791 .699 .424 .528 .691 .420 .522 (d) document level, fine grained entity type Baseline .838 .693 .758 .644 .533 .583 .564 .196 .291 .538 .187 .278 .799 .531 .638 .733 .487 .585 .267 .049 .082 .267 .049 .082 Base .822 .672 .739 .594 .486 .534 .446 .513 .477 .419 .482 .448 .738 .6 .662 .657 .534 .589 .512 .250 .336 .350 .171 .230 Base + CE .809 .752 .78 .586 .546 .565 .521 .223 .313 .521 .223 .313 .743 .618 .675 .65 .541 .59 .35 .171 .23 .275 .134 .180 Base + CE + FT .811 .722 .764 .599 .534 .565 .54 .362 .433 .507 .339 .406 .759 .603 .672 .684 .544 .606 .453 .177 .254 .406 .159 .228 Base + CE + BERT .885 .799 .84 .696 .629 .661 .654 .304 .415 .654 .304 .415 .719 .686 .702 .625 .596 .610 .304 .104 .155 .250 .085 .127 Base + CE + BERTnewly .896 .790 .840 .675 .595 .633 .568 .223 .321 .568 .223 .321 .808 .603 .690 .696 .520 .595 .000 .000 .000 .000 .000 .000 Base + CE + FT + BERT .883 .800 .839 .717 .649 .682 .741 .371 .494 .679 .339 .452 .794 .631 .703 .715 .568 .633 .341 .183 .238 .318 .171 .222 Base + CE + FT + BERTnewly .881 .841 .861 .703 .671 .687 .705 .384 .497 .689 .375 .486 .792 .644 .71 .704 .572 .631 .233 .043 .072 .067 .012 .021 Base + CE + FT + BERT (single) .882 .853 .867 .729 .704 .716 .741 .357 .482 .741 .357 .482 .734 .726 .73 .650 .642 .646 .438 .299 .355 .393 .268 .319</figDesc><table coords="8,77.39,161.22,460.58,519.55"><row><cell></cell><cell></cell><cell></cell><cell cols="14">(a) multi-segment level, coarse grained entity type</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Literal coarse</cell><cell></cell><cell></cell><cell cols="4">Metonymic coarse</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Configuration</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fuzzy</cell><cell></cell><cell></cell><cell>Strict</cell><cell></cell><cell></cell><cell>Fuzzy</cell><cell></cell><cell></cell><cell>Strict</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="12">.825 .721 .769 .693 .606 .646 .541 .179 .268 .541 .179 .268</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Base</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="12">.776 .69 .73 .618 .55 .582 .5 .424 .459 .495 .42 .454</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Base + CE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="12">.806 .739 .771 .649 .594 .62 .552 .379 .45 .545 .375 .444</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Base + CE + FT</cell><cell></cell><cell></cell><cell></cell><cell cols="12">.789 .78 .784 .65 .642 .646 .481 .339 .398 .468 .33 .387</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Base + CE + BERT</cell><cell></cell><cell></cell><cell cols="12">.886 .801 .841 .782 .707 .743 .424 .397 .41 .41 .384 .396</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Base + CE + BERT -newly</cell><cell></cell><cell cols="12">.859 .818 .838 .719 .685 .702 .417 .384 .4 .417 .384 .4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Base + CE + FT + BERT</cell><cell></cell><cell cols="12">.866 .836 .851 .767 .739 .753 .664 .362 .468 .656 .357 .462</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Literal fine Fuzzy Base + Configuration Strict</cell><cell></cell><cell></cell><cell cols="4">Metonymic fine Fuzzy Strict</cell><cell></cell><cell></cell><cell cols="5">Component Fuzzy Strict</cell><cell></cell><cell cols="2">Fuzzy</cell><cell cols="2">Nested</cell><cell>Strict</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell></cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>Baseline</cell><cell cols="24">.838 .693 .758 .644 .533 .583 .564 .196 .291 .538 .187 .278 .799 .531 .638 .733 .487 .585 .267 .049 .082 .267 .049 .082</cell></row><row><cell>Base</cell><cell cols="24">.8 .67 .729 .548 .459 .499 .476 .451 .463 .472 .446 .459 .774 .531 .630 .692 .475 .563 .383 .140 .205 .333 .122 .179</cell></row><row><cell>Base + CE</cell><cell cols="24">.825 .708 .762 .562 .482 .519 .594 .366 .453 .594 .366 .453 .779 .556 .649 .720 .514 .600 .5 .067 .118 .364 .049 .086</cell></row><row><cell>Base + CE + FT</cell><cell cols="24">.801 .763 .781 .568 .541 .554 .567 .228 .325 .533 .214 .306 .762 .598 .67 .682 .535 .600 .425 .207 .279 .375 .183 .246</cell></row><row><cell>Base + CE + BERT</cell><cell cols="24">.889 .781 .831 .658 .578 .616 .532 .366 .434 .519 .357 .423 .803 .579 .673 .715 .515 .599 .000 .000 .000 .000 .000 .000</cell></row><row><cell>Base + CE + BERT -newly</cell><cell cols="24">.865 .748 .802 .613 .53 .568 .54 .241 .333 .54 .241 .333 .821 .504 .625 .732 .449 .557 .000 .000 .000 .000 .000 .000</cell></row><row><cell>Base + CE + FT + BERT</cell><cell cols="24">.866 .818 .842 .672 .634 .653 .702 .263 .383 .643 .241 .351 .804 .563 .662 .712 .499 .587 .357 .03 .056 .143 .012 .022</cell></row><row><cell>Base + + Fine-tuning (unfreezing) BERT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Base + CE + BERT</cell><cell cols="24">.877 .806 .840 .654 .600 .626 .434 .379 .405 .429 .375 .400 .77 .598 .673 .673 .523 .588 .267 .049 .082 .133 .024 .041</cell></row><row><cell>Base + CE + BERT -newly</cell><cell cols="24">.885 .782 .83 .672 .593 .63 .739 .29 .417 .705 .277 .397 .818 .524 .639 .745 .477 .582 .107 .018 .031 .071 .012 .021</cell></row><row><cell cols="4">Base + Configuration</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Literal coarse Fuzzy Strict</cell><cell></cell><cell></cell><cell cols="4">Metonymic coarse Fuzzy Strict</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="12">.825 .721 .769 .693 .606 .646 .541 .179 .268 .541 .179 .268</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Base</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="12">.812 .686 .743 .671 .566 .614 .444 .536 .486 .444 .536 .486</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Base + CE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="12">.802 .762 .782 .658 .625 .641 .575 .272 .370 .566 .268 .364</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Base + CE + FT</cell><cell></cell><cell></cell><cell></cell><cell cols="12">.815 .737 .774 .673 .608 .639 .510 .469 .488 .505 .464 .484</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Base + CE + BERT</cell><cell></cell><cell></cell><cell></cell><cell cols="12">.871 .831 .851 .779 .743 .760 .684 .232 .347 .684 .232 .347</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Base + CE + BERT -newly</cell><cell></cell><cell cols="12">.890 .828 .858 .788 .733 .759 .564 .277 .371 .545 .268 .359</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Base + Configuration</cell><cell cols="4">Literal fine Fuzzy Strict</cell><cell></cell><cell></cell><cell cols="4">Metonymic fine Fuzzy Strict</cell><cell></cell><cell></cell><cell cols="4">Component Fuzzy Strict</cell><cell></cell><cell cols="2">Fuzzy</cell><cell cols="2">Nested</cell><cell cols="2">Strict</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell></cell><cell>R</cell><cell>F</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,656.80,203.45,7.86"><p>https://github.com/ktodorov/eval-historical-texts.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,645.84,335.86,7.86;4,144.73,656.80,26.62,7.86"><p>https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus [accessed 2020-07-16].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,144.73,645.84,196.12,7.86"><p>https://github.com/impresso/CLEF-HIPE-2020.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,144.73,656.80,222.84,7.86"><p>https://github.com/impresso/CLEF-HIPE-2020-scorer.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Our model achieves its best performance on French followed by German, while we do not consider our English results to be yet useful for comparison, given the limitations of the dataset we used. A few results clearly emerge:</p><p>-Each embedding module contributes to the overall performance on most sub-tasks and evaluation metrics. Character-level and BERT embeddings are particularly important for performance, while in-domain FastText embeddings seem to help in particular for tags other than literal.</p><p>-Fine-tuning pre-trained embeddings in general does not improve performance, despite requiring more computation resources.</p><p>-A single-task approach performs better than multi-task in general, even if the differences are often minor. It must be noted that, with our setup, six single-task runs require 2.5 times more time on average to converge than one multi-task run using a document split. Instead, six single-task runs using a multi-segment split are as fast as one multi-task run. Comparing one-to-one, a single-task run is on average twice as fast.</p><p>-A document-level split of the data is in general better than a multi-segment split, highlighting how larger windows of context are helpful with our model.</p><p>When compared to the results of the other task participants <ref type="bibr" coords="11,416.85,118.99,9.96,8.74" target="#b4">[5]</ref>, our model performs well in general, and notably well on the NERC-Fine sub-task where we achieve best performance on several evaluation metrics and in particular a high precision.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,200.65,337.64,7.86;11,151.52,211.61,300.75,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,383.17,200.65,97.42,7.86;11,151.52,211.61,18.94,7.86">The Groningen Meaning Bank</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Evang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Venhuizen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bjerva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,192.35,211.61,136.40,7.86">Handbook of linguistic annotation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="463" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,222.15,337.64,7.86;11,151.52,233.11,329.07,7.86;11,151.52,244.07,329.07,7.86;11,151.52,255.03,329.07,7.86;11,151.52,265.99,122.56,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,357.15,222.15,123.44,7.86;11,151.52,233.11,247.64,7.86">An embarrassingly simple approach for transfer learning from pretrained language models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chronopoulou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Baziotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,421.57,233.11,59.02,7.86;11,151.52,244.07,329.07,7.86;11,151.52,255.03,215.61,7.86">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2089" to="2095" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="11,142.96,276.53,337.64,7.86;11,151.52,287.49,329.07,7.86;11,151.52,298.45,329.07,7.86;11,151.52,309.41,329.07,7.86;11,151.52,320.37,329.07,7.86;11,151.52,331.33,205.24,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,364.92,276.53,115.67,7.86;11,151.52,287.49,213.60,7.86">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="11,385.79,287.49,94.80,7.86;11,151.52,298.45,329.07,7.86;11,151.52,309.41,174.01,7.86">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">Jun 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="11,142.96,341.88,337.64,7.86;11,151.52,352.83,329.07,8.12;11,151.52,364.44,28.24,7.47" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Colavizza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rochat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Kaplan</surname></persName>
		</author>
		<ptr target="https://infoscience.epfl.ch/record/221391" />
		<title level="m" coord="11,379.99,341.88,100.61,7.86;11,151.52,352.83,128.07,7.86">Diachronic evaluation of ner systems on old newspapers</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,374.34,337.64,7.86;11,151.52,385.30,329.07,7.86;11,151.52,396.26,329.07,7.86;11,151.52,407.22,329.07,7.86;11,151.52,418.18,72.95,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,391.69,374.34,88.90,7.86;11,151.52,385.30,287.04,7.86">Extended Overview of CLEF HIPE 2020: Named Entity Processing on Historical Newspapers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Romanello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Flückiger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clematide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,367.71,396.26,112.88,7.86;11,151.52,407.22,324.27,7.86">CLEF 2020 Working Notes. Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,428.72,337.64,7.86;11,151.52,439.68,329.07,7.86;11,151.52,450.64,329.07,7.86;11,151.52,461.60,261.38,8.12" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,264.71,428.72,215.89,7.86;11,151.52,439.68,99.02,7.86">Robust lexical features for improved neural network named-entity recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ghaddar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Langlais</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C18-1161" />
	</analytic>
	<monogr>
		<title level="m" coord="11,271.12,439.68,209.47,7.86;11,151.52,450.64,105.41,7.86">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-08">Aug 2018</date>
			<biblScope unit="page" from="1896" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,469.88,337.63,10.13;11,151.52,483.10,329.07,7.86;11,151.52,494.06,329.07,7.86;11,151.52,505.02,60.92,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,360.74,472.15,119.85,7.86;11,151.52,483.10,49.79,7.86">Bag of tricks for efficient text classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">É</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,224.35,483.10,256.25,7.86;11,151.52,494.06,201.48,7.86">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct coords="11,142.96,515.57,337.64,7.86;11,151.52,526.53,329.07,7.86;11,151.52,537.49,329.07,7.86;11,151.52,548.45,185.99,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,342.14,515.57,138.45,7.86;11,151.52,526.53,232.10,7.86">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,405.51,526.53,75.09,7.86;11,151.52,537.49,232.15,7.86;11,439.15,537.49,37.86,7.86">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001-06">Jun 2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
	<note>ICML &apos;01</note>
</biblStruct>

<biblStruct coords="11,142.96,558.99,337.63,7.86;11,151.52,569.95,329.07,7.86;11,151.52,580.91,329.07,7.86;11,151.52,591.87,329.07,7.86;11,151.52,602.83,251.55,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,453.69,558.99,26.90,7.86;11,151.52,569.95,166.66,7.86">Neural architectures for named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1030" />
	</analytic>
	<monogr>
		<title level="m" coord="11,338.59,569.95,142.00,7.86;11,151.52,580.91,329.07,7.86;11,151.52,591.87,145.33,7.86">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06">Jun 2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,613.37,337.98,7.86;11,151.52,624.98,193.00,7.47" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rk6qdGgCZ" />
		<title level="m" coord="11,253.74,613.37,180.12,7.86">Fixing Weight Decay Regularization in Adam</title>
		<imprint>
			<date type="published" when="2018-02">Feb 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,634.88,337.97,7.86;11,151.52,645.81,329.07,7.89;11,151.52,656.80,164.15,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,259.76,634.88,141.67,7.86">A survey on transfer learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2009.191</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2009.191" />
	</analytic>
	<monogr>
		<title level="j" coord="11,414.56,634.88,66.03,7.86;11,151.52,645.84,188.50,7.86">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010-10">Oct 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,119.67,337.97,7.86;12,151.52,130.61,329.07,7.89;12,151.52,141.59,228.78,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,225.74,119.67,212.71,7.86">Natural language processing for historical texts</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Piotrowski</surname></persName>
		</author>
		<idno type="DOI">10.2200/S00436ED1V01Y201207HLT017</idno>
		<ptr target="https://doi.org/10.2200/S00436ED1V01Y201207HLT017" />
	</analytic>
	<monogr>
		<title level="j" coord="12,449.87,119.67,30.72,7.86;12,151.52,130.63,212.06,7.86">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="157" />
			<date type="published" when="2012-09">Sep 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,152.55,337.98,7.86;12,151.52,163.51,22.02,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="12,195.09,152.55,236.90,7.86">Neural Transfer Learning for Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-02">Feb 2019</date>
			<biblScope unit="page">329</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,174.47,337.98,7.86;12,151.52,185.43,329.07,7.86;12,151.52,196.39,158.58,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,284.85,174.47,195.74,7.86;12,151.52,185.43,206.14,7.86">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">T K</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,382.59,185.43,98.00,7.86;12,151.52,196.39,16.79,7.86">Proceedings of CoNLL-2003</title>
		<meeting>CoNLL-2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="142" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,207.34,337.97,7.86;12,151.52,218.30,329.07,7.86;12,151.52,229.26,329.07,7.86;12,151.52,240.22,261.34,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,300.02,207.34,180.57,7.86;12,151.52,218.30,20.22,7.86">How to Fine-Tune BERT for Text Classification?</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32381-3_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32381-316" />
	</analytic>
	<monogr>
		<title level="s" coord="12,386.25,218.30,94.35,7.86;12,151.52,229.26,41.98,7.86;12,247.01,229.26,138.73,7.86">Chinese Computational Linguistics</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="194" to="206" />
			<date type="published" when="2019">2019</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
	<note>Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct coords="12,142.62,251.18,337.98,7.86;12,151.52,262.14,329.07,7.86;12,151.52,273.10,329.07,8.12;12,151.52,284.06,200.23,8.11" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="12,332.99,262.14,147.60,7.86;12,151.52,273.10,159.21,7.86">HuggingFace&apos;s Transformers: Stateof-the-art Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771[cs</idno>
		<idno>arXiv:</idno>
		<ptr target="1910.03771" />
		<imprint>
			<date type="published" when="2020-02">Feb 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
