<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.88,115.96,323.60,12.62;1,154.34,133.89,306.68,12.62;1,228.52,151.82,158.32,12.62">Hybrid Statistical and Attentive Deep Neural Approach for Named Entity Recognition in Historical Newspapers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,235.83,189.49,64.96,8.74"><forename type="first">Ghaith</forename><surname>Dekhili</surname></persName>
							<email>dekhili.ghaith@courrier.uqam.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Quebec in Montreal</orgName>
								<address>
									<addrLine>201 President Kennedy avenue</addrLine>
									<postCode>H2X 3Y7</postCode>
									<settlement>Montreal</settlement>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.49,189.49,56.04,8.74"><forename type="first">Fatiha</forename><surname>Sadat</surname></persName>
							<email>sadat.fatiha@uqam.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Quebec in Montreal</orgName>
								<address>
									<addrLine>201 President Kennedy avenue</addrLine>
									<postCode>H2X 3Y7</postCode>
									<settlement>Montreal</settlement>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.88,115.96,323.60,12.62;1,154.34,133.89,306.68,12.62;1,228.52,151.82,158.32,12.62">Hybrid Statistical and Attentive Deep Neural Approach for Named Entity Recognition in Historical Newspapers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C65EADF040B491B7A11F57CD3C1B6BA1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Neural Networks</term>
					<term>Attention Mechanism</term>
					<term>Contextualized Word Embeddings</term>
					<term>Character Embeddings</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural networks-based models have proved their efficiency on Named Entities Recognition, one of the well-known NLP task. Besides, attention mechanism has become an integral part of compelling sequence modeling and transduction models on various tasks. This technique allows context representation in a sequence by taking into consideration neighboring words. In this study, we propose an architecture that involves BiLSTM layers combined with a CRF layer and an attention layer in between. This was augmented with pre-trained contextualized word embeddings and dropout layers. Moreover, apart from using word representations, we use character-based representations, extracted by CNN layers, to capture morphological and orthographic information. Our experiments show an improvement in the overall performance. We notice that our attentive neural model augmented with contextualized word embeddings gives higher scores compared to our baselines. To the best of our knowledge, there is no study which combines the application of attention mechanism and contextualized word embeddings on NER and historical newspapers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This work is done as part of the HIPE (Identifying Historical People, Places and other Entities) shared task, "organised as a CLEF 2020 evaluation Lab and dedicated to the evaluation of named entity processing on historical newspapers in French, German and English" <ref type="bibr" coords="1,283.35,581.15,14.61,8.74" target="#b10">[11]</ref>. The shared task is organized as part of "impresso Media Monitoring of the Past", a project focused on information extraction in historical newspapers. 1  Named Entity Recognition and Classification (NERC) is a sub-task of information extraction and Natural Language Processing (NLP). It consists on identifying certain textual objects such as names of persons, organizations and places.</p><p>Early NER systems were based on handcrafted rules, lexicons, orthographic features and external knowledge resources. This was followed by "feature engineering based NER systems" and machine learning <ref type="bibr" coords="2,356.36,190.72,14.61,8.74" target="#b24">[25]</ref>. Starting with <ref type="bibr" coords="2,437.48,190.72,9.96,8.74" target="#b6">[7]</ref>, neural networks based systems with a minimum of feature engineering have become popular. Such models are interesting because they typically do not need domain specific resources like in earlier systems, and are thus qualified to be more domain independent. Many neural architectures have been introduced, most of them based on some form of Recurrent Neural Networks (RNN) over characters, sub-words and/or word embeddings <ref type="bibr" coords="2,293.87,262.46,14.61,8.74" target="#b29">[30]</ref>.</p><p>NER systems based on knowledge do not need labeled data as they rely on lexicon resources and domain specific knowledge. These systems perform well in cases where the lexicon is exhaustive, but fail when the information does not exist in domain dictionaries <ref type="bibr" coords="2,262.19,310.28,14.61,8.74" target="#b29">[30]</ref>. A second drawback of these systems is that they require domain experts to construct and maintain the knowledge resources. Finally, these systems can be used only on domains and languages for which they were designed, because of the specific features they had learned during training <ref type="bibr" coords="2,134.77,358.10,14.61,8.74" target="#b11">[12]</ref>.</p><p>Supervised machine learning models learn how to make predictions during training on couples of inputs and their expected outputs, and can be used in place of handcrafted rules <ref type="bibr" coords="2,250.55,393.96,14.61,8.74" target="#b29">[30]</ref>.</p><p>NER task becomes more challenging when applied on historical and cultural heritage collections. On the one side, inputs can be extremely noisy, with errors which differ from the ones in tweet misspellings or speech transcription hesitations <ref type="bibr" coords="2,158.63,441.78,15.50,8.74" target="#b21">[22,</ref><ref type="bibr" coords="2,176.90,441.78,7.75,8.74" target="#b4">5,</ref><ref type="bibr" coords="2,187.43,441.78,11.62,8.74" target="#b27">28]</ref>. On the other side, the language that we have is mostly of earlier stage, "which renders usual external and internal evidences less effective (e.g., the usage of different naming conventions and presence of historical spelling variations)" <ref type="bibr" coords="2,172.92,477.65,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="2,186.67,477.65,7.01,8.74" target="#b2">3]</ref>. Finally, "archives and texts from the past are not as anglophone as in today's information society, making multilingual resources and processing capacities even more essential" <ref type="bibr" coords="2,273.44,501.56,15.50,8.74" target="#b25">[26,</ref><ref type="bibr" coords="2,292.50,501.56,11.62,8.74" target="#b10">11]</ref>. In this context, the objective of CLEF HIPE 2020 shared task is threefold: strengthening the robustness of existing approaches on non-standard inputs; enabling performance comparison of NE processing on historical texts; and in the long run, fostering efficient semantic indexing of historical documents in order to support scholarship on digital cultural heritage collections <ref type="bibr" coords="2,238.13,579.40,14.61,8.74" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Main NER approaches are based on computational linguistics and machine learning. <ref type="bibr" coords="2,153.55,644.16,15.50,8.74" target="#b12">[13]</ref> proposed ProMiner which is based on a dictionary of synonyms to identify genes and proteins mentions in text and link them to their corresponding ids in the dictionary. <ref type="bibr" coords="3,231.62,118.99,15.50,8.74" target="#b26">[27]</ref> presented an approach based on dictionaries as well for NER in the medical domain. There are other well-known rules based NER systems such as LaSIE-II <ref type="bibr" coords="3,247.23,142.90,14.61,8.74" target="#b14">[15]</ref>, NetOwl <ref type="bibr" coords="3,305.41,142.90,15.50,8.74" target="#b16">[17]</ref> and FASTUS <ref type="bibr" coords="3,385.20,142.90,9.96,8.74" target="#b0">[1]</ref>. These systems are mainly based on semantic and syntactic rules to recognize entities <ref type="bibr" coords="3,425.95,154.86,14.61,8.74" target="#b19">[20]</ref>.</p><p>Among machine learning applied techniques, we quote Hidden Markov Model (HMM), Maximum Entropy, decision trees, Support Vector Machines (SVM) and Conditional Random Fields (CRF). <ref type="bibr" coords="3,309.86,191.29,15.50,8.74" target="#b17">[18]</ref> proposed a CRF model and include morphological features, Part-Of-Speech (POS) Tags, and words sequences. <ref type="bibr" coords="3,465.09,203.25,15.50,8.74" target="#b15">[16]</ref> used a CRF too, and show that using Word2Vec pre-trained word embeddings improves NER models performances.</p><p>On the other hand, neural networks based models have proved their efficiency on NER tasks. Long Short-Term Memory (LSTM) <ref type="bibr" coords="3,363.09,251.64,15.50,8.74" target="#b13">[14]</ref> based neural networks have been widely used in different NLP applications thanks to their ability to detect long-term dependencies. These models showed good results compared to traditional approaches, even if they do not need dictionaries, Gazetteers or other additional information. <ref type="bibr" coords="3,237.80,299.46,10.52,8.74" target="#b5">[6]</ref> presented a hybrid model by combining Bidirectional LSTM (BiLSTM) and a Convolutional Neural Network (CNN). <ref type="bibr" coords="3,415.23,311.41,15.50,8.74" target="#b18">[19]</ref> introduced a neural model similar to <ref type="bibr" coords="3,245.28,323.37,9.96,8.74" target="#b5">[6]</ref>, based on BiLSTM combined with a CRF. <ref type="bibr" coords="3,442.90,323.37,15.50,8.74" target="#b22">[23]</ref> used the attention mechanism to develop a model which takes advantage of sentence level and document level hierarchical contextual representations. <ref type="bibr" coords="3,420.14,347.28,10.52,8.74" target="#b7">[8]</ref> introduced BERT, acronym of Bidirectional Encoder Representations from Transformers, "which is a language model designed for pre-training deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers" <ref type="bibr" coords="3,233.25,395.10,9.96,8.74" target="#b7">[8]</ref>. The model obtained new state-of-the-art results on eleven natural language processing tasks. For more details on related work we refer the reader to <ref type="bibr" coords="3,217.62,419.01,14.61,8.74" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background on some Supervised ML Models</head><p>In this section we present some supervised machine learning models used in this research, such as LSTM operating model, BiLSTM which is the combination of two LSTMs, followed by a brief description of the CRF modelling model and its usefulness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Long Short-Term Memory model</head><p>LSTM is a RNN architecture used in the field of deep learning. "This powerful family of connectionist models can capture time dynamics via cycles in the graph" <ref type="bibr" coords="3,168.00,595.77,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="3,186.81,595.77,11.62,8.74" target="#b23">24]</ref>.</p><p>RNNs take as input a vectors sequence (x 1 , x 2 , ..., x n ) at time t and return the hidden state vectors sequence (h 1 , h 2 , ..., h n ), which stocks information learned in actual and previous steps. "Although RNNs can, in theory, learn long dependencies, in practice they fail to do so and tend to be biased towards their most recent inputs in the sequence" <ref type="bibr" coords="3,260.29,656.12,12.98,8.74" target="#b1">[2]</ref>. Thanks to their memory cells, LSTMs are able to resolve this point by capturing useful information from previous states <ref type="bibr" coords="4,450.40,118.99,15.74,8.74" target="#b13">[14]</ref>. A LSTM unit is updated at a time t using the following equations:</p><formula xml:id="formula_0" coords="4,248.95,155.46,231.64,9.65">i t = σ(W i h t-1 + U i x t + b i )<label>(1)</label></formula><formula xml:id="formula_1" coords="4,241.55,174.29,239.05,28.48">f t = σ(W f h t-1 + U f x t + b f ) (2) ct = tanh(W c h t-1 + U c x t + b c )<label>(3)</label></formula><formula xml:id="formula_2" coords="4,258.28,211.96,222.31,9.65">c t = f t c t-1 + i t ct<label>(4)</label></formula><formula xml:id="formula_3" coords="4,246.58,230.79,234.02,9.65">o t = σ(W o h t-1 + U o x t + b o )<label>(5)</label></formula><formula xml:id="formula_4" coords="4,268.08,249.62,212.51,9.65">h t = o t tanh(c t )<label>(6)</label></formula><p>where:</p><p>• σ is the element-wise sigmoid function • is the element-wise product • x t is the input vector at time t • h t is the hidden state (could be called output) vector stocking useful information at (and before) time t</p><formula xml:id="formula_5" coords="4,141.74,351.20,317.67,33.86">• U i , U f , U c , U o are the weight matrices of different gates for input x t • W i ,W f ,W c ,W o denote the weight matrices for hidden state h t • b i , b f , b c , b o</formula><p>are the bias vectors <ref type="bibr" coords="4,149.71,397.93,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="4,168.52,397.93,11.62,8.74" target="#b23">24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bidirectional LSTM</head><p>A LSTM computes a hidden state vector -→ h t representative of the left context in a sentence at every step t <ref type="bibr" coords="4,249.91,461.19,14.61,8.74" target="#b18">[19]</ref>. To take advantage of information that we could get from treating the same sentence in reverse, <ref type="bibr" coords="4,338.60,473.15,10.52,8.74" target="#b8">[9]</ref> proposed the BiLSTM model. The idea is to use an another LSTM, to generate a second hidden state vector ←h t representative of the right context in the sentence. Concatenating these two vectors leads to a representation ht=[ -→ h t ; ←h t ] of the word in its general context. The resulting representation is useful for numerous tagging applications <ref type="bibr" coords="4,451.27,520.97,14.61,8.74" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CRF</head><p>"A very simple but surprisingly effective tagging model is to use the h t 's as features to make independent tagging decisions for each output y t " <ref type="bibr" coords="4,408.67,584.24,15.50,8.74" target="#b20">[21]</ref>. In sequence labeling tasks, taking into consideration neighboring labels could be helpful while analyzing a given input sentence, like in some "grammar" rules where a noun more likely follows an adjective than a verb, this can be equivalent in NER to the fact that I-ORG cannot follow I-PERS <ref type="bibr" coords="4,324.13,632.06,14.61,8.74" target="#b23">[24]</ref>.</p><p>Therefore, as in the research presented by <ref type="bibr" coords="4,346.50,644.16,14.61,8.74" target="#b18">[19]</ref>, we model label sequence jointly using a CRF, instead of modeling them independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Extracting Character Features Using a CNN</head><p>CNN layers have become ubiquitous in many NLP tasks. As in <ref type="bibr" coords="5,418.62,143.23,9.96,8.74" target="#b5">[6]</ref>, we use for each word a convolution and a max layer to extract a per-character feature and optionally the character type to obtain at the end a new character embedding with resulting features. A special token "PADDING" has been used on both sides of words to keep the same length of sequences. In this section we present a brief description of CNN applied to text <ref type="bibr" coords="5,321.67,203.01,14.61,8.74" target="#b30">[31]</ref>. Input Layer An input sequence x with n elements, each one is represented by a d-dimensional vector, can be represented as a map of features of dimensionality d x n. The bottom of figure <ref type="figure" coords="5,256.70,502.31,4.98,8.74" target="#fig_0">1</ref> shows the input layer as a rectangle with multiple columns <ref type="bibr" coords="5,173.56,514.26,14.61,8.74" target="#b30">[31]</ref>.</p><p>Convolution Layer Convolution layer is employed to represent learning by sliding w-grams over an input sequence (x 1 , x 2 , ..., x n ). We consider a vector c i R wd as the concatenated embeddings of w entries (x i-w+1 , ..., x i ), where w is the filter width and 0 &lt; i &lt; s + w. We pad embeddings of x i , where i &lt; 1 or i &gt; n, with zeros. We then represent the w-grams (x i-w+1 , ..., x i ) by a new vector p i R d using the convolution weights W R d * wd :</p><formula xml:id="formula_6" coords="5,265.27,635.60,215.33,9.65">p i = tanh(W c i + b) (7)</formula><p>where b R d is the bias <ref type="bibr" coords="5,241.41,656.12,14.61,8.74" target="#b30">[31]</ref>.</p><p>Maxpooling We use w-gram representations pi (i = 1...s+w-1) to generate the input sequence x representation by applying maxpooling: x j = max(p 1,j , p 2,j , ...) where (j = 1, ..., d) <ref type="bibr" coords="6,219.92,142.90,14.61,8.74" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Attention mechanism</head><p>"The attention mechanism has become an integral part of compelling sequence modeling and transduction models in various tasks" <ref type="bibr" coords="6,361.59,203.82,16.44,8.74" target="#b28">[29]</ref>. This technique allows to represent the context in a sequence by taking into consideration neighboring words <ref type="bibr" coords="6,163.16,227.73,14.61,8.74" target="#b28">[29]</ref>. For more details on attention mechanism we refer the reader to <ref type="bibr" coords="6,462.32,227.73,14.61,8.74" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Proposed Approach</head><p>This study aims to compare the effectiveness of our proposed attentive neural approach in recognizing and classifying NEs in historical newspapers with a comparison to a statistical model augmented with orthographic features and two other neural models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Statistical approach</head><p>Statistical approaches based on CRF, SVM or Perceptron have proven good performances using only handcrafted features in many NLP tasks such as NERC <ref type="bibr" coords="6,134.77,391.41,9.96,8.74" target="#b5">[6]</ref>. In our work, we use CRFsuite<ref type="foot" coords="6,284.34,389.83,3.97,6.12" target="#foot_0">2</ref> implementation of CRF provided by HIPE team. Among all CRFs implementations, CRFsuite is the fastest one for training the model and labeling data.</p><p>In our baseline we use orthographic basic spelling features extracted from words such as prefix and suffix, the casing of the initial character, and whether it is a digit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural network approach</head><p>In this section we present our NER neural model followed by a brief description of used input embeddings and additional features.</p><p>Proposed NER Model As in <ref type="bibr" coords="6,272.74,541.56,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="6,285.68,541.56,11.62,8.74" target="#b18">19]</ref>, we use in our architecture BiLSTM layers for the extraction of word-level features. These layers are followed by an attention layer. We also use a CRF layer on the top of our model, augmented with some features such as dropout layers. Figure <ref type="figure" coords="6,314.20,577.43,4.98,8.74">2</ref> presents our proposed architecture. Apart from using word representations, we also use character representations to extract morphological and orthographic features. As shown in Figure <ref type="figure" coords="6,447.30,601.34,3.87,8.74">2</ref>, word embeddings are given to a BiLSTM. l i and r i represent the word i in its left and right contexts respectively. The concatenation of these two vectors represent the word's context c i <ref type="bibr" coords="6,212.49,637.20,14.61,8.74" target="#b18">[19]</ref>.</p><p>Fig. <ref type="figure" coords="7,154.40,385.72,4.13,7.89">2</ref>. The main architecture of our NER system using BiLSTM, attention and CRF layers <ref type="bibr" coords="7,160.69,396.71,9.73,7.86" target="#b5">[6,</ref><ref type="bibr" coords="7,173.49,396.71,10.75,7.86" target="#b18">19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Embeddings</head><p>The input layers of our model are vector representations of words. "Learning independent representations for word types from the limited NER training data is a difficult problem: there are simply too many parameters to reliably estimate" <ref type="bibr" coords="7,227.89,500.70,14.61,8.74" target="#b18">[19]</ref>. In our study, we use pre-trained contextualized word embeddings to initialize our look-up table and to enrich our training dataset.</p><p>In our experiments, we use indomain Flair embeddings provided by HIPE organizers. "These embeddings were computed with a context of 250 characters, 1 hidden layer of size 2048, and a dropout of 0.1. Input was normalized with lowercasing, replacement of digits by 0, everything else was kept as in the original text" <ref type="bibr" coords="7,159.79,572.43,14.61,8.74" target="#b10">[11]</ref>. Extracting character-level representations allows us to take advantage of features related to the domain in hand. Following <ref type="bibr" coords="7,365.44,584.39,9.96,8.74" target="#b5">[6]</ref>, we use a CNN layer to represent each word based on its characters. We initialize a lookup table randomly with values between -0.5 and 0.5 to generate a character representation of 25 dimensions. The character set is formed by all characters present in the dataset, with PADDING and UNKNOWN tokens, used for the CNN and all other characters respectively. Figure <ref type="figure" coords="7,299.72,644.16,4.98,8.74">3</ref>  <ref type="bibr" coords="7,308.98,644.16,10.52,8.74" target="#b5">[6]</ref> presents an example where we give the word "Picasso" characters embeddings to a CNN. Fig. <ref type="figure" coords="8,154.40,300.69,4.13,7.89">3</ref>. An architecture using character embeddings of the word "Picasso" in CNN <ref type="bibr" coords="8,468.31,300.72,9.22,7.86" target="#b5">[6]</ref>.</p><p>Additional features As information related to capitalisation has been removed during word embeddings' map construction. We used a separate look-up table to add this feature with the following options: allCaps (the word is in capital letters), upperInitial (only the first letter is capitalized), lowercase (the word is lower cased) and mixedCaps (capital and small letters are mixed) <ref type="bibr" coords="8,442.37,383.39,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="8,456.91,383.39,7.01,8.74" target="#b5">6]</ref>. In our work, we used additional character-based features as well, by using a lookup table, to generate a vector which represent the character's type (uppercase, lowercase, punctuation or other).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Evaluations</head><p>In this section we present our experiments and the results obtained with different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task Description</head><p>The CLEF HIPE 2020 shared task includes two NE processing tasks with subtasks of different level of difficulty. In our work we participate in NERC coarsegrained sub-task of the NERC task. This task includes the recognition and classification of entity mentions according to high-level entity types. In our case the types used for annotations are: LOC, ORG, PERS, PROD and TIME.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training data</head><p>"The shared task corpus is composed of digitized and OCRed articles originating from Swiss, Luxembourgish and American historical newspaper collections and selected on a diachronic basis" <ref type="bibr" coords="9,269.36,118.99,16.14,8.74" target="#b10">[11]</ref>. <ref type="foot" coords="9,289.54,117.42,3.97,6.12" target="#foot_1">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training and implementation details</head><p>As in <ref type="bibr" coords="9,162.36,329.11,10.52,8.74" target="#b5">[6]</ref> we use in our experiments the IOB tagging scheme which stands for Begin, Inside and Outside. This schema allows us to mark the position of the word in the named entity. We implement our model using Keras library with Tensorflow as a backend. As in <ref type="bibr" coords="9,279.93,364.98,9.96,8.74" target="#b5">[6]</ref>, we initialize LSTM states with zero vectors. Except for the character and word embeddings whose initializations have been described previously, we initialize all lookup tables randomly. We train our model with mini-batchs using nadam optimization algorithm. As in <ref type="bibr" coords="9,426.92,400.85,15.50,8.74" target="#b18">[19]</ref> we use a single layer for both forward and backward LSTMs and we apply dropout layers to make our model learn from word and character features. Furthermore, applying dropout was effective in reducing overfitting and improving our model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation Measures</head><p>NERC task in CLEF HIPE 2020 shared task is evaluated in terms of Precision, Recall and F-measure (F1). Evaluation is done at entity level according to two metrics: micro average, with the consideration of all TP, FP, and FN<ref type="foot" coords="9,232.01,540.31,3.97,6.12" target="#foot_2">4</ref> over all documents, and macro average, with the average of document's micro figures. NERC benefits from strict and fuzzy evaluation regimes. For NERC, the strict regime corresponds to exact boundary matching and the fuzzy to overlapping boundaries <ref type="bibr" coords="9,418.89,577.75,14.61,8.74" target="#b10">[11]</ref>.</p><p>For more details on evaluation metrics we refer the reader to <ref type="bibr" coords="9,403.37,601.89,14.61,8.74" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Models Evaluation</head><p>In this section we evaluate different used models which have been already described above. In table <ref type="table" coords="10,237.24,149.93,4.98,8.74" target="#tab_1">2</ref> we cite main differences between models. Results Tables <ref type="table" coords="10,206.07,316.34,7.75,8.74" target="#tab_2">3,</ref><ref type="table" coords="10,216.15,316.34,3.87,8.74" target="#tab_3">4</ref>, 5 and 6 show a comparison of results obtained with different studied models.</p><p>Discussion According to the results presented in tables 3 and 4, we notice on the one hand that the use of in domain contextualized word embeddings and attention mechanism lead to a higher F-measure for LOC, PROD and TIME entities compared to all other models in both fuzzy and strict regimes. On the other hand the statistical model augmented with orthographic features performs better in both ORG and PERS entities, this could be explained by the importance of syntactic information provided by these features and the large portion of information that they encode which are essential in the NERC task. Now if we consider metonymic sense, according to table 5, all neural models perform better than the statistical model augmented with orthographic features in both regimes, and our model has higher scores than the two other neural models, except model 3 which has higher F-measure in strict regime. Moreover, even if other models have higher precision, our model showed higher recall which lead to a higher F-measure. We are convinced by the fact that "actively tackling the problem of OCR noise and hyphenation issues helps to achieve better recall" <ref type="bibr" coords="10,461.21,524.61,15.51,8.74" target="#b10">[11]</ref>. These results show that neural models especially our proposed model, where we use contextualized word embeddings and attention mechanism, perform far better than the statistical model on all entities when it is about metonymic sense. Now if we consider table <ref type="table" coords="10,265.08,572.43,3.87,8.74" target="#tab_5">6</ref>, we notice that model 2 and model 3 perform better than the statistical model and barely better than our proposed model on the ORG entity, which shows that these models were more able to generalize on test data in this stage.</p><p>All these improvements prove the efficiency of our neural model architecture and of different features used in training, especially contextualized word embeddings trained on large quantities of raw data and character embeddings extracted from specific domain dataset. Therefore, our neural model is able to extract necessary knowledge from training data, without using handcrafted features.</p><p>An important aspect of the CLEF HIPE 2020 shared task corpus, and for historical newspaper data in general, is the noise generated by OCR. As reported in <ref type="bibr" coords="11,146.56,166.81,14.61,8.74" target="#b10">[11]</ref>, noisy mentions affect remarkably the model's performance: "little noise as 0.1 severely hurts the system's ability to predict an entity and may cut its performance by half" <ref type="bibr" coords="11,227.59,190.72,15.83,8.74" target="#b10">[11]</ref>. In our study, we do not report results obtained on the dev set as in the final step, after using dev set to fine tune our model's parameters, we used train and dev sets for training. However, we would to confirm the degradation of our model's performance, caused in part by the fact that "11 % of all mentions in test set contain OCR mistakes" <ref type="bibr" coords="11,400.02,238.55,17.27,8.74" target="#b10">[11]</ref>.       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we presented a hybrid approach for NERC applied on historical newspapers. In our experiments, we used orthographic features related to words syntax. Besides, we used word and character embeddings, which allow us to detect morphological and orthographic features related to a specific domain. Our experiments show an improvement in the overall performance. We notice that our attentive neural model augmented with contextualized word embeddings performs better compared to our baselines overall. To the best of our knowledge, there is no study which combines the application of attention mechanism and contextualized word embeddings in NERC for historical newspapers domain.</p><p>As a future work, we aim to investigate the usefulness of adding additional features in the hybrid architecture and the use of external resources such as ontologies and other knowledge and common sense bases. Applying multi-task learning will be part of our future work, as well. Moreover, it would be relevant to apply explainability techniques on the neural network models in order to better explain and analyze the results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,209.85,421.56,192.58,7.89;5,208.47,236.70,198.42,170.09"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Basic representation of CNN layers [31].</figDesc><graphic coords="5,208.47,236.70,198.42,170.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="13,201.83,530.43,6.12,21.55;13,201.83,523.78,6.12,3.97;13,201.83,427.73,6.12,21.55;13,201.83,421.07,6.12,3.97;13,201.83,322.17,6.12,21.55;13,201.83,315.52,6.12,3.97;13,201.83,226.95,6.12,13.63;13,201.83,203.33,6.12,20.93;13,209.80,553.24,6.12,20.17;13,209.80,502.68,6.12,19.54;13,209.80,450.54,6.12,20.16;13,209.80,398.07,6.12,19.54;13,209.80,344.98,6.12,20.16;13,209.80,292.51,6.12,19.54;13,209.80,239.42,6.12,20.16;13,209.80,184.85,6.12,19.54;13,222.57,591.04,6.12,19.31;13,222.57,577.60,6.12,5.36;13,222.57,560.43,6.12,5.78;13,222.57,541.81,6.12,9.11;13,222.57,526.73,6.12,5.36;13,222.57,509.55,6.12,5.78;13,222.57,490.93,6.12,9.11;13,222.57,475.85,6.12,5.36;13,222.57,458.68,6.12,5.78;13,222.57,439.11,6.12,9.11;13,222.57,423.07,6.12,5.36;13,222.57,405.90,6.12,5.78;13,222.57,386.32,6.12,9.11;13,222.57,370.29,6.12,5.36;13,222.57,353.12,6.12,5.78;13,222.57,333.55,6.12,9.11;13,222.57,317.51,6.12,5.36;13,222.57,300.34,6.12,5.78;13,222.57,280.76,6.12,9.11;13,222.57,264.73,6.12,5.36;13,222.57,247.56,6.12,5.78;13,222.57,227.98,6.12,9.11;13,222.57,209.85,6.12,5.36;13,222.57,190.57,6.12,5.78;13,222.57,170.99,6.12,9.11;13,235.35,591.78,6.12,17.85;13,235.35,573.20,6.12,14.17;13,235.35,558.22,6.12,10.20;13,235.35,541.27,6.12,10.20;13,235.35,522.32,6.12,14.17;13,235.35,507.35,6.12,10.20;13,235.35,490.39,6.12,10.20;13,235.35,471.44,6.12,14.17;13,235.35,454.49,6.12,14.16;13,235.33,435.62,6.14,16.07;13,235.35,418.66,6.12,14.17;13,235.35,401.70,6.12,14.17;13,235.33,382.84,6.14,16.07;13,235.35,365.88,6.12,14.17;13,235.35,348.92,6.12,14.17;13,235.33,330.06,6.14,16.07;13,235.35,313.10,6.12,14.17;13,235.35,296.14,6.12,14.17;13,235.33,277.28,6.14,16.07;13,235.35,260.32,6.12,14.17;13,235.35,243.36,6.12,14.17;13,235.33,224.50,6.14,16.07;13,235.35,205.44,6.12,14.17;13,235.35,186.37,6.12,14.17;13,235.33,167.51,6.14,16.07;13,243.32,590.15,6.12,21.09;13,243.32,578.30,6.12,3.97;13,243.32,561.34,6.12,3.97;13,243.32,544.38,6.12,3.97;13,243.32,527.42,6.12,3.97;13,243.32,510.46,6.12,3.97;13,243.32,493.51,6.12,3.97;13,243.32,476.54,6.12,3.97;13,243.32,459.58,6.12,3.97;13,243.32,441.67,6.12,3.97;13,243.32,423.76,6.12,3.97;13,243.32,406.80,6.12,3.97;13,243.32,388.89,6.12,3.97;13,243.32,370.99,6.12,3.97;13,243.32,354.02,6.12,3.97;13,243.32,336.12,6.12,3.97;13,243.32,318.20,6.12,3.97;13,243.32,301.25,6.12,3.97;13,243.32,283.34,6.12,3.97;13,243.32,265.42,6.12,3.97;13,243.32,248.47,6.12,3.97;13,243.32,230.55,6.12,3.97;13,243.32,210.54,6.12,3.97;13,243.32,191.47,6.12,3.97;13,243.32,173.56,6.12,3.97;13,251.29,592.83,6.12,15.73;13,251.29,573.19,6.12,14.17"><head></head><label></label><figDesc>.18 .28 .625 .18 .28 .494 .351 .411 .494 .351 .411 .565 .351 .433 .565 .351 .433 .468 .468 .468 .423 .423 .423</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,180.12,115.83,255.13,255.12"><head></head><label></label><figDesc></figDesc><graphic coords="7,180.12,115.83,255.13,255.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,134.77,118.99,345.83,129.80"><head>Table 1 .</head><label>1</label><figDesc>Table1shows an overview of the French corpus statistics. Overview of French Corpus Statistics<ref type="bibr" coords="9,393.34,240.93,14.36,7.86" target="#b10">[11]</ref>.</figDesc><table coords="9,206.49,168.58,202.39,56.73"><row><cell cols="5">Datasets #docs #tokens #mentions %noisy</cell></row><row><cell>Train</cell><cell cols="2">158 129,925</cell><cell>7885</cell><cell>-</cell></row><row><cell>Dev</cell><cell>43</cell><cell>29,571</cell><cell>1938</cell><cell>-</cell></row><row><cell>Test</cell><cell>43</cell><cell>32,035</cell><cell>1802</cell><cell>12.15</cell></row><row><cell>All</cell><cell cols="2">244 191,531</cell><cell>11,625</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,144.61,184.05,326.14,79.71"><head>Table 2 .</head><label>2</label><figDesc>Different studied models.</figDesc><table coords="10,144.61,184.05,326.14,56.73"><row><cell>Models</cell><cell>Statistical Orth. features Neural Cont. WE Att. mech.</cell></row><row><cell>Model 1</cell><cell></cell></row><row><cell>Model 2</cell><cell></cell></row><row><cell>Model 3</cell><cell></cell></row><row><cell>Our model</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,135.34,167.81,7.89,443.14"><head>Table 3 .</head><label>3</label><figDesc>Our models results for NERC-Coarse in French, considering literal sense of entities (micro average).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,186.58,153.48,169.05,471.80"><head>Table 4 .</head><label>4</label><figDesc>797 .824 .778 .727 .752 808 .879 .842 .728 .793 .759 .777 .833 .804 .704 .754 .728 .871 .837 .854 .798 .767 .782 .525 .344 .416 .524 .361 .427 .476 .328 .388 .617 .475 .537 .553 .426 .481 TIME .805 .623 .702 .488 .377 .426 .865 .849 .857 .519 .509 .514 .833 .755 .792 .542 .491 .515 .841 .698 .763 .568 .472 .515 ALL .824 .736 .777 .698 .623 .659 .755 .758 .757 .644 .646 .645 .736 .741 .738 .621 .625 .623 .796 Our models results for NERC-Coarse in French, considering literal sense of entities (macro average).</figDesc><table coords="12,192.56,154.14,163.08,471.13"><row><cell>ORG .596 .454 .515 .586 .446 .507 .482 .408 .442 .427 .362 .392 .5 .438 .467 .43 .377 .402 .488 .454 .47 .405 .377 .39</cell><cell>PERS .851 .753 .799 .624 .552 .586 .722 .681 .701 .553 .522 .537 .722 .707 .714 .53 .52 .525 .752 .622 .68 .484 .4 .438</cell><cell>PROD .565 .426 486 .457 .344 .393 .55 .361 .436 .72 .756 .66 .598 .627</cell><cell>Model 1 Model 2 Model 3 Our model</cell><cell>Fuzzy Strict Fuzzy Strict Fuzzy Strict Fuzzy Strict</cell><cell>Label P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1</cell><cell>LOC .843 .789 .815 .776 .72 .748 .799 .864 .821 .723 .782 .742 .777 .828 .787 .708 .757 .717 .847 .828 .823 .775 .757 .752</cell><cell>ORG .598 .501 .629 .565 .468 .589 .455 .371 .457 .382 .301 .379 .343 .353 .38 .298 .288 .325 .444 .511 .526 .335 .375 .393</cell><cell>PERS .849 .745 .809 .622 .551 .596 .745 .704 .74 .597 .568 .595 .717 .7 .706 .563 .55 .554 .75 .609 .68 .502 .414 .462</cell><cell>PROD .582 .564 .621 .47 .477 .519 .518 .352 .43 .514 .349 .426 .411 .255 .362 .366 .228 .322 .546 .539 .677 .5 .489 .614</cell><cell>TIME .848 .671 .836 .569 .458 .571 .883 .906 .928 .575 .586 .602 .85 .83 .868 .537 .548 .566 .87 .751 .884 .691 .582 .689</cell><cell>ALL .851 .734 .779 .72 .622 .661 .78 .767 .771 .672 .657 .662 .755 .745 .746 .651 .639 .641 .796 .712 .747 .669 .592 .624</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,178.08,157.58,7.89,463.59"><head>Table 5 .</head><label>5</label><figDesc>Our models results for NERC-Coarse in French, considering metonymic sense of entities (micro average).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="13,251.27,156.56,167.09,465.64"><head>Table 6 .</head><label>6</label><figDesc>.179 .278 .625 .179 .278 .494 .348 .408 .494 .348 .408 .565 .348 .431 .565 .348 .431 .468 .464 .466 .423 .42 .422 Our models results for NERC-Coarse in French, considering metonymic sense of entities (macro average). .278 .448 .565 .278 .448 .362 .42 .551 .362 .42 .551 .477 .437 .592 .477 .437 .492 .34 .36 .458 .32 .</figDesc><table coords="13,378.73,175.82,39.63,428.75"><row><cell>Our model</cell><cell>Fuzzy Strict</cell><cell>F1 P R F1 P R F1</cell><cell></cell></row><row><cell></cell><cell>Strict</cell><cell>R</cell><cell></cell></row><row><cell>Model 3</cell><cell></cell><cell>F1 P</cell><cell></cell></row><row><cell></cell><cell>Fuzzy</cell><cell>R</cell><cell></cell></row><row><cell></cell><cell></cell><cell>P</cell><cell></cell></row><row><cell>Model 2</cell><cell>Strict</cell><cell>F1 P R F1</cell><cell></cell></row><row><cell></cell><cell>Fuzzy</cell><cell>R</cell><cell></cell></row><row><cell></cell><cell>Strict</cell><cell>R F1 P</cell><cell></cell></row><row><cell>Model 1</cell><cell>Fuzzy</cell><cell>R F1 P Label P</cell><cell>ORG .565</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,428.16,173.30,6.14,391.33"><head></head><label></label><figDesc>.278 .448 .565 .278 .448 .362 .0.42 .551 .362 .42 .551 .477 .436 .592 .477 .436 .592 .34 .36 .457 .32 .341 .431</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="6,144.73,657.44,193.00,7.47"><p>http://www.chokkan.org/software/crfsuite/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="9,144.73,623.92,335.86,7.86;9,144.73,634.88,335.87,7.86;9,144.73,645.84,55.13,7.86"><p>From the Swiss National Library, the Luxembourgish National Library, and the Library of Congress (Chronicling America project), respectively. Original collections correspond to</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="9,210.61,645.84,231.46,7.86;9,137.50,655.03,3.65,5.24;9,144.73,656.80,174.83,7.86"><p>Swiss and Luxembourgish titles, and a dozen for English.<ref type="bibr" coords="9,137.50,655.03,3.65,5.24" target="#b3">4</ref> True positive, False positive, False negative</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="15,155.24,170.80,325.35,8.74;15,155.24,182.75,183.47,8.74;15,354.67,182.75,125.93,8.74;15,155.24,194.71,325.34,8.74;15,155.24,206.67,325.35,8.74;15,155.24,218.62,215.48,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,354.67,182.75,125.93,8.74;15,155.24,194.71,171.67,8.74">SRI International FASTUS systemMUC-6 test results and analysis</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Appelt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Hobbs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bear</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Israel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kameyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kehler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tyson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,351.09,194.71,129.50,8.74;15,155.24,206.67,291.14,8.74">Sixth Message Understanding Conference (MUC-6): Proceedings of a Conference Held in Columbia</title>
		<meeting><address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">November 6-8, 1995. 1995</date>
			<biblScope unit="page" from="237" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,155.24,230.09,325.35,8.74;15,155.24,242.05,325.35,8.74;15,155.24,254.00,84.13,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,335.23,230.09,145.37,8.74;15,155.24,242.05,140.09,8.74">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,303.91,242.05,171.95,8.74">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,155.24,265.47,325.34,8.74;15,155.24,277.43,325.35,8.74;15,155.24,289.38,325.35,8.74;15,155.24,301.34,325.35,8.74;15,155.24,313.29,75.52,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,218.05,265.47,262.54,8.74;15,155.24,277.43,18.64,8.74">A large-scale comparison of historical text normalization systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bollmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,193.32,277.43,287.27,8.74;15,155.24,289.38,325.35,8.74;15,155.24,301.34,33.04,8.74">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="15,155.24,324.76,325.35,8.74;15,155.24,336.72,325.34,8.74;15,155.24,348.67,325.34,8.74;15,155.24,360.63,317.12,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,343.37,324.76,137.23,8.74;15,155.24,336.72,256.42,8.74">Naming the past: Named entity and Animacy recognition in 19th century Swedish literature</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Borin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kokkinakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Olsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,430.93,336.72,49.65,8.74;15,155.24,348.67,325.34,8.74;15,155.24,360.63,44.30,8.74">Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaT-eCH 2007)</title>
		<meeting>the Workshop on Language Technology for Cultural Heritage Data (LaT-eCH 2007)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,155.24,372.10,325.35,8.74;15,155.24,384.06,325.35,8.74;15,155.24,396.01,325.35,8.74;15,155.24,407.97,119.97,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,423.74,372.10,56.85,8.74;15,155.24,384.06,320.90,8.74">Impact of ocr errors on the use of digital libraries: Towards a better access to information</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chiron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Coustaty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Visani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Moreux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,192.13,396.01,255.73,8.74">ACM/IEEE Joint Conference on Digital Libraries (JCDL)</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="249" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,155.24,419.44,325.34,8.74;15,155.24,431.39,325.35,8.74;15,155.24,443.35,84.13,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,288.84,419.44,191.75,8.74;15,155.24,431.39,39.95,8.74">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,204.91,431.39,271.53,8.74">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,155.24,454.82,325.34,8.74;15,155.24,466.77,325.35,8.74;15,155.24,478.73,236.84,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,207.83,466.77,226.61,8.74">Natural language processing (almost) from scratch</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,447.48,466.77,33.11,8.74;15,155.24,478.73,129.92,8.74">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,155.24,490.20,325.35,8.74;15,155.24,502.15,325.35,8.74;15,155.24,514.11,95.80,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="15,393.44,490.20,87.15,8.74;15,155.24,502.15,280.28,8.74">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,155.24,525.58,325.35,8.74;15,155.24,537.54,325.34,8.74;15,155.24,549.49,138.92,8.74" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="15,155.24,537.54,320.04,8.74">Transition-based dependency parsing with stack long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,155.24,560.96,325.34,8.74;15,155.24,572.92,325.35,8.74;15,155.24,584.87,325.35,8.74;15,155.24,596.83,325.35,8.74;15,155.24,608.78,212.63,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,439.22,560.96,41.37,8.74;15,155.24,572.92,325.35,8.74;15,155.24,584.87,49.44,8.74">Extended Overview of CLEF HIPE 2020: Named Entity Processing on Historical Newspapers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Romanello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Flückiger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clematide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,155.24,596.83,325.35,8.74;15,155.24,608.78,127.43,8.74">CLEF 2020 Working Notes. Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,155.24,620.25,325.34,8.74;15,155.24,632.21,325.35,8.74;15,155.24,644.16,325.35,8.74;15,155.24,656.12,325.34,8.74;16,155.24,118.99,325.35,8.74;16,155.24,130.95,325.35,8.74;16,155.24,142.90,325.35,8.74;16,155.24,154.86,105.98,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,439.88,620.25,40.71,8.74;15,155.24,632.21,325.35,8.74;15,155.24,644.16,64.90,8.74">Overview of CLEF HIPE 2020: Named Entity Recognition and Linking on Historical Newspapers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Romanello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Flückiger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clematide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,191.56,118.99,289.04,8.74;16,155.24,130.95,325.35,8.74;16,155.24,142.90,64.03,8.74">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the 11th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="16,327.06,142.90,153.54,8.74;16,155.24,154.86,26.64,8.74">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,155.24,166.75,325.35,8.74;16,155.24,178.70,325.35,8.74;16,155.24,190.66,79.14,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,319.51,166.75,161.08,8.74;16,155.24,178.70,198.49,8.74">Recent named entity recognition and classification techniques: A systematic review</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,364.39,178.70,111.49,8.74">Computer Science Review</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="43" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,155.24,202.55,325.35,8.74;16,155.24,214.50,325.35,8.74;16,155.24,226.46,91.88,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,437.23,202.55,43.37,8.74;16,155.24,214.50,212.53,8.74">Prominer: rule-based protein and gene entity recognition</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hanisch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fundel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-T</forename><surname>Mevissen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fluck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,384.33,214.50,91.78,8.74">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="S14" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,155.24,238.35,325.35,8.74;16,155.24,250.30,136.08,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,309.36,238.35,105.84,8.74">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,424.61,238.35,55.99,8.74;16,155.24,250.30,34.37,8.74">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,155.24,262.19,325.35,8.74;16,155.24,274.15,325.35,8.74;16,155.24,286.10,325.35,8.74;16,155.24,298.06,325.34,8.74;16,155.24,310.01,130.02,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,261.92,274.15,218.67,8.74;16,155.24,286.10,111.94,8.74">University of Sheffield: Description of the LaSIE-II system as used for MUC-7</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Azzam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Huyck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,289.61,286.10,190.98,8.74;16,155.24,298.06,205.21,8.74">Seventh Message Understanding Conference (MUC-7): Proceedings of a Conference Held in</title>
		<meeting><address><addrLine>Fairfax, Virginia</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan</publisher>
			<date type="published" when="1998-05-01">April 29 -May 1, 1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,155.24,321.90,325.35,8.74;16,155.24,333.86,325.35,8.74;16,155.24,345.81,325.35,8.74;16,155.24,357.77,258.06,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,363.52,321.90,117.08,8.74;16,155.24,333.86,161.12,8.74">Distributed word representations improve NER for e-commerce</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-D</forename><surname>Ruvini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,339.04,333.86,141.55,8.74;16,155.24,345.81,253.46,8.74">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing<address><addrLine>Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,155.24,369.66,310.72,8.74;16,465.97,368.08,13.38,6.12;16,155.24,381.61,325.35,8.74;16,155.24,393.57,325.35,8.74;16,155.24,405.52,222.85,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,292.42,369.66,173.54,8.74;16,465.97,368.08,13.38,6.12;16,155.24,381.61,158.84,8.74">IsoQuest Inc.: Description of the NetOwl T M extractor system as used for MUC-7</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Krupka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,339.59,381.61,141.00,8.74;16,155.24,393.57,298.14,8.74">Seventh Message Understanding Conference (MUC-7): Proceedings of a Conference Held in Fairfax</title>
		<meeting><address><addrLine>Virginia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-05-01">April 29 -May 1, 1998. 1998</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,155.24,417.41,325.35,8.74;16,155.24,429.37,325.35,8.74;16,155.24,441.32,325.35,8.74;16,155.24,453.28,84.14,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="16,392.20,417.41,88.39,8.74;16,155.24,429.37,306.17,8.74">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,155.24,441.32,325.35,8.74;16,155.24,453.28,12.01,8.74">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,155.24,465.17,325.35,8.74;16,155.24,477.12,325.35,8.74;16,155.24,489.08,175.84,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="16,155.24,477.12,228.58,8.74">Neural architectures for named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,416.54,477.12,64.05,8.74;16,155.24,489.08,81.00,8.74">Proceedings of NAACL-HLT 2016</title>
		<meeting>NAACL-HLT 2016</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,155.24,500.96,325.34,8.74;16,155.24,512.92,110.90,8.74" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="16,295.36,500.96,185.22,8.74;16,155.24,512.92,46.70,8.74">A survey on deep learning for named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,155.24,524.81,325.35,8.74;16,155.24,536.76,325.35,8.74;16,155.24,548.72,325.35,8.74;16,155.24,560.67,313.19,8.74" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="16,229.15,536.76,251.44,8.74;16,155.24,548.72,174.99,8.74">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,351.27,548.72,129.32,8.74;16,155.24,560.67,281.31,8.74">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,155.24,572.56,325.35,8.74;16,155.24,584.52,325.34,8.74;16,155.24,596.47,325.35,8.74;16,155.24,608.43,276.86,8.74" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="16,419.92,572.56,60.67,8.74;16,155.24,584.52,137.98,8.74">Impact of ocr quality on named entity linking</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Linhares Pontes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sidere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,191.54,596.47,289.06,8.74;16,155.24,608.43,26.39,8.74">Digital Libraries at the Crossroads of Digital Information for the Future</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Jatowt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Maeda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Syn</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="102" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,155.24,620.32,325.34,8.74;16,155.24,632.27,259.87,8.74" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="16,296.54,620.32,184.04,8.74;16,155.24,632.27,122.55,8.74">Hierarchical contextualized representation for named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<idno>CoRR, abs/1911.02257</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,155.24,644.16,325.34,8.74;16,155.24,656.12,325.34,8.74;17,155.24,118.99,325.35,8.74;17,155.24,130.95,212.65,8.74" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="16,244.14,644.16,236.45,8.74;16,155.24,656.12,47.28,8.74">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,225.61,656.12,254.98,8.74;17,155.24,118.99,130.59,8.74;17,333.69,118.99,64.05,8.74">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
	<note>: Long Papers)</note>
</biblStruct>

<biblStruct coords="17,155.24,142.90,325.34,8.74;17,155.24,154.86,238.86,8.74" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="17,270.50,142.90,210.08,8.74;17,155.24,154.86,31.00,8.74">A survey of named entity recognition and classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,194.53,154.86,117.88,8.74">Lingvisticae Investigationes</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="26" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,155.24,166.81,325.34,8.74;17,155.24,178.77,325.35,8.74;17,155.24,190.72,125.33,8.74" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="17,327.04,166.81,153.55,8.74;17,155.24,178.77,63.87,8.74">Making europe&apos;s historical newspapers searchable</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Neudecker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Antonacopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,250.18,178.77,230.42,8.74;17,155.24,190.72,25.99,8.74">12th IAPR Workshop on Document Analysis Systems (DAS)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="405" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,155.24,202.68,325.34,8.74;17,155.24,214.64,325.35,8.74;17,155.24,226.59,325.35,8.74;17,155.24,238.55,262.17,8.74" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="17,390.19,214.64,90.41,8.74;17,155.24,226.59,325.35,8.74;17,155.24,238.55,37.66,8.74">Named entity recognition over electronic health records through a combined dictionary-based approach</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Quimbaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Múnera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A G</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C D</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M M</forename><surname>Velandia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A G</forename><surname>Peña</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Labbé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,202.03,238.55,116.94,8.74">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="61" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,155.24,250.50,325.35,8.74;17,155.24,262.46,243.46,8.74" xml:id="b27">
	<monogr>
		<title level="m" type="main" coord="17,283.92,250.50,196.67,8.74;17,155.24,262.46,164.48,8.74">A Research Agenda for Historical and Multilingual Optical Character Recognition</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cordell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct coords="17,155.24,274.41,325.35,8.74;17,155.24,286.37,140.32,8.74;17,310.81,286.37,125.01,8.74;17,451.07,286.37,29.52,8.74;17,155.24,298.32,95.80,8.74" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="17,310.81,286.37,120.47,8.74">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>CoRR, abs/1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,155.24,310.28,325.35,8.74;17,155.24,322.23,325.34,8.74;17,155.24,334.19,325.35,8.74;17,155.24,346.14,184.86,8.74" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="17,278.47,310.28,202.12,8.74;17,155.24,322.23,164.61,8.74">A survey on recent advances in named entity recognition from deep learning models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,341.33,322.23,139.26,8.74;17,155.24,334.19,207.99,8.74">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2145" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,155.24,358.10,325.35,8.74;17,155.24,370.05,297.79,8.74" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="17,341.97,358.10,138.62,8.74;17,155.24,370.05,160.40,8.74">Comparative study of CNN and RNN for natural language processing</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<idno>CoRR, abs/1702.01923</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
