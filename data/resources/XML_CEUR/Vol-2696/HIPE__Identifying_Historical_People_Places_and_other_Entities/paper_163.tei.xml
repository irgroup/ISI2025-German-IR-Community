<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.90,115.96,327.55,12.62;1,174.20,133.89,266.95,12.62">Named Entity Disambiguation and Linking on Historic Newspaper OCR with BERT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,222.12,171.89,54.72,8.74"><forename type="first">Kai</forename><surname>Labusch</surname></persName>
							<email>kai.labusch@sbb.spk-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Staatsbibliothek zu Berlin -Preußischer Kulturbesitz</orgName>
								<address>
									<postCode>10785</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.00,171.89,84.77,8.74"><forename type="first">Clemens</forename><surname>Neudecker</surname></persName>
							<email>clemens.neudecker@sbb.spk-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Staatsbibliothek zu Berlin -Preußischer Kulturbesitz</orgName>
								<address>
									<postCode>10785</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.90,115.96,327.55,12.62;1,174.20,133.89,266.95,12.62">Named Entity Disambiguation and Linking on Historic Newspaper OCR with BERT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">00FEB872AE7C3E000DF9D2CDBA825738</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Named Entity Recognition</term>
					<term>Entity Linking</term>
					<term>BERT</term>
					<term>OCR</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a named entity disambiguation and linking (NED, NEL) system that consists of three components: (i) Lookup of possible candidates in an approximative nearest neighbour (ANN) index that stores BERT-embeddings. (ii) Evaluation of each candidate by comparison of text passages of Wikipedia performed by a purpose-trained BERT model. (iii) Final ranking of candidates on the basis of information gathered from previous steps. We participated in the CLEF 2020 HIPE NERC-COARSE and NEL-LIT tasks for German, French, and English. The CLEF HIPE 2020 results show that our NEL approach is competitive in terms of precision but has low recall performance due to insufficient knowledge base coverage of the test data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our participation in the CLEF HIPE 2020 NER-COARSE and NEL-LIT task 1  has been conducted as part of the Qurator 2 project within the Berlin State Library (Staatsbibliothek zu Berlin -Preußischer Kulturbesitz, SBB). One goal of the SBB in the Qurator project is the development of a system that identifies persons, locations and organizations within digitized historical text material obtained by Optical Character Recognition (OCR) and then links recognized entities to their corresponding Wikidata-IDs. Here, we provide a high-level overview of the functionality of our system; for details, take a deeper look at the information provided together with the source code 3 .</p><p>The paper is structured as follows: after a brief introduction of the background and use case, a short summary of the Named Entity Recognition system is provided in chapter 2. Chapter 3 outlines the Entity Linking approach developed in greater detail. Chapter 4 covers the chosen method for evaluation of candidates for entity linking and chapter 5 continues with a description of their ranking. Following a discussion of the results obtained in the NER-COARSE and NEL-LIT tasks in chapter 6, we wrap up with some concluding remarks and potentials for further improvement in chapter 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Background</head><p>The SBB is continuously digitizing its copyright-free holdings and making them publicly available online in various formats for viewing and browsing <ref type="foot" coords="2,437.28,244.68,3.97,6.12" target="#foot_0">4</ref> or automated <ref type="foot" coords="2,161.89,256.64,3.97,6.12" target="#foot_1">5</ref> download. As part of an on-going process, a growing amount of OCRderived full-texts of the digitized printed material is provided in ALTO <ref type="foot" coords="2,443.84,268.59,3.97,6.12" target="#foot_2">6</ref> format for internal use cases such as full-text indexing and other information retrieval tasks.</p><p>With an increasing amount of digitized sources becoming available online, the need for automated ways of extracting additional information from these sources increases as well. Disciplines such as the Digital Humanities create use cases for text and data mining or the semantic enrichment of the full-texts with e.g. Named Entity Recognition and Linking (e.g. for the re-construction of historical social networks <ref type="foot" coords="2,200.77,364.80,3.97,6.12" target="#foot_3">7</ref> ).</p><p>The boost in popularity of neural networks in the early 2010s, which are not only capable of dealing with large amounts of data (i. e., big data), but also require enormous amounts of data to be trained on in order to produce high quality results, has addressed this need. However, due to the historical nature of the documents being digitized in libraries, standard methods and procedures from the NLP domain typically require additional adaptation in order to successfully deal with the historical spelling variation and the remaining noise resulting from OCR errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Qurator</head><p>The Qurator project[9], funded by the German Federal Ministry of Education and Research (BMBF), for a timeframe of three years (11/2018-10/2021), is based in the metropolitan region Berlin/Brandenburg. The consortium of ten project partners from research and industry combines vast expertise in areas such as Language as well as Knowledge Technologies, Artificial Intelligence and Machine Learning.</p><p>The project's main goal is the development of a sustainable technology platform that supports knowledge workers in various industries. The platform will simplify the curation of digital content and accelerate it dramatically. AI techniques are integrated into curation technologies and curation workflows in the form of domain specific solutions covering the entire life cycle of content curation. The solutions being developed focus on curation services for the domains of culture, media, health and industry.</p><p>Within the Qurator consortium, the SBB is responsible for the task are "Curation Technologies for Digitized Cultural Heritage". The main goals of this task area lie in the development and adaptation of novel, AI/ML-based approaches from the document analysis and NLP domains for the improvement of the quality of OCR full-texts and the semantic enrichment of the derived full-texts with NER and NEL. The baseline for this development are the digitized collections of the SBB, with approximately 175,000 (August 2020) digitized documents from the timeframe 1400-1920. While most of the documents are in German, there is great variation with many other European and also Asian languages being present in the collection. The collection comprises documents from a wide array of publication formats, including books, newspapers, journals, maps, letters, posters, and many more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">HIPE</head><p>The introduction of the CLEF HIPE 2020 shared task provided a welcome opportunity to assess the performance of our own NER and NED systems in comparison with others within the frame of a common and realistic benchmark setting. HIPE proposes two tasks, NER and NEL, for French, German and English, with OCRed historical newspapers as input. The SBB's digitization strategy has traditionally put a strong focus on historic newspapers, with projects like Europeana Newspapers <ref type="bibr" coords="3,233.93,429.83,14.34,8.74" target="#b7">[8]</ref> producing millions of pages of OCR from digitized newspapers.</p><p>Recent years have also brought about the application of deep learning models for NER and NEL, where HIPE first puts these developments to the test for more challenging historical and noisy materials. We therefore expect that many valuable insights and directions for future work will results from participation in the HIPE shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Named Entity Recognition</head><p>Before entity disambiguation starts, the input text is run through a named entity recognition (NER) system that tags all person (PER), location (LOC) and organization (ORG) entities. For the CLEF HIPE 2020 task, we used a BERT <ref type="bibr" coords="3,464.28,579.26,16.31,8.74" target="#b2">[3]</ref> based NER-system that has been developed previously at SBB and described in <ref type="bibr" coords="3,134.77,603.17,9.96,8.74" target="#b4">[5]</ref>.</p><p>We employed our off-the-shelf system 8 and did not use CLEF HIPE 2020 NER training data for fine-tuning. Our off-the-shelf system does not currently support product (PROD) and time (TIME) entities. The German NER system has been trained simultaneously on recent and historical German NER ground truth. In case of French and English, we used our multilingual model, i.e., a single BERT model that was trained for NER on combined German, French, Dutch and English NER labeled data.</p><p>Starting from multilingual BERT-Base Cased, we applied unsupervised pretraining composed of the "Masked-LM" and "Next Sentence Prediction" tasks proposed by <ref type="bibr" coords="4,190.98,203.18,10.52,8.74" target="#b2">[3]</ref> using 2,333,647 pages of unlabeled historical German text from the DC-SBB dataset <ref type="bibr" coords="4,227.59,215.13,9.96,8.74" target="#b5">[6]</ref>. Furterhmore, we performed supervised pre-training on NER ground truth using the Europeana Newspapers <ref type="bibr" coords="4,371.04,227.09,9.96,8.74" target="#b6">[7]</ref>, ConLL-2003 <ref type="bibr" coords="4,445.28,227.09,15.50,8.74" target="#b11">[12]</ref> and GermEval-2014 <ref type="bibr" coords="4,205.02,239.04,10.52,8.74" target="#b0">[1]</ref> datasets.</p><p>In the according cross-evaluation, it was found that unsupervised pre-training on DC-SBB data worsens BERT performance in the case of contemporary training/test pairs while the performance improves for most experiments that test on historical ground truth. The best performance for our model is achieved by combining pre-training using DC-SBB + GermEval + CoNLL and results obtained from that are comparable to the state-of-the-art (see table <ref type="table" coords="4,388.68,311.27,3.87,8.74">1</ref>). For the discussion of the performance of our NER system in the particular context of HIPE, please see chapter 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P R F1</head><p>[5] DC-SBB+GermEval+CoNLL 81.1 ±1.2 87.8 ±1.4 84.3 ±1.1 <ref type="bibr" coords="4,136.16,399.64,14.34,7.86" target="#b9">[10]</ref> Newspaper (1703-1875) --85.31 <ref type="bibr" coords="4,136.16,410.60,14.34,7.86" target="#b10">[11]</ref> Newspaper (1888-1945) --77.51</p><p>Table <ref type="table" coords="4,163.25,434.56,4.13,7.89">1</ref>. Performance comparison of different historical German NER BERT models.</p><p>Results in <ref type="bibr" coords="4,176.84,445.54,9.73,7.86" target="#b4">[5]</ref> were obtained by 5-fold cross validation, results in <ref type="bibr" coords="4,390.24,445.54,14.34,7.86" target="#b9">[10]</ref> and <ref type="bibr" coords="4,424.41,445.54,14.34,7.86" target="#b10">[11]</ref> have been obtained for some 80/20 training/test split.</p><p>3 Entity Linking: Lookup of Candidates text that have been annotated by human authors with references that can serve as ground truth.</p><p>The knowledge base has been directly derived from Wikipedia through the identification of persons, locations and organizations within the German Wikipedia by recursive traversal of its category structure:</p><p>-PER: All pages that are part of the categories "Frau" or "Mann" or of one of the reachable sub-categories of "Frau" and "Mann". One problem with this approach is that fictional "persons" are typically not contained in that selection.</p><p>-LOC: All pages that are part of the category "Geographisches Objekt" or one of its sub-categories. We exclude everything that is part of "Geographischer Begriff" or one of its sub-categories. -ORG: All pages that are part of the category "Organisation" or one of its sub-categories.</p><p>Note: we plan to use the structured information of Wikidata in order to more reliably identify PER, LOC and ORG entities within Wikipedia which should make the heuristic approach of knowledge base creation obsolete. Some pages might end up in multiple entity classes at the same time due to the category structure of the German Wikipedia. In order to create disjunct entity classes, we first remove from the entity class ORG everything that is also included in PER or LOC. In a second step, we remove everything from the entity class LOC that is also part of PER or ORG. It has been pointed out by one of our reviewers that this step is conceptually not required by our approach and it actually will be obsolete as soon as we identify PER, LOC and ORG entities on the basis of Wikidata.</p><p>To construct knowledge bases for French and English, we first map the identified German Wikipedia entity pages to their corresponding Wikidata-IDs and then the Wikidata-IDs back to the corresponding French and English Wikipedia pages. Table <ref type="table" coords="5,191.30,620.25,4.98,8.74" target="#tab_0">2</ref> shows the size of the knowledge bases per category and language. Note, that the knowledge bases for French and English are significantly smaller than the German one due to loss of many entities within the Wikipedia -Wikidata mapping.</p><p>What is the cause of that loss? We checked at random a number of entities of all types (PER,LOC,ORG) that had been lost in the mapping between the German and the French or English Wikipedia. In all cases Wikidata actually did not contain a reference to some English or French version of Wikipedia. Hence either there actually is not a French or English version of that Wikipedia page available or the correct linking has not been established so far. We expect to end up with much larger knowledge bases by use of structured data from Wikidata for identification of entities.</p><p>After the unmasked CLEF HIPE 2020 test data had been published, we computed the coverage of our per language knowledge bases, i.e., the percentage of Wikidata entity IDs (NEL-LIT) in the test data that actually can be found in the corresponding knowledge base. That percentage is an upper bound on the systems performance. As you can see in table 2, the coverage is similar for German and French (roughly 70%) whereas it is significantly worse for English (roughly 50%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity Lookup Index</head><p>After the knowledge bases have been established, for each of them an entity lookup index is created by computation of BERT embeddings of the page titles of the identified PER, LOC and ORG Wikipedia pages. The BERT embeddings are obtained from a combination of different layers of the evaluation model (see chapter 4). The embedding vectors of the tokens of the page titles are stored in an approximative nearest neighbour (ANN) index <ref type="bibr" coords="6,350.69,396.50,9.96,8.74" target="#b1">[2]</ref>. We use cosine similarity as distance measure and the ANN index uses 100 random projection search trees. There are separate ANN indices per supported language and per supported entity category.</p><p>Given some NER-tagged surface that is part of the input text, up to 400 linking candidates below a cut-off distance of 0.1 are selected by lookup of the nearest neighbours of the surface's embedding within the approximative nearest neighbour index of the corresponding language and entity category.</p><p>According to our observations the performance of our system improves with a higher number of candidates considered. Of course there is some upper limit to that, however, more important is the computational complexity that grows with the number of linking candidates. We did not systematically evaluate the effect of the number of linking candidates but we used a number of linking candidates that is sufficiently high. Note that there is also an interaction with the cut-off distance since in many cases there are not as many as 400 nearest neighbours within a distance of less than 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation of Candidates</head><p>For each entity of the knowledge bases (see chapter 3.1) there are text passages in Wikipedia where some human Wikipedia editor has linked to that particular entity. How many linked text passages we have for some particular entity differs SQL-table "sentences" id : A unique number that identifies each sentence. text : A JSON-array that contains the tokens of the sentence. Example:</p><p>["Der", "Begriff", "wurde", "von", "Georg", "Christoph", "Lichtenberg", "eingebracht", "."] entities : A JSON-array of same length as "text" that contains for each token of the sentence the target entity if the token is part of an Wikipedia link that has been created by some Wikipedia author. If a token is not part of an Wikipedia link its corresponding entity is empty. Example:</p><p>["", "", "", "", "Georg Christoph Lichtenberg", "Georg Christoph Lichtenberg", "Georg Christoph Lichtenberg", "", ""]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQL-table "links"</head><p>id : A unique number that identifies each Wikipedia entity reference. sentence : The sentence-id of the sentence where the reference occurs (sentences.id). target : The target entity of the reference. Example:</p><formula xml:id="formula_0" coords="7,181.57,287.35,109.70,6.12">"Georg Christoph Lichtenberg"</formula><p>Table <ref type="table" coords="7,162.93,309.95,4.13,7.89">3</ref>. The SQLITE sentence database consists of two tables. The "sentences" table contains all the sentences of the Wikipedia where some Wikipedia author referenced a PER, LOC, or ORG entity. The "links" table enumerates all references to PER, LOC or ORG entities in the Wikipedia. In order to get all the sentences of the German wikipedia where "Georg Christoph Lichtenberg" has been referenced by some Wikipedia author, the following SQL statement is used: SELECT links.target, sentences.id, sentences.text, sentences.entities FROM links JOIN sentences ON links.sentence=sentences.id WHERE links.target=="Georg Christoph Lichtenberg" widely depending on the entity. Some entities have thousands of links available whereas other entities have only very few.</p><p>We created a SQLITE database that provides quick access to the mentions of some particular entity. Using the Wikipedia page title of the entity as key, for instance, "Georg Christoph Lichtenberg", the database returns all sentences where some human editor explicitly linked to "Georg Christoph Lichtenberg". The database can be derived programmatically from the Wikipedia without any human annotation being involved. Table <ref type="table" coords="7,335.38,496.71,4.98,8.74">3</ref> gives a short description of the structure of the SQLITE database.</p><p>Using that database, we created a training dataset that consists of random sentence pairs (A,B) where sentences (A,B) either reference the same entity or different entities. That training dataset defines a binary classification problem: Do sentences A and B refer to the same item or not?</p><p>We trained a BERT model with respect to this binary classification problem per supported language that we call the "evaluation model" in the following. Given some arbitrary sentence pair (A,B), the evaluation model outputs the probability of the two sentences refering to the same item.</p><p>During entity disambiguation, we build up to 50 sentence pairs (A,B) for each candidate that has been found in the lookup step (see chapter 3.2). The sentence pairs are composed in such a way that sentence A is part of the input text where the entity that is to be linked is mentioned and sentence B is a sentence from Wikipedia where that particular candidate has been linked to by a Wikipedia author. The higher the number of evaluated sentence pairs per candidate is, the more reliable the ranking model (see Section 5) can determine the overall matching probability. Again, the computational complexity increases with the number of sentence pairs. Additionally, in most cases there is only a very limited number of reference sentences from Wikipedia available such that it is not possible to generate a large number of unique sentence pairs. The choice of 50 sentence pairs is a trade-off that takes into account these considerations.</p><p>Application of the evaluation model to each sentence pair results in a corresponding matching probability. The sets of sentence pair matching probabilities of all candidates are then further processed by the ranking model (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ranking of Candidates</head><p>During previous steps, sets of possible entity candidates have been obtained for all the parts of the input text that have been NER-tagged. For each candidate, a number of sentence pairs have been examined by the evaluation model, resulting in a set of sentence pair probabilities per candidate.</p><p>The ranking step finally determines an ordering of the candidates per linked entity according to the probability that it is the "correct" entity the part of the input text is actually referring to.</p><p>We compute statistical features of the sets of sentence pair probabilities of the candidates, among them: mean, median, min, max, standard deviation as well as various quantiles. Additionally we sort all the sentence pair probabilities and compute ranking statistics over all the candidates.</p><p>Then, based on the statistical features that describe the set of sentence pair probabilities of each candidate, a random forest model computes the overall probability that some particular candidate is actually the "correct" corresponding entity. The random forest model is the only component of our system where the CLEF HIPE 2020 data was used for training.</p><p>Finally the candidates are sorted according to the overall matching probabilities that have been estimated by the random forest model. The final output of our NED system is the sorted list of candidates where candidates that have a matching probability less than 0.2 are cut off.</p><p>Our NED system does not implement the NIL entity that means either it returns a non-empty list of Wikidata IDs that have been sorted in descending order according to their overall matching probabilities or the result is "-" if there is not any candidate that has matching probability above 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Table <ref type="table" coords="8,162.82,644.16,4.98,8.74">4</ref> lists the NER performance of our off-the-shelf NER system (SBB) on the CLEF HIPE 2020 test data in the NER-COARSE-LIT task. Additionally, it also contains the results of the best performing system (L3i). In case of the SBB system, strict NER performance is significantly worse than fuzzy NER performance. That observation holds for the L3i system too, however, for our system the effect is much more pronounced. Strict NER is a much more demanding task, nevertheless, we partly attribute the difference in performance to the training data of our NER system (see <ref type="bibr" coords="9,263.65,178.77,10.30,8.74" target="#b4">[5]</ref>), which has been created according to multiple slightly different NER annotation standards and also to the fact that we did not fine tune the NER system by using training data provided by the CLEF HIPE 2020 task organizers.</p><p>According to our observations, the OCR quality of the French data is slightly better than the German one and both French and German have better OCR quality than the English text material. By OCR quality, we primarily mean the overall quality of the entire text but not the mean Levenshtein distances of the entity text passages with respect to the original text. NER performance resembles that observation, i.e., French and German are comparable whereas English is significantly worse. Therefore our current hypothesis is that these differences are partly caused by the sensitivity of our NER-tagger to OCR noise within the surrounding text.  <ref type="table" coords="9,164.66,579.31,4.13,7.89">4</ref>. NER-COARSE results of our (SBB) off-the-shelf BERT based NER system on the CLEF HIPE 2020 test data in comparison to the best performing system (L3i). The SBB system has not been trained on the CLEF HIPE 2020 data and does not support PROD and TIME entities. For German, the system has been trained on recent and historical German data simultaneously whereas for French and English, we employed a multilingual system that has been trained on German, Dutch, French and English data at the same time.</p><p>Table <ref type="table" coords="10,163.01,118.99,4.98,8.74">5</ref> shows the NEL performance of our system if our BERT based NERtagging is used as input whereas table 6 contains the results that have been reported when the NER ground truth had been provided to the NEL system. The two tables show that NEL performance significantly improves if NER ground truth is provided. Interestingly, the recall of the French and German NEL system is similar although the French knowledge base is significantly smaller than the German one. This observation can be explained by the fact that coverage of the test data of the knowledge bases for German in French is similar (see Table <ref type="table" coords="10,451.54,215.12,3.87,8.74" target="#tab_0">2</ref>). We attribute the much lower recall for the English test data to much lower coverage of the test data of the English knowledge base (see Table <ref type="table" coords="10,387.24,239.03,3.87,8.74" target="#tab_0">2</ref>).</p><p>Precision of the German and French SBB system is comparable, again precision of the English system is significantly worse, even if NER-groundtruth is provided. We explain in Section 5 that our system provides a list of candidates that have matching probability above 0.2 that is sorted in descending order according to the matching probability. Hence, given a bad coverage of the knowledge base, as it is the case for English, non matching candidates will inevitably move up in that sorted list, i.e., the drop in precision can also be explained by the bad coverage of the knowledge base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lang Team Evaluation</head><p>Label P R F1</p><p>DE SBB NEL-LIT-micro-fuzzy-@1 ALL 0.540 0.304 0.389 DE SBB NEL-LIT-micro-fuzzy-relaxed-@1 ALL 0.561 0.315 0.403 DE SBB NEL-LIT-micro-fuzzy-relaxed-@3 ALL 0.590 0.332 0.425 DE SBB NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.601 0.338 0.432 FR SBB NEL-LIT-micro-fuzzy-@1 ALL 0.594 0.310 0.407 FR SBB NEL-LIT-micro-fuzzy-relaxed-@1 ALL 0.616 0.321 0.422 FR SBB NEL-LIT-micro-fuzzy-relaxed-@3 ALL 0.624 0.325 0.428 FR SBB NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.629 0.328 0.431 EN SBB NEL-LIT-micro-fuzzy-@1 ALL 0.257 0.097 0.141 EN SBB NEL-LIT-micro-fuzzy-relaxed-@1 ALL 0.257 0.097 0.141 EN SBB NEL-LIT-micro-fuzzy-relaxed-@3 ALL 0.299 0.112 0.163 EN SBB NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.299 0.112 0.163 Table <ref type="table" coords="10,164.24,543.15,4.13,7.89">5</ref>. NEL-LIT results with NER-tagging performed by our off-the-shelf system. French and German performance is similar, English is significantly worse. The stark performance differences between German and French versus English can mainly be explained by differences in coverage of the test data of the knowledge bases (see Table <ref type="table" coords="10,134.77,587.01,3.58,7.86" target="#tab_0">2</ref>).</p><p>Table <ref type="table" coords="10,162.07,632.21,4.98,8.74">7</ref> reports on the best NEL-LIT results per team where the NER task has been performed by each teams own NER system. Table <ref type="table" coords="10,386.27,644.16,4.98,8.74">8</ref> reports on the best NEL-LIT results per team where the NER ground truth has been provided to Lang Team Evaluation Label P R F1</p><p>DE SBB NEL-LIT-micro-fuzzy-@1 ALL 0.615 0.349 0.445 DE SBB NEL-LIT-micro-fuzzy-relaxed-@1 ALL 0.636 0.361 0.461 DE SBB NEL-LIT-micro-fuzzy-relaxed-@3 ALL 0.673 0.382 0.488 DE SBB NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.686 0.389 0.497 FR SBB NEL-LIT-micro-fuzzy-@1 ALL 0.677 0.371 0.480 FR SBB NEL-LIT-micro-fuzzy-relaxed-@1 ALL 0.699 0.383 0.495 FR SBB NEL-LIT-micro-fuzzy-relaxed-@3 ALL 0.710 0.390 0.503 FR SBB NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.716 0.393 0.507 EN SBB NEL-LIT-micro-fuzzy-@1 ALL 0.344 0.119 0.177 EN SBB NEL-LIT-micro-fuzzy-relaxed-@1 ALL 0.344 0.119 0.177 EN SBB NEL-LIT-micro-fuzzy-relaxed-@3 ALL 0.390 0.135 0.200 EN SBB NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.390 0.135 0.200 Table <ref type="table" coords="11,163.36,291.44,4.13,7.89">6</ref>. NEL-LIT results with NER ground truth provided. As expected, availability of NER ground truth significantly improves NEL results (see Table <ref type="table" coords="11,407.32,302.42,4.61,7.86">5</ref> for comparison). The stark performance differences between German and French versus English can mainly be explained by differences in coverage of the test data of the knowledge bases (see Table <ref type="table" coords="11,178.60,335.30,3.58,7.86" target="#tab_0">2</ref>). each team. In both tables, i.e., Table <ref type="table" coords="11,295.76,380.83,4.98,8.74">7</ref> and Table <ref type="table" coords="11,349.60,380.83,3.87,8.74">8</ref>, the results have been sorted according to precision. It turns out that our SBB NEL-system performed quite competitively in terms of precision but rather abysmally in terms of recall. We attribute the bad recall performance to multiple reasons:</p><p>-Due to the construction of the knowledge bases, many entities end up without representation. Even for German, that has the best coverage, coverage is only 71%. -The lookup step of our NEL system has not been extensively optimized up to now. The embeddings, for instance, that are stored in the approximative nearest neighbour indices have been selected only on an initial guess basis and have not been optimized for performance. Which layers of the model to use and how to combine them heavily impacts the properties of the lookup step. Additionally the parameters of the approximative nearest neighbour indices such as type of similarity measure, number of lookup trees and cutoff distance, have been chosen on a initial guess basis too and could be further optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The results of our participation in the HIPE task highlight where the biggest potential for improvement of our NER / NEL / NED system is to be expected:</p><p>Lang Team Evaluation Label P R F1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DE L3i</head><p>NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.627 0.636 0.632 DE SBB NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.601 0.338 0.432 DE UvA.ILPS NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.311 0.345 0.327</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FR L3i</head><p>NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.695 0.705 0.700 FR SBB NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.629 0.328 0.431 FR IRISA NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.560 0.490 0.523 FR UvA.ILPS NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.397 0.220 0.283 FR ERTIM NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.150 0.084 0.108 EN L3i NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.674 0.662 EN UvA.ILPS NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.304 0.458 0.366 EN SBB NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.299 0.112 0.163 Table <ref type="table" coords="12,164.03,313.05,4.13,7.89">7</ref>. NEL-LIT results per team with NER-tagging performed by the teams own NER system. The results have been sorted according to precision.</p><p>Lang Team Evaluation Label P R F1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DE L3i</head><p>NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.696 0.696 0.696 DE SBB NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.686 0.389 0.497 DE aidalight-baseline NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.440 0.435 0.437</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FR L3i</head><p>NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.746 0.743 0.744 FR SBB NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.716 0.393 0.507 FR Inria-DeLFT NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.604 0.670 0.635 FR IRISA NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.590 0.588 0.589 FR aidalight-baseline NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.516 0.508 0.512</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EN L3i</head><p>NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.744 0.744 0.744 EN Inria-DeLFT NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.633 0.685 0.658 EN UvA.ILPS NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.607 0.580 0.593 EN aidalight-baseline NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.506 0.506 0.506 EN SBB NEL-LIT-micro-fuzzy-relaxed-@5 ALL 0.390 0.135 0.200 Table <ref type="table" coords="12,164.73,601.54,4.13,7.89">8</ref>. NEL-LIT results per team with NER ground truth provided. The results have been sorted according to precision.</p><p>-OCR-performance is crucial since it is the start of the processing chain and OCR noise causes, as expected, bad results in all the subsequent processing steps.</p><p>-NEL recall performance of the SBB system has the biggest potential for improvement. An obvious path to improvement of recall performance is a better construction of the knowledge bases that should lead to a better overall representation of entities. -An extensive evaluation and optimization of the lookup step that includes hardening against OCR noise could improve recall. -NER results of other teams show that huge improvements in terms of NER performance even under the presence of noise are possible <ref type="bibr" coords="13,399.16,237.57,11.86,8.74" target="#b3">[4]</ref>. That improvement directly benefits the NED/NEL steps. We will therefore carefully evaluate how these improvements have been achieved in order to optimize our own NER-tagger.</p><p>Due to the diverse nature of the CLEF HIPE 2020 task data, in particular due to the differences in OCR quality, for us, the performance evaluation has resulted in valuable insights into our NER/NED/NEL system. The HIPE task data is in our opinion quite realistic, which means that we expect our system will have to handle similar data in the real world. Hence, we consider our participation in the HIPE competition as an important and constructive step on the path towards improving NER/NED processing of real world text material that has been obtained by OCR of historical documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,173.70,396.17,91.33,7.86;9,347.91,396.17,22.40,7.86;9,387.82,396.17,6.27,7.86;9,411.09,396.17,30.07,7.86;9,173.70,412.13,267.95,7.86;9,173.70,423.09,267.95,7.86;9,173.70,439.05,267.95,7.86;9,173.70,450.01,267.95,7.86;9,173.70,464.56,267.95,7.86;9,173.70,475.52,267.95,7.86;9,173.70,491.48,267.95,7.86;9,173.70,502.44,267.95,7.86;9,173.70,516.98,267.95,7.86;9,173.70,527.94,267.95,7.86;9,173.70,543.90,267.95,7.86;9,173.70,554.86,267.95,7.86;9,134.77,579.31,25.36,7.89"><head></head><label></label><figDesc>-COARSE-LIT-micro-fuzzy ALL 0.870 0.886 0.878 DE SBB NE-COARSE-LIT-micro-fuzzy ALL 0.730 0.708 0.719 DE L3i NE-COARSE-LIT-micro-strict ALL 0.790 0.805 0.797 DE SBB NE-COARSE-LIT-micro-strict ALL 0.499 0.484 0.491 FR L3i NE-COARSE-LIT-micro-fuzzy ALL 0.912 0.931 0.921 FR SBB NE-COARSE-LIT-micro-fuzzy ALL 0.765 0.689 0.725 FR L3i NE-COARSE-LIT-micro-strict ALL 0.831 0.849 0.840 FR SBB NE-COARSE-LIT-micro-strict ALL 0.530 0.477 0.502 EN L3i NE-COARSE-LIT-micro-fuzzy ALL 0.794 0.817 0.806 EN SBB NE-COARSE-LIT-micro-fuzzy ALL 0.642 0.572 0.605 EN L3i NE-COARSE-LIT-micro-strict ALL 0.623 0.641 0.632 EN SBB NE-COARSE-LIT-micro-strict ALL 0.347 0.310 0.327 Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,549.99,345.83,114.86"><head>Table 2 .</head><label>2</label><figDesc>Therefore, a knowledge base that contains structured information like Wikidata is not sufficient. Instead we need additional continuous text where the entities that are part of the knowledge base are discussed, mentioned and referenced. Hence, we derive the knowledge base such that each entity in it has a corresponding Wikipedia page since the Wikipedia articles contain continuous Size of knowledge-base per category per language. French and English knowledge bases are significantly smaller than the German one due to loss of entities in the Wikipedia -Wikidata mapping. Coverage of CLEF HIPE 2020 NEL-LIT test data Q-IDs is similar for German and French while being significantly worse for English.</figDesc><table coords="5,228.30,120.97,158.76,56.70"><row><cell cols="2">Lang PER LOC ORG coverage of</cell></row><row><cell></cell><cell>test data</cell></row><row><cell>DE 671398 374048 136044</cell><cell>71%</cell></row><row><cell>FR 217383 155856 39305</cell><cell>68%</cell></row><row><cell>EN 324607 198570 58730</cell><cell>47%</cell></row></table><note coords="4,134.77,549.99,313.75,8.77;4,134.77,572.43,345.82,8.74;4,134.77,584.39,345.82,8.74;4,134.77,596.34,345.83,8.74;4,134.77,608.30,28.24,8.74"><p><p>3.1 Construction of knowledge base for PER, LOC and ORG</p>Our entity linking and disambiguation works by comparison of continuous text snippets where the entities in question are mentioned. A purpose-trained BERT model (the evaluation model) performs that text comparison task (see chapter 4).</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,623.92,163.23,7.86"><p>https://digital.staatsbibliothek-berlin.de</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="2,144.73,634.88,86.65,7.86"><p>https://oai.sbb.berlin</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="2,144.73,645.84,151.20,7.86"><p>https://www.loc.gov/standards/alto/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="2,144.73,656.80,119.19,7.86"><p>https://sonar.fh-potsdam.de/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="3,144.73,645.84,187.90,7.86"><p>https://impresso.github.io/CLEF-HIPE-2020/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="3,144.73,656.80,163.77,7.86"><p>https://github.com/qurator-spk/sbb ner</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.96,428.29,337.64,7.86;13,151.52,439.25,329.07,7.86;13,151.52,450.21,315.06,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,363.69,428.29,116.90,7.86;13,151.52,439.25,121.32,7.86">Germeval 2014 named entity recognition: Companion paper</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Benikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kisselew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,280.56,439.25,200.03,7.86;13,151.52,450.21,141.00,7.86">Proceedings of the KONVENS GermEval Shared Task on Named Entity Recognition</title>
		<meeting>the KONVENS GermEval Shared Task on Named Entity Recognition<address><addrLine>Hildesheim, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="104" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,460.84,337.64,7.86;13,151.52,471.80,138.08,7.86" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bernhardsson</surname></persName>
		</author>
		<ptr target="https://github.com/spotify/annoy" />
		<title level="m" coord="13,224.54,460.84,225.76,7.86">Annoy: Approximate Nearest Neighbors in C++/Python</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,482.43,337.64,7.86;13,151.52,493.36,329.07,7.89;13,151.52,504.35,131.41,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="13,335.66,482.43,144.93,7.86;13,151.52,493.39,192.90,7.86">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,514.98,337.64,7.86;13,151.52,525.94,329.07,7.86;13,151.52,536.90,329.07,7.86;13,151.52,547.86,329.07,7.86;13,151.52,558.82,156.35,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,388.95,514.98,91.65,7.86;13,151.52,525.94,329.07,7.86;13,151.52,536.90,24.85,7.86">Introducing the CLEF 2020 HIPE shared task: Named entity recognition and linking on historical newspapers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Romanello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bircher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clematide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,251.01,547.86,134.56,7.86">Advances in information retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Castells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Martins</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="524" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,569.45,337.63,7.86;13,151.52,580.41,329.07,7.86;13,151.52,591.37,329.07,7.86;13,151.52,602.33,329.07,7.86;13,151.52,613.29,329.07,7.86;13,151.52,624.25,197.13,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,335.25,569.45,145.34,7.86;13,151.52,580.41,180.47,7.86">BERT for Named Entity Recognition in Contemporary and Historic German</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Labusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Neudecker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhöfer</surname></persName>
		</author>
		<ptr target="https://corpora.linguistik.uni-erlangen.de/data/konvens/proceedings/papers/KONVENS2019paper4.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="13,356.96,580.41,123.63,7.86;13,151.52,591.37,162.06,7.86">Proceedings of the 15th Conference on Natural Language Processing</title>
		<title level="s" coord="13,397.24,591.37,48.82,7.86">Long Papers</title>
		<meeting>the 15th Conference on Natural Language Processing<address><addrLine>KONVENS; Erlangen, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>German Society for Computational Linguistics &amp; Language Technology</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,634.88,337.63,7.86;13,151.52,645.84,329.07,7.86" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Labusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhöfer</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3257041</idno>
		<ptr target="https://doi.org/10.5281/zenodo" />
		<title level="m" coord="13,272.40,634.88,208.19,7.86;13,151.52,645.84,124.60,7.86">OCR Fulltexts of the Digital Collections of the Berlin State Library (DC-SBB)</title>
		<imprint>
			<date type="published" when="2019-06-26">June 26th 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,119.67,337.63,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,329.07,7.86;14,151.52,152.55,329.07,7.86;14,151.52,163.51,36.97,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,211.78,119.67,264.40,7.86">An open corpus for named entity recognition in historic newspapers</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Neudecker</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/L16-1689" />
	</analytic>
	<monogr>
		<title level="m" coord="14,165.22,130.63,315.37,7.86;14,151.52,141.59,99.13,7.86">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Portorož, Slovenia</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2016-05">May 2016</date>
			<biblScope unit="page" from="4348" to="4352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,174.47,337.63,7.86;14,151.52,185.43,329.07,7.86;14,151.52,196.39,329.07,7.86;14,151.52,207.34,21.58,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,296.33,174.47,184.25,7.86;14,151.52,185.43,15.16,7.86">Making europe&apos;s historical newspapers searchable</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Neudecker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Antonacopoulos</surname></persName>
		</author>
		<idno type="DOI">10.1109/DAS.2016.83</idno>
		<ptr target="https://doi.org/10.1109/DAS.2016" />
	</analytic>
	<monogr>
		<title level="m" coord="14,188.80,185.43,270.57,7.86">2016 12th IAPR Workshop on Document Analysis Systems (DAS)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-04">April 2016</date>
			<biblScope unit="page" from="405" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,218.30,337.63,7.86;14,151.52,229.26,329.07,7.86;14,151.52,240.22,329.07,7.86;14,151.52,251.18,329.07,7.86;14,151.52,262.14,329.07,7.86;14,151.52,273.10,329.07,7.86;14,151.52,284.06,329.07,7.86;14,151.52,294.99,329.07,7.89;14,151.52,305.98,135.05,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="14,384.22,284.06,96.37,7.86;14,151.52,295.02,186.43,7.86">QURATOR: Innovative Technologies for Content and Data Curation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rehm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bourgonje</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hegele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Kintzel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ostendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zaczynska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Räuchle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rauenbusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rutenburg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Quantz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Böttger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fricke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thomsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paschke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Qundus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Karam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Weichhardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fillies</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Neudecker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Labusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rezanezhad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhöfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Siewert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bunk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Pintscher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aleynikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Heine</surname></persName>
		</author>
		<idno>CoRR abs/2004.12195</idno>
		<ptr target="https://arxiv.org/abs/2004.12195" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,316.93,337.97,7.86;14,151.52,327.89,329.07,7.86;14,151.52,338.85,97.38,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,237.81,316.93,201.30,7.86">A named entity recognition shootout for German</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riedl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Padó</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P18-2020.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="14,463.04,316.93,17.55,7.86;14,151.52,327.89,65.07,7.86">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="120" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,349.81,337.98,7.86;14,151.52,360.77,322.76,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Baiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07592</idno>
		<ptr target="https://arxiv.org/abs/1906.07592" />
		<title level="m" coord="14,249.44,349.81,231.15,7.86;14,151.52,360.77,14.98,7.86">Towards robust named entity recognition for historic german</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.62,371.73,337.98,7.86;14,151.52,382.69,329.07,7.86;14,151.52,393.65,329.07,7.86;14,151.52,404.61,329.07,7.86;14,151.52,415.56,219.96,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,310.49,371.73,170.10,7.86;14,151.52,382.69,195.30,7.86">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">Tjong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sang</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">F</forename><surname>De Meulder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename></persName>
		</author>
		<idno type="DOI">10.3115/1119176.1119195</idno>
		<ptr target="https://doi.org/10.3115/1119176.1119195" />
	</analytic>
	<monogr>
		<title level="m" coord="14,369.38,382.69,111.22,7.86;14,151.52,393.65,269.17,7.86;14,188.52,404.61,45.18,7.86">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
	<note>CONLL &apos;03</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
