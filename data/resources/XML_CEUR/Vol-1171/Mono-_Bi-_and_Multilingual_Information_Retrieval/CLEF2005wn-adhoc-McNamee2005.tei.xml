<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,117.12,86.37,345.97,12.64">Exploring New Languages with HAIRCUT at CLEF 2005</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,259.20,115.80,61.67,8.96"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
							<email>paul.mcnamee@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University Applied Physics Laboratory</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,117.12,86.37,345.97,12.64">Exploring New Languages with HAIRCUT at CLEF 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5061C0CF91BE8CBF7E4321CFF73E0952</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Information Systems]: Content Analysis and Indexing -linguistic processing</term>
					<term>indexing; H.3.3 [Information Systems] : Information Search and Retrieval -query formulation Cross-language information retrieval</term>
					<term>character n-gram tokenization</term>
					<term>pre-translation query expansion</term>
					<term>parallel corpora</term>
					<term>translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>JHU/APL has long espoused the use of language-neutral methods for cross-language information retrieval. This year we participated in the ad hoc cross-language track and submitted both monolingual and bilingual runs. We undertook our first investigations in the Bulgarian and Hungarian languages. In our bilingual experiments we used several nontraditional CLEF query languages such as Greek, Hungarian, and Indonesian, in addition to several western European languages. We found that character n-grams remain an attractive option for representing documents and queries in these new languages. In our monolingual tests n-grams were more effective than unnormalized words for retrieval in Bulgarian (+30%) and Hungarian (+63%). Our bilingual runs made use of subword translation, statistical translation of character n-grams using aligned corpora, when parallel data were available, and web-based machine translation, when no suitable data could be found.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>HAIRCUT <ref type="foot" coords="1,115.32,467.13,3.28,5.83" target="#foot_0">1</ref> is a Java-based information retrieval system that has been developed at the Johns Hopkins University Applied Physics Laboratory. An early version of HAIRCUT was created for use in the TREC-6 evaluation. One of the original issues that we wanted to investigate with the HAIRCUT system was whether character n-gram tokenization was an effective technique for ad hoc text retrieval. Earlier work using ngrams had been viewed with skepticism <ref type="bibr" coords="1,243.36,515.28,11.76,8.96" target="#b2">[3]</ref> and it was our intent to compare n-grams and words in an identical framework (i.e., keeping the retrieval system constant). Our early results were promising and we found that the use of n-grams conveys substantial advantages when non-English collections were used <ref type="bibr" coords="1,479.52,538.20,10.71,8.96" target="#b6">[7]</ref>.</p><p>JHU/APL was a participant in the first CLEF evaluation, and since then, we have been able to apply our techniques in the ten languages explored in the ad hoc tasks, as well as in Chinese, Japanese, and Korean (at NTCIR), and Arabic (at TREC). We have found n-gram tokenization to be surprisingly effective across these diverse languages. We believe n-grams are effective, in part, because they account for morphological variation and provide robustness in the face of slight orthographic mismatching. N-grams also obviate the need to perform decompounding (e.g., in German) or word segmentation (e.g., in Chinese).</p><p>In addition to the use of character n-gram tokenization we make use of a statistical language model of retrieval and combination of evidence from multiple retrievals. For bilingual retrieval we include pretranslation query expansion using comparable collections, statistical translation from aligned parallel collections, and when translation resources are scarce, reliance on language similarity alone. This year we continue experimenting with a technique we first applied at the CLEF 2003 evaluation: subword translation, translation of the constituent n-grams in queries rather than words <ref type="bibr" coords="1,357.12,699.12,10.71,8.96" target="#b7">[8]</ref>. For translation we used aligned parallel corpora instead of bilingual wordlists, when possible, and other resources (e.g., Web-based MT) when not. Subword translation attempts to overcome obstacles in dictionary-based translation, such as word lemmatization, matching of multiple word expressions, and inability to handle out-of-vocabulary words such as common surnames <ref type="bibr" coords="2,158.76,84.72,15.46,8.96" target="#b11">[12]</ref>.</p><p>We submitted official runs for the monolingual and bilingual tracks. For all of our runs we used the HAIRCUT system and a statistical language model similarity calculation. Some of our official runs were based solely on n-gram processing; however, we thought that by using a combination of n-grams and words or stemmed words better performance could sometimes be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>HAIRCUT supports several ways of representing documents using an order independent, bag-of-terms model. Note we are frequently using character n-grams, not words as indexing terms. Our general approach is to process the text of each document, reducing all terms to lower-case. Words were deemed to be whitespace delimited tokens in the text; however, we preserve only the first 4 digits of a number and we truncate any particularly long tokens (those greater than 35 characters in length). We make no attempt at compound splitting. Once words are identified we optionally perform transformations on the words to create indexing terms (e.g., stemming using the Snowball stemmer). Starting in 2003 we began removing diacritical marks, believing that they are of little importance. So-called stopwords are retained in our index and the dictionary is created from all words present in the corpus. At query time we ignore high frequency terms for reasons of efficiency, and because such terms typically add little to query performance. (By default, query terms occurring in greater than 20% of documents are ignored).</p><p>We continue to use a statistical language model for retrieval akin to those presented by Ponte and Croft <ref type="bibr" coords="2,492.36,329.28,16.80,8.96" target="#b12">[13]</ref> and Hiemstra <ref type="bibr" coords="2,127.80,340.80,11.76,8.96" target="#b3">[4]</ref> with Jelinek-Mercer smoothing <ref type="bibr" coords="2,270.84,340.80,11.76,8.96" target="#b4">[5]</ref> (i.e., linear interpolation). In this model, the probability of relevance is given as:</p><formula xml:id="formula_0" coords="2,181.68,363.75,217.96,31.71">[ ] ? ? - Q q C q P a D q P a Q D P ) | ( ) 1 ( + ) | ( = ) | ( ,</formula><p>where Q is a query, D is a document, C is the collection as a whole, and a is a smoothing parameter. The probabilities on the right side of the equation are replaced by their maximum likelihood estimates when scoring a document. The language model has the advantage that term weights are mediated by the corpus. It has been our experience has been that this type of probabilistic model outperforms a vector-based cosine model or a binary independence model with Okapi BM25 weighting.</p><p>Character n-grams, sequences of n consecutive characters, have been used for a number of tasks in human language technology (e.g., spelling correction <ref type="bibr" coords="2,263.64,499.92,15.46,8.96" target="#b13">[14]</ref>, diacritics restoration <ref type="bibr" coords="2,374.52,499.92,15.46,8.96" target="#b10">[11]</ref>, and language identification <ref type="bibr" coords="2,71.04,511.44,10.58,8.96" target="#b0">[1]</ref>). Their use for IR dates to the mid-1970s where they were used primarily as a technique to decrease dictionary size. At that time n=2 or n=3 were typical lengths, and for a fixed alphabet size a substantial reduction in memory requirements could be realized. Over time as physical memory costs fell significantly, research in the mid-1990s led to n-grams being considered as an alternative indexing representation to words or stemmed words (see <ref type="bibr" coords="2,175.08,557.40,10.58,8.96" target="#b2">[3]</ref>). There are several variations on n-gram indexing; here we concentrate on overlapping character n-grams of a fixed length (typically n=4 or n=5). For the text 'prime_minister' and n=7 the resulting n-grams are: '_prime_', 'prime_m', 'rime_mi', 'ime_min', 'me_mini', 'e_minis', 'minist', 'ministe', 'inister', and 'nister_'. The single n-gram 'ime_min' that occurs at the word boundary is fairly distinct indicator of the query phrase 'prime minister' and it would not be generated from a sentence like 'the finance minister ordered prime rib for lunch' which might cause a false match using words alone as indexing terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Task</head><p>For our monolingual work we created indexes for each language using the permissible document fields appropriate to each collection. Our four basic methods for tokenization were unnormalized words, stemmed words obtained through the use of the Snowball stemmer (when available), 4-grams, and 5-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information about each index is shown in Table 1 (below).</head><p>Selection of 4-grams and 5-grams as indexing terms was based on a comprehensive study across the CLEF languages that investigated n-gram length <ref type="bibr" coords="2,251.88,744.48,11.76,8.96" target="#b8">[9]</ref> and established that 4-grams and 5-grams seem to work equally well for monolingual retrieval. Our language model requires a single smoothing constant; we used ?=0.3 with both words and stems, and ?=0.5 with 4-grams and 5-grams. Each of our base runs used blind relevance feedback (queries expanded to 60 terms; terms selected using 20 top-ranked and 75 low-ranked documents from the top 1000). Figure <ref type="figure" coords="3,226.87,108.36,5.04,8.96" target="#fig_0">1</ref> charts performance using our four different term indexing strategies, in isolation. In the Bulgarian and Hungarian languages, substantial benefits were seen when n-grams were used -30% and 63% relative improvements, respectively. In the other languages, n-grams performed similarly to words and somewhat worse than the use of stemmed words (e.g., in English and French). Our previous experience has shown that n-grams produce larger benefits in languages with greater morphological complexity.  Our submitted runs were based on a combination of several base runs using various options for tokenization. Our method for combination is to normalize scores by probability mass and to then merge documents by score. All of our submitted runs were automatic runs and used only the title and description topic fields. We produced three to five runs in each language that were created from combinations of the base runs. Runs were labeled aplmoxx[a-e], where xx indicates the language of interest. Runs whose names end with a terminal 'a' were produced by combining a 5-gram base run with a stemmed word base run; a terminal 'b' indicates fusion of a 4-grams and stemmed words; terminal 'c' is used for runs that used both 4-grams and 5grams; the suffix 'd' indicates solitary use of 4-grams; and, a terminal 'e' indicates the use of 5-grams alone. Monolingual performance based on mean average precision is reported in Table <ref type="table" coords="3,392.75,726.12,3.78,8.96" target="#tab_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual Task</head><p>Our preferred approach to bilingual retrieval is based on the following procedure: (1) apply pre-translation query expansion using the source language CLEF corpus; (2) translate terms statistically using aligned parallel corpora, where terms can be words, stems, or n-grams; (3) and, perform retrieval using the query terms that were projected into the target language, possibly with additional relevance feedback. We have had good success using aligned parallel corpora to extract statistical translations. Others have also relied on corpus-based translation; however, we recently demonstrated significant improvements in bilingual performance by translating character n-grams directly. We call this 'subword translation'. Additionally we also translate stemmed words and words. This year we were only able to use this technique for the English, French, and Portuguese target collections as we lacked parallel resources in Bulgarian and Hungarian.</p><p>For the 2002 and 2003 campaigns we relied on a single source for parallel texts, the Official Journal of the E.U. [15], which is published in the official languages (20 languages as of May 2004). The Journal is available in each of the E.U. languages and consists mainly of governmental topics, for example, trade and foreign relations. For the CLEF 2003 evaluation we had obtained 33 GB of PDF files that we distilled into approximately 300 MB of alignable text, per language. In December 2003 we began the process of mining archival issues of the Journal, beginning with 1998. This process took nearly five months. We obtained data from January 1998 through April 2004 -over six years of data. This is nearly 80 GB of PDF files, or roughly 750 MB of plain text per language. We extracted text using the pdftotext program; however this software cannot extract the Greek data set; we were left with data in ten languages, from which 45 possible alignments are possible. Though focused on European topics, the time span is three to ten years after most of the CLEF-2004 document collection. Though aware of smaller, but aligned parallel data (e.g., Philip Koehn's Europarl corpus <ref type="bibr" coords="4,101.04,624.24,11.34,8.96" target="#b5">[6]</ref>) we did not utilize additional data for reasons of homogeneity and convenience. We managed to use this data for stem-to-stem translation in the CLEF 2004 evaluation and we used this data again this year for word, stem, and n-gram translation.</p><p>To align data between two languages, we would: o convert the data from PDF format to plain text (this introduced some errors, especially when processing diacritical marks in the earlier years); o apply rules for splitting the text into sections (the data was page-aligned, we desired paragraphsized chunks); o and, align files using Church's char_align <ref type="bibr" coords="4,277.56,727.68,10.71,8.96" target="#b1">[2]</ref>.</p><p>To induce a translation for a given source language term, we proceed by: o identifying documents (i.e., approximately paragraphs) containing the source language term; o examining the set of corresponding documents from the target language portion of the aligned collection; o producing a score for each term that occurs in at least one of the target language paragraphs (more on this below); o and finally, selecting the single term with the largest translation score for the source language term. Our method for scoring candidate translations does not require translation model software such as GIZA++. Rather, we rely on information theoretic scores (e.g., symmetric conditional probability or mutual information) to rank terms. We adopt the same technique we rely on for pseudo relevance feedback -a method we have developed called affinity sets. Terms are weighted based on their inverse document frequency (IDF) and the difference between their relative frequency in the set of documents under consideration and the global set of documents. This measure is related to mutual information; however, we believe our technique is more general as it permits the set of documents to be identified through any means, including potentially, query-specific attempts at retrieval and translation.</p><p>We performed pairwise alignments between languages pairs, for example, between English and Portuguese. Once aligned, we indexed each pairwise-aligned collection using the technique described earlier on the CLEF-2005 document collections. That is, we created four indexes per sub-collection, per language -one each of words, stems, 4-grams and 5-grams. This year, rather than create a translation dictionary for every term in a source language index, we translated terms on demand using the algorithm presented above. So far we have been using 1-best translation, but we can generate multiple weighted translations for each term. We have not found this necessary as techniques such as pre-translation query expansion are capable of generating many terms related to a query; thus the harm introduced by a dubious translation is lessened. Our experience on the CLEF 2003 and 2004 bilingual test sets led us to believe that direct translation of 5-grams would likely be the most effective single technique, but that combination using runs generated by translating multiple term types might yield an improvement <ref type="bibr" coords="5,265.56,383.40,15.46,8.96" target="#b9">[10]</ref>.</p><p>Unfortunately, our data from the Official Journal of the EU did not cover two of the target language collections (i.e., Bulgarian and Hungarian). To support translation to or from these languages we relied on query translation using web-based machine translation. We also used MT to use the Greek and Indonesia query sets against English documents. The online services we used are located at:</p><p>• http://babelfish.altavista.com (GR to EN) • http://www.toggletext.com/kataku_trial.php (IN to EN) • http://www.bultra.com/test_e.htm (BG to/from EN) • http://www.tranexp.com/ (HU to/from EN)</p><p>As can be seen in Table <ref type="table" coords="5,176.43,517.80,5.04,8.96" target="#tab_2">3</ref> (below), our results using corpus-based subword translation achieved bilingual performance between 78% and 87% of our best monolingual runs for the given target language. Table <ref type="table" coords="5,504.12,529.32,5.04,8.96" target="#tab_3">4</ref> details our results using available machine translation software. The resultant bilingual performance depends heavily on the individual translation engine used (from 26% to 85% of our best monolingual baselines). In some cases the result of fusing multiple runs using different target-side tokenization of the machine translation output resulted in an improvement, for example, run aplbiidend had a 4% absolute improvement in mean average precision of aplbiidena, which used 5-grams alone. In a couple of cases we directly compared the use of 4-grams and 5-grams on the MT output and found the results to be very similar (e.g., compare aplbienbg[a/e] and aplbienhu[a/e]). In Bulgarian and Hungarian it seems that 4-grams may have a slight advantage over 5-grams, though additional testing should be performed to verify that the differences are statistically significant. However, the use of n-grams over raw words seems clearly indicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>JHU/APL participated in the ad hoc tasks in the CLEF 2005 evaluation, using our language-neutral approach that prominently features character n-gram tokenization and statistical translation using aligned parallel corpora. This year we had to rely on web-based machine translation for mappings between several language pairs, for which we had been unable to obtain suitable parallel data. We compared words, a popular suffix stemmer, and n-grams of lengths four and five on the monolingual collections, all using the same retrieval engine and language model similarity metric. We found that n-grams continued to work well for monolingual retrieval, though their superiority was only apparent in Bulgarian and Hungarian.</p><p>We continued to combine runs produced through disparate retrievals, which, in the past, we have seen a modest (e.g., 10% relative) improvement. This year, however, we noted that our single-best tokenization method outperformed merging of disparate runs (compare Figure <ref type="figure" coords="6,333.12,344.28,5.04,8.96" target="#fig_0">1</ref> and the results in Table <ref type="table" coords="6,436.31,344.28,3.64,8.96" target="#tab_1">2</ref>).</p><p>For bilingual retrieval we employed subword translation in several official runs, with good effect. However we still lack parallel corpora for Bulgarian and Hungarian. We would like to expand on these experiments if we can locate appropriate data. Our results from this year agree with previous findings that character n-grams remain effective and an attractive alternative, especially in languages with complex morphology or ones in which resources (e.g., morphological analyzers or stemmers) are difficult to obtain or use. Our recipe for bilingual retrieval appears effective, but is best accomplished when parallel data are available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,71.04,599.76,337.32,8.96"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Relative effectiveness of tokenization methods on the CLEF 2005 test sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,71.04,188.88,427.42,393.76"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="3,100.25,188.88,340.86,148.72"><row><cell cols="6">. Summary information about the test collection and index data structures</cell></row><row><cell cols="2">language #docs</cell><cell>#rel</cell><cell cols="4">index size (MB) / unique terms (1000s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>words</cell><cell>stems</cell><cell>4-grams</cell><cell>5-grams</cell></row><row><cell>BG</cell><cell>67341</cell><cell>778</cell><cell>57 / 67</cell><cell>---</cell><cell cols="2">154 / 193 251 / 769</cell></row><row><cell>EN</cell><cell cols="6">166754 2063 143 / 302 123 / 236 504 / 166 827 / 916</cell></row><row><cell>FR</cell><cell cols="6">177450 2537 129 / 328 107 / 226 393 / 159 628 / 838</cell></row><row><cell>HU</cell><cell>49530</cell><cell>939</cell><cell>59 / 549</cell><cell>---</cell><cell cols="2">121 / 150 200 / 741</cell></row><row><cell>PT</cell><cell cols="6">210734 2904 178 / 418 140 / 254 529 / 174 868 / 907</cell></row><row><cell></cell><cell></cell><cell cols="3">Effect of Differing Tokenization</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,71.04,73.20,347.76,257.48"><head>Table 2 .</head><label>2</label><figDesc>Official results for monolingual task.</figDesc><table coords="4,161.28,85.20,257.52,245.48"><row><cell>run id</cell><cell cols="2">Fields Terms</cell><cell>MAP</cell><cell cols="2">Rel. Found Relevant</cell></row><row><cell>aplmobgc</cell><cell>TD</cell><cell>4+5</cell><cell>0.3058</cell><cell>706</cell><cell>778</cell></row><row><cell>aplmobgd</cell><cell>TD</cell><cell>4</cell><cell>0.3203</cell><cell>678</cell><cell>778</cell></row><row><cell>aplmobge</cell><cell>TD</cell><cell>5</cell><cell>0.2768</cell><cell>699</cell><cell>778</cell></row><row><cell>aplmoena</cell><cell>TD</cell><cell cols="2">5+snow 0.4346</cell><cell>1930</cell><cell>2063</cell></row><row><cell>aplmoenb</cell><cell>TD</cell><cell cols="2">4+snow 0.4222</cell><cell>1900</cell><cell>2063</cell></row><row><cell>aplmoenc</cell><cell>TD</cell><cell>4+5</cell><cell>0.3898</cell><cell>1877</cell><cell>2063</cell></row><row><cell>aplmoend</cell><cell>TD</cell><cell>4</cell><cell>0.3692</cell><cell>1808</cell><cell>2063</cell></row><row><cell>aplmoene</cell><cell>TD</cell><cell>5</cell><cell>0.3873</cell><cell>1889</cell><cell>2063</cell></row><row><cell>aplmofra</cell><cell>TD</cell><cell cols="2">5+snow 0.4114</cell><cell>2422</cell><cell>2537</cell></row><row><cell>aplmofrb</cell><cell>TD</cell><cell cols="2">4+snow 0.4122</cell><cell>2427</cell><cell>2537</cell></row><row><cell>aplmofrc</cell><cell>TD</cell><cell>4+5</cell><cell>0.3765</cell><cell>2283</cell><cell>2537</cell></row><row><cell>aplmofrd</cell><cell>TD</cell><cell>4</cell><cell>0.3608</cell><cell>2109</cell><cell>2537</cell></row><row><cell>aplmofre</cell><cell>TD</cell><cell>5</cell><cell>0.3801</cell><cell>2274</cell><cell>2537</cell></row><row><cell>aplmohuc</cell><cell>TD</cell><cell>4+5</cell><cell>0.4063</cell><cell>893</cell><cell>939</cell></row><row><cell>aplmohud</cell><cell>TD</cell><cell>4</cell><cell>0.4112</cell><cell>893</cell><cell>939</cell></row><row><cell>aplmohue</cell><cell>TD</cell><cell>5</cell><cell>0.4056</cell><cell>891</cell><cell>939</cell></row><row><cell>aplmoptc</cell><cell>TD</cell><cell>4+5</cell><cell>0.3610</cell><cell>2446</cell><cell>2904</cell></row><row><cell>aplmoptd</cell><cell>TD</cell><cell>4</cell><cell>0.3246</cell><cell>2343</cell><cell>2904</cell></row><row><cell>aplmopte</cell><cell>TD</cell><cell>5</cell><cell>0.3654</cell><cell>2450</cell><cell>2904</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,71.04,632.64,409.44,58.64"><head>Table 3 .</head><label>3</label><figDesc>JHU/APL's official results for bilingual task using corpus-based translation.</figDesc><table coords="5,99.60,644.64,380.88,46.64"><row><cell>run id</cell><cell cols="4">Source Target Fields Terms</cell><cell>MAP</cell><cell cols="2">% mono Rel. Found Relevant</cell></row><row><cell>aplbienfrc</cell><cell>EN</cell><cell>FR</cell><cell>TD</cell><cell cols="3">5-grams 0.3442 78.62%</cell><cell>2108</cell><cell>2537</cell></row><row><cell>aplbienptb</cell><cell>EN</cell><cell>PT</cell><cell>TD</cell><cell cols="3">5-grams 0.3130 85.39%</cell><cell>2053</cell><cell>2904</cell></row><row><cell>aplbiesptb</cell><cell>ES</cell><cell>PT</cell><cell>TD</cell><cell cols="3">5-grams 0.3185 87.16%</cell><cell>2268</cell><cell>2904</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,71.04,705.84,410.16,46.64"><head>Table 4 .</head><label>4</label><figDesc>JHU/APL's official results for bilingual task using machine translation.</figDesc><table coords="5,98.76,717.84,382.44,34.64"><row><cell>run id</cell><cell cols="4">Source Target Fields Terms</cell><cell>MAP</cell><cell cols="2">% mono Rel. Found Relevant</cell></row><row><cell>aplbigrena</cell><cell>GR</cell><cell>EN</cell><cell>TD</cell><cell cols="3">5-grams 0.2418 54.94%</cell><cell>1388</cell><cell>2063</cell></row><row><cell>aplbihuena</cell><cell>HU</cell><cell>EN</cell><cell>TD</cell><cell cols="3">5-grams 0.1944 44.17%</cell><cell>1363</cell><cell>2063</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,76.80,747.48,404.16,8.96"><p>HAIRCUT stands for the Hopkins Automated Information Retriever for Combing Unstructured Text.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,89.04,473.81,419.92,8.10;6,71.04,484.13,240.47,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,215.63,473.81,126.71,8.10">N-Gram Based Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Cavnar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Trenkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,362.88,473.81,146.08,8.10;6,71.04,484.13,165.54,8.10">Proceedings of the Third Symposium on Document Analysis and Information Retrieval</title>
		<meeting>the Third Symposium on Document Analysis and Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="161" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,89.04,494.45,420.10,8.10;6,71.04,504.89,292.31,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,147.66,494.45,262.54,8.10">Char_align: A program for aligning parallel texts at the character level</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,421.44,494.45,87.70,8.10;6,71.04,504.89,235.66,8.10">Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 31st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,89.04,515.21,420.12,8.10;6,71.04,525.53,71.63,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,154.11,515.21,309.10,8.10">Gauging Similarity with n-grams: Language-Independent Categorization of Text</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Damashek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,479.40,515.21,26.04,8.10">Science</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="843" to="848" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,89.04,535.85,420.13,8.10;6,71.04,546.17,177.48,8.10" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,142.92,535.85,195.18,8.10">Using Language Models for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>The Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Center for Telematics and Information Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph. D. Thesis</note>
</biblStruct>

<biblStruct coords="6,89.04,556.49,420.04,8.10;6,71.04,566.93,318.23,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,187.19,556.49,258.15,8.10">Interpolated Estimation of Markov Source Parameters from Sparse Data</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,143.16,566.93,113.95,8.10">Pattern Recognition in Practice</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Gelsema</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Kanal</surname></persName>
		</editor>
		<meeting><address><addrLine>North Holland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page" from="381" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,89.04,577.25,420.15,8.10;6,71.04,587.57,108.58,8.10" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,131.17,577.25,250.57,8.10">Europarl: A multilingual corpus for evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<ptr target="http://www.isi.edu/koehn/publications/europarl/" />
		<imprint/>
	</monogr>
	<note type="report_type">Unpublished</note>
</biblStruct>

<biblStruct coords="6,89.04,597.89,420.12,8.10;6,71.04,608.21,438.11,8.10;6,71.04,618.53,111.48,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,248.04,597.89,164.66,8.10">The JHU/APL HAIRCUT System at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,130.08,608.21,239.55,8.10">Proceedings of the Eighth Text REtrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Eighth Text REtrieval Conference (TREC-8)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="500" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,89.04,628.97,420.05,8.10;6,71.04,639.29,168.71,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,205.03,628.97,242.25,8.10">JHU/APL Experiments in Tokenization and Non-Word Translation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,455.16,628.97,53.93,8.10;6,71.04,639.29,102.12,8.10">Working Notes of the CLEF 2003 Workshop</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,89.04,649.61,420.09,8.10;6,71.04,659.93,154.67,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,217.03,649.61,272.59,8.10">Character N-gram Tokenization for European Language Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,71.04,659.93,77.34,8.10">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="73" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,89.04,670.25,331.92,8.10;6,420.96,668.34,4.69,5.40;6,430.32,670.25,78.83,8.10;6,71.04,680.57,438.08,8.10;6,71.04,691.01,20.51,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,215.20,670.25,107.52,8.10">Translating Pieces of Words</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,334.44,670.25,86.52,8.10;6,420.96,668.34,4.69,5.40;6,430.32,670.25,78.83,8.10;6,71.04,680.57,294.79,8.10">Proceedings of the 28 th Annual International Conference on Research and Development in Information Retrieval (SIGIR-2005)</title>
		<meeting>the 28 th Annual International Conference on Research and Development in Information Retrieval (SIGIR-2005)<address><addrLine>Salvador, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08">August 2005</date>
			<biblScope unit="page" from="643" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,89.04,701.33,420.08,8.10;6,71.04,711.65,371.27,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,212.17,701.33,276.43,8.10">Letter Level Learning for Language Independent Diacritics Restoration</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nastase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,71.04,711.65,295.58,8.10">Proceedings of the 6th Conference on Natural Language Learning (CoNLL-2002)</title>
		<meeting>the 6th Conference on Natural Language Learning (CoNLL-2002)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="105" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,89.04,721.97,420.11,8.10;6,71.04,732.29,310.31,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,300.31,721.97,208.84,8.10;6,71.04,732.29,155.00,8.10">Dictionary-Based Cross-Language Information Retrieval: Problems, Methods, and Research Findings</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pirkola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hedlund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Keskusalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,235.68,732.29,77.34,8.10">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="209" to="230" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,89.04,73.01,420.10,8.10;7,71.04,83.33,438.10,8.10;7,71.04,93.65,106.55,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,208.82,73.01,200.45,8.10">A Language Modeling Approach to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,429.00,73.67,80.14,7.24;7,71.04,83.33,388.50,8.10">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,89.04,103.97,422.22,8.10;7,71.04,114.41,220.91,8.10" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,277.39,103.97,226.18,8.10">The Use of Trigram Analysis for Spelling Error Detection</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Zamora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zamora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,71.04,114.41,151.08,8.10">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="305" to="316" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
