<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,120.56,75.33,354.19,12.64;1,218.96,91.41,157.27,12.64">CLEF 2005: Multilingual Retrieval by Combining Multiple Multilingual Ranked Lists</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,248.48,120.12,26.88,8.96"><forename type="first">Luo</forename><surname>Si</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Language Technology Institute</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,294.81,120.12,51.88,8.96"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Language Technology Institute</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,120.56,75.33,354.19,12.64;1,218.96,91.41,157.27,12.64">CLEF 2005: Multilingual Retrieval by Combining Multiple Multilingual Ranked Lists</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">53060787EC523C3AE0D23D16CC829B3A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval Algorithms</term>
					<term>Experimentation Multilingual retrieval</term>
					<term>Crosslingual retrieval</term>
					<term>Results merging</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We participated in two tasks: Multi-8 two-years-on retrieval and Multi-8 results merging. For our multi-8 two-years-on retrieval work, simple multilingual ranked lists are first built by merging ranked lists of different languages that are generated by single types of retrieval algorithms. Then, algorithms are proposed to combine these simple multilingual ranked lists into a single ranked list. Empirical study shows that the approach of combining multilingual retrieval results can substantially improve the accuracies over single multilingual ranked lists.</p><p>Multi-8 results merging task is our primary interest. This task is viewed as similar to the results merging task of federated search. Query-specific and language-specific models are proposed to calculate comparable document scores for a small amount of documents and estimate logistic models by using information of these documents. The logistic models are used to estimate comparable scores for all documents and thus the documents can be sorted into a final ranked list. A set of experiments demonstrated the advantage of the query-specific and language-specific models against several other alternatives.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The first task as Multi-8 two-years-on is a multilingual retrieval <ref type="bibr" coords="1,366.16,502.44,12.06,8.96" target="#b4">[5,</ref><ref type="bibr" coords="1,378.22,502.44,8.04,8.96" target="#b6">6,</ref><ref type="bibr" coords="1,386.25,502.44,8.04,8.96" target="#b9">9,</ref><ref type="bibr" coords="1,394.29,502.44,12.06,8.96" target="#b13">13,</ref><ref type="bibr" coords="1,406.35,502.44,12.06,8.96" target="#b14">14,</ref><ref type="bibr" coords="1,418.40,502.44,12.06,8.96" target="#b15">15]</ref> task, which is to search documents in eight languages with queries in a single language (i.e., English queries in this work). One method is to tune accurate bilingual retrieval results <ref type="bibr" coords="1,363.25,525.48,16.68,8.96" target="#b14">[14,</ref><ref type="bibr" coords="1,379.93,525.48,12.51,8.96" target="#b15">15]</ref> (or monolingual results of documents in the same language as queries) and then merge the bilingual retrieval results together. For each bilingual run, previous research <ref type="bibr" coords="1,240.21,548.40,12.24,8.96" target="#b4">[5,</ref><ref type="bibr" coords="1,252.46,548.40,12.24,8.96" target="#b14">14,</ref><ref type="bibr" coords="1,264.70,548.40,12.24,8.96" target="#b15">15]</ref> has demonstrated how to do many instances of bilingual retrieval by tuning the methods of translating the query into target language and then generate an accurate bilingual run. Finally, those bilingual runs generated with different methods are merged into a final multilingual ranked list. However, it may not be easy to merge accurate bilingual retrieval results into accurate multilingual retrieval results. One reason is that the ranges and distributions of document scores within these bilingual ranked lists can be very different as quite different retrieval methods have been tuned to generate accurate bilingual results of different languages separately <ref type="bibr" coords="1,420.57,617.40,15.93,8.96" target="#b14">[14,</ref><ref type="bibr" coords="1,436.49,617.40,11.95,8.96" target="#b15">15]</ref>. Therefore, it is difficult to merge those bilingual result lists with quite different characteristics.</p><p>One alternative approach of generating multilingual retrieval result is to first generate simple bilingual runs by same type of retrieval algorithm with the same configuration and then merge the bilingual results into a simple multilingual ranked list. Many simple multilingual results can be obtained by applying different retrieval algorithms with different retrieval configurations. Finally, those simple multilingual ranked lists can be combined into a more accurate multilingual ranked list. This method has been shown to be effective in the work of <ref type="bibr" coords="1,293.77,709.43,11.72,8.96" target="#b4">[5]</ref> by combing multilingual results from retrieval methods based on query translation and document translation. The multilingual retrieval system described in this work focuses on generating multilingual retrieval results by simple retrieval algorithms and also on combining several multilingual retrieval lists together into a final ranked list of high accuracy. In this work, we have proposed several methods to combine multilingual retrieval results. The empirical study shows that the approach of combining multilingual retrieval results can substantially improve the accuracies over single multilingual ranked lists.</p><p>The second task as Multi-8 results merging task is to merge ranked lists of eight different languages (i.e., bilingual or monolingual) into a single final list. This task is very similar to the results merging task of federated search <ref type="bibr" coords="2,190.52,131.64,15.43,8.96" target="#b16">[16]</ref>, which merges multiple ranked lists from different web resources into a single list. Results merging task is our primary interest and our goal is to investigate the effectiveness of applying similar results merging algorithms as federated search task and compare their accuracies with other results merging algorithms.</p><p>Previous research in <ref type="bibr" coords="2,173.66,189.12,16.72,8.96" target="#b14">[14,</ref><ref type="bibr" coords="2,190.38,189.12,12.54,8.96" target="#b15">15]</ref> has proposed to build logistic models to estimate probabilities of relevance for all documents in bilingual ranked lists by their ranks and document scores in these bilingual lists. This method is studied in this paper and a new variant of this method is proposed to improve the merging accuracy. These methods are language-specific methods as they build different models for different languages to estimate the probabilities of relevance. However, for different queries, they apply the same model for documents from a specific language, which may be problematic as documents from this language may contribute different values for different queries (e.g., there are a lot of relevant documents in German for query A but very few for query B).</p><p>Based on this observation, we propose query-specific and language-specific results merging algorithms similar to those of federated search. For each query and each language, a few top ranked documents from each resource are downloaded, indexed and translated to English. Language-independent document scores are calculated for those downloaded documents and a logistic model is built for mapping all document scores in this ranked list to comparable language-independent document scores. Multiple logistic models are built in a similar manner for ranked lists in different languages and comparable scores can be estimated for all documents. Finally, all documents are ranked according to their comparable document scores. Experiments have been conducted to show that query-specific and language-specific merging algorithms outperform several other results merging algorithms. Furthermore, the query-specific and language-specific merging algorithms need to process (i.e., download, index and translate) very limited amount of documents (e.g., 10 per &lt;query, language&gt; pair) to acquire accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Multilingual Retrieval System</head><p>Accurate multilingual retrieval results are generated in this work by combining retrieval results from multiple multilingual retrieval methods. Specifically, we consider retrieval algorithms based on translating queries and retrieval algorithms based on translating documents. This section first describes basic text preprocessing procedures for different languages. Then it presents details of multilingual retrieval algorithms based on query translation and document translation, and finally proposes methods to combine the results from multiple multilingual retrieval algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text Preprocessing</head><p>Stopword Lists: One of the first steps of preprocessing text documents for information retrieval is to throw away stopwords. The Inquery stopword list <ref type="bibr" coords="2,303.82,592.08,11.72,8.96" target="#b2">[3]</ref> is used in this work for English documents. Stopword lists of Finnish, French, German, Italian, Spanish and Swedish are acquired from<ref type="foot" coords="2,459.44,601.41,3.24,5.83" target="#foot_0">1</ref> , while the snowball stopword<ref type="foot" coords="2,165.80,612.93,3.24,5.83" target="#foot_1">2</ref> list is used for Dutch.</p><p>Stemming: After stopwords have been excluded, other content words are stemmed by different stemming algorithms. Porter stemmer is used for English words. Dutch stemming algorithm is acquired from 2 and stemming algorithms from 1 are used for the other six languages.</p><p>Decompounding: Dutch, Finnish, German and Swedish are compound rich languages. All words that appear in the CLEF corpus and have lengths of more than 3 are considered as potential base words. In order to avoid too aggressive decompounding, we only consider base words that have higher collection frequencies than the word in consideration. Specifically, linking elements as -s-, -e-, and -en are considered for Dutch, no linking elements for Finnish, elements as -s-, -n-, -e-and -en-for German and -s-, -e-, -u-and -o-for Swedish. The same set of decompounding procedure has been used in previous research <ref type="bibr" coords="3,162.58,97.20,10.65,8.96" target="#b7">[7]</ref>.</p><p>Parallel corpus for word translation: Online machine translation <ref type="bibr" coords="3,348.41,120.12,12.26,8.96" target="#b4">[5,</ref><ref type="bibr" coords="3,360.67,120.12,12.26,8.96" target="#b14">14,</ref><ref type="bibr" coords="3,372.92,120.12,12.26,8.96" target="#b15">15]</ref> systems have been utilized to translate queries and documents for multilingual information retrieval systems. However, online systems may be updated, converted to commercial use and become unavailable <ref type="bibr" coords="3,451.26,143.16,15.40,8.96" target="#b13">[13]</ref>. If free connections still exist, the translation via online systems is associated with large amount of communication cost and can be very slow. Therefore, the translation process in this work is mainly accomplished in a word-by-word manner by using translation matrices generated by parallel corpus. Specifically, the parallel corpus of European Parliament proceedings 1996-2001<ref type="foot" coords="3,412.52,186.93,3.24,5.83" target="#foot_2">3</ref> is used to build seven pairs of models between English and other seven languages. The GIZA++ <ref type="bibr" coords="3,393.36,200.64,16.72,8.96" target="#b10">[10]</ref> tool is utilized to build the mappings of translating English words into words of the other languages or translating words in other languages into English words. Each translation pair is associated with a probability value that indicates how probable the translation is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multilingual Retrieval via Query Translation</head><p>One straightforward multilingual retrieval method is to translate English queries into other languages, and then search those translated queries and merge the retrieval results from different languages into a single multilingual ranked list.</p><p>English query words are first translated into words in other languages by using translation matrices built from parallel corpus. Each English word is translated into top three candidates in the translation matrices of other languages. All the three translated words of an English word are associated with normalized weights (i.e., the sum of the weights is 1) according to the weights in translation matrices. One implicit problem of translating words with the help from parallel corpus is that some English words may not have translations as the vocabulary of the parallel corpus is limited. Therefore, we utilize word-by-word translation results from online machine translation software Systran<ref type="foot" coords="3,479.24,408.21,3.24,5.83" target="#foot_3">4</ref> as a complement. As the number of words within the queries (total 60 queries) is limited, the communication cost of acquiring these translations from Systran is small. Specifically, all English queries terms are translated into words in six other languages except Dutch (Systran does not provide translation service from English to Dutch). These sets of translation representations are combined with translations built from parallel corpus while weight of 0.2 is assigned to the translation by Systran and weight of 0.8 is assigned to the translation with parallel corpus.</p><p>The translated queries are used to search indexes built in each language. Okapi <ref type="bibr" coords="3,437.95,502.44,16.77,8.96" target="#b11">[11,</ref><ref type="bibr" coords="3,454.72,502.44,12.58,8.96" target="#b12">12]</ref> retrieval algorithm is applied to accomplish this and each query term is weighted by its weight in the translation representation. Bilingual retrieval results are acquired for those translated queries as well as monolingual results of English queries. As the same retrieval algorithm is applied on corpus of different languages with original/translated queries of the same lengths (i.e., the sum of weights of words in translated queries is always equal to the length of original English query), the raw scores in the ranked lists are somewhat comparable. Therefore, these ranked lists are merged together by their resource-specific scores into a final ranked list.</p><p>Another multilingual retrieval algorithm based on query translation takes advantage of query expansion by pseudo relevance feedback. Specifically, for resource of each language, query expansion is accomplished by adding 10 most common query terms within top 10 ranked documents of the initial retrieval result. The refined queries are used to generate new ranked lists of the resources and the ranked lists are then merged together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multilingual Retrieval via Document Translation</head><p>An alternative multilingual retrieval method is to translate all documents in other languages into English and apply the same original English queries. This method may have advantage against the retrieval method based on query translation as the translation of longer documents may better represent the semantic meaning than the translation of short queries. Previous research <ref type="bibr" coords="4,409.30,85.68,11.72,8.96" target="#b4">[5]</ref> has also shown that the translation of a word from another language to English may be complementary to the translation from English to this language. For example, although one term in English may not be correctly translated into the corresponding German word, this German word may be correctly translated into the English term.</p><p>The document translation work is conducted using translation matrices built from parallel corpus. For each word in a language other than English, its top three English translations are considered. Five word slots are allocated to the three candidates of each untranslated word with proportion to the normalized translation probabilities of the three words. All the translated documents as well as the original English documents are collected into a single database and indexed. Furthermore, the Okapi retrieval algorithm is applied on the single indexed database with original English queries to retrieve documents. Okapi retrieval algorithm without query expansion as well as Okapi retrieval algorithm with query expansion by pseudo relevance feedback (i.e., 10 additional query terms from top 10 ranked documents) is used in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Combine Multilingual Ranked Lists</head><p>The basic assumption of improving ranking accuracy by combining ranked lists is that relevant documents are generally retrieved by multiple multilingual retrieval algorithms while different retrieval algorithms tend to retrieve different irrelevant documents. Similar idea has been successfully utilized in Metasearch of information retrieval <ref type="bibr" coords="4,234.83,352.92,10.69,8.96" target="#b0">[1]</ref>.</p><p>Therefore, one simple combination algorithm is proposed to favor documents retrieved by more retrieval methods as well as high ranking documents retrieved by single types of retrieval methods. Let drs k_mj denote the resource-specific raw document score for the jth document retrieved from the mth ranked list for kth query, drs k_m_max and drs k_m_min represents the maximum and minimum document scores in this ranked list respectively. Then, the normalized score of the jth document is calculated as:</p><formula xml:id="formula_0" coords="4,193.88,431.95,282.60,32.17">) ( ) ( min _ _ max _ _ min _ _ _ _ m k m k m k mj k mj k rs rs rs rs s d d d d d - - = (1)</formula><p>where ds k_mj is the normalized document score. After the normalization step, the document scores among all ranked lists are summed up for a specific document and all documents can be ranked accordingly. Note that this method can be seen as a variant of the well-known CombSUM <ref type="bibr" coords="4,452.15,492.46,11.66,8.99" target="#b8">[8]</ref> algorithm for Meta information retrieval. This method is called equal weight combination method in this work.</p><p>One particular issue about the proposed simple combination method is that it uses linear method (i.e., Equation <ref type="formula" coords="4,129.89,538.42,4.18,8.99" target="#formula_2">1</ref>) to normalize document scores and it treats the votes from multiple systems with equal weights. It is possible to design better score normalization method as well as more sophisticated weights for different systems in order to achieve better ranking accuracy. The idea is used in our algorithm to learn better score normalization method and the weights of systems with the help of training data. Formally, let us assume that there are M ranked lists to combine, and the normalized document scores of the mth ranked list are calculated as in Equation <ref type="formula" coords="4,389.30,595.90,3.77,8.99" target="#formula_2">1</ref>. Then the final combined document scores for a specific document d is calculated as:</p><formula xml:id="formula_1" coords="4,183.56,622.51,292.92,26.61">∑ = = M m r m m final m d score w M d score 1 ) ( 1 ) (<label>(2)</label></formula><p>where score final (d) is the final combined document score and score m (d) (which is zero if the document is not in the mth ranked list) represents the normalized score for this document from the mth ranked list. are the model parameters, where the pair of (w m , r m ) represents the weight of the vote and the exponential normalization factor for the mth ranked list respectively. The final ranked list can thus be generated with respect to the final scores calculated from Equation <ref type="formula" coords="4,472.17,707.49,3.77,9.00" target="#formula_1">2</ref>.</p><p>Maximizing ranking accuracy is the rule to derive desired model parameters of this combination model. In this work, ranking accuracy is represented formally as mean average precision (MAP) criterion. Let us assume that there are K training queries, the MAP criterion is represented formally as:</p><formula xml:id="formula_2" coords="5,218.12,380.42,258.36,31.02">∑ ∑ + ∈ + k D j k k j j rank K ) (<label>1 (3)</label></formula><p>where + k D is the set of the ranks of relevant documents in the final ranked list for kth training query, and ) ( j rank k + is the corresponding rank only among relevant documents. The multilingual retrieval task of CLEF as well as many other information retrieval evaluations uses the MAP criterion to evaluate retrieval accuracy.</p><p>In order to avoid the overfitting problem of model parameter estimation, two regularization items are introduced for w and r respectively. Together with the ranking accuracy criterion in Equation 3, the training optimization problem is represented as follows: After the desired model parameters have been estimated, it can be applied on test queries to combine ranked lists of different retrieval systems. This method is called learning combination method in this work.</p><formula xml:id="formula_3" coords="5,111.68,522.82,364.80,31.57">∑ ∑ ∑ ∑ = = ∈ + - - - -         = + M m m M m m k D j k r w b r a w j j rank K r w k 1 2 1 2 , * ) * 2 ) 1 ( * 2 ) 1 ( ) ( 1 (log max arg ) , (<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Results: Multilingual Retrieval</head><p>Multilingual retrieval results are composed of documents from different languages. Therefore, it is helpful to first investigate the retrieval accuracies of results from single types of languages. Table <ref type="table" coords="5,500.41,698.48,5.01,9.01" target="#tab_0">1</ref> shows the bilingual retrieval results by translating English queries into other languages and the monolingual retrieval result of English. All the runs have utilized query term expansion by pseudo relevant feedback as described in Section 2. It can be seen that the retrieval accuracies of singe types of languages vary from 0.35 to 0.46. queries. Query expansion has been used for this retrieval method and the additional terms are extracted from the top 10 ranked documents among all English documents and the translated documents from other languages. It can be seen that although the retrieval method based on query expansion and the retrieval method based on document expansion generate similar results on several language (e.g., German and Spanish), their effectiveness on some other languages (e.g., Dutch and Finnish) are rather different. Note that the monolingual retrieval results of English with methods based on query translation and document translation are not exactly the same due to different configurations of retrieval methods (e.g., different query expansion methods).</p><p>The English monolingual retrieval results and bilingual retrieval results are merged together into a multilingual ranked list by the raw document scores. Table <ref type="table" coords="6,340.47,189.08,5.01,9.01">3</ref> shows the results of five multilingual retrieval algorithms on training queries (first 20 queries), test queries (next 40 queries) and the overall accuracy. It can be seen that multilingual retrieval algorithms based on query translation and algorithms based on document translation produce results of similar accuracy (i.e., 0.327-0.356), while the retrieval method based on document expansion that does not use query expansion has a small advantage. The results from multilingual retrieval system by <ref type="bibr" coords="6,352.04,246.56,16.73,9.01" target="#b15">[15]</ref> (merged by the trained logistic transformation model by maximizing MAP as described in Section 4.1) are also shown in Table <ref type="table" coords="6,480.82,258.08,5.01,9.01">3</ref> as it is considered in this work for multilingual result combination. It can be seen that the accuracy of UniNE system is very close to the other four algorithms. Table <ref type="table" coords="6,344.09,281.12,5.01,9.01">3</ref> suggests that ranked lists of individual multilingual retrieval systems may not be very effective compared to bilingual results (i.e., bilingual results in Table <ref type="table" coords="6,154.18,304.16,3.76,9.01" target="#tab_0">1</ref>, 2 and 5).</p><p>One key step to improve the accuracy of multilingual retrieval result is to combine results of several multilingual retrieval methods. Two combination methods described in Section 2 as equal weight combination method and learning combination method are applied in this work. They are used to combine the results of the five retrieval algorithms described above. The combination results are shown in Table <ref type="table" coords="6,128.48,373.04,3.77,9.01">4</ref>. It can be seen that the accuracies of combined multilingual result lists are substantially higher than the accuracies of results from single types of multilingual retrieval algorithms. This demonstrates the power to combine multilingual retrieval results. Detailed analysis shows that although the training combination method is consistently better than the equal weight combination method for the same configurations (i.e., the same number of ranked lists to combine), its advantage is very small. One possible reason is that the accuracies of the five retrieval algorithms are close and it does not make too much difference to adjust the voting weights among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results Merge for Multilingual Retrieval</head><p>The second task we participated in CLEF 2005 is results merging for multilingual retrieval. Two sets of ranked lists across eight different languages are provided within this task and the goal is to merge these individual ranked lists together into two single lists with high accuracy. This is a difficult task as: i).</p><p>Ranked lists from different languages may have different score ranges due to different retrieval strategies such as methods of query translation or query expansion <ref type="bibr" coords="6,370.38,543.32,16.05,9.01" target="#b14">[14,</ref><ref type="bibr" coords="6,386.44,543.32,12.04,9.01" target="#b15">15]</ref>; ii). The corpus statistics (e.g., inverse document frequency) of different languages may be quite different; and iii). In multilingual federated search environment, there is no control over retrieval algorithms that the resources use. These characteristics make it hard to directly compare document scores among ranked lists of different languages.</p><p>Previous research <ref type="bibr" coords="6,165.12,612.32,16.75,9.01" target="#b14">[14,</ref><ref type="bibr" coords="6,181.87,612.32,12.56,9.01" target="#b15">15]</ref> has proposed solution of learning query-independent and language-specific model by relevance judgment of previous queries to transform language-specific document scores into probabilities of relevance so that documents across different languages can be ranked by their estimated probabilities of relevance. However, this method may not be very accurate as a single queryindependent transformation model is built for each language but the retrieved results of different queries of this language may have different characteristics. An alternative approach is to index all returned documents across different languages and apply a retrieval algorithm with the same retrieval strategy to compute comparable document scores. This method can be more accurate than the first approach as results from different queries are treated separately. However, this approach is associated with a large amount of computation costs and possible communication costs (i.e., when documents of different languages can only be accessed by sending requests to remote servers).</p><p>In this paper, a new approach is proposed to learn query-specific and language-specific models of translating language-specific document scores into comparable document scores. In particular, a small set of documents from each language is indexed at retrieval time to compute comparable document scores, and then a query-specific and language-specific model is trained by both comparable document scores and language-specific document scores of this small set of documents. By applying these models on ranked lists of all languages, comparable document scores can be obtained for all the returned documents and the final ranked list can be achieved. This approach has an advantage to avoid the requirement of human relevance judgment data for training. It only uses automatically computable document scores as surrogate of relevance judgment data and thus is similar as the semi-supervised learning results merging method in federated search <ref type="bibr" coords="7,321.03,177.56,15.41,9.01" target="#b16">[16]</ref>. Empirical study shows that this new approach is effective and high accuracy can be achieved by indexing a small number of documents at retrieval time.</p><p>This section is organized as follows: In section 4.1, an approach of learning query-independent and language-specific logistic transformation merging model is described and a new extension of learning the model by maximizing mean average precision is proposed; In Section 4.2, we describe the new approach of learning query-specific and language-specific result merging algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learn Query-Independent and Language-Specific Merging Model via Relevance Training Data</head><p>To make the retrieved results from different ranked lists comparable, one natural idea is to map all the document scores into the probabilities of relevance and rank all documents accordingly. Particularly, logistic transformation model has been successfully utilized in previous study to achieve this goal <ref type="bibr" coords="7,89.96,366.68,15.87,9.01" target="#b14">[14,</ref><ref type="bibr" coords="7,105.83,366.68,11.90,9.01" target="#b15">15]</ref>. This method has been shown to be more effective than round robin results merging, raw score results merging and several other alternatives. Let us assume that there are altogether I ranked lists from different languages to be merged, each of them provides J documents for each query and there are altogether K training queries with human relevance judgment. Particularly, d k_ij represents the jth document from the ith language of training query k. The pair (r k_ij , ds k_ij ) represents the rank of this document and the document score (normalized by Equation <ref type="formula" coords="7,365.46,424.16,4.19,9.01" target="#formula_2">1</ref>) respectively. By the logistic transformation model, the estimated probability of relevance of this document is:</p><formula xml:id="formula_4" coords="7,166.64,456.68,309.84,24.82">) exp( 1 1 ) | ( _ _ _ i ij k i ij k i ij k c ds b r a d rel P + + + = (5)</formula><p>where a i ,b i and c i are the parameters of language-specific model that transforms all document scores of different queries from the ith language into the corresponding probabilities of relevance. The optimal parameter values are acquired generally by maximizing the log-likelihood (MLE) of training data, which is formally represented as:</p><formula xml:id="formula_5" coords="7,186.44,541.98,290.04,24.97">)) | ( log( ) | ( _ , , _ * ij k j i k ij k d rel P d rel P ∑ (6)</formula><p>where P*(rel|d k_ij ) is the empirical probability value of a particular document. This is derived from human relevance judgment data, which is 1 when this document is relevant and 0 otherwise. This objective function is a convex function, which has only one global optimal solution.</p><p>One particular issue of training logistic transformation model by maximizing log-likelihood is that it equally treats each relevant document. However, this may not be a desired criterion in real world application. For example, a relevant document out of total 2 relevant documents for a query is generally more important to users than a relevant document out of total 100 relevant documents for another query. Therefore, queries are generally treated equally in information retrieval evaluation instead of individual relevant documents. This is formally represented by the mean average precision (MAP) criterion as described in Equation 3. The multilingual retrieval task of CLEF as well as many other information retrieval tasks uses the MAP criterion to evaluate retrieval accuracy. The new algorithm of training logistic model for mean average precision is called logistic model with MAP goal in this paper. This is believed to be a more direct method to improve the MAP value of multilingual retrieval system in CLEF and also a better method to reflect users' preference than training logistic model with MLE criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One natural extension of training logistic transformation model by</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learn Query-Specific and Language-Specific Merging Model</head><p>One particular problem about language-specific logistic transform merging model introduced in Section 4.1 is that it applies the same model on results of different queries from each language. This is problematic when result lists of different queries have similar score distributions but have different distributions of probability of relevance. This suggests that query-specific model should be studied for high merging accuracy of multilingual retrieval.</p><p>Previous research has proposed query-specific merging method that uses the two step Retrieval Status Values (RSV) <ref type="bibr" coords="8,148.99,285.68,11.44,9.01" target="#b9">[9,</ref><ref type="bibr" coords="8,160.43,285.68,11.44,9.01" target="#b13">13]</ref>. For each query, this method indexes top ranked documents of different languages at the retrieval time and computes comparable document scores. One choice is to translate all top ranked documents (i.e., 1000) of different languages into a single language, index them and apply a singe centralized retrieval algorithm to generate a final ranked list. However, this method is associated with a large amount of computation costs of translating and indexing many documents.</p><p>In multilingual federated search environment, the cost of processing retrieved documents is even higher as the contents of all documents to translate are not directly accessible and they must be downloaded from corresponding servers. This is also true for a multilingual federated search environment where contents of all available documents cannot be directly crawled into a single centralized database. This means that generally the corpus statistics (e.g., corpus inverse document frequencies) are not available and can only be simulated by collecting statistics from sampled documents.</p><p>To propose methods that work in stricter environments, we follow the multilingual federated search approach in this research. There exists a resource that contains all documents from one language. These resources provide searching services of their documents.</p><p>Query-based sampling method is utilized in this work to learn corpus statistics from each resource with a particular language <ref type="bibr" coords="8,178.08,492.67,10.64,9.01" target="#b3">[4]</ref>. Specifically, random one-term queries are sent to each resource and retrieve about 4 documents for each query. Altogether 3,000 documents are collected from each resource. The sampled documents from each resource are collected together to create the centralized sample database for this resource so that corpus statistics such as inverted document frequencies can be estimated from this database.</p><p>The above paragraphs describe the procedures to acquire estimated corpus statistics in multilingual federated search environment. With this information, retrieved documents from individual resources will be assigned comparable document scores and merged into a single final ranked list. Previous research <ref type="bibr" coords="8,129.16,596.11,11.70,9.01" target="#b4">[5]</ref> and the empirical results of two-years-on multilingual retrieval task in this work demonstrate the advantage of utilizing evidence by both translating queries and translating documents. The goal of the query-specific and language-specific results merging algorithms in this work is to assign comparable document scores to all retrieved documents by combining document scores of retrieval methods based on query translation and scores based on document translation.</p><p>To obtain comparable document scores based on query translation, the original English query is first translated into other languages in word-by-word manner by using translation matrices as described in Section 2. These translated queries and the original English query are sent to the eight resources and retrieve eight sets of individual ranked lists. As in federated search environment, it is generally difficult to require all resources to use the same type of retrieval algorithm with the same type of configuration (e.g., Okapi with the same feedback procedure). The returned document scores may not be directly comparable. Therefore, to compute comparable scores of retrieved documents, the documents need to be downloaded and indexed, and then the same retrieval algorithm is applied on the downloaded documents with the same configuration. Particularly, the retrieved documents are downloaded and an Okapi retrieval algorithm is applied on these documents with corpus statistics from the centralized sample database of the corresponding resource.</p><p>Comparable document scores based on document translation are acquired by applying a single Okapi retrieval method on all retrieved English documents and all the translated documents from resources with other languages. Specifically, all retrieved documents in languages other than English are first translated into English in word-by-word manner using translation matrices, and then are merged into a single set of documents with documents that are originally in English. Furthermore, this set of documents is indexed and an Okapi retrieval algorithm is applied on this set of documents with corpus statistic from the centralized sample database of English resource. As this results merging method downloads (also indexes and translates) all documents in the given ranked lists, it is called complete downloading method.</p><p>Two sets of comparable document scores based on retrieval methods of query translation and document translation are merged together into a single set with the method described in Section 2. Specifically, the two sets of scores are first normalized separately and then are combined into a new ranked list with the equal weight combination method described in Section 2.</p><p>It can be noted from the description that a large amount of online costs is associated with the complete downloading result merging algorithm. Within federated search environment, communication cost is associated with downloading each document. Furthermore, computation costs as indexing and translation are also associated to process each downloaded document. These problems are particularly serious as they happen in an online manner. Therefore, a more efficient algorithm is much more desired for operational system.</p><p>The key idea to calculate comparable document scores more efficiently is to only calculate scores for a small set of representative documents. Particularly, a small set of retrieved documents from each resource is selected; the above procedure of downloading and calculating new scores based on query translation and document translation is applied on this set of documents. These documents that have both language-specific scores and calculated comparable scores serve as training data for learning a logistic model, which estimates the comparable document scores for other documents that have not been downloaded and indexed.</p><p>Generally top ranked documents of retrieved documents from each resource are more probable to be relevant, they are selected for downloading and calculating comparable document scores. Let us assume top L documents from the ranked list of each resource are downloaded to calculate comparable scores. Let the pair (dc k'_il , ds k'_il ) denote the normalized comparable document score and normalized language-specific score for the lth downloaded document of the ith resource for k' the query. Let the pair (a k'_i , b k'_i ) denote the parameters of the corresponding query-specific and language-specific model. These parameters are learned by solving the following optimization problem to minimize the mean squared error between exact normalized comparable scores and the estimated comparable scores as: <ref type="bibr" coords="9,465.92,558.29,10.56,8.10" target="#b7">(7)</ref> where D L is the downloaded L documents from the resource and D NL is a pseudo set of L documents with pseudo normalized comparable scores zero and pseudo normalized language-specific scores zero. This set of pseudo documents is introduced in order to make sure that the learned model ranks documents in the correct way (i.e., documents with higher language-specific scores are ranked higher in the ranked list with comparable scores than documents with lower language-specific scores).</p><formula xml:id="formula_6" coords="9,129.80,545.02,253.92,30.82">( ) 2 ) , ( * _ ' * _ ' ) ) 1 exp( 1 1 ( min arg , _ ' _ ' _ ' * + * + - = ∑ ∨ ∈ b d a d b a il k NL L il k il k S D D d C b a i k i k</formula><p>Finally, logistic models can be learned for all resources in the same way. They are applied to all retrieved documents from all resources and the documents can be ranked according to their estimated comparable scores. Note that only language-specific document scores are used in the logistic model in Equation 7 while document ranks in language-specific ranked lists are not considered. This is different from Equation <ref type="formula" coords="9,151.52,696.68,3.76,9.01">5</ref>, and is used here in order to reduce the number of parameters for the limited amount of data (i.e., the small set of documents with both comparable scores and language-specific scores).</p><p>Note that exact comparable document scores are available for the documents that have been downloaded and processed. One method to take advantage of these scores is to combine them with the estimated scores. In this work, they are combined together with equal weights (i.e., 0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results: Results Merge</head><p>This section presents the experiment results of different results merging algorithms for ranked lists of different languages. These results merging algorithms work on two sets of ranked lists from UniNE system <ref type="bibr" coords="10,121.49,322.64,16.79,9.01" target="#b14">[14,</ref><ref type="bibr" coords="10,138.28,322.64,12.60,9.01" target="#b15">15]</ref> and HummingBird system 5 . Both of the two sets are composed of ranked lists from eight different languages. The language-specific retrieval accuracies of ranked lists of UniNE and HummingBird systems are shown in Table <ref type="table" coords="10,265.27,345.68,5.01,9.01" target="#tab_3">5</ref> and Table <ref type="table" coords="10,315.59,345.68,5.01,9.01" target="#tab_4">6</ref> respectively. It can be seen from Table <ref type="table" coords="10,482.69,345.68,5.01,9.01" target="#tab_3">5</ref> that all language-specific ranked lists generated by UniNE system except ranked list of Finnish have high accuracy. On the other side, the accuracies of ranked lists generated by HummingBird system are much lower than those of the UniNE system. These two sets of ranked lists are good candidates to evaluate the effectiveness of merging algorithms for both accurate ranked lists and inaccurate ranked lists.</p><p>The first two results merging algorithms to evaluate are the query-independent and language-specific results merging algorithms by optimizing the maximum likelihood criterion (MLE) and the mean average precision (MAP) criterion respectively. Their merging accuracies on both the ranked lists of UniNE system and HummingBird systems are shown in Table <ref type="table" coords="10,358.55,449.12,5.01,9.01">7</ref> and Table <ref type="table" coords="10,413.77,449.12,3.77,9.01">8</ref>. It can be seen that merging accuracies of the UniNE system is much higher than those of the HummingBird system. This is consistent with our expectation as the language-specific ranked lists of UniNE system are better than those of HummingBird system. The merging accuracies of learning algorithms on UniNE system are similar to those reported in <ref type="bibr" coords="10,201.21,495.20,15.32,9.01" target="#b15">[15]</ref>. Furthermore, it can be seen from both Tables <ref type="table" coords="10,407.43,495.20,5.01,9.01">7</ref> and<ref type="table" coords="10,432.27,495.20,5.01,9.01">8</ref> that the learning algorithm optimized for mean average precision criterion is always more accurate than that optimized for maximum likelihood criterion. This demonstrates the power to directly optimize for mean average precision accuracy as treating different queries equally against the strategy of optimizing for maximum likelihood that does not directly evaluate mean average precision. However, the merging accuracy is not good compared to bilingual runs.</p><p>To improve the merging accuracy, query-specific and language-specific algorithms are introduced. Two types of algorithms are evaluated in this work. The first method downloads all documents from ranked lists of different languages and calculates comparable document scores (C_X). The second method downloads top ranked documents and calculates their comparable documents to build logistic models. These models generate estimated comparable document scores and finally combine the estimated scores with acquired exact comparable scores wherever they are available (Top_X_C05). The experimental results of different variants of these algorithms on UniNE system and HummingBird system are shown in Tables <ref type="table" coords="10,205.98,656.11,5.01,9.01">9</ref> and<ref type="table" coords="10,231.65,656.11,10.03,9.01" target="#tab_0">10</ref> respectively. Note that both these two algorithms do not require human relevance judgment for training data. Therefore, the results on training query set and test query set are obtained separately without using any relevance judgment data. It can be seen from Table <ref type="table" coords="10,203.15,690.67,5.01,9.01">9</ref> and Table <ref type="table" coords="10,257.20,690.67,9.93,9.01" target="#tab_0">10</ref> that both these two query-specific and language-specific merging algorithms substantially outperform query-independent and language-specific algorithms. The accuracies of the two query-specific and language-specific methods (i.e., C_X and Top_X_C05) are close on the UniNE system. It is interesting that the Top_150_C05 method outperforms all C_X runs on the UniNE system. This means that the combination of estimated comparable scores and exact comparable scores can be more accurate than exact comparable scores in some cases. Detailed analysis shows that the estimation of comparable document scores is related with document scores from ranked lists of UniNE systems. The estimated document scores can be seen as combination results from not only the two retrieval methods that based on query translation and document translation but also the retrieval method of UniNE system. Therefore, the combined results that are related with three retrieval systems may be better than those of exact comparable scores from two retrieval systems. It is encouraging to see that with very limited amount of downloaded documents, the Top_10_C05 method still has more than 10 percent advantage over the query-independent and language-specific result merging algorithms.</p><p>It can be seen from Table <ref type="table" coords="11,202.96,425.60,9.93,9.01" target="#tab_0">10</ref> that the advantage of query-specific and language-specific algorithms over query-independent and language-specific algorithms is even larger for the results on HummingBird system than those on UniNE system. This demonstrates the power of query-specific and language-specific merging algorithms for ineffective ranked lists. It is interesting to note that the Top_X_C05 runs are not as effective as C_X runs on HummingBird System. The reason can be explained that the ranked lists of HummingBird system are not as accurate as those of UniNE systems. The influence of document scores within ranked lists of HummingBird on the estimated comparable score is not as helpful as that from the UniNE system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion:</head><p>This paper describes the algorithms we have studied and proposed for the CLEF 2005 evaluation tasks as: Multi-8 two-years-on retrieval task and Multi-8 results merging task.</p><p>For multi-8 two-years-on retrieval task, our focus is to generate and combine multilingual retrieval results that are built from simple bilingual (or monolingual) ranked lists. Specifically, we first generate multiple multilingual retrieval results by merging bilingual (or monolingual) retrieval results of same types of retrieval algorithms, and then combine the multilingual retrieval results together. Several combination methods have been proposed and empirical studies have demonstrated that the combination of multilingual retrieval results can substantially improve the accuracies over single multilingual ranked lists.</p><p>The task of Multi-8 results merging task is to merge two sets of eight bilingual (or monolingual for English) ranked lists into multilingual ranked lists. This is the primary interest of our work and we have proposed to apply results merging algorithm of federated search task for this problem. Top ranked documents within each ranked list are indexed and translated to compute comparable document scores. Query-specific and language-specific logistic models are built based on comparable document scores of these documents and also the scores of these documents in language-specific ranked lists. These logistic models have been built to estimate comparable document scores for all documents in ranked lists of different languages, and finally all documents are sorted accordingly. Experiments have shown that the new proposed methods outperform previous research and they only need to process (i.e., download, index and translate) very small amount of documents (e.g., 10 per &lt;query, language&gt; pair) to acquire accurate results.</p><p>Although query-specific and language-specific merging algorithm algorithms are much better than previous merging methods, in some cases their accuracies are still not at bilingual levels (e.g., UniNE systems). This suggests the necessity of more sophisticated result merging algorithms for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>Research presented in this paper has been supported in part by an ARDA grant under Phase II of the AQUAINT program. Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors', and do not necessarily reflect those of the sponsor.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.96,562.52,24.35,9.01;5,140.60,560.82,3.00,5.41;5,136.88,562.52,3.33,9.01;5,128.60,562.52,2.50,9.01;5,118.52,562.52,17.82,9.01;5,122.24,562.52,6.68,9.01;5,145.88,562.52,359.43,9.01;5,89.96,576.56,415.27,9.01;5,89.96,588.08,262.16,9.01"><head></head><label></label><figDesc>model parameters and (a,b) are two regularization factors that are set to 4 in this work. This problem is not a convex optimization problem and multiple local maximal values exist. A common solution is to search with multiple initial points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,74.84,78.05,444.01,686.92"><head>Table 1 .</head><label>1</label><figDesc>Table 2  shows the monolingual English retrieval result and bilingual retrieval results by translating documents in other languages into English and searching with English Language-specific retrieval accuracy in mean average precision of retrieval results based on query translation with query term expansion.</figDesc><table coords="5,88.40,78.05,414.56,66.66"><row><cell>Language</cell><cell>Dutch</cell><cell>English</cell><cell>Finnish</cell><cell>French</cell><cell>German</cell><cell>Italian</cell><cell>Spanish</cell><cell>Swedish</cell></row><row><cell>All(MAP)</cell><cell>0.441</cell><cell>0.436</cell><cell>0.361</cell><cell>0.454</cell><cell>0.448</cell><cell>0.421</cell><cell>0.462</cell><cell>0.354</cell></row><row><cell>Language</cell><cell>Dutch</cell><cell>English</cell><cell>Finnish</cell><cell>French</cell><cell>German</cell><cell>Italian</cell><cell>Spanish</cell><cell>Swedish</cell></row><row><cell>All(MAP)</cell><cell>0.386</cell><cell>0.460</cell><cell>0.434</cell><cell>0.418</cell><cell>0.442</cell><cell>0.415</cell><cell>0.439</cell><cell>0.357</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,76.52,147.29,440.77,224.22"><head>Table 2 .</head><label>2</label><figDesc>Language-specific retrieval accuracy in mean average precision of retrieval results based on document translation with query term expansion.</figDesc><table coords="5,88.16,177.05,417.70,194.46"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Methods</cell><cell>Train</cell><cell>Test</cell><cell>All</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>M2_W1</cell><cell>0.384</cell><cell>0.431</cell><cell>0.416</cell></row><row><cell>Methods Qry_fb Doc_nofb Qry_nofb Doc_fb UniNe</cell><cell>Train 0.317 0.346 0.312 0.327 0.322</cell><cell>Test 0.353 0.360 0.335 0.332 0.330</cell><cell>All 0.341 0.356 0.327 0.330 0.327</cell><cell>M2_Trn M3_W1 M3_Trn M4_W1 M4_Trn M5_W1 M5_Trn</cell><cell>0.389 0.373 0.383 0.382 0.389 0.401 0.421</cell><cell>0.434 0.423 0.431 0.432 0.434 0.446 0.449</cell><cell>0.419 0.406 0.415 0.415 0.419 0.431 0.440</cell></row><row><cell cols="4">Table 3. Mean average precision of multilingual retrieval methods. Qry means by query translation. Doc means by document translation, nofb means no pseudo relevance feedback, fb means pseudo relevant back.</cell><cell cols="4">Table 4. Mean average precision of merged multilingual list of different methods. M_X means to combine X results in the order of: 1). query translation with feedback, 2). document translation without feedback, 3). query translation without query</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">expansion, 4). document translation with query</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">expansion and 5). UniNE system. W1: means</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">combine with equal weight, Trn means combine with</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>trained weights.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,89.96,726.32,415.34,38.27"><head></head><label></label><figDesc>MLE criterion is to train the model with MAP criterion. Particularly, different sets of model parameters {a i ,b i and c i, 1&lt;=i&lt;=I} generate The training procedure of maximizing MAP searches for a set of model parameters that generates the highest MAP value. However, this problem is not a convex optimization problem and multiple local maximal values exist. A common solution is to search with multiple initial points.</figDesc><table coords="7,89.96,753.01,227.38,11.57"><row><cell>different sets of relevant documents as</cell><cell>{D k</cell><cell>,1</cell><cell>&lt;=</cell><cell>k</cell><cell>&lt;=</cell><cell>K</cell><cell>}</cell></row></table><note coords="7,266.12,748.81,3.30,7.83;7,318.92,754.40,166.57,9.01"><p><p>+</p>and thus achieve different MAP values.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,88.28,78.05,437.98,691.62"><head>Table 5 .</head><label>5</label><figDesc>Language-specific retrieval accuracy in mean average precision of retrieval results from UniNE system.</figDesc><table coords="10,89.96,759.66,198.68,10.02"><row><cell>5 http://www.hummingbird.com/products/searchserver/</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,86.24,142.73,430.74,8.10"><head>Table 6 .</head><label>6</label><figDesc>Language-specific retrieval accuracy in mean average precision of retrieval results from HummingBird system</figDesc><table coords="11,79.76,77.69,435.75,205.86"><row><cell>Methods</cell><cell>Train</cell><cell>Test</cell><cell>All</cell><cell>Methods</cell><cell>Train</cell><cell>Test</cell><cell>All</cell></row><row><cell>Top_150_C05</cell><cell>0.360</cell><cell>0.412</cell><cell>0.395</cell><cell>Top_150_C05</cell><cell>0.278</cell><cell>0.297</cell><cell>0.291</cell></row><row><cell>Top_30_C05</cell><cell>0.357</cell><cell>0.399</cell><cell>0.385</cell><cell>Top_30_C05</cell><cell>0.260</cell><cell>0.268</cell><cell>0.265</cell></row><row><cell>Top_15_C05</cell><cell>0.346</cell><cell>0.402</cell><cell>0.383</cell><cell>Top_15_C05</cell><cell>0.235</cell><cell>0.253</cell><cell>0.247</cell></row><row><cell>Top_10_C05</cell><cell>0.330</cell><cell>0.393</cell><cell>0.372</cell><cell>Top_10_C05</cell><cell>0.222</cell><cell>0.248</cell><cell>0.239</cell></row><row><cell>Top_5_C05</cell><cell>0.296</cell><cell>0.372</cell><cell>0.347</cell><cell>Top_5_C05</cell><cell>0.210</cell><cell>0.234</cell><cell>0.226</cell></row><row><cell>C_500</cell><cell>0.356</cell><cell>0.384</cell><cell>0.374</cell><cell>C_500</cell><cell>0.315</cell><cell>0.333</cell><cell>0.326</cell></row><row><cell>C_150</cell><cell>0.352</cell><cell>0.391</cell><cell>0.378</cell><cell>C_150</cell><cell>0.290</cell><cell>0.302</cell><cell>0.298</cell></row><row><cell>C_1000</cell><cell>0.356</cell><cell>0.382</cell><cell>0.373</cell><cell>C_1000</cell><cell>0.324</cell><cell>0.343</cell><cell>0.337</cell></row><row><cell cols="4">Table 9. Mean average precision of merged</cell><cell cols="4">Table 10. Mean average precision of merged</cell></row><row><cell cols="4">multilingual lists of different methods on UniNE</cell><cell cols="4">multilingual lists of different methods on</cell></row><row><cell cols="4">result lists. Top_x indicates x top documents are</cell><cell cols="4">HummingBird result lists. Top_x indicates x top</cell></row><row><cell cols="4">downloaded to generate logistic transformation</cell><cell cols="4">documents are downloaded to generate logistic</cell></row><row><cell cols="4">model, C05 indicates both scores from logistic</cell><cell cols="4">transformation model, C05 indicates both scores from</cell></row><row><cell cols="4">transformation model and centralized document</cell><cell cols="4">logistic transformation model and centralized document</cell></row><row><cell cols="4">scores are utilized when they are available and they</cell><cell cols="4">scores are utilized when they are available and they are</cell></row><row><cell cols="4">are combined with a linear weight as 0.5. C_X</cell><cell cols="4">combined with a linear weight as 0.5. C_X means top</cell></row><row><cell cols="4">means top X documents from each list are merged by</cell><cell cols="4">X documents from each list are merged by their</cell></row><row><cell cols="2">their centralized doc scores.</cell><cell></cell><cell></cell><cell>centralized doc scores.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,95.24,751.13,109.38,8.10"><p>http://www.unine.ch/info/clef/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,95.24,761.57,121.66,8.10"><p>http://www.snowball.tartarus.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,95.24,751.13,199.78,8.10"><p>http://people.csail.mit.edu/koehn/publications/europarl/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,95.24,761.57,141.10,8.10"><p>http://www.systransoft.com/index.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,100.66,290.36,404.79,9.01;12,110.96,301.80,379.78,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,261.95,290.36,92.86,9.01">Models for Metasearch</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,374.60,290.40,130.85,8.96;12,110.96,301.80,375.87,8.96">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,100.66,313.28,404.49,9.01;12,110.96,324.80,304.88,9.01" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,338.79,313.28,166.36,9.01;12,110.96,324.80,135.54,9.01">The Mathematics of Statistical Machine Translation: Parameter Estimation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,253.16,324.84,105.08,8.96">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="312" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,100.66,336.32,404.48,9.01;12,110.96,347.84,196.17,9.01" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,295.70,336.32,203.51,9.01">TREC and TIPSTER experiments with INQUERY</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Broglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,110.96,347.88,164.56,8.96">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,100.66,359.24,404.58,9.01;12,110.96,370.76,159.45,9.01" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,247.71,359.24,160.35,9.01">Query-based sampling of text databases</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,415.40,359.28,89.84,8.96;12,110.96,370.80,80.17,8.96">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="130" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,100.66,382.28,377.45,9.01" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="12,251.68,382.28,201.27,9.01">Cross-language Retrieval Experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,483.05,382.28,22.30,9.01;12,110.96,393.80,288.46,9.01" xml:id="b5">
	<monogr>
		<title level="m" coord="12,160.76,393.84,234.26,8.96">Results of the CLEF2002 cross-language evaluation forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,100.66,405.32,404.64,9.01;12,110.96,416.84,394.27,9.01;12,110.96,428.24,299.98,9.01" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,457.36,405.32,47.94,9.01;12,110.96,416.84,378.63,9.01">Dublin City University at CLEF 2004: Experiments in Monolingual, Bilingual and Multilingual Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Judge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khasin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lam-Adesina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,172.40,428.28,234.16,8.96">Results of the CLEF2004 cross-language evaluation forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,100.66,439.76,404.56,9.01;12,110.96,451.28,394.35,9.01;12,110.96,462.84,26.37,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,431.32,439.76,73.91,9.01;12,110.96,451.28,83.58,9.01">The University of Amsterdam at CLEF</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><forename type="middle">De</forename><surname>Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Börkur</forename><surname>Sigurbjörnsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,295.04,451.32,210.27,8.96;12,110.96,462.84,21.97,8.96">Results of the CLEF2003 cross-language evaluation forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,100.66,474.32,404.78,9.01;12,110.96,485.88,344.09,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,180.52,474.32,175.32,9.01">Analyses of multiple evidence combination</title>
		<author>
			<persName coords=""><forename type="middle">J H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,375.20,474.36,130.24,8.96;12,110.96,485.88,340.18,8.96">Proceedings of the 20th Annual Int&apos;l ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 20th Annual Int&apos;l ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,100.66,497.24,404.65,9.01;12,110.96,508.76,389.50,9.01" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,331.16,497.24,174.15,9.01;12,110.96,508.76,72.33,9.01">SINAI on CLEF 2002: Experiments with merging strategies</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><forename type="middle">M</forename><surname>Martinez-Santiago</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Urena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,261.92,508.80,234.14,8.96">Results of the CLEF2002 cross-language evaluation forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.41,520.28,400.00,9.01;12,110.96,531.80,337.64,9.01" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,250.92,520.28,158.46,9.01">Improved Statistical Alignment Models</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,428.84,520.32,76.56,8.96;12,110.96,531.84,280.28,8.96">Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.41,543.32,399.93,9.01;12,110.96,554.76,154.42,8.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,238.35,543.32,147.79,9.01">Experiments using the Lemur toolkit</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,403.76,543.36,101.57,8.96;12,110.96,554.76,149.73,8.96">Proceedings of the Tenth Text Retrieval Conference (TREC-10)</title>
		<meeting>the Tenth Text Retrieval Conference (TREC-10)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.41,566.24,399.78,9.01;12,110.96,577.76,394.15,9.01;12,110.96,589.32,273.15,8.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,254.59,566.24,250.60,9.01;12,110.96,577.76,139.55,9.01">Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,268.40,577.80,236.71,8.96;12,110.96,589.32,269.25,8.96">Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.41,600.80,399.80,9.01;12,110.96,612.32,394.43,9.01;12,110.96,623.76,135.34,8.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,257.78,600.80,247.42,9.01;12,110.96,612.32,195.88,9.01">CONTROL: CLEF-2003 with Open, Transparent Resources Off-Line. Experiments with merging strategies</title>
		<author>
			<persName coords=""><forename type="first">Rogati</forename><forename type="middle">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,394.88,612.36,105.31,8.96">Results of the CLEF</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
	<note>cross-language evaluation forum</note>
</biblStruct>

<biblStruct coords="12,105.40,635.24,399.95,9.01;12,110.96,646.76,299.91,9.01" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,176.51,635.24,313.12,9.01">Report on CLEF-2002 Experiments: Combining multiple sources of evidence</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,172.40,646.80,234.10,8.96">Results of the CLEF2002 cross-language evaluation forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.41,658.28,399.92,9.01;12,110.96,669.84,135.27,8.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,177.01,658.28,143.20,9.01">Report on CLEF-2003 Experiments</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,402.44,658.32,102.88,8.96;12,110.96,669.84,130.87,8.96">Results of the CLEF2003 cross-language evaluation forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.40,681.32,399.76,9.01;12,110.96,692.72,302.24,9.01" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,238.78,681.32,266.39,9.01;12,110.96,692.72,29.26,9.01">A Semi-Supervised Learning Method to Merge Search Engine Results</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,157.76,692.76,171.15,8.96">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="457" to="491" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
