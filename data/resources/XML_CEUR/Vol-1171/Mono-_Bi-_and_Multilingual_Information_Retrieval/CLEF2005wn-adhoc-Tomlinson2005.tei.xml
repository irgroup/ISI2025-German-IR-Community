<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,112.77,146.21,377.45,18.08;1,117.34,168.13,225.07,18.08;1,342.41,166.35,22.10,12.55;1,371.20,168.13,114.45,18.08">European Ad Hoc Retrieval Experiments with Hummingbird SearchServer TM at CLEF 2005</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2005-08-21">August 21, 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,259.83,203.18,83.34,10.46;1,271.20,217.12,60.62,10.46"><forename type="first">Stephen</forename><surname>Tomlinson Hummingbird</surname></persName>
							<email>stephen.tomlinson@hummingbird.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Ottawa</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,112.77,146.21,377.45,18.08;1,117.34,168.13,225.07,18.08;1,342.41,166.35,22.10,12.55;1,371.20,168.13,114.45,18.08">European Ad Hoc Retrieval Experiments with Hummingbird SearchServer TM at CLEF 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2005-08-21">August 21, 2005</date>
						</imprint>
					</monogr>
					<idno type="MD5">3DA653D3D3D87E072766A92FC1F80653</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval Measurement</term>
					<term>Performance</term>
					<term>Experimentation Bulgarian Retrieval</term>
					<term>Hungarian Retrieval</term>
					<term>First Relevant Score</term>
					<term>Per-Topic Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hummingbird participated in the 4 monolingual information retrieval tasks (Bulgarian, French, Hungarian and Portuguese) of the Ad-Hoc Track of the Cross-Language Evaluation Forum (CLEF) 2005. In the ad hoc retrieval tasks, the system was given 50 natural language queries, and the goal was to find all of the relevant documents (with high precision) in a particular document set. We conducted diagnostic experiments with different techniques for matching word variations and handling stopwords. We found that the experimental stemmers significantly increased mean average precision for the 4 languages. Analysis of individual topics found that the algorithmic Bulgarian and Hungarian stemmers encountered some unanticipated stopword collisions. A comparison to an experimental 4-gram technique suggested that Hungarian stemming would further benefit from decompounding. A blind feedback technique which significantly increased mean average precision for some languages was also significantly detrimental to the rank of the first relevant retrieved for one language.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hummingbird SearchServer<ref type="foot" coords="1,208.85,669.79,3.97,7.32" target="#foot_0">1</ref> is a toolkit for developing enterprise search and retrieval applications. The SearchServer kernel is also embedded in other Hummingbird products for the enterprise.</p><p>SearchServer works in Unicode internally <ref type="bibr" coords="1,292.67,694.78,10.51,10.46" target="#b2">[3]</ref> and supports most of the world's major character sets and languages. The major conferences in text retrieval experimentation (CLEF <ref type="bibr" coords="1,499.71,706.73,9.96,10.46" target="#b1">[2]</ref>, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>The CLEF 2005 Ad-Hoc Track document sets consisted of tagged (SGML-formatted) news articles in 4 different languages: Bulgarian, French, Hungarian and Portuguese. Table <ref type="table" coords="2,433.46,363.15,4.98,10.46" target="#tab_0">1</ref> gives the sizes.</p><p>The CLEF organizers created 50 natural language "topics" (numbered 251-300) and translated them into many languages. One topic was discarded for Bulgarian because it had no relevant documents. Table <ref type="table" coords="2,171.69,399.02,4.98,10.46" target="#tab_0">1</ref> gives the final number of topics for each language and their average number of relevant documents (along with the lowest, median and highest number of relevant documents of any topic). For more information on the CLEF test collections, see the track overview paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Indexing</head><p>Our indexing approach was the mostly the same as last year <ref type="bibr" coords="2,350.57,469.20,14.61,10.46" target="#b14">[15]</ref>. Accents were not indexed except for the combining breve in Bulgarian. The apostrophe was treated as a word separator for the 4 investigated languages. The custom text reader, cTREC, was updated to maintain support for the CLEF guidelines of only indexing specifically tagged fields. Some stop words were excluded from indexing (e.g. "the", "by" and "of" in English). For these experiments, the stop word list for Portuguese was based on the Porter list <ref type="bibr" coords="2,425.73,528.97,9.96,10.46" target="#b6">[7]</ref>, and the lists for Bulgarian and Hungarian were based on Savoy's <ref type="bibr" coords="2,303.92,540.93,9.96,10.46" target="#b8">[9]</ref>. We used our own list for French.</p><p>Unlike previous years, this year we added AL="0-9" to the stopfiles to specify that the digits 0-9 were to be treated as alphabet characters (e.g. so that "G7" would be indexed as 1 term instead of 2).</p><p>By default, the SearchServer index supports both exact matching (after some Unicode-based normalizations, such as decompositions and conversion to upper-case) and morphological matching (e.g. inflections, derivations and compounds, depending on the linguistic component used).</p><p>For many languages (including French and Portuguese), SearchServer provides the option of finding inflections based on lexical stemming (i.e. stemming based on a dictionary or lexicon for the language). For example, in English, "baby", "babied", "babies", "baby's" and "babying" all have "baby" as a stem. Specifying an inflected search for any of these terms will match all of the others. The lexical stemming of the post-6.0 experimental development version of SearchServer used for the experiments in this paper was based on internal stemming component 3.7.0.15. We treat each linguistic component as a black box in this paper.</p><p>Lexical stemming in SearchServer typically does "inflectional" stemming which generally retains the part of speech (e.g. a plural of a noun is typically stemmed to the singular form). It typically does not do "derivational" stemming which would often change the part of speech or the meaning more substantially (e.g. "performer" is not stemmed to "perform").</p><p>Lexical stemming in SearchServer includes compound-splitting (decompounding) for compound words in particular languages (such as Dutch, Finnish, German and Swedish). For example, in German, "babykost" (baby food) has "baby" and "kost" as stems.</p><p>Lexical stemmers can produce more than one stem, even for non-compound words. For example, in English, "axes" has both "axe" and "axis" as stems (different meanings), and in French, "important" has both "important" (adjective) and "importer" (verb) as stems (different parts of speech). SearchServer records all the stem mappings at index-time to support maximum recall and does so in a way to allow searching to weight some inflections higher than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Searching</head><p>We experimented with the SearchServer CONTAINS predicate. Our test application specified SearchSQL to perform a boolean-OR of the query words. For example, for Bulgarian topic 279 whose Title was "Референдуми в Швейцария" (Swiss referendums), a corresponding SearchSQL query would be:</p><formula xml:id="formula_0" coords="3,90.00,300.27,271.93,46.33">SELECT RELEVANCE('2:3') AS REL, DOCNO FROM CLEF05BG WHERE FT_TEXT CONTAINS 'Референдуми'|'в'|'Швейцария' ORDER BY REL DESC;</formula><p>(Note that "в" is a stopword for Bulgarian so its inclusion in the query wouldn't actually add any matches.)</p><p>Most aspects of the SearchServer relevance value calculation are the same as described last year <ref type="bibr" coords="3,111.29,395.91,14.61,10.46" target="#b14">[15]</ref>. Briefly, SearchServer dampens the term frequency and adjusts for document length in a manner similar to Okapi <ref type="bibr" coords="3,198.04,407.87,10.51,10.46" target="#b7">[8]</ref> and dampens the inverse document frequency using an approximation of the logarithm. These calculations are based on the stems of the terms (roughly speaking) when doing morphological searching (i.e. when SET TERM_GENERATOR 'word!ftelp/inflect' was previously specified). The SearchServer RELEVANCE_METHOD setting was set to '2:3' and RELEVANCE_DLEN_IMP was set to 750 for all experiments in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Diagnostic Runs</head><p>For the diagnostic runs listed in Tables 2, the run names consist of a language code ("BG" for Bulgarian, "FR" for French, "HU" for Hungarian and "PT" for Portuguese) followed by one of the following labels:</p><p>• "lex": (FR and PT only): The run used SearchServer lexical stemming. The /inflect option (SET TERM_GENERATOR 'word!ftelp/inflect') was specified.</p><p>• "lexnos": Same as "lex" except that /nostop was additionally specified which prevents query terms from being discarded if all of their stems are stopwords (note that stopwords themselves were still not found because they were not indexed).</p><p>• "lexall": Same as "lex" except that a separate index was used which did not stop any words from being indexed (specifying /nostop would make no difference with this index).</p><p>• "lexsing": Same as "lex" except that /single was additionally specified (so that just one stemming interpretation was used at search time).</p><p>• "neu" (BG and HU only): Same as "lex" except that the experimental Neuchatel stemmer was used <ref type="bibr" coords="3,156.79,699.22,9.96,10.46" target="#b8">[9]</ref>.</p><p>• "neunos": Same as "lexnos" except that the Neuchatel stemmer was used.</p><p>• "neuall": Same as "lexall" except that the Neuchatel stemmer was used. • "neuposs" (HU only): Same as "neu" except that the call to the remove_possessive function was skipped. (Prof. Savoy suggested to us that it was unclear if removing possessive pronouns was a good idea, which we interpreted as uncertainty about the remove_possessive function.)</p><p>• "sn" (FR and PT only): Same as "lex" except that the Porter (Snowball) stemmer <ref type="bibr" coords="5,482.87,153.98,10.51,10.46" target="#b6">[7]</ref> was used.</p><p>• "snru" (BG only): Same as "neu" except that the Porter (Snowball) stemmer for Russian was used.</p><p>• "4gram": Same as "lexall" except that the run used a different index which primarily consisted of the 4-grams of terms, e.g. the word 'search' would produce index terms of 'sear', 'earc' and 'arch'. No stemming was done; searching used the IS_ABOUT predicate (instead of the CONTAINS predicate) with morphological options disabled to search for the 4-grams of the query terms.</p><p>• "none": The run disabled morphological searching. (The run used the same index as "lex" for FR and PT and the same index as "neu" for HU and BG, but SET TERM_GENERATOR '' was specified so that variations from stemming were not matched.)</p><p>Note that all diagnostic runs just used the Title field of the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Evaluation Measures</head><p>Traditionally in ad hoc retrieval experiments, the primary evaluation measure is "average precision". For a topic, it is the average of the precision after each relevant document is retrieved (using zero as the precision for relevant documents which are not retrieved). By convention, it is based on the first 1000 retrieved documents for the topic. The score ranges from 0.0 (no relevants found) to 1.0 (all relevants found at the top of the list). Average precision takes into account both precision and recall, and it is very good for detecting retrieval differences because even small differences in the ranks of relevant documents affect the score. "Mean Average Precision" (MAP) is the mean of the average precision scores over all of the topics (i.e. all topics are weighted equally).</p><p>If one wishes to focus on just the first relevant document, the traditional measure is "Reciprocal Rank" (RR). For a topic, it is 1 r where r is the rank of the first row for which a desired page is found, or zero if a desired page was not found. "Mean Reciprocal Rank" (MRR) is the mean of the reciprocal ranks over all the topics.</p><p>An experimental measure introduced in this paper (along with the companion web retrieval paper <ref type="bibr" coords="5,117.79,530.68,15.49,10.46" target="#b11">[12]</ref>) is "First Relevant Score" (denoted "FRS"). Like reciprocal rank, it is based on just the rank of the first relevant retrieved for a topic, but it is better suited to per-topic analysis. FRS is 1.08 1-r where r is the rank of the first row for which a desired page is found, or zero if a desired page was not found. Like reciprocal rank, finding the first relevant at rank 1 produces a score of 1.0. At rank 2, FRS is just 7 points lower (0.93), whereas RR is 50 points lower (0.50). At rank 3, FRS is another 7 points lower (0.86), whereas RR is 17 points lower (0.33). At rank 10, FRS is 0.50, whereas RR is 0.10. FRS is greater than RR for ranks 2 to 52 and lower for ranks 53 and beyond. A possible interpretation of FRS is that it may be an indicator of the percentage of potential result list reading the system saved the user to get to the first relevant, assuming that users are less and less likely to continue reading as they get deeper into the result list.</p><p>"Success@n" is the percentage of topics for which at least one relevant document was returned in the first n rows. Like the other first relevant measures, this measure hides a lot of retrieval differences (particularly in recall), but it is more intuitive and may be an indicator of a user's impression of a method's robustness across topics. This paper lists Success@1, Success@5 and Success@10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Statistical Significance Tables</head><p>For tables comparing 2 diagnostic runs (such as Table <ref type="table" coords="5,328.99,744.16,3.87,10.46" target="#tab_2">3</ref>), the columns are as follows:</p><p>• "Expt" specifies the experiment. The language code is given, followed by the labels of the 2 runs being compared. The difference is the first run minus the second run. For example, "FR lex-none" specifies the difference of subtracting the scores of the French 'none' run from the French 'lex' run (of Table <ref type="table" coords="6,246.79,146.40,3.87,10.46" target="#tab_1">2</ref>).</p><p>• "∆MAP" is the difference of the mean average precision scores of the two runs being compared (and "∆FRS" is the difference of the (mean) FRS scores).</p><p>• "95% Conf" is an approximate 95% confidence interval for the difference (calculated from plus/minus twice the standard error of the mean difference). If zero is not in the interval, the result is "statistically significant" (at the 5% level), i.e. the feature is unlikely to be of neutral impact (on average), though if the average difference is small (e.g. &lt;0.020) it may still be too minor to be considered "significant" in the magnitude sense.</p><p>• "vs." is the number of topics on which the first run scored higher, lower and tied (respectively) compared to the second run. These numbers should always add to the number of topics (49 for Bulgarian, 50 for the others).</p><p>• "3 Extreme Diffs (Topic)" lists 3 of the individual topic differences, each followed by the topic number in brackets (the topic numbers range from 251 to 300). The first difference is the largest one of any topic (based on the absolute value). The third difference is the largest difference in the other direction (so the first and third differences give the range of differences observed in this experiment). The middle difference is the largest of the remaining differences (based on the absolute value).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results of Morphological Experiments</head><p>In the per-topic analysis, the official topic translations were used as much as possible. Online translation services were consulted at times ( <ref type="bibr" coords="6,293.00,436.29,10.79,10.46" target="#b4">[5]</ref> was sometimes helpful for Hungarian, and we found the Russian-to-English translations at <ref type="bibr" coords="6,291.90,448.24,10.51,10.46" target="#b0">[1]</ref> often worked for Bulgarian). Prof. Savoy also assisted with some Bulgarian words. But any translation errors are the responsibility of the author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Impact of Stemming</head><p>Table <ref type="table" coords="6,116.63,518.42,4.98,10.46" target="#tab_2">3</ref> isolates the impact of stemming on the average precision measure (e.g. "FR lex-none" is the difference of the "FR-lex" and "FR-none" runs of Table <ref type="table" coords="6,325.46,530.37,3.87,10.46" target="#tab_1">2</ref>). For each of the 4 languages, the increase in mean average precision was statistically significant (i.e. zero was not in the approximate 95% confidence interval). In FRS, there was higher variance, and only the increase for Hungarian was statistically significant. Note that for some queries, it was still better to only match the original query form (not variations from stemming); SearchServer allows this option to be controlled for each query term at search-time. Table <ref type="table" coords="6,132.52,602.10,4.98,10.46" target="#tab_2">3</ref> shows that topic 279 (Swiss referendums) was substantially affected by stemming for all 4 languages, so we examine it for each language:</p><p>• HU-279 (Svájci népszavazások): Without Hungarian stemming, no document contained both of the query terms. No relevant document contained the query word 'népszavazások'. Only some of the relevant documents even contained 'Svájci' (and lots of non-relevants also did). With stemming, average precision was 87 points higher from extra matches such as 'svájciak', 'Svájc', 'Svájcban', 'Svájcot', 'Svájcról', 'népszavazáson', 'népszavazás', 'népszavazást' and 'népszavazással'.</p><p>• BG-279 (Референдуми в Швейцария): With Bulgarian stemming, average precision was 58 points higher from extra matches for 'referendums' such as референдум and референдума. • PT-279 (Referendos suíços): The query word 'suíços' was common in the relevant documents, but many relevants just used 'referendo' and not the query word 'referendos'. Average precision was 35 points higher with Portuguese stemming; extra matches included 'referendo', 'suíço', 'suíça' and 'suíças'.</p><p>• FR-279 (Référendums en Suisse): This French topic scored lower with stemming (the rank of the first relevant fell from 1 to 13, and average precision fell from 0.10 to 0.01). It appears that the relevant documents were more likely to use the plural 'Référendums' than the singular 'Référendum', and the latter was a more common word which generated lots of matches when stemming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Impact of Experimental /nostop Option</head><p>Table <ref type="table" coords="7,117.06,603.36,4.98,10.46" target="#tab_3">4</ref> isolates the impact of using the SearchServer /nostop option. The option had no effect on the 50 French and Portuguese topics, and it affected only a few of the Bulgarian and Hungarian topics. The /nostop option prevents query terms from being discarded if all of their stems are stopwords (note that stopwords themselves are still not found because they are not indexed). The default is to not use /nostop because past experiments otherwise found a lot of spurrious matches in some languages (such as Finnish and Korean). We investigate some of the topics flagged in Table <ref type="table" coords="7,117.39,675.09,3.87,10.46" target="#tab_3">4</ref>:</p><p>• HU-265 (A Deutsche Bank szerzeményei (Deutsche Bank Takeovers)): The query word 'Bank' stemmed to 'ban' (in) which was a stopword, so by default, the word 'Bank' was not matched in the documents. With the /nostop option, 'Bank' was matched and average precision was 13 points higher. (Incidentally, this issue is presumably why Table <ref type="table" coords="7,478.70,732.21,4.98,10.46" target="#tab_2">3</ref> shows that stemming scored 12 points lower on HU-265; without stemming, 'Bank' was found in the documents.) Perhaps this issue would not have arisen with a lexical stemmer which would preserve the meaning more closely.</p><p>• HU-292 (Német városok újjáépítése (Rebuilding German Cities)): The query word 'Német' (German) stemmed to 'nem' (not) which was a stopword and so this useful word was dropped from the query by default. With the /nostop option, average precision was 40 points higher.</p><p>• HU-282 (Elítéltekkel szembeni durva bánásmód (Prison Abuse)): In this topic, the default scored higher. Using /nostop changed the rank of the first relevant from 3 to 7. The stopword list contained 'szemben' (in front of), and the query word 'szembeni' presumably is a related noise word, and discarding it was useful. The /nostop option kept 'szembeni', which only occurred in 319 documents, so it had a high enough weighting from inverse document frequency to hurt precision.</p><p>• BG-273 (Разширяването на НАТО (NATO Expansion)): НАТО (NATO) stemmed to НА (on) which was a stopword, so the default behaviour removed a key word from the query. With /nostop, the first relevant score was 80 points higher.</p><p>• BG-267 (Най-добрите чуждоезикови филми (Best Foreign Language Films)): The query word филми (films) stemmed to филм (film) which surprisingly was a stopword, so the default behaviour discarded a key query term. Our supplier <ref type="bibr" coords="8,378.70,332.92,10.51,10.46" target="#b8">[9]</ref> has confirmed that this was an error in the Bulgarian stopword list.</p><p>• BG-257 (Етническото прочистване на Балканите (Ethnic Cleansing in the Balkans)): The query word Балканите (Balkans) stemmed to балкан (Balkan mountain) which surprisingly was a stopword. Even though it turned out that precision was a little higher without the Balkans term in this case, in general this appears to be another error in the stopword list.</p><p>In the topics we examined, in 3 cases the default behaviour of dropping useful terms may have been from the stemmers for Bulgarian and Hungarian being algorithmic instead of lexical (a lexical stemmer typically does not change the meaning of a word, except when words are ambiguous). It appears for algorithmic stemmers it may be better to use the /nostop option by default.</p><p>In another 2 cases, it appears the stoplist was in error, which illustrates the usefulness of the CLEF judged test collections: they enable an analyst who does not understand a language to find issues in a resource for the language and make inferences about its quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Impact of Indexing All Words</head><p>Table <ref type="table" coords="8,118.64,539.81,4.98,10.46" target="#tab_4">5</ref> isolates the impact of indexing all words (i.e. of not using a stopword list). None of the mean differences were statistically significant, but Bulgarian and Hungarian had some large per-topic differences in average precision which we investigate:</p><p>• HU-292 (Német városok újjáépítése (Rebuilding German Cities)): We saw earlier that this topic benefitted from the /nostop option (average precision up 40 points), but when indexing all words, average precision fell back (33 points). The reason was that the common word 'nem' (not) was now indexed, so 'Német' (German), which stems to 'nem' with the algorithmic stemmer, had a much lower inverse document frequency than before, and this useful word received less weight. (Even if it had received more weight, there would have been potential confusion with all the indexed occurrences of 'nem'.)</p><p>• BG-271 (Бракове между хомосексуални (Gay Marriages)): The stopword между (between) was not in the 2 relevant documents. When it was indexed, its inclusion caused some nonrelevants to be preferred, and average precision dropped 55 points.</p><p>• BG-295 (Пране на пари (Money Laundering)): This topic scored higher when indexing all words. Surprisingly, the word пари (money) was a stopword, presumably another error (the Bulgarian stoplist apparently needs a review). It seems fine that на (on) was a stopword. In practice, indexing all words may not be so troublesome because it is typically easy for users to omit noise words from the query, and stemming issues can be worked around by disabling the finding of word variants (SearchServer makes it optional at search-time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison to 4-grams</head><p>Compound words appear to be fairly common in Hungarian, but the algorithimic stemmer did not perform decompounding, a technique we have found to be useful for languages such as Finnish <ref type="bibr" coords="9,494.73,550.31,14.61,10.46" target="#b14">[15]</ref>. However, <ref type="bibr" coords="9,132.26,562.26,10.51,10.46" target="#b3">[4]</ref> has found that using 4-grams as index terms works well in ad hoc ranking experiments for many European languages, including compound-word languages. Table <ref type="table" coords="9,414.49,574.21,4.98,10.46">6</ref> compares our 4-gram runs to the stemming runs which indexed all words (because we did not use stopwords with our 4gram index). As anticipated, there was a statistically significant increase in mean average precision for Hungarian, though there was a decrease for Portuguese which was also statistically significant. We look at the largest per-topic differences for Hungarian:</p><p>• HU-255 (Internetfüggők (Internet Junkies)): Average precision was 46 points higher with 4-grams for this topic (a compound word). The stemmer found the 3 relevant documents which contained 'internetfüggő' or the original query word 'internetfüggők'. 4-grams matched other variants such as 'Internetfüggőség' (Internet dependence), 'internetfüggőséggel' and 'internetfüggőségben' and found all 6 relevant documents. 4-grams also matched other potentially helpful words such as 'internet', 'internetezők', 'internetezés', 'komputerfüggőséget' and 'függővé'. But 4-grams also produced unwanted matches, such as 'intervallum' (interval) and 'Szinte' (as good as); these both came from the 4-gram 'inte'. If the stemmer had just additionally matched 'Internetfüggőség', all 6 relevants would have found, but we're still investigating if the -seg suffix is one that a Hungarian stemmer should generally remove or not.</p><p>• HU-292 (Német városok újjáépítése (Rebuilding German Cities)): On this topic, 4-grams still just found 1 of the 2 relevant documents, but it moved it from rank 3 to 1 (compared to the stemming run). While 4-grams additionally matched 'újjáépítik', the bigger advantage was probably that the 4-gram method did not match 'nem' which we know from earlier was a troublesome match for the stemming run.</p><p>• HU-283 (James Bond-filmek (James Bond Films)): On this topic, the 4-gram run scored 30 points lower in average precision than the stemming run. The 4-gram run favored documents with the 'filmek' pattern (which corresponded to three 4-grams ('film', 'ilme' and 'lmek') and so it received roughly 3 times the weight compared to the stemming run). However, the relevant documents tended not to use 'filmek'; instead they tended to use other variants matched by the stemmer such as 'film', 'filmet', 'filmnél', 'filmben' and 'filmhez'.</p><p>• HU-286 (Futballsérülések (Football Injuries)): This topic had no matches in the stemming run, but a relevant document was ranked first in the 4-gram run. 4-gram matches in the relevant documents included 'futballista', 'futballkapus' (goalkeeper), 'futballválogatott', 'vállsérülést', 'vállsérüléssel', 'vállsérülés', 'sérülés' (injury), 'sérült' and 'sérültet'. This might be a case for which decompounding would be helpful.</p><p>• HU-261 (Jövendőmondás (Fortune-telling)): The stemming run only matched the one document which contained 'jövendőmondást' and 'jövendőmondás' and it was judged nonrelevant, so it scored 0 on this topic. The 4-gram returned 1 of the 3 relevant documents at rank 2 (the others weren't ranked in the top 100). Matches in the relevant document included 'jövendölők' and 'jövendőmondók'. The latter of these perhaps could have been matched with additional stemming rules, but the former would require a stemmer to do decompounding (or, if the user had decompounded the query, the latter would require index-time decompounding to match).</p><p>SearchServer can find character sequences inside European words without n-gramming if the user specifies wildcards, so for precise searches it's unclear if n-gram indexes would add value. N-gram approaches typically produce larger indexes and its queries can be slower for common word-searching cases. We're not aware of them being used in practice for European language retrieval, except perhaps by web search engines for url indexing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Comparison to Alternate Stemmers</head><p>Table <ref type="table" coords="10,117.17,557.31,4.98,10.46">7</ref> compares alternate stemming approaches to the approach we used in our submitted runs. Unfortunately, we have run out of time to examine more topics in detail for this draft paper, but we note in particular that it seems not to matter very much on average whether the remove_possessive function of the Hungarian stemmer is called or not.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,90.00,117.46,353.58,202.87"><head>Table 1 :</head><label>1</label><figDesc>Sizes of CLEF 2005 Ad-Hoc Track Test Collections</figDesc><table coords="2,90.00,130.20,353.58,190.13"><row><cell>Language</cell><cell>Text Size (uncompressed)</cell><cell>Documents</cell><cell>Topics</cell><cell>Rel/Topic</cell></row><row><cell cols="2">2 Methodology</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,102.48,253.40,395.06,351.33"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="4,102.48,253.40,395.06,351.33"><row><cell></cell><cell></cell><cell cols="3">: Mean Scores of Diagnostic Title-only runs</cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell>FRS</cell><cell>Success@1</cell><cell>Success@5</cell><cell>Success@10</cell><cell>MRR</cell><cell>MAP</cell></row><row><cell>BG-neuall</cell><cell>0.782</cell><cell>15/49 (31%)</cell><cell>38/49 (78%)</cell><cell>41/49 (84%)</cell><cell>0.500</cell><cell>0.255</cell></row><row><cell>BG-neunos</cell><cell>0.781</cell><cell>16/49 (33%)</cell><cell>38/49 (78%)</cell><cell>41/49 (84%)</cell><cell>0.507</cell><cell>0.263</cell></row><row><cell>BG-4gram</cell><cell>0.758</cell><cell>20/49 (41%)</cell><cell>32/49 (65%)</cell><cell>40/49 (82%)</cell><cell>0.525</cell><cell>0.264</cell></row><row><cell>BG-snru</cell><cell>0.757</cell><cell>17/49 (35%)</cell><cell>34/49 (69%)</cell><cell>40/49 (82%)</cell><cell>0.499</cell><cell>0.242</cell></row><row><cell>BG-neu</cell><cell>0.749</cell><cell>15/49 (31%)</cell><cell>35/49 (71%)</cell><cell>39/49 (80%)</cell><cell>0.476</cell><cell>0.259</cell></row><row><cell>BG-none</cell><cell>0.685</cell><cell>14/49 (29%)</cell><cell>30/49 (61%)</cell><cell>35/49 (71%)</cell><cell>0.440</cell><cell>0.195</cell></row><row><cell>FR-sn</cell><cell>0.820</cell><cell>27/50 (54%)</cell><cell>40/50 (80%)</cell><cell>43/50 (86%)</cell><cell>0.645</cell><cell>0.318</cell></row><row><cell>FR-lex</cell><cell>0.810</cell><cell>25/50 (50%)</cell><cell>39/50 (78%)</cell><cell>42/50 (84%)</cell><cell>0.618</cell><cell>0.302</cell></row><row><cell>FR-lexnos</cell><cell>0.810</cell><cell>25/50 (50%)</cell><cell>39/50 (78%)</cell><cell>42/50 (84%)</cell><cell>0.618</cell><cell>0.302</cell></row><row><cell>FR-lexall</cell><cell>0.810</cell><cell>25/50 (50%)</cell><cell>39/50 (78%)</cell><cell>43/50 (86%)</cell><cell>0.618</cell><cell>0.301</cell></row><row><cell>FR-4gram</cell><cell>0.809</cell><cell>24/50 (48%)</cell><cell>41/50 (82%)</cell><cell>43/50 (86%)</cell><cell>0.617</cell><cell>0.279</cell></row><row><cell>FR-lexsing</cell><cell>0.802</cell><cell>25/50 (50%)</cell><cell>39/50 (78%)</cell><cell>42/50 (84%)</cell><cell>0.615</cell><cell>0.299</cell></row><row><cell>FR-none</cell><cell>0.778</cell><cell>20/50 (40%)</cell><cell>38/50 (76%)</cell><cell>43/50 (86%)</cell><cell>0.549</cell><cell>0.232</cell></row><row><cell>HU-4gram</cell><cell>0.834</cell><cell>24/50 (48%)</cell><cell>39/50 (78%)</cell><cell>45/50 (90%)</cell><cell>0.619</cell><cell>0.341</cell></row><row><cell>HU-neunos</cell><cell>0.789</cell><cell>26/50 (52%)</cell><cell>36/50 (72%)</cell><cell>42/50 (84%)</cell><cell>0.625</cell><cell>0.287</cell></row><row><cell>HU-neuall</cell><cell>0.788</cell><cell>25/50 (50%)</cell><cell>37/50 (74%)</cell><cell>41/50 (82%)</cell><cell>0.614</cell><cell>0.280</cell></row><row><cell>HU-neu</cell><cell>0.788</cell><cell>25/50 (50%)</cell><cell>37/50 (74%)</cell><cell>42/50 (84%)</cell><cell>0.613</cell><cell>0.274</cell></row><row><cell>HU-neuposs</cell><cell>0.769</cell><cell>24/50 (48%)</cell><cell>36/50 (72%)</cell><cell>41/50 (82%)</cell><cell>0.588</cell><cell>0.271</cell></row><row><cell>HU-none</cell><cell>0.671</cell><cell>17/50 (34%)</cell><cell>30/50 (60%)</cell><cell>37/50 (74%)</cell><cell>0.464</cell><cell>0.184</cell></row><row><cell>PT-sn</cell><cell>0.892</cell><cell>30/50 (60%)</cell><cell>43/50 (86%)</cell><cell>47/50 (94%)</cell><cell>0.712</cell><cell>0.269</cell></row><row><cell>PT-lexall</cell><cell>0.865</cell><cell>30/50 (60%)</cell><cell>42/50 (84%)</cell><cell>46/50 (92%)</cell><cell>0.707</cell><cell>0.300</cell></row><row><cell>PT-lex</cell><cell>0.856</cell><cell>31/50 (62%)</cell><cell>42/50 (84%)</cell><cell>45/50 (90%)</cell><cell>0.714</cell><cell>0.300</cell></row><row><cell>PT-lexnos</cell><cell>0.856</cell><cell>31/50 (62%)</cell><cell>42/50 (84%)</cell><cell>45/50 (90%)</cell><cell>0.714</cell><cell>0.300</cell></row><row><cell>PT-lexsing</cell><cell>0.843</cell><cell>30/50 (60%)</cell><cell>40/50 (80%)</cell><cell>44/50 (88%)</cell><cell>0.699</cell><cell>0.290</cell></row><row><cell>PT-none</cell><cell>0.821</cell><cell>28/50 (56%)</cell><cell>39/50 (78%)</cell><cell>43/50 (86%)</cell><cell>0.662</cell><cell>0.246</cell></row><row><cell>PT-4gram</cell><cell>0.815</cell><cell>27/50 (54%)</cell><cell>41/50 (82%)</cell><cell>41/50 (82%)</cell><cell>0.662</cell><cell>0.231</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,99.06,117.46,404.88,142.51"><head>Table 3 :</head><label>3</label><figDesc>Impact of Stemming on Average Precision and First Relevant Score</figDesc><table coords="7,99.06,132.14,404.88,127.82"><row><cell>Expt</cell><cell>∆MAP</cell><cell>95% Conf</cell><cell>vs.</cell><cell>3 Extreme Diffs (Topic)</cell></row><row><cell>HU-neu-none</cell><cell>0.090</cell><cell>( 0.038, 0.143)</cell><cell>32-11-7</cell><cell>0.87 (279), 0.77 (294), -0.12 (265)</cell></row><row><cell>FR-lex-none</cell><cell>0.070</cell><cell>( 0.028, 0.112)</cell><cell>29-16-5</cell><cell>0.53 (297), 0.45 (284), -0.12 (275)</cell></row><row><cell>BG-neu-none</cell><cell>0.064</cell><cell>( 0.005, 0.123)</cell><cell>29-15-5</cell><cell>0.90 (271), 0.58 (279), -0.50 (258)</cell></row><row><cell>PT-lex-none</cell><cell>0.054</cell><cell>( 0.027, 0.080)</cell><cell>34-13-3</cell><cell>0.35 (279), 0.30 (286), -0.09 (296)</cell></row><row><cell></cell><cell>∆FRS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HU-neu-none</cell><cell>0.117</cell><cell>( 0.024, 0.209)</cell><cell>19-10-21</cell><cell>1.00 (271), 0.98 (294), -0.83 (262)</cell></row><row><cell>BG-neu-none</cell><cell>0.064</cell><cell>(-0.042, 0.170)</cell><cell>16-17-16</cell><cell>0.96 (294), 0.86 (269), -0.87 (273)</cell></row><row><cell>PT-lex-none</cell><cell>0.035</cell><cell>(-0.017, 0.087)</cell><cell>12-7-31</cell><cell>0.69 (263), 0.60 (254), -0.54 (282)</cell></row><row><cell>FR-lex-none</cell><cell>0.033</cell><cell>(-0.032, 0.097)</cell><cell>15-8-27</cell><cell>0.73 (276), 0.64 (284), -0.60 (279)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,107.05,292.57,388.90,119.16"><head>Table 4 :</head><label>4</label><figDesc>Impact of /nostop Option on Average Precision and First Relevant Score</figDesc><table coords="7,107.05,307.81,388.90,103.92"><row><cell>Expt</cell><cell>∆MAP</cell><cell>95% Conf</cell><cell>vs.</cell><cell>3 Extreme Diffs (Topic)</cell></row><row><cell>HU-nos-neu</cell><cell>0.013</cell><cell>(-0.005, 0.031)</cell><cell>3-1-46</cell><cell>0.40 (292), 0.13 (265), -0.03 (282)</cell></row><row><cell>BG-nos-neu</cell><cell>0.005</cell><cell>(-0.003, 0.012)</cell><cell>2-2-45</cell><cell>0.17 (273), 0.06 (267), -0.01 (257)</cell></row><row><cell>FR-nos-lex</cell><cell>0.000</cell><cell>n/a</cell><cell>0-0-50</cell><cell>0.00 (276), 0.00 (252), 0.00 (300)</cell></row><row><cell>PT-nos-lex</cell><cell>0.000</cell><cell>n/a</cell><cell>0-0-50</cell><cell>0.00 (276), 0.00 (252), 0.00 (300)</cell></row><row><cell></cell><cell>∆FRS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BG-nos-neu</cell><cell>0.031</cell><cell>(-0.010, 0.072)</cell><cell>3-1-45</cell><cell>0.80 (273), 0.57 (267), -0.05 (257)</cell></row><row><cell>HU-nos-neu</cell><cell>0.001</cell><cell>(-0.014, 0.015)</cell><cell>1-1-48</cell><cell>0.26 (292), 0.00 (253), -0.23 (282)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,96.06,117.46,410.88,317.34"><head>Table 5 :</head><label>5</label><figDesc>Impact of Indexing All Words on Average Precision and First Relevant Score</figDesc><table coords="9,96.06,132.14,410.88,302.65"><row><cell>Expt</cell><cell>∆MAP</cell><cell>95% Conf</cell><cell>vs.</cell><cell>3 Extreme Diffs (Topic)</cell></row><row><cell>PT-all-nos</cell><cell>-0.000</cell><cell>(-0.003, 0.002)</cell><cell>18-17-15</cell><cell>0.03 (280), -0.01 (259), -0.02 (282)</cell></row><row><cell>FR-all-nos</cell><cell>-0.001</cell><cell>(-0.005, 0.003)</cell><cell>24-17-9</cell><cell>-0.07 (262), 0.01 (290), 0.01 (289)</cell></row><row><cell>HU-all-nos</cell><cell>-0.006</cell><cell>(-0.021, 0.008)</cell><cell>7-7-36</cell><cell>-0.33 (292), -0.05 (265), 0.05 (274)</cell></row><row><cell>BG-all-nos</cell><cell>-0.008</cell><cell>(-0.034, 0.018)</cell><cell>16-17-16</cell><cell>-0.55 (271), -0.14 (268), 0.20 (295)</cell></row><row><cell></cell><cell>∆FRS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PT-all-nos</cell><cell>0.009</cell><cell>(-0.007, 0.025)</cell><cell>5-1-44</cell><cell>0.38 (282), 0.06 (263), -0.07 (291)</cell></row><row><cell>BG-all-nos</cell><cell>0.001</cell><cell>(-0.008, 0.010)</cell><cell>3-4-42</cell><cell>0.13 (263), -0.07 (268), -0.07 (271)</cell></row><row><cell>FR-all-nos</cell><cell>-0.000</cell><cell>(-0.009, 0.008)</cell><cell>4-4-42</cell><cell>0.10 (286), -0.09 (258), -0.09 (288)</cell></row><row><cell>HU-all-nos</cell><cell>-0.000</cell><cell>(-0.010, 0.009)</cell><cell>1-3-46</cell><cell>0.16 (282), -0.04 (299), -0.14 (292)</cell></row><row><cell cols="5">Table 6: 4-grams vs. Stems in Average Precision and First Relevant Score</cell></row><row><cell>Expt</cell><cell>∆MAP</cell><cell>95% Conf</cell><cell>vs.</cell><cell>3 Extreme Diffs (Topic)</cell></row><row><cell>HU-4gr-all</cell><cell>0.060</cell><cell>( 0.018, 0.103)</cell><cell>32-17-1</cell><cell>0.46 (255), 0.33 (292), -0.30 (283)</cell></row><row><cell>BG-4gr-all</cell><cell>0.009</cell><cell>(-0.028, 0.046)</cell><cell>25-24-0</cell><cell>0.50 (258), 0.25 (254), -0.33 (285)</cell></row><row><cell>FR-4gr-all</cell><cell>-0.021</cell><cell>(-0.048, 0.005)</cell><cell>18-31-1</cell><cell>0.25 (291), 0.22 (263), -0.20 (273)</cell></row><row><cell>PT-4gr-all</cell><cell>-0.068</cell><cell>(-0.104,-0.032)</cell><cell>14-35-1</cell><cell>-0.43 (259), -0.28 (286), 0.22 (297)</cell></row><row><cell></cell><cell>∆FRS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HU-4gr-all</cell><cell>0.046</cell><cell>(-0.036, 0.128)</cell><cell>15-15-20</cell><cell>1.00 (286), 0.93 (261), -0.81 (251)</cell></row><row><cell>FR-4gr-all</cell><cell>-0.001</cell><cell>(-0.041, 0.039)</cell><cell>13-15-22</cell><cell>0.60 (279), 0.26 (281), -0.40 (259)</cell></row><row><cell>BG-4gr-all</cell><cell>-0.024</cell><cell>(-0.093, 0.045)</cell><cell>17-14-18</cell><cell>-0.82 (274), 0.56 (270), 0.59 (288)</cell></row><row><cell>PT-4gr-all</cell><cell>-0.051</cell><cell>(-0.134, 0.032)</cell><cell>7-17-26</cell><cell>-1.00 (259), -0.83 (292), 0.96 (260)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,725.66,407.81,9.49;1,90.00,736.24,309.41,8.37"><p>SearchServer TM , SearchSQL TM and Intuitive Searching TM are trademarks of Hummingbird Ltd. All other copyrights, trademarks and tradenames are the property of their respective owners.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Impact of /single Option</head><p>Table <ref type="table" coords="10,118.55,639.44,4.98,10.46">8</ref> isolates the impact of using the SearchServer /single option. This option only makes a difference for the SearchServer lexical stemmers which can produce more than one stem for a term. Like last year <ref type="bibr" coords="10,182.49,663.36,14.61,10.46" target="#b14">[15]</ref>, our method for including all stems without overweighting some of the terms apparently was effective. Even in the high-variance first relevant score measure, the bigger differences favored including all stems.   <ref type="bibr" coords="12,200.60,390.53,15.49,10.46" target="#b13">[14]</ref> for more details). From the Description fields for Bulgarian, French and Portuguese, instruction words such as "find", "relevant" and "document" were automatically removed (based on looking at some older topic lists, not this year's topics; this step was skipped for Hungarian because we lacked an older topic list).</p><p>The submitted French and Portuguese Title-only runs (i.e. "humFR05t" and "humPT05t" of Table <ref type="table" coords="12,116.77,450.31,4.43,10.46">9</ref>) correspond to the "lex" diagnostic runs (i.e. "FR-lex" and "PT-lex" of Table <ref type="table" coords="12,452.53,450.31,4.43,10.46">2</ref>) except that the submitted runs used an older experimental version of SearchServer (though there don't appear to have been any differences that affected the runs). The submitted Bulgarian and Hungarian Title-only runs (i.e. "humBG05t" and "humHU05t"of Table <ref type="table" coords="12,350.22,486.17,4.43,10.46">9</ref>) correspond to the "neu" diagnostic runs (i.e. "BG-neu" and "HU-neu" of Table <ref type="table" coords="12,279.25,498.12,3.87,10.46">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Impact of Adding the Description Field</head><p>Table <ref type="table" coords="12,118.42,544.40,9.96,10.46">10</ref> isolates the impact of adding the Description field to the query. Though adding the Description tended to increase the scores on average (and in some cases this result was statistically significant), one should keep in mind that the Description often repeated the Title words, which hence received twice the weight in the combined query. We would expect to see more variance if the Title was replaced by the Description instead of being augmented by it</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Impact of Blind Feedback</head><p>Table <ref type="table" coords="12,117.57,638.49,9.96,10.46">11</ref> isolates the impact of the blind feedback technique (based on using the first 2 returned rows to expand the query). While mean average precision increased for all 4 languages (and the increase was statistically significant for 3 of them), the first relevant score decreased for all 4 languages (and the decrease was statistically significant for the other 1 of them).</p><p>The blind feedback technique presumably works best if relevant documents appear in the first 2 rows, in which case first relevant score cannot be improved. If the first 2 rows do not contain relevant documents, then using those rows to expand the query may hurt the query and push down the first relevant even further.</p><p>This result may explain in part why blind feedback techniques are not known to be used in practice even though they have been popular with experimenters for several years in ad hoc evaluations (which typically focus on mean average precision).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,105.49,534.25,335.20,10.46" xml:id="b0">
	<monogr>
		<ptr target="http://babelfish.altavista.com/tr" />
		<title level="m" coord="13,105.49,534.25,184.77,10.46">AltaVista&apos;s Babel Fish Translation Service</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.49,553.99,332.13,10.46" xml:id="b1">
	<monogr>
		<ptr target="http://www.clef-campaign.org/" />
		<title level="m" coord="13,105.49,553.99,188.54,10.46">Cross-Language Evaluation Forum web site</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.49,573.73,407.50,10.46;13,100.52,585.69,114.70,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,186.64,573.73,218.56,10.46">Converting the Fulcrum Search Engine to Unicode</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Hodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,413.30,573.73,99.70,10.46;13,100.52,585.69,84.08,10.46">Sixteenth International Unicode Conference</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.49,605.42,407.51,10.46;13,100.52,617.38,281.32,10.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,269.51,605.42,243.50,10.46;13,100.52,617.38,48.01,10.46">JHU/APL Experiments in Tokenization and Non-Word Translation</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,156.21,617.38,194.60,10.46">Working Notes for the CLEF 2003 Workshop</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.49,637.12,22.98,10.46;13,148.18,637.12,39.58,10.46;13,207.50,637.12,84.05,10.46;13,315.41,637.12,81.29,10.46;13,416.44,637.12,28.77,10.46;13,464.96,637.12,48.04,10.46;13,100.52,649.08,171.12,10.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,207.50,637.12,79.38,10.46">English-Hungarian</title>
		<ptr target="http://dict.sztaki.hu/english-hungarian" />
	</analytic>
	<monogr>
		<title level="m" coord="13,105.49,637.12,22.98,10.46;13,148.18,637.12,33.92,10.46;13,315.41,637.12,81.29,10.46;13,416.44,637.12,28.77,10.46;13,464.96,637.12,43.68,10.46">Hungarian-English Online Dictionary</title>
		<imprint/>
	</monogr>
	<note>MTA SZTAKI</note>
</biblStruct>

<biblStruct coords="13,105.49,668.82,32.78,10.46;13,156.14,668.82,58.66,10.46;13,232.65,668.82,18.60,10.46;13,269.11,668.82,43.71,10.46;13,330.70,668.82,11.93,10.46;13,360.50,668.82,10.93,10.46;13,389.29,668.82,39.12,10.46;13,446.28,668.82,25.18,10.46;13,489.33,668.82,23.67,10.46;13,100.52,680.77,213.71,10.46" xml:id="b5">
	<monogr>
		<ptr target="http://research.nii.ac.jp/∼ntcadm/index-en.html" />
		<title level="m" coord="13,105.49,668.82,32.78,10.46;13,156.14,668.82,58.66,10.46;13,232.65,668.82,18.60,10.46;13,269.11,668.82,43.71,10.46;13,330.70,668.82,11.93,10.46;13,360.50,668.82,10.93,10.46;13,389.29,668.82,39.12,10.46;13,446.28,668.82,25.18,10.46;13,489.33,668.82,18.94,10.46">NTCIR (NII-NACSIS Test Collection for IR Systems) Home Page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.49,700.51,116.65,10.46;13,241.61,700.51,271.39,10.46;13,100.52,712.46,231.03,10.46" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="13,180.36,700.51,41.78,10.46;13,241.61,700.51,187.41,10.46">Snowball: A language for stemming algorithms</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://snowball.tartarus.org/texts/introduction.html" />
		<imprint>
			<date type="published" when="2001-10">October 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.49,732.21,407.51,10.46;13,100.52,744.16,173.50,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,473.43,732.21,39.57,10.46;13,100.52,744.16,33.91,10.46">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,143.40,744.16,98.94,10.46">Proceedings of TREC-3</title>
		<meeting>TREC-3</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,105.49,110.53,407.51,10.46;14,100.52,122.49,137.79,10.46" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
		<ptr target="http://www.unine.ch/info/clef/" />
		<title level="m" coord="14,192.77,110.53,315.69,10.46">CLEF and Multilingual information retrieval resource page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.47,142.41,402.54,10.46;14,100.52,154.37,270.20,10.46" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,372.06,142.41,100.85,10.46">Overview of WebCLEF</title>
		<author>
			<persName coords=""><forename type="first">Jaap</forename><surname>Börkur Sigurbjörnsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,145.09,154.37,194.60,10.46">Working Notes for the CLEF 2005 Workshop</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="14,110.47,174.29,303.79,10.46" xml:id="b10">
	<monogr>
		<ptr target="http://trec.nist.gov/" />
		<title level="m" coord="14,110.47,174.29,206.29,10.46">Text REtrieval Conference (TREC) Home Page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.47,194.22,402.52,10.46;14,100.52,206.18,55.70,10.46;14,156.21,205.10,12.85,7.32;14,175.41,206.18,337.60,10.46;14,100.52,218.13,48.11,10.46" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,217.40,194.22,295.59,10.46;14,100.52,206.18,55.70,10.46;14,156.21,205.10,12.85,7.32;14,175.41,206.18,41.39,10.46">European Web Retrieval Experiments with Hummingbird SearchServer TM at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,317.99,206.18,195.02,10.46;14,100.52,218.13,17.69,10.46">Working Notes for the CLEF 2005 Workshop</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note>To appear in</note>
</biblStruct>

<biblStruct coords="14,110.47,238.06,402.52,10.46;14,100.52,250.01,55.70,10.46;14,156.21,248.95,12.85,7.32;14,172.88,250.01,212.44,10.46" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,214.31,238.06,298.68,10.46;14,100.52,250.01,55.70,10.46;14,156.21,248.95,12.85,7.32;14,172.88,250.01,60.34,10.46">Experiments in 8 European Languages with Hummingbird SearchServer TM at CLEF 2002</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,241.08,250.01,113.62,10.46">Proceedings of CLEF 2002</title>
		<meeting>CLEF 2002</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.47,269.94,402.52,10.46;14,100.52,281.89,143.06,10.46;14,243.57,280.82,12.85,7.32;14,260.60,281.89,252.41,10.46;14,100.52,293.85,48.11,10.46" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,198.87,269.94,314.12,10.46;14,100.52,281.89,143.06,10.46;14,243.57,280.82,12.85,7.32;14,260.60,281.89,39.24,10.46">Lexical and Algorithmic Stemming Compared for 9 European Languages with Hummingbird SearchServer TM at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,329.90,281.89,183.11,10.46;14,100.52,293.85,17.69,10.46">Working Notes for the CLEF 2003 Workshop</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.47,313.77,402.52,10.46;14,100.52,325.73,55.70,10.46;14,156.21,324.66,12.85,7.32;14,172.88,325.73,293.83,10.46" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,209.72,313.77,303.27,10.46;14,100.52,325.73,55.70,10.46;14,156.21,324.66,12.85,7.32;14,172.88,325.73,38.87,10.46">Finnish, Portuguese and Russian Retrieval with Hummingbird SearchServer TM at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,241.08,325.73,194.60,10.46">Working Notes for the CLEF 2004 Workshop</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
