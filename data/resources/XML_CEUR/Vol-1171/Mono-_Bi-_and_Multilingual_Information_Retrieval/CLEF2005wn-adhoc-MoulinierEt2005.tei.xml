<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,99.58,148.86,403.85,15.15;1,253.65,170.78,95.72,15.15">Thomson Legal and Regulatory Experiments at CLEF-2005</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,221.45,204.67,77.85,8.74"><forename type="first">Isabelle</forename><surname>Moulinier</surname></persName>
							<email>isabelle.moulinier@thomson.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Thomson Legal and Regulatory</orgName>
								<address>
									<addrLine>610 Opperman Drive</addrLine>
									<postCode>55123</postCode>
									<settlement>Eagan</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.99,204.67,59.55,8.74"><forename type="first">Ken</forename><surname>Williams</surname></persName>
							<email>ken.williams@thomson.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Thomson Legal and Regulatory</orgName>
								<address>
									<addrLine>610 Opperman Drive</addrLine>
									<postCode>55123</postCode>
									<settlement>Eagan</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,99.58,148.86,403.85,15.15;1,253.65,170.78,95.72,15.15">Thomson Legal and Regulatory Experiments at CLEF-2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7930A269CB1328F5EF988F4B8168F018</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Content Analysis and Indexing]: Indexing methods</term>
					<term>Linguistic processing</term>
					<term>H.3.3 [Information Search and Retrieval]: Query formulation</term>
					<term>relevance feedback</term>
					<term>H.3.4 [Systems and Software]: Performance evaluation</term>
					<term>J.5 [Arts and Humanities]: Language translation Experimentation, Performance, Languages, Algorithms Natural Language Processing, Bilingual Information Retrieval, Hungarian Language, French Language, Portuguese Language, Machine Translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the 2005 Cross-Language Evaluation Forum, Thomson Legal and Regulatory participated in the Hungarian, French, and Portuguese monolingual search tasks as well as French-to-Portuguese bilingual retrieval. Our Hungarian participation focused on comparing the effectiveness of different approaches toward morphological stemming. Our French and Portuguese monolingual efforts focused on different approaches to Pseudo-Relevance Feedback (PRF), in particular the evaluation of a scheme for selectively applying PRF only in the cases most likely to produce positive results. Our French-to-Portuguese bilingual effort applies our previous work in query translation to a new pair of languages. All experiments were performed using our proprietary search engine. We remain encouraged by the overall success of our efforts, with our main submissions for each of the four tasks performing above the overall CLEF median. However, none of the specific enhancement techniques we attempted in this year's forum showed significant improvements over our initial results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Thomson Legal and Regulatory participated in the Hungarian, French, and Portuguese monolingual search tasks as well as French-to-Portuguese bilingual retrieval.</p><p>Our Hungarian participation further evaluates the configuration developed in prior participations for compounding languages such as German or Finnish. We rely on morphological stemming to normalize derivations and factor compound terms. As morphological stemming may generate multiple stems for a given term, we compare the effectiveness of selecting a single stem with selecting all stems.</p><p>In our CLEF 2004 participation, we applied pseudo-relevance feedback blindly to all queries, even though this approach can be detrimental to some query results. In this participation, we take a first step toward selectively applying pseudo-relevance feedback. We apply our simple approach to our French and Portuguese runs.</p><p>Finally, our bilingual runs extend our previous work to two more languages. Our approach relies on query translation, where queries are translated term by term using translation resources built from parallel corpora.</p><p>We describe our experimental framework in Section 2, and present our monolingual and bilingual runs in Sections 3 and 4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental framework</head><p>The cornerstone of our experimental framework is our proprietary search engine which supports Boolean and Natural language search. Natural language search is based on an inference network retrieval model similar to INQUERY <ref type="bibr" coords="2,252.98,286.34,10.51,8.74" target="#b0">[1]</ref> and has been shown effective when compared to Boolean search on legal content <ref type="bibr" coords="2,196.38,298.29,9.97,8.74" target="#b4">[6]</ref>. For our CLEF experiments, we extended the search experience by incorporating the pseudo-relevance feedback functionality described in Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexing</head><p>Our indexing unit for European languages is a word. We identify words in sequences of characters using localized tokenization rules (for example, apostrophes are handled differently for French or Italian).</p><p>Each word is normalized for morphological variations. This includes identifying compounds if needed. We use the Inxight morphological stemmer [3] to perform such normalization which, in addition, can be configured to handle missing case and diacritic information.</p><p>Morphological stemming can produce multiple stems for a given term. We have introduced the option of selecting a single stem or keeping all stems. If candidate stems include compound terms, we select the stem with the fewest compound parts. If candidate stems are simple terms, we select the first one.</p><p>We do not remove stopwords from indices, as indexing supports both full-text search and natural language search. Stopwords are handled during search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Search</head><p>Once documents are indexed, they can be searched. Given a query, we apply two steps: query formulation and document scoring.</p><p>Query formulation identifies "concepts" from natural language text by removing stopwords and other noise phrases, and imposes a Bayesian belief structure on these concepts. In many cases, each term in the natural language text represents a concept, and a flat structure gives the same weight to all concepts. However, phrases, compounds or misspellings can introduce more complex concepts, using operators such as "natural phrase," "compound," or "synonym." The structure is then used to score documents.</p><p>Scoring takes evidence from each document as a whole, as well as from the best dynamic portion of each document. The best portion is computed dynamically based on proximal concept occurrences. Each concept contributes a belief to the document (and portion) score. We use a standard tf-idf scheme for computing term beliefs in all our runs. The belief of a single concept is given by: bel term (Q) = 0.4 + 0.6</p><formula xml:id="formula_0" coords="2,314.38,689.73,72.67,9.65">• tf norm • idf norm</formula><p>where</p><formula xml:id="formula_1" coords="2,150.77,716.13,300.27,23.23">tf norm = log(tf + 0.5) log(tf max + 1.0) and idf norm = log(C + 0.5) -log(df ) log(C + 1.0)</formula><p>tf is the number of occurrences of the term within the document, tf max is the maximum number of occurrences of any term within the document, df is the number of documents containing the term and C the total number of documents in the collection. The various constants in the formulae have been determined by prior testing on manually-labeled data. tf max is a weak indicator of document length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pseudo-relevance feedback</head><p>We have incorporated a pseudo-relevance feedback module into our search system. We follow the approach outlined by Haines and Croft <ref type="bibr" coords="3,263.63,217.54,9.97,8.74" target="#b1">[2]</ref>.</p><p>We select terms for query expansion using a Rocchio-like formula and add the selected terms to the query. The added terms are weighted either using a fixed weight or a frequency-based weight.</p><formula xml:id="formula_2" coords="3,136.07,258.15,372.69,28.50">sw = α • qf • idf norm + β |R| d∈R (tf norm • idf norm ) - γ |R| d∈R (tf norm • idf norm ) (<label>1</label></formula><formula xml:id="formula_3" coords="3,508.76,264.89,4.24,8.74">)</formula><p>where qf is the query weight, R is the set of documents considered relevant, R the set of documents considered not relevant, and |X| denotes the cardinality of set X. The α, β and γ weights are set experimentally. The sets of documents R and R are extracted from the document list returned by the original search: R correspond to the top n documents and R to the bottom m, where n and m are determined through experiments on training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Monolingual experiments</head><p>Our monolingual participation focuses on normalization for Hungarian and pseudo-relevance feedback for French and Portuguese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hungarian experiments</head><p>As mentioned in Section 2.1, we use a morphological stemmer to identify compounds and normalize terms. The stemmer can be configured to allow for missing case and diacritic information. In addition, we can select to use one stem, or all stems. At search time, compounds are treated as "natural phrases," i.e. as words within a proximity of 3. In addition, multiple stems are grouped under a single operator so that terms with multiple stems do not contribute more weight than terms with one single stem. Finally, we used the Hungarian stopword list developed by the Université de Neuchâtel <ref type="bibr" coords="3,382.76,528.13,9.97,8.74">[7]</ref>.</p><p>We submitted two runs, each with its own indexing scheme:</p><p>• Run tlrTDhuE keeps all stems and allows for missing case and diacritic information.</p><p>• Run tlrTDhuSC keeps a single stem per term and does not correct missing information. As illustrated by Table <ref type="table" coords="3,206.02,697.82,3.88,8.74" target="#tab_0">1</ref>, there is no overall significant difference between the two runs, still we observe marked differences on a per-query basis: tlrTDhuSC outperforms tlrTDhuE on 25 queries and underperforms on 20 queries (differences range from a few percent to over 50%). This, we believe, is due to two factors: concepts in queries differ depending on the stemming approach; so do terms in the indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pseudo-relevance feedback experiments</head><p>Pseudo-relevance feedback (PRF) is known to be useful on average but can be detrimental to the performance of individual queries. This year, we took a first step towards predicting whether or not PRF would aid individual queries.</p><p>We followed the following methodology: we selected our parameters for PRF using training data from previous CLEF participations for both French and Portuguese. We then manually derived a simple prediction rule that identifies those queries where PRF was very detrimental. Our decision rule is composed of two components: the score of the top ranked document and the maximum score any document can achieve for a given query, computed by setting the tf norm factor in belief scores to 1. Our prediction rule is of the form: if maxscore &gt;= Min_MS_Value and (maxscore &lt; MS_Threshold or bestscore &gt;= Min_TD_Value) Apply PRF else Don't apply PRF Using training data, we searched for the best parameters in this three-dimensional space (Min MS Value, MS Threshold, and Min TD Value).</p><p>Our French and Portuguese results, reported in Table <ref type="table" coords="4,341.46,337.63,3.88,8.74" target="#tab_1">2</ref>, show that PRF applied to all queries improved performance (although the difference is not always statistically significant) but that PRF applied to selected queries did not provide additional improvement.</p><p>It is interesting to note that PRF, selective or not, degrades the Reciprocal Rank measure, i.e. the average rank of the first relevant document. This indicates that our PRF setting decreases precision in the top-ranked documents, although it increases recall overall. A comparative summary is provided in Table <ref type="table" coords="4,180.03,409.36,3.88,8.74" target="#tab_2">3</ref> Runs ending in 2 are the PRF runs using the following configuration: add 5 terms from the top 10 documents; terms are selected with α = β = 1 and γ = 0; expansion uses a fixed weight of 0.5 for each added term. Runs ending in 1 are PRF runs using the prediction rule. † indicates a statistically significant difference using the Wilcoxon signed-rank test and a p-value of 0.05.</p><p>Although it performed reasonably well on our initial training data, our PRF selection rule often applied PRF when it was detrimental, and failed to apply it when it would have helped. Table <ref type="table" coords="4,508.02,708.85,4.98,8.74" target="#tab_3">4</ref> gives more details on the prediction effectiveness or lack thereof. The number of queries for which PRF degraded performance is not unexpected as we did not intend to cover all cases with our heuristic. What is surprising is the low number of cases where our prediction rule prevented PRF from helping performance. We believe that the parameters we selected over-fitted the training data. Retrospectively, this is not all that surprising as we use raw values rather than proportions or normalized values. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">French to Portuguese bilingual experiments</head><p>Our 2005 bilingual experiments follow the approach we established during our CLEF 2004 participation. We performed bilingual search by translating query terms. Translation resources were trained from parallel corpora using the GIZA++ statistical machine translation package <ref type="bibr" coords="5,478.73,515.26,9.96,8.74" target="#b3">[5]</ref>.</p><p>We created a bilingual lexicon by training the IBM Model 3 on the Europarl parallel corpus <ref type="bibr" coords="5,90.00,539.17,10.52,8.74" target="#b2">[4]</ref> as we found Model 3 to provide better translations than Model 1. We selected at most three translations per term, excluding translations with probabilities smaller than 0.1. During query formulation, translations were grouped under a *SUM 1 operator so that concepts are given the same importance regardless of the number of translations. In addition, translations were weighted by their probabilities.</p><p>Table <ref type="table" coords="5,131.92,598.94,4.98,8.74" target="#tab_4">5</ref> summarizes our bilingual runs. We submitted runs with and without pseudo-relevance feedback. The PRF runs show a behavior similar to our monolingual runs as reciprocal rank degrades but recall improves. Five times out of 6, the prediction rule predicted correctly that PRF should not be applied. However the number of cases when PRF was applied and performance dropped was also high (around 20).</p><p>The bilingual runs achieved between 60 and 65% of the average precision of monolingual runs. This performance is comparable to our results with German to French search, but not as promising as our training runs, which reached 80%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We remain encouraged by the overall success of our efforts, with our main submissions for each of the four tasks performing above the overall CLEF median. However, none of the specific enhancement techniques we attempted in this year's forum showed significant improvements over our initial results. For monolingual retrieval in Hungarian, a highly morphological language, we explored two techniques for morphological stemming in order to identify compound terms and normalize them, but were unable to find significant differences between the results.</p><p>For monolingual retrieval in French and Portuguese, where we have previously shown pseudorelevance feedback (PRF) to increase overall performance, we attempted to find a heuristic to identify specific queries for which PRF would be helpful. So far we have been unable to achieve this to a significant degree. In the future, we intend to explore additional techniques such as the use of machine learning including feature engineering as in <ref type="bibr" coords="6,350.05,449.55,10.51,8.74" target="#b5">[8]</ref> and methods for using normalized values rather than raw values to prevent over-fitting.</p><p>For bilingual retrieval from French to Portuguese, we achieve good performance relative to other submissions, but perhaps like other forum participants, we remain disappointed in the bilingual performance relative to the same queries performed in a monolingual setting. We need to better understand the differences between our official and training runs. In addition, we plan on investigating the usefulness of translation disambiguation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,603.09,423.00,80.81"><head>Table 1 :</head><label>1</label><figDesc>Performance for our official Hungarian runs comparing the effectiveness of the two stemming choices.</figDesc><table coords="3,141.77,603.09,319.47,46.99"><row><cell></cell><cell>MAP</cell><cell cols="2">R-Prec Reciprocal Rank</cell></row><row><cell></cell><cell>(Above/Equal/Below Median)</cell><cell></cell><cell></cell></row><row><cell>tlrTDhuE</cell><cell>0.2952 (27/0/23)</cell><cell>0.3210</cell><cell>0.5872</cell></row><row><cell>tlrTDhuSC</cell><cell>0.2964 (30/0/20)</cell><cell>0.2999</cell><cell>0.6070</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,409.36,423.01,226.48"><head>Table 2 :</head><label>2</label><figDesc>. Official runs for French and Portuguese. Runs ending in 3 correspond to the base run without PRF.</figDesc><table coords="4,116.83,432.02,369.34,170.00"><row><cell>Run</cell><cell>MAP</cell><cell cols="3">R-Prec Reciprocal Rank Recall</cell></row><row><cell></cell><cell>(Above/Equal/Below Median)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>tlrTDfr3</cell><cell>0.3735 (23/2/25)</cell><cell>0.3879</cell><cell>0.7014</cell><cell>0.8912</cell></row><row><cell>tlrTDfrRF2</cell><cell>0.4039  † (35/0/15)</cell><cell>0.4012</cell><cell>0.6806</cell><cell>0.9141</cell></row><row><cell>tlrTDfrRFS1</cell><cell>0.4  † (33/1/16)</cell><cell>0.3990</cell><cell>0.6806</cell><cell>0.9119</cell></row><row><cell>tlrTfr3</cell><cell>0.2925</cell><cell>0.3027</cell><cell>0.6163</cell><cell>0.7789</cell></row><row><cell>tlrTfrRF2</cell><cell>0.3073</cell><cell>0.3313</cell><cell>0.5729</cell><cell>0.8232</cell></row><row><cell>tlrTfrRFS1</cell><cell>0.3046</cell><cell>0.3278</cell><cell>0.5729</cell><cell>0.8215</cell></row><row><cell>tlrTDpt3</cell><cell>0.3501 (30/0/20)</cell><cell>0.3734</cell><cell>0.7542</cell><cell>0.8729</cell></row><row><cell>tlrTDptRF2</cell><cell>0.3742 (31/3/16)</cell><cell>0.3904</cell><cell>0.6704</cell><cell>0.9016</cell></row><row><cell>tlrTDptRFS1</cell><cell>0.3584 (31/3/16)</cell><cell>0.3805</cell><cell>0.6718</cell><cell>0.8939</cell></row><row><cell>tlrTpt3</cell><cell>0.2712</cell><cell>0.3141</cell><cell>0.6816</cell><cell>0.7358</cell></row><row><cell>tlrTptRF2</cell><cell>0.2844  †</cell><cell>0.3215</cell><cell>0.6682</cell><cell>0.7544</cell></row><row><cell>tlrTptRFS1</cell><cell>0.2830</cell><cell>0.3208</cell><cell>0.6515</cell><cell>0.7544</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,116.67,156.65,369.68,217.05"><head>Table 3 :</head><label>3</label><figDesc>Comparison between base runs and PRF runs using the MAP measure.</figDesc><table coords="5,116.67,156.65,369.68,217.05"><row><cell>Runs</cell><cell cols="4"># queries degraded No change # queries improved</cell></row><row><cell>tlrTDfr3 vs. tlrTDfrRF2</cell><cell>11</cell><cell></cell><cell>0</cell><cell>39</cell></row><row><cell>tlrTDfr3 vs. tlrTDfrRFS1</cell><cell>11</cell><cell></cell><cell>5</cell><cell>34</cell></row><row><cell>tlrTfr3 vs. tlrTfrRF2</cell><cell>23</cell><cell></cell><cell>2</cell><cell>25</cell></row><row><cell>tlrTfr3 vs. tlrTfrRFS1</cell><cell>23</cell><cell></cell><cell>3</cell><cell>24</cell></row><row><cell>tlrTDpt3 vs. tlrTDptRF2</cell><cell>21</cell><cell></cell><cell>0</cell><cell>29</cell></row><row><cell>tlrTDpt3 vs. tlrTDptRFS1</cell><cell>18</cell><cell></cell><cell>9</cell><cell>23</cell></row><row><cell>tlrTpt3 vs. tlrTptRF2</cell><cell>17</cell><cell></cell><cell>2</cell><cell>31</cell></row><row><cell>tlrTpt3 vs. tlrTptRFS1</cell><cell>17</cell><cell></cell><cell>3</cell><cell>30</cell></row><row><cell cols="2">Compared runs</cell><cell cols="3">Correct Misses Errors</cell></row><row><cell cols="2">tlrTDfr3 vs. tlrTDfrRFS1</cell><cell>0</cell><cell>5</cell><cell>11</cell></row><row><cell cols="2">tlrTfr3 vs. tlrTfrRFS1</cell><cell>0</cell><cell>1</cell><cell>23</cell></row><row><cell cols="2">tlrTDpt3 vs. tlrTDptRFS1</cell><cell>3</cell><cell>6</cell><cell>18</cell></row><row><cell cols="2">tlrTpt3 vs. tlrTptRFS1</cell><cell>0</cell><cell>1</cell><cell>17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,90.00,386.82,423.01,44.60"><head>Table 4 :</head><label>4</label><figDesc>Effectiveness of our prediction rule. Correct corresponds to cases when the prediction rule correctly avoided applying PRF. Misses corresponds to cases when PRF would have helped but was not applied. Errors corresponds to cases when the rule applied PRF and the performance degraded.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,101.09,700.11,152.47,8.44"><head>Table 5 :</head><label>5</label><figDesc>Official runs for French to Portuguese search. Runs ending in 3 correspond to the base run without PRF. Runs ending in 2 are the PRF runs use the following configuration: add 5 terms from the top 10 documents; terms are selected with α = β = 1 and γ = 0; expansion uses a fixed weight of 0.5 for each added term. Runs ending in 1 use the prediction rule prior to applying PRF.</figDesc><table coords="5,101.09,700.11,152.47,8.44"><row><cell>1 A *SUM node averages the beliefs of its</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,105.50,575.91,407.50,8.74;6,105.50,587.86,5.09,8.74;6,110.59,586.29,8.06,6.12;6,122.71,587.86,370.58,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,277.21,575.91,137.21,8.74">The INQUERY retrieval system</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Broglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,435.43,575.91,77.57,8.74;6,105.50,587.86,5.09,8.74;6,110.59,586.29,8.06,6.12;6,122.71,587.86,309.79,8.74">Proceedings of the 3 rd International Conference on Database and Expert Systems Applications</title>
		<meeting>the 3 rd International Conference on Database and Expert Systems Applications<address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,607.46,407.50,8.74;6,105.50,619.42,407.50,8.74;6,105.50,631.38,176.34,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,235.20,607.46,189.45,8.74">Relevance feedback and inference networks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,450.59,607.46,62.41,8.74;6,105.50,619.42,407.50,8.74;6,105.50,631.38,92.98,8.74">Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,670.58,407.50,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,152.42,670.58,300.56,8.74">Europarl: A multilingual corpus for evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Draft</note>
</biblStruct>

<biblStruct coords="6,105.50,690.18,407.50,8.74;6,105.50,702.13,237.10,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,266.03,690.18,246.97,8.74;6,105.50,702.13,28.27,8.74">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName coords=""><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,142.90,702.13,113.08,8.74">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,721.73,407.50,8.74;6,105.50,733.69,138.54,8.74;6,244.04,732.11,7.68,6.12;6,255.70,733.69,257.30,8.74;6,105.50,745.64,354.73,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,155.50,721.73,357.50,8.74;6,105.50,733.69,25.37,8.74">Natural language vs. boolean query evaluation: a comparison of retrieval performance</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,152.60,733.69,91.44,8.74;6,244.04,732.11,7.68,6.12;6,255.70,733.69,257.30,8.74;6,105.50,745.64,183.76,8.74">Proceedings of the 17 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="212" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,131.95,407.50,8.74;7,105.50,143.90,407.51,8.74;7,105.50,155.86,407.51,8.74;7,105.50,167.81,22.70,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,391.76,131.95,121.24,8.74;7,105.50,143.90,174.12,8.74">Juru at trec 2004: Experiments with prediction of query difficulty</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yom-Tov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Darlow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amitay</surname></persName>
		</author>
		<idno>SP 500-261</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,496.20,143.90,16.81,8.74;7,105.50,155.86,226.42,8.74">The Thirteenth Text Retrieval Conference (TREC 2004)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
