<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,191.48,74.13,223.69,12.64">CLEF 2005: Ad Hoc Track Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.96,112.20,57.47,8.96"><forename type="first">Giorgio</forename><forename type="middle">M</forename><surname>Di</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,250.63,112.20,50.85,8.96"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.80,112.20,68.79,8.96"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gjones@computing.dcu.ie</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.34,112.20,49.27,8.96"><forename type="first">Carol</forename><surname>Peters</surname></persName>
							<email>carol.peters@isti.cnr.it</email>
							<affiliation key="aff2">
								<orgName type="institution">ISTI-CNR</orgName>
								<address>
									<addrLine>Area di Ricerca</addrLine>
									<postCode>56124</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,191.48,74.13,223.69,12.64">CLEF 2005: Ad Hoc Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B40A96948446EDE359B2A428AA9D7049</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 [Systems and Software]: Performance evaluation Experimentation, Performance, Measurement, Algorithms Multilingual Information Access, Cross-Language Information Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the objectives and organization of the CLEF 2005 ad hoc track and discuss the main characteristics of the tasks offered to test monolingual, bilingual and multilingual textual document retrieval. The performance achieved for each task is presented and a preliminary analysis of results is given. The paper focuses in particular on the multilingual tasks which reused the test collection created in CLEF 2003 in an attempt to see if an improvement in system performance over time could be measured, and also to examine the multilingual results merging problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Ad Hoc retrieval track is generally considered as the core track in CLEF. The aim of this track is to promote the development of monolingual and cross-language textual document retrieval systems. As in past years, the CLEF 2005 ad hoc track was structured in three tasks, testing systems for monolingual (querying and finding documents in one language), bilingual (querying in one language and finding documents in another language) and multilingual (querying in one language and finding documents in multiple languages) retrieval, thus helping groups to make the progression from simple to more complex tasks. The document collections used were taken from the CLEF multilingual comparable corpus of news documents.</p><p>The Monolingual and Bilingual tasks were principally offered for Bulgarian, French, Hungarian and Portuguese target collections. Additionally, in the bilingual task only, newcomers (i.e. groups that had not previously participated in a CLEF cross-language task) or groups using a "new-to-CLEF" query language could choose to search the English document collection. The aim in all cases was to retrieve relevant documents from the chosen target collection and submit the results in a ranked list.</p><p>The Multilingual task was based on the CLEF 2003 multilingual-8 test collection which contained news documents in eight languages: Dutch, English, French, German, Italian, Russian, Spanish, and Swedish. There were two subtasks. a traditional multilingual retrieval task requiring participants to carry out retrieval and merging (Multi-8 Two-Years-On), and a new task focussing only on the multilingual results merging problem using standard sets of ranked retrieval output (Multi-8 Merging Only). One of the goals for the first task was to see whether it is possible to measure progress over time in multilingual system performance at CLEF by reusing a test collection created in a previous campaign. In running the merging only task our aim was to encourage participation by researchers interested in exploring the multilingual merging problem without the need to build retrieval systems for the document languages.</p><p>In this paper we describe the track setup, the evaluation methodology and the participation in the different tasks (Section 2), and present the main characteristics of the experiments and show the results (Sections 3 -5). The final section provides a brief summing up. For information on the various approaches and resources used by the groups participating in this track and the issues they focused on, we refer the reader to the other papers in these Working Notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Track Setup</head><p>The ad hoc track in CLEF adopts a corpus-based, automatic scoring method for the assessment of system performance, based on ideas first introduced in the Cranfield experiments <ref type="bibr" coords="2,392.53,195.84,11.69,8.96" target="#b0">[1]</ref> in the late 1960s. The test collection used consists of a set of "topics" describing information needs and a collection of documents to be searched to find those documents that satisfy the information needs. Evaluation of system performance is then done by judging the documents retrieved in response to a topic with respect to their relevance, and computing the recall and precision measures. The distinguishing feature of CLEF is that it applies this evaluation paradigm in a multilingual setting. This means that the criteria normally adopted to create a test collection, consisting of suitable documents, sample queries and relevance assessments, have been adapted to satisfy the particular requirements of the multilingual context. All language dependent tasks such as topic creation and relevance judgment are performed in a distributed setting by native speakers. Rules are established and a tight central coordination is maintained in order to ensure consistency and coherency of topic and relevance judgment sets over the different collections, languages and tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Test Collection</head><p>This year, for the first time, separate test collections were used in the ad hoc track: the monolingual and bilingual tasks were based on document collections in Bulgarian, English, French, Hungarian and Portuguese, whereas the two multilingual tasks reused a test collection -documents, topics and relevance assessments -created in CLEF 2003.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Documents:</head><p>The document collections used for the CLEF 2005 ad hoc tasks are part of the CLEF multilingual corpus of news documents described in the Introductory paper to these Working Notes <ref type="bibr" coords="2,429.27,424.32,10.78,8.96" target="#b1">[2]</ref>. In the monolingual and bilingual tasks, the English, French and Portuguese collections consisted of national newspapers and news agencies for the period 1994 and 1995. Different variants were used for each language. Thus, for English we had both US and British newspapers, for French we had a national newspaper of France plus Swiss French news agencies, and for Portuguese we had national newspapers from both Portugal and Brazil. This meant that, for each language, there were significant differences in orthography and lexicon over the sub-collections. This is a real world situation and system components, i.e. stemmers, translation resources, etc., should be sufficiently robust to handle such variants. The Bulgarian and Hungarian collections used in these tasks were new in CLEF 2005 and consisted of national newspapers for the year 2002 <ref type="foot" coords="2,317.36,516.06,3.00,5.40" target="#foot_0">1</ref> . This meant that the collections we used in the ad hoc mono-and bilingual tasks this year were not all for the same time period. This had important consequences on topic creation. For the multilingual tasks we reused the CLEF 2003 multilingual document collection. This consisted of news documents for 1994-95 in the 8 languages listed above in the Introduction.</p><p>Topics: Topics in CLEF are structured statements representing information needs; the systems use the topics to derive their queries. Each topic consists of three parts: a brief "title" statement; a one-sentence "description"; a more complex "narrative" specifying the relevance assessment criteria. Sets of 50 topics were created for the CLEF 2005 ad hoc mono-and bilingual tasks. One of the decisions taken early on in the organization of the CLEF ad hoc tracks was that the same set of topics would be used to query all collections, whatever the task.</p><p>There are a number of reasons for this: it makes it easier to compare results over different collections, it means that there is a single master set that is rendered in all query languages, and a single set of relevance assessments for each language is sufficient for all tasks. However, the fact that the collections used in the CLEF 2005 ad hoc mono-and bilingual tasks were from two different time periods <ref type="bibr" coords="2,345.09,657.24,92.86,8.96">(1994-1995 and 2002)</ref> made topic creation particularly difficult. It was not possible to create time-dependent topics that referred to particular date-specific events as all topics had to refer to events that could have been reported in any of the collections, regardless of the dates. This meant that the CLEF 2005 topic set is somewhat different from the sets of previous years as the topics tend to be of broad coverage. For this reason, it was difficult to construct topics that would find a limited number of relevant documents in each collection, and a -probably excessive -number of topics used for the 2005 mono-and bilingual tasks have a very large number of relevant documents. We have yet to analyze the possible impact of this fact on results calculation, but we suspect that it has meant that this year's ad hoc test collection is less effective in "discriminating" between the performance of different systems.</p><p>The topic sets for the mono-and bilingual tasks were prepared in thirteen languages: Amharic, Bulgarian, Chinese, English, French, German, Greek, Hungarian, Indonesian, Italian, Portuguese, Russian, and Spanish. Twelve were actually used and, as usual, English was by far the most popular. To counter this, in previous years, we placed restrictions on the possible topic languages for the bilingual task. We will probably reinstate some such constraint in CLEF 2006 in order to promote the testing of systems with less common languages.</p><p>For the multilingual task, the CLEF 2003 Dutch, English and Spanish sets of 60 topics were used. They were divided into two sets: 20 topics for training and 40 for testing.</p><p>Here below we give the English version of a typical topic from CLEF 2005: &lt;top&gt;&lt;num&gt; C254 &lt;/num&gt; &lt;EN-title&gt; Earthquake Damage &lt;/EN-title&gt; &lt;EN-desc&gt; Find documents describing damage to property or persons caused by an earthquake and specifying the area affected. &lt;/EN-desc&gt; &lt;EN-narr&gt; Relevant documents will provide details on damage to buildings and material goods or injuries to people as a result of an earthquake. The geographical location (e.g. country, region, city) affected by the earthquake must also be mentioned. &lt;/EN-narr&gt;&lt;/top&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relevance Assessment</head><p>Relevance assessment for the mono-and bilingual tasks was performed by native speakers. The multilingual tasks used the relevance assessments of 2003. The practice of assessing the results on the basis of the longest, most elaborate formulation of the topic (the narrative) means that only using shorter formulations (title and/or description) implicitly assumes a particular interpretation of the user's information need that is not (explicitly) contained in the actual query that is run in the experiment. The fact that such additional interpretations are possible has influence only on the absolute values of the evaluation measures, which in general are inherently difficult to interpret. However, comparative results across systems are usually stable regardless of different interpretations.</p><p>The number of documents in large test collections such as CLEF makes it impractical to judge every document for relevance. Instead approximate recall values are calculated using pooling techniques. The results submitted by the participating groups were used to form a pool of documents for each topic and language by collecting the highly ranked documents from all submissions. This pool was used for subsequent relevance judgment. After calculating the effectiveness measures, the results were analyzed and run statistics produced and distributed. A discussion of the results is given in Section 4. The individual results for all official ad hoc experiments in CLEF 2005 are given in Appendix at the end of these Working Notes. The stability of pools constructed in this way and their reliability for post-campaign experiments is discussed in <ref type="bibr" coords="3,435.66,500.87,11.72,8.96" target="#b2">[3]</ref> with respect to the CLEF 2003 pools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Participation Guidelines</head><p>To carry out the retrieval tasks of the CLEF campaign, systems have to build supporting data structures. Allowable data structures include any new structures built automatically (such as inverted files, thesauri, conceptual networks, etc.) or manually (such as thesauri, synonym lists, knowledge bases, rules, etc.) from the documents. They may not, however, be modified in response to the topics, e.g. by adding topic words that are not already in the dictionaries used by their systems in order to extend coverage. Some CLEF data collections contain manually assigned, controlled or uncontrolled index terms. The use of such terms has been limited to specific experiments that have to be declared as "manual" runs.</p><p>Topics can be converted into queries that a system can execute in many different ways. Participants submitting more than one set of results have used both different query construction methods and variants within the same method. CLEF strongly encourages groups to determine what constitutes a base run for their experiments and to include these runs (officially or unofficially) to allow useful interpretations of the results. Unofficial runs are those not submitted to CLEF but evaluated using the trec_eval package. This year we have used the new package written by Chris Buckley for TREC (trec_eval 7. <ref type="bibr" coords="3,356.97,706.43,3.98,8.96" target="#b2">3</ref>) and available from the TREC website As a consequence of limited evaluation resources, a maximum of 4 runs for each multilingual task and a maximum of 12 runs overall for the bilingual tasks, including all language combinations, was accepted. The number of runs for the monolingual task was limited to 12 runs. No more than 4 runs were allowed for any individual language combination. Overall, participants were allowed to submit at most 32 runs in total for the multilingual, bilingual and monolingual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Result Calculation</head><p>Evaluation campaigns such as TREC and CLEF are based on the belief that the effectiveness of IR systems can be objectively evaluated by an analysis of a representative set of sample search results. For this, effectiveness measures are calculated based on the results submitted by the participant and the relevance assessments. Popular measures usually adopted for exercises of this type are Recall and Precision. Details on how they are calculated for CLEF are given in <ref type="bibr" coords="4,161.60,186.60,10.69,8.96" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Participants and Experiments</head><p>As shown in Table <ref type="table" coords="4,151.78,242.52,3.76,8.96" target="#tab_0">1</ref>, a total of 23 groups from 15 different countries submitted results for one or more of the Ad-hoc tasks -a slight decrease on the 26 participants of last year. A total of 254 experiments were submitted, nearly the same as the 250 experiments of 2003. Thus, there is a slight increase in the average number of submitted runs per participant: from 9.6 runs/participant of 2004 to 11 runs/participant of this year. As stated, participants were required to submit at least one title+description ("TD") run per task in order to increase comparability between experiments. The large majority of runs (188 out of 254, 74.02%) used this combination of topic fields, 54 (21.27%) used all fields, 10 (3.94%) used the title field, and only 2 (0.79%) used the description field. The majority of experiments were conducted using automatic query construction. Manual runs tend to be a resource-intensive undertaking and it is likely that most participants interested in this type of work concentrated their efforts on the interactive track. A breakdown into the separate tasks is shown in Table <ref type="table" coords="5,514.50,84.60,3.76,8.96" target="#tab_1">2</ref>.</p><p>Thirteen different topic languages were used for ad hoc experiments-the Dutch run was in the multilingual tasks and used the CLEF 2003 topics. As always, the most popular language for queries was English, and French was second. Note that Bulgarian and Hungarian, the new collections added this year, were also quite popular as new monolingual tasks -Hungarian was also used in one case a topic language in a bilingual run. The number of runs per topic language is shown in Table <ref type="table" coords="5,242.83,142.08,3.76,8.96" target="#tab_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Monolingual Experiments</head><p>As stated, monolingual retrieval was offered for the following target collections: Bulgarian, French, Hungarian, and Portuguese. As can be seen from Table <ref type="table" coords="5,251.78,467.04,3.76,8.96" target="#tab_1">2</ref>, the number of participants and runs for each language was quite similar, with the exception of Bulgarian, which has a slightly smaller participation. This year just 5 groups out of 16 (31,25%) submitted monolingual runs only (down from ten groups last year), and just one of these groups was a first time participant in CLEF. This is in contrast with previous years where many new groups only participated in monolingual experiments. This year, most of the groups submitting monolingual runs were doing this as part of their bilingual or multilingual system testing activity. Table <ref type="table" coords="5,116.74,536.04,4.98,8.96">4</ref> shows the top five groups for each target collection, ordered by mean average precision. The table reports: the short name of the participating group; the run identifier, specifying whether the run has participated in the pool or not, and the page in Appendix A containing all figures and graphs for this run; the mean average precision achieved by the run; and the performance difference between the first and the last participant. The pages of appendix A containing the overview graphs are indicated under the name of the sub-task. Table <ref type="table" coords="5,519.47,582.00,4.98,8.96">4</ref> regards runs using title + description fields only (the mandatory run).</p><p>All the groups in the top five had participated in previous editions of CLEF. Both pooled and not pooled runs are in the best entries for each track. Finally, it can be noticed that the trend observed in the previous editions of CLEF is confirmed: differences for top performers for tracks with languages introduced in past campaigns are small: in particular only 5.35% in the case of French (French monolingual has been offered in CLEF since 2000) and 7.55% in the case of Portuguese, which was introduced last year. However, for the new languages, Bulgarian and Hungarian, the differences are much greater, in the order of 25%, showing that there should be room for improvement if these languages are offered in future campaigns.      Although, as has already been mentioned, English was by far the most popular language for queries, some less common and interesting query to target language pairs were tried, e.g. Amharic, Spanish and German to French, and French to Portuguese. The track overview paper in the post-workshop proceedings will provide a more in depth analysis of the approaches adopted for these tasks in CLEF 2005. One of the objectives will be to see if the hypothesis concerning a "blueprint for a successful CLIR system" proposed in <ref type="bibr" coords="11,387.15,119.04,11.70,8.96" target="#b5">[6]</ref> can be confirmed.</p><p>A main focus in the monolingual tasks was the development of new or the adaptation of existing stemmers and/or morphological analysers for the "new" CLEF languages: Bulgarian and Hungarian. Any comments on the outcomes?</p><p>The multilingual tasks at CLEF 2005 were intended to assess whether re-use of the CLEF 2003 Multi-8 task data could be used as an indication of progress in multilingual information retrieval and to provide common sets of ranked lists to enable specific exploration of merging strategies for multilingual IR. The submissions to these tasks show that multilingual performance can indeed be improved beyond that reported at CLEF 2003 both when performing the complete retrieval process and when merging ranked result lists generated by other groups. The initial running of this task suggests that there is scope for further improvement in multilingual IR from exploiting ongoing improvements in IR methods, but also from focused exploration of merging techniques.</p><p>Encouraged by the results of the multilingual tasks, we are currently considering running a similar X-yearson task for the mono-and/or bilingual experiments in CLEF 2006, again with the aim of seeing if it is possible to measure progress over time by testing new or updated systems against existing test collections.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,70.88,72.89,453.52,8.10;9,70.88,83.21,35.85,8.10"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Comparison between Multilingual 2-Years-On and CLEF 2003 Multilingual-8. Interpolated Recall vs Average Precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,70.88,399.41,453.54,8.10;9,70.88,409.73,35.85,8.10"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparison between Multilingual 2-Years-On and CLEF 2003 Multilingual-8. Document Cut-off Values vs Precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,70.88,399.65,453.49,8.10;10,70.88,410.09,35.85,8.10"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison between Multilingual Merging and CLEF 2003 Multilingual-8. Document Cut-off Values vs Precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,128.60,692.48,2.61,4.33;10,172.76,692.48,5.25,4.33;10,199.76,692.48,24.33,4.33;10,245.72,692.48,5.25,4.33;10,324.20,692.48,7.65,4.33;10,370.16,692.48,7.65,4.33;10,430.88,692.48,7.53,4.33;10,475.40,692.48,10.17,4.33;10,121.52,688.64,6.81,4.33;10,118.88,663.44,9.21,4.33;10,118.88,638.60,9.21,4.33;10,118.88,613.64,9.21,4.33;10,118.88,588.80,9.21,4.33;10,118.88,563.96,9.21,4.33;10,118.88,539.00,9.21,4.33;10,118.88,514.16,9.21,4.33;10,118.88,489.20,9.21,4.33;10,118.88,464.36,9.21,4.33;10,116.24,439.40,11.85,4.33;10,263.60,698.60,83.04,4.33;10,109.64,553.67,4.33,24.33;10,187.40,434.48,237.81,4.33;10,425.84,448.52,35.02,4.33;10,425.84,454.64,32.48,4.33;10,425.84,460.88,34.46,4.33;10,425.84,467.00,46.08,4.33;10,425.84,473.12,42.24,4.33"><head></head><label></label><figDesc>Merging best performers wrt. Multilingual CLEF 2003 -Retrieved documents vs Precision UNET150w05test dcu.Prositqgm2 UJAMENEDFRR bkmul8en3 (first 2003) UTAmul1 (fifth 2003)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,125.72,300.41,343.80,162.39"><head>Table 1 .</head><label>1</label><figDesc>CLEF 2005 ad hoc participants -new groups are indicated by *</figDesc><table coords="4,125.72,319.20,343.80,143.60"><row><cell cols="2">Budapest U. Tech.&amp;Econom (Hungary)* U.Amsterdam -Informatics (Netherlands)</cell></row><row><cell>CLIPS-Grenoble (France)</cell><cell>U.Buffalo -SUNY -Informatics (USA)</cell></row><row><cell>CMU -Lang.Tech. (USA)</cell><cell>U.Geneva -Inf.Systems (Switzerland)*</cell></row><row><cell>Daedalus &amp; Madrid Univs (Spain)</cell><cell>U.Glasgow -IR (UK)</cell></row><row><cell>Dublin City U. -Computing. (Ireland)</cell><cell>U.Hildesheim -Inf.Sci (Germany)</cell></row><row><cell>ENS des Mines St Etienne (France)*</cell><cell>U.Indonesia -Comp.Sci (Indonesia)*</cell></row><row><cell>Hummingbird Core Tech. (Canada)</cell><cell>U.Jaen -Intell.Systems (Spain)</cell></row><row><cell>Johns Hopkins U. (USA)</cell><cell>U.Lisbon (Portugal)</cell></row><row><cell>Moscow State U.-Computing (Russia)*</cell><cell>U.Neuchatel (Switzerland)</cell></row><row><cell>Swedish Inst.Comp.Sci (Sweden)</cell><cell>U.Stockholm, NLP (Sweden)</cell></row><row><cell>Thomson Legal Regulatory (USA)</cell><cell>U.Surugadai -Cultural Inf. (Japan)</cell></row><row><cell>U. Alicante -Comp.Sci + (Spain)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,179.00,502.49,248.76,177.39"><head>Table 2 .</head><label>2</label><figDesc>CLEF 2005 ad hoc experiments</figDesc><table coords="4,179.00,522.29,248.76,157.59"><row><cell>Track</cell><cell># Participants</cell><cell cols="2"># Runs (%)</cell></row><row><cell>AH-2-years-on</cell><cell>4</cell><cell>21</cell><cell>8.27%</cell></row><row><cell>AH-Merging</cell><cell>3</cell><cell>20</cell><cell>7.87%</cell></row><row><cell>AH-Bilingual-X2BG</cell><cell>4</cell><cell>12</cell><cell>4.72%</cell></row><row><cell>AH-Bilingual-X2EN</cell><cell>4</cell><cell>13</cell><cell>5.12%</cell></row><row><cell>AH-Bilingual-X2FR</cell><cell>9</cell><cell>31</cell><cell>12.20%</cell></row><row><cell>AH-Bilingual-X2HU</cell><cell>3</cell><cell>7</cell><cell>2.76%</cell></row><row><cell>AH-Bilingual-X2PT</cell><cell>8</cell><cell>28</cell><cell>11.02%</cell></row><row><cell>AH-Monolingual-BG</cell><cell>7</cell><cell>20</cell><cell>7.87%</cell></row><row><cell>AH-Monolingual-FR</cell><cell>12</cell><cell>38</cell><cell>14.97%</cell></row><row><cell>AH-Monolingual-HU</cell><cell>10</cell><cell>32</cell><cell>12.60%</cell></row><row><cell>AH-Monolingual-PT</cell><cell>9</cell><cell>32</cell><cell>12.60%</cell></row><row><cell>TOTAL</cell><cell></cell><cell>254</cell><cell>100.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,212.84,169.73,169.57,216.15"><head>Table 3 .</head><label>3</label><figDesc>List of experiments by topic language</figDesc><table coords="5,215.96,189.54,163.44,196.34"><row><cell></cell><cell>Language 2</cell><cell cols="2"># Runs (%)</cell></row><row><cell cols="2">EN English</cell><cell>85</cell><cell>33.47%</cell></row><row><cell cols="2">FR French</cell><cell>42</cell><cell>16.54%</cell></row><row><cell cols="2">HU Hungarian</cell><cell>33</cell><cell>12.99%</cell></row><row><cell>PT</cell><cell>Portuguese</cell><cell>32</cell><cell>12.60%</cell></row><row><cell cols="2">BG Bulgarian</cell><cell>20</cell><cell>7.87%</cell></row><row><cell>ES</cell><cell>Spanish</cell><cell>15</cell><cell>5.91%</cell></row><row><cell>ID</cell><cell>Indonesian</cell><cell>8</cell><cell>3.15%</cell></row><row><cell cols="2">DE German</cell><cell>6</cell><cell>2.36%</cell></row><row><cell cols="2">AM Amharic</cell><cell>4</cell><cell>1.57%</cell></row><row><cell cols="2">GR Greek</cell><cell>3</cell><cell>1.18%</cell></row><row><cell>IT</cell><cell>Italian</cell><cell>3</cell><cell>1.18%</cell></row><row><cell cols="2">RU Russian</cell><cell>2</cell><cell>0.79%</cell></row><row><cell cols="2">NL Dutch</cell><cell>1</cell><cell>0.39%</cell></row><row><cell cols="2">TOTAL</cell><cell>254</cell><cell>100.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,70.88,72.89,453.52,281.60"><head></head><label></label><figDesc>Recall vs Average Precision.</figDesc><table coords="9,106.77,106.41,348.12,248.08"><row><cell></cell><cell></cell><cell></cell><cell cols="7">Comparison of Multilingual 2-Years-On best performers wrt. Multilingual CLEF 2003 -Interpolated Recall vs Average Precision</cell><cell></cell></row><row><cell></cell><cell>100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">adhocM5Trntest</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">UJAPRFRSV2RR</cell></row><row><cell></cell><cell>90%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">esml9XstiSTp AUTOEN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">bkmul8en3 (first 2003)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">UTAmul1 (fifth 2003)</cell></row><row><cell></cell><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average Precision</cell><cell>50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0% 0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Interpolated Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,109.64,434.12,375.93,268.45"><head></head><label></label><figDesc>Comparison between Multilingual Merging and CLEF 2003 Multilingual-8. Interpolated Recall vs Average Precision.</figDesc><table coords="9,109.64,434.12,375.93,268.45"><row><cell cols="3">0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Figure 3. 0% Average Precision</cell><cell>10%</cell><cell cols="6">20% Comparison of Multilingual Merging best performers wrt. Multilingual CLEF 2003 -Interpolated Recall vs Average Precision 30% 40% 50% 60% 70% 80% UNET150w05test 90% dcu.Prositqgm2 UJAMENEDFRR bkmul8en3 (first 2003) UTAmul1 (fifth 2003)</cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Interpolated Recall</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Comparison of Multilingual 2-Years-On best performers wrt. Multilingual CLEF 2003 -Retrieved documents vs Precision</cell><cell></cell></row><row><cell></cell><cell>100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">adhocM5Trntest</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">UJAPRFRSV2RR</cell></row><row><cell></cell><cell>90%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">esml9XstiSTp AUTOEN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">bkmul8en3 (first 2003)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">UTAmul1 (fifth 2003)</cell></row><row><cell></cell><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>R-Precision</cell><cell>50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>30</cell><cell>100</cell><cell>200</cell><cell>500</cell><cell>1000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Retrieved Documents (logarithmic scale)</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,76.16,738.17,390.84,8.10"><p>It proved impossible to find national newspapers in electronic form for 1994 and/or 1995 in these languages.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,76.16,697.37,386.58,8.10"><p>Throughout the paper, language names are sometimes shortened by using their ISO-639 2-letter equivalent.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Bilingual Experiments</head><p>The bilingual task was structured in four subtasks (X → BG, FR, HU or PT target collection) plus, as usual, an additional subtask with English as target language -this last task was restricted to newcomers in a CLEF crosslanguage task or to groups using unusual or new topic languages (Amharic, Greek, Indonesian, and Hungarian) Table <ref type="table" coords="6,96.69,519.84,4.98,8.96">5</ref> shows the best results for this task. Note that both pooled and not pooled runs are in the best entries for each track, with the exception of Bilingual X → EN. For bilingual retrieval evaluation, a common method is to compare results against monolingual baselines. We have the following results for CLEF 2005:</p><p>• X -&gt; FR: 85% of best monolingual French IR system • X -&gt; PT: 88% of best monolingual Portuguese IR system • X -&gt; BG: 74% of best monolingual Bulgarian IR system • X -&gt; HU: 73% of best monolingual Hungarian IR system Similarly to monolingual, this is an interesting result. Whereas, the figures for French and Portuguese reflect those of recent literature <ref type="bibr" coords="6,173.02,626.16,10.63,8.96" target="#b4">[5]</ref>, it can be seen that for the new languages where there has been little CLIR system experience and testing so far, there is much room for improvement. It is interesting to note that when CLIR system evaluation began in 1997 at TREC-6 the best CLIR systems had the following results:</p><p>• EN -&gt; FR: 49% of best monolingual French IR system • EN -&gt; DE: 64% of best monolingual German IR system </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multilingual Experiments</head><p>Table <ref type="table" coords="7,98.24,554.04,4.98,8.96">6</ref> shows results for the best entries for the multilingual tasks and is organized similarly to Table <ref type="table" coords="7,516.99,554.04,3.76,8.96">4</ref>.  <ref type="table" coords="7,142.62,645.96,3.76,8.96">6</ref>. These figures are slightly different from the original results which appear in the CLEF 2003 proceedings <ref type="bibr" coords="7,121.65,657.48,10.66,8.96" target="#b2">[3]</ref>, although the ranking of these runs is unchanged.</p><p>It can be seen from Table <ref type="table" coords="7,193.27,669.00,4.98,8.96">6</ref> that the performance difference between the first and the last participant for the 2-Years-On track is much greater (nearly 3 times) than the corresponding difference in 2003, even if the task performed in these two tracks is the same. On the other hand, the performance difference for the Merging track is nearly one third of the corresponding difference in 2003: it seems that merging the results of the run reduces the gap between the best and the last performer, even though there is still a considerable difference (35.63%) if compared to the small differences of most popular monolingual languages, e.g. 5.35% of monolingual French.</p><p>We can note that the top participant of the 2-Years-On track achieves a 15.89% performance improvement with respect to the top participant of CLEF 2003 Multi-8. On the other hand, the fourth participant of the 2-Years-On track has a 59.15% decrease in performance with respect to the fourth participant of CLEF 2003 Multi-8. Similarly, we can note that the top participant of the Merging track achieves a 6.24% performance improvement with respect to the top participant of 2003.</p><p>In general, we can note that for the 2-Years-On track there is a performance improvement only for the top participant, while the performances deteriorate quickly for the other participants with respect to 2003. On the other hand, for the Merging track the performance improvement of the top participant with respect to 2003 is less than in the case of the 2-Years-On track. . There is also less variation between the submissions for the Merging task than seen in the earlier 2003 runs. This is probably due to the fact that the participants were using the same ranked lists, and that the variation in performance arises only from the merging strategies adopted. Table <ref type="table" coords="8,95.72,210.89,3.41,8.10">6</ref>. Best entries for the multilingual task (title+description topic fields only). The performance difference between the best and the last (up to 5) placed group is given (in terms of average precision).  <ref type="figure" coords="8,331.58,536.88,4.98,8.96">1</ref> shows the average precision at the different interpolated recall levels, while Figure <ref type="figure" coords="8,228.21,548.28,4.98,8.96">2</ref> shows the precision at different document cut-off values. Figures <ref type="figure" coords="8,502.27,548.28,4.98,8.96">3</ref> and<ref type="figure" coords="8,70.88,559.80,4.98,8.96">4</ref> show corresponding results for the Multilingual Merging task. Trends in these figures are similar to those seen in Table <ref type="table" coords="8,108.20,571.32,3.76,8.96">6</ref>. The top performing submission to the Multilingual 2-Years-On and Merging tasks are both clearly higher than the best submission to the CLEF 2003 task. The variation between submissions for 2-Years-On is also greater than that observed for the Merging only task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participant</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>From a first rapid glance at the reports from the groups that participated in the bilingual ad hoc tasks, it appears that this year's experiments provide a good overview of most of the traditional approaches to CLIR when matching between query and target collection, including n-gram indexing, machine translation, machinereadable bilingual dictionaries, multilingual ontologies, pivot languages, query and document translationperhaps corpus-based approaches were less used than in previous years continuing a trend first noticed in CLEF 2004. Veteran groups were mainly concerned with fine tuning and optimizing strategies already tried in previous years. The issues examined were the usual ones: word-sense disambiguation, out-of-dictionary vocabulary, ways to apply relevance feedback, results merging, etc.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,75.02,334.06,449.56,9.94;11,88.88,345.89,203.35,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,146.12,335.45,178.77,8.10">The Cranfield Tests on Index Language Devices</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cleverdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,480.82,335.45,43.76,8.10;11,88.88,345.89,77.08,8.10">Readings in Information Retrieval</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Sparck-Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Willett</surname></persName>
		</editor>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="47" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,75.02,356.62,223.74,9.94" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="11,128.57,358.01,113.86,8.10">What happened in CLEF 2005?</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="11,75.02,368.86,449.28,9.94;11,88.88,380.69,414.37,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,143.07,370.25,121.36,8.10">CLEF 2003 -Overview of results</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,282.38,370.25,241.92,8.10;11,88.88,380.69,16.26,8.10">Fourth Workshop of the Cross-Language Evaluation Forum, CLEF 2003</title>
		<title level="s" coord="11,271.04,380.69,128.50,8.10">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003. 2004</date>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="44" to="63" />
		</imprint>
	</monogr>
	<note>Revised papers</note>
</biblStruct>

<biblStruct coords="11,75.02,391.42,448.40,9.94;11,88.88,403.25,434.84,8.10;11,88.88,413.57,74.61,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,193.38,392.81,147.44,8.10">CLEF 2003 Methodology and Metrics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,364.74,392.81,158.68,8.10;11,88.88,403.25,111.95,8.10">Fourth Workshop of the Cross-Language Evaluation Forum, CLEF 2003</title>
		<title level="s" coord="11,370.00,403.25,130.78,8.10">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003. 2004</date>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="7" to="20" />
		</imprint>
	</monogr>
	<note>Revised papers</note>
</biblStruct>

<biblStruct coords="11,75.02,424.30,433.04,9.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,199.76,425.69,203.26,8.10">The Impact of Evaluation on Multilingual Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,408.87,425.69,44.52,8.10">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="603" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,75.02,436.54,448.54,9.94;11,88.88,448.37,110.36,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,213.27,437.93,203.41,8.10">Cross-Language Evaluation Forum: Objectives, Results</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,423.68,437.93,99.88,8.10;11,88.88,448.37,31.70,8.10">Achievements, Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="5" to="29" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
