<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.44,74.05,285.06,12.64;1,104.16,96.13,385.50,12.64;1,254.16,112.21,85.62,12.64">Using Syntactic Dependency and Language Model X-IOTA IR System Used for CLIPS Mono &amp; Bilingual Experiments for CLEF 2005</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,228.72,155.44,70.34,8.96"><forename type="first">Loïc</forename><surname>Maisonnasse</surname></persName>
							<email>loic.maisonnasse@imag.fr</email>
						</author>
						<author>
							<persName coords="1,306.12,155.44,59.17,8.96"><forename type="first">Gilles</forename><surname>Sérasset</surname></persName>
						</author>
						<author>
							<persName coords="1,254.16,201.52,85.69,8.96"><forename type="first">Jean-Pierre</forename><surname>Chevallet</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire CLIPS-IMAG</orgName>
								<address>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IPAL-CNRS</orgName>
								<address>
									<postCode>I2R</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">STAR</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.44,74.05,285.06,12.64;1,104.16,96.13,385.50,12.64;1,254.16,112.21,85.62,12.64">Using Syntactic Dependency and Language Model X-IOTA IR System Used for CLIPS Mono &amp; Bilingual Experiments for CLEF 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9731DBEBA3BAB39D22D3EDDBCB11ACE2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This document describes the CLIPS experiments done for the CLEF 2005 campaign. We use surface-syntactic parser in order to extract new indexing terms. These terms are syntactic dependencies. Our goal is to evaluate their interest for an information retrieval task. We used them under different forms in different information retrieval models, particularly in a language model.</p><p>For the bilingual part we tried two simple tests on Spanish and German to French evaluation, for the translation we use a lemmatization and a dictionary.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the previous participation of the laboratory CLIPS at CLEF <ref type="bibr" coords="1,338.52,392.32,10.69,8.96" target="#b0">[1]</ref>, we promote the use of surface-syntactic parsers in order to extract indexing terms. Last year, we only extracted simple indexing terms, this year we have tried to exploit the structure extracted by the parser. By this, we have made two different evaluations; in the first one, we divided the structure extracted into complex descriptors, which contains a part of the global structure. In the second one, we use the syntactic dependencies between lemmas extracted by the shallow parser in a language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Sub-structure training on monolingual run</head><p>In this part, we evaluate the use of sub-structures as indexing terms. The shallow parser produces a structure, by using only the lemmas as we did last year, we only use a part of the extracted information. This year we evaluate the interest of the structural information produced by the parser. Different type of parser are available, in this paper we use a dependency parser as that kind of parser seems to be more appropriate for the information retrieval task <ref type="bibr" coords="1,125.28,536.08,10.69,8.96" target="#b1">[2]</ref>. Different works have already been made to use syntactic dependency structures. Some of those works use the dependency structure in order to extract phrases. For example, in <ref type="bibr" coords="1,334.08,565.12,11.71,8.96" target="#b2">[3]</ref> the author produces a structure close from a dependency tree for all documents sentences. After that, he applies some patterns on the structure for extracting phrases then some selected phrases are added to the others descriptors in the document index. At last, the tf-idf weighting schema is adjusted in order to give a higher idf for the extracted phrase. This way, a 20% gain on the average precision is provided on information retrieval results. Nevertheless, this gain cannot be directly linked with the use of a dependency structure.</p><p>On the presumption that converting the structures to phrases leads to lose information, others works have tried to use directly the syntactic dependency structure. In <ref type="bibr" coords="1,288.84,651.52,10.69,8.96" target="#b3">[4]</ref>, a dependency tree is extracted from each Japanese sentence, mainly on the documents titles. The matching between a query and the documents is provided by a projection matching of the query tree on the documents trees. In addition, to provide, a better matching some cut can be made on the tree. In <ref type="bibr" coords="1,184.68,686.08,10.78,8.96" target="#b4">[5]</ref>, the COP parser (Constituent Object Parser) is used to extract dependency trees. In the query the user has to select important terms and to indicate dependencies between them, the query is then compare to the documents by different types of matching. As the two preceding works provided only one unambiguous structure by sentence, in <ref type="bibr" coords="1,228.96,720.52,11.71,8.96" target="#b5">[6]</ref> the author incorporates syntactic ambiguity in the extracted structure. His model is apply on phrases, the similarity is provided by tree matching but the IR results are lower than the results obtained by considering only the phrases represented in the tree.</p><p>In our evaluation, we consider an intermediary representation level. For this purpose, we use sub-structures, which are composed by one dependency relation. By this representation a sentence is consider as a set of substructures that we call dependencies. In our formalism, the representation of the sentence "the cat eats the mouse" is represented by the set: {DET(the, cat), DET(the, mouse), SUBJ(cat, eat), VMOD(mouse, eat)}. Where "the" is the determiner of "cat", "cat" is the subject of "eat", etc. In this representation we lose only the information that there is two "the" in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Experimentation schema</head><p>For this experiment, we only used the French corpus. On this corpus, we experiment the use of the dependency descriptor. For this purpose, we use an experimental sequence, described in the following schema: First, the different documents of the collection are analysed with the French parser XIP (Xerox Incremental Parser) <ref type="bibr" coords="2,103.32,449.08,11.71,8.96" target="#b6">[7]</ref> from these documents two descriptors are extracted: the dependencies and the lemmas. In a first experiment, we consider these descriptors separately and create two indexes. One contains the lemmas and the other the dependencies. We query these two indexes separately with the dependencies and the lemmas extracted from the queries by the same parser. We compare the results obtain with the two descriptors for different weighting schemes. In a second experiment, we regroup the two descriptors in the same index and we evaluate the results for different weightings schemes.</p><p>For training, we have used the French corpus of CLEF 2003. In this corpus, there are 3 sets of documents. For each set, we have selected the following fields: TITLE and TEXT for "le monde 94", TI KW LD TX ST for "sda 94" and "sda 95". For the queries, we have selected the fields FR-title FR-descr Fr-narr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dependencies versus lemmas</head><p>We have first compared the result obtain by using dependencies to the result obtain with lemmas. In these experimentation lemmas are used as baseline as they have already show they usability in the CLIPS experiments last year <ref type="bibr" coords="2,110.40,610.36,10.69,8.96" target="#b0">[1]</ref>. After parsing the documents with XIP, we have transformed the output into a common XML simplified format (Figure <ref type="figure" coords="2,177.24,621.88,3.63,8.96">2</ref>). From this XML, in one side we have extracted the lemmas: for these descriptors, we have filtered only nouns, proper nouns, verbs, adjectives and numbers. &lt;LUNIT&gt; &lt;NODE num="2" tag="DET" lemma="le" ...&gt;les&lt;/NODE&gt; &lt;NODE num="3" tag="NOUN" lemma="manifestation" ... &gt;manifestations&lt;/NODE&gt; &lt;NODE num="5" tag="PREP" lemma="contre" ... &gt;contre&lt;/NODE&gt; &lt;NODE num="7" tag="DET" lemma="le" ... &gt;le&lt;/NODE&gt; &lt;NODE num="8" tag="NOUN" lemma="transport" ... &gt;transport&lt;/NODE&gt; &lt;NODE num="10" tag="PREP" lemma="de" ... &gt;de&lt;/NODE&gt; &lt;NODE num="12" tag="NOUN" lemma="déchet" ... &gt;déchets&lt;/NODE&gt; &lt;NODE num="14" tag="ADJ" lemma="radioactif" ... &gt;radioactifs&lt;/NODE&gt; &lt;NODE num="16" tag="PREP" lemma="par" ... &gt;par&lt;/NODE&gt; &lt;NODE num="18" tag="NOUN" lemma="conteneur" ... &gt;conteneurs&lt;/NODE&gt; &lt;NODE num="23" tag="SENT" lemma="." ... &gt;.&lt;/NODE&gt; &lt;DEP name="NMOD" ... w0="déchet" w1="radioactif"/&gt; &lt;DEP name="NMOD" ... w0="manifestation" w1="contre" w2="transport"/&gt; &lt;DEP name="NMOD" ... w0="transport" w1="de" w2="déchet"/&gt; &lt;DEP name="NMOD" ... w0="déchet" w1="par" w2="conteneur"/&gt; &lt;DEP name="DETERM" ... w0="le" w1="manifestation"/&gt; &lt;DEP name="DETERM" ... w0="le" w1="transport"/&gt; &lt;/LUNIT&gt; On the other side, we extract the dependencies (Figure <ref type="figure" coords="3,295.08,438.76,3.63,8.96">3</ref>). As the number of dependencies can be very high, we query separately each documents set and then merge the results. We have compared IR results obtains with these two descriptors for different weighting schema. We used the following weighting schemes on the document and on the query descriptors:</p><p>For the documents nnn:</p><p>Only the term frequency is used.</p><p>lnc: Use a log on term frequency and the cosine is used as the final normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ltc:</head><p>The classical tf*idf with a log on the term frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>nRn: Derivation from randomness</head><p>For the queries nnn:</p><p>Only the term frequency is used.</p><p>bnn:</p><p>The binary model, the terms presents are associated to the value 1, and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>lnn:</head><p>A log is used on the term frequency.</p><p>npn: Idf variant used by okapi.</p><p>ntn: classical idf.</p><p>For more details, see <ref type="bibr" coords="3,156.60,683.32,10.69,8.96" target="#b0">[1]</ref>.</p><p>We have first evaluated the c coefficient for the derivation from randomness weighting (nRn) on the document and with an nnn weighting on the queries. Results for the two descriptors are shown on Over all weighting, the dependencies descriptor performs better than the lemmas only for the nnn weighting. The derivation from randomness performs better than the others documents weighting for the two descriptors and the result are stable connecting to the query weighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Lemmas and dependencies</head><p>As in a first experiment we have used the dependencies and the lemmas separately. In this part, we merged the two descriptors in one index and evaluated the different weighting schemes for that index.</p><p>As for the preceding experimentation, we first evaluate the derivation from randomness (Table <ref type="table" coords="4,462.96,509.68,4.12,8.96" target="#tab_3">4</ref>) and then we evaluate the different weighting methods ( The results obtained in this evaluation are better than those obtain for the dependencies alone but they are lower than those obtain for the lemmas. The reason is that the dependencies and the lemmas are considered as equivalent descriptors when these two descriptors are clearly from two different levels as the dependencies contains the lemmas. This particularity was not taken in account in this experimentation. Nevertheless, as we want to evaluate the use of dependencies we have sent a run with nRn nnn weighting with both dependencies and lemmas for the monolingual run and with the coefficient c at 2.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Language model</head><p>In a second experimentation, we have integrated the structure extracted by XIP in a language model based information retrieval system. Some work have already been made in order to use dependencies between terms in a language model in <ref type="bibr" coords="5,154.20,314.44,11.15,8.96" target="#b7">[8]</ref> <ref type="bibr" coords="5,165.35,314.44,11.15,8.96" target="#b8">[9]</ref>. These works used statistical based methods in order to get the tree representation of a sentence; here we used a linguistically extracted structure.</p><p>In order to use a language model based on dependencies, we have parsed the different documents sets. We have transformed the output into the XML simplified format. From this XML, we have filtered only nouns, proper nouns, verbs, adjectives and numbers and the dependency that connect only these descriptors. For each sentence, we obtain a graph where the nodes are the significant elements of the sentence linked by dependencies (Figure <ref type="figure" coords="5,70.92,389.44,3.63,8.96" target="#fig_2">4</ref>). We have used these graphs to apply a language model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Language model</head><p>The language model we used is a simplified version of the model proposed in <ref type="bibr" coords="5,391.80,587.20,10.60,8.96" target="#b7">[8]</ref>. This model assumes that the query generation is formulated as a two-stage process. In a first time a linkage is generated from the document following the distribution P(L|D). In a second time the query is generated following the distribution P(Q|L,D), the query terms are generated at this stage according on the terms linked in L. So in this model the probability of the query P(Q|D) over all possible linkage Ls is :</p><formula xml:id="formula_0" coords="5,187.68,649.32,220.11,25.16">= = Ls Ls D L Q P D L P D L Q P D Q P ) , ( ) ( ) , ( ) (</formula><p>The authors assume that the sum</p><formula xml:id="formula_1" coords="5,214.80,686.24,59.31,21.23">Ls D L Q P ) , (</formula><p>over all the possible linkages Ls is dominated by a single linkage L, which is the most probable linkage. Here we consider that the most possible linkage L is those extract by our parser. The authors finally obtain:</p><formula xml:id="formula_2" coords="5,210.36,409.66,147.60,97.07">manifestation transport déchets Radioactif conteneur = ∈ + + = m i L j i j i i D L q q MI D q P D L P D Q P .. 1 ) , ( ) , , ( ) ( )) ( log( ) ( Where ) ( ) ( ) , , ( log ) , , ( D q P D q P D L q q P D L q q MI j i j i j i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=</head><p>Here we use simple estimation of the different parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">P(L|D)</head><p>We estimate P(L|D) as the probability that two terms are linked if they appear in the same sentences in the document. On this estimation, we made an interpolation of the document probability with the collection probability. So we obtain the following formulation:</p><formula xml:id="formula_3" coords="6,73.08,235.41,300.63,31.18">) , ( ) , , ( ) , ( ) , , ( ) 1 ( ) ( ) ( ) , ( j i j i L j i d j i j i d L l q q C R q q C q q D R q q D D l P D L P ∏ ∏ ∈ ∈ + - ∝ = λ λ</formula><p>where l denotes a dependency between two terms ) , , ( R q q D j i denotes the number of time that q i and q j are linked in a sentence of the document ) , ( j i q q D denotes the number of time that q i and q j appear in the same sentence.</p><p>) , (</p><formula xml:id="formula_4" coords="6,107.76,346.06,107.42,12.69">j i q q C , ) , , ( R q q C j i</formula><p>denotes the equivalent number but evaluated on the whole collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">P(q i |D)</head><p>We estimate P(q i |D) as the probability that a term appear in a document, and we made an interpolation on the collection.</p><formula xml:id="formula_5" coords="6,73.08,419.17,180.76,17.36">) ( ) ( ) 1 ( ) ( C q P D q P D q P i l i l i λ λ + - =</formula><p>In the two last estimation, if a lemma or a dependency does not appear in the collection the probability is set to zero, consequently the whole probability will be set to zero. To avoid this, in the query we consider only the dependencies and the lemmas found in the whole collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">MI(qi, qj|L,D)</head><p>For this estimation, we use the same estimation as the one used in the article <ref type="bibr" coords="6,378.48,510.52,10.69,8.96" target="#b7">[8]</ref>.</p><p>) , (*, ) ,*, (</p><formula xml:id="formula_6" coords="6,88.32,529.30,186.14,29.50">) (*,*, ) , ,<label>( log ) , , ( R q</label></formula><formula xml:id="formula_7" coords="6,72.96,529.30,197.15,31.42">C R q C R C R q q C D L q q MI j i j i j i =</formula><p>where C(x,y,R) denotes the count of link between the words x and y, the * symbolize all possible word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>We apply this model on the Collection of CLEF 2003, the results obtain are presented in the following table where we evaluate the variation of the coefficients l and d . l / d Average precision 0,1/0,5 0.2749 0,2/0,5 0.2724 0,3/0,5 0.2697 0,4/0,5 0.2536 0,5/0,5 0.2495 0,6/0,5 0.2428 0,1/0,9999 0.2778 0,2/0,9999 0.2951 0,3/0,9999 0.2890 Table <ref type="table" coords="7,262.80,216.64,4.18,8.96">6</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>: varation of l and d</head><p>We see that the results are better when the coefficient l is around 3 and when the coefficient d is high. So the results are better when the dependencies in the query are not take in account. This thinks may comes from the fact that we use simple estimations; better estimation on the probability may give better results.</p><p>We submit a run for this language model with the coefficient l at 0.3 and the coefficient d at 0.9999, the same experimental condition are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Bilingual</head><p>For the crosslingual evaluation, we have made two simple run from German and Spanish to French. For those two run, we use the three query fields : XX-title, XX-descr, XX-narr. And we query the French collection with the same fields as in the monolingual experiments. In this evaluation, the queries words are lemmatized and then we translate those lemma with the web dictionary interglot<ref type="foot" coords="7,305.88,369.97,3.24,5.83" target="#foot_0">1</ref> .</p><p>For the lemmatization, we use TreeTagger<ref type="foot" coords="7,246.48,387.49,3.24,5.83" target="#foot_1">2</ref> for the German queries and we use agme lemmatizer <ref type="bibr" coords="7,476.64,389.68,16.75,8.96" target="#b9">[10]</ref> for the Spanish queries. On these lemmatizers if there is an ambiguity, we conserve all the possible forms. We translate the lemmas with the dictionary and we keep all the translations found. At last, we query the French lemmas index with the derivation from randomness weighting on the documents.</p><p>On clef 2003, we obtain an average precision of 0.0902 for the German queries and an average precision of 0.0799 for the Spanish queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Monolingual</head><p>For this evaluation, we sent three different runs. Two of those runs were based on dependencies with lemmas index with a weighting schema 'nRn nnn' with the coefficient c at 2.25. The first of those two run 'FR0' use only the fields FR-title FR-descr of the queries, the second 'FR1' use the all fields. The last run 'FR2'submitted for the French monolingual task used the language model described in section 3. We can see that as 'FR1' use the field 'FR-narr' for the query his results are lover than the run 'FR0' which doesn't use this field. This artefact may comes from the fact that we have not used a program that process the topic in order to remove unrelevant phrase as 'Les documents pertinents doivent expliquer' (the relevant documents must explains). We observe that the results obtain on CLEF 2005 are lower than those obtain on CLEF 2003 specially when we used the three query fields the result for CLEF 2005 are more than two times lover than the results for CLEF 2003. This result may come from the fact that the narrative part of the queries seems to be shorter in CLEF 2005. Another difference could be seen between 'FR1' and 'FR2' as these two runs show a difference of about </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Bilingual</head><p>For this experiment, we send two runs for each source language. One of those two run used the topic field XXtitle and XX-descr. The second use in addition of those two fields the field XX-narr. <ref type="bibr" coords="8,283.08,198.16,8.30,8.96">de</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>For our participation at clef 2005 we began to evaluate the use of syntactic dependency structures extracted by a parser in an information retrieval task. In our first experiment, we tried to exploit the structure by using descriptors that capture a part of the structure. In our second experiment, we exploited directly the structure extracted by the parser in a language model. The two experiments show that the structure is exploitable, but the results are still lower than those obtained using only lemmas with appropriate weightings.</p><p>As the syntactic structure has shown to be exploitable in IR some improvements could be applied on this model. We used here the XIP parser, but this parser does not give information on the quality of the structure. Integrating this kind of information on the dependencies extracted could improve the IR results. Using a parser, that extract deeper syntactic dependencies may give better results for the information retrieval task. At last, our language model uses simple estimations, better estimation may improve the results.</p><p>Our conviction is that detailed syntactic information, which is already available using existing parsers, is to improve results (especially, precision) in information retrieval tasks. However, such detailed information has to be combined with classical descriptors as, taken alone, it does not show better results. Obviously, we still have to find ways to combine the advantages of classical, raw descriptors with the added value of fine grain syntactic information in a single model. Independently of the task, we see that using the narrative part of the queries lowers our results for the next participation in order to improve our results we have to make an module that selects only the important part of the topic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,225.60,412.48,143.98,8.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: experimental procedure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,93.84,287.56,407.55,8.96;3,79.56,299.08,436.03,8.96"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: XML Information for the sentence : "les manifestations contre le transport de déchets radioactifs par conteneurs." (Demonstrations against the transport of radioactive waste by containers)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,76.32,524.08,442.66,8.96;5,190.44,535.60,214.38,8.96"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: structure used by the langue model for the sentence: "les manifestations contre le transport de déchets radioactifs par conteneurs en Allemagne."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,70.92,712.24,453.57,20.48"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="4,129.48,92.44,335.94,153.44"><row><cell>c</cell><cell>Average precision</cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>4.89</cell><cell>c</cell><cell>Average precision</cell></row><row><cell>2</cell><cell>25.53</cell><cell>0</cell><cell>0.0152</cell></row><row><cell>3</cell><cell>25.50</cell><cell>0,5</cell><cell>0.4362</cell></row><row><cell>4</cell><cell>25.83</cell><cell>1</cell><cell>0.4647</cell></row><row><cell>4.25</cell><cell>25.93</cell><cell>1,75</cell><cell>0.4700</cell></row><row><cell>4.5</cell><cell>26.01</cell><cell>1,5</cell><cell>0.4703</cell></row><row><cell>4.75</cell><cell>26.00</cell><cell>2</cell><cell>0.4687</cell></row><row><cell>5</cell><cell>25.88</cell><cell>2,25</cell><cell>0.4728</cell></row><row><cell>5.5</cell><cell>25.84</cell><cell>2,5</cell><cell>0.4709</cell></row><row><cell>6</cell><cell>25.84</cell><cell>3</cell><cell>0.4577</cell></row><row><cell>10</cell><cell>25.64</cell><cell></cell><cell></cell></row></table><note coords="3,461.16,712.24,63.33,8.96;3,70.92,723.76,369.93,8.96"><p><p><p><p><p>and on Table</p>2</p>. After that we have evaluated the others weighting methods, results are presented on Table</p>3</p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,108.72,249.04,377.29,148.52"><head>Table 1 : Variation of c for nRn nnn (dependencies alone) Table 2 variation of c for nRn nnn (lemmas alone)Table 3 : lemmas or dependencies average precision</head><label>13</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,241.50,521.08,108.72,138.56"><head>Table 5</head><label>5</label><figDesc></figDesc><table coords="4,242.52,521.08,107.70,138.56"><row><cell>).</cell><cell></cell></row><row><cell>c</cell><cell>Average precision</cell></row><row><cell>0</cell><cell>0,0207</cell></row><row><cell>1</cell><cell>0,3798</cell></row><row><cell>1,5</cell><cell>0,3941</cell></row><row><cell>2</cell><cell>0,3947</cell></row><row><cell>2,25</cell><cell>0,3947</cell></row><row><cell>2,5</cell><cell>0,3934</cell></row><row><cell>3</cell><cell>0,3922</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,76.56,296.92,419.70,374.96"><head>Table 4 : Variation of c for nRn nnn (lemmas and dependencies)</head><label>4</label><figDesc></figDesc><table coords="4,76.56,296.92,419.70,88.40"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Query weighting</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Document</cell><cell></cell><cell></cell><cell>lemmas</cell><cell></cell><cell></cell><cell></cell><cell cols="3">dependencies</cell><cell></cell></row><row><cell>Weighting</cell><cell>nnn</cell><cell>bnn</cell><cell>lnn</cell><cell>npn</cell><cell>ntn</cell><cell>nnn</cell><cell>bnn</cell><cell>lnn</cell><cell>npn</cell><cell>ntn</cell></row><row><cell>nnn</cell><cell>1,82</cell><cell>0,81</cell><cell>1,57</cell><cell>21,43</cell><cell>16,43</cell><cell>9,01</cell><cell>5,56</cell><cell>8,16</cell><cell>18,21</cell><cell>17,96</cell></row><row><cell>lnc</cell><cell>35,02</cell><cell>31,27</cell><cell>36,22</cell><cell>34,30</cell><cell>37,46</cell><cell>18,92</cell><cell>17,46</cell><cell>19,17</cell><cell>21,93</cell><cell>21,94</cell></row><row><cell>ltc</cell><cell>33,13</cell><cell>33,93</cell><cell>35,94</cell><cell>32,86</cell><cell>33,79</cell><cell>21,14</cell><cell>18,94</cell><cell>20,86</cell><cell>21,66</cell><cell>21,66</cell></row><row><cell>nRn</cell><cell>47,28</cell><cell>38,34</cell><cell>45,55</cell><cell>45,23</cell><cell>48,35</cell><cell>26,01</cell><cell>22,56</cell><cell>25,90</cell><cell>24,95</cell><cell>24,94</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,184.92,164.80,225.49,8.96"><head>Table 5 : lemmas and dependencies average precision</head><label>5</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,122.40,198.16,353.35,71.12"><head>Table 8 : bilingual results</head><label>8</label><figDesc></figDesc><table coords="8,122.40,198.16,353.35,59.00"><row><cell></cell><cell>-&gt;fr</cell><cell></cell><cell></cell><cell>es-&gt;fr</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Title+</cell><cell></cell></row><row><cell></cell><cell cols="2">title+ desc title+desc+narr</cell><cell>desc</cell><cell>title+desc+narr</cell></row><row><cell>average precision</cell><cell>6.88</cell><cell>4.98</cell><cell>4.23</cell><cell>3.69</cell></row><row><cell>precision at 5 documents</cell><cell>17.20</cell><cell>12.80</cell><cell>10.80</cell><cell>11.60</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="7,76.68,736.00,80.17,8.96"><p>http://interglot.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,76.68,747.52,349.21,8.96"><p>http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,81.61,565.24,442.83,8.96;8,70.92,576.76,453.59,8.96;8,70.92,588.16,112.86,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,198.45,565.24,325.99,8.96;8,70.92,576.76,272.29,8.96">Using surface-syntactic parser and Deviation from Randomness. X-IOTA IR system used for CLIPS Mono &amp; Bilingual Experiments for CLEF</title>
		<author>
			<persName coords=""><forename type="first">Chevallet</forename><surname>Sérasset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,379.44,576.76,145.07,8.96;8,70.92,588.16,24.32,8.96">Cross Language Evaluation Forum CLEF</title>
		<imprint>
			<date type="published" when="2004">2004. 2004. 2004</date>
			<biblScope unit="page" from="17" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,84.13,613.72,379.13,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,126.33,613.72,193.22,8.96">Head/Modifier Frames for Information Retrieval</title>
		<author>
			<persName coords=""><surname>Koster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,330.00,613.72,55.46,8.96">CICLing 2004</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="420" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.61,639.16,442.73,8.96;8,70.92,650.68,328.98,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,227.99,639.16,296.34,8.96;8,70.92,650.68,56.79,8.96">The effect of information retrieval method using dependency relationship between words</title>
		<author>
			<persName coords=""><forename type="first">Takasu</forename><surname>Matsumura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adachi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,142.80,650.68,169.33,8.96">Proceedings of the RIAO 2000 Conference</title>
		<meeting>the RIAO 2000 Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1043" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.61,676.24,442.73,8.96;8,70.92,687.76,328.98,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,227.99,676.24,296.34,8.96;8,70.92,687.76,56.79,8.96">The effect of information retrieval method using dependency relationship between words</title>
		<author>
			<persName coords=""><forename type="first">Takasu</forename><surname>Matsumura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adachi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,142.80,687.76,169.33,8.96">Proceedings of the RIAO 2000 Conference</title>
		<meeting>the RIAO 2000 Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1043" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.61,713.20,442.88,8.96;8,70.92,724.72,453.57,8.96;8,70.92,736.24,70.86,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,169.19,713.20,343.84,8.96">The Constituent Object Parser: Syntactic Structure Matching for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Haas</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,70.92,724.72,448.81,8.96">SIGIR&apos;89, 12th International Conference on Research and Development in Information Retrieval, SIGIR</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,81.61,73.12,442.85,8.96;9,70.92,84.64,107.58,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,134.73,73.12,249.60,8.96">Using NLP or NLP resources for information retrieval tasks</title>
		<author>
			<persName coords=""><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,399.24,73.12,125.22,8.96;9,70.92,84.64,35.07,8.96">Natural Language Information Retrieval</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="99" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,81.61,110.08,442.80,8.96;9,70.92,121.60,169.26,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,238.68,110.08,243.14,8.96">Robustness beyond shallowness: incremental deep parsing</title>
		<author>
			<persName coords=""><forename type="first">Chanod</forename><surname>Ait-Mokhtar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roux</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,494.40,110.08,30.01,8.96;9,70.92,121.60,88.93,8.96">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="page" from="121" to="144" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,81.61,147.04,354.77,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,137.21,147.04,212.74,8.96">Dependence language model for information retrieval</title>
		<author>
			<persName coords=""><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,363.12,147.04,46.23,8.96">SIGIR-2004</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,81.61,172.60,442.88,8.96;9,70.92,184.12,453.69,8.96;9,70.92,195.64,102.06,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,184.65,172.60,329.32,8.96">Capturing term dependencies using a language model based on sentence trees</title>
		<author>
			<persName coords=""><forename type="first">Allan</forename><surname>Nallapati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,70.92,184.12,453.69,8.96;9,70.92,195.64,22.82,8.96">Proceedings of the 2002 ACM CIKM International Conference on Information and Knowledge Management, CIKM</title>
		<meeting>the 2002 ACM CIKM International Conference on Information and Knowledge Management, CIKM</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,86.35,221.08,438.00,8.96;9,70.92,232.60,453.55,8.96;9,70.92,244.12,100.86,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,200.57,221.08,323.79,8.96;9,70.92,232.60,152.74,8.96">Approach to construction of automatic morphological analysis systems for inflective languages with little effort</title>
		<author>
			<persName coords=""><forename type="first">Sidorov</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,235.20,232.60,289.28,8.96;9,70.92,244.12,21.57,8.96">Computational Linguistics and Intelligent Text Processing (CICLing-2003)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="215" to="220" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
