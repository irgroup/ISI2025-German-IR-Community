<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,96.58,148.86,409.84,15.15">Hungarian Monolingual Retrieval at CLEF 2005</title>
				<funder ref="#_S9yFnZD">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_EuEAhAy #_9gSWDr3 #_Zj6gPfp #_nrqgnjK #_5XqqeDq #_QA8WzTM #_JecgGyg #_u8Vmw3u">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,225.85,182.75,55.37,8.74"><forename type="first">Anna</forename><surname>Tordai</surname></persName>
							<email>atordai@science.uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Kruislaan 403</addrLine>
									<postCode>1098 SJ</postCode>
									<settlement>Amsterdam</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.15,182.75,75.99,8.74"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Kruislaan 403</addrLine>
									<postCode>1098 SJ</postCode>
									<settlement>Amsterdam</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,96.58,148.86,409.84,15.15">Hungarian Monolingual Retrieval at CLEF 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">15AB738EB1F906631BA40B0CFCF32FC4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Stemming, Morphological analysis, Hungarian language</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our official runs for the ad hoc monolingual task in Hungarian for CLEF 2005. We conducted experiments with four stemmers of varying impact. The experiments indicate that stemmers focusing on noun inflection are as effective as more broadly oriented stemmers, and that extensive stemming is especially beneficial for Hungarian monolingual retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In our participation in the CLEF ad hoc task this year, we focused exclusively on monolingual retrieval for Hungarian. This is the first year Hungarian is part of CLEF and it is an ideal opportunity to test our work on the effects of stemming in Hungarian. Previous work on languages that are morphologically richer than English, such as Finnish, indicate that there should be benefits from morphological analysis such as stemming, lemmatization, and compound analysis <ref type="bibr" coords="1,476.60,590.68,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="1,490.93,590.68,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="1,502.48,590.68,7.01,8.74" target="#b4">5]</ref>. We have developed a number of suffix-stripping algorithms of varying impact, all focusing on inflectional suffixes. Our goal is to determine the degree of stemming that would prove beneficial for retrieval effectiveness, in terms of both precision and recall. We expect to see improvements for recall for the stemmers, but in addition, we hope that our "light" stemmers keep precision at an acceptable level. The "heavy" stemmer we developed is also expected to improve recall, but it will probably hurt precision.</p><p>The paper is organized as follows. Section 2 describes the traits of the Hungarian language that are important from an information retrieval point of view. Section 3 contains a description algorithmic stemmers along with an evaluation. Section 4 describes the retrieval system we used. Section 5 concerns the official and non-official runs, finally followed by a conclusion.</p><p>Hungarian is an agglutinative language remotely related to Finnish and Estonian, and a member of the Ob-Ugric languages <ref type="bibr" coords="2,209.15,145.80,9.97,8.74" target="#b6">[7]</ref>. The Hungarian language is highly inflectional, rich in compound words, and has an extensive inflectional and derivational morphology. To illustrate this, nouns have 16 to 24 cases depending on the classification system. Additionally, if person, number and possession are added for a single noun there may be as many as 1400 forms <ref type="bibr" coords="2,428.78,181.66,9.97,8.74" target="#b1">[2]</ref>. Adjectives also have case, person, number and possession, as well as degree, pushing the number of forms to around 2700. Verbs have fewer forms, with person, number, tense, transitivity adding up to 59. These numbers merely illustrate the inflectional variety of the language. Additionally, there is an extensive system of derivational suffixes, many of them changing the part of speech of a word.</p><p>Compound words are frequent in Hungarian, presenting an additional challenge for retrieval. Compound nouns can be formed by two nouns and a participle and a noun. Adjectives can also be formed by the combination of a noun and adjective. Compounding was not addressed at this time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Algorithmic Stemmers</head><p>In this section we describe and evaluate the stemmers used in our retrieval experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Description of the Stemmers</head><p>The stemmers were built in the Snowball language <ref type="bibr" coords="2,318.56,378.35,10.52,8.74" target="#b8">[9]</ref> and are rule-based stemmers focusing on inflectional suffixes in Hungarian. Using the Szeged Corpus <ref type="bibr" coords="2,351.16,390.30,9.97,8.74" target="#b0">[1]</ref>, which is a collection of annotated texts ranging from novels, children's essays, legal texts, newspaper articles to computer books, we created a list of the most frequent types of morphosyntactic tags. This helped to determine which suffixes appear most often in the text and guided the construction of the stemmers.</p><p>We developed four types of stemmers:</p><p>• Light1 -handling frequent noun cases, plural and frequent owners.</p><p>• Light2 -handling all noun cases, plural and frequent owners.</p><p>• Medium -handling frequent noun cases, plural, frequent owners and frequent verb tenses.</p><p>• Heavy -handling most inflectional suffixes.</p><p>We will now discuss the stemmers in more detail. The lightest stemmer, Light1, only handles 14 frequent noun cases, plural and the most frequent possessive cases. It is the least invasive stemmer but statistics suggest it might still have a significant impact. Of all the nouns in the Szeged corpus 26% were in uninflected form. The most frequent types of suffixes cover 36% of the nouns. These were the ones targeted by Light1 with the exception of the single letter suffix 'k' indicating plurality. Even without it, at least half of all nouns should be indexed in their stem form. Since adjectives have the same case, number and possession suffixes as nouns they also become stemmed along with numerals which also share a number of cases with nouns.</p><p>The second stemmer, Light2, is similar to Light1 except that it handles 21 noun cases instead of just 14, also removing single letter suffixes such as the accusative 't' and superessive 'n'. The Light1 and Light2 stemmers both take word length into account, making sure the remainder is at least a valid vowel-consonant combination.</p><p>The third stemmer, Medium, removes 12 frequent noun cases, plural, possession and combinations of ownership and plurality. It also handles frequent verb tense-person-number combinations as well as the degree of adjectives. In addition, suffixes forming ordinals and fractions out of numerals were also removed. The last stemmer, Heavy, is the most aggressive, removing 21 noun cases, handling plurality and possession. For verbs it handles infinitive, indicative, conditional and subjunctive moods.</p><p>Unfortunately, there are a number of difficulties for the stemmers such as overstemming and homonymy. As an example of overstemming, the word nemzet, meaning 'nation', is already in stemmed form but the heavy stemmer removes the 'et' suffix since it is a valid accusative ending leaving the invalid nemz as the stem. This problem could be alleviated by expanding the stemmer with the use of an exceptions list containing certain frequent words.</p><p>To illustrate the problem of homonymy, consider the word nevet, which either means the verb 'to laugh' or the noun 'name' in accusative form. For the latter the stemmer ought to remove the 'et' ending and swap 'e' for 'é' to produce név ; for the former it must leave the word untouched. What complicates the decision whether to remove this suffix, is that accusative is the most frequent case in the Szeged corpus after the nominative case. At present the stemmers that remove the accusative case overstem the verb form. It would be interesting to see if a lemmatizer would have an edge over an algorithmic stemmer when it comes to these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluating the Stemming Algorithms</head><p>The stemmers were evaluated both intrinsically and extrinsically. For the intrinsic evaluation, we used Paice's method based on error counting <ref type="bibr" coords="3,287.12,436.34,9.96,8.74" target="#b7">[8]</ref>. According to this method, two values determine the quality of a stemmer: understemming and overstemming. In order to determine these values, a list of words is separated into conceptual groups formed by semantically and morphologically related words. This is the target, and an ideal stemmer should conflate words to these conceptual groups.</p><p>The stemmers were used to stem the word list, and following the Paice method their correspondence to the conceptual groups was measured. This resulted in an understemming (UI) and overstemming measure (OI). By dividing the overstemming index by the understemming measure we get the stemming weight (SW) which is a measure of the strength of the stemmer.</p><p>Paice also offers a different way to combine the two measures (UI and OI) to determine the general relative accuracy of the stemmers. This measure, called error rate relative to truncation, or ERRT, is useful for deciding on the best overall stemmer in cases where one stemmer is better in terms of understemming but worse in terms of overstemming. To calculate the ERRT we created a baseline using length truncation by reducing the words in the world list to their n first letters where n was 9, 10, 11 and 12. The overstemming and understemming measure of these truncated lists defines the truncation line. The values of any reasonable stemmers are found between this line and the origin. Figure <ref type="figure" coords="3,266.57,627.63,4.98,8.74" target="#fig_0">1</ref> shows the UI and OI values for each stemmer with the truncation line. Generally, the further the stemmer is from this line, the better it performs on the word lists. By drawing a line that passes through the origin, the datapoint identified by the pair (UI,OI) consisting of the stemmer's understemming and overstemming index, respectively, and that intersects the truncation line, we obtain the distances necessary to calculate the ERRT value of each stemmer. These are the distance from the origin to the stemmer's (UI,OI) divided by the distance from the origin to the intersection (with the truncation line). Low overstemming and understemming indexes are the desired feature in a stemmer. Stemmers that are closer to the origin have lower UI and OI values which means the distance is also shorter. The 'best' stemmer would also have the lowest ERRT value compared to the rest. Table <ref type="table" coords="4,133.02,421.74,4.98,8.74" target="#tab_0">1</ref> contains the UI, OI, SW and ERRT values for each of the four stemmers used. As expected, Light1, being the lightest stemmer, has the highest understemming index, while Heavy has the lowest value. The high value for understemming for Light1 indicates that it leaves many words unstemmed or just understemmed. The reverse is true for the overstemming index. The Medium stemmer has a lower understemming and higher overstemming index than Light2 which, at first sight, seems surprising. However, 54% of the words in the list are nouns, and since Light2 removes all noun cases just like the Heavy stemmer, but unlike Light1 and Medium, these scores make sense. The Medium stemmer focuses on some frequent noun cases and verbs. Verbs only form 23% of the word list so the reason for the somewhat unexpected values is simply due to the fact that the Medium stemmer stems fewer words than Light2. Overall, when it comes to stemming a word list, a stemmer handling all noun cases yields better results than one restricted to the most frequent noun cases and verb tenses. We suspect that this will apply to a lesser extent for retrieval as words are unique in the word list unlike in a normal corpus.</p><p>The high ERRT value of Light1 indicates that although it has very low overstemming it leaves too many words understemmed making it too light. The same is true for the Medium stemmer, which loses out because it focuses on verbs even though there are fewer verbs than nouns in the word list. In this sense Light2 and Heavy come out as winners having the lowest ERRT values. What would this mean when used in an information retrieval setting? An analysis of English topics used in CLEF 2004 showed that after stopping over 65% of the words were nouns, only 10% verbs and 12% adjectives. A post submission analysis confirmed these findings for the 2005 Hungarian topics, with 60% of nouns, 23% adjectives and 17% verbs after stopping. Thus, even if a stemmer only concentrates on stemming nouns it should still have an impact on recall if not precision. Based on the ERRT values we expect the runs with Light2 and Heavy stemmers to yield a better recall than the other two stemmers and the baseline (no stemming at all). At the same time, precision will probably be negatively affected by the Heavy stemmer. These results suggest that the run with Light2 should have the highest recall and precision values since it has a low understemming ratio and should still stem a large percentage of words. Let's see. Now that we have described the stemmers that we have developed, we turn to our retrieval experiments and submissions. First, we used Lucene (off-the-shelf) for indexing and retrieval with a standard vector space model <ref type="bibr" coords="5,226.04,157.75,9.97,8.74" target="#b5">[6]</ref>.</p><p>In addition, we used a stopword list which was created using the Szeged Corpus <ref type="bibr" coords="5,447.51,169.71,9.96,8.74" target="#b0">[1]</ref>. We created a list from the 300 most frequent words in the corpus. Numbers and homonyms were removed for the list and it was expanded with pronouns. The result was a list of 188 words. 1 Both the index and queries were stopped. Diacritics were left untouched.</p><p>The Hungarian document collection for CLEF 2005 consists of a collection of the newspaper Magyar Hírlap from 2002. The document collection was encoded in UTF-8. As the Snowball stemmers were created for ISO Latin encoding the entire collection was converted into ISO Latin 1 encoding without any loss of textual data. For each document the title, lead and description fields were allowed to be used for retrieval; they were all indexed.</p><p>There were 50 queries and we used both the title (T) and description (D) fields for retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CLEF 200Experiments</head><p>In this section describe the results of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Runs</head><p>We submitted four official runs for the monolingual Hungarian ad-hoc task, one for each of the four stemmers we developed:</p><p>• Light1 (run id: UAmsMoHu4AnV)</p><p>• Light2 (run id: UAmsMoHu3AnL)</p><p>• Medium (run id: UAmsMoHu2AnG)</p><p>• Heavy (run id: UAmsMoHu1AnH)</p><p>We conducted several post-submission experiments once the assessments and the results of the submitted runs had been made available:</p><formula xml:id="formula_0" coords="5,104.94,521.81,63.08,48.59">• Base • Base + stop • 6-grams</formula><p>Some of these additional experiments serve as baseline runs to assess the overall impact of the Snowball stemmers and of stopping. In addition we ran experiments using character n-grams. The run with 6-grams was not stopped; for this run the corpus was indexed with the original word and its 6-grams. The 6-gram performed better than 7-grams and 8-grams, which are not discussed in here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Retrieval Results</head><p>The Mean Average Precision (MAP) scores in Table <ref type="table" coords="5,322.02,675.68,4.98,8.74" target="#tab_1">2</ref> show that of the stemmed runs the Heavy stemmer performed best, closely followed by Light2, while Medium and Light1 perform worse. This confirms the results of the ERRT values in Section 4 and suggests that extensively stemming nouns yields good results and even more extensive stemming improves precision. It is worth noting that the MAP score of the 6-Gram run is only slightly below the score of Light2. In fact,  we see that using 6-grams on this corpus is almost as good as using stemming. When looking at the Interpolated Recall vs Average Precision (Figure <ref type="figure" coords="6,330.39,569.30,4.43,8.74" target="#fig_1">2</ref>) we see that the Light2 run has almost exactly the same values as the Heavy run. When it comes to R-precision scores Light2 has the best scores beating the Heavy run.</p><p>Interestingly, the 6-Gram run retrieves the largest number of relevant documents but does not rank them sufficiently high, as the MAP scores are not the best ones. Judging from the combination of number of retrieved relevant documents, R-precision and MAP scores, Light2 ranks the relevant documents the highest even though it does not retrieve as many as the 6-gram or Heavy runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>The retrieval performance of the stemmed runs follows our expectations from Section 4. Of the four stemmers, the Heavy and Light2 stemmers performed best, followed by Medium and Light. The Light2 stemmer shows the importance of stemming nouns when it comes to retrieval in Hungarian. The Heavy stemmer indicates that even extensive stemming does not lower precision like it is known to do for English retrieval.</p><p>We will now look at the performance of the stemmed runs on certain topics. As in general, the Heavy and Light2 stemmers performed much better per topic than the other two stemmers did. We will compare the stronger group to the weaker group in order to determine why there was such a difference in performance as well as why the stemmers performed below the median. Topic 289 is an example where all of the runs were below the median. In this document the task is to retrieve documents about the Falkland islands. &lt;num&gt; C289 &lt;/num&gt; &lt;HU-title&gt; Falkland-szigetek &lt;/HU-title&gt; &lt;HU-desc&gt; Keressünk a Falkland-szigetekről szóló cikkeket.&lt;/HU-desc&gt; Some of the relevant documents were not retrieved. One of the reasons for this was that the term Falkland-szigetek was indexed as a single word. Hyphenated words are frequent in Hungarian for dates, acronyms and when foreign words or brand names become inflected (e.g., for NATO becomes NATO-nak ). One of the relevant documents contained the words Falkland-szigetek ("Falkland islands") in separate form while another contained the term Falkland-háború ("Falkland wars") and these were not retrieved.</p><p>Another document was not found because it contained the adjective form Falkland-szigeti meaning "from the Falkland islands." This is a derivative suffix and, as mentioned earlier, derivative suffixes are not removed by our stemmers. However, this type of suffix is so frequent it will be removed in future versions of the stemmer.</p><p>One of the topics where the Heavy and Light2 runs did much better (noticeably higher than the median) than the weaker ones was Topic 259. Documents relevant for this topic contain information about movies that have been awarded the Golden Bear at the Berlin film festival. &lt;num&gt; C259 &lt;/num&gt; &lt;HU-title&gt; Aranymedve &lt;/HU-title&gt; &lt;HU-desc&gt; Mely filmek kaptak Aranymedve díjat a berlini filmfesztiválon? &lt;/HU-desc&gt; While all four relevant documents were retrieved in each run it was their ranking that resulted in different precision scores. The Heavy and Light2 stemmers correctly stemmed one form of the word "Aranymedve" in one of the relevant documents as well as the word "filmfesztivál". This boosted the ranking of these documents. The word "film" was correctly stemmed by all stemmers in all the documents but as this word appears frequently in irrelevant document as well it resulted in lower rankings for the Medium and Light1 runs. It is worth noting that the word filmfesztiválon is a compound of film and fesztiválon. Although the word film was in the query, we believe that decompounding the word fesztiválon would have helped to would have further boosted the ranking. Now that we know the form of the topics we will also be able to adjust the stopword list so as to include the words keressünk (search) and cikkeket (articles).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The experiments on which we report in this paper confirm that stemming in Hungarian greatly improves retrieval effectiveness. They show that a stemmer focusing merely on the inflection of nouns works almost as well as a more broadly oriented stemmer. Merely stemming frequent noun and verb inflections however yields worse results than using 6-grams. Our results are sobering as a 6-grammed run performed almost as well as the best performing stemmed run.</p><p>Our stemmers themselves can be improved upon and hyphenated words will have to be addressed differently in the future. A detailed error analysis has shown that decompounding will probably boost rankings and help retrieve additional documents. Analyzing the impact of decompounding on Hungarian monolingual retrieval is left as future work, though.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,192.81,389.92,217.38,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: UI × OI plot with the ERRT distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,122.21,525.52,358.58,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Interpolated Recall vs. Average Precision for official and unofficial runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,174.12,110.82,254.76,88.78"><head>Table 1 :</head><label>1</label><figDesc>Performance of the stemmers on the word-groups</figDesc><table coords="3,194.50,110.82,213.99,56.96"><row><cell></cell><cell>UI</cell><cell>OI</cell><cell>SW</cell><cell>ERRT</cell></row><row><cell>Light1</cell><cell cols="3">0.75 0.0000028 0.0000037</cell><cell>0.81</cell></row><row><cell>Light2</cell><cell cols="3">0.59 0.0000053 0.0000089</cell><cell>0.66</cell></row><row><cell cols="4">Medium 0.64 0.0000081 0.0000127</cell><cell>0.73</cell></row><row><cell>Heavy</cell><cell cols="3">0.53 0.0000134 0.000025</cell><cell>0.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,101.09,740.98,284.69,8.66"><head>Table 2 :</head><label>2</label><figDesc>Overview of MAP scores and R-precision scores for the official and unofficial runs. Best scores are in bold face.</figDesc><table coords="6,171.22,110.82,260.56,93.22"><row><cell></cell><cell>MAP</cell><cell cols="2">R-prec Relevant Docs Retrieved</cell></row><row><cell>Light1</cell><cell>0.2150</cell><cell>0.2416</cell><cell>700</cell></row><row><cell>Light2</cell><cell cols="2">0.2799 0.2905</cell><cell>734</cell></row><row><cell>Medium</cell><cell>0.2330</cell><cell>0.2556</cell><cell>717</cell></row><row><cell>Heavy</cell><cell cols="2">0.2819 0.2839</cell><cell>740</cell></row><row><cell>Base</cell><cell>0.1831</cell><cell>0.2096</cell><cell>591</cell></row><row><cell cols="2">Base + stop 0.1836</cell><cell>0.2014</cell><cell>607</cell></row><row><cell>6-Gram</cell><cell>0.2787</cell><cell>0.2903</cell><cell>747</cell></row></table><note coords="5,101.09,740.98,284.69,8.66"><p><p>1 </p>The stopword list is available at http://ilps.science.uva.nl/Resources/.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p><rs type="person">Maarten de Rijke</rs> was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">264-70-050</rs>, <rs type="grantNumber">354-20-005</rs>, <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">612.000.207</rs>, <rs type="grantNumber">612.066.302</rs>, <rs type="grantNumber">612.069.006</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_S9yFnZD">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_EuEAhAy">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_9gSWDr3">
					<idno type="grant-number">264-70-050</idno>
				</org>
				<org type="funding" xml:id="_Zj6gPfp">
					<idno type="grant-number">354-20-005</idno>
				</org>
				<org type="funding" xml:id="_nrqgnjK">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_5XqqeDq">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_QA8WzTM">
					<idno type="grant-number">612.000.207</idno>
				</org>
				<org type="funding" xml:id="_JecgGyg">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_u8Vmw3u">
					<idno type="grant-number">612.069.006</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,105.50,212.52,404.17,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,105.50,212.52,373.72,8.74">Szeged Corpus. A morpho-syntactically annotated and POS tagged Hungarian corpus</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,232.44,407.51,8.74;8,105.50,244.40,283.00,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,248.34,232.44,211.81,8.74">Specifications and notation for lexicon encoding</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Erjavec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Monachini</surname></persName>
		</author>
		<idno>MULTEXT -East</idno>
		<imprint>
			<date type="published" when="1997-12-17">December 17, 1997</date>
		</imprint>
	</monogr>
	<note type="report_type">COP Project 106</note>
</biblStruct>

<biblStruct coords="8,105.50,264.32,407.50,8.74;8,105.50,276.28,407.51,8.74;8,105.50,288.23,216.73,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,495.85,264.32,17.15,8.74;8,105.50,276.28,154.06,8.74">The University of Amsterdam at CLEF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fissaha Adafre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Van Hage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lacerda De Melo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,447.49,276.28,65.52,8.74;8,105.50,288.23,127.44,8.74">Working Notes for the CLEF 2004 Workshop</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Borri</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,308.16,407.51,8.74;8,105.50,320.11,208.13,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,316.93,308.16,196.07,8.74;8,105.50,320.11,40.40,8.74">Monolingual document retrieval for European languages</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hollink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,154.80,320.11,90.22,8.74">Information retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="33" to="52" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,340.04,407.50,8.74;8,105.50,351.99,407.50,8.74;8,105.50,363.95,273.51,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,354.89,340.04,158.10,8.74;8,105.50,351.99,158.60,8.74">Stemming and lemmatization in the clustering of finnish text documents</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Korenius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Laurikkala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jarvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Juhola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,290.35,351.99,222.64,8.74;8,105.50,363.95,174.13,8.74">Proceedings of the Thirteenth ACM conference on Information and knowledge management</title>
		<meeting>the Thirteenth ACM conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="625" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,383.88,359.74,9.02" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><surname>Lucene</surname></persName>
		</author>
		<ptr target="http://jakarta.apache.org/lucene/" />
		<title level="m" coord="8,143.26,383.88,110.72,8.74">The Lucene search engine</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,403.80,407.50,9.02;8,105.50,416.47,18.46,8.30" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,161.00,403.80,106.17,8.74">The Hungarian language</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Megyesi</surname></persName>
		</author>
		<ptr target="http://www.speech.kth.se/~bea/hungarian.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,435.68,407.51,8.74;8,105.50,447.64,310.51,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,160.33,435.68,310.02,8.74">Method for evaluation of stemming algorithms based on error counting</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Paice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,479.89,435.68,33.12,8.74;8,105.50,447.64,213.72,8.74">Journal of The American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="632" to="649" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,467.56,407.50,9.02;8,105.50,479.52,22.70,8.74" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,150.76,467.56,173.23,8.74">The Snowball string processing language</title>
		<author>
			<persName coords=""><surname>Snowball</surname></persName>
		</author>
		<ptr target="http://snowball.tartarus.org/" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
