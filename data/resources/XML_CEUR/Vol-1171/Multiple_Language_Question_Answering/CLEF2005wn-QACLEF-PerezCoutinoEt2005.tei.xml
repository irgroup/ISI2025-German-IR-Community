<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.96,73.87,311.46,12.90;1,192.96,89.83,209.41,12.90">Experiments for tuning the values of lexical features in Question Answering for Spanish</title>
				<funder>
					<orgName type="full">Human Language Technologies Laboratory of INAOE</orgName>
				</funder>
				<funder ref="#_3SpZu4e">
					<orgName type="full">CONACYT</orgName>
				</funder>
				<funder ref="#_VEhGUFp">
					<orgName type="full">SNI-Mexico</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.28,119.26,94.87,9.15"><forename type="first">Manuel</forename><surname>Pérez-Coutiño</surname></persName>
						</author>
						<author>
							<persName coords="1,293.16,119.26,105.92,9.15"><forename type="first">Manuel</forename><surname>Montes-Y-Gómez</surname></persName>
						</author>
						<author>
							<persName coords="1,192.96,130.78,90.10,9.15"><forename type="first">Aurelio</forename><surname>López-López</surname></persName>
						</author>
						<author>
							<persName coords="1,304.32,130.78,97.98,9.15"><forename type="first">Luis</forename><surname>Villaseñor-Pineda</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Óptica y Electrónica (INAOE)</orgName>
								<orgName type="institution">Instituto Nacional de Astrofísica</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Luis Enrique Erro No</orgName>
								<address>
									<postCode>CP, 72840</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Sta. Ma. Tonantzintla</orgName>
								<address>
									<settlement>Pue</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.96,73.87,311.46,12.90;1,192.96,89.83,209.41,12.90">Experiments for tuning the values of lexical features in Question Answering for Spanish</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">88893B1E993F3718B97C0C7CCDAE8C5D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering for Spanish</term>
					<term>Lexical Context</term>
					<term>Natural Language Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the prototype developed by the Language Technologies Laboratory at INAOE for Spanish monolingual QA evaluation task at CLEF 2005. Our approach is centered in the use of lexical features in order to identify possible answers to factual questions. Such method is supported by an alternative one based on pattern recognition in order to identify candidate answers to definition questions. The methods applied at different stages of the system and prototype architecture for question answering are described. The paper shows and discusses the results achieved with this approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Current information requirements claim for efficient mechanisms capable of interact with users in a more natural way. Question answering systems has been proposed as a feasible option for the creation of such mechanisms and the research in this field presents a continue growing both in interest as well as in complexity <ref type="bibr" coords="1,465.00,331.93,10.69,11.04" target="#b2">[3]</ref>. This paper presents the prototype developed at the Language Technologies laboratory of INAOE for the Spanish monolingual QA evaluation task at CLEF 2005. The experiments performed this year by our group continue with its last year work <ref type="bibr" coords="1,114.36,366.49,11.72,11.04" target="#b4">[5]</ref> in the following points, a) the approach is centered in the analysis of the lexical context related to each named entity selected as candidate answer; b) the information used to discriminate candidate and final answers relies on a shallow NLP processing (POS and named entities tagging) and statistical factors. On the other hand, there are some important modifications in the prototype architecture that allow the system to have a better performance (recall) at the initial stages. At the same time there have been some simplifications in the general architecture. For instance, we have made a shallow question classification process; and the answer discrimination process relies only on the information located in the target documents. Thus discarding the internet searching and extracting modules.</p><p>The paper is focused in the discussion of the processes involved in factual question answering. Nevertheless it is presented a section with the description of the methods used for answer definition questions. The rest of this paper is organized as follows; section two describes the architecture of the prototype; section three to section six details the internal processes of the system; section seven discusses the results achieved by the system; and finally section eight exposes our conclusions and discusses further work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prototype Architecture</head><p>The system is based on our last year methodology <ref type="bibr" coords="1,278.52,547.81,11.72,11.04" target="#b4">[5]</ref> but with some significant modifications in the prototype. Figure <ref type="figure" coords="1,100.08,559.21,4.98,11.04" target="#fig_0">1</ref> shows the main blocks of the system. It could be observed that the treatment of factoid and definition questions occurs independently. Factoid questions resolution relies on a hybrid system involving the following stages: question processing, which includes the extraction of named entities and lexical context in the question, as well as question classification to define the semantic class of the answer expected to respond to a given question; documents processing, where the preprocessing of the supporting document collection is done in parallel by a passage retrieval system (PRS)and a shallow NLP (similar to the question processing); searching, where a set of candidate answers is obtained from the modeled passages retrieved by the PRS; and finally answer extraction, where candidate answers are weighted and ranked in order to produce the final answer recommendation of the system. On the other hand, definition questions are treated directly with a methodology supported by a couple of lexical patterns that allow finding and selecting the set of possible answers. Next sections describe each of these stages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Question Processing</head><p>QA systems traditionally perform a question processing stage in order to know in advance the type (semantic class) of the answer expected by a given question and thus, reduce the searching space to only those information fragments related to the semantic class found. Our prototype implements this stage following a direct approach involving the next steps: 1. Question is parsed with a set of heuristic rules in order to get its semantic class. 2. Question is tagged with the MACO POS tagger <ref type="bibr" coords="2,299.64,483.37,11.72,11.04" target="#b0">[1]</ref> 3. Question's named entities are identified and classified using MACO. The first step is responsible of identify the semantic class of the expected answer. In the experiments performed with the training data set, we found that when the number of classes was minimal (just 3 classes: date, quantity and proper noun) it was possible to achieve similar results in precision to those achieved when we use more than five classes, for instance person, organization, location, date, quantity and other. Steps 2 and 3 produce information that is used later in searching stage to match questions and candidate answer context, contributing to the weighted schema.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Documents Processing</head><p>This year we experiment with a hybrid<ref type="foot" coords="2,229.44,605.78,3.00,6.51" target="#foot_0">1</ref> approach for document processing that has allowed simplifying greatly this stage. The processing of target documents is composed of two parts, first the whole document collection is tagged with MACO <ref type="bibr" coords="2,142.49,628.09,16.59,11.04" target="#b0">[1]</ref>, gathering the POS tags as well as named entities identification and classification for each document in the collection. The second part of this stage is performed by the JIRS <ref type="bibr" coords="2,411.12,639.61,11.72,11.04" target="#b1">[2]</ref> passage retrieval system (PRS), which create the index for the searching process. The index gathered by JIRS and the tagged collection are aligned phrase by phrase for each document in the collection. This way, the system could retrieve later the relevant passages for a given question with JIRS, and then use their tagged form for the answer extraction process.</p><p>Due to the document processing stage, searching stage is also performed in two steps. As we mention, the first step is to retrieve the relevant passages for the given question. This step is performed by JIRS, taking as input the question without previous processing.</p><p>JIRS is a PSR specially suited for question answering systems. JIRS ranks the retrieved passages based on the computation of a weight for each passage. The weight of a passage is related to the lager n-gram structure of the question that can be found in the passage itself. The larger the n-gram structure, the greater the weight of the passage. The next example illustrates this concept.</p><p>Assume that the user question is "Who is the president of Mexico?" and that two passages were obtained with the following texts: "Vicente Fox is the president of Mexico…" (p 1 ) and "The president of Spain visited Mexico in last February…" (p 2 ).</p><p>The original question is divided into five sets of n-grams (5 is the number of question terms without the question word Who) the following sets are gathered:</p><p>5-gram: ''is the President of Mexico''.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4-gram:</head><p>''is the President of'', ''the President of Mexico''.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3-gram:</head><p>''is the President'', ''the President of'', ''President of Mexico''.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-gram:</head><p>''is the'', ''the President'', ''President of'', ''of Mexico''. 1-gram: ''is'', ''the'', ''President'', ''of'', ''Mexico''.</p><p>Next, the five sets of n-grams from the two passages are gathered. The passage p 1 contains all the n-grams of the question (the one 5-gram, the two 4-grams, the three 3-grams, the four 2-grams and the five 1-grams of the question). Therefore the similarity of the question with this passage is 1.</p><p>The sets of n-grams of the passage p 2 contain only the "the President of" 3-gram, the "the President"' and "President of" 2-grams and the following 1-grams: "the", "President", "of" and "Mexico". The similarity for this passage give us a lower value than for p 1 because the second passage is very different with respect to the original question, although it contains all the relevant terms of the question.</p><p>Previous evaluation of JIRS also demonstrates that it is possible to achieve coverage of over 60% for the first 20 passages. That is, the possible answer to a given question is found between the first 20 passages retrieved by JIRS for over 60% of the training set. We refer the reader to <ref type="bibr" coords="3,314.40,395.77,11.72,11.04" target="#b1">[2]</ref> in order to get a complete discussion of the similarity metrics used by JIRS and its evaluation.</p><p>Once the relevant passages are selected, the second step requires the POS tagged form of each passage in order to gather the representation used to extract the answer. Due to some technical constraints we were unable to finish the implementation for the alignment of the tagged collection and the JIRS index before test set release. Therefore the tagging of relevant passages was performed online with the disadvantage of a couple of extra hours for such processing.</p><p>Tagged passages are represented in the same way that in <ref type="bibr" coords="3,315.72,476.29,11.84,11.04" target="#b3">[4]</ref> where each retrieved passage is modeled by the system as a factual text object whose content refers to several named entities even when it is focused on a central topic. As mentioned, named entities could be one of these: persons, organizations, locations, dates, quantities and miscellaneous <ref type="foot" coords="3,127.56,511.38,3.00,6.65" target="#foot_1">2</ref> . The model assumes that the named entities are strongly related to their lexical context, especially to nouns (subjects) and verbs (actions). Thus, a passage can be seen as a set of entities and their lexical context. Such representation is used later in order to match question's representation with the best set of candidates gathered from passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Answer Extraction</head><p>Answer extraction is performed according to the type of question, factual or definition. Next subsections detail the processes involved in answering each one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Answering Factoid Questions</head><p>The system makes no difference between factual and temporal restricted factual questions in order to extract their possible answer. Given the set of retrieved passages and their representations (named entities and their contexts) the system computes a weight for each candidate answer (named entity) based on two main factors: a) the activation and deactivation of some features at different steps of the system, and b) the coefficient gather by the formula 1.</p><p>The features listed in table 1 allow us to configure the system in order to change its behavior, for instance, deactivate the question classification step, allowing to the final answer selection to rely on no more information that just statistical computations. The opposite case could be, deactivate frequency features and let the final answer selection to rely on the matching between question and candidate answers context.</p><formula xml:id="formula_0" coords="4,104.40,111.13,384.78,57.03">              - - + + ∩ + ∩ * = 1 1 ) ( ) ( k P P F P F C C C NE NE NE n t i A i A q A q q A q q A ω i=1..k; k=number of passages retrieved by JIRS Formula 1</formula><p>Where A ω is the assigned weight for a candidate answer; q t is 1 if the semantic class of the candidate answer is the same that the question's one and 0 in other case; n is a normalization factor based on the number of activated features, q NE is the set of named entities in the question and Once the system computes the weight for all candidate answers, these are ranked by decreasing sort order, taking as answer the one with the greatest weight. Section 7 describes some experiments performed with training data set and the results achieved with both, training and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Answering Definitions</head><p>Our system uses an alternative method to answer definition questions. This method makes use of some regularities of language and some stylistic conventions of news letters to capture the possible answer for a given definition question. A similar approach was presented in <ref type="bibr" coords="4,276.12,609.49,10.90,11.04" target="#b5">[6,</ref><ref type="bibr" coords="4,287.02,609.49,7.27,11.04" target="#b6">7]</ref>.</p><p>The process of answering a definition question considers to main tasks. First, the definition extraction, which detects the text segments that contains the description or meaning of a term (in particular those related with the name of a person or an organization). Then, the definition selection, where the most relevant description of a given question term is identified and the final answer of the system is generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Definition Extraction</head><p>The language regularities and the stylistic conventions of news letters are captured by two basic lexical patterns. These patterns allow constructing two different definition catalogs. The first one includes a list of pairs of acronym-meaning. The other one consists of a list of referent-description couples.</p><p>In order to extract the acronym-meaning pairs we use an extraction pattern based on the use of parentheses.</p><formula xml:id="formula_1" coords="5,240.00,77.05,275.36,11.84">w 1 &lt;meaning&gt; ( &lt;acronym&gt; ) (i)</formula><p>In this pattern, w 1 is a lowercase non stopword, &lt;meaning&gt; is a sequence of words starting with an uppercase letter (that can also include some stopwords), and &lt;acronym&gt; indicates a single word also starting with an uppercase letter.</p><p>By means of this pattern we could identify pairs like [PARM -Partido Auténtico de la Revolución Mexicana]. In particular this pair was catch from the following paragraph: "El Partido Auténtico de la Revolución Mexicana (PARM) nombró hoy, sábado, a Álvaro Pérez Treviño candidato presidencial de ese organismo para las elecciones federales del 21 de agosto de 1994".</p><p>In contrast, the extraction of referent-description pairs is guided by the occurrence of a special kind of appositive phrases. This information was encapsulated in the following extraction pattern.</p><formula xml:id="formula_2" coords="5,230.52,227.53,287.60,11.84">w 1 w 2 &lt;description&gt; , &lt;referent&gt; ,<label>(ii)</label></formula><p>Where w 1 may represent any word, except for a preposition, w 2 is a determiner, &lt;description&gt; is a free sequence of words, and &lt;referent&gt; indicates a sequence of words starting with an uppercase letter or being in the stopwords list.</p><p>Applying this extraction pattern over the below paragraph we could find the pair [Alain Lombard -El director de la Orquesta Nacional de Burdeos].</p><p>"El director de la Orquesta Nacional de Burdeos, Alain Lombard, ha sido despedido por el Ayuntamiento de esta ciudad, que preside Alain Juppé, tras un informe que denuncia malos funcionamientos y gastos excesivos".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Definition Selection</head><p>The main quality of the extraction patterns is their generality. However, this generality causes the patterns to often extract non relevant information, i.e., information that does not indicate a relation acronym-meaning or concept-description. For instance, when using the extraction pattern (i) to analyze the following news we obtain the incorrect definition pair [Ernie -AFS]. In this case the resultant pair does not express an acronym-meaning relation; instead it indicates a person-nationality association.</p><p>Ernie Els (AFS) se mantiene en cabeza de la lista de ganancias de la "Orden de Mérito" de golf, con más de 17 millones de pesetas, mientras que el primer español es Miguel Angel Martín, situado en el puesto decimoséptimo, con 4.696.020.</p><p>Given that the catalogs contains a mixture of correct and incorrect relation pairs, it is necessary to do an additional process in order to select the most probable answer for a given definition question. The proposed approach is supported on the idea that, on the one hand, the correct information is more abundant than the incorrect one, and on the other hand, that the correct information is redundant.</p><p>Thus, the process of definition selection considers the following two criteria:</p><p>1. The more frequent definition in the catalog has the highest probability to be the correct answer.</p><p>2. The largest and therefore more specific definitions tend to be the more pertinent answers.</p><p>The following example illustrates the process. Assume that user question is "who is Félix Ormazabal?", and that the definition catalog contains the records showed below. Then, the method selects the description "diputado general de Alava" as the most probable answer. This decision is based on the fact that this answer is the more frequent description related to Félix Ormazabal in the catalog.</p><p>Félix Ormazabal: Joseba Egibar: Félix Ormazabal: candidato alavés: Félix Ormazabal: diputación de este territorio: Félix Ormazabal: presidente del PNV de Alava y candidato a diputado general: Félix Ormazabal: nuevo diputado general Félix Ormazabal: diputado Foral de Alava Félix Ormazabal: través de su presidente en Alava Félix Ormazaba : diputado general de Alava Félix Ormazabal: diputado general de Alava Félix Ormazabal: diputado general de Alava This section discusses some training experiments and the decision criteria used to select the configuration of the experiments evaluated at QA@CLEF2005 monolingual track for Spanish. Given that we have used the same modules for answering definitions in all our runs for monolingual QA, including those described in "INAOE-UPV Joint Participation at CLEF 2005: Experiments in Monolingual Question Answering", the discussion on these results has been documented in that paper. Thus the rest of this document is intended to discus the results on factual question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Training Experiments</head><p>As we mention earlier, the approach used in our system is similar to the one used last year <ref type="bibr" coords="6,450.24,189.37,10.69,11.04" target="#b4">[5]</ref>, an analysis of such system show us that it was necessary to experiment with different values for the parameters involved in the answer extraction stage (see table <ref type="table" coords="6,210.72,212.29,3.63,11.04" target="#tab_0">1</ref>). For instance, last year the system relied in a document model considering only four elements (just nouns and/or verbs) at left and right for the named entities context. This year we performed several experiments using context lengths from four elements to the whole passage retrieved, we also experiment with different elements: nouns, proper nouns, verbs, adjectives and adverbs. Table <ref type="table" coords="6,465.84,246.85,4.98,11.04" target="#tab_1">2</ref> shows some configurations tested with the training set. Then, Figure <ref type="figure" coords="6,297.96,258.37,4.98,11.04" target="#fig_1">2</ref> shows the results achieved with the training set applying the configurations showed in table 2. Notice that these results correspond to the factual question answering. Figure <ref type="figure" coords="6,111.48,429.61,4.98,11.04" target="#fig_1">2</ref> shows that the best performance was reached with the "Exp. 7" which combines the following feature values, first the system classify the question as one of the following classes: Date, Question, Proper Noun (which includes person, organizations and locations); next the system retrieves the relevant passages with (length=1 phrase) and makes the proper representation for each named entity found in it. At this step, the context is formed by 8 elements at left and/or right of the named entity, and considers verbs, common names, named entities and adjectives. The extraction stage filters those candidates answers whose context does not contain at least one of the question's named entity, and finally computes the weight for each candidates according to formula 1 (see table 2 for exp. 7 configuration).</p><p>Another interesting experiment was the analysis of the questions answered by this methodology. We estimate that the "union" of the results gathered with the configurations showed in table 2 could reach over 24% if the best configuration was selected online, i.e., for each question select the best configuration of the system which could return an accurate answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Evaluation</head><p>We participate in the evaluation with two runs, both were gathered using the same configuration of experiment 7 (see table <ref type="table" coords="6,112.68,608.53,3.63,11.04" target="#tab_1">2</ref>). The first one inao051eses analyzes the first 800 passages retrieved by JIRS, while our second run inao052eses analyzes only the first 100 passages retrieved by JIRS. Table <ref type="table" coords="6,368.28,620.05,4.98,11.04" target="#tab_2">3</ref> shows the results of the evaluation.</p><p>Despite the fact that our results (for factual questions) were over 10% better than last year, we believe that the approach described is near to its limits of accuracy. A shallow analysis of the results shows that the proposed system is suited for questions with some stylistic characteristics whose answer is commonly found in the near context of some reformulation of the question into the passages. While for others, more elaborated factual questions is unable to identify the right answer. That is the case of questions whose expected answer is an object or some abstract entity which can not be identified a priori by a shallow NLP or without a knowledge base.</p><p>Another point to note is that in some cases, the statistical factor given by the frequency of occurrence of a candidate answer becomes a secondary aspect that could yield to a wrong selection of an answer.</p><p>A detailed analysis of these results will help us to take the next direction in our research. We have begun some experiments in order to get the right configuration for each question online, that is, to select automatically the appropriate configuration for a given question based on question's attributes. Another direction in our research is to include more features that allow us to perform a better selection and discrimination of candidate answers, more over, that allow to consider objects and abstract entities that are currently excluded by the methodology. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>This work has presented an approach for QA in Spanish centered in the use of lexical features for factual questions resolution that is complemented with a pattern matching approach for definition question resolution. The results achieved in the monolingual track for Spanish have improved our last year performance by over 10% on factual questions and over 30% on definition questions. It is important to note that the approach was able to answer over 30% of temporal restricted factual questions without additions or modifications to the proposed approach.</p><p>After a shallow analysis of these results we have begun to work in two directions: first the inclusion of other features that allow us to respond questions whose answer is not necessarily expressed as a reformulation of the question into the target documents. Currently our work in this direction is based on the study of a syntactic ana- lysis of the retrieved passages, and in the inclusion of external knowledge. The second direction of research is the automatic selection of features online in order to get the best performance of the system given a question.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,125.52,363.03,355.77,9.97;2,88.08,373.35,430.44,9.97;2,107.16,383.79,381.09,9.97;2,97.68,89.52,397.08,264.84"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Block diagram of the system. Factoid and definition questions are treated independently. Factual questions require the following stages: question processing, documents processing, searching and answer selection. Definition questions use a pattern approach for definition extraction and definition selection process.</figDesc><graphic coords="2,97.68,89.52,397.08,264.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,132.84,351.15,329.61,9.97"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Results achieved with training set, applying the configurations showed in table 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,70.92,215.66,453.63,294.50"><head>Table 1 .</head><label>1</label><figDesc>Features list used in factoid question answering.</figDesc><table coords="4,348.36,215.66,176.12,15.00"><row><cell>A NE is the set of named entities in the con-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,87.12,293.31,419.44,123.91"><head>Table 2 .</head><label>2</label><figDesc>Configurations of some experiments performed with training set. First column corresponds to the feature list showed in table 1. NC,NE V,NC,NE V,NC,NE V,NC,NE V,NC,NE V,NC,NE,QA V,NC,NE,QA V,NC,NE,QA V,NC,NE,QA</figDesc><table coords="6,87.12,322.12,409.71,95.09"><row><cell cols="2">Exp. 1</cell><cell>Exp. 2</cell><cell>Exp. 3</cell><cell>Exp. 4</cell><cell>Exp. 5</cell><cell>Exp. 6</cell><cell>Exp. 7</cell><cell>Exp. 8</cell><cell>Exp. 9</cell></row><row><cell>1</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell></row><row><cell>2</cell><cell>0</cell><cell cols="2">D,Q,NP D,Q,P,O,G</cell><cell>0</cell><cell>D,Q,NP</cell><cell>0</cell><cell>D,Q,NP</cell><cell>0</cell><cell>D,Q,NP</cell></row><row><cell>3 V,4</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>8</cell><cell>8</cell><cell>Pasaje</cell><cell>Passage</cell></row><row><cell>5</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>6</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>7</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>8</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>9</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,91.56,441.03,412.22,160.90"><head>Table 3 .</head><label>3</label><figDesc>Results of submitted runs.</figDesc><table coords="7,91.56,460.90,412.22,141.03"><row><cell>Run</cell><cell>inao051eses</cell><cell>inao052eses</cell></row><row><cell>Right</cell><cell>84 (34F + 40D + 10 TRF)</cell><cell>79 (32F + 40D + 7 TRF)</cell></row><row><cell>Wrong</cell><cell>110</cell><cell>116</cell></row><row><cell>ineXact</cell><cell>5</cell><cell>4</cell></row><row><cell>Unsupported</cell><cell>1</cell><cell>1</cell></row><row><cell>Overall Accuracy</cell><cell>42.00%</cell><cell>39.50%</cell></row><row><cell>Factoid Questions</cell><cell>28.81%</cell><cell>27.12%</cell></row><row><cell>Definition Questions</cell><cell>80.00%</cell><cell>80.00%</cell></row><row><cell>Temporally Restricted Factoid Questions</cell><cell>31.25%</cell><cell>21.88%</cell></row><row><cell>Answer string "NIL"</cell><cell>Precision= 0.23</cell><cell>Precision= 0.19</cell></row><row><cell></cell><cell>Recall=0.80</cell><cell>Recall=0.80</cell></row><row><cell></cell><cell>F-score=0.36</cell><cell>F-score=0.31</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,77.04,735.75,447.42,9.97;2,79.56,746.67,319.29,9.97"><p>The qualification of hybrid to this approach means that the system combines shallow NLP information (POS and named entity tagging) and statistical, language independent information encapsulated into JIRS.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,76.20,746.67,370.29,9.97"><p>The semantic classes used rely on the capability of the named entity classifier used in our experiments.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was done under partial support of <rs type="funder">CONACYT</rs> (Project Grants <rs type="grantNumber">U39957-Y</rs> and <rs type="grantNumber">43990</rs>), <rs type="funder">SNI-Mexico</rs>, and the <rs type="funder">Human Language Technologies Laboratory of INAOE</rs>. We also like to thanks to the CLEF as well as <rs type="institution">EFE agency</rs> for the resources provided.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3SpZu4e">
					<idno type="grant-number">U39957-Y</idno>
				</org>
				<org type="funding" xml:id="_VEhGUFp">
					<idno type="grant-number">43990</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,74.68,178.33,449.91,11.04;8,85.08,189.97,257.85,11.04" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,194.88,178.47,275.52,10.81">A Flexible Distributed Architecture for Natural Language Analyzers</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Padró</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,488.88,178.33,35.72,11.04;8,85.08,189.97,80.85,11.04">Proceedings of the LREC&apos;02</title>
		<meeting>the LREC&apos;02<address><addrLine>Las Palmas de Gran Canaria, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,74.68,202.33,449.75,11.04;8,85.08,213.73,439.29,11.04;8,85.08,225.37,112.29,11.04" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,395.28,202.47,129.15,10.81;8,85.08,213.87,132.61,10.81">A Passage Retrieval System for Multilingual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gómez-Soriano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanchis-Arnal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,275.76,213.73,248.61,11.04;8,85.08,225.37,16.09,11.04">the 8th International Conference on Text, Speech and Dialog, TSD</title>
		<imprint>
			<publisher>Springer LNAI</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,74.68,237.73,449.55,11.04;8,85.08,249.25,439.30,11.04;8,85.08,260.77,439.41,11.04;8,85.08,272.41,128.97,11.04" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,501.00,237.87,23.24,10.81;8,85.08,249.39,259.41,10.81">Overview of the CLEF 2004 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Erbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,363.00,249.25,161.38,11.04;8,85.08,260.77,169.87,11.04">Working Notes for the Cross Language Evaluation Forum Workshop</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Francesca</forename><surname>Borri</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, England; Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">September 2004. 2004</date>
		</imprint>
		<respStmt>
			<orgName>ISTI-CNR</orgName>
		</respStmt>
	</monogr>
	<note>CLEF-2004</note>
</biblStruct>

<biblStruct coords="8,74.68,284.77,449.65,11.04;8,85.08,296.17,439.40,11.04;8,85.08,307.81,52.29,11.04" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,484.44,284.91,39.90,10.81;8,85.08,296.31,204.62,10.81">Toward a Document Model for Question Answering Systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pérez-Coutiño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>López-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,309.48,296.17,122.39,11.04">Advances in Web Intelligence</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3034</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,74.68,320.17,449.77,11.04;8,85.08,331.69,405.30,11.04;8,490.44,330.23,5.04,7.18;8,498.24,331.69,26.12,11.04;8,85.08,343.21,439.53,11.04;8,85.08,354.85,222.57,11.04" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,488.78,320.17,35.68,11.04;8,85.08,331.69,261.85,11.04">Question Answering for Spanish Supported by Lexical Context Annotation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pérez-Coutiño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>López-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,459.48,331.69,30.90,11.04;8,490.44,330.23,5.04,7.18;8,498.24,331.69,26.12,11.04;8,85.08,343.21,302.35,11.04">of the 5 th Workshop of the Cross-Language Evaluation Forum (CLEF 2004)</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, England</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004-09">September 2004. 2005</date>
		</imprint>
	</monogr>
	<note>to appear in proceedings</note>
</biblStruct>

<biblStruct coords="8,74.68,367.21,449.80,11.04;8,85.08,378.73,73.65,11.04" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,214.44,367.35,269.85,10.81">Learning Surface Text Patterns for a Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,504.48,367.21,20.01,11.04;8,85.08,378.73,44.05,11.04">ACL Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,74.68,391.33,329.32,11.04;8,434.52,391.33,22.65,11.04" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,135.00,391.47,264.49,10.81">Identifying Definitions in Text Collections for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Saggion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
