<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,112.08,75.22,382.38,12.52">20th Century Esfinge (Sphinx) solving the riddles at CLEF 2005</title>
				<funder ref="#_63bAwsy">
					<orgName type="full">Portuguese Fundação para a Ciência e Tecnologia</orgName>
				</funder>
				<funder>
					<orgName type="full">POSI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,281.88,113.33,42.94,9.05"><surname>Luís Costa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Linguateca at SINTEF ICT</orgName>
								<address>
									<addrLine>Pb ; 124 Blindern</addrLine>
									<postCode>0314</postCode>
									<settlement>Oslo</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,112.08,75.22,382.38,12.52">20th Century Esfinge (Sphinx) solving the riddles at CLEF 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F3AC76E6EA4C7538AA0FE835A3E00B4D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Esfinge is a general domain Portuguese question answering system. It tries to apply simple techniques to large amounts of text. Esfinge participated last year in the monolingual QA track, but the results were compromised by several basic errors. This year, participation was intended to correct the basic errors of last year and work for the first time in the multilingual QA track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Esfinge overview</head><p>The sphinx in the Egyptian/Greek mythology was a demon of destruction that sat outside Thebes and asked riddles to all passers-by. She strangled all the people unable to answer <ref type="bibr" coords="1,344.89,310.73,10.69,9.05">[1]</ref>, but the times have changed and now Esfinge has to answer questions herself. Fortunately, CLEF's organization is much more benevolent when analysing the results of the QA task. performance Esfinge (http://acdc.linguateca.pt/Esfinge/) is a question answering system developed for the Portuguese which is based on the architecture proposed by Eric Brill <ref type="bibr" coords="1,304.68,356.69,10.60,9.05" target="#b0">[2]</ref>. Brill suggests that it is possible to get state of the art results, applying simple techniques to large quantities of data.</p><p>Esfinge starts by converting a question into patterns of plausible answers. These patterns are queried in several text collections (CLEF text collections and the Web) to obtain snippets of text where the answers are likely to be found. Then, the system harvests these snippets for word N-grams. The N-grams will be later ranked according to their frequency, length and the patterns used to recover the snippets where the N-grams were found (these patterns are scored a priori). Several simple techniques are used to discard or enhance the score of each of the Ngrams. Finally the answer will be the top ranked N-gram or NIL if neither of the N-grams passes all the filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Strategies for CLEF 2005</head><p>During last year participation, several problems compromised the results. The main objectives for this year were to correct these problems, and to participate in the multilingual tasks.</p><p>This year, in addition to the European Portuguese text collection (Público), the organization also provided a Brazilian Portuguese collection (Folha). This new collection helped Esfinge, since one of the problems encountered last year was precisely that the document collection only had texts written in the European variant and some of the answers discovered by the system were in the Brazilian variant, therefore difficult to justify <ref type="bibr" coords="1,488.40,571.25,10.69,9.05" target="#b1">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pre-processing</head><p>IMS Corpus Workbench <ref type="bibr" coords="1,172.32,615.29,11.71,9.05" target="#b2">[4]</ref> was used again to encode the document collections. Each document was divided in sets of three sentences. Last year other text unit sizes were tried (namely 50 contiguous words and one sentence), but the results using three sentence sets were slightly better. The sentence segmentation and tokenization was done using the Perl Module Lingua::PT::PLNbase developed at Linguateca and freely available at CPAN. For the English documents, the sentence segmentation and tokenization programs used by DISPARA in the COMPARA project <ref type="bibr" coords="1,153.24,672.77,11.59,9.05" target="#b3">[5]</ref> were used. Two different strategies were tested. In the first one, the system searched the answers in the Web and used the CLEF document collection to confirm these answers (Run 1). In the second one, it searched the answers in the CLEF document collection only (Run 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 1</head><p>This experiment used the strategy described in another paper by <ref type="bibr" coords="2,333.12,288.05,32.14,9.05">Brill [6]</ref>: answers are searched in the Web, and then the system tries to find documents in the document collection supporting those answers.</p><p>For each question in the QA track, Esfinge performed the following tasks: Question reformulation. The question is submitted to the question reformulation module. This module uses a pattern file that associates patterns of questions with patterns of plausible answers. The result is a set of pairs (answer pattern, score). Some patterns were added this year to the patterns file, based on last year's questions.</p><p>The following pattern is one of the patterns included in that file:</p><formula xml:id="formula_0" coords="2,82.32,407.39,149.94,9.11">Onde ([^\s?]*) ([^?]*)\??/"$2 $1"/20</formula><p>It means that for a question including the word Onde (Where), followed by some words, a possible pattern for an answer will be the words following the one immediately after Onde, followed by the word after Onde in a phrase pattern.</p><p>As an example, take the question Onde fica Lillehammer? (Where is Lillehammer located?). This generates the pattern Lillehamer fica with a score of 20, that can be used to search for documents containing an answer to the question.</p><p>Passage extraction. The patterns obtained in the previous module are submitted to Google. Then, the system extracts the document snippets {S 1 , S 2 … S n } from Google's results pages.</p><p>It was detected in the experiments made with the system that certain types of sites may compromise the quality of the returned answers. To overcome this problem it was created a list of address patterns which are not to be considered (the system does not consider documents stored in addresses that match these patterns). This list includes patterns such as blog, humor, piadas (jokes). These patterns were created manually, but in the future it may be rewarding to use more complex techniques to classify web pages <ref type="bibr" coords="2,364.68,584.33,10.69,9.05" target="#b5">[7]</ref>.</p><p>Another improvement over last year experiment was that if no documents are recovered from the Web, the system tries to recover them from CLEF's document collection. When searching in the document collection, the stop-words without context are discarded. For example in the query "o" "ditador" "cubano" "antes" "da" "revolução" (the Cuban dictator before the revolution), the words o and da are discarded while in the query "o ditador cubano antes da revolução" (phrase pattern) they are not discarded. Last year the 22 most frequent words in the CETEMPúblico corpus <ref type="bibr" coords="2,222.24,653.21,11.71,9.05" target="#b6">[8]</ref> were discarded. This year in addition to those, some other words were discarded. The choice of these words was the result of the tests performed with the system. Some examples are chama (is called), fica (is located), país (country) and se situa (is). One may find these words in questions, but using them in the search pattern may increase the difficulty to find documents containing its answers. An example is the question Com que país faz fronteira a Coreia do Norte? (What country does North Korea border on?). It is more likely to find sentences like A Coreia do Norte faz fronteira com a China (North Korea borders with China) than sentences including the word país.</p><p>When the system neither recovers documents from the Web, nor from CLEF's document collection, one last try is made by stemming some words in the search patterns. The system uses the morphological analyser jspell <ref type="bibr" coords="3,70.92,73.01,11.71,9.05" target="#b7">[9]</ref> to check the PoS of the various words in each query. Then the words classified as common nouns, adjectives, verbs and numbers are stemmed using the module Lingua::PT::Stemmer freely available at CPAN, implementing a Portuguese stemming algorithm proposed by Moreira &amp; Huyck <ref type="bibr" coords="3,355.69,95.93,15.43,9.05" target="#b8">[10]</ref>. This provides the system with more general search patterns that will be used to search documents in the document collection.</p><p>If documents are retrieved using any of the previous techniques, at the end of this stage the system has a set of document passages {P1, P2 … Pn} hopefully containing answers to the question. If no documents are retrieved, the system stops here and returns the answer NIL (no answer found).</p><p>Figure <ref type="figure" coords="3,316.20,569.74,4.98,8.88">2</ref> N-grams harvesting. The distribution of word N-grams (from length 1 to length 3) of the first 100 document excerpts recovered on the previous module is computed. The system uses the Ngram Statistics Package (NSP) <ref type="bibr" coords="3,70.92,625.73,16.75,9.05" target="#b9">[11]</ref> for that purpose.</p><p>Then, the word N-grams are ordered using the following formula: At the end of this stage, the system has an ordered set of possible answers {A1, A2 … An}.</p><p>Named entity recognition/classification in the N-grams. This module was developed for this year participation, hoping that the use of a named entity recognition (NER) system could improve the results (at least for some types of questions). An extra motivation for using a NER system was the HAREM (Evaluation Contest of Named Entity Recognition Systems for Portuguese) <ref type="bibr" coords="4,190.08,118.97,15.53,9.05" target="#b10">[12]</ref>. This event boosted the development or improvement of already existent NER systems for Portuguese. One of the participants was SIEMES <ref type="bibr" coords="4,323.52,130.49,16.75,9.05" target="#b11">[13]</ref> which was developed in the Linguateca node located in Porto, and obtained the best recall among all the systems participating in HAREM.</p><p>SIEMES detects and classifies named entities in a wide range of categories. Esfinge used a sub-set of these categories: Human, Country, Settlement (includes cities, villages, etc), Geographical Locations (locations with no political entailment, like for example Africa), Date and Quantity.</p><p>Esfinge uses a pattern file that associates patterns of questions with the type of expected result. The following pattern is included in that file:</p><formula xml:id="formula_1" coords="4,82.32,222.35,188.11,9.11">Quant(o|a)s.*/VALOR TIPO="QUANTIDADE</formula><p>This pattern means that a question starting with Quantos (how many -masculine form) or Quantas (how many -feminine form) should have a QUANTIDADE (quantity) type answer.</p><p>What the system does in this module is to check whether the question matches with any of the patterns in the "question pattern"/"answer type" file. If it does, the 200 best scored word N-grams are submitted to SIEMES. Then the results returned by SIEMES are analysed to check whether the NER system recognizes named entities classified as one of the desired types. If such named entities are recognized, their ranking in the list of possible answers will be enhanced.</p><p>The NER system is used in the "Who" questions in a slightly different way. First it is used to check whether there is a person in the question and if that happens, the NER system is not invoked on the candidate answers (example: Who is Fidel Ramos?). There are some exceptions to this rule however and some special patterns to deal with them too (example: Who is John Lennon's widow?). When there is not a person in the question, the NER system is always invoked to find instances of persons for the Who questions.</p><p>Figure <ref type="figure" coords="4,316.20,719.74,4.98,8.88">3</ref> N-gram filtering. In this module the list of possible answers is submitted to a set of filters (by ranking order), namely:</p><p>• A filter that discards words contained in the questions. Ex: the answer Satriani is not desired for the question Quem é Joe Satriani? (Who is Joe Satriani?) and should be discarded.</p><p>• A filter that discards answers contained in a list of 'undesired answers'. This list was built with the help of Esfinge's log file. The frequency list of all the solutions provided by Esfinge to the 2004 CLEF QA track questions was computed (not only the best answer, but all the answers that managed to go through all system's filters). With this frequency list and some common sense, the list of 'undesired answers' was built. The words in this list are frequent words that do not really answer questions in isolation (like pessoas/persons, nova/new, lugar/place, grandes/big, exemplo/example). Later some other answers were added to this list, as a result of the tests performed with the system. The list includes now 92 entries.</p><p>• A filter that uses the morphologic analyser jspell <ref type="bibr" coords="5,285.00,235.97,11.71,9.05" target="#b7">[9]</ref> to check the PoS of the various tokens in each answer. This filter is only used if the system could not predict the type of answer for the question (using the "question pattern"/"answer type" file) or if SIEMES was not able to find any answer of the desired type. Jspell returns a set of possible PoS tags for each token. Esfinge considers some PoS as "interesting": adjectives (adj), common nouns (nc), numbers (card) and proper nouns (np). All answers whose first and final token are not classified as one of these "interesting" PoS are discarded.</p><p>• A filter that checks whether the system can find a document supporting the answer in the collection. This filter is only used if the system retrieved documents from the Web. When the system cannot retrieve documents from the Web, it retrieves them from CLEF's document collection, and since the N-grams are extracted from these documents there is no need for this filter. It searches the document collection for documents containing both the candidate answer and a pattern obtained from the question reformulation module.</p><p>N-gram composition. The motivation to use this very simple module arose from the analysis of last year's results and some additional tests performed in the system. Sometimes the answers returned by the system were fragments of the right answers. To minimize this problem, a very simple composition algorithm was implemented this year. When an answer passes all the filters in the previous module, the system does not return that answer immediately and stops like in last year. Instead it checks whether there are more candidate answers containing the answer which was found. Each of these candidate answers are submitted to the filters described in the previous module and if one of them succeeds to pass all the filters, this candidate answer becomes the new answer to be returned as result.</p><p>Final answer. The final answer is the candidate answer with the highest score in the set of candidate answers which are not discarded by any of the filters described above. If all the answers are discarded by the filters, then the final answer is NIL (meaning that the system is not able to find an answer in the document collection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 2</head><p>The difference in this run was that the Web was not used as a resource. The answers were only searched in CLEF's document collection. Consequently, another difference to the algorithm used for the first run was that it was not necessary to check whether there was a document in the collection supporting the answers found since the document collection was the only source used to find them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">EN-PT multilingual task</head><p>In this experiment the questions were translated using the module Lingua::PT::Translate freely available at CPAN. This module provides an easy interface to Altavista's Babelfish translating tool. After the translation this experiment followed the algorithm described for the PT-PT monolingual task in run 1 (the run which seemed to have the best results). 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results sent to the organization are presented and discussed in this section together with the error analysis performed for one of the runs and some considerations about CLEF 2005 set of questions motivated by this error analysis. To create the tables, the question set was divided in categories that intended to check how well the various strategies used by Esfinge perform. For example the category "People" includes all questions where the system expects to have the name of a person as answer such as the "Who" questions, in which the NER system is invoked to find names of persons in the recovered documents. This assumption is usually correct, but there are some exceptions however. An example is the question Who was Barings taken over by? in which the answer is not a person, but a bank. Other interesting categories are "Places", "Quantities" and "Dates" where the NER system is also used to find instances of those categories in the recovered texts. The category "What is the name of X" does not include some matching patterns, in which it is easy to infer that the answer will be of type person. What is Nick Leeson's wife's name? is a good example. The same applies for the categories "Name X" and "Which X" in which some questions are not included and are instead placed in another category because the type of answer is easy to infer. Examples of these kinds of questions are Name a city with 650,000 inhabitants and Which country is Alexandria in? From table 1 it is possible to conclude that the runs submitted for the Portuguese source/Portuguese target task obtain similar results. The run that used the Web (Run 1) got slightly better results, as last year. One can also see that the results in Run 1 are more homogenous than the ones in the second run. Some results are consistently bad, like definitions not involving people (What is X) and not obvious naming (Name X), but that is not surprising since Esfinge does not have special features to deal with definitions. The results of the second run for the questions of type "People" and "Date" are better both comparing to the other types of questions and to the same type of questions in the first run.</p><p>Comparing with last year's results (right columns in the table), one can see that the results improved consistently in almost all types of questions. 1 The official result is 46 right answers, but during the evaluation of the results I found two more right answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># questions</head><note type="other">#</note><p>Table <ref type="table" coords="8,96.85,84.53,4.98,9.05" target="#tab_1">2</ref> shows the number of questions with right answers in both runs (Run 1 and Run 2), the number of questions with right answers only on the first run (Run 1 and not(Run 2)), the number of questions with right answers only on the second run (Run 2 and not(Run 1)) and the number of questions with a right answer in at least one of the runs (Run 1 or Run 2). One can observe that the two runs perform better with different types of questions, which suggests that both of the strategies used are still worthwhile to experiment and study. Table <ref type="table" coords="8,259.20,348.70,3.76,8.88">3</ref>. Causes for wrong answers</p><p>The system's log file was used to investigate the causes for the wrong answers. The system registers in this file all the analysed word N-grams for each of the questions. When word N-grams are rejected by some of the filters, this information is also recorded in the log file.</p><p>In Table <ref type="table" coords="8,119.29,400.49,3.76,9.05">3</ref>, a detailed error analysis for the first run is provided. For some of the questions, it was possible to detect more than one reason for failure. In these cases, both reasons were counted.</p><p>From this evaluation, it is possible to create sets of questions with the same type of problems that can be used to debug and improve the system. From the results in Table <ref type="table" coords="9,177.01,73.01,4.98,9.05" target="#tab_2">4</ref> it is possible to conclude that most of the questions with right answers are the ones where the NER system was not used (14 out of 25). However, an error analysis similar to the one performed for the PT-PT task will be needed to take more solid conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Some considerations about CLEF 2005 set of questions</head><p>The error analysis is not only useful to find the reasons motivating system errors. Here and there one is confronted with some interesting cases. I will describe two of them. The question Who is Josef Paul Kleihues? doesn't have an answer in the document collection according to the organization, but is this really true? There is a document with the following text (freely translated from the Portuguese original):</p><p>People from Galicia like good architecture. In Santiago de Compostela, besides the "Centro Galego de Arte Contemporânea" designed by Siza Vieira, it was built in the historical center a gym designed by the german Josef Paul Kleihues.</p><p>One of Esfinge's runs returned the answer Arquitectura (architecture) giving as support the text from where the previous excerpt was extracted. One may question which answer would be more useful for a hypothetical user.: NIL or the answer provided by Esfinge? I found another curious example in the question Which was the largest Italian party?. On one of the runs Esfinge returned the answer Força Itália supporting it with a document stating that Força Itália is the largest Italian party (it was true at the time the document was written). The organization considered this answer wrong, however, because they wanted an Italian party that was the largest in the past, but was no longer the largest.</p><p>In my opinion the answer provided by the system was acceptable, because the question is being asked in 2005, so one can ask which was the largest Italian party, and one can support an answer with a document from 1994 saying that the largest Italian party is X.</p><p>Although I can understand the point of view of the organization, I think that this kind of question is confusing and polemic even for humans, therefore not particularly useful to evaluate Q&amp;A systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Additional experiments</head><p>The error analysis (condensed on table <ref type="table" coords="9,233.06,457.97,4.18,9.05">3</ref>) provided an insight on the problems affecting the system's performance.</p><p>Some effort was invested in the problems that seemed easier to solve. Namely on the "Error in tokenization", "Problems with the NER system" and "Missing patterns in the file question pattern/answer type". The results of the system after this improvement using the same strategy as in Run 1 are presented in table 5. On that table it is also possible to check how each part of the system helps global performance: the results obtained either without using the NER system or without using the morphological analyser are presented. One can see that (in different types of questions) both this components are helping the system. Table <ref type="table" coords="10,109.20,115.18,3.76,8.88">5</ref>. Results in the PT-PT monolingual task after improvements in the system using the first run strategy Applying the system to the 2004 questions after the improvements and using the same strategy as in Run 1 provides the results presented in table 6. The cause for the better results this year could be the possibility that this year's questions were easier than last year's, but this table shows that the system performs better with last year's questions as well. Table <ref type="table" coords="10,109.44,399.70,3.76,8.88">6</ref>. Results in the PT-PT task after improvements in the system using the first run strategy on 2004 questions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># questions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding remarks</head><p>The results show that Esfinge improved comparing to last year: the results are better both with this year's and last year's questions. Another conclusion is that the two tested strategies perform better with different types of questions, which suggests that both are still worthwhile to experiment and study further.</p><p>The experiments performed to check how each part of the system helps global performance shown that (in different types of questions) both the NER system and the morphological analyser improve the system's performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,285.36,159.46,35.82,8.88;2,168.72,70.80,269.16,80.16"><head>Figure 1 2</head><label>1</label><figDesc>Figure 1</figDesc><graphic coords="2,168.72,70.80,269.16,80.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,82.32,660.29,392.06,9.05;3,70.92,683.21,89.82,9.05;3,70.92,694.73,248.29,9.05;3,70.92,706.25,75.30,9.05"><head>N</head><label></label><figDesc>-gram score = ∑ (F * S * L), through the first 100 snippets resulting from the web search where: F = N-gram frequency S = Score of the search pattern which recovered the document L = N-gram length</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,285.36,435.70,35.82,8.88;6,230.52,70.92,145.56,356.28"><head>Figure 4</head><label>4</label><figDesc>Figure 4</figDesc><graphic coords="6,230.52,70.92,145.56,356.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,169.80,188.45,33.91,9.05;8,394.08,165.41,4.98,9.05;8,382.68,176.93,27.78,9.05;8,380.52,188.33,32.07,9.05;8,169.79,200.57,190.78,9.05;8,169.80,212.09,27.19,9.05;8,391.56,212.09,10.02,9.05;8,169.80,225.65,104.35,9.05;8,391.57,225.65,10.02,9.05;8,169.79,239.09,190.06,9.05;8,391.56,239.09,10.02,9.05;8,169.79,252.65,146.35,9.05;8,391.58,252.65,10.02,9.05;8,169.80,266.09,83.23,9.05;8,391.57,266.09,10.02,9.05;8,169.80,279.65,152.39,9.05;8,391.57,279.65,10.02,9.05;8,169.80,293.09,71.58,9.05;8,391.56,293.09,10.02,9.05;8,169.80,306.65,125.59,9.05;8,391.57,306.65,10.02,9.05;8,169.80,318.77,190.78,9.05;8,169.80,330.29,79.55,9.05;8,394.09,330.29,4.98,9.05"><head></head><label></label><figDesc>NER system 11Missing patterns in the file "question pattern"/"answer type" 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,179.52,392.64,247.56,318.60"><head></head><label></label><figDesc></figDesc><graphic coords="4,179.52,392.64,247.56,318.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,70.92,73.30,428.82,267.84"><head>Table 1 .</head><label>1</label><figDesc>Results by type of question in the PT-PT monolingual task</figDesc><table coords="7,70.92,73.30,428.82,250.00"><row><cell cols="2">3.1 PT-PT monolingual task</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>#ques-</cell><cell>#</cell><cell>%</cell></row><row><cell></cell><cell cols="2"># questions</cell><cell># Run 1</cell><cell>% Run 1</cell><cell># Run 2</cell><cell>% Run 2</cell><cell>tions 2004</cell><cell>Esfinge's best 2004</cell><cell>Esfinge's best 2004</cell></row><row><cell>People</cell><cell></cell><cell>47</cell><cell>11</cell><cell>23%</cell><cell>15</cell><cell>32%</cell><cell>43</cell><cell>8</cell><cell>19%</cell></row><row><cell cols="2">"(Que|Qual) X" -"Which</cell><cell></cell><cell>9</cell><cell>25%</cell><cell>5</cell><cell>14%</cell><cell>42</cell><cell>7</cell><cell>17%</cell></row><row><cell>X"</cell><cell></cell><cell>36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Place</cell><cell></cell><cell>33</cell><cell>9</cell><cell>27%</cell><cell>7</cell><cell>21%</cell><cell>41</cell><cell>7</cell><cell>17%</cell></row><row><cell>"Quem</cell><cell>(é|foi|era)</cell><cell>27</cell><cell>6</cell><cell>22%</cell><cell>6</cell><cell>22%</cell><cell>17</cell><cell>2</cell><cell>12%</cell></row><row><cell cols="2">&lt;HUM&gt;" -"Who (is|was)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;HUM&gt;"</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Quantity</cell><cell></cell><cell>18</cell><cell>4</cell><cell>22%</cell><cell>3</cell><cell>17%</cell><cell>23</cell><cell>4</cell><cell>17%</cell></row><row><cell>Date</cell><cell></cell><cell>15</cell><cell>3</cell><cell>20%</cell><cell>5</cell><cell>33%</cell><cell>15</cell><cell>0</cell><cell>0%</cell></row><row><cell cols="2">"Que é X" -"What is X"</cell><cell>15</cell><cell>2</cell><cell>13%</cell><cell>0</cell><cell>0%</cell><cell>15</cell><cell>1</cell><cell>7%</cell></row><row><cell>Como se</cell><cell></cell><cell>5</cell><cell>4</cell><cell>80%</cell><cell>2</cell><cell>40%</cell><cell>0</cell><cell>0</cell><cell>0%</cell></row><row><cell cols="2">chama|chamou|chamava X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">-What is X called</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Mencione/Indique/Nomeie</cell><cell>4</cell><cell>0</cell><cell>0%</cell><cell>0</cell><cell>0%</cell><cell>3</cell><cell>1</cell><cell>33%</cell></row><row><cell>X -Name X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell></cell><cell>200</cell><cell>48 1</cell><cell>24%</cell><cell>43</cell><cell>22%</cell><cell>199</cell><cell>30</cell><cell>15%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,65.40,476.27,464.45,256.79"><head>Table 2 .</head><label>2</label><figDesc>Combined results</figDesc><table coords="7,65.40,476.27,464.45,238.79"><row><cell></cell><cell></cell><cell></cell><cell>Run 1 and Run 2</cell><cell>%</cell><cell>Run 1 and not(Run 2) #</cell><cell>%</cell><cell>not(Run 1) and Run 2 #</cell><cell>%</cell><cell>Run 2 or Run 1 #</cell><cell>%</cell></row><row><cell>People</cell><cell></cell><cell>47</cell><cell>8</cell><cell>17%</cell><cell>3</cell><cell>6%</cell><cell>7</cell><cell>15%</cell><cell>18</cell><cell>38%</cell></row><row><cell cols="2">"(Que|Qual) X" -"Which</cell><cell>36</cell><cell>4</cell><cell>11%</cell><cell>5</cell><cell>14%</cell><cell>0</cell><cell>0%</cell><cell>9</cell><cell>25%</cell></row><row><cell>X"</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Place</cell><cell></cell><cell>33</cell><cell>5</cell><cell>15%</cell><cell>4</cell><cell>12%</cell><cell>2</cell><cell>6%</cell><cell>11</cell><cell>33%</cell></row><row><cell>"Quem</cell><cell>(é|foi|era)</cell><cell>27</cell><cell>2</cell><cell>7%</cell><cell>4</cell><cell>15%</cell><cell>4</cell><cell>15%</cell><cell>10</cell><cell>37%</cell></row><row><cell cols="2">&lt;HUM&gt;" -"Who (is|was)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;HUM&gt;"</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Quantity</cell><cell></cell><cell>18</cell><cell>3</cell><cell>17%</cell><cell>1</cell><cell>6%</cell><cell>0</cell><cell>0%</cell><cell>4</cell><cell>22%</cell></row><row><cell>Date</cell><cell></cell><cell>15</cell><cell>2</cell><cell>13%</cell><cell>1</cell><cell>7%</cell><cell>3</cell><cell>20%</cell><cell>6</cell><cell>40%</cell></row><row><cell cols="2">"Que é X" -"What is X"</cell><cell>15</cell><cell>0</cell><cell>0%</cell><cell>2</cell><cell>13%</cell><cell>0</cell><cell>0%</cell><cell>2</cell><cell>13%</cell></row><row><cell>Como se</cell><cell></cell><cell>5</cell><cell>2</cell><cell>40%</cell><cell>2</cell><cell>40%</cell><cell>0</cell><cell>0%</cell><cell>4</cell><cell>80%</cell></row><row><cell cols="2">chama|chamou|chamava X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">-What is X called</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Mencione/Indique/Nomeie</cell><cell>4</cell><cell>0</cell><cell>0%</cell><cell>0</cell><cell>0%</cell><cell>0</cell><cell>0%</cell><cell>0</cell><cell>0%</cell></row><row><cell>X -Name X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Totals</cell><cell></cell><cell>200</cell><cell>26</cell><cell>13%</cell><cell>22</cell><cell>11%</cell><cell>16</cell><cell>8%</cell><cell>64</cell><cell>32%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,70.92,456.82,369.54,265.56"><head>Table 4 .</head><label>4</label><figDesc>Results by type of question in the EN-PT multilingual task</figDesc><table coords="8,70.92,456.82,349.23,247.56"><row><cell>3.2 EN-PT multilingual task</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>#</cell><cell>%</cell></row><row><cell></cell><cell></cell><cell>#</cell><cell>Right</cell><cell>Right</cell></row><row><cell></cell><cell></cell><cell>questions</cell><cell>answers</cell><cell>answers</cell></row><row><cell>People</cell><cell></cell><cell>47</cell><cell>6</cell><cell>13%</cell></row><row><cell cols="2">"(Que|Qual) X" -"Which</cell><cell>36</cell><cell>6</cell><cell>17%</cell></row><row><cell>X"</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Places</cell><cell></cell><cell>33</cell><cell>2</cell><cell>6%</cell></row><row><cell>"Quem</cell><cell>(é|foi|era)</cell><cell>27</cell><cell>6</cell><cell>22%</cell></row><row><cell cols="2">&lt;HUM&gt;" -"Who (is|was)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;HUM&gt;"</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Quantities</cell><cell></cell><cell>18</cell><cell>1</cell><cell>6%</cell></row><row><cell>Dates</cell><cell></cell><cell>15</cell><cell>2</cell><cell>13%</cell></row><row><cell cols="2">"Que é X" -"What is X"</cell><cell>15</cell><cell>0</cell><cell>0%</cell></row><row><cell>Como se</cell><cell></cell><cell>5</cell><cell>2</cell><cell>40%</cell></row><row><cell cols="2">chama|chamou|chamava X</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">-What is X called</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Mencione/Indique/Nomeie</cell><cell>4</cell><cell>0</cell><cell>0%</cell></row><row><cell>X -Name X</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Totais</cell><cell></cell><cell>200</cell><cell>25</cell><cell>13%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>I thank <rs type="person">Diana Santos</rs> for reviewing previous versions of this paper, <rs type="person">Alberto Simões</rs> for the hints on using the Perl Modules "jspell" , "Lingua::PT::PLNbase" and Lingua::PT::Translate, <rs type="person">Luís Sarmento</rs>, <rs type="person">Luís Cabral</rs> and <rs type="person">Ana Sofia Pinto</rs> for supporting the use of the NER system SIEMES and <rs type="person">Paul Rayson</rs> for supporting the use of CLAWS Web Tagger [14] (it was planned to send a run for the PT-EN multilingual task, but it was not possible to finish it in time to send it to the organization).</p><p>This work is financed by the <rs type="funder">Portuguese Fundação para a Ciência e Tecnologia</rs> through grant <rs type="grantNumber">POSI/PLP/43931/2001</rs>, co-financed by <rs type="funder">POSI</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_63bAwsy">
					<idno type="grant-number">POSI/PLP/43931/2001</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,74.32,110.99,450.16,8.18;11,82.32,121.31,211.02,8.18" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,113.89,110.99,243.41,8.18">Processing Natural Language without Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,444.48,110.99,80.00,8.18;11,82.32,121.31,16.29,8.18">CICLing 2003. LNCS 2588</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="360" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,74.32,131.63,444.76,8.18;11,82.32,141.90,420.16,8.24;11,82.32,152.22,434.49,8.24;11,82.32,162.59,191.90,8.18" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,117.48,131.63,267.44,8.18">First Evaluation of Esfinge -a Question Answering System for Portuguese</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,231.95,141.90,270.52,8.23;11,82.32,152.22,179.15,8.23">Advances in Cross-Language Information Retrieval: Fifth Workshop of the Cross-Language Evaluation Forum (CLEF 2004)</title>
		<title level="s" coord="11,82.32,162.59,128.45,8.18">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, UK; Heidelberg, Alemanha</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004-09-17">15-17 September 2004</date>
		</imprint>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct coords="11,74.32,173.03,450.29,8.18;11,82.32,183.35,240.47,8.18" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,290.28,173.03,234.34,8.18;11,82.32,183.35,49.82,8.18">The IMS Corpus Workbench: Corpus Query Processor (CQP): User&apos;s Manual</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">M</forename><surname>Schulze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Koenig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-03-08">March 8, 1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Stuttgart</orgName>
		</respStmt>
	</monogr>
	<note>CQP V2.2</note>
</biblStruct>

<biblStruct coords="11,74.32,193.67,449.99,8.18;11,82.32,203.94,442.23,8.24;11,82.32,214.26,225.09,8.24" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,121.80,193.67,236.75,8.18">DISPARA, a system for distributing parallel corpora on the Web</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,107.87,203.94,332.14,8.24">Advances in Natural Language Processing (Third International Conference, PorTAL 2002</title>
		<title level="s" coord="11,159.72,214.31,21.23,8.18">LNAI</title>
		<editor>
			<persName><forename type="first">Elisabete</forename><surname>Ranchhod</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><surname>Nuno</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mamede</surname></persName>
		</editor>
		<meeting><address><addrLine>Faro, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002-06">June 2002. 2002</date>
			<biblScope unit="volume">2389</biblScope>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,74.32,224.75,450.17,8.18;11,70.92,235.07,453.57,8.18;11,70.91,245.39,43.62,8.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,273.12,224.75,129.02,8.18">Data-Intensive Question Answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,116.04,235.07,256.22,8.18">Information Technology: The Tenth Text Retrieval Conference, TREC</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="393" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,74.32,255.66,450.23,8.23;11,82.32,265.98,378.10,8.24" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,211.32,255.71,177.94,8.18">User-aware page classification in a search engine</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Aires</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Aluísio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,405.71,255.66,118.84,8.23;11,82.32,265.98,200.58,8.24">Proceedings of Stylistic Analysis Of Text For Information Access, SIGIR 2005 Workshop</title>
		<meeting>Stylistic Analysis Of Text For Information Access, SIGIR 2005 Workshop<address><addrLine>Salvador, Bahia, Brasil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">19 de Agosto de 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,74.32,276.35,450.06,8.18;11,82.32,286.79,354.54,8.18" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,172.92,276.35,214.52,8.18">Evaluating CETEMPúblico, a free resource for Portuguese</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,407.29,276.35,117.10,8.18;11,82.32,286.79,208.70,8.18">Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 39th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toulouse</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-07-11">9-11 July 2001</date>
			<biblScope unit="page" from="442" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,74.32,297.11,450.24,8.18;11,82.31,307.43,442.12,8.18;11,82.32,317.75,265.38,8.18" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,197.64,297.11,326.92,8.18;11,82.31,307.43,25.60,8.18">Jspell.pm -um módulo de análise morfológica para uso em Processamento de Linguagem Natural</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Simões</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,275.64,307.43,248.80,8.18;11,82.32,317.75,43.31,8.18">Actas do XVII Encontro da Associação Portuguesa de Linguística (APL 2001)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gonçalves</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Correia</surname></persName>
		</editor>
		<meeting><address><addrLine>Lisboa</addrLine></address></meeting>
		<imprint>
			<publisher>APL Lisboa</publisher>
			<date type="published" when="2002">2-4 Outubro 2001. 2002</date>
			<biblScope unit="page" from="485" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,78.50,328.86,332.56,8.24;11,411.12,326.06,5.04,5.93;11,419.28,328.86,105.31,8.24;11,82.32,339.54,442.17,8.24;11,82.32,349.91,184.07,8.18" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,196.32,328.91,192.25,8.18">A Stemming algorithm for the Portuguese Language</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">M</forename><surname>Orengo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Huyck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,406.56,328.86,4.50,8.24;11,411.12,326.06,5.04,5.93;11,419.28,328.86,105.31,8.24;11,82.32,339.54,215.39,8.24">8 th Internacional Symposium on String Processing and Information Retrieval (SPIRE&apos;2001)</title>
		<meeting><address><addrLine>Laguna de San Rafael, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Publications</publisher>
			<date type="published" when="2001">13-15 de Novembro de 2001</date>
			<biblScope unit="page" from="183" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,78.50,360.23,445.94,8.18;11,82.32,370.67,442.26,8.18;11,82.32,380.99,66.78,8.18" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,189.37,360.23,249.27,8.18">The Design, Implementation, and Use of the Ngram Statistic Package</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,457.32,360.23,67.12,8.18;11,82.32,370.67,351.04,8.18">Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<meeting>the Fourth International Conference on Intelligent Text Processing and Computational Linguistics<address><addrLine>Mexico City</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-02">February 2003</date>
			<biblScope unit="page" from="370" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,78.50,391.31,331.37,8.18" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Santos</surname></persName>
		</author>
		<title level="m" coord="11,128.76,391.31,231.64,8.18">Relatório da Linguateca de 15 de Maio de 2004 a 14 de Maio de</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,78.50,401.63,445.97,8.18;11,82.32,411.95,105.60,8.18" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="11,230.65,401.63,293.81,8.18;11,82.32,411.95,38.48,8.18">REPENTINO -A collaborative wide-scope gazetteer for Entity Recognition in Portuguese</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sarmento</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cabral</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct coords="11,78.50,422.27,445.91,8.18;11,82.32,432.71,197.46,8.18" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,207.72,422.27,93.24,8.18">The CLAWS Web Tagger</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rayson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Garside</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,307.33,422.27,56.72,8.18">ICAME Journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="121" to="123" />
			<date type="published" when="1998">1998</date>
			<publisher>Norwegian Computing Centre for the Humanities</publisher>
			<pubPlace>Bergen</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
