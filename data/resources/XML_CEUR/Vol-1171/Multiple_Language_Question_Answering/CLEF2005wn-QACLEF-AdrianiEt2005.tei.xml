<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,108.54,70.04,394.89,11.84">University of Indonesia Participation at Query Answering-CLEF 2005</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,252.78,108.94,54.10,8.48"><forename type="first">Mirna</forename><surname>Adriani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">University of Indonesia Depok</orgName>
								<address>
									<postCode>16424</postCode>
									<country key="ID">Indonesia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,108.54,70.04,394.89,11.84">University of Indonesia Participation at Query Answering-CLEF 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B2294820AA3E24A69CFDD912B883AEE2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a report on our participation in the Indonesian-English question-answering task of the 2005 Cross-Language Evaluation Forum (CLEF). We chose to translate an Indonesian query set into English using a commercial machine translation tool called Transtool. We used a linguistic tool, the Monty Tagger, to find the answer to the question in a passage that has the same tagging as the query.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The University of Indonesia IR-group participate in the bilingual Query-Answering (QA) task in Cross Language Evaluation Forum (CLEF) 2005, i.e., the Indonesian-English QA. We use commercial machine translation software called Transtool<ref type="foot" coords="1,234.54,347.44,3.05,5.49" target="#foot_0">1</ref> to translate an Indonesian query set into English. We learned from our previous work <ref type="bibr" coords="1,150.47,360.22,11.02,8.48" target="#b1">[2]</ref> that freely available dictionaries on the Internet did not provide sufficiently good translation terms, as their vocabulary was very limited. We hoped that we could improve the result using machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Question Answering Process</head><p>There were a number of steps to process the queries that we received from CLEF. Since what we got was English queries, we manually translated the 200 original English queries from CLEF into Indonesian. We then applied a tagging algorithm <ref type="bibr" coords="1,201.92,466.24,10.24,8.48" target="#b1">[2,</ref><ref type="bibr" coords="1,215.29,466.24,7.87,8.48" target="#b2">3]</ref> that we developed to categorize Indonesian queries according to the type of question, such as location, person, date, measure, etc. So, the question word 'dimana' (where) triggers the algorithm to insert a location tag to the query. After applying the tagger, the Indonesian query was then translated back into English using Transtool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Finding the Answer in the Document</head><p>The translated queries were run using an information retrieval system to retrieve the top 100 documents. These top 100 documents were then split into passages each containing two sentences. Each passage was then ran through Monty Tagger<ref type="foot" coords="1,181.50,581.14,3.05,5.49" target="#foot_1">2</ref> to identify the name entity of each word such as NN, NNP for nouns, and CD for numbers, etc. Based on the category of the query, i.e., based on its query tag: location, person, etc., we specified that terms with NN tags are the answers to location type questions, and terms with CD tags are the answer to date type questions, and so forth.</p><p>In order to give the score to the passages, we use a similar scoring technique as the one used by Li and Croft <ref type="bibr" coords="1,508.55,639.34,11.02,8.48" target="#b3">[4]</ref> in their QA work. The scoring rules are as follows:</p><p>1. Give 0 to a passage if its tag is not the same as the query tag.</p><p>2. Give 0 to a passage if the number of words in the query is smaller than some specified threshold, otherwise give the passage a score equal to the number of words in the query (count_m). 3. Add 0.5 if all words in the query are in the passage. 4. Add 0.5 if the order of words in the passage is the same as the query. 5. Calculate the final score of the passage: Score = score + count_m / passage_size where: count_m = the number of the words in a query. passage_size = the number of the words in the passage.</p><p>Once the passages obtained their scores, get the top 20 passages that have the highest scores. The answer to the question in the passage can be estimated by calculating the distance between the candidate words and the queries words. The passage that has the smallest distance is the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>The collection contains English documents from the Glasgow Herald and the Los Angeles Times. There are 200 queries that are being used in this year QA task. We use Lemur<ref type="foot" coords="2,349.02,281.14,3.05,5.49" target="#foot_2">3</ref> information retrieval system to index and retrieve the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Our work focused on the bilingual task using Indonesian queries to retrieve English documents. The result that we submitted is the translated queries from using the machine translation tool. The average number of words in the original English queries is 8.50 words, in the Indonesian queries is 7.89 words, and in the translated English queries is 8.94 words. Transtool failed to translate 8 Indonesian words into English.</p><p>Our result shows that only two correct answers are being found (Table <ref type="table" coords="2,369.29,410.80,3.41,8.48">1</ref>). There were 36 inexact (ambiguous) answers and 162 wrong answers. One of the reasons why the result was so poor was because our tagger did not provide specific enough tagging to the passages. As a result, the tagging in most passages was too general. For example NN tags could be the answer to questions about location or about organization. We did not have enough time to analyze the results before submitting them and correct this problem.</p><formula xml:id="formula_0" coords="2,173.46,491.38,237.06,53.57">Task : Bilingual QA Evaluation W (wrong) 162 U (unsupported) 0 X (inexact) 36 R (right)<label>2</label></formula><p>Table <ref type="table" coords="2,223.81,558.58,3.55,8.48">1</ref>. evaluation of the QA result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Summary</head><p>Our participation in the QA-CLEF this year provided us with invaluable experience in working on the QA task. The result of using the algorithm that we developed in this work was relatively poor. Our plan for the future is to use better linguistic tools and improve the scoring algorithm to produce more accurate answers.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,97.92,692.44,173.60,8.48"><p>See http://www.geocities.com/cdpenerjemah/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,97.92,703.24,192.03,8.48"><p>See http://web.media.mit.edu/~hugo/montytagger/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,97.92,703.24,129.23,8.48"><p>See http://www.lemurproject.org/.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,96.13,107.86,423.27,8.48;3,109.50,118.66,63.75,8.48" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,159.04,107.86,151.74,8.48">Bilingual CLEF for English and Dutch</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Adriani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,330.36,107.86,143.44,8.48">CLEF 2001 Working Note Workshop</title>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,96.13,129.52,423.51,8.48;3,109.50,140.32,21.15,8.48;3,130.68,138.40,4.73,5.49;3,137.82,140.32,162.03,8.48" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,357.83,129.52,161.81,8.48;3,109.50,140.32,21.15,8.48;3,130.68,138.40,4.73,5.49;3,137.82,140.32,96.73,8.48">Question Answering by Passage Selection: The 9 th Text retrieval Conference</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">I E</forename><surname>Kisman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,241.22,140.32,30.65,8.48">TREC-9)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,96.13,151.12,283.58,8.48;3,379.74,149.14,4.73,5.49;3,387.30,151.12,132.15,8.48;3,109.50,161.98,33.70,8.48" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Hull</surname></persName>
		</author>
		<title level="m" coord="3,159.80,151.12,219.92,8.48;3,379.74,149.14,4.73,5.49;3,387.30,151.12,132.15,8.48;3,109.50,161.98,6.81,8.48">Xerox TREC-8 Question Answering Track Report: The 8 th Text Retrieval Conference (TREC-8)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,96.13,172.78,423.28,8.48;3,109.50,183.58,75.26,8.48;3,184.80,181.60,4.79,5.49;3,191.88,183.58,168.81,8.48" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,233.58,172.78,214.23,8.48">Evaluating Question-Answering Techniques in Chinese</title>
		<author>
			<persName coords=""><forename type="first">Xiaoyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Dan Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,468.06,172.78,51.35,8.48;3,109.50,183.58,75.26,8.48;3,184.80,181.60,4.79,5.49;3,191.88,183.58,140.85,8.48">NIST Special Publication: The 10 th Text Retrieval Conference (TREC-10)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
