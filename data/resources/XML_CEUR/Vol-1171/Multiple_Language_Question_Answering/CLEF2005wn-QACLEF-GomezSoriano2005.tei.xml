<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.51,148.86,311.99,15.15;1,104.48,170.78,394.03,15.15">Monolingual and Cross-language QA using a QA-oriented Passage Retrieval System</title>
				<funder ref="#_3bb99rv">
					<orgName type="full">ICT EU-India</orgName>
				</funder>
				<funder ref="#_zbgkURp #_2wGSXVe">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2005-08-20">August 20, 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.51,204.67,53.88,8.74"><forename type="first">José</forename><surname>Manuel</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Sistemas Informáticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politécnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,208.71,204.67,101.56,8.74"><roleName>Empar</roleName><forename type="first">Gómez</forename><surname>Soriano</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Sistemas Informáticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politécnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.58,204.67,56.79,8.74"><forename type="first">Bisbal</forename><surname>Asensi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Sistemas Informáticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politécnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.09,204.67,68.98,8.74"><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
							<email>dbuscaldi@dsic.upv.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Sistemas Informáticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politécnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.06,218.62,50.81,8.74"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
							<email>prosso@dsic.upv.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Sistemas Informáticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politécnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,284.84,218.62,64.11,8.74"><forename type="first">Emilio</forename><surname>Sanchis</surname></persName>
							<email>esanchis@dsic.upv.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Sistemas Informáticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politécnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.28,218.62,24.66,8.74;1,178.67,232.57,19.81,8.74"><forename type="first">Arnal</forename><surname>Dpto</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Sistemas Informáticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politécnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,207.86,232.57,9.96,8.74"><surname>De</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Sistemas Informáticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politécnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.51,148.86,311.99,15.15;1,104.48,170.78,394.03,15.15">Monolingual and Cross-language QA using a QA-oriented Passage Retrieval System</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2005-08-20">August 20, 2005</date>
						</imprint>
					</monogr>
					<idno type="MD5">9BA4373DD9FBAC584A8CF06D00707200</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval; H.3.4 Systems and Software Measurement</term>
					<term>Performance</term>
					<term>Experimentation Question Answering</term>
					<term>Passage Retrieval</term>
					<term>Query Classification</term>
					<term>Answer Extraction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the work done by the RFIA group at the Departamento de Sistemas Informáticos y Computación of the Universidad Politécnica of Valencia for the 2005 edition of the CLEF Question Answering task. We participated in three monolingual tasks: Spanish, Italian and French, and in two cross-language tasks: spanish to english and english to spanish. Since this was our first participation, we focused our work on the passage-based search engine while using simple pattern matching rules for the Answer Extraction phase. As regards the cross-language tasks, we had resort to the most common web translation tools.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The basic functionality of a Question Answering (QA) system is to allow a user to question in natural language a non-structured document collection in order to look for the correct answer. In the case of the cross-language task the collection is constituted by documents written in a language different from the one used in the query, which increases the task difficulty.</p><p>A QA system can be divided, usually, into three main modules: Question Classification (QC), document or Passage Retrieval (PR) and Answer Extraction (AE). The aim of the first module is to recognize the type or category of the expected answer (e.g. if it is a Person, Quantity, Date, etc.) from the user question. The second module obtains the passages (or pieces of text) which contain the terms of the question. Finally, the answer extraction module uses the information collected by the previous modules in order to extract the correct answer. Sometimes the QC module can provide the other modules with additional information extracted from the query. In such cases the module can be named Question Analyzer.</p><p>The most relevant part of our work is made up by the Passage Retrieval system, specifically oriented to the QA task, whereas most QA systems use classical PR methods <ref type="bibr" coords="2,440.65,123.98,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,455.19,123.98,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="2,466.97,123.98,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="2,478.75,123.98,7.01,8.74" target="#b3">4]</ref>. Our PR method is also language independent, because the question and passage processing phases do not use any knowledge about the lexicon and the syntax of the corresponding language. A SVM approach combined with pattern rules has been used for the QC module. Due to the fact that this was our first participation to the CLEF QA task, the AE module was developed using simple pattern-matching rules, and therefore resulted to be somehow coarse, due both to the small number of question categories and to the lack of time to define all the needed patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Description of QA System</head><p>The architecture of our QA system is shown in Fig. <ref type="figure" coords="2,314.65,250.47,4.13,8.74" target="#fig_0">1</ref>. Given a user question, this will be handed over to the Question Analysis (in our case it does not only classify the questions, but extracts also some constraints to be used in the Answer Extraction phase) and Passage Retrieval modules. Next, the Answer Extraction obtains the answer from the expected type, constraints and passages returned by Question Analysis and Passage Retrieval modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Analysis</head><p>The main objective of this module is to obtain the expected answer type from the question. This is a crucial step of the processing since the Answer Extraction module uses a different strategy depending on the expected answer type, and errors in this phase account for the 36.4% of the total number of errors in Question Answering as reported by Moldovan et al. <ref type="bibr" coords="3,406.00,166.27,9.97,8.74" target="#b4">[5]</ref>. The different answer types that can be treated by our system are shown in Table <ref type="table" coords="3,354.59,178.23,3.88,8.74" target="#tab_0">1</ref>.</p><p>A SVM classifier trained over a corpus of 1, 393 questions in English and Spanish from the past TREC<ref type="foot" coords="3,118.50,200.57,3.97,6.12" target="#foot_0">1</ref> QA test sets has been coupled with a simple pattern-based classifier. The answer of both classifiers are evaluated by a sub-module that selects the most specific category between the ones returned by the classifiers. For instance, the answer extraction module applies a specialized strategy if the expected type of the answer is "COUNTRY", that is a sub-category of "LOCATION". The patterns are organized in a 3-levels hierarchy, where each category is defined by one or more patterns written as regular expressions. For instance, the Italian patterns for the category "city" are: .*(che|quale) .*citt\'a .+ and (qual|quale) .*la capitale .+. The questions that do not match any defined pattern are labeled with OTHER. The QC system based on patterns was used stand-alone for both Italian and French, because of the unavailability of corpora for these languages.</p><p>Together with the usual Query Classification task, the module analyzes the query with the purpose of identifying the constraints to be used in the Answer Extraction (AE) phase. These constraints are made by sequences of words extracted from the POS-tagged query by means of POS patterns and rules. For instance, any sequence of nouns (such as "ozone hole") is considered as a relevant pattern. The POS-taggers used were the SVMtool<ref type="foot" coords="3,364.26,600.92,3.97,6.12" target="#foot_1">2</ref> for English and Spanish, and the TreeTagger<ref type="foot" coords="3,138.76,612.87,3.97,6.12" target="#foot_2">3</ref> for Italian and French.</p><p>We distinguish two classes of constraints: a target constraint, which can be considered the object of the question, and zero or more contextual constraints, keeping the information that has to be included in the retrieved passage in order to have a chance of success in extracting the correct answer. For example, in the following question: "How many inhabitants were there in Sweden in 1989?" inhabitants is the target constraint, while Sweden and 1989 are the contextual constraints. There is always only one target constraint for each question, but the number of contextual constraint is not fixed. For instance, in "Who is Jorge Amado?" the target constraint is Jorge Amado but there are no contextual constraints.</p><p>In the case of the Cross-language task, the module works over an optimal translation of the input query. Four translations are obtained through the following web tools: Google<ref type="foot" coords="4,461.82,122.40,3.97,6.12" target="#foot_3">4</ref> , Systran<ref type="foot" coords="4,505.76,122.40,3.97,6.12" target="#foot_4">5</ref> , Babelfish<ref type="foot" coords="4,128.84,134.36,3.97,6.12" target="#foot_5">6</ref> and Freetrans<ref type="foot" coords="4,196.80,134.36,3.97,6.12" target="#foot_6">7</ref> . For each translation a trigram chain is obtained in the following way: let w = (w 1 , . . . , w n ) be the sequence of the words in the translation, then a trigram chain is a set of trigrams T = {(w 1 , w 2 , w 3 ), (w 2 , w 3 , w 4 ), . . . (w n-2 , w n-1 , w n )}. Then each of the trigrams t ∈ T is submitted to a web search engine (we opted for MSN Search<ref type="foot" coords="4,381.83,170.22,3.97,6.12" target="#foot_7">8</ref> ) as a string: "w i w i+1 w i+2 ", obtaining the web count c(t) of that trigram. The weight of each trigram chain (and therefore of the corresponding translation) is obtained by means of Formula 1.</p><formula xml:id="formula_0" coords="4,186.85,217.11,326.15,26.14">W (T ) = t∈T ĉ(t) where ĉ(t) = log c(t) c(t) &gt; 1 0.1 c(t) ≤ 1 (1)</formula><p>The optimal translation is the one with the highest trigram chain weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage Retrieval</head><p>The user question is handed over also to the Search Engine and N-grams Extraction modules.</p><p>Passages with the relevant terms (i.e., without stopwords) are found by the Search Engine using the classical IR system. Sets of unigrams, bigrams, ..., n-grams are extracted from the extended passages and from the user question. In both cases, n will be the number of question terms. With the n-gram sets of the passages and the user question we will make a comparison in order to obtain the weight of each passage. The weight of a passage will be heavier if the passage contains greater n-gram structures of the question.</p><p>For instance, if we ask "Who is the President of Mexico? " the system could retrieve two passages: one with the expression "...Vicente Fox is the President of Mexico...", and the other one with the expression "...Carlo Azeglio Ciampi is the President of Italy...". Of course, the first passage must have more importance because it contains the 5-gram "is the President of Mexico", whereas the second passage only contains the 4-gram "is the President of ", since the "is the President of Italy" 5-gram is not in the original question. To calculate the weight of n-grams of every passage, first the greatest relevance of n-gram in the passage is identify and we assign to this a weight equal to the sum of all term weights. Next, other n-grams less relevant are searched. These n-grams are not composed by terms of found n-brams. The weight of these n-grams will be the sum of all their weight terms divided by two. The weight of every term comes fixed by (2):</p><formula xml:id="formula_1" coords="4,251.76,511.98,261.24,22.31">w k = 1 - log(n k ) 1 + log(N ) .<label>(2)</label></formula><p>Where n k is the number of passages in which the associated term to the weight w k appears and N is the number of system passages. We make the assumption that stopwords occur in every passage (i.e., n k takes the value of N ). For instance, if the term appears once in the passage collection, its weight will be equal to 1 (the greatest weight). Whereas if it is a stopword its weight will be the lowest.</p><p>Depending on the style used to submit a question, sometimes a term unrelated to the question can obtain a greater weight than those assigned to the Name Entities (NE)<ref type="foot" coords="4,415.54,611.84,3.97,6.12" target="#foot_8">9</ref> . Therefore, the (2) is changed to give more weight to the NE than the rest of question terms and so to force its presence in the first passages of the ranking. In order to identify the NE a natural language processing is not used. We showed that in the most questions the NE start with either an uppercase letter or a number. Once the terms are weighted, these are normalized for the sum of all terms are equal to 1.</p><p>To calculate the weight of n-grams of every passage, first the greatest relevance of n-gram in the passage is identify and we assign to this a weight equal to the sum of all term weights. Next other n-grams less relevant are searched. These n-grams are not composed by terms of found n-grams. The weight of these n-grams will be the sum of all their weight terms. A n-gram weight is divided by two in order to avoid that its weight will be the same of the complete n-gram.</p><p>The passage retrieval engine, JIRS, can be obtained at the following URL: http://leto.dsic.upv.es:8080/jirs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Extraction</head><p>The input of this module is constituted by the n passages returned by the PR module and the constraints (including the expected type of the answer) obtained through the Question Analysis module. A TextCrawler is instantiated for each of the n passages with a set of patterns for the expected type of the answer and a pre-processed version of the passage text. Some patterns can be used for all languages; for instance, when looking for proper names, the pattern is the same for all languages. The pre-processing of passage text consists in separating all the punctuation characters from the words and in stripping off the annotations of the passage. It is important to keep the punctuation symbols because we observed that they usually offer important clues for the individuation of the answer: for instance, it is more frequent to observe a passage containing "The president of Italy, Carlo Azeglio Ciampi" than one containing "The president of Italy IS Carlo Azeglio Ciampi' ; moreover, movie and book titles are often put between apices.</p><p>The positions of the passages in which occur the constraints are marked before passing them to the TextCrawlers. Some spell-checking function has been added in this phase by using Levenshtein distance to compare strings. The TextCrawler begins its work by searching all the passage's substrings matching the expected answer pattern. Then a weight is assigned to each found substring s, depending on the positions of the constraints, if s does not include any of the constraint words. Let us define w t (s) and w c (s) as the weights assigned to a substring s as a function, respectively, of its distance from the target constraints (3) and the context constraints (4) in the passage.</p><formula xml:id="formula_2" coords="5,233.18,457.18,279.81,15.05">w t (s) = max 0&lt;k≤|p(t)| close(s, p k (t))<label>(3)</label></formula><formula xml:id="formula_3" coords="5,215.25,482.05,297.75,31.18">w c (s) = 1 |c| |c| i=0 max 0&lt;j≤|p(ci)| near(s, p j (c i ))<label>(4)</label></formula><p>Where c is the vector of contextual constraints, p(c i ) is the vector of positions of the constraint c i in the passage, t is the target constraint and p(t) is the vector of positions of the target constraint t in the passage. Close and near are two proximity function defined as:</p><formula xml:id="formula_4" coords="5,216.73,564.87,296.27,62.38">near(s, p) = exp - d(s, p) -1 5 2 (5) close(s, p) = exp - d(s, p) -1 2 2 (6)</formula><p>Where p is a position in the passage and d(s, p) is computed as:</p><formula xml:id="formula_5" coords="5,240.48,660.99,272.52,15.89">d(s, p) = min i=0,i=|s| (s i -p) 2<label>(7)</label></formula><p>Where s i indicates the position of the i-th word of the substring s. The proximity functions can roughly be seen as fuzzy membership functions, where close(s,p) means that the substring s is adjacent to the word at the position p, and near(s,p) means that the substring s is not far from the word at position p. The 2 and 5 values roughly indicate the range within the position p where the words are considered really "close" and "near", and have been selected after some experiments with the CLEF2003 QA Spanish test set. Finally, the weight is assigned to the substring s in the following way:</p><formula xml:id="formula_6" coords="6,198.67,143.33,310.09,46.57">w(s) =        w t (s) • w c (s) if |p(t)| &gt; 0 ∧ |c| &gt; 0 w c (s) if |p(t)| = 0 ∧ |c| &gt; 0 w t (s) if |c| = 0 ∧ |p(t)| &gt; 0 0 elsewhere. (<label>8</label></formula><formula xml:id="formula_7" coords="6,508.76,163.33,4.24,8.74">)</formula><p>This means that if in the passage have been found both the target constraint and the contextual constraints, the product of the weights obtained for every constraint will be used; otherwise, only the weight obtained for the constraints found in the passage will be used.</p><p>Usually, the type of expected answer directly affects the weighting formula. For instance, the "DEFINITION" questions (such as "Who is Jorge Amado?") usually contain only the target constraint, while "QUANTITY" questions (such as "How many inhabitants are there in Sweden?") contain both target and contextual constraints. For the other question types the target constraint is rarely found in the passage, and weight computation relies only on the contextual constraints (e.g. "From what port did the ferry Estonia leave for its last trip?", port is the target constraint but it is not mandatory in order to found the answer, since it is most common to say "The Estonia left from Tallinn", from which the reader can deduce that Tallinn is -or at least has-a port, than "Estonia left from the port of Tallinn").</p><p>The filter module takes advantage of some knowledge resources, such as a mini knowledge base or the web, in order to discard the candidate answers which do not match with an allowed pattern or that do match with a forbidden pattern. For instance, a list of country names in the four languages has been included in the knowledge base in order to filter country names when looking for countries. When the filter rejects a candidate, the TextCrawler provide it with the next best-weighted candidate, if there is one.</p><p>Finally, when all TextCrawlers end their analysis of the text, the Answer Selection module selects the answer to be returned by the system. The following strategies have been developed:</p><p>• Simple voting (SV): The returned answer corresponds to the candidate that occurs most frequently as passage candidate.</p><p>• Weighted voting (WV): Each vote is multiplied for the weight assigned to the candidate by the TextCrawler and for the passage weight as returned by the PR module.</p><p>• Maximum weight (MW): The candidate with the highest weight and occurring in the best ranked passage is returned.</p><p>• Double voting (DV): As simple voting, but taking into account the second best candidates of each passage.</p><p>• Top (TOP): The candidate elected by the best weighted passage is returned.</p><p>SV is used for every "NAME" type question, while WV is used for all other types. For "NAME" questions, when two candidates obtain the same number of votes, the Answer Selection module looks at the DV answer. If there is still an ambiguity, then the WV strategy is used. For other types of question, the module use directly the MW. TOP is used only to assign the confidence score to the answer, obtained by dividing the number of strategies giving the same answer by the total number of strategies <ref type="bibr" coords="6,204.04,650.00,11.63,8.74" target="#b4">(5)</ref>, multiplied for other measures depending on the number of passages returned (n p /N , where N is the maximum number of passages that can be returned by the PR module and n p is the number of passages actually returned) and the averaged passage weight. The weighting of NIL answers is slightly different, since is obtained as 1 -n p /N if n p &gt; 0, 0 elsewhere.</p><p>In our system, candidates are compared by means of a partial string match, therefore Boris Eltsin and Eltsin are considered as two votes for the same candidate. Later, the Answer Selection module returns the answer in the form occuring most frequently.</p><p>For this participation we developed an additional web-corrected weighting strategy, based on web counts of the question constraints. With this strategy, the MSN Search engine is initially queried with the target and contextual constraints, returning a p c number of pages containing them. Then, for each of the candidate answers, another search is done by putting the candidate answer itself together with the constraints, obtaining p a pages. Therefore, the final weight assigned to the candidate answer is multiplied by p a /p c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We submitted two runs for each of the following monolingual task: Spanish, Italian and French, while only one run was submitted for the Spanish-English and English-Spanish cross-language tasks. The second runs (labelled upv 052 ) of the monolingual tasks use the web-corrected weighting strategy, while the first runs use the clean system, without the recourse to the web. In Table <ref type="table" coords="7,90.00,250.47,4.98,8.74" target="#tab_1">2</ref>  It can be observed that the web weighting produced worse results, even if the 0.00% obtained for the upv 052eses run for definition questions can be due to an undetected problem. Definition questions obtained better results than other kinds of questions, and we suppose this is due to the ease in identifying the target constraint in these cases. Moreover, the results for the Spanish monolingual tasks are better than the other ones, and we believe this is due mostly to the fact that the question classification was performed combining the results of the SVM and pattern classifiers, whereas for French and Italian the expected type of the answer was obtained only via the patternbased classifier. Another reason can be that the majority of the preliminary experiments were done over the CLEF2003 Spanish corpus, therefore resulting in the definition of more accurate patterns for the Spanish Answer Extractor.</p><p>In order to evaluate the impact of the answer types, we grouped the results obtained for the best run by the defined categories , as shown in Table <ref type="table" coords="7,341.36,578.14,3.88,8.74" target="#tab_2">3</ref>. As it can be seen, the best results have been obtained for the "LOCATION.COUNTRY" category, as expected, due to the use of a customized knowledge source. The worst results have been obtained for the questions "OTHER", for which there is not a defined strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Further Work</head><p>The obtained results are comparable to those we obtained over the past year corpus, and therefore are as expected. The main drawback of the system is constituted by the cost of defining patterns for the Answer Extraction module: many experiments are needed in order to obtain a satisfactory pattern, and this has to be done for each expected answer type in each category. Moreover, apart from some well-defined categories for which a pattern can be defined, in other cases is almost impossible to identify a pattern that can match with all the answers of such questions. Therefore, we plan to use in the future both machine learning approaches in order to master this problem, together with more knowledge bases, since the small country database allowed to obtain good results for the COUNTRY questions. In the cross-language task, the Passage Retrieval module worked well despite the generally acknowledged low quality of web translations, allowing to obtain results slightly worse than those obtained in the monolingual task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,209.06,639.08,184.88,8.74;2,110.40,271.17,382.20,352.80"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Main diagram of the QA system</figDesc><graphic coords="2,110.40,271.17,382.20,352.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,187.43,271.60,228.14,199.57"><head>Table 1 :</head><label>1</label><figDesc>QC pattern classification categories.</figDesc><table coords="3,187.43,271.60,228.14,177.70"><row><cell>L0</cell><cell>L1</cell><cell>L2</cell></row><row><cell>NAME</cell><cell>ACRONYM</cell><cell></cell></row><row><cell></cell><cell>PERSON</cell><cell></cell></row><row><cell></cell><cell>TITLE</cell><cell></cell></row><row><cell></cell><cell>LOCATION</cell><cell>COUNTRY</cell></row><row><cell></cell><cell></cell><cell>CITY</cell></row><row><cell></cell><cell></cell><cell>GEOGRAPHICAL</cell></row><row><cell>DEFINITION</cell><cell></cell><cell></cell></row><row><cell>DATE</cell><cell>DAY</cell><cell></cell></row><row><cell></cell><cell>MONTH</cell><cell></cell></row><row><cell></cell><cell>YEAR</cell><cell></cell></row><row><cell></cell><cell>WEEKDAY</cell><cell></cell></row><row><cell>QUANTITY</cell><cell>MONEY</cell><cell></cell></row><row><cell></cell><cell>DIMENSION</cell><cell></cell></row><row><cell></cell><cell>AGE</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,90.00,250.47,423.00,171.03"><head>Table 2 :</head><label>2</label><figDesc>we show the overall accuracy obtained in all the runs. Accuracy results for the submitted runs.</figDesc><table coords="7,146.72,273.13,309.56,107.97"><row><cell>task</cell><cell>run</cell><cell cols="2">overall factoid definition</cell><cell>tr</cell><cell>nil conf</cell></row><row><cell cols="3">es-es upv 051 33.50% 26.27%</cell><cell cols="3">52.00% 31.25% 0.19 0.21</cell></row><row><cell></cell><cell cols="2">upv 052 18.00% 22.88%</cell><cell cols="3">0.00% 28.12% 0.10 0.12</cell></row><row><cell>it-it</cell><cell cols="2">upv 051 25.50% 20.00%</cell><cell cols="3">44.00% 16.67% 0.10 0.15</cell></row><row><cell></cell><cell cols="2">upv 052 24.00% 15.83%</cell><cell cols="3">50.00% 13.33% 0.06 0.12</cell></row><row><cell>fr-fr</cell><cell cols="2">upv 051 23.00% 17.50%</cell><cell>46.00%</cell><cell cols="2">6.67% 0.06 0.11</cell></row><row><cell></cell><cell cols="2">upv 052 17.00% 15.00%</cell><cell cols="3">20.00% 20.00% 0.07 0.07</cell></row><row><cell cols="3">en-es upv 051 22.50% 19.49%</cell><cell cols="3">34.00% 15.62% 0.15 0.10</cell></row><row><cell cols="3">es-en upv 051 17.00% 12.40%</cell><cell cols="3">28.00% 17.24% 0.15 0.07</cell></row></table><note coords="7,296.95,395.58,216.04,6.99;7,90.00,405.05,423.00,6.99;7,90.00,414.51,223.45,6.99"><p>Overall: overall accuracy, factoid: accuracy over factoid questions; definition: accuracy over definition questions; tr: accuracy over temporally restricted questions; nil: precision over nil questions; conf: confidence-weighted score.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,158.20,110.82,286.59,210.33"><head>Table 3 :</head><label>3</label><figDesc>Accuracy results for the upv 051eses run, grouped by answer type.</figDesc><table coords="8,179.82,110.82,243.37,188.46"><row><cell>category</cell><cell cols="2">questions accuracy</cell></row><row><cell>NAME</cell><cell>2</cell><cell>0.0%</cell></row><row><cell>NAME.PERSON</cell><cell>25</cell><cell>28.0%</cell></row><row><cell>NAME.TITLE</cell><cell>1</cell><cell>0.0%</cell></row><row><cell>NAME.LOCATION</cell><cell>6</cell><cell>16.7%</cell></row><row><cell>NAME.LOCATION.COUNTRY</cell><cell>14</cell><cell>92.8%</cell></row><row><cell>NAME.LOCATION.CITY</cell><cell>2</cell><cell>100.0%</cell></row><row><cell>NAME.LOCATION.GEO</cell><cell>2</cell><cell>0.0%</cell></row><row><cell>DEFINITION</cell><cell>61</cell><cell>44.3%</cell></row><row><cell>DATE</cell><cell>11</cell><cell>36.3%</cell></row><row><cell>DATE.DAY</cell><cell>4</cell><cell>0.0%</cell></row><row><cell>DATE.YEAR</cell><cell>2</cell><cell>0.0%</cell></row><row><cell>QUANTITY</cell><cell>21</cell><cell>33.3%</cell></row><row><cell>QUANTITY.AGE</cell><cell>4</cell><cell>25.0%</cell></row><row><cell>TIME</cell><cell>4</cell><cell>0.0%</cell></row><row><cell>OTHER</cell><cell>41</cell><cell>4.8%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,728.00,71.80,6.99"><p>http://trec.nist.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.24,737.50,149.17,6.99"><p>http://www.lsi.upc.edu/ nlp/SVMTool/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,105.24,747.00,331.04,6.99"><p>http://www.ims.uni-stuttgart.de/projekte/ corplex/TreeTagger/DecisionTreeTagger.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,105.24,690.02,102.14,6.99"><p>http://translate.google.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,105.24,699.52,104.54,6.99"><p>http://www.systranbox.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,105.24,709.03,110.65,6.99"><p>http://babelfish.altavista.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="4,105.24,718.53,110.19,6.99"><p>http://ets.freetranslation.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="4,105.24,728.04,84.31,6.99"><p>http://search.msn.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="4,105.24,737.54,407.75,6.99;4,90.00,747.00,322.94,6.99"><p>The NE are names of persons, organizations, places, dates, etc. The NE are the most important terms of the question and it does not make sense return passages which do not contain these words.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="institution">CONACyT</rs> for partially supporting this work under the grant <rs type="grantNumber">43990A-1</rs> as well as <rs type="projectName">R2D2 CICYT</rs> (<rs type="grantNumber">TIC2003-07158-C04-03</rs>) and <rs type="funder">ICT EU-India</rs> (<rs type="grantNumber">ALA/95/23/2003/077-054</rs>) research projects. A special acknowledgement to <rs type="person">Manuel Montés y Gómez</rs> for support during his stance at the <rs type="institution">Universidad Politécnica de Valencia</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_zbgkURp">
					<idno type="grant-number">43990A-1</idno>
					<orgName type="project" subtype="full">R2D2 CICYT</orgName>
				</org>
				<org type="funding" xml:id="_3bb99rv">
					<idno type="grant-number">TIC2003-07158-C04-03</idno>
				</org>
				<org type="funding" xml:id="_2wGSXVe">
					<idno type="grant-number">ALA/95/23/2003/077-054</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,105.50,535.08,407.50,8.74;8,105.50,547.03,278.35,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,323.39,535.08,189.61,8.74;8,105.50,547.03,60.02,8.74">Multilingual question/answering: the DIO-GENE system</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,190.90,547.03,157.35,8.74">The 10th Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,566.96,407.50,8.74;8,105.50,578.91,407.51,8.74;8,105.50,590.87,27.68,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,290.72,566.96,222.27,8.74;8,105.50,578.91,43.94,8.74">Cross-language question answering at the university of helsinki</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Aunimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kuuskoski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Makkonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,178.15,578.91,282.34,8.74">Workshop of the Cross-Lingual Evaluation Forum (CLEF 2004)</title>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,610.79,407.51,8.74;8,105.50,622.75,355.28,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,317.76,610.79,128.27,8.74">Question answering in spanish</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Izquierdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Muoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,469.47,610.79,43.54,8.74;8,105.50,622.75,229.86,8.74">Workshop of the Cross-Lingual Evaluation Forum (CLEF 2003)</title>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,642.67,407.50,8.74;8,105.50,654.63,407.51,8.74;8,105.50,666.58,311.36,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,243.73,642.67,269.27,8.74;8,105.50,654.63,337.25,8.74">Experiments on robust nl question interpretation and multilayered document annotation for a cross-language question/answering system</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,469.47,654.63,43.54,8.74;8,105.50,666.58,229.86,8.74">Workshop of the Cross-Lingual Evaluation Forum (CLEF 2004)</title>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,686.51,407.51,8.74;8,105.50,698.44,407.51,8.77;8,105.50,710.42,34.87,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,383.72,686.51,129.29,8.74;8,105.50,698.47,244.44,8.74">Performance issues and error analysis in an open-domain question answering system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,363.09,698.47,101.39,8.74">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="133" to="154" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
