<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.56,136.80,311.16,12.61;1,252.84,153.36,89.82,12.61">Cross Lingual Question Answering using QRISTAL for CLEF 2005</title>
				<funder ref="#_SpWDESm #_Ds3aQtE">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,195.12,190.69,76.32,11.04"><forename type="first">Dominique</forename><surname>Laurent</surname></persName>
							<email>dlaurent@synapse-fr.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Synapse Développement</orgName>
								<address>
									<addrLine>33 rue Maynard</addrLine>
									<postCode>31000</postCode>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.16,190.69,60.53,11.04"><forename type="first">Patrick</forename><surname>Séguéla</surname></persName>
							<email>p.seguela@synapse-fr.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Synapse Développement</orgName>
								<address>
									<addrLine>33 rue Maynard</addrLine>
									<postCode>31000</postCode>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.48,190.69,54.58,11.04"><forename type="first">Sophie</forename><surname>Nègre</surname></persName>
							<email>sophie.negre@synapse-fr.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Synapse Développement</orgName>
								<address>
									<addrLine>33 rue Maynard</addrLine>
									<postCode>31000</postCode>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.56,136.80,311.16,12.61;1,252.84,153.36,89.82,12.61">Cross Lingual Question Answering using QRISTAL for CLEF 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D5306602566D8FD1423232ECC4DC59D5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>QRISTAL <ref type="bibr" coords="1,144.12,317.65,11.72,11.04" target="#b7">[8]</ref> is a question answering system making intensive use of natural language processing both for indexing documents and extracting answers. It recently ranked first in the EQueR evaluation campaign (Evalda, Technolangue [3]). This article proposes a functional description of the system. Then, it presents our results for the CLEF 2005 campaign and a critical description of the system. QRISTAL is possibly the first Question Answering system available on the consumer market. That fact generates drastic constraints and explains the technical choices we detail here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>QRISTAL (French acronym for "Question Answering Integrating Natural Language Processing Techniques") is a cross lingual question answering system for French, English, Italian, Portuguese and Polish. It was designed to extract answers both from documents stored on a hard disk and from Web pages by using traditional search engines (Google, MSN, AOL, etc.). To our knowledge, this system is the first to be marketed for general public. We are now integrating Qristal in professional applications. For example, Qristal is currently used in the M-CAST European project of E-content (22249, Multilingual Content Aggregation System based on TRUST Search Engine). Everybody can try the Qristal technology for French at www.qristal.fr. Note that the testing corpus for the testing web page is the grammar handbook proposed at http://www.synapse-fr.com.</p><p>For each language, a linguistic module analyzes questions and searches for potential answers. For CLEF 2005, the French, English, Portuguese and Italian modules were used for question analysis. Only the French module was used for answers extraction. The linguistic modules were developed by different companies. They share however a common architecture and similar resources (general taxonomy, typology of questions and answers and terminological fields).</p><p>For French, our system is based on the Cordial technology. It massively uses NLP tools, such as syntactic analysis, semantic disambiguation, anaphora resolution, metaphor detection, handling of converses, named entities extraction as well as conceptual and domain recognition. As the product is being marketed, it required a constant optimization of the various modules so that the software remains extremely fast. Users are now accustomed to obtain something that looks like an answer within a very short time, not exceeding three seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture</head><p>The architecture of the Qristal system is described on figure <ref type="figure" coords="2,312.12,111.73,4.98,11.04">1</ref> : Figure <ref type="figure" coords="2,254.40,392.65,3.77,11.04">1</ref>. Architecture of the system Qristal is a complete engine for indexation and answers extraction. However, it doesn't index the Web. Indexing is processed only for documents based on disks. Web search uses a meta-search engine we have implemented.</p><p>Our company is responsible for the indexing process of Qristal. Moreover, it ensures the integration and interoperability between all linguistic modules. Both English and Italian modules were developed by Expert System Company. The Portuguese module was developed by the Priberam Company which also takes part in CLEF 2005 for Portuguese monolingual. The Polish module was developed by the TiP Company. These modules were developed within the European project TRUST <ref type="bibr" coords="2,292.56,498.97,11.72,11.04" target="#b8">[9]</ref> (Text Retrieval Using Semantic Technologies). Note that currently, for another European project (M-CAST, Multilingual Content Aggregation System based on Trust Search Engine), the same partners develop a client-server version of this system in order to exploit digital resources of libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multicriteria indexing</head><p>While indexing documents, the technology automatically identifies the document language of and the system calls the corresponding language module. There are as many indexes as languages identified in the corpus. Documents are treated per blocks. The size of each block is approximately 1 kilobyte. Block limits are settled on the end of sentences or paragraphs. This size of block (1 kb) appeared to be optimal during our tests. Some indexes relate to blocks like fields or taxonomy whereas other relate to words, like idioms or named entities (see figure <ref type="figure" coords="2,97.08,654.97,3.63,11.04">2</ref>).</p><p>Each linguistic module processes a syntactic and semantic analysis for each block to be indexed. It fills a complete structure of data for each sentence. This structure is passed to the general processor that uses it to increment the various indexes. Figure <ref type="figure" coords="2,231.72,702.25,4.98,11.04">2</ref> describes the linguistic processes launched while indexing, question analyzing and answer extracting. This description is accurate for the French module. Other language modules are very close to that framework but don't always include all its elements. For example, English and Italian modules do not include an indexing based on heads of derivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2. Linguistic analysis process for indexation</head><p>Texts are converted into Unicode. Then, they are divided into one kilobyte blocks. This reduces the index size as only the number of occurrences per block is stored for a given lemma. This number of occurrences is used to infer the relevance of each block while searching a given lemma in the index. In fact we here use lemmas but the system stores heads of derivation and not lemmas. For example, symmetric, symmetrical, asymmetry, dissymmetrical or symmetrize will be indexed in the same entry : symmetry.</p><p>Each text block is analyzed syntactically and semantically. Considering results of this analysis, 8 different indexes are built for:</p><p>• heads of derivation. A head of derivation can be a sense for a word. In French, the verb voler has 2 different meanings (to steal or to fly). The meaning "dérober" (to steal) will lead to vol (robbery), voleur (thief) or voleuse (female thief). The second meaning, "se mouvoir dans l'air" (to fly), will lead to vol (flight), volant (flying as an adjective), voleter ( to flutter) or envol (taking flight) and all its forms.</p><p>• proper names. If they appear in our dictionaries.</p><p>• idioms. Those idioms are listed in our idioms dictionaries. They encompass approximately 50 000 entries, like word processing, fly blind or as good as your word.</p><p>• named entities. Named entities are extracted from texts. George W. Bush or Defense Advanced Research Project Agency are named entities.</p><p>• concepts. Concepts are nodes of our general taxonomy. 2 levels of concepts are indexed. The first level lists 256 categories, like "visibility". The second level, actually the leaves of our taxonomy, lists 3387 subcategories, like "lighting" or "transparency",</p><p>• fields. 186 fields, like "aeronautics", "agriculture", etc.,</p><p>• question and answer types for categories like "distance", "speed", "definition", "causality", etc.,</p><p>• keywords of the text.</p><p>For each language, the indexing process is similar. Extracted data are the same. Thus, the handling of those data is independent of their original language. This is particularly important for cross language question answering.</p><p>For the French language, the rate of correct grammatical disambiguation (distinction between name-verbadjective-adverb) is higher than 99%. The rate of semantic disambiguation is approximately 90% for 9 000 polysemous words and approximately 30 000 senses for these words. Note that this number of senses is markedly inferior to the Larousse one (Larousse is one of the most famous French dictionaries). Note however that our idioms dictionary covers a large number of the senses mentioned in this kind of dictionaries. The indexing speed varies between 200 and 400 Mo per hour with a Pentium 3 GHz, according to the size and number of indexed files.</p><p>Indexing question types is undoubtedly one of the most original aspects of our system. While the analysis of the blocks is being made, possible answers are located. For example, a name of function for a person (like baker, minister, director of public prosecutions), a date of birth (like born on April 28, 1958), a causality (like due to snow drift or because of freezing), a consequence (like leading to serious disruption or facilitating the management of the traffic). This caused the block to be indexed like being able to provide an answer for a given question type.</p><p>Presently Building a keyword index for each text is also peculiar to our system. Dividing text into blocks made it compulsory. Isolated blocks cannot explicitly mention main subjects of the original text although sentences of these blocks relate to these subjects. The keyword index makes it possible to add contextual information about the main subjects of the text for blocks. Keywords can be a concept, a person, an event, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Answer extraction</head><p>After the user typed its question, it is syntactically and semantically analyzed by the system. Question type is inferred. We would like here to draw the attention to the fact that questions are shorter than texts. This lack of context makes the semantic analysis of the question more dubious. That's why the semantic analysis processed on the question is more comprehensive than the analysis processed on texts. Moreover, users have the possibility to interactively force a sense. This possibility, however, was not used for CLEF as the entire process was automatic. The result of the semantic analysis of the question is a weight for each sense of each word recognized as a pivot. For example, sense 1 is recognized with 20%, sense 2 with 65% and sense 3 with 15%. This weight, together with synonyms, question and answer types or concepts, is considered while searching the index. Thus all senses of a word are taken into account during the index search. This prevents from dramatic consequences due to errors in the semantic disambiguation while making the most of good analysis.</p><p>After question analysis, all indexes are searched and the best ranked blocks are analyzed again. As one can notice on figure <ref type="figure" coords="4,112.44,632.53,3.77,11.04">2</ref>, the analysis of the selected blocks is close to the analysis processed while indexing or question analyzing. On top of this "classic" analysis, a weight for each sentence is inferred. This weight is based on the number of words, synonyms and named entities found in this sentence, the presence of an answer corresponding to the question type and a correspondence between the fields and domain.</p><p>After this analysis, sentences are ranked. Then, an additional analysis is processed to extract named entities, idioms or lists that match the answer. This extraction relies on the syntactic characteristics of those groups.</p><p>For a question on a corpus located on a hard disk, the response time is approximately 3 seconds with a Pentium 3 GHz. On the Web, first answers are provided after 2 seconds. Then the system computes a progressive refining during ten seconds, according to user's parameters like the number of words, the number of analyzed pages, etc.</p><p>We tested many answer justification modules, mostly implemented from Web <ref type="bibr" coords="5,393.12,85.57,10.69,11.04" target="#b3">[4]</ref>, <ref type="bibr" coords="5,411.00,85.57,11.72,11.04" target="#b6">[7]</ref> or <ref type="bibr" coords="5,438.36,85.57,15.43,11.04" target="#b12">[13]</ref>. Our technology enables, as an option, to use such a module of justification. It consists in searching the web with the words of the question looking for potential answers the system inferred. However this process is seldom selected by users as it increases the response time of a few seconds. It was not used in CLEF 2005 either. The only justification module we used was an internal module which makes the most of the semantic information for proper names enclosed in our dictionaries. For more than 40 000 proper names, we possess information about the country of origin, the year of birth and death, the function for people, country, the area and population for a city, etc. We think this justification module is at the origin of some "unjustified" answers. As a matter of fact, it caused the system to rank first a text including the answer even if the system did not find any clear justification of that answer in the text.</p><p>For cross language question answering, English is used as pivot language. The fact that most users are only interested in documents in their own language and English motivated that choice. Thus, for cross language answering, the system processes generally only one translation. For this evaluation, both Portuguese to French and Italian to French runs required two translations: from source language to English and then from English to French. QRISTAL does not use any Web Services for translation because of response time. Only words or idioms recognized as pivots are translated.  One can notice the quality of answers is lower for cross lingual answering. In fact, the English to French run finds approximately 60% of the answers found in the French to French run. The Portuguese to French run finds 57% of the French to French run. Because of a bug we didn't manage to solve during the CLEF campaign, the Italian module did not translate pivot words to English. Thus, for the Italian to French run, results were computed only using words common in both languages. Fortunately, most proper names are identical between French and Italian. Anyway, taxonomy, question types and fields were used for this Italian to French run.</p><p>Precision and recall for NIL questions are quite poor. Nil questions are questions without any existing answer in the corpus. Our search engine was designed to find answers (!) and strategies dedicated to the detection of questions without any answer are quite unsatisfactory. Actually, the main routine in this process compares the possible date of the question and the date of text to remove texts anterior to the date of the question. Figure <ref type="figure" coords="6,101.04,588.37,4.98,11.04" target="#fig_0">4</ref> presents statistics for answers evaluated as 'R' that stands for right. But CLEF proposed two other qualifications for answers that is 'U' for unjustified and 'X' for inexact. We think 'U' and 'X' answers would be often accepted by users, even 'X' answers if they are presented with their context. For question 57 Qui est Flavio Briatore ? (Who is Flavio Briatore?), the answer provided by our system was directeur général de Benetton Formula (general manager of Benetton Formula), whereas the awaited answer was directeur général de Benetton Formula 1 (general manager of Benetton Formula 1). Likewise, for question 96 A quel parti politique Jacques Santer appartient-il ? (Which political party does Jacques Santer belong to ?), the provided answer by Qristal was Parti chrétien-social dès 1966 (Christian Social Party since 1966) whereas the awaited answer was Parti chrétien-social (Christian Social party). This lead us to consider statistics for all answers considered as "not wrong", that is right (R), unjustified (U) or inaccurate (X): This table underlines the relation between errors in translation and the performance of the multilingual system.</p><formula xml:id="formula_0" coords="6,183.36,721.45,27.00,11.04">French</formula><p>Portuguese module is rather more precise than the English module but it is penalized by the double translation, from Portuguese to English and then from English to French. For example, for question 11 (Which French Prime Minister committed suicide?) Prime and minister are translated into French as prime and ministre quite far from the correct translation that is premier ministre ! Then we had a closer look to questions where the monolingual process finds the answer but the cross language does not. This leads us to the following remark. Questions are defined by reading the corpus and, deliberately or not, people formulating questions tend to reuse words or expressions mentioned in the text of the identified answer. On one hand, this influences the capacity of the system and the importance of each module in the overall process. For example, the use of synonyms is not that important for CLEF as it normally is. On the other hand, for cross language question answering, translations can be fuzzy and potentially quite far from the targeted word or expression especially when one uses English as an intermediate language. This way, translated words are quite often different from the terms mentioned both the question and the answer.</p><p>For . But the answer provided by the French module is président du Conseil italien extracted from "Décidant finalement de renoncer à demander un vote de confiance au Parlement, le président du conseil italien, Silvio Berlusconi, a remis, jeudi 22 décembre dans l'après-midi, sa démission au président de la République, Oscar Luigi Scalfaro." One can notice that the answer has nothing to do with a ministry. Moreover, the English word resignation correctly translated into French as resignation is quite far from the word démission (abdication).</p><p>To determine the various roles of every part of our system, we disconnected some modules and measured the overall performance. With the 200 questions of CLEF, the most important module was the question and answer type extractor. Disconnecting it causes an 11.5% drop for the monolingual campaign. This module is used to index, to search the index, and to extract exact answers from blocks. Other modules have secondary roles. We noted a drop of 2% by disconnecting synonyms dictionaries and a 3% drop by giving the same weight to all pivots. Note however that main components such as the part of speech tagger are almost impossible to test separately as all the system relies on it.</p><p>Priberam, the company responsible for the Portuguese module in our engine, participated in CLEF 2005 in the Portuguese to Portuguese evaluation track. It is interesting to note that they obtained results very similar to our results for the monolingual run <ref type="bibr" coords="7,198.12,737.89,25.33,11.04">[1] [2]</ref>. This seems to objectively validate our common choices and our resulting technology that is very close to the best systems available for English which have participated for many years now in this type of evaluation via the TREC evaluation campaigns <ref type="bibr" coords="8,337.80,97.33,29.71,11.04">[6] [14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Outlines</head><p>For 4 years now we sell out our question answering system on the French market. According to user's reactions and remarks we established a list of compulsory elements for such a system not to be directly rejected by users:</p><p>• Response time must not exceed 3 seconds, and preferably 1 or 2 seconds. By any means, a first answer must be displayed within this period.</p><p>• Success rate must approach 100%. A system providing only one answer out of 3 or less is acceptable only by a reduced number of users.</p><p>• Questions like those used for TREC or CLEF represent only a part of user's questions. Users often types "why" or "how" questions. Such a system has to handle it correctly.</p><p>An 80% of correct answers for monolingual search and 60% for multilingual is a minimum to convince a large audience of the interest of question answering systems. To reach such a level of performance, while reducing response time, very few approaches are available. Unfortunately, it is not possible to use strategies based on the Web redundancy <ref type="bibr" coords="8,143.64,320.41,35.95,11.04">[10] [12]</ref>. Actually, user's search often relates to specific corpus not redundant on the Web. Moreover, parallel research strategies or parallel justification using the Web take too long and are therefore inadequate.</p><p>However, we identified the following approaches to improve our technology:</p><p>• A general improvement of our resources. Particularly the semantic disambiguation process and the translation dictionaries.</p><p>• A refinement of the typology of questions and a precise definition of named entities expected in the answer for various question types would be of higher interest.</p><p>• An improvement of the answer delimitation process would reduce the number of "inaccurate" answers.</p><p>• Handling the presentation of documents. Documents are considered as rough text, without taking into account possible tags, titles, paragraphs, bolded parts, etc.</p><p>• The use of databases for question answering. The best translation systems use translation memory. Question answering systems could use memory as well. This process would imply the construction of a database storing factual predicates. This construction could be based on an automatic analysis of the Web. We could imagine storing questions and related answers users ask the system as well. A major interest of such databases is that accessing them is very quick. Thus, by using them, the system can process an answer instantly and then search for justification -or invalidation -in documents of the corpus.</p><p>• A specific handling for questions that cannot have an answer on the Web or in the corpus. Questions like What is the weather forecast for tomorrow? cannot be addressed by question answering systems based on corpus analysis. For those questions, you would rather send a well formatted request to weather forecast dedicated web sites than to general search engines. Same remarks concerning questions about itineraries or prices for which specialized sites are more likely to generate an appropriate answer, if there is at least one answer to the question, of course !.</p><p>As treating non factual question is of higher interest we encourage evaluation campaigns such as CLEF to take them into account <ref type="bibr" coords="8,144.60,678.01,10.69,11.04" target="#b4">[5]</ref>. Note that the evaluation of those questions and answer pairs is more difficult.</p><p>Analyzing the questions and user's reactions in front of provided answers underlines the gap between the words used in the question and those mentioned in the expected answer. This especially occurs for questions like what do clients think about the Large hotel in Berlin? or which guarantees offer my blue card? Lexical (synonyms, analogies) or thematic assistances could help the user to better find expected answers.</p><p>More technically, the M-CAST European project (http://www.m-cast.infovide.pl) in which our company as well as our Italian, Portuguese and Polish partners take part will enable us to test our system in a professional environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>QRISTAL is the first question answering system marketed for general public and professionals. Results obtained for the EQUER evaluation, largely confirmed by those we obtained during CLEF evaluation, show that the intensive use of NLP technologies for analyzing the question, indexing texts and extracting answers leads to a good outcome. The fact that Priberam, which uses the same framework, obtained very close results for Portuguese encourages this assertion.</p><p>Our results, to be compared to the best international prototypes, can however be regarded as insufficient, particularly when it comes to Web searching. As we use a meta-search engine, that is an engine using the results of other search engines of the Web, we have no control over the indexation process. Moreover, downloading pages provided by Web search engines is time consuming and therefore limited. Thus, Web searching with our present technology is not as accurate as corpus searching.</p><p>One can assume that present boolean search engines will be replaced by NLP systems in several years. Nevertheless, demonstrating the advantages of such systems and revealing present weaknesses of boolean search engines will take quite a long time, partly because experts in this domain often are Boolean search experts!</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,190.20,423.01,214.90,11.04"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4 : Results of our 4 runs for each question type</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,70.92,129.00,453.48,251.16"><head></head><label></label><figDesc></figDesc><graphic coords="2,70.92,129.00,453.48,251.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,70.92,85.08,453.36,275.64"><head></head><label></label><figDesc></figDesc><graphic coords="3,70.92,85.08,453.36,275.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,70.92,256.45,453.49,124.80"><head></head><label></label><figDesc>, our question typology includes 86 types of questions. Those types are divided into two subcategories: factual types and non factual types. Factual types are dimension, surface, weight, speed, percentage, temperature, price, number of inhabitants or work of art. Nonfactual types are form, possession, judgement, goal, causality, opinion, comparison or classification. For the EQueR evaluation [3] [8], 492 questions out of 500 were classified according to this typology with only 6 errors. For CLEF 2005, results were as follows:</figDesc><table coords="4,74.40,330.73,427.66,29.28"><row><cell></cell><cell>French</cell><cell>English</cell><cell>Italian</cell><cell>Portuguese</cell></row><row><cell>Good choice</cell><cell>95.5 %</cell><cell>87.0 %</cell><cell>74.5 %</cell><cell>91.5 %</cell></row></table><note coords="4,201.72,370.21,191.79,11.04"><p>Figure 3. Success rate for question type analysis</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,70.92,312.46,453.56,434.19"><head>3 Results for CLEF 2005 QRISTAL</head><label></label><figDesc>was evaluated for CLEF 2005 for French to French, English to French, Portuguese to French and Italian to French. That is 1 monolingual and 3 multilingual campaigns. For each one of these tasks, we processed only one run. Note that results obtained in CLEF 2005 could have been obtained with the commercial version of our Qristal software, in the version of May 2005. Results of the general task For French to French, these results are better than those we obtained for the EQueR campaign. That is 64% for CLEF 2005 and 52% for EQueR 2004. Compared to EQueR, CLEF proposed additional difficulties as an exact answer is required. Only a passage of 50 maximum characters was asked for EQueR. On the contrary it seems the level of difficulty of the questions of CLEF is lower than those of EQueR which included for example research of lists as answers. This overall improvement of our performances validates the developments we did last year and particularly for the extraction of definitions.</figDesc><table coords="5,130.68,407.25,336.57,256.84"><row><cell>64% French-French accuracy 0% 10% 20% 30% 40% 50% 60% 70% Overall Factoid questions Figure 3. Results per category are as follows: 39,50% English-French 86,0% 0,0% 90,0% 100,0% Définition questions 64,0% 72,0 % 80,0% 68,0% 70,0% 59,2% 54,0% 60,0% 39,5 % 30,3 % 10,0% 25,5% 15,0% 20,0% 30,0% 26,7% 50,0% 36,5% 40,0%</cell><cell>25,50% Italian-French Portuguese-36,50% French Temporally restricted factoid questions NIL precision NIL recall 46,7% 0,23 0,25 22,6 % 0,14 0,30 23,3% 0,07 0,15 0,13 20,0% 0,45</cell><cell>Right French-French English-French Portuguese-French Italian-French</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,74.40,721.45,436.50,29.28"><head></head><label></label><figDesc>The difference between Italian and both English and Portuguese was explained before. It is due to the fact that no translation is processed from Italian to English. For the other language pairs, we tried to determine the reasons of that gap. Thus, we scanned all translations and list words that had inadequate or inexistent translations. Results of this work are presented in the following table.</figDesc><table coords="6,74.40,721.45,436.50,29.28"><row><cell></cell><cell></cell><cell>English-French</cell><cell>Portuguese-French</cell><cell>Italian-French</cell></row><row><cell>Total of pivot words</cell><cell></cell><cell>623</cell><cell>701</cell><cell>769</cell></row><row><cell cols="2">Not translated from source language to English</cell><cell></cell><cell>47</cell><cell>451</cell></row><row><cell cols="2">Not translated from English to French</cell><cell>4</cell><cell>6</cell><cell></cell></row><row><cell cols="2">Badly translated from English to French</cell><cell>55</cell><cell>63</cell><cell></cell></row><row><cell cols="2">Total for translation mistakes</cell><cell>59 (9.5%)</cell><cell>116 (16.5 %)</cell><cell>451 (58.6 %)</cell></row><row><cell></cell><cell>-French</cell><cell>English-French</cell><cell>Portuguese-French</cell><cell>Italian-French</cell></row><row><cell>Not wrong (R+U+X)</cell><cell>138 (69.0%)</cell><cell>92 (46.0%)</cell><cell>85 (42.5%)</cell><cell>64 (32.0 %)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors thank <rs type="person">Bruno Wieckowski</rs> and all engineers and linguists that took part in the development of QRISTAL. They also thank the <rs type="institution">Italian company Expert System</rs> and the <rs type="institution">Portuguese company Priberam</rs> for allowing them to use their modules for question analysis in English, Italian and Portuguese. They finally thank the <rs type="institution">European Commission</rs> which supported and still supports our development efforts through <rs type="projectName">TRUST</rs> and <rs type="projectName">M-CAST</rs> projects.</p><p>Last but not least, authors thank <rs type="person">Carol Peters</rs>, <rs type="person">Alessandro Vallin</rs>, <rs type="person">Danilo Giampiccolo</rs> and <rs type="person">Christelle Ayache</rs> for the remarkable organization of CLEF.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_SpWDESm">
					<orgName type="project" subtype="full">TRUST</orgName>
				</org>
				<org type="funded-project" xml:id="_Ds3aQtE">
					<orgName type="project" subtype="full">M-CAST</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,87.84,536.53,436.54,11.04;9,70.92,548.29,453.42,11.04;9,70.92,560.19,46.53,10.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,392.04,536.53,132.34,11.04;9,70.92,548.29,162.04,11.04">Design &amp; Implementation of a Semantic Search Engine for Portuguese</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,241.32,548.43,283.02,10.89;9,70.92,560.19,42.30,10.89">Proceedings of the Fourth Conference on Language Resources and Evaluation</title>
		<meeting>the Fourth Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.36,583.81,437.10,11.04;9,70.92,595.57,453.45,11.04;9,70.92,607.33,31.65,11.04" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,443.76,583.81,80.70,11.04;9,70.92,595.57,137.08,11.04">Priberam&apos;s question answering system for Portuguese</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,217.32,595.71,194.16,10.89">Working Notes for the CLEF 2005 Workshop</title>
		<meeting><address><addrLine>Wien, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005. 21-23 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,631.09,434.46,11.04;9,70.92,642.85,424.53,11.04" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,278.52,631.09,245.94,11.04;9,70.92,642.85,66.38,11.04">Campagne d&apos;évaluation EQueR-EVALDA : Évaluation en question-réponse</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vilnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,144.00,642.99,23.80,10.89">TALN</title>
		<editor>
			<persName><forename type="first">Tutoriels</forename><surname>Ateliers</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="63" to="72" />
			<date type="published" when="2005">2005. 2005. juin 2005</date>
			<pubPlace>Dourdan, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,86.64,666.61,437.73,11.04;9,70.92,678.37,375.33,11.04" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,327.48,666.61,192.32,11.04">Exploiting Redundancy in Question Answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,70.92,678.51,322.41,10.89">Proceedings of 24th Annual International ACM SIGIR Conference (SIGIR 2001)</title>
		<meeting>24th Annual International ACM SIGIR Conference (SIGIR 2001)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="358" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,85.68,702.13,438.70,11.04;9,70.92,713.89,176.73,11.04" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,163.08,702.13,361.30,11.04;9,70.92,714.03,50.78,10.89">L&apos;évaluation des systèmes de question-réponse, Évaluation des systèmes de traitement de l&apos;information</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Lavoisier</publisher>
			<biblScope unit="page" from="77" to="98" />
		</imprint>
		<respStmt>
			<orgName>TSTI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,86.04,85.57,438.42,11.04;10,70.92,97.33,453.49,11.04;10,70.92,109.23,104.61,10.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,461.16,85.57,63.30,11.04;10,70.92,97.33,266.49,11.04">Answer Mining by Combining Extraction Techniques with Abductive Reasoning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bensley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,346.20,97.47,178.21,10.89;10,70.92,109.23,76.24,10.89">Proceedings of The Twelfth Text Retrieval Conference (TREC</title>
		<meeting>The Twelfth Text Retrieval Conference (TREC</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.84,132.85,436.64,11.04;10,70.92,144.61,426.33,11.04" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,448.44,132.85,76.04,11.04;10,70.92,144.61,90.26,11.04">The University of Amsterdam at QALEF</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>De Rijke M</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,188.88,144.75,164.73,10.89">Working Notes of the Workshop of CLEF</title>
		<meeting><address><addrLine>Bath</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">2004. 2004. 2004. september 2004</date>
			<biblScope unit="page" from="15" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,85.56,168.37,438.81,11.04;10,70.92,180.13,246.57,11.04" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,228.12,168.37,173.89,11.04">QRISTAL, système de Questions-Réponses</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Seguela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,409.44,168.51,23.80,10.89">TALN</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="53" to="62" />
			<date type="published" when="2005">2005. 2005. juin 2005</date>
			<pubPlace>Dourdan, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,85.68,203.89,438.78,11.04;10,70.92,215.65,453.42,11.04;10,70.92,227.41,157.05,11.04" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,344.88,203.89,179.58,11.04;10,70.92,215.65,228.20,11.04">Multilingual Semantic and Cognitive Search Engine for Text Retrieval Using Semantic Technologies</title>
		<author>
			<persName coords=""><forename type="first">Laurent</forename><forename type="middle">D</forename><surname>Varone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fuglewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,307.08,215.79,217.26,10.89;10,70.92,227.55,94.31,10.89">First International Workshop on Proofing Tools and Language Technologies</title>
		<meeting><address><addrLine>Patras, Grèce</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,94.08,251.17,430.26,11.04;10,70.92,262.93,403.26,11.04" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,342.60,251.17,181.74,11.04;10,70.92,262.93,139.91,11.04">Is It the Right Answer? Exploiting Web Redundancy for Answer Validation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Prevete R</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,217.44,263.07,205.51,10.89">Proceedings of the 40th Annual Meeting of the ACL</title>
		<meeting>the 40th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="425" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.48,286.69,433.91,11.04;10,70.92,298.45,453.46,11.04;10,70.92,310.21,221.25,11.04" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,123.72,298.45,297.21,11.04">Overview of the CLEF 2004 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Erbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>De Rijke M</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sutcliffe R</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,430.80,298.59,93.58,10.89;10,70.92,310.35,77.61,10.89">Working Notes of the Workshop of CLEF</title>
		<meeting><address><addrLine>Bath</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">2004. 2004. september 2004</date>
			<biblScope unit="page" from="15" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.04,333.97,432.33,11.04;10,70.92,345.73,75.57,11.04" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,170.40,333.97,205.88,11.04">From Document Retrieval to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<idno>2003-4</idno>
	</analytic>
	<monogr>
		<title level="s" coord="10,385.32,334.11,103.59,10.89">ILLC Dissertation Series</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>ILLC</publisher>
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.64,369.49,431.84,11.04;10,70.92,381.25,453.46,11.04;10,70.92,393.01,221.25,11.04" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,258.00,369.49,266.48,11.04;10,70.92,381.25,349.36,11.04">Experiments on Robust NL Question Interpretation and Multilayered Document Annotation for a Cross-Language Question/Answeringg System</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,430.44,381.39,93.94,10.89;10,70.92,393.15,77.61,10.89">Working Notes of the Workshop of CLEF</title>
		<meeting><address><addrLine>Bath</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">2004. 2004. september 2004</date>
			<biblScope unit="page" from="15" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.84,416.77,430.50,11.04;10,70.92,428.53,221.01,11.04" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,210.72,416.77,248.62,11.04">Overview of the TREC 2003 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="middle">M</forename><surname>Voorhees E</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec12/t12_proceedings.html" />
	</analytic>
	<monogr>
		<title level="j" coord="10,469.92,416.77,19.85,11.04">NIST</title>
		<imprint>
			<biblScope unit="page" from="54" to="68" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
