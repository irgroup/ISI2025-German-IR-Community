<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,127.58,148.72,347.97,15.51;1,189.22,170.63,224.61,15.51">Overview of the CLEF 2005 Multilingual Question Answering Track</title>
				<funder ref="#_d34erzH">
					<orgName type="full">Spanish ment</orgName>
				</funder>
				<funder ref="#_ghuKgmh">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_hqxBF54 #_WxggrwN #_EFj4aSP #_xV93pgf #_z9aPQyr #_thhmscV #_WKQZ6vV #_Esu5kMZ #_ppHqXvj">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2005-09-09">September 9, 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,104.72,204.10,76.24,9.96;1,184.29,202.73,1.36,6.97"><forename type="first">Alessandro</forename><surname>Vallin</surname></persName>
							<email>vallin@celct.it.</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CELCT</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">UNED</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,212.20,204.10,79.36,9.96;1,294.87,202.73,1.83,6.97"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
							<email>magnini@itc.it.</email>
							<affiliation key="aff1">
								<orgName type="institution">ITC-Irst</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.79,204.10,85.70,9.96;1,411.82,202.73,1.83,6.97"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
							<email>giampiccolo@celct.it.</email>
							<affiliation key="aff2">
								<orgName type="institution">CELCT</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="institution">DFKI</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,439.74,204.10,52.45,9.96;1,495.50,202.73,1.83,6.97"><forename type="first">Lili</forename><surname>Aunimo</surname></persName>
							<email>aunimo@cs.helsinki.fi.</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Helsinki</orgName>
								<address>
									<settlement>Finnland</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,111.45,218.05,76.45,9.96;1,191.23,216.67,2.47,6.97"><forename type="first">Christelle</forename><surname>Ayache</surname></persName>
							<email>ayache@elda.org.btb</email>
							<affiliation key="aff4">
								<orgName type="department">ELDA/ELRA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,220.29,218.05,63.84,9.96"><forename type="first">Petya</forename><surname>Osenova</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CELCT</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">UNED</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.51,218.05,60.56,9.96;1,380.41,216.67,1.36,6.97"><forename type="first">Anselmo</forename><surname>Peas</surname></persName>
							<email>anselmo@lsi.uned.es.</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CELCT</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">UNED</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,409.47,218.05,75.99,9.96;1,488.78,216.67,1.83,6.97"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ITC-Irst</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,149.34,232.00,79.27,9.96;1,231.94,230.62,1.83,6.97"><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
							<email>bogdan.sacaleanu@dfki.de</email>
							<affiliation key="aff1">
								<orgName type="institution">ITC-Irst</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CELCT</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="institution">DFKI</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.45,232.00,57.75,9.96"><forename type="first">Diana</forename><surname>Santos</surname></persName>
							<email>diana.santos@sintef.no.</email>
							<affiliation key="aff2">
								<orgName type="institution">CELCT</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="institution">DFKI</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.06,232.00,72.67,9.96"><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
							<email>richard.sutcliffe@ul.ie.</email>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">U niversityof Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,127.58,148.72,347.97,15.51;1,189.22,170.63,224.61,15.51">Overview of the CLEF 2005 Multilingual Question Answering Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2005-09-09">September 9, 2005</date>
						</imprint>
					</monogr>
					<idno type="MD5">80D4695200DC2295008A5C326A03B469</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>I.2 [Artificial Intelligence]: I.2.7 Natural Language Processing Measurement, Performance, Experimentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The general aim of the third CLEF Multilingual Question Answering Track was to set up a common and replicable evaluation framework to test both monolingual and cross-language Question Answering (QA) systems that process queries and documents in several European languages. Nine target languages and ten source languages were exploited to enact 8 monolingual and 73 cross-language tasks. Twenty-four groups participated in the exercise.Overall results showed a general increase in performance in comparison to last year. The best performing monolingual system irrespective of target language answered 64.5% of the questions correctly (in the monolingual Portuguese task), while the average of the best performances for each target language was 42.6%. The cross-language step instead entailed a considerable drop in performance. In addition to accuracy, the organisers also measured the relation between the correctness of an answer and a system's stated confidence in it, showing that the best systems did not always provide the most reliable confidence score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The CLEF QA evaluation campaign conducted in 2005 <ref type="foot" coords="2,330.41,131.90,3.97,6.97" target="#foot_0">1</ref> was the result of the experience acquired during the two previous campaigns and of the proposals suggested in last year's workshop in order to make the track more challenging and realistic. At a first look one realizes that over the years the series of QA competitions at CLEF has registered a steady increment in the number of participants and languages involved, which is particularly encouraging as multilinguality is one of the main characteristic of these exercises. In fact, in the first campaign, which took place in 2003, eight groups from Europe and North America participated in nine tasks, three monolingual -Dutch, Italian and Spanish-and five bilingual, where questions were formulated in five source languages -Dutch, French, German, Italian-and answer were searched in an English corpus collection. In 2004 eighteen groups took part to the competition, submitting 48 runs. Nine source languages -Bulgarian, Dutch, English, Finnish, French, German, Italian, Portuguese and Spanish-and 7 target languages -all the source languages but Bulgarian and Finnish, which had no corpus available-were considered in the task. In 2005 the number of participants rose to twenty-four, 67 runs were submitted, and 10 source languages -the same as those used in the previous year plus Indonesian-and 9 source languages -the same used as sources, except Indonesian which had no corpus available-were exploit in 8 monolingual and seventy-three cross-language tasks. Moreover, some innovation was introduced concerning the type of questions proposed in the exercise and the metrics used in the evaluation. Despite the expectation of some the organisers for more radical innovations were partially disappointed, this edition of QA@CLEF was altogether successful and can be considered a good starting point for next campaigns. A preliminary overview of the 2005 QA track is presented in this paper, explaining more in details the procedure followed to build the test sets and providing a preliminary analysis of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks</head><p>The tasks proposed in 2005 QA campaign were characterised by a basic continuity with what had been done in 2004. In fact, to the demand for more radical innovation a more conservative approach was preferred, as most organizers opted to further investigate the procedures consolidated in the last two campaigns before moving to the next stage. As a matter of fact, the task remained basically the same as that proposed in 2005, although some minor changes were actually introduced, i.e. a new type of questions, and two new measures, namely K1 measure and r value. Both question type and measure were borrowed from the Spanish pilot task proposed at CLEF 2004 <ref type="bibr" coords="2,113.25,534.74,9.96,9.96" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Languages</head><p>Ten source languages -Bulgarian, Dutch, English, Finnish, French, German, Italian, Portuguese, Spanish and, as an experiment, Indonesian-and 9 target languages -all the source languages except Indonesian-were considered at the 2005 CLEF QA track. Eighty-one tasks were setup, 8 monolingual -Bulgarian, Dutch, English, Finnish, French, German, Italian, Portuguese, Spanishand 73 bilingual. In this way, all the possible combinations between source and target languages were exploited, but for two exceptions: Indonesian, being included in a cross-language QA competition, was used only as a source in the Indonesian-English task, meanwhile the monolingual English task was discarded as it has been abundantly tested in TREC campaigns, according to the decision taken in the previous competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Evaluation Exercise</head><p>As in the previous campaign, 200 questions were provided as an input in all tasks, and exact answer-strings were required as an output. The target corpora in all the languages were collections of newspapers and news agencies' articles, whose texts had been SGML tagged. Each document had a unique identifier (docid) that systems had to return together with the answer, in order to support it. The corpora, released by ELRA/ELDA, were large, unstructured, opendomain text collections. Although the numbers of questions was the same as last year, there were changes regarding the type of questions and their distribution. Meanwhile How-and Object questions were not included in 2005 task, since they were considered particularly problematic in the evaluation phase, a new subtype of factoid questions was introduced, called temporally restricted questions, which constrained by either an event -e.g. Who was Uganda's President during Rwanda's war? -, a date -e.g. Which Formula 1 team won the Hungarian Grand Prix in 2004? -or a period of time-e.g. Who was the President of the European Commission from 1985 to 1995?. Up to 30 temporally restricted questions could be included in each task. As far as the three major type of questions -Factoids (F), Definition (D) and NIL (N)-are concerned, the breakdown, both suggested and real, is shown in Table <ref type="table" coords="3,383.95,309.17,3.87,9.96" target="#tab_0">1</ref>. In order to increase the overlap between the test sets of different target languages, this year a certain number of topics were assigned to each language and a particular effort was made in order to get general questions, which could easily find an answer also in the other corpora. As a result, no question was actually answered in all 9 languages, but the inter-language partial overlap was increased anyway, as 21 questions appeared in 5 target languages, 66 questions in 4 target languages, 159 questions in 3 target languages and 265 in 2 target languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Test Set Preparation</head><p>The procedure for question generation was the same as that adopted in the previous campaigns. Nine groups were involved in the generation, translation and manual verification of the questions: the Bulgarian Academy of Science, Sofia, Bulgaria (CLPP) was in charge for Bulgarian; the Deutsches Forschungszentrum fr Knstliche Intelligenz Saarbrcken, Germany, (DFKI) for German; the Evaluations and Language Resources Distribution Agency Paris, France(ELRA/ELDA) for French; the Center for the Evaluation of Language and Communication Technologies Trento, Italy (CELCT) for Italian; Linguateca, Oslo (Norway), Braga, Lisbon&amp; Porto for Portuguese; the Universidad Nacional de Educacin a Distancia Madrid, Spain (UNED) for Spanish, the University of Amsterdam, The Netherland for Dutch; the University of Helsinki, Finland for Finnish; the University of Limerick, Ireland for English; and the Department of Computer Science of University of Indonesia joined the activity translating 200 English questions into Indonesian, in order to set up the cross-language Indonesian-English task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Generation</head><p>The questions in the test sets addressed large open domain corpora, mostly represented by the same comparable document collections used last year: NRC Handelsblad <ref type="bibr" coords="4,415.63,141.79,97.36,9.96">(years 1994 and 1995)</ref> and Algemeen <ref type="bibr" coords="4,153.33,153.76,81.12,9.96">Dagblad (1994 and</ref><ref type="bibr" coords="4,237.33,153.76,23.78,9.96">1995)</ref> for Dutch; Los Angeles Times (1994) and Glasgow Herald (1995) for English; Le Monde (1994) and SDA <ref type="bibr" coords="4,301.68,165.71,77.90,9.96">French (1994 and</ref><ref type="bibr" coords="4,383.57,165.71,23.78,9.96">1995)</ref> for French; Frankfurter Rundschau (1994), Der <ref type="bibr" coords="4,196.08,177.66,74.09,9.96">Spiegel (1994 and</ref><ref type="bibr" coords="4,273.85,177.66,23.78,9.96">1995)</ref> and SDA German <ref type="bibr" coords="4,384.98,177.66,71.00,9.96">(1994 and 1995)</ref> for German; La Stampa (1994) and SDA <ref type="bibr" coords="4,217.84,189.62,77.18,9.96">Italian (1994 and</ref><ref type="bibr" coords="4,299.05,189.62,23.78,9.96">1995)</ref> for Italian; <ref type="bibr" coords="4,378.41,189.62,90.84,9.96">PUBLICO(1994 and</ref><ref type="bibr" coords="4,473.27,189.62,23.78,9.96">1995)</ref> for <ref type="bibr" coords="4,90.00,201.57,69.10,9.96">Portuguese and</ref><ref type="bibr" coords="4,164.04,201.57,70.97,9.96">EFE (1994 and</ref><ref type="bibr" coords="4,239.96,201.57,23.78,9.96">1995)</ref> for Spanish. This year two new corpora were added, Aamulehti <ref type="bibr" coords="4,139.77,213.52,23.13,9.96">(1994)</ref><ref type="bibr" coords="4,167.52,213.52,23.13,9.96">(1995)</ref> for Finnish, and Sega and Standard for Bulgarian. Unfortunately the Bulgarian corpora dated back to 2002, so that the information contained in it was difficulty comparable with the that of the other corpora. According to the consolidate procedure, 100 questions were produced in each target language (except Indonesian), manually searching relevant documents for at least one answer. The questions were then translated into English, so that could be understood and reused by all the other groups. Answers were not translated this year, as it was a time-consuming and basically useless activity. The co-ordinators attempted to balance the difficulty the test sets according to the different answer types of the questions already used in the previous campaigns, i.e. TIME , MEASURE, PERSON, ORGANISATION, LOCATION, and OTHER. HOW and OBJECT questions were however inserted in this exercise because generate ambiguous responses, which are quite difficult to be assessed. As said, up to thirty temporally restricted questions were allowed, and were themselves classified according to the above mentioned types, ie, time, measure, etc Particular care was taken this year in choosing 10% of NIL questions. In fact, some organizers realised that in the previous campaigns NIL questions were quite easily identified by systems, as they were manually generated searching for named entities which were not in the corpora. On the contrary, this time NIL questions were selected randomly from those that seemed to have no answer in the document collections, and were double-checked Once the 900 questions were formulated in the original source languages, translated into English and collected in a common XML format, native speakers of each source language, with a good command of English were recruited to translate the English version of all the other questions trying to adhere as much as possible to the original. This process was as challenging as any translation job can be, since many cultural discrepancies and misunderstanding easily creep in. Anyway, as was already pointed out in 2004 "he fact that manual translation captured some of the cross-cultural as well as cross-language problems is good since QA systems are designed to work in the real world" <ref type="bibr" coords="4,194.98,512.40,9.96,9.96" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gold Standard</head><p>Once all the 900 questions were translated into ten source languages -the Indonesian group translated only the final 200 English question-, 100 additional questions for each target language were selected from the other source languages, so that at the end each language had 200 questions. The added questions were manually verified and searched for answers in the corpus of the respective language. The collection was called Multi9-05, and was presented in the same XML format adopted in 2004. The entire collection is made up of 205 definition questions and 695 factoid , which are quite well balanced according to their types, being divided as follows: 110 MEASURE; 154 PERSON; 136 LOCATION; 103 ORGANISATION, 107 OTHER, 85 TIME. The total number of temporally restricted questions is 149. Although this new kind of questions appeared to be quite interesting, no comprehensive analysis of the results in this group of questions have been made so far, and the experiment requires to be furtherly investigated. The Multi9-05 can now be added to the previous campaigns' collections, which already represent a useful reusable benchmark resource. The proposal to integrate the missing answers with the correct results provided by the systems during the exercise has remained unanswered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Participants</head><p>The positive trend in terms of participation registered in 2004 was confirmed in the last campaign. From the original 8 groups who participated in 2003 QA task, submitting a total of 19 runs in 9 tasks, the number of competitors has raised to twenty-four, which represent an increase of 33% respect to last year, when 18 groups took part in the exercise. The total of submitted runs was sixty-seven. All the participants in 2005 competition were from Europe, with the exception of group from University of Indonesia which tried the experimental cross-language task Indonesian-English. </p><formula xml:id="formula_0" coords="5,116.28,284.30,367.63,82.29">2 - - 1 1 - - - - - - - - - - - - DEs - - 3 2 1 1 - - - - - - - - - - - - ENs - - 3 2 3 2 - - 1 1 - - - - 1 1 ESs - - - - 1 1 13 7 - - - - - - - - - - FIs - - - - 2 1 - - 2 1 - - - - - - - - FRs - - - - 4 2 - - - - 10 7 - - - - - - INs 1 1 ITs - - - - 2 1 2 1 - - 1 1 6 3 - - - - NLs - - - - - - - - - - - - - - 3 2 - - PTs - - - - - - - - - - 1 1 - - - -<label>4 3</label></formula><p>As shown in table <ref type="table" coords="5,185.85,391.63,3.87,9.96" target="#tab_1">2</ref>, the systems were tested only against 22 of the 81 activated tasks. Monolingual English was discarded this year, as it was in last competition, because the task has been sufficiently investigated in TREC campaigns, and as far as Indonesian is concerned, only the task with English as a target was set up. The non-activated tasks are represented by a blank cell in table <ref type="table" coords="5,114.91,439.45,3.87,9.96" target="#tab_1">2</ref>. All nine monolingual tasks (in bold in the table) were tested at least by 1 systems, being French (FR) and Spanish (ES) the most chosen languages. As far as bilingual tasks are concerned, 15 participants altogether chose to test their systems in a cross-language task. English was as usual the most frequent target language, being involved in 8 cross-lingual tasks completed by 9 participants; Spanish was chosen as a target in a cross-language task by three groups, and so was French, meanwhile only one system tried a cross-language task with Portuguese (PT) as a target, i.e. EN-PT. All the other languages was not considered as a target in bilingual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The procedure adopted to assess the systems's outputs was practically the same as the last year. Participants were allowed to submit just one response per question and up to two runs per task, which were judged by human assessors according to correctness and exactness -where correctness expresses whether the answer is clear and pertinent, while exactness evaluates whether the information is either too much or too less. Like in 2004 only exact answers were allowed, and the responses were judged as Right, Wrong, ineXact or Unsupported (when the answer-string contained a correct answer but the returned docid did not support it). As a partial analysis of the inter-tagger agreement has shown, the exactness is still a major problem in evaluation, as most disagreement between judges concerns this parameter. Definition questions, which were introduced last years, and were considered particularly difficult also because they could raise problems in assessing their exactness, generally scored quite well, proving that as they are now they are less challenging than one thought. In fact, the answer often consists in the solution of an acronym, when they concern organisation, or is expressed as an apposition of the proper name, when persons are concerned. As said, the introduction of Temporal Restricted Questions hasn't been properly analysed yet. As a general remark, it must be said that their number in the test sets was probably too small to provide significative data on their impact on systems' results. Furthermore, some of them were "false temporally restricted", as said above, and a system could retrieve an answer without even considering the temporal restriction. The main measure used for the evaluation was the accuracy, i.e the fraction of right answers. The answers were returned unranked (i.e. in the same order as in the test set), but a confidence value, that could range between 0 and 1, could be added to each string and be considered to calculate an additional Confidence-weighted Score (CWS) <ref type="bibr" coords="6,345.16,183.18,116.17,9.96">[NOTA OVERVIEW 2004]</ref>. This year two additional evaluation measures, i.e. the K1 value and r coefficient, borrowed by [VEDERE NOTA], were experimentally introduced, in order to find a comprehensive measure which takes into account both accuracy and confidence. Anyway, being confidence an additional and optional value, only some systems could be assigned the CWS, and consequently the K1 and r coefficient; therefore an analysis based on this measures is not very significant at the moment. In comparison to last year, the performances of the systems in this campaign show a general improvement, although a significant variation remains among target languages. In fact, in 2004 the best performing monolingual system irrespective of target language (henceforth 'best overall') answered 45.5% of the questions correctly, while the average of the best performances for each target language (henceforth 'average of best') was 32.1%. In 2005 the best overall and average of best figures were 64.5% (in the monolingual Portuguese task)-representing an increase of 19 pointand 42.6% respectively. As far as bilingual tasks are concerned, as usual the cross-lingual step generically entailed a considerable drop in performance. In the following nine sections the results of the runs for each target language are thoroughly discussed. For each target language two kinds of results are given, summarized in two tables. One presents the overall performance, giving the number of right (R), wrong (W), inexact (X), and unsupported (U) answers; the accuracy, in general and on Factoids (F), Definitions (D) and Temporal (T); Precision (P), Recall (R) and F measure for NIL questions; and finally CWS, K1 and r of each run. The second table shows the accuracy of the systems with respect to the answer types, i.e Definition, sub-classified as Organisation (Or) and Person (Pe), and Factoid and Temporally Restricted, sub-classified as location (Lo), measure (Me), organisation (Or), other (Ot), person (Pe) and time <ref type="bibr" coords="6,153.20,446.20,16.37,9.96">(Ti)</ref>. Below each answer type, the number of posed questions of that type is shown in square brackets. The last row of the second table shows a virtual run, called Combination, in which the classification "right answer" is assigned to a question if any of the participating systems found it. The objective of this combination run is to show the potential achievement if one merged all answers and considered the set of answers right, provided one answer were right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Bulgarian as Target</head><p>For the first time Bulgarian was addressed as a target language at CLEF 2005. Thus, no comparison can be made with previous results from the same task, but some comments on the present ones are in order. This year two groups participated in monolingual evaluation tasks with Bulgarian as a target language: IRST, Trento and BTB, LML, IPP, Sofia. Two runs were submitted for Bulgarian-Bulgarian. Both results are below the desired figures (27.50 % and 18.50 % correct answers), but they outperform their own results from the last year where Bulgarian was used as a source language and English -as a target. Obviously, the Inexact and Unsupported value metrics do not have substantial impact over the final estimations. It seems that as a group the definition questions are the best assessed type (40 % and 42 %). Then come the factoid ones. The worst performance goes to the temporally restricted questions. Then, NIL questions exhibit better recall than precision. It might be explained by the fact that the systems return NIL when they are not sure in the answer. Only IRST group results provide a confidence weighted score.</p><p>It is interesting to discuss the results according to the answer types. Recall that definitions did well as a group. However, when divided further into Organization and Person types, it turns out that the Organization type was better handled by one of the participants, while the Person type was better handled by the other. From non-temporally restricted factoids Organizations and Other have been the most problematic types. From temporally restricted factoids Measure was unrecognized, but the number of these questions was not so high anyway. Person subtype was not detected as well, which is a bit surprising fact. Most of the problems concerning assessors' agreement were in one 'green area': between Wrong and Inexact. Recall that it was also a problem at CLEF 2004. Here we do not have in mind easy cases, such as: What is FARC? The system answered 'Columbia' instead of answering 'Revolutionary Armed Forces of Colombia' or at least 'Revolutionary Armed Forces'. We have in mind subtle cases as follows: (1) too general answers, but still correct (Q: What is ESA? A: 'agency' instead of '(European) space agency'), and (2) partial answers, but still correct (Q: Who was proclaimed patron of Europe by the Pope on 31 December 1980? A: 'St. Cyril' instead of 'St. Cyril and Methodius'). Under the former type we consider answers that are given only some 'top ontological' categorization. Under the latter we consider cases, in which part of the answer is presented, but the other part is missing. Very often it concerns questions of measure (Q: How much did Greenpeace earn in 1999? A: '134' instead of '$ 134 mln.'). This year for the first time Bulgarian was tested as a target language at the CLEF track. Two groups made runs on Bulgarian-Bulgarian task. The results are promising in spite of being lower than the half of the correctly recognized answers. So, we consider this a good start. The two extraction systems will be improved on the evaluation feedback. They need to handle better local contexts as well as to try to handle non-local support information. In the evaluation phase the most problematic still seems to be the definition of the Inexact answer. Inexactness exhibit gradability. In this respect it either should be defined in a more elaborate way (concerning generality and partiality, and per answer type), or there should be introduced a more objective system of final evaluation. Our suggestion is that inexact answers have to contain the head noun of the correct answer. The degree of inexactness depends on the recognized modifiers of the head. If the correct answer is a coordination, then the inexactness is determined also by presence of each coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">German as Target</head><p>There were three research groups that took part in this year's evaluation for the QA-track having German as target language. The number of total system runs submitted by the participants was six, with three runs for every of the two source languages: German and English. The results of evaluation for every participant group are shown in the tables below.</p><p>For the monolingual German runs the results for definition and temporal questions are better then those for factoid questions. As table 6 shows, within the definition questions, results are For the cross-lingual English-German runs, best results were registered for definition questions, followed by factoid questions, and with poor results by temporal questions. Again, best results for definition questions were for ORGANIZATION answer types and for factoid questions the order of accuracy remains unchanged with respect to the monolingual runs.</p><p>Results computed for a "virtual" system, through aggregation of all existing results, show an increase of almost 35% for the monolingual task, and 20% for the cross-lingual task, in accuracy over the best results achieved by participating systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">English as Target</head><p>Overall, twelve cross-lingual runs with English as a target were submitted. The results are shown in Tables <ref type="table" coords="8,132.95,594.63,4.98,9.96" target="#tab_6">7</ref> and<ref type="table" coords="8,163.95,594.63,3.87,9.96" target="#tab_7">8</ref>. The best scoring system overall was DFKI DEEN Run 1 with 25.5%. This score includes all three types of question, i.e. Factoid, Definition and Temporal. For Factoid questions alone, the highest scoring was DLTG FREN Run 1 (20.66%). For Definition questions alone, the highest scoring was DFKI DEEN Run 1 (50%). For Temporal question alone, three systems had an equal top score, DLTG FREN Run 2, IRST BGEN Run 1 and LIRE FREN Run 2 (all 20.69%). DFKI's main advantage over other systems was their ability to answer definition questions -their score of 50% was well ahead of the next best score of 38% achieved by IRST ITEN Run 1 and IRST ITEN Run 2. Last year, results were only single-judged with all answers to a given question being judged by one assessor using an adapted version of the NIST software. Four assessors each did 50 questions, there being 200 in all. Any issues found by assessors were then discussed and resolved at a series of plenary sessions. This year, all results were double-judged using the same software and with six The degree of agreement between assessors was found to range between 91.41% and 94.90%, computed as follows: For questions 1-66 there were 66 questions and 12 runs, 792 judgements in all. 68 differences were recorded, so the level of agreement is (792-68)/792, i.e. 91.41%. For questions 67-133, there were 804 jusgements with 69 differences recorded, i.e. 91.42% agreement. Finally, for questions 134-200 there were again 804 judgements with 41 differences recorded, i.e. 94.90% agreement. In almost all cases, points of disagreement could be tracked down to problematic questions which either had no clear answer (but several vague ones) or which had several possible answers depending on the interpretation of the question. Definition questions were once again included this year but a method of assessing them was not decided upon prior to the competition. In other words, participants did not really know what sort of system to build for definitions and we as assessors were unsure how to go about judging the answers. In consequence we used the same approach as last year: If an answer contained information relevant to the question and also contained no irrelevant information, it was judged R if supported, and U otherwise. If both relevant and irrelevant information was present it was judged X. Finally, if no relevant information was present, the answer was judged W. Two main types of system were used by participants, those which attempted to return an exact factoid-style answer to a question, and those which returned one or more text passages from documents in the collection. Generally, the former type of system is attempting a harder task because it is returning more concise information than is the latter type of system. For this reason, our evaluation method is designed to favour the former type. This was an arbitrary decision, taken in the absence of further guidelines. Our judgements are as accurate as we can make them within our own criteria but we should point out that different criteria could produce different results. Concerning the overall assessment process, we had no procedural difficulties as the format of the data was the same as last year and Michael Mulcahy in particular had already devoted a great deal of time to the adaptation of the software and the development of additional utilities in 2004. Also, most of the assessors were familiar both with the software and with the judgement criteria. We arrived at two conclusions during the assessment process. Firstly, the main points of difference between assessors in judging answers can be traced back to intrinsic problems associated with certain questions. In other words we need to devote more time to the problem of generating good questions which on the one hand are of the kind which potential users of our systems might pose, and on the other hand have clear answers. We should arrive at objective tests which can be applied to a candidate question and its answers to enable its suitability for use in CLEF to be assessed. Secondly, the situation in respect of definition questions was not ideal for either participants or assessors. This could affect our results for the EN target language as well as their relationship to the results for other target languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Spanish as Target</head><p>Seven groups submitted 18 runs having Spanish as target language: 13 of them had also Spanish as source language, 2 had Italian and 3 had English. Notice that is the first time that bilingual runs were submitted.</p><p>Table <ref type="table" coords="10,133.60,408.79,4.98,9.96" target="#tab_8">9</ref> shows the number of correct answers, CWS, K1 and correlation coefficient for all systems. Table <ref type="table" coords="10,157.95,420.74,9.96,9.96" target="#tab_9">10</ref> shows the number of correct answers for each type of question. Table <ref type="table" coords="10,474.63,420.74,9.96,9.96" target="#tab_10">11</ref> shows the number of correct answers for each type of temporal restriction. Table <ref type="table" coords="10,414.88,432.70,9.96,9.96" target="#tab_11">12</ref> shows the evolution of the most important criteria in the systems performance for the last three years. The virtual combination run was able to answer correctly 73.50% of the questions. The best performing system achieved an overall accuracy of 42% but it only gave a right answer for the 56% of the questions correctly answered by the combination run. Thus, we can expect improvements of the systems in a short term. As shown in Table <ref type="table" coords="10,176.56,739.47,8.48,9.96" target="#tab_9">10</ref>, systems generally behaved better with questions about definitions, loca- tions, persons and organizations. However, when the question type was measure, the accuracy tended to be lower. Indeed, this type of question has turned out to be the most difficult this year.</p><p>In the factoids without temporal restrictions, the best performing system answered correctly 29.66% of the questions, a very similar accuracy comparing with the results in 2004 (see Table <ref type="table" coords="11,106.05,430.48,8.30,9.96" target="#tab_11">12</ref>). Concerning questions with temporal restriction, the systems with the best behavior answered correctly 34.38% of the questions, a similar result comparing with overall accuracy. As shown in Table <ref type="table" coords="11,189.78,726.46,8.48,9.96" target="#tab_9">10</ref>, when considering the question type, the accuracy scores present small differences. Nevertheless, when the restriction type (date, event and period) is taken into account, the differences are more important (see Table <ref type="table" coords="12,301.34,111.46,8.30,9.96" target="#tab_10">11</ref>). It is worth mentioning that for questions restricted by event, the virtual combination run clearly outperforms individual systems separately (low overlapping on correct answers). In definition questions the best performing system obtained 80% of accuracy. The improvement is remarkable considering that in the 2004 track the best systems answered correctly 70% of the questions. Regarding NIL questions, the best systems achieved a recall of 0.80. F-measure improvements are also remarkable, with an increase of about 26% with respect to last year (0.30 in 2004 vs. 0.38 in 2005). Systems have also clearly improved their confidence self-score. While in 2004 the system with higher correlation coeficience (r) reached 0.17 <ref type="bibr" coords="12,291.90,336.50,9.96,9.96" target="#b1">[2]</ref>, in 2005 the highest r value was 0.56. As shown in Table <ref type="table" coords="12,173.49,348.45,8.48,9.96" target="#tab_11">12</ref>, the best performing systems reached and overall accuracy of 24.5%, 32.5% and 42% in 2003, 2004 and 2005, respectively (increasing +71% during the three years). In order to analyze the interannotator agreement, we have randomly selected 4 out of 18 runs which have been judged by two assessor with different levels of expertise. Most of the differences among assessors can be found when judging an answer as Right or as ineXact. In many cases, an assessor without experience assess as Right an answer that an experienced assessor would judge as ineXact. Table <ref type="table" coords="12,169.73,534.04,9.96,9.96" target="#tab_13">14</ref> shows the maximun variation of correct answers for these four runs (average = ± 2.9%). Finally we can conclude that both the improvement in systems' self-evaluation, the scores obtained by the participanting systems (73.50% in combination, 42% individually), and the systems' evolution during the last three years, let us expect a significant improvement in Spanish question answering technologies in the near future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Finnish as Target</head><p>The year 2005 was the first year when Finnish existed as a target language. Only one group submitted runs for this task, and both of the runs were monolingual. The artificial combination run presented in table <ref type="table" coords="13,188.43,250.11,9.96,9.96" target="#tab_15">16</ref> shows that the upper bound on the performance of a system that would merge the results of the existing runs and somehow select the right answers from the combined pool of candidate answers is 26.50%. This is by far the lowest monolingual combination run score among the participating languages. The next one is bulgarian with a combination score of 36.00 % (see Table <ref type="table" coords="13,149.54,297.93,3.87,9.96" target="#tab_3">4</ref>). However, when we calculate the average score for the monolingual runs of each target language, we can see that Finnish is not very far behind, for the average accuracy of the Finnish runs is 21.00%, that of the Bulgarian ones is 23.00%, that of the Italian ones is 24,08%, that of the French ones is 25,20%, and so on. The confidence scores that the systems having Finnish as target assign to the answers only very faintly reflect the assessor's opinion on the correctness of the answer, as can be seen from the correlation coefficient between the system's score and correctness (r) in Table <ref type="table" coords="13,214.45,369.66,8.48,9.96" target="#tab_14">15</ref>. The evaluation of the Finnish answers was not straightforward because the evaluation guidelines <ref type="bibr" coords="13,112.42,536.86,10.52,9.96" target="#b0">[1]</ref> don't discuss word affixes with regard to the exactness of the answers. Finnish is a highly inflecting language where each noun, for example, has 15 different cases. In addition to cases, nouns can also contain possessive suffixes and clitics. Most of the answers to the CLEF questions are noun phrases. The cases, possessive suffixes and clitics typically express meanings that are in the other target languages of the evaluation campaign expressed by separate words such as prepositions, pronouns and adverbs. Thus, one single word in Finnish may convey consederably more information than a single word in the other target languages. For example, the word talossanikin means also in my house. Our understanding of the guidelines was that the answer should be taken from text as such, without any modifications, such as lemmatization. Now, due to the rich affixing, the answer that is not lemmatized may contain additional information that disturbs the evaluator, and he is tempted to judge the answer inexact. However, judging as inexact all those answers that are not in the form required by the question could not be done, because that is not required according to the guidelines. When deciding how to assess the Finnish answers, we observed how the judgements had been done with regard to cases in the other target languages. For example, in German, the case may cause modifications in the determiner. However, those answers whose head noun is not in the nominative case even though that is the case requested by the question, are marked as correct. For example: Question: 62 D PER Wer ist Goodwill Zwelithini? Answer: R 0062 dem König der Zulus<ref type="foot" coords="14,214.94,315.22,3.97,6.97" target="#foot_1">2</ref> . Thus, we decided to judge as correct in Finnish also those answers that are not in the form required by the question. For example: Question: 65 F PER Kuka on ohjannut elokuvan Hamlet liikemaailmassa? Answer: R 0067 Mika Kaurismäen<ref type="foot" coords="14,442.71,339.14,3.97,6.97" target="#foot_2">3</ref> . In fact, most of the problematic question forms in the test set for Finnish are of the type where the answer is given in the genetive case and the case required by he question is the nominative case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">French as Target</head><p>Seven research groups took part in evaluation tasks using French as target language: Synapse Développement (France), CEA-LIST/LIC2M (France), LIMSI-LIR (France), Université de Nantes, LINA (France), Helsinki University (Finland), Universitat Politècnica de València, UPV (Spain) and TOVA, a joint system between UPV and the Instituto Nacional de Astrofísica Óptica y Electrónica (Mexico). All participating groups took part in the monolingual task: four groups submitted one run and three groups submitted two runs FR-FR. Only Synapse Développement took part in the bilingual tasks. This group submitted three runs, one run per source language: Italian, English and Portuguese. Table <ref type="table" coords="14,261.33,494.39,9.96,9.96" target="#tab_16">17</ref> shows the results of the assessment of the thirteen submitted runs. This year, many groups participated in the Question Answering tasks with French as a target. It appears that the number of participants for the French task has increased significantly: seven this year as opposed to one last year. The best results were obtained by Synapse Développement for one of the monolingual runs (syna051frfr). This group ranked 2nd and 3rd in the two English-French and Portuguese-French runs which is better than all the other monolingual French runs. The two monolingual runs by the spanish TOVA group reached the 4th and 5th positions.</p><p>The correct answers given for all the runs are presented in table 18, sorted by type of answer (location, measure, organization, etc.). The results show the limits of the system developed by Synapse Développement, which obviously lie in factoid-other (9/20), factoid-measure (10/20) and factoid-time (11/20), whereas results are much better for definition and factoid-person questions. The aim of the virtual run called combination is to provide an upperbound on the possible performance of a system that would merge the existing runs and somehow select the right answers from the combined pool of candidate answers. The best run (syna051frfr) is able to supply 76.19% of the correct answers of combination. This ratio could be enhanced if results for factoid-measure or factoid-time questions were better. The main problem encountered during the assessment of answers was related to the temporally restricted factoid questions. This year and for the first time in CLEF this kind of questions was included in the test sets. We thought that the generation of this kind of questions would be relatively easy, but did not foresee that the assessment on those questions would be so difficult.</p><p>In fact, many temporally restricted factoid questions have not been built properly as there was no logic of restriction at all. The question "In which famous capital was the Eiffel Tower built in 1889?" is a good example. Here, "in 1889" is a redundant information rather than a temporally restriction and will be ignored by the system: the correct answer returned with a document associating the Eiffel Tower to Paris will be a right answer even if it does not specify that the Eiffel Tower was built in 1889.</p><p>Therefore, from the beginning of the assessment phase on, many questions arise such as "Should the date be included in the document joined to the answer?", "Should all the items included in the question be found in the document in order to consider the answer as correct?". Now we know how to handle those temporally restricted factoid questions and such problems should not occur next year. This year, as far as French language is concerned, the best system obtained very good results : 128 correct answers out of 200. In all the QA@CLEF tracks, these are the best results ever obtained for the French used as target language. Moreover, we could see a growing interest in Question Answering from the European research community: the QA@CLEF-2005 attracted more participants in evaluation tasks using French as target language than the previous editions. In addition, the benchmark resources built for these evaluations contributed to the development and the improvment of systems, and could be used again as training resources in the next edition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Italian as Target</head><p>Three groups participated in the Italian monolingual task, and no one in the other bilingual tasks with Italian as target. A total of six runs were submitted, two each research group: ITC-Irst, the Universidad Politécnica de Valencia (UPV) and a joint experiment by UPV and the Mexican INAOE (Instituto Nacional de Astrofísica, Óptica y Electrónica). As table 19 shows the best system (the one developed by UPV and INAOE) answered correctly to 27.5% of the questions, and the other two systems achieved similar results.</p><p>In 2004 two teams had participated in the Italian monolingual task, submitting a total of 3 runs. The best performer had an overall accuracy of 28%, while the average performance was The manual assessment procedure was the same as it was in 2004. Two assessors had a brief training session (based on the 2004 submissions) that aimed at making them familiar with the evaluation tool interface and at solving preliminary doubts. Both assessors judged all the six runs and then the answers with different judgments were double-checked and received a third, final judgment. Table <ref type="table" coords="16,165.41,609.33,9.96,9.96" target="#tab_20">21</ref> gives the number of different judgments per run and the inter-assessor kappa coefficient, which is quite high (average value is 0.874).  Table <ref type="table" coords="18,133.42,448.64,9.96,9.96" target="#tab_3">24</ref> presents the five runs. This year there was a first crosslingual run, from English to Portuguese, by Esfinge, with significantly worse results than the monolingual runs, as might be expected. As to the monolingual results, the Esfinge system showed some improvement as compared to last year, although its best run was still unable to equal PTUE system's score. PTUE's results, however, were slightly worse than last year's. The clear winner in all respects was Priberam's system, which, in fact, was the best participating system in the whole QA@CLEF. Table <ref type="table" coords="18,118.22,520.37,9.96,9.96" target="#tab_22">25</ref> breaks down the correct answers by kind of entity, as well as provides a combination score: a question is considered answered if any system has been able to provide a right answer (assuming that a user would be able to check easily, in case of multiple answers, the right one). In this, we see that Portuguese language ranks as second, after French. Another relevant remark is that definitions do not seem to be more difficult on average than factoid questions, as was the case last year. We believe, however, that this is due to a considerable simplification of precisely what "definition questions" are, where they boil down to mainly ask for a person's profession or title. We did some further analysis of the results in order to have other measures of confidence in the systems, which are displayed in table 26. We looked specifically at (i) the cases where no answer was given (null answer), which keep the user in a state of ignorance, no matter the system was right in providing the null answer or wrong because it could not find it; (ii) the cases where any user could at once see the answer was rubbish (rubbish); and (iii) the cases where the wrong answers could be misleading (dangerous). Of course it depends on the ignorance of the questioner, and we were very conservative in imagining total ignorance. Probably most of the "dangerous" questions would at once be spotted as system's mistakes by an ordinary useror at least arise some suspicion. The results show that the PTUE system is both the most reliable (less non-NIL wrong answers) and the most conservative system (most empty answers), the more "dangerous" one being Esfinge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper presented the multilingual Question Answering evaluation campaign organized at CLEF 2005. QA@CLEF considerably increased both in number of participants -we are now closer to the Question Answering track at TREC-and also in the number of languages involved. It is also relevant that this year we were able to activate a task with Bulgarian as a target, a language of a new EU member country. A pilot cross-language task with Indonesian as source and English as target has been also activated. Being the organization of the task at its third year, is now well tested, although involving nine different Institutions of as many different countries, and showed to be able to support the high number of exchanges required by the organization of the task. This is particularly significative considering that all the organizations involved in QA@CLEF guarantee their support on a voluntary basis. The increased number of participants allowed to carried out a number of interesting comparisons among systems participating at the same task (this was one of the drawback of the 2004 campaign). In addition, it is worth to mention that Question Answering techniques for European languages, being mainly based on NLP tools and resources for the respective languages, demand for better tool and resources. In a cross-language perspective the integration of such resources is also crucial. Finally, having (at least partially) achieved its goal to promote Question Answering for European languages, there is now a quite large scientific community in Europe on Question Answering, QA@CLEF is now ready to propose its own view on QA, designing a roadmap for the next years multilingual QA systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,164.31,339.95,274.39,125.94"><head>Table 1 :</head><label>1</label><figDesc>Test set breakdown according to question type</figDesc><table coords="3,232.43,375.23,138.12,90.66"><row><cell></cell><cell>F</cell><cell>D</cell><cell>T</cell><cell>NIL</cell></row><row><cell>suggested</cell><cell>[120]</cell><cell>[50]</cell><cell>[30]</cell><cell>[20]</cell></row><row><cell>BG</cell><cell>116</cell><cell>50</cell><cell>34</cell><cell>22</cell></row><row><cell>DE</cell><cell>135</cell><cell>42</cell><cell>23</cell><cell>20</cell></row><row><cell>EN</cell><cell>121</cell><cell>50</cell><cell>29</cell><cell>20</cell></row><row><cell>ES</cell><cell>118</cell><cell>50</cell><cell>32</cell><cell>20</cell></row><row><cell>FI</cell><cell>111</cell><cell>60</cell><cell>29</cell><cell>20</cell></row><row><cell>FR</cell><cell>120</cell><cell>50</cell><cell>30</cell><cell>20</cell></row><row><cell>IT</cell><cell>120</cell><cell>50</cell><cell>30</cell><cell>20</cell></row><row><cell>NL</cell><cell>114</cell><cell>60</cell><cell>26</cell><cell>20</cell></row><row><cell>PT</cell><cell>135</cell><cell>42</cell><cell>23</cell><cell>18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,116.28,231.88,368.45,59.40"><head>Table 2 :</head><label>2</label><figDesc>Runs and Participants</figDesc><table coords="5,116.28,267.16,368.45,24.11"><row><cell></cell><cell cols="2">BGt</cell><cell cols="2">DEt</cell><cell cols="2">ENt</cell><cell cols="2">ESt</cell><cell></cell><cell>FIt</cell><cell cols="2">FRt</cell><cell></cell><cell>ITt</cell><cell cols="2">NLt</cell><cell cols="2">PTt</cell></row><row><cell></cell><cell>R</cell><cell>P</cell><cell>R</cell><cell>P</cell><cell>R</cell><cell>P</cell><cell>R</cell><cell>P</cell><cell>R</cell><cell>P</cell><cell>R</cell><cell>P</cell><cell>R</cell><cell>P</cell><cell>R</cell><cell>P</cell><cell>R</cell><cell>P</cell></row><row><cell>BGs</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,95.98,118.37,425.41,64.98"><head>Table 3 :</head><label>3</label><figDesc>Results in the tasks with Bulgarian as target</figDesc><table coords="7,95.98,143.69,425.41,39.65"><row><cell></cell><cell></cell><cell>Right</cell><cell>W</cell><cell>X</cell><cell>U</cell><cell></cell><cell>Right</cell><cell></cell><cell></cell><cell>NIL [22]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>run</cell><cell>#</cell><cell>%</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>% F</cell><cell>% D</cell><cell>% T</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>CWS</cell><cell>K1</cell><cell>r</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[116]</cell><cell>[50]</cell><cell>[34]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>irst051bgbg M</cell><cell>55</cell><cell>27.50</cell><cell>130</cell><cell>13</cell><cell>2</cell><cell>25.00</cell><cell>40.00</cell><cell>17.65</cell><cell>0.15</cell><cell>0.41</cell><cell>0.22</cell><cell>0.144</cell><cell>-0.035</cell><cell>0.160</cell></row><row><cell>btb051bgbg M</cell><cell>37</cell><cell>18.50</cell><cell>160</cell><cell>3</cell><cell>-</cell><cell>10.34</cell><cell>42.00</cell><cell>11.76</cell><cell>0.05</cell><cell>0.41</cell><cell>0.10</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,90.00,263.04,423.00,94.87"><head>Table 4 :</head><label>4</label><figDesc>Results in the tasks with Bulgarian as target (breakdown according to answer type</figDesc><table coords="7,95.98,300.59,412.73,57.32"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Correct Answers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Definition</cell><cell></cell><cell></cell><cell cols="2">Factoid</cell><cell></cell><cell></cell><cell cols="5">Temporally restricted factoid</cell><cell></cell><cell>Total</cell></row><row><cell>run</cell><cell>Or</cell><cell>Pe</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>Ti</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>#</cell><cell>%</cell></row><row><cell></cell><cell>[25]</cell><cell>[25]</cell><cell>[19]</cell><cell>[20]</cell><cell>[18]</cell><cell>[19]</cell><cell>[20]</cell><cell>[20]</cell><cell>[7]</cell><cell>[7]</cell><cell>[4]</cell><cell>[4]</cell><cell>[12]</cell><cell></cell><cell></cell></row><row><cell>irst051bgbg M</cell><cell>6</cell><cell>14</cell><cell>6</cell><cell>4</cell><cell>2</cell><cell>3</cell><cell>7</cell><cell>7</cell><cell>-</cell><cell>3</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>55</cell><cell>27.50</cell></row><row><cell>btb051bgbg M</cell><cell>13</cell><cell>8</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>-</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>37</cell><cell>18.50</cell></row><row><cell>combination</cell><cell>16</cell><cell>17</cell><cell>6</cell><cell>4</cell><cell>2</cell><cell>3</cell><cell>7</cell><cell>9</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>-</cell><cell>1</cell><cell>72</cell><cell>36.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,90.00,118.37,433.86,169.54"><head>Table 5 :</head><label>5</label><figDesc>Results in the tasks with German as target</figDesc><table coords="8,90.00,143.69,433.86,144.22"><row><cell></cell><cell></cell><cell>Right</cell><cell>W</cell><cell>X</cell><cell>U</cell><cell></cell><cell>Right</cell><cell></cell><cell></cell><cell>NIL [20]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>run</cell><cell>#</cell><cell>%</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>% F</cell><cell>% D</cell><cell>% T</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>CWS</cell><cell>K1</cell><cell>r</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[135]</cell><cell>[42]</cell><cell>[23]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dfki051dede M</cell><cell>87</cell><cell>43.50</cell><cell>100</cell><cell>13</cell><cell>-</cell><cell>35.83</cell><cell>66.00</cell><cell>36.67</cell><cell>0.29</cell><cell>0.65</cell><cell>0.40</cell><cell>0.385</cell><cell>0.095</cell><cell>0.300</cell></row><row><cell>fuha051dede M</cell><cell>72</cell><cell>36.00</cell><cell>119</cell><cell>9</cell><cell>-</cell><cell>25.00</cell><cell>70.00</cell><cell>23.33</cell><cell>0.14</cell><cell>1.00</cell><cell>0.25</cell><cell>0.346</cell><cell>0.221</cell><cell>0.665</cell></row><row><cell>dfki052dede M</cell><cell>54</cell><cell>27.00</cell><cell>127</cell><cell>19</cell><cell>-</cell><cell>15.00</cell><cell>52.00</cell><cell>33.33</cell><cell>0.28</cell><cell>0.65</cell><cell>0.39</cell><cell>0.227</cell><cell>0.045</cell><cell>0.386</cell></row><row><cell>dfki051ende C</cell><cell>46</cell><cell>23.00</cell><cell>141</cell><cell>12</cell><cell>1</cell><cell>16.67</cell><cell>50.00</cell><cell>3.33</cell><cell>0.09</cell><cell>0.10</cell><cell>0.09</cell><cell>0.201</cell><cell>0.060</cell><cell>0.483</cell></row><row><cell>dfki052ende C</cell><cell>31</cell><cell>15.50</cell><cell>159</cell><cell>8</cell><cell>2</cell><cell>8.33</cell><cell>42.00</cell><cell>0.00</cell><cell>0.08</cell><cell>0.10</cell><cell>0.09</cell><cell>0.137</cell><cell>0.040</cell><cell>0.564</cell></row><row><cell>uhiq051ende C</cell><cell>10</cell><cell>5.00</cell><cell>161</cell><cell>29</cell><cell>-</cell><cell>0.83</cell><cell>18.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.006</cell><cell>-0.310</cell><cell>0.080</cell></row><row><cell cols="15">better for ORGANIZATION as for PERSON answer types. For factoid questions, best results</cell></row><row><cell cols="15">were attained for TIME, PERSON, LOCATION and ORGANIZATION answer types, in order of</cell></row><row><cell cols="15">their mention, while for temporal questions, results were equally good for PERSON, MEASURE</cell></row><row><cell cols="5">and ORGANIZATION answer types.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,90.00,392.96,423.87,138.85"><head>Table 6 :</head><label>6</label><figDesc>Results in the tasks with German as target (breakdown according to answer type)</figDesc><table coords="8,95.98,431.07,417.89,100.74"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Correct Answers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Definition</cell><cell></cell><cell></cell><cell cols="2">Factoid</cell><cell></cell><cell></cell><cell cols="5">Temporally restricted factoid</cell><cell></cell><cell>Total</cell></row><row><cell>run</cell><cell>Or</cell><cell>Pe</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>Ti</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>#</cell><cell>%</cell></row><row><cell></cell><cell>[29]</cell><cell>[21]</cell><cell>[21]</cell><cell>[20]</cell><cell>[20]</cell><cell>[19]</cell><cell>[20]</cell><cell>[20]</cell><cell>[4]</cell><cell>[13]</cell><cell>[3]</cell><cell>[3]</cell><cell>[7]</cell><cell></cell><cell></cell></row><row><cell>dfki051dede M</cell><cell>22</cell><cell>11</cell><cell>7</cell><cell>6</cell><cell>5</cell><cell>3</cell><cell>8</cell><cell>14</cell><cell>2</cell><cell>5</cell><cell>1</cell><cell>-</cell><cell>3</cell><cell>87</cell><cell>43.50</cell></row><row><cell>fuha051dede M</cell><cell>20</cell><cell>15</cell><cell>5</cell><cell>2</cell><cell>3</cell><cell>8</cell><cell>5</cell><cell>7</cell><cell>-</cell><cell>4</cell><cell>1</cell><cell>-</cell><cell>2</cell><cell>72</cell><cell>36.00</cell></row><row><cell>dfki052dede M</cell><cell>20</cell><cell>6</cell><cell>2</cell><cell>-</cell><cell>3</cell><cell>4</cell><cell>3</cell><cell>6</cell><cell>2</cell><cell>4</cell><cell>1</cell><cell>-</cell><cell>3</cell><cell>54</cell><cell>27.00</cell></row><row><cell>combination</cell><cell>28</cell><cell>17</cell><cell>10</cell><cell>8</cell><cell>6</cell><cell>10</cell><cell>9</cell><cell>16</cell><cell>2</cell><cell>7</cell><cell>1</cell><cell>-</cell><cell>3</cell><cell>117</cell><cell>58.50</cell></row><row><cell>dfki051ende C</cell><cell>21</cell><cell>4</cell><cell>6</cell><cell>1</cell><cell>-</cell><cell>1</cell><cell>4</cell><cell>8</cell><cell>-</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>46</cell><cell>23.00</cell></row><row><cell>dfki052ende C</cell><cell>20</cell><cell>1</cell><cell>2</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>4</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>31</cell><cell>15.50</cell></row><row><cell>uhiq051ende C</cell><cell>3</cell><cell>6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>10</cell><cell>5.00</cell></row><row><cell>combination</cell><cell>22</cell><cell>8</cell><cell>7</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>6</cell><cell>8</cell><cell>-</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56</cell><cell>28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,90.00,118.37,430.79,218.68"><head>Table 7 :</head><label>7</label><figDesc>Results in the tasks with English as target Two independently judged questions 1-66, two judged 67-133 and two judged 134-200, there being 200 questions in total once again. The judgements were then automatically compared using the diff utility. A list of variant judgements was then prepared and presented to each pair of assessors for resolution.</figDesc><table coords="9,90.00,143.69,430.79,157.49"><row><cell></cell><cell></cell><cell>Right</cell><cell>W</cell><cell>X</cell><cell>U</cell><cell></cell><cell>Right</cell><cell></cell><cell></cell><cell>NIL [20]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>run</cell><cell>#</cell><cell>%</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>% F</cell><cell>% D</cell><cell>% T</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>CWS</cell><cell>K1</cell><cell>r</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[121]</cell><cell>[50]</cell><cell>[29]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dfki051deen C</cell><cell>51</cell><cell>25.50</cell><cell>141</cell><cell>8</cell><cell>-</cell><cell>18.18</cell><cell>50.00</cell><cell>13.79</cell><cell>0.22</cell><cell>0.50</cell><cell>0.31</cell><cell>0.203</cell><cell>0.000</cell><cell>0.322</cell></row><row><cell>irst051iten C</cell><cell>47</cell><cell>23.50</cell><cell>145</cell><cell>6</cell><cell>2</cell><cell>19.83</cell><cell>38.00</cell><cell>13.79</cell><cell>0.18</cell><cell>0.35</cell><cell>0.24</cell><cell>0.118</cell><cell>-0.141</cell><cell>0.240</cell></row><row><cell>lire052fren C</cell><cell>38</cell><cell>19.00</cell><cell>152</cell><cell>9</cell><cell>1</cell><cell>16.53</cell><cell>24.00</cell><cell>20.69</cell><cell>0.24</cell><cell>0.45</cell><cell>0.31</cell><cell>0.048</cell><cell>-0.201</cell><cell>0.088</cell></row><row><cell>irst051bgen C</cell><cell>37</cell><cell>18.50</cell><cell>145</cell><cell>17</cell><cell>1</cell><cell>17.36</cell><cell>20.00</cell><cell>20.69</cell><cell>0.17</cell><cell>0.35</cell><cell>0.23</cell><cell>0.079</cell><cell>-0.270</cell><cell>0.055</cell></row><row><cell>dltg051fren C</cell><cell>36</cell><cell>18.00</cell><cell>149</cell><cell>15</cell><cell>-</cell><cell>20.66</cell><cell>12.00</cell><cell>17.24</cell><cell>0.11</cell><cell>0.30</cell><cell>0.16</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>dltg052fren C</cell><cell>36</cell><cell>18.00</cell><cell>151</cell><cell>13</cell><cell>-</cell><cell>19.83</cell><cell>12.00</cell><cell>20.69</cell><cell>0.10</cell><cell>0.30</cell><cell>0.14</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>upv051esen C</cell><cell>34</cell><cell>17.00</cell><cell>156</cell><cell>9</cell><cell>1</cell><cell>12.40</cell><cell>28.00</cell><cell>17.24</cell><cell>0.15</cell><cell>0.50</cell><cell>0.23</cell><cell>0.072</cell><cell>-0.105</cell><cell>0.152</cell></row><row><cell>lire051fren C</cell><cell>28</cell><cell>14.00</cell><cell>156</cell><cell>14</cell><cell>2</cell><cell>13.22</cell><cell>18.00</cell><cell>10.34</cell><cell>0.21</cell><cell>0.15</cell><cell>0.18</cell><cell>0.043</cell><cell>-0.225</cell><cell>0.237</cell></row><row><cell>irst052iten C</cell><cell>26</cell><cell>13.00</cell><cell>168</cell><cell>6</cell><cell>-</cell><cell>5.79</cell><cell>38.00</cell><cell>-</cell><cell>0.22</cell><cell>0.50</cell><cell>0.31</cell><cell>0.114</cell><cell>-0.328</cell><cell>0.414</cell></row><row><cell>hels051fien C</cell><cell>25</cell><cell>12.50</cell><cell>164</cell><cell>10</cell><cell>1</cell><cell>12.40</cell><cell>12.00</cell><cell>13.79</cell><cell>0.17</cell><cell>0.55</cell><cell>0.27</cell><cell>0.050</cell><cell>-0.338</cell><cell>0.022</cell></row><row><cell>hels052fien C</cell><cell>20</cell><cell>10.00</cell><cell>167</cell><cell>11</cell><cell>2</cell><cell>10.74</cell><cell>8.00</cell><cell>10.34</cell><cell>0.21</cell><cell>0.40</cell><cell>0.27</cell><cell>0.041</cell><cell>-0.332</cell><cell>0.058</cell></row><row><cell>uixx051inen C</cell><cell>2</cell><cell>1.00</cell><cell>162</cell><cell>36</cell><cell>-</cell><cell>-</cell><cell>4.00</cell><cell>-</cell><cell>0.40</cell><cell>0.10</cell><cell>0.16</cell><cell>8e-05</cell><cell>-0.770</cell><cell>0.253</cell></row><row><cell>assessors:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,90.00,356.34,423.22,179.10"><head>Table 8 :</head><label>8</label><figDesc>Results in the tasks with English as target (breakdown according to answer type)</figDesc><table coords="9,95.98,394.45,417.24,140.99"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Correct Answers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Definition</cell><cell></cell><cell></cell><cell cols="2">Factoid</cell><cell></cell><cell></cell><cell cols="5">Temporally restricted factoid</cell><cell></cell><cell>Total</cell></row><row><cell>run</cell><cell>Or</cell><cell>Pe</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>Ti</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>#</cell><cell>%</cell></row><row><cell></cell><cell>[25]</cell><cell>[25]</cell><cell>[20]</cell><cell>[20]</cell><cell>[20]</cell><cell>[21]</cell><cell>[20]</cell><cell>[20]</cell><cell>[2]</cell><cell>[9]</cell><cell>[3]</cell><cell>[5]</cell><cell>[10]</cell><cell></cell><cell></cell></row><row><cell>dfki051deen C</cell><cell>12</cell><cell>13</cell><cell>4</cell><cell>4</cell><cell>5</cell><cell>1</cell><cell>2</cell><cell>6</cell><cell>-</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>2</cell><cell>51</cell><cell>25.50</cell></row><row><cell>irst051iten C</cell><cell>4</cell><cell>15</cell><cell>7</cell><cell>1</cell><cell>3</cell><cell>3</cell><cell>2</cell><cell>8</cell><cell>-</cell><cell>1</cell><cell>-</cell><cell>1</cell><cell>2</cell><cell>47</cell><cell>23.50</cell></row><row><cell>lire052fren C</cell><cell>5</cell><cell>7</cell><cell>6</cell><cell>4</cell><cell>4</cell><cell>1</cell><cell>-</cell><cell>5</cell><cell>-</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>38</cell><cell>19.00</cell></row><row><cell>irst051bgen C</cell><cell>6</cell><cell>4</cell><cell>7</cell><cell>4</cell><cell>4</cell><cell>1</cell><cell>3</cell><cell>2</cell><cell>-</cell><cell>2</cell><cell>1</cell><cell>-</cell><cell>3</cell><cell>37</cell><cell>18.50</cell></row><row><cell>dltg051fren C</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>4</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>36</cell><cell>18.00</cell></row><row><cell>dltg052fren C</cell><cell>3</cell><cell>3</cell><cell>8</cell><cell>4</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>8</cell><cell>1</cell><cell>3</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>36</cell><cell>18.00</cell></row><row><cell>upv051esen C</cell><cell>7</cell><cell>7</cell><cell>2</cell><cell>3</cell><cell>3</cell><cell>1</cell><cell>-</cell><cell>6</cell><cell>-</cell><cell>4</cell><cell>-</cell><cell>1</cell><cell>-</cell><cell>34</cell><cell>17.00</cell></row><row><cell>lire051fren C</cell><cell>5</cell><cell>4</cell><cell>7</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>-</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>2</cell><cell>-</cell><cell>1</cell><cell>28</cell><cell>14.00</cell></row><row><cell>irst052iten C</cell><cell>4</cell><cell>15</cell><cell>-</cell><cell>2</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>26</cell><cell>13.00</cell></row><row><cell>hels051fien C</cell><cell>6</cell><cell>-</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>5</cell><cell>-</cell><cell>2</cell><cell>-</cell><cell>2</cell><cell>-</cell><cell>25</cell><cell>12.50</cell></row><row><cell>hels052fien C</cell><cell>4</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>6</cell><cell>-</cell><cell>1</cell><cell>-</cell><cell>2</cell><cell>-</cell><cell>20</cell><cell>10.00</cell></row><row><cell>uixx051inen C</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2</cell><cell>1.00</cell></row><row><cell>combination</cell><cell>19</cell><cell>23</cell><cell>17</cell><cell>11</cell><cell>10</cell><cell>6</cell><cell>6</cell><cell>13</cell><cell>1</cell><cell>6</cell><cell>2</cell><cell>4</cell><cell>5</cell><cell>123</cell><cell>61.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="10,95.98,475.42,428.72,199.67"><head>Table 9 :</head><label>9</label><figDesc>Results in the tasks with Spanish as target</figDesc><table coords="10,95.98,500.75,428.72,174.35"><row><cell></cell><cell></cell><cell>Right</cell><cell>W</cell><cell>X</cell><cell>U</cell><cell></cell><cell>Right</cell><cell></cell><cell></cell><cell>NIL [20]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>run</cell><cell>#</cell><cell>%</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>% F</cell><cell>% D</cell><cell>% T</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>CWS</cell><cell>K1</cell><cell>r</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[118]</cell><cell>[50]</cell><cell>[32]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>inao051eses M</cell><cell>84</cell><cell>42.00</cell><cell>110</cell><cell>5</cell><cell>1</cell><cell>28.81</cell><cell>80.00</cell><cell>31.25</cell><cell>0.23</cell><cell>0.80</cell><cell>0.36</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>tova051eses M</cell><cell>82</cell><cell>41.00</cell><cell>109</cell><cell>7</cell><cell>2</cell><cell>28.81</cell><cell>80.00</cell><cell>25.00</cell><cell>0.24</cell><cell>0.55</cell><cell>0.33</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>inao052eses M</cell><cell>79</cell><cell>39.50</cell><cell>116</cell><cell>4</cell><cell>1</cell><cell>27.12</cell><cell>80.00</cell><cell>21.88</cell><cell>0.19</cell><cell>0.80</cell><cell>0.31</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>tova052eses M</cell><cell>77</cell><cell>38.50</cell><cell>113</cell><cell>8</cell><cell>2</cell><cell>23.73</cell><cell>80.00</cell><cell>28.12</cell><cell>0.22</cell><cell>0.55</cell><cell>0.32</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>upv051eses M</cell><cell>67</cell><cell>33.50</cell><cell>119</cell><cell>13</cell><cell>1</cell><cell>26.27</cell><cell>52.00</cell><cell>31.25</cell><cell>0.19</cell><cell>0.30</cell><cell>0.23</cell><cell>0.218</cell><cell>0.043</cell><cell>0.338</cell></row><row><cell>alia051eses M</cell><cell>66</cell><cell>33.00</cell><cell>110</cell><cell>24</cell><cell>-</cell><cell>29.66</cell><cell>40.00</cell><cell>34.38</cell><cell>0.25</cell><cell>0.45</cell><cell>0.32</cell><cell>0.170</cell><cell>-0.273</cell><cell>0.038</cell></row><row><cell>aliv051eses M</cell><cell>65</cell><cell>32.50</cell><cell>116</cell><cell>18</cell><cell>1</cell><cell>28.81</cell><cell>46.00</cell><cell>25.00</cell><cell>0.26</cell><cell>0.25</cell><cell>0.26</cell><cell>0.15</cell><cell>-0.224</cell><cell>0.223</cell></row><row><cell>alia052eses M</cell><cell>60</cell><cell>30.00</cell><cell>114</cell><cell>26</cell><cell>-</cell><cell>26.27</cell><cell>36.00</cell><cell>34.38</cell><cell>0.24</cell><cell>0.45</cell><cell>0.32</cell><cell>0.153</cell><cell>-0.323</cell><cell>0.038</cell></row><row><cell>talp051eses M</cell><cell>58</cell><cell>29.00</cell><cell>122</cell><cell>20</cell><cell>-</cell><cell>27.97</cell><cell>36.00</cell><cell>21.88</cell><cell>0.26</cell><cell>0.70</cell><cell>0.38</cell><cell>0.089</cell><cell>-0.185</cell><cell>-0.011</cell></row><row><cell>talp052eses M</cell><cell>54</cell><cell>27.00</cell><cell>133</cell><cell>13</cell><cell>-</cell><cell>25.42</cell><cell>32.00</cell><cell>25.00</cell><cell>0.22</cell><cell>0.65</cell><cell>0.33</cell><cell>0.078</cell><cell>-0.210</cell><cell>-0.043</cell></row><row><cell>mira051eses M</cell><cell>51</cell><cell>25.50</cell><cell>138</cell><cell>11</cell><cell>-</cell><cell>26.27</cell><cell>34.00</cell><cell>9.38</cell><cell>0.08</cell><cell>0.10</cell><cell>0.09</cell><cell>0.123</cell><cell>-0.302</cell><cell>0.315</cell></row><row><cell>mira052eses M</cell><cell>46</cell><cell>23.00</cell><cell>140</cell><cell>14</cell><cell>-</cell><cell>22.03</cell><cell>34.00</cell><cell>9.38</cell><cell>0.08</cell><cell>0.10</cell><cell>0.09</cell><cell>0.103</cell><cell>-0.343</cell><cell>0.316</cell></row><row><cell>upv052eses M</cell><cell>36</cell><cell>18.00</cell><cell>155</cell><cell>9</cell><cell>-</cell><cell>22.88</cell><cell>0.00</cell><cell>28.12</cell><cell>0.10</cell><cell>0.40</cell><cell>0.16</cell><cell>0.128</cell><cell>0.041</cell><cell>0.563</cell></row><row><cell>upv051enes C</cell><cell>45</cell><cell>22.50</cell><cell>139</cell><cell>14</cell><cell>2</cell><cell>19.49</cell><cell>34.00</cell><cell>15.62</cell><cell>0.15</cell><cell>0.20</cell><cell>0.17</cell><cell>0.103</cell><cell>-0.033</cell><cell>0.197</cell></row><row><cell>mira052enes C</cell><cell>39</cell><cell>19.50</cell><cell>151</cell><cell>8</cell><cell>2</cell><cell>16.95</cell><cell>28.00</cell><cell>15.62</cell><cell>0.17</cell><cell>0.25</cell><cell>0.20</cell><cell>0.088</cell><cell>-0.394</cell><cell>0.227</cell></row><row><cell>mira051enes C</cell><cell>39</cell><cell>19.50</cell><cell>153</cell><cell>7</cell><cell>1</cell><cell>16.95</cell><cell>28.00</cell><cell>15.62</cell><cell>0.17</cell><cell>0.25</cell><cell>0.20</cell><cell>0.093</cell><cell>-0.392</cell><cell>0.230</cell></row><row><cell>mira051ites C</cell><cell>36</cell><cell>18.00</cell><cell>154</cell><cell>10</cell><cell>-</cell><cell>16.95</cell><cell>26.00</cell><cell>9.38</cell><cell>0.10</cell><cell>0.15</cell><cell>0.12</cell><cell>0.068</cell><cell>-0.437</cell><cell>0.224</cell></row><row><cell>mira052ites C</cell><cell>35</cell><cell>17.50</cell><cell>154</cell><cell>11</cell><cell>-</cell><cell>16.95</cell><cell>24.00</cell><cell>9.38</cell><cell>0.10</cell><cell>0.15</cell><cell>0.12</cell><cell>0.071</cell><cell>-0.447</cell><cell>0.219</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="11,90.00,118.93,423.00,239.28"><head>Table 10 :</head><label>10</label><figDesc>Results in the tasks with Spanish as target (breakdown according to answer type)</figDesc><table coords="11,95.98,157.03,416.05,201.17"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Correct Answers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Definition</cell><cell></cell><cell></cell><cell cols="2">Factoid</cell><cell></cell><cell></cell><cell cols="5">Temporally restricted factoid</cell><cell></cell><cell>Total</cell></row><row><cell>run</cell><cell>Or</cell><cell>Pe</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>Ti</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>#</cell><cell>%</cell></row><row><cell></cell><cell>[25]</cell><cell>[25]</cell><cell>[21]</cell><cell>[17]</cell><cell>[22]</cell><cell>[19]</cell><cell>[20]</cell><cell>[19]</cell><cell>[6]</cell><cell>[7]</cell><cell>[6]</cell><cell>[6]</cell><cell>[7]</cell><cell></cell><cell></cell></row><row><cell>inao051eses M</cell><cell>20</cell><cell>20</cell><cell>5</cell><cell>3</cell><cell>4</cell><cell>7</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>84</cell><cell>42.00</cell></row><row><cell>tova051eses M</cell><cell>20</cell><cell>20</cell><cell>4</cell><cell>3</cell><cell>8</cell><cell>4</cell><cell>8</cell><cell>7</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>82</cell><cell>41.00</cell></row><row><cell>inao052eses M</cell><cell>20</cell><cell>20</cell><cell>5</cell><cell>2</cell><cell>5</cell><cell>6</cell><cell>9</cell><cell>5</cell><cell>-</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>79</cell><cell>39.50</cell></row><row><cell>tova052eses M</cell><cell>20</cell><cell>20</cell><cell>4</cell><cell>3</cell><cell>8</cell><cell>3</cell><cell>3</cell><cell>7</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>77</cell><cell>38.50</cell></row><row><cell>upv051eses M</cell><cell>12</cell><cell>14</cell><cell>10</cell><cell>4</cell><cell>5</cell><cell>2</cell><cell>7</cell><cell>3</cell><cell>3</cell><cell>4</cell><cell>-</cell><cell>1</cell><cell>2</cell><cell>67</cell><cell>33.50</cell></row><row><cell>alia051eses M</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>3</cell><cell>7</cell><cell>3</cell><cell>10</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>66</cell><cell>33.00</cell></row><row><cell>aliv051eses M</cell><cell>15</cell><cell>8</cell><cell>8</cell><cell>3</cell><cell>6</cell><cell>2</cell><cell>10</cell><cell>5</cell><cell>4</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>3</cell><cell>65</cell><cell>32.50</cell></row><row><cell>alia052eses M</cell><cell>9</cell><cell>9</cell><cell>9</cell><cell>2</cell><cell>6</cell><cell>4</cell><cell>8</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>60</cell><cell>30.00</cell></row><row><cell>talp051eses M</cell><cell>16</cell><cell>2</cell><cell>11</cell><cell>2</cell><cell>5</cell><cell>4</cell><cell>8</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>58</cell><cell>29.00</cell></row><row><cell>talp052eses M</cell><cell>16</cell><cell>-</cell><cell>12</cell><cell>1</cell><cell>4</cell><cell>3</cell><cell>6</cell><cell>4</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>54</cell><cell>27.00</cell></row><row><cell>mira051eses M</cell><cell>8</cell><cell>9</cell><cell>7</cell><cell>3</cell><cell>6</cell><cell>4</cell><cell>10</cell><cell>1</cell><cell>-</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell>51</cell><cell>25.50</cell></row><row><cell>mira052eses M</cell><cell>8</cell><cell>9</cell><cell>6</cell><cell>3</cell><cell>4</cell><cell>2</cell><cell>10</cell><cell>1</cell><cell>-</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell>46</cell><cell>23.00</cell></row><row><cell>upv052eses M</cell><cell>-</cell><cell>-</cell><cell>10</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>8</cell><cell>4</cell><cell>3</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>3</cell><cell>36</cell><cell>18.00</cell></row><row><cell>combination</cell><cell>23</cell><cell>24</cell><cell>19</cell><cell>10</cell><cell>16</cell><cell>10</cell><cell>16</cell><cell>10</cell><cell>5</cell><cell>5</cell><cell>3</cell><cell>2</cell><cell>4</cell><cell>147</cell><cell>73.50</cell></row><row><cell>upv051enes C</cell><cell>6</cell><cell>11</cell><cell>7</cell><cell>3</cell><cell>3</cell><cell>2</cell><cell>5</cell><cell>3</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2</cell><cell>45</cell><cell>22.50</cell></row><row><cell>mira051enes C</cell><cell>6</cell><cell>8</cell><cell>6</cell><cell>5</cell><cell>2</cell><cell>2</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>39</cell><cell>19.50</cell></row><row><cell>mira052enes C</cell><cell>6</cell><cell>8</cell><cell>6</cell><cell>5</cell><cell>2</cell><cell>2</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>39</cell><cell>19.50</cell></row><row><cell>mira051ites C</cell><cell>6</cell><cell>7</cell><cell>2</cell><cell>1</cell><cell>4</cell><cell>1</cell><cell>8</cell><cell>4</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2</cell><cell>36</cell><cell>18.00</cell></row><row><cell>mira052ites C</cell><cell>5</cell><cell>7</cell><cell>2</cell><cell>1</cell><cell>4</cell><cell>1</cell><cell>8</cell><cell>4</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2</cell><cell>35</cell><cell>17.50</cell></row><row><cell>combination</cell><cell>11</cell><cell>16</cell><cell>10</cell><cell>5</cell><cell>7</cell><cell>5</cell><cell>9</cell><cell>6</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>78</cell><cell>39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="11,95.86,495.19,411.26,203.82"><head>Table 11 :</head><label>11</label><figDesc>Results of the assessment process for questions with temporal restriction</figDesc><table coords="11,222.71,516.30,157.57,182.71"><row><cell></cell><cell cols="3">question restriction type</cell></row><row><cell>run</cell><cell>date</cell><cell>event</cell><cell>period</cell></row><row><cell></cell><cell>[12]</cell><cell>[10]</cell><cell>[10]</cell></row><row><cell>alia051eses</cell><cell>3</cell><cell>3</cell><cell>5</cell></row><row><cell>alia052eses</cell><cell>3</cell><cell>3</cell><cell>5</cell></row><row><cell>aliv051eses</cell><cell>4</cell><cell>3</cell><cell>1</cell></row><row><cell>inao051eses</cell><cell>5</cell><cell>2</cell><cell>3</cell></row><row><cell>inao052eses</cell><cell>4</cell><cell>1</cell><cell>2</cell></row><row><cell>mira051eses</cell><cell>1</cell><cell>2</cell><cell>-</cell></row><row><cell>mira052eses</cell><cell>1</cell><cell>2</cell><cell>-</cell></row><row><cell>mira051enes</cell><cell>2</cell><cell>2</cell><cell>1</cell></row><row><cell>mira052enes</cell><cell>2</cell><cell>2</cell><cell>1</cell></row><row><cell>mira051ites</cell><cell>-</cell><cell>3</cell><cell>-</cell></row><row><cell>mira052ites</cell><cell>-</cell><cell>3</cell><cell>-</cell></row><row><cell>talp051eses</cell><cell>2</cell><cell>3</cell><cell>2</cell></row><row><cell>talp052eses</cell><cell>2</cell><cell>4</cell><cell>2</cell></row><row><cell>tova051eses</cell><cell>5</cell><cell>-</cell><cell>3</cell></row><row><cell>tova052eses</cell><cell>5</cell><cell>1</cell><cell>3</cell></row><row><cell>upv051eses</cell><cell>4</cell><cell>3</cell><cell>3</cell></row><row><cell>upv052eses</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>upv051enes</cell><cell>1</cell><cell>3</cell><cell>1</cell></row><row><cell>combination</cell><cell>6</cell><cell>8</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="12,135.83,202.00,331.34,61.16"><head>Table 12 :</head><label>12</label><figDesc>Evolution of systems performance with Spanish as target</figDesc><table coords="12,186.12,223.12,230.76,40.05"><row><cell></cell><cell>Best</cell><cell>Best</cell><cell>Best</cell><cell>Best</cell><cell>Best</cell></row><row><cell>Year</cell><cell>Overall Acc.</cell><cell>in Fact</cell><cell>in Def</cell><cell>NIL (F)</cell><cell>r</cell></row><row><cell>2003</cell><cell>24.5 %</cell><cell>24.5 %</cell><cell>-</cell><cell>0.25</cell><cell>-</cell></row><row><cell>2004</cell><cell>32.5 %</cell><cell>31.11 %</cell><cell>70.00 %</cell><cell>0.30</cell><cell>0.17</cell></row><row><cell>2005</cell><cell>42 %</cell><cell>29.66 %</cell><cell>80.00 %</cell><cell>0.38</cell><cell>0.56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="12,105.49,391.73,392.01,69.53"><head>Table 13 :</head><label>13</label><figDesc>Results of agreement test of runs with Spanish as target language</figDesc><table coords="12,105.49,412.85,392.01,48.42"><row><cell></cell><cell># Correct</cell><cell># Correct</cell><cell># Correct</cell><cell># Correct</cell><cell>Disagreement</cell><cell>Kappa</cell><cell>Maximun</cell></row><row><cell>run</cell><cell>(Official)</cell><cell>(2nd assessor)</cell><cell>(lenient)</cell><cell>(strict)</cell><cell>#</cell><cell></cell><cell>variation</cell></row><row><cell>es1</cell><cell>66</cell><cell>67</cell><cell>71</cell><cell>62</cell><cell>15</cell><cell>0.87</cell><cell>± 2 %</cell></row><row><cell>es2</cell><cell>58</cell><cell>63</cell><cell>64</cell><cell>57</cell><cell>11</cell><cell>0.89</cell><cell>± 3 %</cell></row><row><cell>en</cell><cell>45</cell><cell>48</cell><cell>51</cell><cell>42</cell><cell>11</cell><cell>0.87</cell><cell>± 4.5 %</cell></row><row><cell>it</cell><cell>35</cell><cell>35</cell><cell>39</cell><cell>31</cell><cell>10</cell><cell>0.86</cell><cell>± 2 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="12,105.49,577.33,392.01,69.53"><head>Table 14 :</head><label>14</label><figDesc>Results of agreement test of runs with Spanish as target language</figDesc><table coords="12,105.49,598.45,392.01,48.42"><row><cell></cell><cell># Correct</cell><cell># Correct</cell><cell># Correct</cell><cell># Correct</cell><cell>Disagreement</cell><cell>Kappa</cell><cell>Maximun</cell></row><row><cell>run</cell><cell>(Official)</cell><cell>(2nd assessor)</cell><cell>(lenient)</cell><cell>(strict)</cell><cell>#</cell><cell></cell><cell>variation</cell></row><row><cell>es1</cell><cell>66</cell><cell>67</cell><cell>71</cell><cell>62</cell><cell>15</cell><cell>0.87</cell><cell>± 2 %</cell></row><row><cell>es2</cell><cell>58</cell><cell>63</cell><cell>64</cell><cell>57</cell><cell>11</cell><cell>0.89</cell><cell>± 3 %</cell></row><row><cell>en</cell><cell>45</cell><cell>48</cell><cell>51</cell><cell>42</cell><cell>11</cell><cell>0.87</cell><cell>± 4.5 %</cell></row><row><cell>it</cell><cell>35</cell><cell>35</cell><cell>39</cell><cell>31</cell><cell>10</cell><cell>0.86</cell><cell>± 2 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="13,95.98,118.37,419.13,64.98"><head>Table 15 :</head><label>15</label><figDesc>Results in the tasks with Finnish as target</figDesc><table coords="13,95.98,143.69,419.13,39.65"><row><cell></cell><cell></cell><cell>Right</cell><cell>W</cell><cell>X</cell><cell>U</cell><cell></cell><cell>Right</cell><cell></cell><cell></cell><cell>NIL [20]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>run</cell><cell>#</cell><cell>%</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>% F</cell><cell>% D</cell><cell>% T</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>CWS</cell><cell>K1</cell><cell>r</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[111]</cell><cell>[60]</cell><cell>[29]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hels051fifi M</cell><cell>46</cell><cell>23.00</cell><cell>131</cell><cell>23</cell><cell>-</cell><cell>18.92</cell><cell>25</cell><cell>34.98</cell><cell>0.13</cell><cell>0.35</cell><cell>0.19</cell><cell>0.090</cell><cell>-0.202</cell><cell>0.064</cell></row><row><cell>hels052fifi M</cell><cell>38</cell><cell>19.00</cell><cell>140</cell><cell>22</cell><cell>-</cell><cell>15.32</cell><cell>23.33</cell><cell>24.14</cell><cell>0.12</cell><cell>0.30</cell><cell>0.17</cell><cell>0.074</cell><cell>-0.230</cell><cell>0.093</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="13,90.00,411.00,423.00,95.42"><head>Table 16 :</head><label>16</label><figDesc>Results in the tasks with Finnish as target (breakdown according to answer type)</figDesc><table coords="13,95.98,449.11,412.73,57.31"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Correct Answers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Definition</cell><cell></cell><cell></cell><cell cols="2">Factoid</cell><cell></cell><cell></cell><cell cols="5">Temporally restricted factoid</cell><cell></cell><cell>Total</cell></row><row><cell>run</cell><cell>Or</cell><cell>Pe</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>Ti</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>#</cell><cell>%</cell></row><row><cell></cell><cell>[27]</cell><cell>[33]</cell><cell>[21]</cell><cell>[10]</cell><cell>[15]</cell><cell>[20]</cell><cell>[28]</cell><cell>[17]</cell><cell>[4]</cell><cell>[5]</cell><cell>[5]</cell><cell>[5]</cell><cell>[10]</cell><cell></cell><cell></cell></row><row><cell>hels051fifi M</cell><cell>6</cell><cell>9</cell><cell>4</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>9</cell><cell>4</cell><cell>1</cell><cell>4</cell><cell>-</cell><cell>1</cell><cell>4</cell><cell>46</cell><cell>23.00</cell></row><row><cell>hels052fifi M</cell><cell>8</cell><cell>6</cell><cell>4</cell><cell>1</cell><cell>2</cell><cell>-</cell><cell>7</cell><cell>3</cell><cell>1</cell><cell>3</cell><cell>-</cell><cell>1</cell><cell>2</cell><cell>38</cell><cell>19.00</cell></row><row><cell>combination</cell><cell>8</cell><cell>10</cell><cell>5</cell><cell>1</cell><cell>3</cell><cell>1</cell><cell>11</cell><cell>4</cell><cell>1</cell><cell>4</cell><cell>-</cell><cell>1</cell><cell>4</cell><cell>53</cell><cell>26.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="14,95.98,118.37,431.43,174.17"><head>Table 17 :</head><label>17</label><figDesc>Results in the tasks with French as target</figDesc><table coords="14,95.98,143.69,431.43,132.51"><row><cell></cell><cell cols="2">Right</cell><cell>W</cell><cell>X</cell><cell>U</cell><cell></cell><cell>Right</cell><cell></cell><cell></cell><cell>NIL [20]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>run</cell><cell>#</cell><cell>%</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>% F</cell><cell>% D</cell><cell>% T</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>CWS</cell><cell>K1</cell><cell>r</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[120]</cell><cell>[50]</cell><cell>[30]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>syna051frfr M</cell><cell>128</cell><cell>64.00</cell><cell>62</cell><cell>8</cell><cell>2</cell><cell>59.17</cell><cell>86.00</cell><cell>46.67</cell><cell>0.23</cell><cell>0.25</cell><cell>0.24</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>tova052frfr M</cell><cell>70</cell><cell>35.00</cell><cell>120</cell><cell>10</cell><cell>-</cell><cell>27.50</cell><cell>66.00</cell><cell>13.33</cell><cell>0.14</cell><cell>0.30</cell><cell>0.19</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>tova051frfr M</cell><cell>69</cell><cell>34.50</cell><cell>121</cell><cell>10</cell><cell>-</cell><cell>26.67</cell><cell>66.00</cell><cell>13.33</cell><cell>0.13</cell><cell>0.25</cell><cell>0.17</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>upv051frfr M</cell><cell>46</cell><cell>23.00</cell><cell>143</cell><cell>7</cell><cell>4</cell><cell>17.50</cell><cell>46.00</cell><cell>6.67</cell><cell>0.06</cell><cell>0.10</cell><cell>0.07</cell><cell>0.115</cell><cell>-0.048</cell><cell>0.210</cell></row><row><cell>hels051frfr M</cell><cell>35</cell><cell>17.50</cell><cell>156</cell><cell>8</cell><cell>1</cell><cell>16.67</cell><cell>22.00</cell><cell>13.33</cell><cell>0.10</cell><cell>0.45</cell><cell>0.17</cell><cell>0.108</cell><cell>-0.196</cell><cell>0.281</cell></row><row><cell>upv052frfr M</cell><cell>34</cell><cell>17.00</cell><cell>160</cell><cell>5</cell><cell>1</cell><cell>15.00</cell><cell>20.00</cell><cell>20.00</cell><cell>0.07</cell><cell>0.20</cell><cell>0.10</cell><cell>0.073</cell><cell>-0.057</cell><cell>0.207</cell></row><row><cell>lire051frfr M</cell><cell>33</cell><cell>16.50</cell><cell>145</cell><cell>20</cell><cell>2</cell><cell>15.83</cell><cell>14.00</cell><cell>23.33</cell><cell>0.09</cell><cell>-</cell><cell>0.09</cell><cell>0.072</cell><cell>-0.358</cell><cell>0.260</cell></row><row><cell>hels052frfr M</cell><cell>33</cell><cell>16.50</cell><cell>157</cell><cell>10</cell><cell>-</cell><cell>15.00</cell><cell>22.00</cell><cell>13.33</cell><cell>0.09</cell><cell>0.40</cell><cell>0.15</cell><cell>0.097</cell><cell>-0.230</cell><cell>0.247</cell></row><row><cell>lina051frfr M *</cell><cell>29</cell><cell>14.50</cell><cell>144</cell><cell>21</cell><cell>3</cell><cell>17.95</cell><cell>6.00</cell><cell>16.67</cell><cell>0.15</cell><cell>0.20</cell><cell>0.17</cell><cell>0.048</cell><cell>-0.470</cell><cell>0.151</cell></row><row><cell>lcea051frfr M</cell><cell>28</cell><cell>14.00</cell><cell>165</cell><cell>3</cell><cell>4</cell><cell>18.33</cell><cell>0.00</cell><cell>20.00</cell><cell>0.33</cell><cell>0.05</cell><cell>0.09</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>syna051enfr C</cell><cell>79</cell><cell>39.50</cell><cell>108</cell><cell>10</cell><cell>3</cell><cell>30.25</cell><cell>72.00</cell><cell>22.58</cell><cell>0.14</cell><cell>0.30</cell><cell>0.19</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>syna051ptfr C</cell><cell>73</cell><cell>36.50</cell><cell>115</cell><cell>9</cell><cell>3</cell><cell>26.67</cell><cell>68.00</cell><cell>23.33</cell><cell>0.07</cell><cell>0.15</cell><cell>0.10</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>syna051itfr C</cell><cell>51</cell><cell>25.50</cell><cell>136</cell><cell>11</cell><cell>2</cell><cell>15.00</cell><cell>54.00</cell><cell>20.00</cell><cell>0.13</cell><cell>0.45</cell><cell>0.21</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note coords="14,95.98,285.56,138.50,6.97"><p>* Results calculated over 197 questions.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="15,90.00,118.93,423.00,213.78"><head>Table 18 :</head><label>18</label><figDesc>Results in the tasks with French as target (breakdown according to answer type)</figDesc><table coords="15,95.98,157.03,416.29,175.67"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Correct Answers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Definition</cell><cell></cell><cell></cell><cell cols="2">Factoid</cell><cell></cell><cell></cell><cell cols="5">Temporally restricted factoid</cell><cell cols="2">Total</cell></row><row><cell>run</cell><cell>Or</cell><cell>Pe</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>Ti</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>#</cell><cell>%</cell></row><row><cell></cell><cell>[25]</cell><cell>[25]</cell><cell>[20]</cell><cell>[20]</cell><cell>[20]</cell><cell>[20]</cell><cell>[20]</cell><cell>[20]</cell><cell>[6]</cell><cell>[2]</cell><cell>[5]</cell><cell>[10]</cell><cell>[7]</cell><cell></cell><cell></cell></row><row><cell>syna051frfr M</cell><cell>21</cell><cell>22</cell><cell>12</cell><cell>10</cell><cell>13</cell><cell>9</cell><cell>16</cell><cell>11</cell><cell>4</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>3</cell><cell>128</cell><cell>64.00</cell></row><row><cell>tova052frfr M</cell><cell>19</cell><cell>14</cell><cell>3</cell><cell>3</cell><cell>7</cell><cell>5</cell><cell>8</cell><cell>7</cell><cell>1</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>70</cell><cell>35.00</cell></row><row><cell>tova051frfr M</cell><cell>19</cell><cell>14</cell><cell>3</cell><cell>2</cell><cell>8</cell><cell>5</cell><cell>7</cell><cell>7</cell><cell>1</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>69</cell><cell>34.50</cell></row><row><cell>upv051frfr M</cell><cell>9</cell><cell>14</cell><cell>3</cell><cell>5</cell><cell>3</cell><cell>2</cell><cell>4</cell><cell>4</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell>46</cell><cell>23.00</cell></row><row><cell>hels051frfr M</cell><cell>4</cell><cell>7</cell><cell>5</cell><cell>5</cell><cell>3</cell><cell>1</cell><cell>3</cell><cell>3</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>-</cell><cell>35</cell><cell>17.50</cell></row><row><cell>upv052frfr M</cell><cell>-</cell><cell>10</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>4</cell><cell>6</cell><cell>3</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>34</cell><cell>17.00</cell></row><row><cell>lire051frfr M</cell><cell>3</cell><cell>4</cell><cell>1</cell><cell>4</cell><cell>5</cell><cell>1</cell><cell>2</cell><cell>6</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>33</cell><cell>16.50</cell></row><row><cell>hels052frfr M</cell><cell>4</cell><cell>7</cell><cell>5</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>3</cell><cell>4</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>-</cell><cell>33</cell><cell>16.50</cell></row><row><cell>lina051frfr M *</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>6</cell><cell>5</cell><cell>1</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>29</cell><cell>14.50</cell></row><row><cell>lcea051frfr M</cell><cell>-</cell><cell>-</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>-</cell><cell>6</cell><cell>4</cell><cell>1</cell><cell>-</cell><cell>1</cell><cell>-</cell><cell>4</cell><cell>28</cell><cell>14.00</cell></row><row><cell>combination</cell><cell>23</cell><cell>23</cell><cell>15</cell><cell>16</cell><cell>16</cell><cell>14</cell><cell>18</cell><cell>16</cell><cell>5</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>5</cell><cell>161</cell><cell>80.5</cell></row><row><cell>syna051enfr C</cell><cell>21</cell><cell>15</cell><cell>8</cell><cell>6</cell><cell>3</cell><cell>9</cell><cell>6</cell><cell>4</cell><cell>2</cell><cell>-</cell><cell>1</cell><cell>3</cell><cell>1</cell><cell>79</cell><cell>39.50</cell></row><row><cell>syna051ptfr C</cell><cell>17</cell><cell>17</cell><cell>6</cell><cell>4</cell><cell>8</cell><cell>7</cell><cell>4</cell><cell>3</cell><cell>4</cell><cell>-</cell><cell>2</cell><cell>1</cell><cell>-</cell><cell>73</cell><cell>36.50</cell></row><row><cell>syna051itfr C</cell><cell>15</cell><cell>12</cell><cell>4</cell><cell>6</cell><cell>2</cell><cell>4</cell><cell>1</cell><cell>1</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>3</cell><cell>1</cell><cell>51</cell><cell>25.50</cell></row><row><cell>combination</cell><cell>21</cell><cell>20</cell><cell>10</cell><cell>10</cell><cell>8</cell><cell>10</cell><cell>8</cell><cell>6</cell><cell>5</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>106</cell><cell>53</cell></row><row><cell cols="5">* Results calculated over 197 questions.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="16,90.00,118.37,429.10,276.34"><head>Table 19 :</head><label>19</label><figDesc>Results in the tasks with Italian as target In 2005 the task itself attracted more research groups, and though the best system was approximately as good as the one of last year, the average overall accuaracy is slightly worse (i.e. 24%), which probably means that the Italian monolingual test set was more challenging in 2005. As far as the types of questions are concerned, it is interesting to notice that definitional questions proved to be easier than factoids. Between 38 and 50% of definitional got a correct answer, while temporally restricted questions were tougher for the three participating systems. Eleven questions (no. 3, 20, 30, 60, 65, 84, 85, 107, 113, 116 and 124) received a correct answer in all the six submitted runs, and five among them are definition questions referred to a person. This suggests that this type of questions have often a straightforward answer that appears between brackets or in appositive form within the text. Table20shows that the factoids with location, person and time as answer type were the easiest for systems, and if the three systems had worked together, they could have achieved an overall accuracy of 46.5%, which encourages research groups to share tools and resources in the future.</figDesc><table coords="16,90.00,143.69,429.10,107.55"><row><cell></cell><cell></cell><cell>Right</cell><cell>W</cell><cell>X</cell><cell>U</cell><cell></cell><cell>Right</cell><cell></cell><cell></cell><cell>NIL [20]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>run</cell><cell>#</cell><cell>%</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>% F</cell><cell>% D</cell><cell>% T</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>CWS</cell><cell>K1</cell><cell>r</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[120]</cell><cell>[50]</cell><cell>[30]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tova052itit M</cell><cell>55</cell><cell>27.50</cell><cell>135</cell><cell>10</cell><cell>-</cell><cell>23.33</cell><cell>42.00</cell><cell>20.00</cell><cell>0.15</cell><cell>0.55</cell><cell>0.24</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>tova051itit M</cell><cell>53</cell><cell>26.50</cell><cell>138</cell><cell>9</cell><cell>-</cell><cell>21.67</cell><cell>42.00</cell><cell>20.00</cell><cell>0.16</cell><cell>0.55</cell><cell>0.24</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>upv 051itit M</cell><cell>51</cell><cell>25.50</cell><cell>142</cell><cell>6</cell><cell>1</cell><cell>20.00</cell><cell>44.00</cell><cell>16.67</cell><cell>0.10</cell><cell>0.15</cell><cell>0.12</cell><cell>0.156</cell><cell>0.012</cell><cell>0.316</cell></row><row><cell>upv 052itit M</cell><cell>48</cell><cell>24.00</cell><cell>148</cell><cell>4</cell><cell>-</cell><cell>15.83</cell><cell>50.00</cell><cell>13.33</cell><cell>0.06</cell><cell>0.15</cell><cell>0.09</cell><cell>0.125</cell><cell>-0.200</cell><cell>0.202</cell></row><row><cell>irst051itit M</cell><cell>44</cell><cell>22.00</cell><cell>137</cell><cell>17</cell><cell>2</cell><cell>19.17</cell><cell>38.00</cell><cell>6.67</cell><cell>0.17</cell><cell>0.20</cell><cell>0.18</cell><cell>0.129</cell><cell>-0.197</cell><cell>0.267</cell></row><row><cell>irst052itit M</cell><cell>38</cell><cell>19.00</cell><cell>145</cell><cell>14</cell><cell>3</cell><cell>14.17</cell><cell>38.00</cell><cell>6.67</cell><cell>0.40</cell><cell>0.10</cell><cell>0.16</cell><cell>0.100</cell><cell>-0.301</cell><cell>0.071</cell></row><row><cell>25.1%.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" coords="16,90.00,414.13,423.00,128.89"><head>Table 20 :</head><label>20</label><figDesc>Results in the tasks with Italian as target (breakdown according to answer type)</figDesc><table coords="16,95.98,452.24,410.26,90.78"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Correct Answers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Definition</cell><cell></cell><cell></cell><cell cols="2">Factoid</cell><cell></cell><cell></cell><cell cols="5">Temporally restricted factoid</cell><cell></cell><cell>Total</cell></row><row><cell>run</cell><cell>Or</cell><cell>Pe</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>Ti</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>#</cell><cell>%</cell></row><row><cell></cell><cell>[25]</cell><cell>[25]</cell><cell>[19]</cell><cell>[21]</cell><cell>[21]</cell><cell>[19]</cell><cell>[20]</cell><cell>[20]</cell><cell>[4]</cell><cell>[4]</cell><cell>[3]</cell><cell>[8]</cell><cell>[11]</cell><cell></cell><cell></cell></row><row><cell>tova052itit M</cell><cell>11</cell><cell>10</cell><cell>4</cell><cell>1</cell><cell>7</cell><cell>1</cell><cell>8</cell><cell>7</cell><cell>1</cell><cell>1</cell><cell>-</cell><cell>1</cell><cell>3</cell><cell>55</cell><cell>27.50</cell></row><row><cell>tova051itit M</cell><cell>11</cell><cell>10</cell><cell>4</cell><cell>1</cell><cell>7</cell><cell>1</cell><cell>6</cell><cell>7</cell><cell>1</cell><cell>1</cell><cell>-</cell><cell>1</cell><cell>3</cell><cell>53</cell><cell>26.50</cell></row><row><cell>upv 051itit M</cell><cell>10</cell><cell>12</cell><cell>6</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>7</cell><cell>4</cell><cell>1</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>3</cell><cell>51</cell><cell>25.50</cell></row><row><cell>upv 052itit M</cell><cell>11</cell><cell>14</cell><cell>5</cell><cell>-</cell><cell>3</cell><cell>1</cell><cell>6</cell><cell>4</cell><cell>1</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>2</cell><cell>48</cell><cell>24.00</cell></row><row><cell>irst051itit M</cell><cell>5</cell><cell>14</cell><cell>7</cell><cell>-</cell><cell>4</cell><cell>2</cell><cell>5</cell><cell>5</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell>44</cell><cell>22.00</cell></row><row><cell>irst052itit M</cell><cell>5</cell><cell>14</cell><cell>3</cell><cell>-</cell><cell>4</cell><cell>1</cell><cell>3</cell><cell>6</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell>38</cell><cell>19.00</cell></row><row><cell>combination</cell><cell>17</cell><cell>20</cell><cell>9</cell><cell>3</cell><cell>8</cell><cell>4</cell><cell>10</cell><cell>11</cell><cell>3</cell><cell>2</cell><cell>-</cell><cell>1</cell><cell>5</cell><cell>93</cell><cell>46.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20" coords="16,95.98,652.61,381.28,90.88"><head>Table 21 :</head><label>21</label><figDesc>Inter-assessor agreement in the evaluation of the Italian runs</figDesc><table coords="16,95.98,678.21,202.86,65.28"><row><cell></cell><cell cols="2">disagreement</cell></row><row><cell>run</cell><cell>different judgments (#)</cell><cell>kappa coefficient</cell></row><row><cell>tova052itit</cell><cell>10</cell><cell>0.895</cell></row><row><cell>tova051itit</cell><cell>11</cell><cell>0.882</cell></row><row><cell>upv 051itit</cell><cell>8</cell><cell>0.909</cell></row><row><cell>upv 052itit</cell><cell>9</cell><cell>0.895</cell></row><row><cell>irst051itit</cell><cell>17</cell><cell>0.828</cell></row><row><cell>irst052itit</cell><cell>15</cell><cell>0.839</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21" coords="18,90.00,118.93,422.99,103.79"><head>Table 23 :</head><label>23</label><figDesc>Results in the tasks with Dutch as target (breakdown according to answer type)</figDesc><table coords="18,95.98,157.03,416.04,65.68"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Correct Answers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Definition</cell><cell></cell><cell></cell><cell cols="2">Factoid</cell><cell></cell><cell></cell><cell cols="5">Temporally restricted factoid</cell><cell></cell><cell>Total</cell></row><row><cell>run</cell><cell>Or</cell><cell>Pe</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>Ti</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>#</cell><cell>%</cell></row><row><cell></cell><cell>[24]</cell><cell>[36]</cell><cell>[30]</cell><cell>[9]</cell><cell>[11]</cell><cell>[20]</cell><cell>[35]</cell><cell>[9]</cell><cell>[5]</cell><cell>[4]</cell><cell>[1]</cell><cell>[4]</cell><cell>[12]</cell><cell></cell><cell></cell></row><row><cell>gron051nlnl M</cell><cell>16</cell><cell>14</cell><cell>13</cell><cell>4</cell><cell>3</cell><cell>12</cell><cell>25</cell><cell>5</cell><cell>3</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>3</cell><cell>99</cell><cell>49.50</cell></row><row><cell>uams051nlnl M</cell><cell>9</cell><cell>18</cell><cell>14</cell><cell>2</cell><cell>3</cell><cell>7</cell><cell>23</cell><cell>5</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5</cell><cell>88</cell><cell>44.00</cell></row><row><cell>uams052nlnl M</cell><cell>8</cell><cell>18</cell><cell>14</cell><cell>2</cell><cell>4</cell><cell>7</cell><cell>23</cell><cell>5</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5</cell><cell>88</cell><cell>44.00</cell></row><row><cell>combination</cell><cell>17</cell><cell>24</cell><cell>21</cell><cell>4</cell><cell>6</cell><cell>13</cell><cell>31</cell><cell>7</cell><cell>5</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>7</cell><cell>136</cell><cell>68.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22" coords="18,90.00,586.83,423.22,130.49"><head>Table 25 :</head><label>25</label><figDesc>Results in the tasks with Portuguese as target (breakdown according to answer type)</figDesc><table coords="18,95.98,624.93,417.24,92.38"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Correct Answers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Definition</cell><cell></cell><cell></cell><cell cols="2">Factoid</cell><cell></cell><cell></cell><cell cols="5">Temporally restricted factoid</cell><cell></cell><cell>Total</cell></row><row><cell>run</cell><cell>Or</cell><cell>Pe</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>Ti</cell><cell>Lo</cell><cell>Me</cell><cell>Or</cell><cell>Ot</cell><cell>Pe</cell><cell>#</cell><cell>%</cell></row><row><cell></cell><cell>[15]</cell><cell>[27]</cell><cell>[30]</cell><cell>[17]</cell><cell>[21]</cell><cell>[15]</cell><cell>[37]</cell><cell>[15]</cell><cell>[5]</cell><cell>[1]</cell><cell>[2]</cell><cell>[6]</cell><cell>[9]</cell><cell></cell><cell></cell></row><row><cell>prib051ptpt M</cell><cell>12</cell><cell>15</cell><cell>26</cell><cell>11</cell><cell>7</cell><cell>8</cell><cell>25</cell><cell>14</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>4</cell><cell>5</cell><cell>129</cell><cell>64.50</cell></row><row><cell>ptue051ptpt M</cell><cell>5</cell><cell>10</cell><cell>10</cell><cell>1</cell><cell>3</cell><cell>3</cell><cell>10</cell><cell>2</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>2</cell><cell>3</cell><cell>50</cell><cell>25.00</cell></row><row><cell>esfg051ptpt M</cell><cell>1</cell><cell>6</cell><cell>9</cell><cell>4</cell><cell>5</cell><cell>2</cell><cell>9</cell><cell>3</cell><cell>1</cell><cell>-</cell><cell>1</cell><cell>3</cell><cell>2</cell><cell>46</cell><cell>23.00</cell></row><row><cell>esfg052ptpt M</cell><cell>-</cell><cell>6</cell><cell>8</cell><cell>3</cell><cell>3</cell><cell>-</cell><cell>13</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>43</cell><cell>21.50</cell></row><row><cell>combination</cell><cell>12</cell><cell>22</cell><cell>28</cell><cell>13</cell><cell>11</cell><cell>10</cell><cell>27</cell><cell>15</cell><cell>3</cell><cell>-</cell><cell>1</cell><cell>6</cell><cell>7</cell><cell>155</cell><cell>77.50</cell></row><row><cell>esfg051enpt C</cell><cell>-</cell><cell>6</cell><cell>3</cell><cell>1</cell><cell>3</cell><cell>1</cell><cell>5</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell>2</cell><cell>-</cell><cell>24</cell><cell>12.00</cell></row><row><cell>combination</cell><cell>-</cell><cell>6</cell><cell>3</cell><cell>1</cell><cell>3</cell><cell>1</cell><cell>5</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell>2</cell><cell>-</cell><cell>24</cell><cell>12.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23" coords="19,90.34,250.24,422.30,83.07"><head>Table 26 :</head><label>26</label><figDesc>Results in the tasks with Portuguese as target (breakdown of bad answers)</figDesc><table coords="19,95.98,276.40,178.75,56.91"><row><cell></cell><cell cols="3">Incorrect or null answers</cell></row><row><cell>run</cell><cell>null answer</cell><cell>rubbish</cell><cell>dangerous</cell></row><row><cell>prib051ptpt</cell><cell>4</cell><cell>13</cell><cell>43</cell></row><row><cell>ptue051ptpt</cell><cell>117</cell><cell>3</cell><cell>20</cell></row><row><cell>esfg051ptpt</cell><cell>68</cell><cell>41</cell><cell>49</cell></row><row><cell>esfg052ptpt</cell><cell>65</cell><cell>34</cell><cell>63</cell></row><row><cell>esfg051enpt</cell><cell>121</cell><cell>21</cell><cell>40</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.25,695.73,284.20,7.97"><p>For more information about QA@CLEF campaigns visit http://clef-qa.itc.it.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="14,105.25,716.69,407.73,7.97;14,90.00,726.15,92.00,7.97"><p>The question requires the head noun of the answer to be in the nominative case -der König -instead of the dative case -dem König.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="14,105.25,735.65,407.71,7.97;14,90.00,745.12,75.02,7.97"><p>The question requires the answer to be in the nominative case -Mika Kaurismäki -instead of the genetive case -Mika Kaurismäen.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p><rs type="person">Anselmo Peñas</rs> and <rs type="person">Valentín Sama Rojo</rs> have been partially supported by the <rs type="funder">Spanish ment</rs> under project <rs type="grantNumber">R2D2-Syembra TIC-2003-07158-C04-02</rs>. Maarten de Rijke was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">264-70-050</rs>, <rs type="grantNumber">354-20-005</rs>, <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">612.000.207</rs>, <rs type="grantNumber">612.066.302</rs>, <rs type="grantNumber">612.069.006</rs>, and <rs type="grantNumber">640.001.501</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_d34erzH">
					<idno type="grant-number">R2D2-Syembra TIC-2003-07158-C04-02</idno>
				</org>
				<org type="funding" xml:id="_ghuKgmh">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_hqxBF54">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_WxggrwN">
					<idno type="grant-number">264-70-050</idno>
				</org>
				<org type="funding" xml:id="_EFj4aSP">
					<idno type="grant-number">354-20-005</idno>
				</org>
				<org type="funding" xml:id="_xV93pgf">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_z9aPQyr">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_thhmscV">
					<idno type="grant-number">612.000.207</idno>
				</org>
				<org type="funding" xml:id="_WKQZ6vV">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_Esu5kMZ">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_ppHqXvj">
					<idno type="grant-number">640.001.501</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A total of 70 disagreement cases were registered, most of them involved the judgment couples R-X (11 cases), R-W (13 cases), U-W (10 cases) and above all X-W (31 cases). Clearly, the evaluation guidelines did not deal extensively with answer exactness, so assessors had some difficulties in deciding which portion of an answer-string was acceptable and which not. In most of the cases (i.e. 26) where an assessor assigned X and the other W, the third and final judgment was W.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Dutch as Target</head><p>This year two teams that took part in the QA@CLEF track used Dutch as their target language: the University of Amsterdam and the University of Groningen. In total, three runs were submitted, all using Dutch as the source language. All runs were assessed by two assessors, with very interassessor agreement (0.950 for gron051nlnl, and 0.976 for uams051nlnl and uams052nlnl). The results of the evaluation for all runs are provided in Tables <ref type="table" coords="17,349.98,253.38,9.96,9.96">22</ref> and<ref type="table" coords="17,382.65,253.38,8.48,9.96">23</ref>. When scored in terms of the percentage of correct (i.e, correct and exact and supported) answers, the run labeled gron051nlnl (submitted by the University of Groningen) clearly outperforms the two runs submitted by the University of Amsterdam: 49.50% vs. 44% and 44%. When compared to the correct answers in the Groningen run, many of the inexact answers in the Amsterdam runs are caused by incorrect definitions; here's an example. 0094 NLNL Wat is Eyal? gron051nlnl: militante joodse groep uams051nlnl: leider van de extreem-rechtse groep This observation is confirmed if we take a closer look. In the 200 questions, six initial words occur more than ten times: Wie (Who), Wat (What), Hoe (How), Welke (Which), Waar (Where) and In (In). The performance of the questions with four of the six initial words is similar for the three runs. For Wat, Groningen obtains 67% right and Amsterdam 39%. This difference mainly caused by the problem with the definition answers just mentioned. For Hoe, Groningen obtains 63% and Amsterdam 36%. Seven of the eight Hoe questions for which only Groningen found the answer, were of the format Hoe heet DEFINITION? (What is the name of DEFINITION?).</p><p>All in all, the Groningen run performs noticeably better than the Amsterdam runs in terms of precision-this is clear from the differences in answers labeled X (inexact): only 18 for Groningen, and as many as 28 and 29 for Amsterdam.</p><p>If we drill down a bit further, and consider the detailed results in Table <ref type="table" coords="17,442.92,609.16,8.48,9.96">23</ref>, we see that Groningen outperforms Amsterdam on Organisations in the Definitions category, and on Other questions in the Factoid category; Amsterdam is slightly better in Person definitions. On other categories, the differences are very minor or non-existent. There is, however, a noticeable difference in performance on NIL questions, with Amsterdam achieving far higher F-scores than Groningen.</p><p>To conclude, let's adopt a somewhat alternative perspective. The differences between the Groningen run and the Amsterdam are mainly in the number of inexact answers; in terms of the number of unsupported or wrong answers the differences are negligible. Put differently, in terms of the number of answers that are "helpful" <ref type="bibr" coords="17,283.99,704.80,9.96,9.96" target="#b3">[4]</ref>, i.e., that would help a user meet her information needs, the three runs all perform at the same level: 117 helpful (i.e., correct or inexact) for the Groningen run, and 116 and 117 helpful for the two Amsterdam runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Portuguese as Target</head><p>In 2005 there were five runs with Portuguese as target, submitted by three different research teams: In addition to the two participants from last year, SINTEF with the Esfinge system and the University of Évora, we had a newcomer from industry, Priberam, a Portuguese company specialized in NLP products. Although a collection of Brazilian Portuguese news was added to the CLEF collection, no Brazilian participants turned up as yet for CLEF. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="20,105.50,190.13,407.46,9.96;20,105.50,202.08,131.71,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="20,105.50,190.13,163.13,9.96">The CLEF QA Track coordinators</title>
		<ptr target="http://clef-qa.itc.it/2005/guidelines.html" />
	</analytic>
	<monogr>
		<title level="m" coord="20,287.58,190.13,126.95,9.96">QA@CLEF 2005 Guidelines</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,105.50,222.01,407.48,9.96;20,105.50,233.96,407.47,9.96;20,105.50,245.92,43.71,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="20,328.63,222.01,179.82,9.96">Question answering pilot task at clef 2004</title>
		<author>
			<persName coords=""><forename type="first">Jesús</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,105.50,233.96,114.03,9.96">Proceedings of CLEF 2004</title>
		<title level="s" coord="20,227.81,233.96,152.52,9.96">Lecture Notes in Computer Science</title>
		<meeting>CLEF 2004</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">3491. 2005</date>
			<biblScope unit="page" from="581" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,105.50,265.85,407.52,9.96;20,105.50,277.80,407.52,9.96;20,105.50,289.75,407.49,9.96;20,105.50,301.72,280.58,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="20,161.03,277.80,295.67,9.96">Overview of the CLEF 2004 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Erbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,195.42,289.75,312.45,9.96">Results of the CLEF 2004 Cross-Language System Evaluation Compaign</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Borri In</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Working Notes for the CLEF 2004 Workshop</note>
</biblStruct>

<biblStruct coords="20,105.50,321.63,407.49,9.96;20,105.50,333.59,407.49,9.96;20,105.50,345.55,178.12,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="20,183.52,321.63,164.86,9.96">Is question answering a rational task</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Spark</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,142.40,333.59,370.59,9.96;20,105.50,345.55,88.88,9.96">Questions and Answers: Theoretical and Applied Perspectives (Second CoLogNET-ElsNET Symposium)</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Moortgat</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="24" to="35" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
