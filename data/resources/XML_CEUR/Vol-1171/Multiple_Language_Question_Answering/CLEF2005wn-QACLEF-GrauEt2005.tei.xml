<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,120.24,98.73,362.33,15.51;1,264.72,120.69,73.75,15.51">Term Translation Validation by Retrieving Bi-terms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,112.20,154.07,56.97,9.96"><forename type="first">Brigitte</forename><surname>Grau</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIR group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>BP 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay Cedex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,177.54,154.07,85.29,9.96"><forename type="first">Anne-Laure</forename><surname>Ligozat</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIR group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>BP 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay Cedex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.45,154.07,61.95,9.96"><forename type="first">Isabelle</forename><surname>Robba</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIR group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>BP 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay Cedex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.84,154.07,73.27,9.96"><forename type="first">Madeleine</forename><surname>Sialeu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIR group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>BP 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay Cedex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,436.77,154.07,53.60,9.96"><forename type="first">Anne</forename><surname>Vilnat</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIR group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>BP 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay Cedex</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,120.24,98.73,362.33,15.51;1,264.72,120.69,73.75,15.51">Term Translation Validation by Retrieving Bi-terms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D5500496B4A2E79221A50A4539D09DBB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For our second participation to the Question Answering task of CLEF, we kept last year's system named MUSCLEF, which uses two translation strategies implemented in two modules. The multilingual module MUSQAT analyzes the French questions, translates "interesting parts", and then uses these translated terms to search the reference collection. The second strategy consists in translating the question in English and applying QALC our existing English module. Our purpose in this paper is to analyze term translations and propose a mechanism for selecting correct ones. The manual evaluation of bi-terms translation leads us to the conclusion that bi-term translations found in corpus can confirm mono-term translations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents our second participation to the Question Answering task of CLEF evaluation campaign. This year we have participated in two tasks: a monolingual task (in French) for which we submitted one run, and a bilingual task (questions in French, answers in English) for which we submitted two runs. Concerning the bilingual task, we used the same two strategies as last year:</p><p>• translation of selected terms issued of the question analysis module, then search in the collection, this first system is called MUSQAT</p><p>• question translation thanks to a machine translation system, then application of QALC our monolingual english system Those strategies are the most commonly adopted, but to our knowledge, any other system except our own implements both. Our whole system is called MUSCLEF; since its architecture is practically the same as last year <ref type="bibr" coords="1,209.64,519.83,84.43,9.96" target="#b16">([Peters et al. 2005]</ref>), we rather chose to present an evaluation of the different translations used last year in MUSCLEF. This manual evaluation was time-consuming thus it has not yet been done for this year's data. It remains nevertheless relevant and lead us to propose a mechanism for selecting correct term translations. We will first present an overview of our system, then we will focus on our recognition of terms in documents, realized by Fastr, and their translation. We will then present an evaluation of these translations followed by results concerning term validation and our global results at the QA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of MUSCLEF</head><p>The global architecture of MUSCLEF is illustrated Figure <ref type="figure" coords="1,349.11,646.31,3.90,9.96">1</ref>. First, its question analysis module aims at deducing characteristics which may help to find possible answers in selected passages. These characteristics are: the expected answer type, the question focus, the main verb and some syntactic characteristics. They are deduced from the morpho-syntactic tagging and syntactic analysis of the question. For this campaign, we developped a grammar of question and used the Cass robust parser<ref type="foot" coords="2,172.20,60.59,3.95,4.85" target="#foot_0">1</ref> to analyze the English questions that were translated using Reverso<ref type="foot" coords="2,480.12,60.59,3.95,4.85" target="#foot_1">2</ref> . As a new type of questions, the temporally restricted questions, was introduced in this year's campaign, we have adjusted question analysis to the category of the question. When a temporal restriction was to be found, we tried to detect it, and to classify it according to the three following types: date, period, and event. The answering strategy was then adapted to the type of temporal constraint.</p><p>For querying the CLEF collection and retrieving passages we used MG<ref type="foot" coords="2,411.36,120.35,3.95,4.85" target="#foot_2">3</ref> . Retrieved documents are then processed: they are re-indexed by the question terms and their linguistic variants, reordered according to the number and the kind of terms found in them, so as to select a subset of them. Named entity recognition processes are then applied. The answer extraction process relies on a weighting scheme of the sentences, followed by the answer extraction itself. We apply different processes according to the kind of expected answer, each of them leading to propose weighted answers.</p><p>The first run we submitted corresponds to the strategy implemented in MUSQAT: translation of selected terms. For the second run, we added a final step consisting in comparing the results issued from both strategies: the translated questions and the translated terms. This module named fusion in Figure <ref type="figure" coords="2,192.18,240.83,3.90,9.96">1</ref>, computes a final score for each potential answer, its principle is to boost an answer if both chains ranked it in the top 5 propositions, even with relatively low scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Focus Answer type</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantically linked words</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main verb Terms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntactic relations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English questions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>French</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Searching terms and variants</head><p>The automatic indexing of documents is performed by FASTR, a transformational shallow parser for the recognition of term occurrences and variants. Terms are transformed into grammar rules and the single words building these terms are extracted and linked to their morphological and semantic families. The morphological family of a single word w is the set M(w) of terms in the CELEX database <ref type="bibr" coords="2,169.97,575.51,62.14,9.96">([CELEX 1998</ref>]) which have the same root morpheme as w. For instance, the morphological family of the noun maker is made of the nouns maker, make and remake, and the verbs to make and to remake. The semantic family of a single word w is the union S(w) of the synsets of WordNet1.6 ( <ref type="bibr" coords="2,195.95,611.39,63.01,9.96">[Fellbaum 1998</ref>]) to which w belongs. A synset is a set of words that are synonymous for at least one of their meanings. Thus, the semantic family of a word w is the set of the words w' such that w' is considered as a synonym of one of the meanings of w. The semantic family of maker, obtained from WordNet1.6, is composed of three nouns: maker, manufacturer, shaper and the semantic family of car is car, auto, automobile, machine, motorcar. Variant patterns that rely on morphological and semantic families are generated through metarules. They are used to extract terms and variants from the document sentences in the selected documents.</p><p>For instance, the following pattern, named NtoSemArg, extracts the occurrence making many automobiles as a variant of the term car maker : VM('maker') RP? PREP? ART? (JJ-NN-NP -VBD-VBG)0-3 NS('car') where RP are particles, PREP prepositions, ART articles, and VBD, VBG verbs. VM('maker') is any verb in the morphological family of the noun maker and NS('car') is any noun in the semantic family of car.</p><p>Relying on the above morphological and semantic families, auto maker, auto parts maker, car manufacturer, make autos, and making many automobiles are extracted as correct variants of the original term car maker through the set of metarules used for the QA-track experiment. Unfortunately, some incorrect variants are extracted as well, such as make those cuts in auto produced by the preceding metarule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Term translation</head><p>Different methods can be used to achieve term translation and we considered the easiest one, which consists in using a bilingual dictionary to translate the terms from the source language to the target language. This simple method presents two drawbacks: it is impossible to directly disambiguate the various meanings of the words to be translated, and the two languages must be of equivalent lexical richness. To give an idea of the ambiguities we may encounter in a QA context, we studied the corpus of 1893 questions in English of TREC. After analysis, we kept 9000 of the 15624 words used in this corpus. The average of the number of meanings was 7.35 in WordNet. The extrema were 1 (example: neurological ) and 59 (example: break ). Around the average value, we found common words such as prize, blood, organization. Hence, we could not consider a dictionary giving only one meaning for a word. Moreover we needed to define a measure of the value of a translation in our QA context.</p><p>With these constraints, we studied the different dictionaries we could use: the online dictionaries (such as Reverso<ref type="foot" coords="3,191.76,418.19,3.95,4.85" target="#foot_3">4</ref> , Systran<ref type="foot" coords="3,235.20,418.19,3.95,4.85" target="#foot_4">5</ref> , Google<ref type="foot" coords="3,276.00,418.19,3.95,4.85" target="#foot_5">6</ref> , Dictionnaire Terminologique<ref type="foot" coords="3,410.64,418.19,3.95,4.85" target="#foot_6">7</ref> or FreeTranslation<ref type="foot" coords="3,501.84,418.19,3.95,4.85" target="#foot_7">8</ref> ), and the dictionaries under GPL licences (such as Magic-Dic<ref type="foot" coords="3,353.40,430.19,3.95,4.85" target="#foot_8">9</ref> , Unidic or FreeDict<ref type="foot" coords="3,445.92,430.19,7.91,4.85" target="#foot_9">10</ref> ). The online dictionaries are generally complete. But they resolve the ambiguity and they only give one translation per word. Another limitation was the fact that we could not modify these dictionaries, and that we had to deal with some technical constraints such as the limited number of requests we may adress and the access time. Concerning the GPL dictionaries, they are obviously less complete, but they can be modified, they are very fast and for most of all, they give several translations for a request, as classical bilingual dictionaries. Among the GPL dictionaries, we chose Magic-dic, because of its evolutivity: terms can be added by any user, but they are verified before being integrated, and FreeDict. For example the query for the French word porte to Magic-Dic gives the following results (we only give an excerpt):</p><p>• porte bagages -luggagerack, luggage rack</p><formula xml:id="formula_0" coords="3,105.00,578.51,151.88,29.88">• porte cigarette -cigarette holder • porte clefs -key-ring</formula><p>• porte plume -fountain pen</p><p>• porte parole, locuteur -spokesman</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• porte -door, gate</head><p>To prevent Magic-dic uncompleteness, and because it has been proved that the use of several dictionaries gives better results than a unique one, we used this year two dictionaries and merged their translations. FreeDict had added 424 different translations of the 690 words. However, these new translations are mainly other synonyms rather than new translations of unknown words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The multilingual module MUSQAT</head><p>We illutrate the strategy defined in our multilingual module MUSQAT on the following example: "Quel est le nom de la principale compagnie aérienne allemande?", which is translated in English "What is the name of the main German airline company?".</p><p>The first step is the parsing of the French question that provides a list of the mono-terms and all the bi-terms (such as adjective/common noun) which are in the question, and eliminates the stop words. The bi-terms are useful, because they allow a disambiguisation by giving a (small) context to a word. In our example, the bi-terms (in their lemmatized form) are: principal compagnie, compagnie aérien, aérien allemand ; and the mono-terms: nom, principal, compagnie, aérien, allemand.</p><p>With the help of the dictionnaries, MUSQAT attempts to translate the bi-terms (when they exist), and the mono-terms. All the proposed translations are taken into account. All the terms are grammatically tagged. If a bi-term cannot be directly translated, it is recomposed from the monoterms, following the English syntax. For our example, we obtained for the bi-terms: principal company/main company, air company, air german; and for the mono-terms: name/appellation, principal/main, company, german. When a word does not exist in the dictionnaries, we keep it as it whithout any diacritic, which is often relevant for proper nouns. Then, all the words are weighted relative to their existence in a lexicon that contains the vocabulary found in Latimes of the Trec collection, so that each word is weighted according to its specificity within this corpus. If a word is not found in this lexicon, we search with MG if documents contain it (or rather its root because MG indexation was made using stemming). If it is not the case, MUSQAT eliminate it from the list of translated terms. By this way, MUSQAT discarded 72 non-translated words (on 439 non-translated mono-terms, the remaining ones often being proper nouns). As we form boolean requests, it was important not to keep inexisting words.</p><p>English terms plus their categories (given by the Tree Tagger) were then given as input to the other modules of the system, instead of the original words. The translation module did not try to solve the ambiguity between the different translations. We account on the document retrieval module to discard irrelevant translations. This module has been improved this year: it always selects passages (the collection was preliminary splitted), but in a very smaller number. It first generates boolean requests, based on proper nouns, numbers and specificity of the words. It aims at retrieving 200 passages maximum, and makes the smaller request with the more specific terms so as to obtain a minimum number of passages, set to 50. Each term of the request is made of the disjonction of the different translations. If the boolean query leads to retrieving too few or too much documents, passage retrieval is made thanks to a ranked research with a query that hold all the terms. If different terms are synonyms, relevant documents are then retrieved with these synonyms. If a word is incoherent within the context, we suppose its influence is not sufficient to generate noise. This hypothesis can only be verified if the question is made of several words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Magic-dic term evaluation</head><p>We manually evaluated the bi-term translations for the 200 questions of CLEF04 given by this module. Table <ref type="table" coords="4,157.16,668.63,5.03,9.96" target="#tab_0">1</ref> presents the results of this evaluation. The system found 375 bi-terms. Among them, 135 are correct translated bi-terms (OK) such as CERN member. 24 are bi-terms contextually false i.e. for which one word is not a good translation in the context of this bi-term, such as accretion hormone instead of growth hormone to translate hormone de croissance. 74 bi-terms are due to an erroneous bi-term constitution (False Bi-Terms), such as able animal in question asking to Give an animal able to.... Finally, 142 bi-terms are (a) completely erroneous translations (False Translation), such as overground escort instead of main company (110) or (b) the translation was absent from the dictionnary (Absent Translations), such as olympique, where the French word has been kept instead of the English termolympic (32). It is obvious on this table that a lot of terms are wrong for different reasons. We decided to confirm those that must be kept by considering their presence or absence in the selected documents.</p><p>To do so, we used FASTR results to evaluate the bi-terms or their variants which are retrieved in the documents. Table <ref type="table" coords="5,188.09,321.23,5.03,9.96" target="#tab_1">2</ref> shows the results of this evaluation. The second column gives the results obtained by FASTR without considering the semantic variations. The third column includes these semantic variations. The last column indicates the percentage of bi-terms FASTR confirms, taking into account the semantic variations. The correct bi-terms are mostly confirmed by FASTR. The contextually false bi-terms obtain a rather high percentage of confirmation due to the semantic variations which lead to recognize correct synonyms of non accurate translated terms. The false bi-terms can be considered as co-occurrences rather than bi-terms. As co-occurrences, they are retrieved by FASTR in the documents and just a few false translations are retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation of terms extracted from question translations</head><p>We also proceeded to a similar evaluation of the terms extracted from the questions translated last year by Systran.</p><p>As a first step we proceeded to an evaluation of the question translations themselves. We evaluated the syntactic quality of the translations, and classified them in correct, false, or quite correct. Table <ref type="table" coords="5,154.49,695.03,5.03,9.96" target="#tab_2">3</ref> recapitulates these results. We also evaluated the terms extracted from these translated questions by our monolingual system QALC. We use the same notations than in table 1. Results are given Table <ref type="table" coords="6,454.85,160.55,3.90,9.96" target="#tab_3">4</ref>. These results are quite interesting: despite the moderate quality of the translations, QALC is able to identify good terms from these questions. We can also notice that we obtain a smaller number of terms following this procedure because there is only one translation by word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>Table <ref type="table" coords="6,117.46,404.15,5.03,9.96" target="#tab_4">5</ref> gives the results that our system obtained at the CLEF04 and CLEF05 campaigns, with the different strategies: (a) with the translation of the terms (MUSQAT), (b) with QALC applied on the translated questions and searching the collection. The evaluation was made by an automatic process that looks for the answer patterns in the system answers, applying regular expressions. These results were computed with 178 answer patterns that we built for the 200 questions of CLEF04 and 188 for the CLEF05 questions.</p><p>The first line indicates the number of correct answers found in the 5 first sentences given by MUSQAT (using term translation) and QALC. The second line, "NE answers", gives the number of correct answers on questions the system categorized as waiting for a Named Entity (the total is 107 in CLEF04 for MUSQAT and 97 for QALC and 91 in CLEF05 for MUSQAT and 66 for QALC). Our total number of questions of this category is far beyond the real number in CLEF05. The third line, "non NE answers", concerns the other questions (the complement to 178 in CLEF04 and to 188 in CLEF05). Results are presented when the system just gives one answer and when it gives 5 answers. The last line indicates the best official result of our system on the 200 questions. The official score of MUSQAT was 22 (11%) in CLEF04 and 28 (14%) in CLEF05, thus we can observe that merging answers obtained by different strategies enables a significative gain. We also can notice that if our CLEF05 system better selects sentences, it is less performant on extracting the answers, specially on named entity answers.</p><p>According to the manual evaluation results of bi-terms translations, we have tested an automatic process for filtering Magic-dic translations on CLEF04 questions. So, if a bi-term or a variant form was found in the selected documents, we kept it as a valid translation and we kept its lemmas as valid mono-term translations. When a validated translation existed for a term, the non-validated translations were taken out. When no translation of a bi-term was found in corpus, we assumed that mono-term translations were wrong and we kept Systran translations. In order to improve the coverage of our translation, we added Systran translation for terms absent from We tested MUSQAT with this new selection. Results are shown Table <ref type="table" coords="7,405.11,303.83,3.90,9.96" target="#tab_5">6</ref>. We see that MUSQAT finds relevant documents for 7 supplementary questions (increase of 4%). MUSQAT extracts 7 supplementary correct answers in the top 5 short answers, with 29 answers in rank 1. MUSQAT obtains here slightly better results than QALC with Systran translations, both for short and long answers. We also measured the number of questions for which the selection process based on FASTR indexing provides documents containing the answer pattern. In the original MUSQUAT, it was possible to find the answer for 80% of questions. Term selection allows to improve this value to 85%. These improvements are not significative enough so we had not incorporated them in this year's version, even if we think that this kind of translation validation is worth being tried. So we plan to realize bi-term validation on a larger corpus. Concerning the absence of translations, we began to increase manually our dictionary from lexicons and gazetteers we use for named entities recognition, specially for acronyms and location names, and we plan to use a bilingual aligned corpus.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,227.76,452.15,147.60,9.96"><head></head><label></label><figDesc>Figure 1: MUSCLEF architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,221.64,152.15,156.33,108.24"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="5,232.56,152.15,145.41,108.24"><row><cell cols="3">: MagicDic terms evaluation</cell></row><row><cell>Bi-Terms</cell><cell>#</cell><cell>%</cell></row><row><cell>OK</cell><cell cols="2">135 36</cell></row><row><cell cols="2">Contextually False 24</cell><cell>6.4</cell></row><row><cell>False</cell><cell>74</cell><cell>19.7</cell></row><row><cell>False Transl</cell><cell cols="2">110 29.3</cell></row><row><cell>Absent Transl</cell><cell>32</cell><cell>8.5</cell></row><row><cell>Total False</cell><cell cols="2">240 64</cell></row><row><cell>Total</cell><cell>375</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,182.76,397.55,237.50,119.76"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table coords="5,182.76,397.55,237.50,119.76"><row><cell></cell><cell cols="3">MagicDic terms validated by Fastr</cell><cell></cell></row><row><cell></cell><cell>#</cell><cell cols="2">#retrieved #retrieved</cell><cell></cell></row><row><cell>Bi-terms</cell><cell></cell><cell>without</cell><cell>including</cell><cell>%</cell></row><row><cell></cell><cell></cell><cell>sem.var.</cell><cell>sem.var.</cell><cell></cell></row><row><cell>OK</cell><cell cols="2">135 61</cell><cell>83</cell><cell>61.5</cell></row><row><cell cols="2">Context. False 24</cell><cell>4</cell><cell>7</cell><cell>29.2</cell></row><row><cell>False</cell><cell>74</cell><cell>11</cell><cell>15</cell><cell>20.3</cell></row><row><cell>False Transl</cell><cell cols="2">110 7</cell><cell>19</cell><cell>17.3</cell></row><row><cell cols="2">Absent Transl 32</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Total</cell><cell cols="2">375 82</cell><cell>120</cell><cell>32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,187.32,68.39,228.28,46.32"><head>Table 3 :</head><label>3</label><figDesc>Questions translations evaluation</figDesc><table coords="6,187.32,80.03,228.28,34.68"><row><cell cols="5">Questions Correct Quite Correct False Total</cell></row><row><cell>#</cell><cell>73</cell><cell>12</cell><cell>115</cell><cell>200</cell></row><row><cell>%</cell><cell>36.5</cell><cell>6.0</cell><cell>57.5</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,181.56,191.39,239.67,108.12"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of terms from translated questions</figDesc><table coords="6,232.56,203.03,137.93,96.48"><row><cell>Bi-Terms</cell><cell>#</cell><cell>%</cell></row><row><cell>OK</cell><cell cols="2">126 75.4</cell></row><row><cell cols="2">Contextually False 0</cell><cell>0</cell></row><row><cell>False</cell><cell>41</cell><cell>24.6</cell></row><row><cell>False Transl</cell><cell>0</cell><cell>0</cell></row><row><cell>Absent Transl</cell><cell>0</cell><cell>0</cell></row><row><cell>Total False</cell><cell>41</cell><cell>24.6</cell></row><row><cell>Total</cell><cell>167</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,90.00,68.39,422.78,233.40"><head>Table 5 :</head><label>5</label><figDesc>By this way, we took off 253 bi-terms in 112 questions, and added 37 translations, with 12 bi-terms, which concerns 35 questions. The last improvement consisted in adding Systran translations that were different from Magic-dic translations (138 terms in 96 questions) to the filtered terms. This last set of terms was compound of 1311 translations for 836 terms in 200 questions (522 terms with 1 translation, 199 with 2 translations, 81 with 3 translations, 25 with 4 translations, 6 with 5 translations and 3 with 6 translations).</figDesc><table coords="7,90.00,68.39,382.89,173.64"><row><cell></cell><cell></cell><cell cols="4">Results at CLEF04 and CLEF05 MUSQAT04 QALC04 MUSQAT05 QALC05</cell></row><row><cell cols="2">Sentences 5 first ranks</cell><cell>56 (31 %)</cell><cell>65 (37 %)</cell><cell>78 (41 %)</cell><cell>87(46 %)</cell></row><row><cell>NE</cell><cell>Rank 1</cell><cell>17</cell><cell>26</cell><cell>16</cell><cell>9</cell></row><row><cell>answers</cell><cell>5 first ranks</cell><cell>33</cell><cell>37</cell><cell>24</cell><cell>11</cell></row><row><cell>Non NE</cell><cell>Rank 1</cell><cell>7</cell><cell>3</cell><cell>16</cell><cell>16</cell></row><row><cell>answers</cell><cell>5 first ranks</cell><cell>12</cell><cell>8</cell><cell>22</cell><cell>24</cell></row><row><cell>Total</cell><cell>Rank 1</cell><cell>24</cell><cell>29</cell><cell>32</cell><cell>25</cell></row><row><cell></cell><cell>%</cell><cell>12%</cell><cell>14.5%</cell><cell>17%</cell><cell>13%</cell></row><row><cell></cell><cell>5 first ranks</cell><cell>44</cell><cell>45</cell><cell>46</cell><cell>39</cell></row><row><cell cols="2">Fusion (official results)</cell><cell cols="2">38 (19 %)</cell><cell cols="2">38 (19 %)</cell></row><row><cell>the dictionary.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,203.52,346.31,195.96,107.04"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table coords="7,203.52,346.31,195.96,107.04"><row><cell></cell><cell cols="2">MUSQAT new results</cell></row><row><cell></cell><cell></cell><cell>MUSQAT</cell></row><row><cell>Sentences</cell><cell>5 first ranks</cell><cell>67</cell></row><row><cell>NE answers</cell><cell>Rank 1</cell><cell>25</cell></row><row><cell></cell><cell>5 first ranks</cell><cell>41</cell></row><row><cell cols="2">Non NE answers Rank 1</cell><cell>4</cell></row><row><cell></cell><cell>5 first ranks</cell><cell>10</cell></row><row><cell>Total</cell><cell>Rank 1</cell><cell>29 (14,5%)</cell></row><row><cell></cell><cell>5 first ranks</cell><cell>51</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,677.56,112.08,7.97"><p>http://www.vinartus.net/spa/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,687.04,87.02,7.97"><p>http://www.reverso.net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,105.24,696.52,218.31,7.97"><p>MG for Managing Gigabytes http://www.cs.mu.oz.au/mg/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,105.24,637.36,120.59,7.97"><p>http://translation2.paralink.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,105.24,646.96,132.82,7.97"><p>http://babel.altavista/translate.dyn</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,105.24,656.44,145.14,7.97"><p>http://www.google.com/language tools</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="3,105.24,665.92,108.49,7.97"><p>http://granddictionnaire.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="3,105.24,675.40,117.83,7.97"><p>http://www.freetranslation.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="3,105.24,684.88,118.09,7.97"><p>http://magic-dic.homeunix.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="3,105.24,694.48,89.54,7.97"><p>http://www.freedict.de/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,93.68,651.95,69.93,9.96" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Brill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,168.41,651.95,344.37,9.96;7,100.56,663.95,224.65,9.96;7,90.00,683.15,422.80,9.96;7,100.56,695.03,232.81,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,407.52,651.95,105.25,9.96;7,100.56,663.95,43.49,9.96">Data-Intensive Question Answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http:www.ldc.upenn.edureadmefilescelex.readme.html" />
	</analytic>
	<monogr>
		<title level="m" coord="7,152.00,663.95,82.52,9.96;7,125.63,695.03,172.25,9.96">Actes Consortium for Lexical Resources</title>
		<editor>
			<persName><forename type="first">Upenns</forename></persName>
		</editor>
		<meeting>s Consortium for Lexical Resources<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CELEX</publisher>
			<date type="published" when="1998">2001. 1998. 1998</date>
		</imprint>
	</monogr>
	<note>TREC 10 Notebook</note>
</biblStruct>

<biblStruct coords="8,94.24,61.43,108.22,9.96" xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>De Chalendar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,207.26,61.43,305.39,9.96;8,100.56,73.43,412.24,9.96;8,100.56,85.31,411.98,9.96;8,100.56,97.31,136.30,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,412.44,73.43,100.36,9.96;8,100.56,85.31,318.84,9.96">The Question Answering System QALC at LIMSI, Experiments in Using Web and WordNet</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>De Chalendar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dalmas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Elkateb-Gara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hurault-Plantet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Illouz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Monceaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Robba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vilnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,429.25,85.31,18.69,9.96">Trec</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="457" to="467" />
			<date type="published" when="2002">2002</date>
			<publisher>Notebook</publisher>
			<pubPlace>Gaithersburg, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.57,117.23,101.65,9.96" xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Chu-Carroll</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,200.90,117.23,311.72,9.96;8,100.56,129.23,412.22,9.96;8,100.56,141.11,227.03,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,197.79,129.23,310.16,9.96">A Multi-Strategy and multi-source Approach to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Welty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krzysztof</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Ferruci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,100.56,141.11,82.52,9.96">TREC 11 Notebook</title>
		<meeting><address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.45,161.03,78.13,9.96" xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Clarke</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,177.39,161.03,335.41,9.96;8,100.56,173.03,411.94,9.96;8,100.56,185.03,127.65,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,126.92,173.03,335.32,9.96">Web Reinforced Question Answering (MultiText Experiments for Trec 2001)</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Mclearn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,470.30,173.03,42.20,9.96;8,100.56,185.03,36.96,9.96">TREC 10 Notebook</title>
		<meeting><address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.74,204.95,59.62,9.96" xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,163.74,204.95,348.90,9.96;8,100.56,216.83,46.28,9.96;8,90.00,236.75,4.99,9.96" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,248.99,204.95,182.82,9.96">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.99,236.75,96.43,9.96" xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Hermjacob</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,196.22,236.75,316.52,9.96;8,100.56,248.75,411.94,9.96;8,100.56,260.75,82.35,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,406.29,236.75,106.45,9.96;8,100.56,248.75,313.98,9.96">Natural Language Based Reformulation Resource and Web Exploitation for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,423.69,248.75,84.20,9.96">TREC 11 Notebook</title>
		<meeting><address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.81,280.67,417.83,9.96;8,100.56,292.55,412.07,9.96;8,100.56,304.55,33.54,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,446.58,280.67,66.07,9.96;8,100.56,292.55,260.05,9.96">Is It the Right Answer? Exploiting Web redundancy for Answer Validation</title>
		<author>
			<persName coords=""><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,368.18,292.55,127.30,9.96">Proceedings of the 40 th ACL</title>
		<meeting>the 40 th ACL</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="425" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.81,324.47,418.08,9.96;8,100.56,336.47,411.89,9.96;8,100.56,348.35,20.50,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,431.83,324.47,81.06,9.96;8,100.56,336.47,253.82,9.96">Mining Knowledge from Repeated Co-occurrences: DIOGENE at TREC-2002</title>
		<author>
			<persName coords=""><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,362.98,336.47,82.76,9.96">TREC 11 Notebook</title>
		<meeting><address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,95.02,368.27,91.72,9.96" xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Moldovan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,191.42,368.27,321.34,9.96;8,100.56,380.27,411.97,9.96;8,100.56,392.27,127.65,9.96;8,100.56,408.11,49.62,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,302.95,380.27,156.79,9.96">LCC Tools for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Morarescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lacatusu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Novischi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Badalescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bolohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,469.37,380.27,43.16,9.96;8,100.56,392.27,36.96,9.96">TREC 11 Notebook</title>
		<meeting><address><addrLine>Gaithersburg, USA CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.28,428.03,77.11,9.96" xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,176.19,428.03,336.45,9.96;8,100.56,440.03,412.01,9.96;8,100.56,452.03,412.05,9.96;8,100.56,463.91,25.30,9.96" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,418.29,428.03,94.35,9.96;8,100.56,440.03,158.17,9.96">CLEF 2004: Ad Hoc Track Overview and Results Analysis</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,265.75,440.03,246.81,9.96;8,100.56,452.03,56.84,9.96">Fifth Workshop of the Cross-Language Evaluation Forum (CLEF 2004)</title>
		<title level="s" coord="8,161.90,452.03,185.10,9.96">Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>in print</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
