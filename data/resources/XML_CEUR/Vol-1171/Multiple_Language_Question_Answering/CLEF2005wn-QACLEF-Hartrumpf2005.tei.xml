<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,124.62,181.23,353.76,15.48;1,172.52,208.41,257.96,15.48">Extending Knowledge and Deepening Linguistic Processing for Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,269.34,246.04,64.32,8.64"><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
							<email>sven.hartrumpf@fernuni-hagen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Hagen at QA@CLEF</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Intelligent Information and Communication Systems (IICS</orgName>
								<orgName type="institution">University of Hagen (FernUniversit√§t in Hagen</orgName>
								<address>
									<postCode>58084</postCode>
									<settlement>Hagen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,124.62,181.23,353.76,15.48;1,172.52,208.41,257.96,15.48">Extending Knowledge and Deepening Linguistic Processing for Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">50A0D1A1179A294194C10A94447E6849</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing-Linguistic processing</term>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Search process</term>
					<term>H.3.4 [Information Storage and Retrieval]: Systems and Software-Performance evaluation (efficiency and effectiveness)</term>
					<term>I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-Semantic networks</term>
					<term>I.2.7 [Artificial Intelligence]: Natural Language Processing-Language parsing and understanding, Language generation Experimentation, Measurement, Performance Question answering, Questions beyond factoids, Deep semantic processing of questions and documents</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The German question answering (QA) system InSicht participated in QA@CLEF for the second time. It relies on complete sentence parsing, inferences, and semantic representation matching. This year, the system was improved in two main directions. First, the background knowledge was extended by large semantic networks and large rule sets. InSicht's query expansion step can produce more alternatives using these resources. A second direction for improvement was to deepen linguistic processing by treating a phenomenon that appears prominently on the level of text semantics: coreference resolution.</p><p>A new source of lexico-semantic relations and equivalence rules has been established based on compound analyses. WOCADI's compound analysis module determined the structure and semantics of compounds when parsing the German QA@CLEF corpus and the German GIRT (German Indexing and Retrieval Test database) corpus. The compound analyses were used in three ways: to project lexico-semantic relations from compound parts to compounds, to establish a subordination hierarchy between compounds, and to derive equivalence rules between nominal compounds and their analytic counterparts, e.g. between Reisimport ('rice import') and Import von Reis ('import of rice'). Another source of new rules were verb glosses from GermaNet, a German WordNet variant. The glosses were parsed and automatically formalized.</p><p>The lack of coreference resolution in InSicht was one major source of missing answers in QA@CLEF 2004. Therefore the coreference resolution module CORUDIS was integrated into the parsing during document processing. The resulting coreference partition of mentions (or markables) from a document is used to derive additional networks where mentions are replaced by mentions from the corresponding coreference chain in that partition.</p><p>The central step in the QA system InSicht, matching (one by one) semantic networks derived from the question parse to document sentence networks, was generalized. Now, a question network can be split at certain semantic relations (e.g. relations for local or temporal specifications); the resulting semantic networks are conjunctively connected.</p><p>To evaluate the different extensions, the QA system was run on all 400 German questions from QA@CLEF 2004 and 2005 with varying setups. Some of these extensions showed positive effects, but currently they are minor and not yet statistically significant. At least three explanations play a role. First, the differences in the semantic representation of questions and document sentences are often minimal and do not require much background knowledge to be related. Second, there are some questions that need a lot of inferential steps. For many such inference chains, formalized inferential knowledge like axioms and meaning postulates for concepts are missing. Third, the low recall values of some natural language processing modules, e.g. the parser and the coreference resolution module, can cause a missing inferential link and thereby a wrong empty answer. Work on the robustness of these modules will help to answer more questions correctly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The German question answering (QA) system InSicht participated in QA@CLEF for the second time. This year, the system was improved in two main directions. First, the background knowledge was extended by large semantic networks and rule sets. InSicht's query expansion step produces more alternative representations using these resources. A second direction for improvement was to deepen linguistic processing by treating a phenomenon that appears prominently on the level of text semantics: coreference resolution.</p><p>The paper starts with a summary of the basic InSicht system (Section 2). Then, the most important improvements since QA@CLEF 2004 are described (Section 3). In Section 4, the resulting system is evaluated on the 400 German questions from QA@CLEF 2004 and 2005. The contribution of different modifications is investigated by running the system with different setups. Some conclusions appear in the final Section 5.</p><p>The deep semantic question answering system InSicht <ref type="bibr" coords="3,306.46,137.47,73.29,8.64" target="#b3">(Hartrumpf, 2005)</ref> relies on complete sentence parsing, inferences, and semantic representation matching. It comprises six main steps.</p><p>In the document processing step, all documents from a given collection are preprocessed by transforming them into a standard XML format (CES, corpus encoding standard, <ref type="bibr" coords="3,398.86,181.95,63.66,8.64" target="#b8">Ide et al. (1996)</ref>) with word, sentence, and paragraph borders marked up by XML elements. Then, all preprocessed documents are parsed by the WOCADI parser <ref type="bibr" coords="3,214.95,211.60,71.64,8.64" target="#b2">(Hartrumpf, 2003)</ref>, yielding a syntactic dependency structure and more importantly a semantic network representation of the MultiNet formalism <ref type="bibr" coords="3,376.93,226.42,58.85,8.64" target="#b5">(Helbig, 2005)</ref> for each document sentence.</p><p>In the second step (query processing), the user's question is parsed by the same parser that processed the documents. Determining the sentence type (here, often a subtype of question) is especially important because it controls some parts of two later steps: query expansion and answer generation. The system does not deal with (expected) answer types or similar concepts; whatever semantic network for a document sentence matches the question and can be reformulated as a natural language expression is a candidate answer.</p><p>Next comes query expansion: Equivalent and similar semantic networks are derived by means of lexicosemantic relations from a computer lexicon (HaGenLex, see <ref type="bibr" coords="3,329.65,359.84,90.97,8.64" target="#b4">Hartrumpf et al. (2003)</ref>) and a lexical database (GermaNet), equivalence rules, and inferential rules like entailments for situations (applied in backward chaining). The result is a set of disjunctively connected semantic networks that try to capture variations of explicit and implicit representations of sentences possibly containing an answer for the user's question.</p><p>In the fourth step (semantic network matching), all document sentences matching at least one of the semantic networks from query expansion are collected. A two-level approach is chosen for efficiency reasons. First, an index of concepts (disambiguated words with IDs from HaGenLex) is consulted with the relevant concepts from the query networks. Second, the retrieved documents are compared sentence network by sentence network to find a match with a query network.</p><p>Answer generation is next: natural language generation rules are applied to matching semantic networks and try to generate a natural language answer from the deep semantic representations. The sentence type and the semantic network itself control the selection of answer rules. The rules also act as a filter for uninformative or bad answers. The result of this step are tuples of generated answer string, answer score, supporting document ID, and supporting sentence ID.</p><p>To deal with typically many candidate answers resulting from answer generation, an answer selection step is required at the end. It implements a strategy that combines a preference for more frequent answers and a preference for more elaborate answers. The best answers (by default only the best answer) and the supporting sentences (and/or the IDs of supporting sentences or documents) are presented to the user that posed the question.</p><p>3 Improvements over the System for QA@CLEF 2004</p><p>There are several areas where InSicht has been improved since QA@CLEF 2004. The most notably changed phases are document processing, query expansion, and semantic network matching.</p><p>(sub "riesenpython.1.1" "riesenschlange.1.1") ('giant python' 'giant snake') (sub "rollhockeynationalmannschaft.1.1" "hockeymannschaft.1.1") ('roller hockey national team' 'hockey team') (sub "weizenexport.1.1" "getreideexport.1.1") ('wheat export' 'crop export') (syno "metrosuizid.1.1" "u-bahnselbstmord.1.1") ('metro suicide' 'underground train self-murder') (syno "rehabilitationskrankenhaus.1.1" "rehaklinik .1.1") ('rehabilitation hospital' 'rehab hospital (clinic)') (syno "wirtschaftmodell.1.3" "√∂konomiemodell.1.3") ('(economy) model' 'economy model') </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Processing</head><p>The coverage of the WOCADI parser has been increased so that for 51% of all QA corpus sentences a full semantic network is produced (compared to 49% for QA@CLEF 2004, see <ref type="bibr" coords="4,413.66,258.15,69.23,8.64" target="#b3">Hartrumpf (2005)</ref>). This was achieved by extending the lexicon HaGenLex and by refining the parser itself. The concept index (a mapping from concept IDs to document IDs), which is used by the matcher for reducing run time, provides more efficient creation and lookup operations than last year because we switched from an external binary tree to a freely available system, qdbm (Quick Database Manager, http://qdbm.sourceforge.net).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">More Query Expansions by Larger Lexico-Semantic Networks and Larger Rule Sets</head><p>A new source of lexico-semantic relations and equivalence rules has been established based on compound analyses. WOCADI's compound analysis module determines structure and semantics of compounds when parsing a text corpus. The 470,000 compound analyses from parsing the German QA@CLEF corpus and the German GIRT corpus were collected. Only compounds that the compound analysis module assigned a semantics where the right (base) compound part is a hyperonym of the compound were considered. Given a compound, synonyms and hyponyms<ref type="foot" coords="4,246.12,460.25,3.69,6.39" target="#foot_0">1</ref> of each compound part are collected by following corresponding relations in the lexicon. Then, each element from the Cartesian product of these alternatives is looked up in the compound analyses mentioned above. If it exists with a given minimal frequency (currently: 1), a relation is inferred based upon the relations between corresponding parts. In case of a contradiction (e.g. the first parts are in a hyponymy relation while the second parts are in a hyperonymy relation), no relation is inferred. This algorithm delivered 16,526 relations: 5,688 SUB (subordination) edges and 10,838 SYNO (synonymy) edges. All of them are lexico-semantic relations between compounds. Some examples are shown in Figure <ref type="figure" coords="4,157.00,565.95,3.74,8.64" target="#fig_0">1</ref>.</p><p>A more direct use of compound analyses is the extraction of subordination edges (MultiNet uses SUB, SUBR, and SUBS edges for nominal compounds depending upon the nouns ontological sort) representing a hyponymy relation between a compound and its base noun (or adjective). This process led to 387,326 new edges.</p><p>A third use of automatic compound analyses is the production of equivalence rules for complementfilling compounds. One can generate for such compounds an equivalence to an analytical form, e.g. between Reisimport ('rice import') and Import von Reis ('import of rice'). 2 Currently, only compounds where the base noun has exactly one complement in the lexicon that can (semantically) be realized by the determining noun are treated in this way, so that for 360,000 analyzed nominal compounds in the QA corpus ((pre ((member ?r1 (preds subs)))) (rule ((?r1 ?n1 "drogenkonsum.1.1") ; drug consumption &lt;-&gt; (?r1 ?n1 "konsum.1.1") (aff ?n1 ?n2) (sub ?n2 "droge.1.1"))) (name "compound_analysis.sg.drogenkonsum.1.1")) ((pre ((member ?r1 (preds subs)))) (rule ((?r1 ?n1 "geb√§udesanierung.1.1") ; building sanitation &lt;-&gt; (?r1 ?n1 "sanierung.1.1") (aff ?n1 ?n2) (sub ?n2 "geb√§ude.1.1"))) (name "compound_analysis.sg.geb√§udesanierung.1.1")) ((pre ((member ?r1 (preds subs)))) (rule ((?r1 ?n1 "holzerzeugung.1.1") ; wood production &lt;-&gt; (?r1 ?n1 "erzeugung.1.1") (rslt ?n1 ?n2) (sub ?n2 "holz.1.1"))) (name "compound_analysis.sg.holzerzeugung.1.1"))</p><p>Figure <ref type="figure" coords="5,122.90,404.19,3.88,8.64">2</ref>: Three automatically generated rules for compounds involving a complement of the base noun. around 13,000 rules were generated. Three simplified MultiNet rules are shown in Figure <ref type="figure" coords="5,462.58,436.23,3.74,8.64">2</ref>. Variables are preceded by a question mark. The attribute pre contains preconditions for variables occurring on both sides of an equivalence rule. The MultiNet relation PREDS corresponds to SUBS and instantiates (or subordinates) not just a single concept but a set of concepts. The relations AFF (affected object) and RSLT (result of a situation) stem from the HaGenLex characterization of the direct object of the base nouns Konsum ('consumption'), Sanierung ('sanitation'), and Erzeugung ('production'). Such an equivalence rule fired only for question qa05 023 (Welcher fr√ºhere Fu√üballspieler wurde wegen Drogenkonsum verurteilt?, 'Which former soccer player was convicted of taking drugs?') because most questions from QA@CLEF 2004 and 2005 are not related to such compounds or their analytical forms.</p><p>Another set of rules available to this year's InSicht stems from parsing verb glosses from GermaNet (a German WordNet variant) and further automatic formalization (see <ref type="bibr" coords="5,374.15,584.48,89.45,8.64" target="#b0">Gl√∂ckner et al. (2005)</ref> for details). Each rule relates one verb reading with one or more readings of other verbs. None of these rules fired during query expansion of QA@CLEF questions. This was not too much of a surprise because the rule set is quite small (around 200 rules). Nevertheless, this path seems promising as soon as more German glosses become available (from GermaNet or other sources like Wikipedia).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Coreference Resolution for Documents</head><p>Looking at last year's questions that turned out to be hard to answer for most systems (see <ref type="bibr" coords="5,470.95,697.19,42.05,8.64;5,90.00,712.02,26.56,8.64" target="#b3">Hartrumpf (2005)</ref> for error classes and frequencies) and looking at some of our own test questions, the lack of coreference resolution was identified as one major source of errors. (This lack caused 6% of InSicht's wrong empty answers for questions from QA@CLEF 2004.) Therefore the coreference resolution module CORUDIS (COreference RUles with DIsambiguation Statistics, see <ref type="bibr" coords="6,368.05,112.34,66.74,8.64" target="#b2">Hartrumpf (2003</ref><ref type="bibr" coords="6,442.33,112.34,23.24,8.64" target="#b1">Hartrumpf ( , 2001) )</ref> for details) was integrated into the parsing during document processing. If a coreference partition of mentions (or markables) from a document is found the simplified and normalized document networks are extended by networks where mentions are replaced by mentions from the corresponding coreference chain in that partition. For example, if document network d contains mention m i and m i is in a nontrivial (i.e. n &gt; 1) coreference chain m 1 , . . . , m i , . . . , m n the following networks are added to the document representation: substituted by the semantics of mention m 2 . Some mention substitutions are avoided if no performance improvement is possible or likely (e.g. if the semantic representations of m 1 and m 2 are identical), but there is still room for beneficial refinements.</p><formula xml:id="formula_0" coords="6,90.00,200.97,89.10,11.60">d m i |m 1 , . . . , d m i |m i-1 , d m i</formula><p>The CORUDIS module is not yet efficient enough (which is not surprising because finding the best coreference partition of mentions is NP-hard) so that the search had to be limited by some parameter settings in order to reduce run time. On the down side, this caused that only for 40% of all texts a partition of mentions was found. Furthermore, only 70% of all texts had been analyzed for coreferences when the evaluation from Section 4 was run. Therefore the improvements achievable by coreference resolution will increase in the near future.</p><p>Coreference resolution has been rarely described for natural language processing (NLP) applications. For example in information extraction, <ref type="bibr" coords="6,248.00,364.35,84.17,8.64" target="#b10">Zelenko et al. (2004)</ref> presented and extrinsically evaluated several coreference resolution modules for such an application but it restricts coreferences to mentions that could be relevant for the given information extraction task. In contrast, WOCADI's coreference resolution module treats all mentions that meet the MUC definition of a markable <ref type="bibr" coords="6,342.64,408.83,129.91,8.64" target="#b7">(Hirschman and Chinchor, 1997)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">More Flexible Matching by Splitting Question Networks</head><p>Last year's InSicht matched semantic networks derived from a question parse to document sentence networks one by one. A more flexible approach turned out to be beneficial for IR and geographical IR; so it was tested in InSicht's QA mode, too. The flexibility comes from the fact that a question network is split if certain graph topologies exist: a network is split in two networks at CIRC, CTXT (nonrestrictive and restrictive context, respectively), LOC (location of objects or situations), and TEMP (temporal specification) edges. The resulting parts are conjunctively connected. Section 4 shows which splitting configurations turned out to be most profitable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>The current InSicht QA system has been evaluated on the QA@CLEF questions from 2004 and 2005. To investigate the impact of different improvements described in Section 3 the setup was varied in different ways, as can be seen in the second and third column of Table <ref type="table" coords="6,342.38,643.90,3.74,8.64" target="#tab_0">1</ref>. The evaluation metrics reported are the number of right, inexact, and wrong answers and the K1-measure (see <ref type="bibr" coords="6,366.91,658.73,79.95,8.64" target="#b6">Herrera et al. (2004)</ref> for a definition). Note that the number of unsupported answers was always zero, so it was omitted for brevity.</p><p>For better comparison, the results for InSicht's official run at QA@CLEF 2004 are shown, too. The K1-measure was much lower than this year because the confidence score of last year's system was tuned for confidence-weighted score (CWS). Now InSicht tries to optimize the K1-measure because the K1-measure seems to be a more adequate metric for evaluating QA systems <ref type="bibr" coords="6,343.22,732.85,82.02,8.64" target="#b6">(Herrera et al., 2004)</ref>. To experiment with cross-language QA, a machine translation of questions was employed. After the machine translation system Systran (as provided on the web) had translated the 200 English questions (from QA@CLEF 2005) into German, they were put into the standard InSicht system. The number of right answers dropped by around 50%, which was mainly due to incorrect or ungrammatical translations. Some translation problems seemed to be systematic, so that a simple postprocessing component could correct some wrong translations, e.g. the temporal question word when was translated as als instead of wenn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The QA system InSicht was extended by new sources of knowledge. From automatic compound analyses, large semantic networks and numerous equivalence rules were derived. The linguistic processing was deepened by integrating the coreference resolution module CORUDIS into document processing.</p><p>When evaluated on all 400 questions from QA@CLEF 2004 and 2005 some of these extensions showed positive effects. But unfortunately the effects are minor and not yet statistically significant. The reasons need further investigation but here are three preliminary observations. First, the differences in the semantic representation of questions and document sentences are often minimal and do not require the kind of knowledge that was generated. We hope that larger test sets will show significant positive effects.</p><p>On the other extreme, there are some questions that need much more inferential steps than currently produced by query expansion. The matching approach is quite strict (precision-oriented, while for example the QA system described by <ref type="bibr" coords="7,205.89,595.82,80.79,8.64" target="#b9">Jijkoun et al. (2004)</ref> is recall-oriented) and can require long inference chains in order to find answers. The main hindrance to building such chains are missing pieces of formalized inferential knowledge, like axioms for MultiNet and meaning postulates for concepts. Some parts of this knowledge can be automatically generated, see for example Section 3.2.</p><p>A third explanation regards the quality of some NLP modules. The still limited recall values of the parser (see Section 3.1), the coreference resolution module (see Section 3.3), and other modules can cause that an inferential link (e.g. a coreference between two nominal phrases) is missing so that a question remains unanswered and a wrong empty (NIL) answer is produced. Such negative effects are typical for applications building on deep syntactico-semantic processing. Therefore the robustness of some NLP modules will be increased in order to answer more questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,156.15,190.21,290.70,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of inferred lexico-semantic relations for compounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,179.60,204.78,16.66,7.65;6,197.25,200.97,42.15,11.35;6,243.41,201.11,51.49,11.46;6,298.91,201.11,213.59,9.76"><head></head><label></label><figDesc>|m i+1 , . . . , d m i |m n where d m 1 |m 2 denotes network d with the semantics of mention m 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,90.00,118.98,423.00,148.33"><head>Table 1 :</head><label>1</label><figDesc>Results for German question sets from QA@CLEF 2004 and 2005. lexsem stands for lexicosemantics relations projected to compounds and hypo for hyponymy relations for compounds (see Section 3.2); S is the set of relations where query networks can be split (see Section 3.4).</figDesc><table coords="7,95.98,157.05,403.08,110.25"><row><cell>Question Set</cell><cell></cell><cell>Setup</cell><cell></cell><cell>Results</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Query Expansion Matching</cell><cell cols="3"># Right # Inexact # Wrong</cell><cell>K1</cell></row><row><cell>2004</cell><cell cols="2">run from QA@CLEF 2004</cell><cell>67</cell><cell>7</cell><cell>0</cell><cell>-0.327</cell></row><row><cell>2004</cell><cell>lexsem</cell><cell>no coreference, S = {LOC}</cell><cell>83</cell><cell>6</cell><cell>0</cell><cell>0.285</cell></row><row><cell>2004</cell><cell>lexsem</cell><cell>coreference, S = {LOC}</cell><cell>83</cell><cell>6</cell><cell>0</cell><cell>0.285</cell></row><row><cell>2005</cell><cell>lexsem</cell><cell>no coreference, S = {}</cell><cell>80</cell><cell>7</cell><cell>1</cell><cell>0.260</cell></row><row><cell>2005</cell><cell>lexsem</cell><cell>no coreference, S = {LOC}</cell><cell>84</cell><cell>7</cell><cell>1</cell><cell>0.280</cell></row><row><cell>2005</cell><cell>lexsem</cell><cell>coreference, S = {LOC}</cell><cell>84</cell><cell>8</cell><cell>0</cell><cell>0.280</cell></row><row><cell>2005</cell><cell>lexsem, hypo</cell><cell>coreference, S = {LOC}</cell><cell>86</cell><cell>8</cell><cell>0</cell><cell>0.290</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,104.35,718.52,408.65,6.91;4,90.00,727.99,286.72,6.91"><p>Hyperonyms can be ignored because hyperonymy is the inverse relation of hyponymy and all inferable relations will also be produced when treating the compound analyses containing the corresponding hyperonym.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,104.35,737.80,408.65,6.91;4,90.00,747.12,151.92,7.05"><p>By way of a lexical change relation, the representations of both formulations are linked to the representation of a formulation with a verb: Reis importieren ('to import rice').</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,90.00,137.47,423.00,8.64;8,99.96,152.12,413.04,8.82;8,99.96,166.94,413.04,8.82;8,99.96,181.95,261.27,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,340.05,137.47,172.95,8.64;8,99.96,152.30,38.49,8.64">From GermaNet glosses to formal meaning postulates</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Gl√∂ckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rainer</forename><surname>Osswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,158.85,152.12,354.15,8.59;8,99.96,166.94,114.84,8.59">Sprachtechnologie, mobile Kommunikation und linguistische Ressourcen -Beitr√§ge zur GLDV-Tagung 2005 in Bonn</title>
		<editor>
			<persName><forename type="first">Bernhard</forename><forename type="middle">;</forename><surname>Fisseni</surname></persName>
		</editor>
		<editor>
			<persName><surname>Hans-Christian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernhard</forename><surname>Schmitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">;</forename><surname>Schr√∂der</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Petra</forename><surname>Wagner</surname></persName>
		</editor>
		<meeting><address><addrLine>Frankfurt am Main</addrLine></address></meeting>
		<imprint>
			<publisher>Peter Lang</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="394" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,204.74,423.00,8.64;8,99.96,219.39,413.03,8.82;8,99.96,234.39,275.90,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,195.16,204.74,300.72,8.64">Coreference resolution with syntactico-semantic rules and corpus statistics</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W01-0717" />
	</analytic>
	<monogr>
		<title level="m" coord="8,99.96,219.39,371.42,8.59">Proceedings of the Fifth Computational Natural Language Learning Workshop (CoNLL-2001)</title>
		<meeting>the Fifth Computational Natural Language Learning Workshop (CoNLL-2001)<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,257.01,423.00,8.82;8,99.96,272.01,168.76,8.64" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,195.69,257.01,218.81,8.59">Hybrid Disambiguation in Natural Language Analysis</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Der Andere Verlag</publisher>
			<pubPlace>Osnabr√ºck, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,294.81,423.00,8.64;8,99.96,309.45,413.04,8.59;8,99.96,324.27,413.03,8.82;8,99.96,339.10,365.87,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,193.64,294.81,302.08,8.64">Question answering using sentence parsing and semantic network matching</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,99.96,309.45,413.04,8.59;8,99.96,324.27,41.51,8.59">Multilingual Information Access for Text, Speech and Images: Results of the Fifth CLEF Evaluation Campaign</title>
		<title level="s" coord="8,165.35,339.10,166.77,8.59">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">;</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,362.07,423.00,8.64;8,99.96,376.72,413.04,8.82;8,99.96,391.72,17.43,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,348.21,362.07,164.79,8.64;8,99.96,376.90,214.69,8.64">The semantically based computer lexicon HaGenLex -Structure and technological environment</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><forename type="middle">;</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Helbig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rainer</forename><surname>Osswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,323.27,376.72,143.51,8.59">Traitement automatique des langues</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="81" to="105" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,414.34,423.00,8.82;8,99.96,429.34,36.25,8.64" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,200.24,414.34,273.33,8.59">Knowledge Representation and the Semantics of Natural Language</title>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Helbig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,452.14,423.00,8.64;8,99.96,466.78,413.04,8.59;8,99.96,481.61,363.77,8.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,324.42,452.14,173.13,8.64">Question answering pilot task at CLEF 2004</title>
		<author>
			<persName coords=""><forename type="first">Jes√∫s</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Pe√±as</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,99.96,466.78,413.04,8.59;8,99.96,481.61,61.90,8.59">Results of the CLEF 2004 Cross-Language System Evaluation Campaign, Working Notes for the CLEF 2004 Workshop</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Francesca</forename><surname>Borri</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="445" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,504.58,423.00,8.64;8,99.96,519.22,413.04,8.82;8,99.96,534.23,100.15,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,297.49,504.58,196.57,8.64">MUC-7 coreference task definition (version 3.0)</title>
		<author>
			<persName coords=""><forename type="first">Lynette</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nancy</forename><surname>Chinchor</surname></persName>
		</author>
		<ptr target="http://www.itl.nist.gov/iaui/894.02/relatedprojects/muc/" />
	</analytic>
	<monogr>
		<title level="m" coord="8,99.96,519.22,275.96,8.59">Proceedings of the 7th Message Understanding Conference (MUC-7)</title>
		<meeting>the 7th Message Understanding Conference (MUC-7)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,556.84,423.00,8.82;8,99.96,571.85,67.28,8.64" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,336.64,556.84,108.25,8.59">Corpus Encoding Standard</title>
		<author>
			<persName coords=""><forename type="first">Nancy</forename><forename type="middle">;</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Priest-Dorman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>V√©ronis</surname></persName>
		</author>
		<ptr target="http://www.cs.vassar.edu/CES/" />
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,594.64,423.00,8.64;8,99.96,609.29,413.04,8.82;8,99.96,624.11,413.03,8.82;8,99.96,639.12,185.94,8.64" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,131.32,609.47,196.40,8.64">The University of Amsterdam at QA@CLEF 2004</title>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><forename type="middle">;</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gilad</forename><surname>Mishne; Maarten De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>David Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karin</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,344.72,609.29,168.28,8.59;8,99.96,624.11,300.25,8.59">Results of the CLEF 2004 Cross-Language System Evaluation Campaign, Working Notes for the CLEF 2004 Workshop</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Francesca</forename><surname>Borri</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="321" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,661.91,423.00,8.64;8,99.96,676.56,413.03,8.82;8,99.96,691.56,399.83,8.64" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,341.21,661.91,171.79,8.64;8,99.96,676.73,29.27,8.64">Coreference resolution for information extraction</title>
		<author>
			<persName coords=""><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Tibbetts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,149.43,676.56,272.94,8.59">ACL-2004: Workshop on Reference Resolution and its Applications</title>
		<editor>
			<persName><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Farwell</surname></persName>
		</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
