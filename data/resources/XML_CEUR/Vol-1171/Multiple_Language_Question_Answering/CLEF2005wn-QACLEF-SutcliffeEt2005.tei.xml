<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,92.13,74.39,411.24,14.50;1,232.41,90.95,130.62,14.50">Cross-Language French-English Question Answering using the DLT System at CLEF 2005</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,188.13,114.75,88.71,8.96"><forename type="first">Richard</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
							<email>richard.sutcliffe@ul.iemichael.mulcahy@ul.ieigal.gabbay@ul.ieaoife.ogorman@ul.ie</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Documents and Linguistic Technology Group Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.05,114.75,68.10,8.96"><forename type="first">Michael</forename><surname>Mulcahy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Documents and Linguistic Technology Group Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.41,114.75,49.14,8.96"><forename type="first">Igal</forename><surname>Gabbay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Documents and Linguistic Technology Group Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,202.29,126.51,64.09,8.96"><forename type="first">Aoife</forename><surname>O'gorman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Documents and Linguistic Technology Group Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.81,126.51,52.01,8.96"><forename type="first">Kieran</forename><surname>White</surname></persName>
							<email>kieran.white@ul.iedarina.slattery@ul.ie</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Documents and Linguistic Technology Group Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,332.85,126.51,60.66,8.96"><forename type="first">Darina</forename><surname>Slattery</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Documents and Linguistic Technology Group Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,92.13,74.39,411.24,14.50;1,232.41,90.95,130.62,14.50">Cross-Language French-English Question Answering using the DLT System at CLEF 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">31D1AAA899B6602B7881CA02B4990D94</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DLT System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Outline</head><p>The basic architecture of our factoid system is standard in nature and comprises query type identification, query analysis and translation, retrieval query formulation, document retrieval, text file parsing, named entity recognition and answer entity selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query Type Identification</head><p>As last year, simple keyword combinations and patterns are used to classify the query into a fixed number of types. Currently there are 69 categories plus the default 'unknown'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Query Analysis and Translation</head><p>This stage is almost identical to last year. We start off by tagging the Query for part-of-speech using XeLDA (2004). We then carry out shallow parsing looking for various types of phrase. Each phrase is then translated using three different methods. Two translation engines and one dictionary are used. The engines are Reverso (2004) and WorldLingo (2004)  which were chosen because we had found them to give the best overall performance in various experiments.</p><p>The dictionary used was the Grand Dictionnaire Terminologique (GDT, 2004) which is a very comprehensive terminological database for Canadian French with detailed data for a large number of different domains. The three candidate translations are then combined -if a GDT translation is found then the Reverso and WorldLingo translations are ignored. The reason for this is that if a phrase is in GDT, the translation for it is nearly always correct. In the case where words or phrases are not in GDT, then the Reverso and WorldLingo translations are simply combined.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table" coords="2,134.01,320.46,4.18,10.29">1</ref><p>: Some of the Question Types used in the DLT system. The second column shows a sample question from this year for each type. Translations are listed in the third column.</p><p>The types of phrase recognised were determined after a study of the constructions used in French queries together with their English counterparts. The aim was to group words together into sufficiently large sequences to be independently meaningful but to avoid the problems of structural translation, split particles etc which tend to occur in the syntax of a question, and which the engines tend to analyse incorrectly.</p><p>The structures used were number, quote, cap_nou_prep_det_seq, all_cap_wd, cap_adj_cap_nou, cap_adj_low_nou, cap_nou_cap_adj, cap_nou_low_adj, low_nou_low_adj, low_nou_prep_low_nou, low_adj_low_nou, nou_seq and wd. These were based on our observations that (1) Proper names usually only start with a capital letter with subsequent words uncapitalised, unlike English; (2) Adjective-Noun combinations either capitalised or not can have the status of compounds in French and hence need special treatment; (3) Certain noun-preposition-noun phrases are also of significance.</p><p>As part of the translation and analysis process, weights are assigned to each phrase in an attempt to establish which parts are more important in the event of query simplification being necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Retrieval Query Formulation</head><p>The starting point for this stage is a set of possible translations for each of the phrases recognised above. For each phrase, a boolean query is created comprising the various alternatives as disjunctions. In addition, alternation is added at this stage to take account of morphological inflections (e.g ' go' &lt;-&gt;' went' , ' company' &lt;-&gt;' companies' etc) and European English vs. American English spelling (' neighbour' &lt;-&gt;' neighbor' , ' labelled' &lt;-&gt;' labeled' etc). The list of the above components is then ordered by the weight assigned during the previous stage and the ordered components are then connected with AND operators to make the complete boolean query. This year we added a component which takes as input the query terms, performs Local Context Analysis (LCA) using the indexed document collection and returns a set of expansion terms. LCA can find terms which are related to a topic by association. For example if the input is 'Kurt Cobain' one output term could be 'Nirvana'. These terms are added to the search expresson in such a way that they boost the relevance of documents which contain them without their being required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Document Retrieval</head><p>A major change this year was the adoption of the Lucene (2005) search engine instead of DTSearch <ref type="bibr" coords="2,478.53,713.43,46.41,8.96;2,70.65,725.19,21.67,8.96">(DTSearch, 2000)</ref>. Lucene was used to index the LA Times and Glasgow Herald collections, with each sentence in the collection being considered as a separate document for indexing purposes. This followed our observation that in most cases the search keywords and the correct answer appear in the same sentence. We use the standard query language.</p><p>In the event that no documents are found, the conjunction in the query (corresponding to one phrase recognised in the query) with the lowest weight is eliminated and the search is repeated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Text File Parsing</head><p>This stage is straightforward and simply involves retrieving the matching 'documents' (i.e. sentences) from the corpus and extracting the text from the markup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Named Entity Recognition</head><p>Named Entity (NE) recognition is carried out in the standard way using a mixture of grammars and lists. The number of NE types was increased to 75 by studying previous CLEF and TREC question sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Answer Entity Selection</head><p>Answer selection was updated this year so that the weight of a candidate answer is the sum of the weights of all search terms co-occurring with it. Because our system works by sentence, search terms must appear in the same sentence as the candidate answer. The contribution of a term reduces with the inverse of its distance from the candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">Temporally Restricted Questions</head><p>This year an additional question type was introduced, temporally restricted factoids. We did not have time to make a study of this interesting idea so instead we simply processed them as normal factoids. Effectively this means that any temporal restrictions are analysed as normal syntactic phrases within the query, are translated and hence become weighted query terms. As with all phases, therefore, the weight assigned depends on the syntactic form of the restriction and not on any estimate of its temporal restricting significance. This approach was in fact quite successful (see results table and discussion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10">Definition Questions</head><p>This year 50 definition questions were included in the set of 200 queries with the remaining 150 being factoid (some temporally restricted, some not). At no stage have we made any study of these questions. For TREC we developed a very primitive component and so this was simply incorporated into the present system. Queries are first classified as def_organisation, def_person or def_unknown. The target is identified in the query (usually the name of an organisation or person). For an organisation query, a standard list of phrases is then added to the search expression, each suggesting that something of note is being said about the organisation. Example phrases are 'was founded' and 'manufacturer of' . All sentences including the target term plus at least one significant phrase are returned. These are concatenated to yield the answer to the question. This approach does work on occasion but the result is rarely concise. For def_person queries the method is the same, but using a different set of phrases such as 'brought up' , 'founded' etc. If the categoriser is unable to decide between def_organisation and def_person, it assigns def_unknown which results in both sets of patterns being used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Runs and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Two Experiments</head><p>We submitted two runs which differed only in their use of LCA. Run 1 used it while Run 2 did not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Results are summarised by query type in Table <ref type="table" coords="3,266.73,682.95,3.77,8.96">2</ref>. Concerning query classification it shows for each query type the number of queries assigned to that type which were correctly categorised along with the number incorrectly categorised. The overall rate of success was 84% which compares closely with the 85% achieved in the same task last year. This figure includes 33 queries which were 'correctly' classified as unknown. If these are not included then the figure becomes 67.5%. Effectively, answering these 33 queries (16.5% of the entire collection) lies outside the envisaged scope of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Type</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classif. Correct Classification Incorrect Classification Run 1</head><p>Run The performance in Run 1 can be summarised as follows. Taking all queries together (i.e. definitions and both types of factoid), 32 of the 168 queries classified properly were correctly answered. Of the 32 queries not classified properly, 4 were still answered correctly. Overall performance was thus 36 / 200, i.e. 18%. For Run 2, 31 of the 168 classified properly were answered correctly with an additional 5 of the 32 not classified properly still being right. This also gives a figure of 36 / 200, i.e. 18%. Our best figure for last year was in Run 1 where 19% was achieved. However, there were no definition questions in 2004 and this year we were able to devote little or no time to developing a component for these. If we consider just the factoid figures, performance in both runs is 26+4 / 150 i.e. 20%.</p><formula xml:id="formula_0" coords="4,72.81,96.12,449.94,262.34">2 Ru n 1 Run 2 C NC R X U W R X U W R X U W R X U W abbrev_expand 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 award 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 company 3 0 0 0 0 3 0 0 0 3 0 0 0 0 0 0 0 0 distance 2 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 film 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 how_many3 10 3 3 0 0 7 4 0 0 6 1 0 0 2 1 0 0 2 hw_mch_mony 3 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 hw_mch_rate 4 0 0 0 0 4 0 0 0 4 0 0 0 0 0 0 0 0 how_old 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 pol_party 3 0 0 0 0 3 0 0 0 3 0 0 0 0 0 0 0 0 population 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 profession 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 title 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 tv_network 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 what_capital 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 what_city 4 0 2 0 0 2 2 0 0 2 0 0 0 0 0 0 0 0 what_country 5 0 1 0 0 4 1 0 0 4 0 0 0 0 0 0 0 0 when 11 0 4 0 0 7 4 0 0 7 0 0 0 0 0 0 0 0 when_date 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 when_month 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 when_year 4 0 3 0 0 1 3 0 0 1 0 0 0 0 0 0 0 0 where 3 0 2 0 0 1 2 0 0 1 0 0 0 0 0 0 0 0 who</formula><p>In terms of our overall position in the French-English task (see Table <ref type="table" coords="4,351.09,626.07,4.98,8.96">6</ref> in the QA summary paper) we are only in positions 5 and 6 out of 12 with the best performance being DFKI German-English at 25.50%. However, it turns out that the main difference between ourselves and high scoring competitors is in the definition questions where they score well and we do poorly. If we consider the performance in factoid questions, broken down into two types, non-temporally restricted and temporally restricted, our performance in the former is 20.66% in Run 1 and 19.83% in Run 2 while in the latter it is 17.24% in Run 1 and 20.69% in Run 2. This makes Run 1 the best system in the group for non-temporally restricted questions alone, and Run 2 the best equal system with LIRE French-English Run 2 for temporally restricted questions alone.</p><p>As mentioned above, we devoted very little time to factoids and hence our very poor result of 6 / 50 correct i.e. 12%. The judgement of definitions was quite strict (we were responsible for it) with any response containing both relevant and non-relevant information being judged as ineXact not Right. This probably explains why the scores assigned to systems in the English target task were lower than in some other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Platform</head><p>We used a Dell PC running Windows NT and having 256 Mb RAM. The majority of the system is written in SICStus Prolog 3.11.1 <ref type="bibr" coords="5,163.41,140.31,64.52,8.96">(SICStus, 2004)</ref> with Part-of-Speech tagging, Web translation and Local Context Analysis components being written in Java.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>The overall performance was 18% which compares with 19% last year and 11.5% the year before. We were able to do very little work on the system this year and in addition there were 50 definition questions for which we only had a very primitive module inherited from our TREC system. If we exclude definitions, our performance compares more favourably with the other systems with Run 1 being the best system overall for normal factoids and Run 2 being equal best with LIRE for temporally restricted factoids.</p><p>Run 1 was our first experiment with Local Context Analysis for term expansion at the document retrieval stage.</p><p>Informal observations have shown that this method provides very good expansion terms which are semantically related by topic and context. However, these experiments did not show any significant advantage for LCA compared to Run 2 which did not use it. Overall performance of the two runs was identical. Performance on nontemporal factoids was marginally better with the LCA (20.66% vs. 19.83%) but it was worse on temporal factoids (17.24% vs. 20.69%). Further analysis is necessariy to see why this was the case.</p><p>Definitions are an interesting category of question and we intend to devote much more time to them next year. We are hoping that the specification of a definition and the precise means by which it can be evaluated will be worked out in the mean time. A major defect of our approach is that it is imprecise. Under our strict scoring, accuracy was only 12%. However, we could easily have considered our inexact answers as correct. This would increase our score from 6 / 50 to 17 / 50, i.e. an improvement from 12% to 34%. To put this another way, if we were to select from the answer sentences more carefully, we could improve our algorithm considerably.</p><p>In CLEF generally, performance in the cross-lingual tasks is much lower than in the monolingual ones. One interesting experiment would be to eliminate the translation component from our system, thus making it monolingual, and then to try it on the English version of the same test collection. The level of performance would of course be higher and by measuring the difference we would be able to estimate how much information we are at present losing in the translation stage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,146.49,350.36,9.06,8.10;4,172.65,350.36,4.50,8.10;4,194.25,350.36,4.50,8.10;4,215.85,350.36,4.50,8.10;4,237.45,350.36,26.10,8.10;4,280.65,350.36,4.50,8.10;4,302.25,350.36,4.50,8.10;4,323.85,350.36,26.10,8.10;4,367.05,350.36,4.50,8.10;4,388.65,350.36,4.50,8.10;4,410.25,350.36,4.50,8.10;4,431.85,350.36,4.50,8.10;4,453.45,350.36,4.50,8.10;4,475.05,350.36,4.50,8.10;4,496.65,350.36,4.50,8.10;4,518.25,350.36,4.50,8.10;4,72.81,360.92,33.54,8.10;4,146.49,360.92,30.66,8.10;4,194.25,360.92,4.50,8.10;4,215.85,360.92,4.50,8.10;4,237.45,360.92,26.10,8.10;4,280.65,360.92,4.50,8.10;4,302.25,360.92,4.50,8.10;4,323.85,360.92,26.10,8.10;4,367.05,360.92,4.50,8.10;4,388.65,360.92,4.50,8.10;4,410.25,360.92,26.10,8.10;4,453.45,360.92,4.50,8.10;4,475.05,360.92,4.50,8.10;4,496.65,360.92,26.10,8.10;4,72.81,372.12,35.78,9.30;4,141.93,372.12,56.82,9.30;4,215.85,372.12,4.50,9.30;4,237.45,372.12,47.70,9.30;4,302.25,372.12,4.50,9.30;4,323.85,372.12,26.10,9.30;4,367.05,372.12,4.50,9.30;4,388.65,372.12,4.50,9.30;4,410.25,372.12,26.10,9.30;4,453.45,372.12,4.50,9.30;4,475.05,372.12,4.50,9.30;4,496.65,372.12,26.10,9.30;4,72.81,383.00,28.02,8.10;4,146.49,383.00,9.06,8.10;4,172.65,383.00,4.50,8.10;4,194.25,383.00,4.50,8.10;4,215.85,383.00,4.50,8.10;4,237.45,383.00,26.10,8.10;4,280.65,383.00,4.50,8.10;4,302.25,383.00,4.50,8.10;4,323.85,383.00,26.10,8.10;4,367.05,383.00,4.50,8.10;4,388.65,383.00,4.50,8.10;4,410.25,383.00,4.50,8.10;4,431.85,383.00,4.50,8.10;4,453.45,383.00,4.50,8.10;4,475.05,383.00,4.50,8.10;4,496.65,383.00,4.50,8.10;4,518.25,383.00,4.50,8.10;4,72.81,393.56,40.02,8.10;4,146.49,393.56,9.06,8.10;4,172.65,393.56,4.50,8.10;4,194.25,393.56,4.50,8.10;4,215.85,393.56,4.50,8.10;4,237.45,393.56,26.10,8.10;4,280.65,393.56,4.50,8.10;4,302.25,393.56,4.50,8.10;4,323.85,393.56,26.10,8.10;4,367.05,393.56,4.50,8.10;4,388.65,393.56,4.50,8.10;4,410.25,393.56,4.50,8.10;4,431.85,393.56,4.50,8.10;4,453.45,393.56,4.50,8.10;4,475.05,393.56,4.50,8.10;4,496.65,393.56,4.50,8.10;4,518.25,393.56,4.50,8.10;4,72.81,404.12,49.50,8.10;4,151.05,404.12,4.50,8.10;4,172.65,404.12,4.50,8.10;4,194.25,404.12,4.50,8.10;4,215.85,404.12,4.50,8.10;4,237.45,404.12,4.50,8.10;4,259.05,404.12,4.50,8.10;4,280.65,404.12,4.50,8.10;4,302.25,404.12,4.50,8.10;4,323.85,404.12,4.50,8.10;4,345.45,404.12,4.50,8.10;4,367.05,404.12,4.50,8.10;4,388.65,404.12,4.50,8.10;4,410.25,404.12,4.50,8.10;4,431.85,404.12,4.50,8.10;4,453.45,404.12,4.50,8.10;4,475.05,404.12,4.50,8.10;4,496.65,404.12,4.50,8.10;4,518.25,404.12,4.50,8.10;4,72.81,415.32,35.78,9.30;4,146.49,415.32,9.06,9.30;4,172.65,415.32,4.50,9.30;4,194.25,415.32,26.10,9.30;4,237.45,415.32,26.10,9.30;4,280.65,415.32,4.50,9.30;4,302.25,415.32,4.50,9.30;4,323.85,415.32,26.10,9.30;4,367.05,415.32,4.50,9.30;4,388.65,415.32,4.50,9.30;4,410.25,415.32,4.50,9.30;4,431.85,415.32,4.50,9.30;4,453.45,415.32,4.50,9.30;4,475.05,415.32,4.50,9.30;4,496.65,415.32,4.50,9.30;4,518.25,415.32,4.50,9.30;4,72.81,426.36,23.90,9.30;4,141.93,426.36,78.42,9.30;4,237.45,426.36,69.30,9.30;4,323.85,426.36,26.10,9.30;4,367.05,426.36,4.50,9.30;4,388.65,426.36,4.50,9.30;4,410.25,426.36,26.10,9.30;4,453.45,426.36,4.50,9.30;4,475.05,426.36,4.50,9.30;4,496.65,426.36,26.10,9.30;4,106.65,449.58,382.30,10.29;4,106.65,461.43,382.26,8.96;4,106.65,473.19,382.29,8.96;4,106.65,484.95,382.30,8.96;4,106.65,496.71,221.49,8.96"><head></head><label></label><figDesc>by Query Type for 2005 Cross-Language French-English Task. The columns C and NC show the numbers of queries of a particular type which were classified correctly and not correctly. Those classified correctly are then broken down into Right, ineXact, Unsupported and Wrong for each of the two runs Run 1 and Run 2. Finally, those classified incorrectly are broken down in the same way.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
