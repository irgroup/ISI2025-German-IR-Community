<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,97.92,148.71,407.51,15.51">Question Answering using Semantic Annotation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,227.40,182.21,52.40,9.96"><forename type="first">Lili</forename><surname>Aunimo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Helsinki</orgName>
								<orgName type="institution" key="instit2">UNIVERSITY OF HELSINKI</orgName>
								<address>
									<postBox>P.O. Box 68</postBox>
									<postCode>FIN-00014</postCode>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.54,182.21,73.33,9.96"><forename type="first">Reeta</forename><surname>Kuuskoski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Helsinki</orgName>
								<orgName type="institution" key="instit2">UNIVERSITY OF HELSINKI</orgName>
								<address>
									<postBox>P.O. Box 68</postBox>
									<postCode>FIN-00014</postCode>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,97.92,148.71,407.51,15.51">Question Answering using Semantic Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">79C2F21798F684BD3210769D4DC2591B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>]H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>]H.5 [Information Interfaces and Presentation]: H.5.2 User Interfaces: Natural Language Question Answering, Evaluation, Experimentation, Multilingual Information Access</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a question answering (QA) system called Tikka. Tikka's approach to QA relies heavily on the semantic annotation of text documents and on the usage of answer extraction patterns. In this way, Tikka applies to QA pattern-based techniques traditionally used in named entity recognition and information extraction. In the experiments presented in this paper, Tikka's performance is evaluated in the following tasks: monolingual Finnish and French and bilingual Finnish-English QA. Its performance in the monolingual tasks is near the average when it is compared with the QA systems' performance that participated in the monolingual French task. In the monolingual Finnish task, Tikka was the only participating system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering (QA) is a task that aims beyond document retrieval and towards natural language understanding. The task and its evaluation in the CLEF 2005 Multilingual Question Answering Track is described in the overview paper <ref type="bibr" coords="1,316.22,581.69,9.91,9.96" target="#b5">[6]</ref>. Our approach to QA relies heavily on the semantic annotation of text documents and on the usage of answer extraction pattern prototypes. In this way, we apply pattern-based techniques traditionally used in named entity recognition and information extraction to QA.</p><p>Figure <ref type="figure" coords="1,136.81,629.45,4.98,9.96" target="#fig_0">1</ref> shows the system architecture of our QA system Tikka. Out of the five modules of the question analysis component, only the question classifier and the topic and target extractor are used to process both Finnish and French. The syntactic parser is used only for Finnish, the semantic annotator only for French, and the translator only for performing translation from Finnish to English. The answer extraction component can handle English, Finnish and French. The input to the system is a question in Finnish or French. The question analysis component forms the query terms for document retrieval, determines the class of the question and its topic and target words, and passes these on to the answer extraction component. The answer extraction component returns an answer to the question. Both of these components are described in detail in the following sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Question Analysis</head><p>The question analysis component of the QA system consists of five software modules: 1) the syntactic parser for Finnish, 2) the semantic annotator, which is detailed in Section 4, 3) the question classifier, 4) the topic and target extractor and 5) the translator, which is described in the system description of the previous version of Tikka, that participated in QA@CLEF 2004 <ref type="bibr" coords="2,499.72,529.13,10.00,9.96" target="#b0">[1]</ref>. All these modules, along with the databases that they use, are illustrated in Figure <ref type="figure" coords="2,456.98,541.13,3.90,9.96" target="#fig_0">1</ref>.</p><p>Table <ref type="table" coords="2,132.72,553.01,4.98,9.96" target="#tab_0">1</ref> shows through an example how question analysis is performed. First, a natural language question is given as input to the system, for example: D FI EN Mik√§ on WWF?<ref type="foot" coords="2,461.16,564.18,3.97,4.84" target="#foot_0">1</ref> . Next, the Finnish question is parsed syntactically and the French question is annotated semantically. Then both questions are classified according to the expected answer type, and the topic and target words are extracted from them. The expected answer types are determined by the multinine corpus, and they are: LOCATION, MEASURE, ORGANIZATION, OTHER, PERSON and TIME. The target words are extracted or inferred from the question and they further restrict the answer type, e.g. age, kilometers and capital city <ref type="bibr" coords="2,244.21,636.77,12.70,9.96" target="#b1">[2]</ref>.The topic words are words extracted from the question that in a sentence containing the answer to the question carry old information. For example, in the question What is WWF?, WWF is the topic because in the answer sentence WWF is the World Wide Fund for Nature., WWF is the old information and the World Wide Fund for Nature is the new information. The old and new information of a sentence are contextually established <ref type="bibr" coords="2,485.65,684.53,9.91,9.96" target="#b8">[9]</ref>. In our case, the question is the context. In Tikka, topic words are useful query terms along with the target words, and they are also used to fill slots in the answer pattern prototypes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Answer Extraction</head><p>The answer extraction component consists of five software modules: 1) the document retriever, 2) the paragraph selector, 3) the semantic annotator, 4) the pattern instantiator and matcher and 5) the answer selector. All these modules, along with the databases that they use, are illustrated in Figure <ref type="figure" coords="3,132.85,416.81,3.90,9.96" target="#fig_0">1</ref>. The dotted arrows that go from the document retriever back to itself as well as from the answer selector back to the document retriever illustrate that if no documents or answers are found, answer extraction starts all over.  The examples are the same as in Table <ref type="table" coords="3,262.06,687.53,3.90,9.96" target="#tab_0">1</ref>, and the processing in this table is a continuation of the question analysis illustrated in that table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">An Example</head><p>Table <ref type="table" coords="4,132.96,111.41,4.98,9.96" target="#tab_2">2</ref> shows through an example how answer extraction is performed. First, the question analysis passes as input to the component the query words, and the topic and target of the question. Next, document retrieval is performed using the query terms. The parameter settings of the document retrieval engine are determined by the number of times document retrieval has been performed for the question at hand. If the document retrieval succeeds, the paragraphs containing at least one query word are filtered out for further processing and they are annotated semantically. After that, a class-specific set of pattern prototypes is instantiated with the topic word and with a possibly existing target word. Each pattern prototype has a score, which reflects its accuracy. The score ranges between 1 and 9. Instantiated patterns are then matched against semantically annotated paragraphs, and answer candidates are extracted. For the example illustrated in Table <ref type="table" coords="4,90.00,230.93,3.90,9.96" target="#tab_2">2</ref>, the pattern prototype and the corresponding instantiated pattern that matches the Finnish answer is:</p><formula xml:id="formula_0" coords="4,90.00,264.46,338.67,17.89">((&lt;[a-z]+&gt;[^&lt;&gt;]+&lt;\/[a-z]+&gt; )+)\( (&lt;[a-z]+&gt;)?TOPIC(&lt;\/[a-z]+&gt;)? \)Score:9 ((&lt;[a-z]+&gt;[^&lt;&gt;]+&lt;\/[a-z]+&gt; )+)\( (&lt;[a-z]+&gt;)?Wwf(&lt;\/[a-z]+&gt;)? \)Score:9</formula><p>The text snippet that matched the above pattern is in Table <ref type="table" coords="4,373.59,295.37,3.90,9.96" target="#tab_5">4</ref>. (The patterns are case insensitive.) Only at least partly semantically annotated candidates can be extracted. The score of a unique answer candidate is the sum of the scores of the patterns that extracted the similar answer instances, or more formally:</p><formula xml:id="formula_1" coords="4,214.56,355.25,298.48,20.70">score(answer) = i in A patternScore(i),<label>(1)</label></formula><p>where A is the set of similar answers and patternScore(i) is the score of the pattern that has matched i in text. The confidence value of a non-NIL answer candidate is determined by the candidate's score and by the total number of candidates. This is illustrated in Figure <ref type="figure" coords="4,483.02,406.01,3.90,9.96" target="#fig_1">2</ref>. For example, if the total number of candidates is between 1 and 5, and the score of the candidate is 17 or greater, confidence is 1, but if the score of the candidate is between 1 and 16, confidence is 0.75. If the confidence score 1 is reached, the answer is selected and no further answers are searched. Otherwise, all paragraphs are searched for answers, and the one with the highest score is selected. Since the confidence of the answer for Finnish in Table <ref type="table" coords="4,384.12,465.89,4.98,9.96" target="#tab_2">2</ref> is 0.25, and the score of the answer is 18, we can deduce that the number of answer candidates is at least 11.</p><p>If document retrieval does not return any documents, or no answer is extracted from the paragraphs, Tikka has several alternative ways in which to proceed, depending on which task it is performing and how many times document retrieval has been tried. This is illustrated in Table <ref type="table" coords="4,117.72,525.65,3.90,9.96">5</ref>. As can be seen from the figure, if no documents are retrieved in the first iteration, the parameter settings of the retrieval engine are altered and document retrieval is performed again in the monolingual Finnish and bilingual Finnish-English tasks. However, in the monolingual French task the system halts and returns NIL with a confidence of 1 as an answer. In the monolingual Finnish task, the system halts after the second try, but in the bilingual English-Finnish task, document retrieval is performed for a third time if either no documents are retrieved or no answer is found. Alternatively, in the monolingual Finnish and English tasks, if documents are retrieved, but no answers are found after the first try, document retrieval is tried once more. In all tasks, the system returns a confidence value of 1 for the NIL answer if no documents are found and a confidence value of 0 for the NIL answer if documents are found but no answer can be extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Document Retrieval</head><p>The document retrieval module of Tikka consists of the vector space model <ref type="bibr" coords="4,410.98,679.37,15.49,9.96" target="#b9">[10]</ref> based search engine Lucene 2 and of the document indices for English, Finnish and French newspaper text built using it. Tikka has one index for the English document collection, two indices for the Finnish document collection and two for the French document collection. In each of the indices, one newspaper article forms one document. The English index (enstem) is a stemmed one. It is stemmed using the implementation of Porter's stemming algorithm <ref type="bibr" coords="5,313.10,317.45,10.57,9.96" target="#b6">[7]</ref> included in Lucene. One index (filemma) to the Finnish collection is created using the lemmatized word forms as index terms. The Connexor's parser is used for the lemmatization. The other Finnish index (fistem) consists of stemmed word forms. The stemming is done by Snowball <ref type="bibr" coords="5,287.30,353.33,10.45,9.96" target="#b7">[8]</ref> project's<ref type="foot" coords="5,344.04,352.50,3.97,4.84" target="#foot_1">3</ref> stemming algorithm for Finnish. A Snowlball stemmer is also used to create one of the indices for French (frstem). The other French index (frbase) is built using the words of the documents as such. This index is case-insensitive.</p><p>In the document retrieval phase, Lucene determines the similarity between the query (q) and the document (d) using the formula presented in Equation 2 <ref type="bibr" coords="5,356.81,401.09,9.91,9.96" target="#b3">[4]</ref>.</p><formula xml:id="formula_2" coords="5,99.36,435.05,409.42,20.58">similarity(q, d) = t in q tf (t in d) ‚Ä¢ idf (t) ‚Ä¢ boost(t.f ield in d) ‚Ä¢ lengthN orm(t.f ield in d), (<label>2</label></formula><formula xml:id="formula_3" coords="5,508.78,435.05,4.25,9.96">)</formula><p>where tf is the term frequency factor for the term t in the document d, and idf (t) is the inverse document frequency of the term. The factor boost adds more weight to the terms appearing in a given field, and it can be set at indexing time. The last factor is a coefficient that normalizes the score according to the length of the field. After all the scores regarding a single query have been calculated, they are normalized from the highest score if that score is greater than 1. Since we do not use the field specific term weighting, the two last terms of the formula can be discarded, and the formula is equal to calculating the dot product between a query with binary term weights and a document with tf idf <ref type="bibr" coords="5,192.72,551.21,10.45,9.96" target="#b4">[5]</ref> term weights.</p><p>Lucene does not use the pure boolean information retrieval (IR) model, but we model the conjunctive boolean query by requiring all of the query terms to appear in each of the documents in the result set. This differs from the pure boolean IR model in that the relevance score for each document is calculated according to Equation <ref type="formula" coords="5,294.51,599.09,3.90,9.96" target="#formula_2">2</ref>, and the documents are ordered according to it. The ordering is important in Tikka. This is what the term boolean means in Figure <ref type="figure" coords="5,471.73,610.97,3.90,9.96">5</ref>. In the same figure, the term ranked means a normal Lucene query where all of the query words are not required to appear in the retrieved documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semantic Annotation</head><p>Semantic annotation is in many ways a similar task to named entity recognition (NER). NER is commonly done based on preset names lists and patterns <ref type="bibr" coords="5,355.80,701.69,15.49,9.96" target="#b10">[11]</ref> or using machine learning techniques <ref type="bibr" coords="5,120.00,713.57,10.00,9.96" target="#b2">[3]</ref>. Our method relies on the first method. The main difference between NER and semantic annotation is that the first one aims at recognizing proper names whereas the second aims at recognizing both proper names and common nouns.</p><p>In Tikka, French questions and selected paragraphs from the search results are annotated semantically. We have 14 semantic classes that are listed in table 3. To the classes consisting mainly of proper nouns, some common nouns are added in order to be able to analyze the questions correctly. For instance, in the gazetteers of the class organization, there are proper names denoting companies (IBM, Toyota), but also some common nouns referring to organizations in each language (school, bank, union). As can be seen from Table <ref type="table" coords="6,322.67,195.17,3.90,9.96" target="#tab_3">3</ref>, the organization gazetteer in English is significantly shorter than those in other two languages. This is due to the NER from Connexor that is used in addition our own semantic annotator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head><p>English The semantic annotator uses a window of two words for identifying the items to be annotated. In that way we can only find the entities consisting of one or two words. The external NER that is used in the English annotation is able to identify person names, organizations and locations. Hence, there are no limitations on the length of entities on these three classes in English. For Finnish, we exploit Connexor's syntactic parser for part of speech recognition to eliminate the words that are not nouns, adjectives or numerals. For French, the semantic annotator builds solely on the text as it is without any linguistic analysis.</p><p>In the text to be annotated, persons are identified based on a list of first names and the subsequent capital word. The subsequent capital words are added to the list of known names in the document. In this way the family names appearing alone later in the document can also be identified to be names of a person. The class location consists of names of large cities that are not capitals and of the names of states and other larger geographical items. To the class measure belong numerals and numeric expressions, for instance dozen. Unit consists of terms such as percent, kilometer. The event class is quite heterogeneous, since to it belong terms like Olympics, Christmas, war and hurricane. Time gazetteer lists time related terms, the names of the months, week days etc. Example annotations for each of the languages can be seen in Table <ref type="table" coords="6,456.84,581.69,3.90,9.96" target="#tab_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis of Results</head><p>Tikka was evaluated by participating in the monolingual Finnish and French tasks and in the bilingual Finnish-English task. The evaluation results are described in detail in Section 4 (Results) of the QA track overview paper <ref type="bibr" coords="6,232.34,660.29,10.00,9.96" target="#b5">[6]</ref>. In each of the tasks, two different parameter settings (run 1 and run 2) for the document retrieval component were tested. These settings are listed in Table <ref type="table" coords="6,505.19,672.29,3.90,9.96">5</ref>. The results of the runs are shown in Figure <ref type="figure" coords="6,285.86,684.29,3.90,9.96" target="#fig_2">3</ref>. We can observe that the difference between runs is not very big for the French monolingual run. The accuracy of the artificial combination run<ref type="foot" coords="6,508.56,695.34,3.97,4.84" target="#foot_2">4</ref> (C) is not much higher than that of the the French monolingual run 1, which means that almost   <ref type="bibr" coords="7,250.95,281.81,10.45,9.96" target="#b5">[6]</ref> and it is used as an example in the Tables <ref type="table" coords="7,452.05,281.81,4.98,9.96" target="#tab_0">1</ref> and<ref type="table" coords="7,479.65,281.81,3.90,9.96" target="#tab_2">2</ref>.</p><p>all answers given by the runs are equal. On the contrary, there is a difference of 4 points between the accuracies of the two monolingual Finnish runs, and in addition, as the difference between run 1 and the combination run for Finnish is 3.5 points, we can conclude that some of the correct answers returned by run 2 are not included in the set of correct answers given by run 1. This means that the different parameter settings of the runs produced an effect on Tikka's overall performance. Between the runs, both the parameters for type of index and the maximum number of documents were altered. In the bilingual Finnish-English task, some difference between the runs can be observed, but the difference is not as big as in he monolingual Finnish task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>Tikka is a QA system that uses pattern-based techniques to extract answers from text. In the experiments presented in this paper, its performance is evaluated in the following tasks: monolingual Finnish and French and bilingual Finnish-English QA. Its performance in the monolingual tasks is near the average when it is compared with the QA systems' that participated in the monolingual French task. In the monolingual Finnish task, Tikka was the only participating system. Table <ref type="table" coords="8,117.60,118.37,3.90,9.96">5</ref>: The parameters for document retrieval used in different runs and in different iterations of the same run. MinS stands for the minimum similarity value between query and document and MaxD stands for the maximum number of documents to be retrieved. The maximum number of iterations is in monolingual Finnish runs 2, in monolingual French runs 1, and in Bilingual English runs 3. In the future, as the document databases are not very big (about 1.6 GB), the documents could be annotated semantically before indexing. This would speed up the interactive processing time and the semantic classes could be used as fields in index creation. In addition, indexing based on paragraph level instead of document level might raise the ranking of the essential results and it would speed up the processing time of the interactive phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Finnish</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,90.00,403.97,423.10,9.96;2,90.00,415.97,423.04,9.96;2,90.00,427.85,422.99,9.96;2,90.00,439.85,283.45,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The system architecture of Tikka. Tikka has two main components: question analysis and answer extraction. Both components use the same semantic annotator, which is illustrated by gray in the figure. The left hand side of each component lists the databases used by it. The rectangles on the right hand side illustrate the software modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,90.00,273.65,422.99,9.96;5,90.00,285.65,161.92,9.96"><head>2Figure 2 :</head><label>2</label><figDesc>Figure 2: The confidence value of a non-NIL answer is a function of the answer's score and the number of unique answer candidates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,90.00,585.17,422.76,9.96;7,90.00,597.17,423.00,9.96;7,90.00,609.17,423.00,9.96;7,90.00,621.05,291.98,9.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A histogram showing the percentage of correct answers (i.e. the accuracy) in Tikka's submitted test runs and in the artificial combination run. The very light gray represents the monolingual runs for Finnish, the a little bit darker gray represents the monolingual French runs and the darkest gray represents the bilingual Finnish-English runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,110.24,423.16,194.02"><head>Table 1 :</head><label>1</label><figDesc>The availability and output of the five modules of question analysis illustrated with the same example sentence for the target languages English, Finnish and French. Answer extraction with these same questions is illustrated in Table2.</figDesc><table coords="3,96.00,110.24,414.08,143.37"><row><cell>Module</cell><cell></cell><cell>Example</cell></row><row><cell></cell><cell>English</cell><cell>Finnish</cell><cell>French</cell></row><row><cell></cell><cell cols="2">D FI EN Mik√§ D FI FI Mik√§ on WWF?</cell><cell>D FR FR Qu'est-ce que</cell></row><row><cell></cell><cell>on WWF ?</cell><cell></cell><cell>la WWF?</cell></row><row><cell>(1) Parser</cell><cell cols="2">1 Mik√§ mik√§ subj:&gt;2 &amp;NH PRON SG NOM</cell><cell>N/A</cell></row><row><cell></cell><cell cols="2">2 on olla main:&gt;0 &amp;+MV V ACT IND PRES SG3</cell></row><row><cell></cell><cell cols="2">3 WWF wwf &amp;NH N</cell></row><row><cell>(2) Semantic</cell><cell></cell><cell>N/A</cell><cell>Qu'est-ce que &lt;organization&gt;</cell></row><row><cell>Annotator</cell><cell></cell><cell></cell><cell>la WWF&lt;/organization&gt;?</cell></row><row><cell>(3) Classifier</cell><cell></cell><cell>Organization</cell></row><row><cell>(4) T &amp; T</cell><cell></cell><cell>Topic: WWF</cell></row><row><cell>Extractor</cell><cell></cell><cell>Target: N/A</cell></row><row><cell>(5) FI ‚Üí EN</cell><cell>WWF</cell><cell>N/A</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,90.00,675.53,423.14,9.96"><head>Table 2 :</head><label>2</label><figDesc>The output of the five different modules of answer extraction illustrated with examples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,96.00,249.17,404.64,115.92"><head>Table 3 :</head><label>3</label><figDesc>The semantic classes and the number of items in the gazetteers for each language.</figDesc><table coords="6,96.00,249.17,398.17,94.08"><row><cell></cell><cell></cell><cell cols="3">French Finnish Class</cell><cell cols="3">English French Finnish</cell></row><row><cell>person</cell><cell>3704</cell><cell>3704</cell><cell>3704</cell><cell>unit</cell><cell>31</cell><cell>35</cell><cell>44</cell></row><row><cell>country</cell><cell>265</cell><cell>215</cell><cell>252</cell><cell>measure</cell><cell>51</cell><cell>50</cell><cell>34</cell></row><row><cell>language</cell><cell>109</cell><cell>79</cell><cell>637</cell><cell>award</cell><cell>15</cell><cell>15</cell><cell>7</cell></row><row><cell>nationality</cell><cell>57</cell><cell>177</cell><cell>85</cell><cell>color</cell><cell>22</cell><cell>20</cell><cell>29</cell></row><row><cell>capital</cell><cell>277</cell><cell>211</cell><cell>277</cell><cell cols="2">profession 95</cell><cell>246</cell><cell>127</cell></row><row><cell>location</cell><cell>5339</cell><cell>5440</cell><cell>5314</cell><cell>time</cell><cell>56</cell><cell>38</cell><cell>38</cell></row><row><cell cols="2">organization 37</cell><cell>968</cell><cell>212</cell><cell>event</cell><cell>29</cell><cell>21</cell><cell>15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,90.00,257.93,423.13,33.84"><head>Table 4 :</head><label>4</label><figDesc>Examples of semantically annotated text snippets in English, Finnish and French, retrieved from newspaper text and answering the question What is WWF?. It is question number 136 and 278 in the multinine corpus</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,737.14,407.49,7.97;2,90.00,746.50,218.83,7.97"><p>D stands for a definition question and FI EN means that the source language is Finnish and the target language is English. In English, the question means What is WWF?</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="5,105.24,733.18,108.59,7.97"><p>http://snowball.tartarus.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="6,105.24,727.66,407.76,7.97;6,90.00,737.14,422.86,7.97;6,90.00,746.50,189.25,7.97"><p>In this case, the artificial combination run represents a run where the system is somehow able to choose for each question the better answer from the two answers provided by the runs 1 and 2. For more information on the combination runs, see the track overview paper<ref type="bibr" coords="6,267.90,746.50,8.51,7.97" target="#b5">[6]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="8,105.24,684.94,97.50,7.97"><p>http://www.connexor.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="8,105.24,694.42,98.98,7.97"><p>http://www.kielikone.fi/en</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>The authors thank <rs type="institution">Connexor Ltd 5</rs> for providing the Finnish and English parsers and NE reconizers, <rs type="institution">Kielikone Ltd</rs> 6 for providing the bilingual dictionary and <rs type="person">Juha Makkonen</rs> for providing the question classifier for Finnish.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,110.52,133.25,402.63,9.96;9,110.52,145.25,402.63,9.96;9,110.52,157.13,402.53,9.96;9,110.52,169.47,402.30,9.18;9,110.52,181.13,402.56,9.96;9,110.52,193.01,56.65,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,339.90,133.25,173.26,9.96;9,110.52,145.25,86.86,9.96">Finnish as Source Language in Bilingual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Lili</forename><surname>Aunimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reeta</forename><surname>Kuuskoski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juha</forename><surname>Makkonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,222.48,157.47,290.57,9.18;9,110.52,169.47,274.06,9.18">Multilingual Information Access for Text, Speech and Images: 5th Workshop of the Cross-Language Evaluation Forum, CLEF 2004</title>
		<title level="s" coord="9,315.96,181.47,152.62,9.18">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2004">September 15-17, 2004. 2005</date>
			<biblScope unit="volume">3491</biblScope>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="9,110.52,212.93,402.55,9.96;9,110.52,224.93,402.57,9.96;9,110.52,236.81,179.81,9.96;9,290.28,235.71,7.67,6.33;9,303.00,237.15,210.06,9.18;9,110.52,248.81,402.73,9.96;9,110.52,260.81,189.76,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,354.54,212.93,158.54,9.96;9,110.52,224.93,46.26,9.96">Cross-language Question Answering for Finnish</title>
		<author>
			<persName coords=""><forename type="first">Lili</forename><surname>Aunimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juha</forename><surname>Makkonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reeta</forename><surname>Kuuskoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,196.20,237.15,94.13,9.18;9,290.28,235.71,7.67,6.33;9,303.00,237.15,210.06,9.18;9,110.52,249.15,18.57,9.18">Proceedings of the 11 th Finnish Artificial Intelligence Conference STeP 2004</title>
		<title level="s" coord="9,336.24,249.15,77.53,9.18">Conference Series</title>
		<editor>
			<persName><forename type="first">Eero</forename><surname>Hyv√∂nen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tomi</forename><surname>Kauppinen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mirva</forename><surname>Salminen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kim</forename><surname>Viljanen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pekka</forename><surname>Ala-Siuru</surname></persName>
		</editor>
		<meeting>the 11 th Finnish Artificial Intelligence Conference STeP 2004<address><addrLine>Vantaa, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>Finnish Artificial Intelligence Society</publisher>
			<date type="published" when="2004-03">September 1-3. 2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="35" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,280.73,402.59,9.96;9,110.52,292.61,270.74,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,400.44,280.73,112.67,9.96;9,110.52,292.61,72.33,9.96">An algorithm that learns what&apos;s in a name</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,192.48,292.95,76.61,9.18">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,312.53,387.16,9.96" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,272.76,312.87,73.05,9.18">Lucene in Action</title>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Hatcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Otis</forename><surname>Gospodnetiƒá</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Manning Publications Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,332.45,402.52,9.96;9,110.52,344.45,242.54,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,209.18,332.45,303.86,9.96;9,110.52,344.45,34.79,9.96">A statistical interpretation os term specificity and its application in retrieval</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Sparck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jones</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,153.36,344.79,112.23,9.18">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,364.37,402.63,9.96;9,110.52,376.37,402.56,9.96;9,110.52,388.25,402.25,9.96;9,110.52,400.25,119.30,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,237.63,376.37,275.45,9.96;9,110.52,388.25,23.71,9.96">Overview of the CLEF 2005 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Aunimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Erbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Penas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,338.28,388.59,169.41,9.18">Proceedins of the CLEF 2005 Workshop</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Francesca</forename><surname>Borri</surname></persName>
		</editor>
		<meeting>eedins of the CLEF 2005 Workshop<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">sept 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,420.17,343.93,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,173.06,420.17,140.84,9.96">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,322.44,420.51,34.34,9.18">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,440.09,402.52,9.96;9,110.52,452.09,280.46,9.96" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,183.38,440.09,224.37,9.96">Snowball: A language for stemming algorithms</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://snowball.tartarus.org/texts/introduction.html[22.8" />
		<imprint>
			<date type="published" when="2001">2001. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,472.01,402.52,9.96;9,110.52,483.89,152.66,9.96" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,357.84,472.35,155.20,9.18;9,110.52,484.23,73.80,9.18">A Comprehensive Grammar of the English Language</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Leech</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Svartvik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Longman</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,503.81,402.44,9.96;9,110.52,515.81,123.38,9.96" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,182.28,504.15,330.68,9.18;9,110.52,516.15,28.67,9.18">The SMART Retrieval System: Experiments in Automatic Document Processing</title>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,535.73,402.43,9.96;9,110.52,547.73,402.51,9.96;9,110.52,559.61,247.73,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,273.16,535.73,239.80,9.96;9,110.52,547.73,117.40,9.96">Learn -Filter -Apply -Forget. Mixed Approaches to Named Entity Recognition</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Volk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simon</forename><surname>Clematide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,254.52,548.07,258.51,9.18;9,110.52,559.95,148.45,9.18">Proceedings of the 6th International Workshop of Natural Language for Information Systems</title>
		<meeting>the 6th International Workshop of Natural Language for Information Systems<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
