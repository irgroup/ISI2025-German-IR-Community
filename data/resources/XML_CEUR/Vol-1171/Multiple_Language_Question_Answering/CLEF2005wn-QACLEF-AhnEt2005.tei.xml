<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,474.27,148.86,38.73,15.15">2005</title>
				<funder ref="#_YTy3k3b #_E5uS8CB">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_nMDk5hW #_X4Eubtm #_CCmvqTA #_VaMUzBy #_UBXFgEX">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_4uatCxd #_erq7y54">
					<orgName type="full">NWO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,203.51,182.75,47.74,8.74"><forename type="first">David</forename><surname>Ahn</surname></persName>
							<email>ahn@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,261.20,182.75,71.54,8.74"><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
							<email>jijkoun@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,342.71,182.75,56.79,8.74"><forename type="first">Karin</forename><surname>MÃ¼ller</surname></persName>
							<email>kmueller@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,211.39,196.70,75.99,8.74"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
						</author>
						<author>
							<persName coords="1,297.35,196.70,47.76,8.74"><forename type="first">Erik</forename><surname>Tjong</surname></persName>
							<email>erikt@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,348.43,196.70,43.17,8.74"><forename type="first">Kim</forename><surname>Sang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">CLEF</orgName>
								<orgName type="institution">The University of Amsterdam at QA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Kruislaan 403</addrLine>
									<postCode>1098 SJ</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,474.27,148.86,38.73,15.15">2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7ED82B12C68BFBBD4C45362E3758285A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Question answering, Questions beyond factoids</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the official runs of our team for the CLEF 2005 question answering track. We took part in the monolingual Dutch task, and focused most of our efforts on refining our existing multi-stream architecture, and porting parts of it to an XMLbased platform.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In previous participations in question answering tracks at both CLEF and TREC we developed a multi-stream question answering architecture which implements multiple ways of identifying candidate answers, complemented with elaborate filtering mechanisms to weed out incorrect candidate answers <ref type="bibr" coords="1,126.73,578.27,10.51,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,140.39,578.27,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="1,151.29,578.27,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="1,162.19,578.27,7.01,8.74" target="#b8">9]</ref>. Part of our efforts for the 2005 edition of the QA@CLEF track were aimed at improving this architecture, in particular the so-called table stream (see Subsection 2.2). Also, to accommodate the new questions with temporal restrictions, a dedicated module was developed. The bulk of our efforts, however, was aimed at porting one of the streams to a "pure" QA-as-XMLretrieval setting, where the target collection is automatically annotated with linguistic information at indexing time, incoming questions are converted to semistructured queries, and evaluation of these queries gives a ranked list of candidate answers.</p><p>Our main findings this year are the following. While our system provides wrong answers for less than 40% of the test questions, we identified some obvious areas for improvement. First, we should work on definition extraction so that both questions asking for definitions and questions requiring resolving definitions can be answered in a better way. Second, we should examine inheritance of document links in the answer tiling process to make sure that the associated module does not cause unnecessary unsupported answers. And third, and most importantly, we should improve our answer filtering module to make sure that the semantic class of the generated answer corresponds with the class required by the question. The paper is organized as follows. In Section 2, we describe the architecture of our QA system. In Section 3, we describe the new XQuesta stream, i.e., the stream that implements QA-as-XMLretrieval. In Section 4, we detail our official runs. In Section 5, we discuss the results we have obtained and give a preliminary analysis of the performance of different components of the system. We conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture</head><p>Many QA systems share the following pipeline architecture. A question is first associated with a question type, from a predefined set such as Date-of-birth or Currency. Then, a query is formulated based on the question, and an information retrieval engine is used to identify a list of documents that are likely to contain the answer. Those documents are sent to an answer extraction module, which identifies candidate answers, ranks them, and selects the final answer. On top of this basic architecture, numerous add-ons have been devised, ranging from logic-based methods <ref type="bibr" coords="2,130.55,461.55,15.50,8.74" target="#b11">[12]</ref> to ones that rely heavily on the redundancy of information available on the World Wide Web <ref type="bibr" coords="2,138.99,473.51,9.96,8.74" target="#b3">[4]</ref>. Essentially, our system architecture implements multiple copies of the standard architecture, each of which is a complete standalone QA system that produces ranked answers, though not necessarily for all types of questions. The overall system's answer is then selected from the combined pool of candidates through a combination of merging and filtering techniques. For a reasonably detailed discussion of our QA system architecture we refer to <ref type="bibr" coords="2,354.08,533.28,9.97,8.74" target="#b1">[2]</ref>. A general overview of the system is given in Figure <ref type="figure" coords="2,169.09,545.24,3.88,8.74" target="#fig_0">1</ref>.</p><p>Question Processing. The first stage of processing, question processing, is common to all the streams. Each of the 200 questions is tagged, parsed, and assigned a question class based on our question classification module. Finally, the expected answer type is determined. See Section 3.2 for more details about question processing for the XQuesta stream, in particular.</p><p>There are seven streams in our system this year, four of which use the CLEF corpus to answer questions and three of which use external sources of information. We now provide a brief description of these seven streams. Note that except for the table stream and XQuesta, which we discuss in Sections 2.2 and 3 respectively, these streams remain unchanged from our system for last year's evaluation <ref type="bibr" coords="2,137.87,664.79,9.97,8.74" target="#b1">[2]</ref>.</p><p>Streams that consult the Dutch CLEF corpus. Four streams generate candidate answers from the Dutch CLEF corpus: Table Lookup, Pattern Match, Ngrams, and XQuesta. The XQuesta stream, which is based on the idea of QA-as-XML-retrieval, is completely new for this year's system; for more details about it, see Section 3.</p><p>The Table Lookup stream uses specialized knowledge bases constructed by preprocessing the collection, exploiting the fact that certain types of information (such as country capitals, abbrevi-ations, and names of political leaders) tend to occur in the document collection in a small number of fixed patterns. When a question type indicates that the question might potentially have an answer in these tables, a lookup is performed in the appropriate knowledge base and answers which are found there are assigned high confidence. For a detailed overview of this stream, see <ref type="bibr" coords="3,499.72,147.89,9.96,8.74" target="#b6">[7]</ref>; for information about improvements made to this stream for this year, see Section 2.2.</p><p>In the Pattern Match stream, zero or more regular expressions are generated for each question according to its type and structure. These patterns, which indicate strings that are highly likely to contain the answer, are then matched against the entire document collection.</p><p>The Ngram stream, similar in spirit to <ref type="bibr" coords="3,270.78,207.66,9.97,8.74" target="#b4">[5]</ref>, constructs a weighted list of queries for each question using a shallow reformulation process, similar to the Pattern Match stream. These queries are fed to a retrieval engine (we use the open-source Lucene, with a standard vector-space model <ref type="bibr" coords="3,490.86,231.57,14.76,8.74" target="#b10">[11]</ref>), and the top retrieved documents are used for harvesting word n-grams. The n-grams are ranked according to the weight of the query that generated them, their frequency, NE type, the proximity to the query keywords and more parameters; the top-ranking n-grams are considered candidate answers.</p><p>Streams that consult the Web.</p><p>Quartz has two streams that attempt to locate answers on the web: Ngram mining and Pattern Match. For Web searches, we use Google, and n-grams are harvested from the returned snippets. Pattern matching, by contrast, is done against full documents returned by Google.</p><p>Wikipedia stream. This stream also uses an external corpus-the Dutch Wikipedia (http: //nl.wikipedia.org), an open-content encyclopedia in Dutch (and other languages). However, since this corpus is much cleaner than news paper text, the stream operates in a different manner. First, the focus of the question is identified; this is usually the main named entity in the question. Then, this entity's encyclopedia entry is looked up; since Wikipedia is standardized to a large extent, this information has a template-like nature. Finally, using knowledge about the templates used in Wikipedia, information such as Date-of-death and First-name can easily be extracted.</p><p>After the streams have produced candidate answers, these answers must be processed in order to choose a single answer to return. Also, for those streams that do not extract answers directly from documents in the CLEF collection, documents in the corpus that support or justify their answers must be found. The modules that perform these tasks are described next.</p><p>Answer Justification. As some of our streams obtain candidate answers outside the Dutch CLEF corpus, and as answers need to be supported, or justified, by a document in the Dutch CLEF corpus, we need to find justification for externally found candidate answers. To this end we construct a query with keywords from a given question and candidate answer, and consider the top-ranking document for this query to be the justification, using an Okapi model, as this tends to do well on early high precision in our experience. Additionally, we use some retrieval heuristics such as marking the answer terms in the query as boolean (requiring them to appear in retrieved documents). Note: in addition to answers from the three streams that use outside data sources, answers from the corpus-based Ngram stream also need to be justified, since these answers are selected by the stream on the basis of support from multiple documents.</p><p>Filtering, Tiling, and Type Checking.</p><p>We use a final answer selection module (similar to that described in <ref type="bibr" coords="3,180.95,627.99,10.80,8.74" target="#b5">[6]</ref>) with heuristic candidate answer filtering and merging, and with stream voting. To compensate for named entity errors made during answer extraction, our type checking module (see <ref type="bibr" coords="3,146.51,651.90,15.50,8.74" target="#b13">[14]</ref> for details) uses several geographical knowledge bases to remove candidates of incorrect type for location questions. Furthermore, we take advantage of the temporally restricted questions in this year's evaluation to re-rank candidate answers for such questions using temporal information; see Section 2.3 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Improvements to table stream</head><p>To the tables used in 2004, we added a table which contained definitions extracted (offline) with two rules: one to extract definitions from appositions and another to create definitions by combining proper nouns with preceding common nouns. This table was used in parallel with the existing roles table, which contained definitions only for people. The new table contained more than three times as many entries (611,077) as the existing one.</p><p>In contrast to earlier versions of the table module, all tables are now stored in SQL format and made available in a MySQL database. The type of an incoming question is converted to sets of tuples containing three elements: table, source field and target field. The table code will search in the source field of the specified table for a pattern and, in cases where a match is found, keep the contents of corresponding target field as a candidate answer. Ideally the search pattern would be computed by the question analysis module but currently we use separate code for this task.</p><p>The table fields only contain noun phrases that are present in the text. This means that they can be used for answering questions such as:</p><formula xml:id="formula_0" coords="4,90.00,251.50,13.24,8.74">(1)</formula><p>Wie is de president van ServiÃ«? Who is the President of Serbia?</p><p>because the phrase president van ServiÃ« can normally be found in the text. However, in general, this stream cannot be used for answering questions such as:</p><p>(2) Wie was de president van ServiÃ« in 1999? Who was the President of Serbia in 1999?</p><p>because the modifier in 1999 does not necessarily always follow the profession. The table code contains backoff strategies (case insensitive vs. case sensitive, exact vs. inexact match) in case a search returns no matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Temporal restrictions</head><p>Twenty-six questions in this year's QA track are tagged as temporally restricted. As the name implies, such questions ask for information relevant to a particular time; the time in question may be given explicitly by a temporal expression (or timex ), as in:</p><p>(3) Q0094: Welke voetballer ontving "De Gouden Bal" in 1995? Which footballer won the European Footballer of the Year award in 1995?</p><p>or it may be given implicitly, with respect to another event, as in:</p><p>(4) Q0160: Hoe oud was Nick Leeson toen hij tot gevangenisstraf werd veroordeeld? How old was Nick Leeson when he was sentenced to prison?</p><p>Furthermore, the temporal restriction may not be to a point in time but to a temporal interval, as in:</p><p>(5) Q0156: Hoeveel medicinale produkten met tijgeringredienten gingen er tussen 1990 en 1992 over de toonbank? How many medicinal products containing tiger ingredients were sold between 1990 and 1992?</p><p>(6) Q0008: Wie speelde de rol van Superman voordat hij verlamd werd? Who played the role of Superman before he was paralyzed?</p><p>In our runs this year, we took advantage of these temporal restrictions to re-rank candidate answers for temporally restricted questions. Because we had already built a module to identify and normalize timexes (see Section 3.1), we limited ourselves to explicit temporal restrictions (i.e., those signalled by timexes). Handling event-based restrictions would require identifying (and possibly temporally locating) events, which is a much more difficult problem.</p><p>For each temporally restricted question, the temporal re-ranker tries to identify the temporal restriction by looking for temporal prepositions (such as in, op, tijdens, voor, na) and timexes in the question. If it succeeds in identifying an explicit temporal restriction, it proceeds with re-ranking the candidate answers.</p><p>For each candidate answer, the justification document is retrieved, and sentences containing the answer and, if there is one, the question focus are extracted from it. If there are any timexes in these sentences, the re-ranker checks whether these timexes are compatible with the restriction. For each compatible timex, the score for the candidate answer is boosted; for each incompatible timex, the score is lowered. The timestamp of the document is also checked for compatibility with the restriction, and the score is adjusted accordingly. The logic involved in checking compatibility of a timex with a temporal restriction is relatively straightforward; the only complications come in handling times of differing granularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">XQuesta</head><p>The XQuesta stream implements a QA-as-XML-retrieval approach <ref type="bibr" coords="5,383.74,274.38,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="5,402.49,274.38,11.63,8.74" target="#b12">13]</ref>. The target collection is automatically annotated with linguistic information offline. Then, incoming questions are converted to semistructured queries, and evaluation of these queries gives a ranked list of candidate answers. We describe the three stages in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Offline annotation</head><p>We automatically processed the Dutch QA collection, identifying sentences and annotating them syntactically and semantically. For processing time reasons, we switched from the full parses used in 2004 to a Dutch part-of-speech tagger and a text chunker. Both were trained on the CGN corpus <ref type="bibr" coords="5,121.40,392.39,15.50,8.74" target="#b14">[15]</ref> in the same way as the tagger and chunker described in <ref type="bibr" coords="5,383.45,392.39,15.50,8.74" target="#b16">[17]</ref> but using the TnT tagger <ref type="bibr" coords="5,90.00,404.35,10.52,8.74" target="#b2">[3]</ref> rather than a general machine learner. Two extra layers of annotation were added: named entities, by the TnT tagger trained on CoNLL-2002 data <ref type="bibr" coords="5,349.10,416.30,14.62,8.74" target="#b15">[16]</ref>, and extra temporal expressions, by a hand-coded rule-based system, because these were not included in the previously mentioned training data. The temporal expression annotation system not only identified temporal expressions but also, where possible, normalized them to a standard format (ISO 8601).</p><p>Here are some examples of the annotations. Example 7 shows a tokenized sentence in which each token has received an abbreviated Dutch part-of-speech tag. <ref type="bibr" coords="5,90.00,496.00,12.73,8.74" target="#b6">(7)</ref> &lt;LID&gt;de&lt;/LID&gt; &lt;ADJ&gt;machtige&lt;/ADJ&gt; &lt;N&gt;burgemeester&lt;/N&gt; &lt;VZ&gt;van&lt;/VZ&gt; &lt;N&gt;Moskou&lt;/N&gt; &lt;LET&gt;,&lt;/LET&gt; &lt;N&gt;Joeri&lt;/N&gt; &lt;N&gt;Loezjkov&lt;/N&gt; &lt;LET&gt;,&lt;/LET&gt; &lt;WW&gt;veroordeelt&lt;/WW&gt; &lt;VNW&gt;dat&lt;/VNW&gt; the powerful mayor of Moscow , Joeri Loezjkov , condemns that In Example 8, the sentence is divided into syntactic chunks. The three most frequently occurring chunk types are noun phrases (NP), verb phrases (VP) and prepositional phrases (PP). Some tokens, nearly always the punctuation signs, are not part of a chunk. By definition, chunks cannot be embedded. <ref type="bibr" coords="5,90.00,607.59,12.73,8.74" target="#b7">(8)</ref> &lt;NP&gt;de machtige burgemeester&lt;/NP&gt; The four annotation layers of the collection were converted to an XML format and stored in separate XML files, to simplify maintenance. Whenever the XQuesta stream requested a document from the collection, all annotation were automatically merged to a single XML document providing full access to the extracted information. In order to produce well-formed XML mark-up after merging, we used a mix of inline and stand-off (with character offsets to refer to original collection text) annotation schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question analysis</head><p>The current question analysis module consists of two parts. The first part determines possible question classes, such as location for the question shown in Example <ref type="bibr" coords="6,401.51,337.26,16.38,8.74" target="#b11">(12)</ref>. We use 31 different question types, some of which belong to a more general class: for example, date birth and date death describe dates of birth and death and are subtypes of the class date. The assignment of the classes is based on manually compiled patterns.</p><p>The second part of our question analysis module is new. Depending on the predicted question class, an expected answer type is assigned. Our new system design allows us to search for structural and semantic information. The expected answer types describe syntactic, lexical or surface requirements which have to be met by the possible answers. The restrictions are formulated in XPath queries which are used to extract specific information from our preprocessed documents. For instance, possible answers to questions such as Example ( <ref type="formula" coords="6,358.18,484.48,8.85,8.74">13</ref>) and ( <ref type="formula" coords="6,398.14,484.48,8.85,8.74">14</ref>) require a named entity as an answer (person and timex, respectively). Besides the named entity information, the answer to the question in Example ( <ref type="formula" coords="6,218.91,508.39,8.85,8.74">14</ref>) has to start with a number. The corresponding XPath queries are NE[@type="PER"] and TIMEX[@val=~/^\d/] respectively. <ref type="bibr" coords="6,90.00,540.16,17.71,8.74" target="#b12">(13)</ref> Q0149: Wie is de burgemeester van Moskou in 1994? Who was the mayor of Moskow in 1994? ( <ref type="formula" coords="6,94.43,571.98,8.86,8.74">14</ref>) Q0014: Wanneer is Luc Jouret geboren? When was Luc Jouret born?</p><p>Table <ref type="table" coords="6,117.96,603.75,4.98,8.74" target="#tab_1">1</ref> displays the rules for mapping the question classes to the expected answer types which were manually developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Extracting and ranking answers</head><p>As described in Section 3.2, incoming questions are automatically mapped to retrieval queries (simply question texts) and XPath expressions corresponding to types of expected answers. Retrieval queries are used to locate relevant passages in the collection. For retrieval, we use nonoverlapping passages of at least 400 characters starting and ending at paragraph boundaries. Then, the question's XPath queries are evaluated on the top 20 retrieved passages, giving lists of XML elements corresponding to candidate answers. For example, for the question in Example 14 above, with the generated XPath query "TIMEX[@val=~/^\d/]", the value "1947 " is extracted from the annotated text in Example 10. The score of each candidate is calculated as the sum of retrieval scores of all passages containing the candidate. Furthermore, the scores are normalized using web hit counts, producing the final ranked list of XQuesta's answer candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Runs</head><p>We submitted two Dutch monolingual runs. The run uams051nlnl used the full system with all streams and final answer selection. The run uams052nlnl, on top of this, used an additional stream: the XQuesta stream with paraphrased questions. As a simple way of paraphrasing questions, we double-translated questions (from Dutch to English and then back to Dutch) using Systran, an automatic MT system. Question paraphrases were only used for query formulation at the retrieval step; question analysis (identification of question types, expected answer types and corresponding XPath queries) was performed on the original questions. Our idea was to see whether simple paraphrasing of retrieval queries would help to find different relevant passages and lead to more correctly answered questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and analysis</head><p>Our question classifier assigned a question type to 187 questions. All classified questions also received an expected answer type. Eleven of the unclassified questions are What/Which NP questions, such as Example 15. The remaining two unclassified questions are of the form Name an NP. <ref type="bibr" coords="7,90.00,655.02,17.71,8.74" target="#b14">(15)</ref> Q0024: Welke ziekte kregen veel Amerikaanse soldaten na de Golfoorlog? Which disease did many American soldiers contract after the Gulf War? Here the answer extreme right group would be correct as an answer to the question What is Eyal?.</p><p>Unfortunately the system generated the answer leader of the extreme right group. This extraction problem also affected the answers for questions that provided a definition and asked for a name. We expect that this problem can be solved by a check of the semantic class of the question focus word and head noun of the answer, both in the answer extraction process and in answer postprocessing. Such a check would also have prevented seven of the 15 other incorrect answers of this group. When we examined the assessments of the answers to the three different question types, we noticed that the proportion of correct answers was the same for definition questions (45%) and factoid questions (47%) but that temporally restricted questions seemed to cause problems (27% correct). Of the 18 incorrect answers in the latter group, four involved an answer which would have been correct in another time period (questions Q0078, Q0084, Q0092 and Q0195). If these questions had been answered correctly, the score for this category would have raised to an acceptable 46% (including the incorrectly assessed answer to question Q0149).</p><p>The temporal re-ranking module described in Section 2 did make a positive contribution, however. For the following two temporally restricted questions: <ref type="bibr" coords="8,90.00,474.15,17.71,8.74" target="#b16">(17)</ref> Q0007: Welke voetballer ontving "De Gouden Bal" in 1995? Which footballer won the European Footballer of the Year award in 1995?</p><p>(18) Q0159: Wie won de Nobelprijs voor medicijnen in 1994? Who won the Nobel Prize for medicine in 1994?</p><p>the highest ranking candidate answer before the temporal re-ranking module was applied was incorrect (Ruud Gullit and Martin Luther King), but the application of the temporal re-ranking module boosted the correct answer (Weah and Martin Rodbell ) to the top position. Additionally, the temporal re-ranking module never demoted a correct answer from the top position. The answers to the temporally restricted questions also indicate the overall problems of the system. Of the remaining 14 incorrect answers, only five were of the expected answer category while nine were of a different category. In the 62 answers of factoid and definition questions which were judged to be wrong, the majority (58%) had an incorrect answer class. An extra answer postprocessing filter which compares the semantic category of the answer and the one expected by the question would prevent such mismatches.</p><p>Our system produced five answers which were judged to be unsupported. One of these was wrong and another was right. A third answer was probably combined from different answers and a link to a document containing a part rather than the whole answer was kept. The remaining two errors were probably also caused by a document link which should not have been kept but the reason for this is unknown. This error analysis suggests three ways to improve our QA system. First, we should work on definition extraction so that both questions asking for definitions and questions requiring the resolution of definitions can be answered in a better way. Second, we should examine inheritance of document links in the answer tiling process to make sure that the associated module does not cause unnecessary unsupported answers. And third, and most importantly, we should improve answer filtering to make sure that the semantic class of the generated answer corresponds with the class required by the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The bulk of our efforts for the 2005 edition of the QA@CLEF track was aimed at implementing a "pure" QA-as-XML-retrieval stream, as part of our multi-stream question answering architecture; here, the target collection is automatically annotated with linguistic information at indexing time, incoming questions are converted to semistructured queries, and evaluation of these queries gives a ranked list of candidate answers. The overall system provides wrong answers for less than 40% of the questions. Our ongoing work is aimed at addressing the main sources of error discovered: definition extraction, inheritance of document links in answer tiling, and semantically informed answer filtering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,103.12,235.03,396.77,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Quartz-2005: the University of Amsterdam's Dutch Question Answering System.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,90.00,357.07,17.71,8.74;6,131.51,357.07,198.13,8.74;6,135.94,369.03,212.81,8.74"><head>( 12 )</head><label>12</label><figDesc>Q0065: Uit welk land komt Diego Maradona? What country does Diego Maradona come from?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,90.00,607.59,423.01,140.24"><head></head><label></label><figDesc>&lt;PP&gt;van&lt;/PP&gt; &lt;NP&gt;Moskou&lt;/NP&gt; , &lt;NP&gt;Joeri Loezjkov&lt;/NP&gt; , &lt;VP&gt;veroordeelt&lt;/VP&gt; &lt;NP&gt;dat&lt;/NP&gt; Example 9 shows the named entities in the text. These are similar to text chunks: they can contain sequences of words and cannot be embedded. Four different named entity types may be identified:</figDesc><table coords="6,90.00,112.02,423.00,96.18"><row><cell>(10)</cell><cell>Luc Jouret werd in &lt;TIMEX val="1947"&gt;1947&lt;/TIMEX&gt; geboren</cell></row><row><cell></cell><cell>Luc Jouret was born in 1947</cell></row><row><cell cols="2">Normalization is more complicated in Example 11; in order to determine that donderdagmorgen</cell></row><row><cell cols="2">refers to 1994-10-06, the system uses the document timestamp (in this case, 1994-10-08) and some</cell></row><row><cell cols="2">simple heuristics to compute the reference.</cell></row><row><cell>(11)</cell><cell>Ekeus verliet Irak &lt;TIMEX val="1994-10-06"&gt;donderdagmorgen&lt;/TIMEX&gt;</cell></row><row><cell></cell><cell>Ekeus left Iraq Thursday morning</cell></row></table><note coords="5,90.00,663.38,394.18,8.74;5,90.00,683.30,13.24,8.74;5,131.51,683.30,267.30,8.79;5,131.51,695.26,228.72,8.79;5,90.00,715.18,423.01,8.74;5,90.00,727.14,423.00,8.74;5,90.00,739.09,119.42,8.74"><p>persons (PER), organizations (ORG), locations (LOC) and miscellaneous entities (MISC). (9) de machtige burgemeester van &lt;NE type="LOC"&gt;Moskou&lt;/NE&gt; , &lt;NE type="PER"&gt;Joeri Loezjkov&lt;/NE&gt; , veroordeelt dat The next two examples show temporal expressions which have been both identified and normalized to ISO 8601 format. In Example 10, normalization is straightforward: the ISO 8601 value of the year 1947 is simply "1947."</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,131.47,110.05,340.06,227.04"><head>Table 1 :</head><label>1</label><figDesc>Overview of the mapping rules from question classes to answer types</figDesc><table coords="7,169.70,110.05,263.61,206.18"><row><cell>Question class</cell><cell>Restrictions on the type of answer</cell></row><row><cell>abbreviation</cell><cell>word in capital letters</cell></row><row><cell>age</cell><cell>numeric value, possible word: jarige</cell></row><row><cell>cause reason</cell><cell>sentence</cell></row><row><cell>city capital</cell><cell>LOC</cell></row><row><cell>color</cell><cell>adjective</cell></row><row><cell>date death, date birth, date</cell><cell>TIMEX, digital number</cell></row><row><cell>definition person</cell><cell>sentence</cell></row><row><cell>definition</cell><cell>noun phrase or sentence</cell></row><row><cell>distance</cell><cell>numeric value</cell></row><row><cell>distinction</cell><cell>noun phrase or a sentence</cell></row><row><cell>expansion</cell><cell>MISC or ORG, noun phrase</cell></row><row><cell>height</cell><cell>numeric value</cell></row><row><cell>language</cell><cell>MISC</cell></row><row><cell>length</cell><cell>numeric value</cell></row><row><cell>location</cell><cell>LOC</cell></row><row><cell>manner</cell><cell>sentence</cell></row><row><cell>monetary unit</cell><cell>MISC</cell></row><row><cell>name</cell><cell>named entity</cell></row><row><cell>number people</cell><cell>numeric value, noun phrase</cell></row><row><cell>number</cell><cell>numeric value</cell></row><row><cell>organization</cell><cell>ORG</cell></row><row><cell>person</cell><cell>PER</cell></row><row><cell>score, size, speed, sum of money</cell><cell>numeric value</cell></row><row><cell>synonym name</cell><cell>PER</cell></row><row><cell>temperature, time period</cell><cell>numeric value</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,90.00,686.90,423.00,56.56"><head>Table 2</head><label>2</label><figDesc>lists the assessment counts for the two University of Amsterdam question answering runs for CLEF-2005 monolingual Dutch (NLNL) task. The two runs had 13 different answers, but the assessors evaluated only two pairs differently. We were surprised about the large number of inexact answers. When we examined the inexact answers of the first run, we found that a disproportional number of these were generated for definition questions: 85% (only 30% of the questions ask</figDesc><table coords="8,180.17,110.79,242.66,33.47"><row><cell>Run</cell><cell cols="4">Right Unsupp. Inexact Wrong</cell></row><row><cell>uams051nlnl</cell><cell>88</cell><cell>5</cell><cell>28</cell><cell>79</cell></row><row><cell>uams052nlnl</cell><cell>88</cell><cell>5</cell><cell>29</cell><cell>78</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,90.00,167.36,423.01,108.31"><head>Table 2 :</head><label>2</label><figDesc>Assessment counts for the 200 answers in the two Amsterdam runs submitted for Dutch monolingual Question Answering (NLNL) in CLEF-2005. Thirteen answers of the two runs were different but only two of these were assessed differently. for definitions). Almost half of the errors (13 out of 28) were caused by the same information extraction problem: determining where a noun phrase starts. Here is an example:</figDesc><table coords="8,90.00,254.97,209.39,20.69"><row><cell>(16)</cell><cell>Q0094: Wat is Eyal?</cell></row><row><cell></cell><cell>A: leider van de extreem-rechtse groep</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by various grants from the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs>. <rs type="person">Valentin Jijkoun</rs>, <rs type="person">Karin MÃ¼ller</rs>, and <rs type="person">Maarten de Rijke</rs> were supported under project number <rs type="grantNumber">220-80-001</rs>. <rs type="person">Erik Tjong Kim Sang</rs> and <rs type="person">Maarten de Rijke</rs> were supported under project number <rs type="grantNumber">264-70-050</rs>. <rs type="person">David Ahn</rs> and <rs type="person">Maarten de Rijke</rs> were supported under project number <rs type="grantNumber">612.066.302</rs>. In addition, <rs type="person">Maarten de Rijke</rs> was also supported by grants from <rs type="funder">NWO</rs>, under project numbers <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">365-20-005</rs>, <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">612.000.207</rs>, and <rs type="grantNumber">612.069.006</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YTy3k3b">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_E5uS8CB">
					<idno type="grant-number">264-70-050</idno>
				</org>
				<org type="funding" xml:id="_4uatCxd">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_erq7y54">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_nMDk5hW">
					<idno type="grant-number">365-20-005</idno>
				</org>
				<org type="funding" xml:id="_X4Eubtm">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_CCmvqTA">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_VaMUzBy">
					<idno type="grant-number">612.000.207</idno>
				</org>
				<org type="funding" xml:id="_UBXFgEX">
					<idno type="grant-number">612.069.006</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.48,455.65,402.52,8.74;9,110.48,467.60,402.52,8.74;9,110.48,479.56,252.16,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,443.22,455.65,69.78,8.74;9,110.48,467.60,83.08,8.74">Using wikipedia at the trec qa track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,382.85,467.60,130.16,8.74;9,110.48,479.56,110.15,8.74">The Thirteenth Text Retrieval Conference (TREC 2004)</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,499.48,402.52,8.74;9,110.48,511.44,402.52,8.74;9,110.48,523.39,402.52,8.74;9,110.48,535.35,402.52,8.74;9,110.48,547.30,122.40,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,429.55,499.48,83.45,8.74;9,110.48,511.44,338.42,8.74">Making stone soup: Evaluating a recall-oriented multi-stream question answering stream for dutch</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,405.98,523.39,107.01,8.74;9,110.48,535.35,367.82,8.74">Multilingual Information Access for Text, Speech and Images: Results of the Fifth CLEF Evaluation Campaign</title>
		<title level="s" coord="9,486.57,535.35,26.43,8.74">LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,567.23,355.02,8.74" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<title level="m" coord="9,159.95,567.23,182.84,8.74">TnT -A Statistical Part-Of-Speech tagger</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>Saarland University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,587.15,402.52,8.74;9,110.48,599.11,402.52,8.74;9,110.48,611.06,22.70,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,269.74,587.15,238.62,8.74">An analysis of the AskMSR question-answering system</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,122.82,599.11,317.72,8.74">Proc. Empirical Methods in Natural Language Processing (EMNLP 2002)</title>
		<meeting>Empirical Methods in Natural Language essing (EMNLP 2002)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,630.99,402.52,8.74;9,110.48,642.95,402.51,8.74;9,110.48,654.90,63.65,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,335.14,630.99,177.85,8.74;9,110.48,642.95,26.59,8.74">Web question answering: Is more always better</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,375.44,642.95,104.99,8.74">Proceedings of SIGIR&apos;02</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Bennett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</editor>
		<meeting>SIGIR&apos;02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,674.83,402.51,8.74;9,110.48,686.78,402.52,8.74;9,110.48,698.74,397.61,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,234.24,674.83,278.75,8.74;9,110.48,686.78,64.89,8.74">Answer Selection in a Multi-Stream Open Domain Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,346.17,686.78,166.83,8.74;9,110.48,698.74,154.90,8.74">Proceedings 26th European Conference on Information Retrieval (ECIR&apos;04)</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Mcdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tait</surname></persName>
		</editor>
		<meeting>26th European Conference on Information Retrieval (ECIR&apos;04)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2997. 2004</date>
			<biblScope unit="page" from="99" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,718.66,402.52,8.74;9,110.48,730.62,402.52,8.74;9,110.48,742.57,82.96,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,314.56,718.66,198.44,8.74;9,110.48,730.62,40.95,8.74">Preprocessing Documents to Answer Dutch Questions</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,176.13,730.62,336.87,8.74;9,110.48,742.57,51.77,8.74">Proceedings of the 15th Belgian-Dutch Conference on Artificial In telligence (BNAIC&apos;03)</title>
		<meeting>the 15th Belgian-Dutch Conference on Artificial In telligence (BNAIC&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,112.02,402.52,8.74;10,110.48,123.98,154.54,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,299.20,112.02,139.64,8.74">How frogs built the Berlin Wall</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,463.34,112.02,49.66,8.74;10,110.48,123.98,48.71,8.74">Proceedings CLEF 2003</title>
		<meeting>CLEF 2003</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,143.90,402.52,8.74;10,110.48,155.86,402.51,8.74;10,110.48,167.81,63.65,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,436.47,143.90,76.54,8.74;10,110.48,155.86,248.50,8.74">The University of Amsterdam at the TREC 2003 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Tsur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,379.02,155.86,102.60,8.74">Proceedings TREC 2003</title>
		<meeting>TREC 2003</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="586" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,187.74,402.53,8.74;10,110.48,199.69,260.55,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,174.83,187.74,253.81,8.74">Use of metadata for question answering and novelty tasks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Litkowksi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,451.26,187.74,61.75,8.74;10,110.48,199.69,230.00,8.74">Proceedings of the Twelfth Text REtrieval Conference (TREC 2003)</title>
		<meeting>the Twelfth Text REtrieval Conference (TREC 2003)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,219.62,359.74,9.02" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><surname>Lucene</surname></persName>
		</author>
		<ptr target="http://jakarta.apache.org/lucene/" />
		<title level="m" coord="10,148.24,219.62,110.72,8.74">The Lucene search engine</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,239.54,402.52,8.74;10,110.48,251.50,402.52,8.74;10,110.48,263.45,129.83,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,382.27,239.54,130.73,8.74;10,110.48,251.50,238.73,8.74">Performance issues and error analysis in an open-domain question answering system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,359.52,251.50,153.48,8.74;10,110.48,263.45,32.72,8.74">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="154" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,283.38,402.52,8.74;10,110.48,295.33,402.52,8.74;10,110.48,307.29,38.75,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,157.74,283.38,202.26,8.74">Retrieval using structure for question answering</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,146.20,295.33,315.68,8.74">Proceedings of the First Twente Data Management Workshop (TDM&apos;04)</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Mihajlovic</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</editor>
		<meeting>the First Twente Data Management Workshop (TDM&apos;04)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="15" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,327.21,402.52,8.74;10,110.48,339.17,402.53,8.74;10,110.48,351.12,53.22,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,325.04,327.21,187.96,8.74;10,110.48,339.17,43.42,8.74">Type Checking in Open-Domain Question Answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Olsthoorn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,175.54,339.17,337.47,8.74;10,110.48,351.12,22.68,8.74">Proceedings of the 16th European Conference on Artificial Intelligence (ECAI 2004)</title>
		<meeting>the 16th European Conference on Artificial Intelligence (ECAI 2004)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,371.05,402.52,8.74;10,110.48,383.00,402.52,8.74;10,110.48,394.96,254.85,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,422.23,371.05,90.77,8.74;10,110.48,383.00,104.37,8.74">CGN, an Annotated Corpus of Spoken Dutch</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schuurman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schouppe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hoekstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Van Der Wouden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,236.17,383.00,276.83,8.74;10,110.48,394.96,133.75,8.74">Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC-03)</title>
		<meeting>the 4th International Workshop on Linguistically Interpreted Corpora (LINC-03)<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,414.88,402.52,8.74;10,110.48,426.84,402.52,8.74;10,110.48,438.79,22.70,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,211.24,414.88,301.76,8.74;10,110.48,426.84,115.33,8.74">Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,248.39,426.84,118.95,8.74">Proceedings of CoNLL-2002</title>
		<meeting>CoNLL-2002<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="155" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,458.72,402.52,8.74;10,110.48,470.67,402.52,8.74;10,110.48,482.63,181.49,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,364.88,458.72,148.12,8.74;10,110.48,470.67,91.58,8.74">Reduction of Dutch Sentences for Automatic Subtitling</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">F</forename><surname>Tjong Kim Sang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>HÃ¶thker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,224.18,470.67,111.63,8.74">Proceedings of CLIN-2003</title>
		<title level="s" coord="10,110.48,482.63,130.31,8.74">Antwerp Papers in Linguistics</title>
		<meeting>CLIN-2003</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">111</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Antwerp</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
