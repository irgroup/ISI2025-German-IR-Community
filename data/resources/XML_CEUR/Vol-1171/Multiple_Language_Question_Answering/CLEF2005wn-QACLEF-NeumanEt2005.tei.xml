<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,120.32,148.86,362.36,15.15;1,145.90,170.78,311.21,15.15">DFKI&apos;s LT-lab at the CLEF 2005 Multiple Language Question Answering Track</title>
				<funder ref="#_gzMAN6A">
					<orgName type="full">BMBF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,212.55,204.67,75.93,8.74"><forename type="first">Günter</forename><surname>Neumann</surname></persName>
							<email>neumann@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="department">LT-Lab</orgName>
								<orgName type="institution">DFKI</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.16,204.67,79.30,8.74"><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
							<email>bogdan@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="department">LT-Lab</orgName>
								<orgName type="institution">DFKI</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,120.32,148.86,362.36,15.15;1,145.90,170.78,311.21,15.15">DFKI&apos;s LT-lab at the CLEF 2005 Multiple Language Question Answering Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">30EDC2469053F685CD8226145BC6F914</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>I.7 [Document and Text Processing]: I.7.1 Document and Text Editing</term>
					<term>I.7.2 Document Preparation</term>
					<term>I.2 [Artificial Intelligence]: I.2.7 Natural Language Processing Algorithms, Design, Experimentation Open-Domain Question Answering, Mono-Lingual German, Cross-Lingual German/English, Qtype-Strategies, Query decomposition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the work done by the QA group of the Language Technology Lab at DFKI for the 2005 edition of the Cross-Language Evaluation Forum (CLEF). We describe the extensions made to our 2004 QA@CLEF German/English QA-system, especially the question-type driven selection of answer strategies. Furthermore, details concerning the processing of definition and temporal questions are described, as well as the results obtained in the monolingual German, bilingual English/German, and bilingual German/English tasks are presented and discussed throughout the paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The basic functionality of a cross-lingual open-domain question/answering (ODQA) system is simple: given a Natural Language query in one language (say German) find answers for that query in textual documents written in another language (say English). In contrast to a standard cross-language IR system, the natural language questions are usually well-formed NL-query clauses (instead of a set of keywords), and the identified answers should be exact answer strings (instead of complete documents containing the answers).</p><p>Since 2003, cross-lingual systems are evaluated as part of a special track at Clef. This year, the task was to process 200 questions of type factoid, temporally restricted, and definition, and to return for each question one exact answer (together with the identifier of the document source from which the answer was extracted) or NIL, if no answer could be found. Last year only factoid and definition questions had to be handled.</p><p>Starting from our 2004-system (cf. <ref type="bibr" coords="1,262.64,723.18,26.36,8.74" target="#b2">[NS05]</ref>), the major efforts we spend for qa@clef2005 were focused on:</p><p>• development of a component-oriented ODQA-core architecture</p><p>• processing definition and temporally restricted questions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• exploration of web-based answer validation</head><p>Beside that we also decided to take part in three different tasks:</p><p>1. monolingual German ODQA: here we could improve our result from last year from 23.5% to 43.5% this year 2. German-English ODQA: here we achieved with 25.5% accuracy a minor improvement compared with our 2004-result (23.5%)</p><p>3. English-German ODQA: this was our first participation in this task and we achieved a result of 23% accuracy For all three tasks, we obtained the best results. We will now describe some interesting technical aspects of our 2005-system -named Quantico -before presenting and discussing the results in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>Based on a number of experiments we made during the development of our ODQA-technology, we developed the hypothesis that a structural analysis of un-structured documents towards the information needs of questions, will support the retrieval of relevant small textual information units through informative IR-queries. However, since we cannot foresee all the different users interests or questions especially in the open-domain context, a challenging research question is: How detailed can the structural analysis be made without putting over a "straitjacket" of a particular interpretation on the un-structured source? Thus, there is a trade-off between off-line and on-line document annotation. Questions and answers are somewhat related in that questions influence the information geometry and hence, the information view and access, cf. <ref type="bibr" coords="2,456.32,461.68,27.49,8.74" target="#b3">[Rij04]</ref>.</p><p>Based on this insights, we developed the ODQA-architecture as depicted in figure <ref type="figure" coords="2,480.33,473.64,3.88,8.74" target="#fig_0">1</ref>. The idea behind the specific design is the assumption that an off-line annotation of the data collection supports an answer type oriented indexing and answer extraction process through the selection of query-type specific strategies (a similar approach is also used by [MHC + 04]). Furthermore, a sentence-oriented preprocessing determining only sentence boundary, named entities (NE) and their co-reference, as well as NE-anchored tuples (see sec. 3) turned out to be a useful level of off-line annotation, at least for the Clef-type of questions.</p><p>In order to achieve a high degree of flexibility of the ODQA-core components in future applications, an important design decision was to a use a central QAController: based on the result of the NL-question analysis component, the QAController decides which of the following strategies will be followed:</p><formula xml:id="formula_0" coords="2,104.94,613.11,95.12,48.59">• Definition Question • Temporal Question • Factoid Question</formula><p>Each strategy of the above-mentioned ones corresponds to different settings for each of the components. For the Factoid Question strategy, for example, the Retrieval Component considers sentences as information units; the Answer Extraction Component defines classes of instances for one of the entity types PERSON, ORGANIZATION, LOCATION, DATE and NUMBER; the Answer Selection Component considers relevant information as being the one more closed (distance metric) to the question keywords and with the most coherent context. were used for this purpose.</p><p>Based on a corpus of almost 500 Mbytes textual data from the Clef corpus for every language taken into consideration (German and English), two indices were created corresponding to pairs of phrases of the form (see also fig. <ref type="figure" coords="3,247.21,643.70,4.98,8.74" target="#fig_0">1</ref> where the (NE,XP) and abbreviation store memorize these indices).</p><p>(Silvio Berlusconi, the Italian prime-minister) and (NAFTA, North American Free Trade Agreement)</p><p>The Retrieval Component for the Definition Question strategy uses these indices and considers the phrases on the right side as the information units containing the possible answer, in case corresponding matching left elements have been identified during the Query Analysis Component.</p><p>Temporally Restricted Questions In order to fulfill the requirements of the 2005 qa@clef task description, we developed specific methods for the treatment of temporally restricted questions, e.g., questions like "Who was the German Chancellor in the year 1980?", "Who was the German Chancellor between 1970 and 1990?", or "Who was the German Chancellor when the Berlin Wall was opened?". It was our goal, to process questions of this kind on basis of our existing technology following a divide-and-conquer approach, i.e., by question decomposition and answer fusion. The highly flexible design of Quantico actually supported us in achieving this goal. Two methods were implemented:</p><p>1. The existing methods for handling factoid questions are used without change to get initial answer candidates. In a follow-up step, the temporal restriction from the question is used to check the answer's temporal consistency.</p><p>2. A temporally restricted questions Q is decomposed into two sub-questions, one referring to the "timeless" proposition of Q, and the other to the temporally restricting part. For example, the question "Who was the German Chancellor when the Berlin Wall was opened?" is decomposed into the two sub-questions "Who was the German Chancellor'" and "When was the Berlin Wall opened?". The answers of both are searched for independently, but checked for consistency in a follow-up answer fusion step. In this step, the found explicit temporal restriction is used to constrain the "timeless" proposition.</p><p>The decomposition of such questions into sub-questions is helpful in cases, where the temporal restriction is only specified implicitly, and hence can only be deduced through application of specific inference rules. Note that the decomposition operation is mainly syntax driven, in that it takes into account the grammatical relationship of the sub-and main clauses identified and analysed by Quantico' parser SMES, cf. <ref type="bibr" coords="4,276.27,396.95,27.87,8.74" target="#b1">[NP02]</ref>.</p><p>Through evaluation of a number of experiments, it turned out that processing of question with method 1.) leads to higher precision, and processing of questions using method 2.) leads to increased recall. An initial evaluation of our Clef-results also suggest, that the methods are critically depending on the quality of recognition of time and date expression (see section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Cross-lingual Methods</head><p>Two strategies were used for responding questions asked in a language different from the one of the documents containing the answer. Both strategies employ online translation services (Altavista, FreeTranslation, etc.) for crossing the language barrier, but at different processing steps: before and after the Analysis Component (see also figure <ref type="figure" coords="4,311.39,535.23,3.88,8.74" target="#fig_1">2</ref>).</p><p>The before-method translated the question string in an earlier step, resulting in several automatic translated strings, of which the best one was then passed on to the Retrieval Component after having been analyzed by the Query Analysis Component. This was the strategy we used in the English-German task. To be more precise: the English source question was translated into several alternative German questions using online MT services. Each German question was then parsed with SMES, Quantico's German parser. The resulting query object was then weighted according to its linguistic well-formedness and its completeness wrt. query information (question type, question focus, answer-type). The assumption behind this weighting scheme is that "a translated string is of greater utility for subsequent processes than another one, if its linguistic analysis is more complete/appropriate."</p><p>The after-method translated the formalized result of the Query Analysis Component by using the question translations, a language modeling and a word alignment tool for creating a mapping of the formal information need from the source language into the target language. We used this strategy in the German-English task along two lines (using the following German query as example: In welchem Jahrzehnt investierten japanische Autohersteller sehr stark? ):</p><p>1. translations as returned by the on-line MT systems are being ranked according to a language model In which decade did Japanese automakers invest very strongly? (0.7) In which decade did Japanese car manufacturers invest very strongly? (0.8) 2. translations with a satisfactory degree of resemblance to a natural language utterance (i.e. linguistically well-formedness), given by a threshold on the language model ranking, are aligned based on several filters: dictionary filter -based on MRD (machine readable dictionaries), PoS filter -based on statistical part-of-speech taggers, and cognates filter -based on string similarity measures (dice coefficient and LCSR (lowest common substring ratio)).</p><p>In: The CLEF evaluation gives evidence that both strategies are comparable in results, whereby the last one is slightly better, due to the fact of not being forced to choose a best translation, but working with and combining all the translations available. That is, considering and combining several, possible different, translations of the same question, the chance of catching a translation error in an earlier phase of the work-flow becomes higher and propagating errors through the whole system becomes less certain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Web Validation</head><p>Our previous Clef-systems where "autistic" in the sense that we did not make use of the Web, neither for answer prediction nor for answer validation. Since we will fuse our current ODQAtechnology with the Web in the near future, we started the development of web-based ODQAstrategies. Using the 2004 qa@clef as a testbed, we implemented an initial prototype of a webvalidator realizing the following approach: Starting point are the M-best answer candidates found by Quantico using the Clef corpus only. Then, for each answer candidate a Google query is constructed from the answer and the the internal representation of the NL-query. The questionanswer pair is send to Google and the resulting total frequency count (TFC) is used to sort the set of answer candidates according to the individual values for TFC. The answer with the highest TFC is then selected as the best answer. The underlying assumption here is, that an IR-query consisting of the NL query terms and the correct answer term will have a higher redundancy on the Web, than one using a false answer candidate. Of course, applying such a method successfully presupposes a semantic independency between answer candidates. For this kind of answers, our method seem to work quite well. However, for answer candidates, which stand in a certain "hidden" relationship (e.g., because a ISA-relation exists between the two candidates), the current method is not sufficient. This is also true for those answer candidates which refer to a different timeline or context than that, preferred by the Web search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head><p>This year, we took part in three tasks: 1.) monolingual German (DE2DE), 2.) cross-lingual English/German (EN2DE), and 3.) cross-lingual German/English (DE2EN). We would like to stress at this point, that in all different tasks, the same ODQA-core machinery was used, extended only for handling the cross-lingual aspects.</p><p>The results can be found in tables 1 (DE2DE), 2 (EN2DE), and 3 (DE2EN), respectively. For the tasks DE2DE and EN2DE we submitted two runs: one without web validation (the runs dfki051dede and dfki051ende) and one with web-validation (the runs dfki052dede and dfki052ende). For the task DE2EN, we only submitted one run without web validation. The system performance for the three tasks was as follows: for the task DE2DE, Quantico needs approx. 3 sec. for one question-answering cycle (about 10 minutes for all 200 questions); for the task EN2DE, Quantico needs approx. 5 sec. (about 17 minutes for all 200 questions), basically due to the extra time, the online machine translation needs. The task DE2EN needs the most computation resources due to online translation, alignment, language model use, etc. (actually approx. 50 minutes are used for all 200 questions). As can be seen from the tables 1 and 2, applying the web validation component (for the best 3 answers determined by Quantico) does lead to a system performance loss. At the point of writing, we have not yet performed a detailed analysis, but it seems that the lack of contextual information causes the major problems, when computing the Google IR-query. Additional problems could be: • the number of German web documents might be still too low, for taking into account redundancy effectively</p><p>• the correct answer extracted from the Clef-corpus does not exist on the web but a "wrong" answer candidate; in that case, the wrong answer candidate would get a higher rank</p><p>• the Clef corpus consists of newspaper articles from 1994 and 1995; thus, the Clef corpus might actually be too old for being validated by the Web, especially if questions referring not to historical events, but to daily news</p><p>• in case of EN2DE, web validation is performed with the German query terms, which resulted from automatic machine translation; errors through the translation of complex and long questions had a negative effect on the recall of the web search However, a first comparison of the assessed results we obtained for the task DE2DE, showed that the web validation is not useless. Comparing the two runs dfki051dede and dfki052dede (cf. table 1), a total of 51 different assignments were observed (e.g., an answer correct in run dfki051dede, was wrong in run dfki052dede). Actually, 13 questions (of which 8 are definition questions), which where answered wrongly in dfki051dede, were now answered correctly in run dfki052dede. 28 questions, which were answered correctly in dfki051dede, were answered wrongly in dfki052dede. However, a closer look showed that about half of this errors, are due to the fact, that we actually performed web validation without taking into account the correct timeline. We assume that enhancing the Google IR-query with respect to Clef-corpus consistent timeline (1994/95) will improve the performance of our web validation strategy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,210.90,437.72,181.20,9.30;3,94.00,108.86,418.33,313.75"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of Quantico.</figDesc><graphic coords="3,94.00,108.86,418.33,313.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,154.89,437.72,293.21,9.30;5,94.00,108.86,418.33,313.75"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of Quantico: cross-lingual perspective</figDesc><graphic coords="5,94.00,108.86,418.33,313.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,153.15,589.10,58.69,8.74;5,136.82,603.04,127.05,8.74;5,136.82,616.99,129.99,8.74;5,136.82,630.94,133.01,8.74;5,136.82,644.89,147.08,8.74;5,136.82,658.83,277.38,8.74;5,136.82,672.78,93.95,8.74;5,136.82,686.73,122.51,8.74"><head></head><label></label><figDesc>[in:1] true 1.0 welchem: [which:0.5] true 0.5 Jahrzehnt: [decade:1] true 1.0 investierten: [invest:1] true 1.0 japanische: [japanese:0.5] true 0.5 Autohersteller: [car manufacturers:0.8, automakers:0.1] true 0.8 sehr: [very:1] true 1.0 stark: [strongly:0.5] true 0.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,160.72,611.51,281.57,53.20"><head>Table 1 :</head><label>1</label><figDesc>Results in the task German-German</figDesc><table coords="6,160.72,631.23,281.57,33.47"><row><cell>R</cell><cell>W X U</cell><cell>F</cell><cell>D</cell><cell>T</cell></row><row><cell cols="5">dfki051dede 87 43.50 100 13 -35.83 66.00 36.67</cell></row><row><cell cols="5">dfki052dede 54 27.00 127 19 -15.00 52.00 33.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,161.37,118.91,280.27,131.94"><head>Table 2 :</head><label>2</label><figDesc>Results in the task English-German</figDesc><table coords="7,161.37,140.57,280.27,110.28"><row><cell>R</cell><cell cols="2">W X U</cell><cell>F</cell><cell>D</cell><cell>T</cell></row><row><cell cols="6">dfki051ende 46 23.00 141 12 1 16.67 50.00 3.33</cell></row><row><cell cols="2">dfki052ende 31 15.50 159 8</cell><cell>2</cell><cell cols="3">8.33 42.00 0.00</cell></row><row><cell cols="5">Table 3: Results in the task German-English</cell></row><row><cell>R</cell><cell cols="2">W X U</cell><cell>F</cell><cell></cell><cell>T</cell></row><row><cell cols="2">dfki051deen 51 25.50 141 8</cell><cell cols="4">-18.18 50.00 13.79</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>The work presented in this paper has been funded by the <rs type="funder">BMBF</rs> project <rs type="projectName">Quetal</rs>, <rs type="grantNumber">FKZ 01 IW C02</rs>. Many thanks to <rs type="person">Rob Basten</rs> for his support in the development of the component for handling temporally restricted questions, <rs type="person">Yuan Ye</rs> for his support in data collection and annotation for the definition handlers, and <rs type="person">Aljeandro Figuero</rs> for his support in the implementation of the web validation strategy.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_gzMAN6A">
					<idno type="grant-number">FKZ 01 IW C02</idno>
					<orgName type="project" subtype="full">Quetal</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,140.90,702.33,372.11,8.74;7,140.89,714.28,372.11,8.74;7,140.89,726.24,344.27,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,140.89,714.28,271.35,8.74">Experiments and analysis of lcc&apos;s two qa systems over trec 2004</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,432.25,714.28,80.76,8.74;7,140.89,726.24,223.88,8.74">Proceedings of The Thirteenth Text Retrieval Conference (TREC 2004)</title>
		<meeting>The Thirteenth Text Retrieval Conference (TREC 2004)<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,140.89,112.02,372.12,8.74;8,140.89,123.98,144.61,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,277.64,112.02,162.40,8.74">A shallow text processing core engine</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Piskorski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,449.10,112.02,63.91,8.74;8,140.89,123.98,48.17,8.74">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="476" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,140.89,143.90,372.12,8.74;8,140.89,155.86,372.11,8.74;8,140.89,167.81,314.39,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,280.16,143.90,232.85,8.74;8,140.89,155.86,367.47,8.74">Experiments on robust nl question interpretation and multi-layered document annotation for a cross-language question/answering system</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sacaleanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,153.35,167.81,39.39,8.74">Clef 2004</title>
		<imprint>
			<publisher>Springer-Verlag LNCS</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="411" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,140.89,187.74,372.11,8.74;8,140.89,199.69,47.82,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,236.94,187.74,171.35,8.74">The Geometry of Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge University Pres</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
