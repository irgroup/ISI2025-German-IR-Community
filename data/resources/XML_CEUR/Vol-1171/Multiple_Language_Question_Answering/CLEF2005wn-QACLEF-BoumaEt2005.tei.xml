<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,96.46,148.91,410.08,15.11;1,506.54,146.25,5.98,11.15">Question Answering for Dutch using Dependency Relations *</title>
				<funder>
					<orgName type="full">Dutch Organisation for Scientific Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.74,181.39,66.30,10.48"><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Science Rijksuniversiteit Groningen</orgName>
								<address>
									<addrLine>Postbus 716</addrLine>
									<postCode>9700 AS</postCode>
									<settlement>Groningen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.53,181.39,42.35,10.48"><forename type="first">Jori</forename><surname>Mur</surname></persName>
							<email>mur@let.rug.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Information Science Rijksuniversiteit Groningen</orgName>
								<address>
									<addrLine>Postbus 716</addrLine>
									<postCode>9700 AS</postCode>
									<settlement>Groningen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.03,181.39,94.38,10.48"><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
							<email>vannoord@let.rug.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Information Science Rijksuniversiteit Groningen</orgName>
								<address>
									<addrLine>Postbus 716</addrLine>
									<postCode>9700 AS</postCode>
									<settlement>Groningen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,191.48,195.34,109.96,10.48"><forename type="first">Lonneke</forename><surname>Van Der Plas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Science Rijksuniversiteit Groningen</orgName>
								<address>
									<addrLine>Postbus 716</addrLine>
									<postCode>9700 AS</postCode>
									<settlement>Groningen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.12,195.34,83.40,10.48"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
							<email>tiedeman@let.rug.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Information Science Rijksuniversiteit Groningen</orgName>
								<address>
									<addrLine>Postbus 716</addrLine>
									<postCode>9700 AS</postCode>
									<settlement>Groningen</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,96.46,148.91,410.08,15.11;1,506.54,146.25,5.98,11.15">Question Answering for Dutch using Dependency Relations *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">66C23F185EA43FC0A79EF181646AB7C2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe Joost, our QA system for Dutch, which makes extensive use of dependency relations. We analyzed the full Dutch CLEF QA corpus syntactically and mined it off-line for information that may be useful for QA. Joost answers questions either by table look-up, or by searching for answers in paragraphs returned by an IR engine. In both cases, dependency relations are used to identify and rank potential answers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Joost is a monolingual QA system for Dutch which makes heavy use of syntactic information. Most questions are answered by retrieving relevant paragraphs from the document collection, using keywords from the question. Next, potential answers are identified and ranked using a number of clues. Apart from obvious clues (i.e. IR-score and the frequency with which the answer was found), we also use syntactic structure to identify and rank answer strings. A second strategy is based upon the observation that certain question types can be anticipated, and the corpus can be searched off-line for answers to such questions. Whereas previous approaches have used regular expressions to extract the relevant relations, we use patterns of dependency relations. To this end, the whole corpus has been analyzed syntactically.</p><p>In the next section, we describe the building blocks of our QA system, i.e. the Alpino dependency parser, utilities for reasoning with dependency relations, relation tables which are extracted off-line, and isa-relations between named entities and concepts. In section 3, we describe Joost. Questions are parsed and classified using Alpino. Depending on the question class, questions are answered by means of table look-up or an IR-based method. General which-questions and definition questions are answered using the automatically acquired isa-relations as an additional resource. In section 4, we discuss the results of Joost on the CLEF 2005 QA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Syntactic Preprocessing</head><p>We have used the Alpino-system to parse the full text collection for the Dutch CLEF QA-task. The resulting dependency parse trees are stored as XML, and can be processed and searched in various ways, for instance, using XPath and XSLT.</p><p>The Alpino-system is a linguistically motivated, wide-coverage, grammar and parser for Dutch. The constraint-based grammar follows the tradition of HPSG <ref type="bibr" coords="1,355.38,715.28,99.62,8.74" target="#b12">(Pollard and Sag, 1994)</ref>. It currently consists of over 500 grammar rules (defined using inheritance) and a large and detailed lexicon (over 100.000 lexemes). To ensure coverage, heuristics have been implemented to deal with unknown words and ungrammatical or out-of-coverage sentences (which may nevertheless contain fragments that are analyzable). The grammar provides a 'deep' level of syntactic analysis, in which whmovement, raising and control, and the Dutch verb cluster (which may give rise to 'crossing dependencies') are given a principled treatment. The output of the system is a dependency graph, compatible with the annotation guidelines of the Corpus of Spoken Dutch.</p><p>A left-corner chart parser is used to create the parse forest for a given input string. A manually corrected treebank of 140.000 words was used to train a maximum entropy disambiguation model. Beam-search is used as a heuristic to extract the most probable parse from the parse forest effeciently. <ref type="bibr" coords="2,125.47,231.57,133.04,8.74" target="#b8">(Malouf and van Noord, 2004)</ref> show that the accuracy of the system, when evaluated on a test-set of 500 newspaper sentences, is over 88%, which is in line with state-of-the-art systems for English.</p><p>A second extension of the system for QA, was the inclusion of a Named Entity Classifier. The Alpino system already includes heuristics for recognizing proper names. Thus, the classifier needs to classify strings which have been assigned a name part of speech by grammatical analysis, as being of the subtype per, org, geo or misc.<ref type="foot" coords="2,294.96,301.73,3.97,6.12" target="#foot_0">1</ref> To this end, we collected lists of person names (120K), geographical names (12K), organization names (26k), and miscalleneous items (2K). The data are primarily extracted from the Twente News Corpus, a collection of over 300 million words of newspaper text, which comes with annotation for the names of people, organizations, and locations, involved in a particular news story. For unknown names, a maximum entropy classifier was trained, using the Dutch part of the shared task for conll 2003. <ref type="foot" coords="2,396.94,361.50,3.97,6.12" target="#foot_1">2</ref> The accuracy on unseen conll data of the resulting classifier (which combines dictionary look-up and a maximum entropy classifier) is 88.2%.</p><p>To this end, the text collection was tokenized (into 78 million words) and segmented into (4.1 million) sentences. Parsing this amount of text takes well over 500 CPU days. We used a Beowulf Linux cluster of 128 Pentium 4 processors<ref type="foot" coords="2,276.73,421.28,3.97,6.12" target="#foot_2">3</ref> to complete the process in about three weeks. The dependency trees are stored as (25 Gb of) XML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reasoning over Dependency Relations</head><p>Several researchers have attempted to use syntactic information, and especially dependency relations, in QA. One approach is to look for an exact match between dependency tuples derived from the question and those present in a potential answer <ref type="bibr" coords="2,344.72,505.00,90.70,8.74" target="#b5">(Katz and Lin, 2003;</ref><ref type="bibr" coords="2,438.70,505.00,69.87,8.74" target="#b7">Litkowski, 2004)</ref>. <ref type="bibr" coords="2,90.00,516.95,92.27,8.74" target="#b0">Attardi et al. (2002)</ref> and <ref type="bibr" coords="2,205.99,516.95,118.73,8.74" target="#b9">Mollá and Gardiner (2005)</ref> compute the match between question and answer using a metric which basically computes the overlap in dependency relations between the two. <ref type="bibr" coords="2,112.91,540.86,153.38,8.74" target="#b13">Punyakanok, Roth, and Yih (2004)</ref> compute the tree edit distance between the dependency trees of the question and answer, and select answers from sentences which minimize this distance.</p><p>We have implemented a system in which dependency patterns derived from the question must be matched by equivalent dependency relations in a potential answer. The dependency analysis of a sentence gives rise to a set of dependency relations of the form Head/HIx, Rel, Dep/DIx , where Head is the root form of the head of the relation, and Dep is the head of the constituent that is the dependent. Hix and DIx are string indices, which distinguish repeated occurrences of the same token in a string, and Rel is the name of the dependency relation. For instance, the dependency analysis of sentence (1-a) is (1-b).</p><p>(1) a. Mengistu kreeg asiel in Zimbabwe (Mengistu was given asylum in Zimbabwe)</p><p>b. krijg/2, su, mengistu/1 , krijg/2, obj1, asiel/3 , krijg/2, mod, in/4 , in/4, obj1, zimbabwe/5</p><p>A dependency pattern is a set of (partially underspecified) dependency relations:</p><p>(2) krijg/K, obj1, asiel/A , krijg/K, su, Su/S</p><p>A pattern may contain variables, represented here by (words starting with) a capital. A pattern P matches a set of dependency relations R if P ⊂ R, under some substitution of variables. The pattern in (2) matches with the set in (1-b), instantiating Su as mengistu.</p><p>Equivalences can be defined to account for the fact that in some cases we want a pattern to match a set of dependency relations that slightly differs from it, but nevertheless expresses the same semantic relation. For instance, the subject of an active sentence may be expressed as a PP-modifier headed by door (by) in the passive:</p><p>(3) a. Zimbabwe verleende asiel aan Mengistu (Zimbabwe gave asylum to Mengistu) b. Aan Mengistu werd asiel verleend door Zimbabwe (Mengistu was given asylum by Zimbabwe)</p><p>The following equivalence accounts for this:</p><p>{ Vb/V,su,Su/S } ⇔ word/W,vc,Vb/V , Vb/V,mod,door/D , door/D,obj1,Su/S</p><p>Here, the verb word is (the root form of) the passive auxiliary, which takes a verbal complement headed by the verb Vb.</p><p>Given an equivalence Lhs ⇔ Rhs, a pattern P containing Lhs is equivalent to a pattern P , which is identical to P , except that Lhs has been replaced by Rhs. A pattern P now also matches with a set of relations R if there is some equivalent pattern P , and P is a subset of R, under some substitution of variables.</p><p>We have implemented 13 additional equivalence rules, to account for, among others, word order variation within appostions, the equivalence of genitives and van-PPs, equivalence between appositions and simple predicative sentence, coordination, and relative clauses. The equivalence rules we have implemented so far express linguistic equivalences, and thus are both general and domain independent. In <ref type="bibr" coords="3,205.18,432.52,163.75,8.74" target="#b1">Bouma, Mur, and van Noord (2005)</ref>, we show that the inclusion of equivalence rules has a positive effect on various components of our QA system. In the future, we hope to extend this with equivalences which are restricted to specific relations (i.e. such as the equivalence between X writes Y and X is the author of Y), using techniques for acquiring such equivalences automatically from parsed corpora as in <ref type="bibr" coords="3,324.68,480.34,94.45,8.74" target="#b6">Lin and Pantel (2001)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Off-line Retrieval</head><p>Off-line methods have proven to be very effective in QA <ref type="bibr" coords="3,336.46,526.62,172.11,8.74" target="#b3">(Fleischman, Hovy, and Echihabi, 2003)</ref>. Before actual questions are known, a corpus is exhaustively searched for potential answers to specific question types (capital, abbreviation, inhabitants, year of birth, ...). The answers are extracted from the corpus off-line and stored in a structured table for quick and easy access.</p><p>Jijkoun, Mur, and de Rijke ( <ref type="formula" coords="3,226.35,586.39,18.45,8.74">2004</ref>) show that extraction patterns defined in terms of dependency relations are more effective than regular expression patterns over surface strings. Following this observation, we used the module for dependency pattern matching to exhaustively search the parsed corpus for potential answers to frequently occurring question types. For instance, the pattern in (4) extracts information about organizations and their founders.</p><p>(4) richt op/R, su, Founder/S , richt op/R, obj1, Founded/O</p><p>The verb oprichten (to found) can take on a wide variety of forms (active, with the particle op split from the root, participle, and infinitival, either the founder or the organization can be the first constituent in the sentence, in passives the founder may be part of a door (by) phrase, and in control constructions the founder may be found as the subject of a governing clause. In all cases, modifiers may intervene between the relevant constituents:</p><p>(5) a. op te richten. (when the General Bank announced to found a "postal bank" with the Belgian Mail).</p><p>Such variation is almost impossible to capture accurately using regular expressions, whereas dependency relations can exploit the fact that in allmost all cases the organization and its founder can be identified as the object and subject of the verb with the root form oprichten. The pattern in (4) suffices to extract this relation from all of the examples above.</p><p>Equivalence rules can be used to deal with other forms of syntactic variation. For instance, once we define a pattern to extract the country and its capital from (6-a), the equivalence rules can be used to match this pattern against the alternative formulations in (6-b)-(6-d) as well. Of course, the same holds for all other relations that are extracted off-line, and thus, the development effort per relation decreases, while recall typically increases.</p><p>Table <ref type="table" coords="4,133.48,480.54,4.98,8.74" target="#tab_0">1</ref> lists all the relations we extracted. Each second and third column list the overall number of extracted tuples and extracted unique tuples (types) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Extracting ISA relations</head><p>Fine-grained named entity classification is useful for answering wh-questions and definition questions. Both <ref type="bibr" coords="4,146.55,550.73,56.70,8.74" target="#b11">Pasça (2004)</ref> and <ref type="bibr" coords="4,227.69,550.73,144.35,8.74" target="#b10">Pantel and Ravichandran (2004)</ref> describe methods for acquiring labels for named entities from large text corpora and evaluate the results in the context of web search and question answering. <ref type="bibr" coords="4,233.83,574.64,143.92,8.74" target="#b10">Pantel and Ravichandran (2004)</ref> use the apposition relation to find potential labels for named entities. The apposition relation is the relation that holds between Delors and president of the European Commission in sentences like Delors, president of the European Commission, arrived yesterday.</p><p>From the fully parsed Dutch CLEF text collection, we extracted 295 unique apposition tuples, consisting of a noun (used as class label) and a named entity. The resulting table contains, for instance, 112 names of ferry boats (Estonia, Anna Maria Lauro, Sally Star etc.) and no less than 2951 national team coaches (Bobby Robson, Jack Charlton, Menotti, Berti Vogts etc.). By focussing on the most frequent label for a named entity, most of the noise can be discarded. For instance, Guus Hiddink occurs 17 times in the extracted apposition tuples, 5 times as bondscoach (national team chef), and once with various other labels (boss, colleague, guest, newcomer, ...). In van der Plas and <ref type="bibr" coords="4,181.44,706.15,60.69,8.74">Bouma (2005)</ref>, we show that automatically acquired class labels for named entities improve the performance of our QA system on which questions and definition questions.</p><p>In this section, we describe the components of our QA system, Joost. Questions are analyzed and assigned a question class. If the class corresponds to a relation for which information has been extracted off-line, answers and corresponding document id's are retrieved from the relevant relation table. Otherwise, Information Retrieval is used to find paragraphs relevant to the question. Linguistic techniques are used to extract potential answers from these paragraphs. Potential answers are ranked on the basis of a score which combines, among others, IR-score, frequency of the answer, and the amount of overlap in dependency relations between question and the sentence from which the answer was extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Analysis</head><p>Question analysis is the task of assigning a specific class (person, location, date, ...) to a question. Syntactic analysis helps to determine the question stem in complex wh-phrases (With which Palestinian organization...) and can help to identify additional properties of the question (i.e. Give the name of a Japanese city that was struck by an earthquake asks for the name of a city, not of an earthquake). Lexical semantic knowledge is required to recognize that Which region in the US has ... asks for a geographical named entity, whereas Which car factory was bought by ... asks for an organizational named entity.</p><p>Each incoming question is parsed by Alpino. To improve parsing accuracy on this specific task, the disambiguation model was retrained on a corpus which contained annotated and manually corrected dependency trees for 650 quiz questions. <ref type="foot" coords="5,309.38,369.83,3.97,6.12" target="#foot_3">4</ref> The retrained model achieves an accuracy of 92.7% and 88.3% on the CLEF 2003 and 2004 questions, respectively. For CLEF 2005, we used a model which was trained on data which also included (manually corrected dependency trees of) the CLEF 2003 and 2004 questions. It achieved an accuracy of 97.6 on CLEF 2005 questions.</p><p>On the basis of the dependency relations returned by the parser the question class is determined. Joost distinguishes between 29 different question classes. 18 question classes are related to the relation tuples that were extracted off-line. Note that a single relation can often be questioned in different ways. For instance, whereas a frequent question type asks for the meaning of an acronym (What does the abbreviation RSI stand for?), a less frequent type asks for the abbreviation of a given term (What is the abbreviation of Mad Cow Disease?). The other 11 question classes identify questions asking for an amount, the date or location of an event, the (first) name of a person, the name of an organization, how-questions, wh-questions, and definition questions.</p><p>For each question class, one or more syntactic patterns are defined. For instance, the following pattern accounts for questions asking for the capital of a country: <ref type="bibr" coords="5,90.00,552.09,12.73,8.74">(7)</ref> wat/W, wh, is/I , is/I, su, hoofdstad/H hoofdstad/H, mod, van/V , van/V, obj1, Country/C Depending on the question class, it is useful to identify one or two additional arguments . For instance, the dependency relations assigned to the question Wat is de hoofdstad van Togo? (What is the capital of Togo?) match with the pattern in (7), and instantiate Country as Togo. Therefore, the question class capital is assigned, with Togo as additional argument. Similarly, Who is the king of Norway? is classified as function(king,Norway), and In which year did the Islamic revolution in Iran start? is classified as date(revolution).</p><p>Some question classes require access to lexical semantic knowledge. For instance, to determine that In which American state is Iron Mountain? asks for a location, the systeem needs to know that state refers to a location, and to determine that Who is the advisor of Yasser Arafat? should be classifed as function(advisor,Yasser Arafat), it needs to know that advisor is a function. We obtained such knowledge mainly from Dutch EuroWordNet <ref type="bibr" coords="5,355.25,696.91,62.09,8.74" target="#b16">(Vossen, 1998)</ref>. The list of function words (indicating function roles such as president, queen, captain, secretary-general, etc.) was expanded semi-automatically with words from the corpus that were distributionally similar to those extracted from EWN (see van der Plas and Bouma (2005) for details).</p><p>Question classification was very accurate for the CLEF 2005 questions. There were a few cases where the additional arguments selected by the system did not seem the most optimal choice. Two clear mistakes were found (e.g. What is the currency of Peru? was classified as currency(of) and not as currency(Peru)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Information Retrieval</head><p>For questions which cannot be answered by the relation tables, traditional keyword-based information retrieval (IR) is used to narrow down the search space for the linguistically informed part of the QA system which identifies answers. On the basis of keywords from the question, the IR system retrieves relevant passages from the corpus.</p><p>Keywords are derived from the question using its content words. Function words and other irrelevant words are removed using a static stop word list. We implemented an interface to seven publicly available IR engines <ref type="bibr" coords="6,219.28,289.80,81.19,8.74" target="#b14">(Tiedemann, 2004)</ref>. We selected Zettair <ref type="bibr" coords="6,398.69,289.80,79.55,8.74" target="#b17">(Zobel et al., 2004</ref>) as the underlying system in our experiments because of speed and recall performance. The entire CLEF QA corpus (in its tokenized plain text version) has been indexed using the IR engine with its standard setup.</p><p>Earlier experiments have shown that a segmentation into paragraphs is most efficient for IR performance in QA. We used the existing markup in the corpus to determine the paragraph boundaries. This resulted in about 1.1 million paragraphs (including headers that have been marked as paragraphs). We did experiments with additional pre-processing, e.g., including proper lemmatization (using Alpino root forms) but we could not improve the IR performance compared to the baseline using standard settings. However, we did include labels of named entities found by Alpino in each paragraph as additional tokens. This makes it possible to search for paragraphs including certain types of named entities (e.g. location names and organizations) and special units (e.g. measure names and temporal expressions) corresponding to question types found by the question analyses component.</p><p>For CLEF, Zettair returns the 40 most relevant paragraphs given a query. For the QA@CLEF 2003 data, this gives a recall of 75%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer Identification and Ranking</head><p>For questions that are answered by means of table look-up, the relation table provides an exact answer string. For other questions, it is necessary to extract answer strings from the set of paragraphs returned by IR. Given a set of paragraph id's, we retrieve from the parsed corpus the dependency relations for the sentences occurring in these paragraphs.</p><p>Various syntactic patterns are defined for (exact) answer identification. For questions asking for the name of a person, organization, or location, or for an amount or date, a constituent headed by a word with the appropriate named entity class has to be found. As all of these occur frequently in the corpus, usually many potential answers will be identified. An important task is therefore to rank potential answers.</p><p>The following features are used to determine the score of a short answer A extracted from sentence S:</p><p>• Syntactic Similarity The proportion of dependency relations from the question which match with dependency relations in S.</p><p>• Answer Context A score for the syntactic context of A.</p><p>• Names The proportion of proper names, nouns, and adjectives from the query which can be found in S and the sentence preceding S.</p><p>• Frequency The frequency of A in all paragraphs returned by IR.</p><p>• IR The score assigned to the paragraph from which A was extracted.</p><p>The score for syntactic similarity implements a preference for answers from sentences with a syntactic structure that overlaps with that of the question. Answer context implements a preference for answers that occur in the context of certain terms from the question. Given a question classified as date(Event), for instance, date expressions which occur as a modifier of Event are preferred over date expressions occurring as sisters of Event, which in turn are preferred over dates which have no syntactic relation to Event.</p><p>The overall score for an answer is the weighted sum of these features. Weights were determined manually using previous CLEF data for tuning. The highest weights are used for Syntactic Similarity and Answer Context. The highest scoring answer is returned as the answer.</p><p>Ranking of answers on the basis of various features was initially developed for IR-based QA only. Answers found by table look-up were ranked only by frequency. Recently, we have started to use the scoring mechanism described above also for answers stemming from table look-up. As the tables contain pointers to the sentence from which a tuple was extracted, we can easily go back to the full sentence, and apply the scoring mechanisms described above.<ref type="foot" coords="7,404.50,287.78,3.97,6.12" target="#foot_4">5</ref> Using more features to rank an answer provides a way to give the correct answer to questions like Who is the German minister of Economy?. The function table contains several names for German ministers, but does not distinguish between different departments. The most frequent candidate is Klaus Kinkel (54 entries), who is minister of foreign affairs. The correct name, Günter Rexrodt, occurs only 11 times. Using Syntactic Similarity and Names as an additional features, Joost manages to give the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Special Cases</head><p>Temporally Restricted Questions. The CLEF 2005 test set contained a number of questions which were temporally restricted:</p><p>(8) a. Which vulcano erupted in June 1991? b. Who was the mayor of Moscow in 1994?</p><p>The temporal information in these questions was treated similarly to all other information in the question, and we did not try to implement techniques which deal specifically with temporal restrictions. The mechanism for scoring potential answers takes into account the syntactic similarity and the overlap in names (including date expressions) between question and answer sentence, and this implements a preference for answers which are extracted from contexts referring to the correct date. Note that, as the same scoring technique is used for answers found by table look-up, this strategy should also be able to find the correct answer for questions such as (8-b), for which the function table might contain more than one answer. Which-questions. General wh-questions, such as ( <ref type="formula" coords="7,347.70,566.60,3.88,8.74">9</ref>), are relatively difficult to answer. Whereas for most question types, the type of the answer is relatively clear (i.e. it should the name of a person or organization, or a date, etc.), this is not the case for wh-questions. To improve the performance of our system on such questions, we make use of two additional knowledge sources. From EuroWordNet, we imported all hypernym relations between nouns. Question (9-a) is assigned the question class which(fruit). We use the hypernym relations to assign a higher score to answers which are hypernyms of fruit.<ref type="foot" coords="7,368.46,676.45,3.97,6.12" target="#foot_5">6</ref> </p><p>As EuroWordNet does hardly include proper names, we also used the isa-relations extracted from appositions containing a named entity, as described in section 2.4.  <ref type="bibr" coords="8,468.64,280.93,4.65,8.74">)</ref>. No less than 60 questions were of this type. Again, we used the isa-relations extracted from appositions to answer such questions. More in particular, our strategy for answering definition questions consisted of two phases:</p><p>• Phase 1: The most frequent class found for a named entity is selected.</p><p>• Phase 2: The sentences which mention the named entity and the class are retrieved and searched for additional information which might be relevant. Snippets of information that are in a adjectival relation or which are a prepositional complement to the class label are selected.</p><p>Frequency is important to ensure that an appropriate class is chosen. The named entity Sabena, for instance, occurs frequently in the corpus, but often with class labels assigned to it, which are not suitable for inclusion in a definition (possibility, partner, company,,.... By focussing on the most frequent class label assigned to a named entity (airline company in this case), we hope to select the most appropriate label for a definition. A disadvantage of this technique is that the class label by itself is not always sufficient for an adequate definition. Therefore, we expand the class labels with modifiers which typically need to be included in a definition. For the question What is Sabena?, our system produces Belgian airline company as answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>The results of the CLEF evaluation are given in table 2. The scores are satisfactory for factoid questions and definitions. It is unclear to us at the moment what the explanation is for the fact that the system performed less well on temporally restricted questions.</p><p>Of the 140 factoid questions, 46 questions were assigned a type corresponding to a relation table . For 35 of these questions, an answer was actually found in one of the tables. The other 11 questions were answered by using the IR-based strategy as fall-back. 52 of the 60 definition questions were answered by the strategy described in section 3.4. For the other definition questions, the general IR-based strategy was used as fall-back. Three definition questions received nil as an answer.</p><p>Parsing errors are the cause of some wrong or incomplete answers. The question Who is Javier Solana?, for instance, is answered with Foreign Affairs, which is extracted from a sentence containing the phrase Oud-minister van buitenlandse zaken Javier Solana (Ex-minister of foreign affairs, Javier Solana). Here, Javier Solana was erroneously analyzed as an apposition of affairs. Similarly, the wrong answer United Nations for the question What is UNEP?, which was extracted from a sentence containing the environment programme of the United Nations (UNEP), which contained the same attachment mistake.</p><p>A frequent cause of errors were answers that were echoing (part of) the question. Currently, the system only filters answers which are a literal substring of the question. This strategy fails in cases like:</p><p>(10) a. Q: Where is Bonn located? A: in Bonn. b. Q: In which city does one find the famous Piazza dei Miracoli? A: at the Piazza dei Miracoli c. Q: In which American state is Iron Mountain located? A: The United States.</p><p>It seems cases like (10-a) and (10-b) could be easily filtered as well. Cases like (10-c) are harder, as they involve two (near) synonyms. Note finally that not all answers which overlap with the question should be filtered, as the answer in ( <ref type="formula" coords="9,289.27,235.40,8.86,8.74">11</ref>) is valid, eventhough the word rocket also occurs in the question.</p><p>(11) Q: What is the name of the rocket used to launch the satellite Clementine? A: Titan rocket</p><p>Our strategy for answering definition questions seemed to work reasonably well, although it did produce a relatively large number of inexact answers (of the 18 answers that were judged inexact, 13 were answers to definition questions). As we explained in section 3.4, this is due to the fact that we select the most frequent class label for a named entity, and only expand this label with adjectival and pp modifiers that are adjacent to the class label (a noun) in the corresponding sentence. Given the constituent the museum Hermitage in St Petersburg, this strategy fails to include in St Petersburg, for instance. We did not include relative clause modifiers, as these tend to contain information which is not appropriate for a definition. However, for the question, Who is Iqbal Masih, this leads the system to answer twelve year old boy, extracted from the constituent twelve year old boy, who fought against child labour and was shot sunday in his home town Muritke.</p><p>Here, at least the first conjunct of the relative clause should have been included. Similarly, we did not include purpose clauses, which leads the system to respond large scale American attempt to the question what was the Manhattan project, instead of large scale American attempt to develop the first (that is, before the Germans) atomic bomb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have shown that dependency parsing of both questions and the full document collection is useful for developing an adequate QA system. Dependency patterns can be used to search the corpus exhaustively for answers to frequent question types and for class labels for named entities, which are used to improve the performance of the system on which-questions and definition questions. Selection of the most likely answer to a question uses a syntactic similarity metric based on dependency relations.</p><p>We have used a limited number of equivalences over dependency relations. An obvious next step is to expand this set with equivalences derived automatically from the parsed corpus (i.e. as in <ref type="bibr" coords="9,101.70,604.82,94.31,8.74" target="#b6">Lin and Pantel (2001)</ref>). The syntactic techniques we employ operate exclusively on individual sentences. In the future, we hope to extend this to techniques which operate on the paragraph level by integrating, among others, a component for coreference resolution. Finally, we want to explore the possibility of using dependency relations to boost the performance of the IR-engine, i.e. as in <ref type="bibr" coords="9,131.01,652.64,71.49,8.74" target="#b2">Cui et al. (2005)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,90.00,400.92,12.73,8.74;4,120.67,400.92,338.00,8.74;4,120.66,412.87,338.00,8.74;4,120.66,424.83,287.95,8.74;4,120.66,436.79,352.89,8.74"><head></head><label></label><figDesc>hoofdstad van Afghanistan, Kabul (the capital of Afghanistan, Kabul) b. Kabul, de hoofdstad van Afghanistan (Kabul, the capital of Afghanistan) c. Afghanistans hoofdstad, Kabul (Afghanistan's capital, Kabul) d. Kabul is de hoofdstad van Afghanistan (Kabul is the capital of Afghanistan)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,90.00,610.36,12.73,8.74;7,120.67,610.36,161.69,8.74;7,120.66,622.32,221.11,8.74"><head></head><label></label><figDesc>fruit contains vitamin C? b. Which ferry sank southeast of the island Utö?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,140.59,741.54,336.00,8.77"><head>Table 1 :</head><label>1</label><figDesc>Size of extracted relation tables. b. Kasparov heeft een nieuwe Russische Schaakbond opgericht en... (Kasparov has founded a new Russian Chess Union and...) c. ... toen de Generale Bank bekend maakte met de Belgische Post een "postbank"</figDesc><table coords="4,101.96,119.79,405.41,68.91"><row><cell>Relation</cell><cell>tuples</cell><cell>uniq</cell><cell>Relation</cell><cell cols="2">tuples uniq</cell><cell>Relation</cell><cell>tuples</cell><cell>uniq</cell></row><row><cell cols="3">Abbreviation 21.497 8.543</cell><cell>Currency</cell><cell>6.619</cell><cell>222</cell><cell>Function</cell><cell cols="2">77.028 46.589</cell></row><row><cell>Age</cell><cell cols="2">22.143 18520</cell><cell>Died Age</cell><cell>1.127</cell><cell>834</cell><cell>Inhabitants</cell><cell>708</cell><cell>633</cell></row><row><cell>Born Date</cell><cell cols="2">2356 1.990</cell><cell>Died Date</cell><cell>583</cell><cell>544</cell><cell>Nobel Prize</cell><cell>169</cell><cell>141</cell></row><row><cell>Born Loc</cell><cell>937</cell><cell>879</cell><cell>Died Loc</cell><cell>664</cell><cell>583</cell><cell></cell><cell></cell></row><row><cell>Capital</cell><cell>2.146</cell><cell>515</cell><cell>Founded</cell><cell>1.021</cell><cell>953</cell><cell></cell><cell></cell></row></table><note coords="3,140.59,741.54,336.00,8.77"><p>Minderop richtte de Tros op toen .... (Minderop founded the Tros when...)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,401.35,701.94,111.65,8.74"><head>Table 2 :</head><label>2</label><figDesc>CLEF scores the question class which(ferry). Candidate answers that are selected by Joost are: Tallinn, Estonia, Raimo Tiilikainen etc. Since, according to our apposition database, Estonia is the only potential answer which isa ferry, this answer is selected.Definition Questions. An important category in CLEF 2005 are questions asking for the definition of a person or organization (i.e. What is Sabena?, Who is Antonio Matarese?</figDesc><table coords="7,401.35,701.94,111.65,8.74"><row><cell>Question (9-b) is assigned</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,717.10,407.75,6.99;2,90.00,726.56,282.58,6.99"><p>Various other entities which sometimes are dealt with by NEC, such as dates and measure phrases, can be identified using the information present in POS tags and dependency labels.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,736.64,152.35,6.64"><p>http://cnts.uia.ac.be/conll2003/ner/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,105.24,745.57,327.49,6.99"><p>which is part of the High-Performance Computing centre of the University of Groningen</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,105.24,725.85,407.76,6.99;5,90.00,735.32,50.39,6.99"><p>From the Winkler Prins spel, a quiz game. The material was made available to us by the publisher, Het Spectrum, bv.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="7,105.24,718.93,267.39,6.99"><p>As no IR is involved in this case, the IR score is set to 1 for all answers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="7,105.24,728.43,407.75,6.99;7,90.00,737.90,101.19,6.99"><p>Unfortunately, EuroWordNet only contains two hypernyms for the synset fruit, none of which could be used to identify an answer to (9-a).</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>* This research was carried out as part of the research program for Interactive Multimedia Information Extraction, imix, financed by nwo, the <rs type="funder">Dutch Organisation for Scientific Research</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,90.00,707.41,422.99,8.74;9,104.94,719.36,408.05,8.74;9,104.94,731.32,205.29,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,132.96,719.36,176.88,8.74">Piqasso: Pisa question answering system</title>
		<author>
			<persName coords=""><forename type="first">Giuseppe</forename><surname>Attardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Cisternino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francesco</forename><surname>Formica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Simi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alessandro</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,332.56,719.36,180.43,8.74;9,104.94,731.32,48.06,8.74">Text REtrieval Conference (TREC) 2001 Proceedings</title>
		<meeting><address><addrLine>Gaithersburg, ML</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="633" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,112.02,423.00,8.74;10,104.94,123.97,408.06,8.74;10,104.94,135.93,197.07,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,350.06,112.02,162.94,8.74;10,104.94,123.97,27.93,8.74">Reasoning over dependency relations for QA</title>
		<author>
			<persName coords=""><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jori</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,158.44,123.97,354.56,8.74;10,104.94,135.93,81.16,8.74">Proceedings of the IJCAI workshop on Knowledge and Reasoning for Answering Questions (KRAQ)</title>
		<meeting>the IJCAI workshop on Knowledge and Reasoning for Answering Questions (KRAQ)<address><addrLine>Edinburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,154.78,423.01,8.74;10,104.94,166.74,397.45,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,427.43,154.78,85.58,8.74;10,104.94,166.74,191.42,8.74">Question answering passage retrieval using dependency relations</title>
		<author>
			<persName coords=""><forename type="first">Hang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renxu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,317.22,166.74,105.39,8.74">Proceedings of SIGIR 05</title>
		<meeting>SIGIR 05<address><addrLine>Salvador, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,185.59,423.01,8.74;10,104.94,197.54,408.06,8.74;10,104.94,209.50,337.16,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,396.23,185.59,116.78,8.74;10,104.94,197.54,271.49,8.74">Offline strategies for online question answering: Answering questions before they are asked</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Fleischman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abdessamad</forename><surname>Echihabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,397.36,197.54,115.64,8.74;10,104.94,209.50,210.74,8.74">Proc. 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,228.35,423.01,8.74;10,104.94,240.30,408.06,8.74;10,104.94,252.26,34.66,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,356.67,228.35,156.34,8.74;10,104.94,240.30,245.87,8.74">Information extraction for question answering: Improving recall through syntactic patterns</title>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jori</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,376.56,240.30,51.14,8.74">Coling 2004</title>
		<meeting><address><addrLine>Geneva</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1284" to="1290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,271.11,423.01,8.74;10,104.94,283.07,408.06,8.74;10,104.94,295.02,237.31,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,252.23,271.11,260.78,8.74;10,104.94,283.07,41.18,8.74">Selectively using relations to improve precision in question answering</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,168.44,283.07,344.56,8.74;10,104.94,295.02,93.88,8.74">Proceedings of the workshop on Natural Language Processing for Question Answering (EACL 2003)</title>
		<meeting>the workshop on Natural Language Processing for Question Answering (EACL 2003)<address><addrLine>Budapest</addrLine></address></meeting>
		<imprint>
			<publisher>EACL</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,313.87,423.00,8.74;10,104.94,325.83,147.36,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,255.63,313.87,216.05,8.74">Discovery of inference rules for question answering</title>
		<author>
			<persName coords=""><forename type="first">Dekan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,480.52,313.87,32.49,8.74;10,104.94,325.83,94.11,8.74">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="343" to="360" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,344.68,423.01,8.74;10,104.94,356.63,408.05,8.74;10,104.94,368.59,217.38,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,219.10,344.68,248.68,8.74">Use of metadata for question answering and novelty tasks</title>
		<author>
			<persName coords=""><forename type="first">Kenneth</forename><forename type="middle">C</forename><surname>Litkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,277.21,356.63,235.78,8.74;10,104.94,368.59,58.60,8.74">Proceedings of the eleventh Text Retrieval Conference (TREC 2003)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>the eleventh Text Retrieval Conference (TREC 2003)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,387.44,423.01,8.74;10,104.94,399.39,408.06,8.74;10,104.94,411.35,177.75,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,301.72,387.44,211.29,8.74;10,104.94,399.39,66.47,8.74">Wide coverage parsing with stochastic attribute value grammars</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Malouf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,193.13,399.39,319.87,8.74;10,104.94,411.35,136.08,8.74">IJCNLP-04 Workshop Beyond Shallow Analyses -Formalisms and statistical modeling for deep analyses</title>
		<meeting><address><addrLine>Hainan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,430.20,423.00,8.74;10,104.94,442.16,408.06,8.74;10,104.94,454.11,59.94,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,258.51,430.20,254.49,8.74;10,104.94,442.16,150.79,8.74">Answerfinder -question answering by combining lexical, syntactic and semantic information</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,276.58,442.16,236.43,8.74">Australasian Language Technology Workshop (ALTW)</title>
		<meeting><address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2005. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,472.96,423.00,8.74;10,104.94,484.92,408.06,8.74;10,104.94,496.87,408.07,8.74;10,104.94,508.83,49.52,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,318.33,472.96,174.23,8.74">Automatically labeling semantic classes</title>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,346.04,484.92,162.59,8.74">HLT-NAACL 2004: Main Proceedings</title>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-05-02">2004. May 2 -May 7</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,527.68,422.99,8.74;10,104.94,539.63,392.67,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,166.48,527.68,245.77,8.74">Acquisition of categorized named entities for web search</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasça</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,434.53,527.68,78.46,8.74;10,104.94,539.63,312.67,8.74">Proceedings of the Thirteenth ACM conference on Information and knowledge management</title>
		<meeting>the Thirteenth ACM conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="137" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,558.49,423.01,8.74;10,104.94,570.44,170.69,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,240.95,558.49,171.51,8.74">Head-driven Phrase Structure Grammar</title>
		<author>
			<persName coords=""><forename type="first">Carl</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Sag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,422.48,558.49,90.53,8.74;10,104.94,570.44,166.22,8.74">Center for the Study of Language and Information Stanford</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,589.29,422.99,8.74;10,104.94,601.25,408.05,8.74;10,104.94,613.20,160.95,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,289.57,589.29,223.42,8.74;10,104.94,601.25,60.69,8.74">Mapping dependency trees: An application to question answering</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,185.74,601.25,327.25,8.74;10,104.94,613.20,61.72,8.74">The 8th International Symposium on Artificial Intelligence and Mathematics (AI&amp;Math 04)</title>
		<meeting><address><addrLine>Fort Lauderdale, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,632.05,423.00,8.74;10,104.94,644.01,233.47,8.74" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,199.65,632.05,276.09,8.74">A comparison of off-the-shelf IR engines for question answering</title>
		<author>
			<persName coords=""><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Leiden, The Netherlands</pubPlace>
		</imprint>
	</monogr>
	<note>Poster presentation at CLIN 2004</note>
</biblStruct>

<biblStruct coords="10,90.00,662.86,423.00,8.74;10,104.94,674.81,408.06,8.74;10,104.94,686.77,162.96,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,303.63,662.86,209.37,8.74;10,104.94,674.81,115.45,8.74">Automatic acquisition of lexico-semantic knowledge for question answering</title>
		<author>
			<persName coords=""><forename type="first">Lonneke</forename><surname>Van Der Plas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,240.32,674.81,268.17,8.74">Proceedings of Ontolex 2005 -Ontologies and Lexical Resources</title>
		<meeting>Ontolex 2005 -Ontologies and Lexical Resources<address><addrLine>Jeju Island, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="10,90.00,705.62,376.71,8.74" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,165.78,705.62,296.34,8.74">Eurowordnet a multilingual database with lexical semantic networks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vossen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,724.47,423.00,8.74;10,104.94,736.43,396.24,8.74" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="10,462.42,724.47,50.58,8.74;10,104.94,736.43,59.94,8.74">The Zettair Search Engine</title>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugh</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Falk</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Yiannis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steffen</forename><surname>Hein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-09">2004. September</date>
			<pubPlace>Melbourne, Australia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Search Engine Group ; RMIT University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
