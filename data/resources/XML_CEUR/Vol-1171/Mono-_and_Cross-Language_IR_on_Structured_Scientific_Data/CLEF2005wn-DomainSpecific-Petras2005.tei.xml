<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,75.48,74.47,425.00,12.19;1,132.96,90.55,309.98,12.19">How One Word Can Make all the Difference -Using Subject Metadata for Automatic Query Expansion and Reformulation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,260.88,119.13,54.26,8.74"><forename type="first">Vivien</forename><surname>Petras</surname></persName>
							<email>vivienp@sims.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Management and Systems</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,75.48,74.47,425.00,12.19;1,132.96,90.55,309.98,12.19">How One Word Can Make all the Difference -Using Subject Metadata for Automatic Query Expansion and Reformulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DF79FE2EDC4075D8B7BBBE46991BA7AA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval --Query Formulation</term>
					<term>H.3.1 Content Analysis and Indexing --Thesauruses</term>
					<term>H.3.4 Systems and Software --Performance evaluation (efficiency and effectiveness)</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation Controlled vocabulary, thesauri, automatic query expansion, entry vocabulary modules</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Query enhancement with domain-specific metadata (thesaurus terms) is analyzed for monolingual and bilingual retrieval on the GIRT social science collection. We describe our technique of Entry Vocabulary Modules, which associates query words with thesaurus terms and suggest its use for monolingual as well as bilingual retrieval. Different weighting and merging schemes for adding keywords to queries as well as translation techniques are described. Query enhancement generally improves average precision scores for both monolingual and bilingual retrieval. We take a closer look at individual queries and discuss how the query enhancements (or substitutions in bilingual retrieval) can change retrieval results quite dramatically. A query-by-query analysis provides deeper insight into strengths and weaknesses of strategies and serves as a cautionary reminder that average precision scores don't always tell the whole story.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Subject metadata (thesaurus terms, classification codes, library subject headings) in bibliographic databases serve several important purposes:</p><p>(i) To provide a concise topical description of the record content; (ii)</p><p>To provide a non-ambiguous term for each concept represented in the database, that is, to control the subject vocabulary; (iii)</p><p>To provide an organization scheme for the documents in the database; (iv)</p><p>To cluster all relevant documents for a concept under one term; <ref type="bibr" coords="1,88.56,560.73,11.68,8.74">(v)</ref> To provide more searchable text for the user (especially in databases with sparse text in their records like library catalogs); (vi)</p><p>To aid retrieval by providing a topical access point that is unambiguous and retrieves a complete and precise document set for a given concept.</p><p>The problem with subject metadata for search is that the system vocabulary may differ from the searcher's vocabulary and so requires additional time and effort for the searcher to search / understand the controlled vocabulary of the system and to find the appropriate term for his or her information need. However, once the vocabulary has been mastered, searches are generally shorter, more precise and also more complete (finding all and only relevant documents).</p><p>In a database that contains subject metadata, it is therefore sensible to use and leverage its unique advantages. In a world that emphasizes ease of use and quick turn-around time, mastering the controlled vocabulary should not be required of the searcher. Automated query formulation support mechanisms help the searcher to find the appropriate search words by acting as an intermediary between the controlled vocabulary of the database and the natural language of the searcher.</p><p>The technique of Entry Vocabulary Modules was designed to be just that: serving as an interface between the query vocabulary of the searcher (natural language) and the controlled vocabulary entries of a database. Given any search word or phrase, it will suggest controlled vocabulary terms that represent the concept of the search. A searcher can use these terms to append to his or her query or to substitute his or her own query terms with those controlled vocabulary terms in the hope of achieving a more precise and complete retrieval.</p><p>Query expansion has been researched in the information retrieval field for a long time <ref type="bibr" coords="2,447.44,153.63,10.64,8.74" target="#b0">[1]</ref>. However, automatic query expansion has been mostly discussed in the context of blind feedback or highly evolved expert systems [e.g. <ref type="bibr" coords="2,158.90,176.67,7.33,8.74" target="#b1">2,</ref><ref type="bibr" coords="2,166.23,176.67,7.33,8.74" target="#b2">3]</ref>. Thesauri are mainly used for manual or interactive query expansion (for an overview, see <ref type="bibr" coords="2,131.32,188.13,10.49,8.74" target="#b3">[4]</ref>), but authors report mixed results <ref type="bibr" coords="2,290.81,188.13,8.00,8.74" target="#b4">[5]</ref><ref type="bibr" coords="2,298.81,188.13,4.00,8.74" target="#b5">[6]</ref><ref type="bibr" coords="2,298.81,188.13,4.00,8.74" target="#b6">[7]</ref><ref type="bibr" coords="2,302.81,188.13,8.00,8.74" target="#b7">[8]</ref> when comparing those techniques to free-text search.</p><p>For CLEF 2005, Berkeley's group 2 experimented with Entry Vocabulary Modules (EVMs) to automatically enhance queries with subject metadata terms or to replace query terms with them. The GIRT collection (German Indexing and Retrieval Test database) contains titles, abstracts and thesaurus terms providing an ideal test bed for monolingual and bilingual retrieval (German and English documents as well as a bilingual thesaurus).</p><p>The paper is organized as follows: first, we briefly introduce the GIRT collection and then explain Entry Vocabulary Modules and the basics of our retrieval technique. Section Five explains the runs for German and English Monolingual retrieval in detail. Section Six explains our translation techniques and how EVMs can be used for query translation. Sections 6.2 and 6.3 compare different translation techniques and discuss combinations for bilingual retrieval for English to German and German to English, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The GIRT Collection</head><p>The GIRT collection (German Indexing and Retrieval Test database) consists of 151,319 documents containing titles, abstracts and thesaurus terms in the social science domain. The GIRT thesaurus terms are assigned from the Thesaurus for the Social Sciences <ref type="bibr" coords="2,285.84,411.21,11.68,8.74" target="#b8">[9]</ref> and are provided in German, English and Russian. Two parallel GIRT corpora in English and German each containing 151,319 records are made available. For a detailed description of GIRT and its uses, see <ref type="bibr" coords="2,261.72,434.25,15.32,8.74" target="#b9">[10]</ref>.</p><p>The English GIRT collection contains only 26,058 abstracts (ca. one out of six records) whereas the German collection contains 145,941 -providing an abstract for almost all documents. Consequently, the German collection contains more terms per record to search on. The English corpus has 1,535,445 controlled vocabulary entries (7064 unique phrases) and the German corpus has 1,535,582 controlled vocabulary entries (7154 unique phrases) assigned. On average, 10 controlled vocabulary terms / phrases are appended to each document.</p><p>Controlled vocabulary terms are not uniformly distributed. Most thesaurus terms occur less than a 100 times, but 307 occur more than 1,000 times and the most frequent one, "Bundesrepublik Deuschland", occurs 60,955 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Entry Vocabulary Modules</head><p>Entry Vocabulary Modules are automatically created search aids that function as intermediaries between the searcher's queries and the controlled vocabulary of a bibliographic database, in this case the GIRT thesaurus. They are referred to as Entry Vocabulary Modules because they provide a mapping from the "query vocabulary" of the searcher to the "entry vocabulary" of the database. A database's entry vocabulary consists of the subject metadata. It is this controlled vocabulary that provides an effective "entry" (access point) to the database records.</p><p>An Entry Vocabulary Module is in fact a dictionary of associations between terms in titles and abstracts in documents and the controlled vocabulary terms associated with the document. If title/abstract words and thesaurus terms co-occur with a higher than random frequency, there exists a likelihood that they are associated. A likelihood ratio statistic is used to measure the association between any natural language term and a controlled vocabulary term. Each pair is assigned an association weight (rank) representing the strength of their association. The higher the rank, the more a thesaurus term represents the concept represented by the document word. The methodology of constructing Entry Vocabulary Modules has been described in detail in <ref type="bibr" coords="3,156.74,96.15,16.70,8.74" target="#b10">[11]</ref> and <ref type="bibr" coords="3,192.84,96.15,15.33,8.74" target="#b11">[12]</ref>.</p><p>Once an Entry Vocabulary Module is constructed and a table of associations and their weights exist, we can look up a word in the dictionary and find its most highly associated thesaurus term. This is how we find thesaurus terms to associate with the GIRT queries. After experimenting with looking up query title and description words, we found that query title words are sufficient to find relevant thesaurus terms. For all CLEF 2005 experiments, only query title words (after stopword removal) were used for thesaurus term look-up. If more than one word appears in the query title, we need to merge the results from the thesaurus term look-ups to receive a list of terms for the query as a whole. We experimented with two merging strategies discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Absolute Rank Merging</head><p>For absolute rank merging, an absolute rank for each thesaurus term is calculated by adding the association weights if it is associated with several title words. The five thesaurus terms with the highest rank are then added to the query. We will use the English GIRT query 132 to illustrate this:  <ref type="table" coords="3,91.05,404.61,19.45,8.74">table</ref> shows a sample of the thesaurus terms associated with each individual title word and the absolute rank order for thesaurus terms after adding the weights for each thesaurus term and ranking again. For child, the association rank of the word "sexual" with the thesaurus term *child* is looked up (325.31 not shown in table ), then added to the association rank of the title word "abuse" with *child* (431.38) and then added to the association for "children" (19711.75). The resulting 20468.45 is the absolute rank for the thesaurus term *child* and makes it the top-ranking thesaurus term for this query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Round Robin Merging</head><p>The above example for absolute rank merging also shows the pitfall of this merging strategy: some association pairs (like "children" -*child* in query 132) have such high weights that other important query word -thesaurus term combinations will be ranked lower no matter what. To avoid this problem, we also tested a round robin merging strategy: for each query word, we looked up the two highest ranked thesaurus terms and added them to the query. The English GIRT query 138 will serve as an example: Title 138: Insolvent Companies Absolute rank merging Round robin merging enterprise liquidity firm indebtedness medium-sized firm enterprise small-scale business firm flotation The first two thesaurus terms in the round robin strategy are highly associated with "insolvent", the second two with "companies". As one can see in the absolute rank strategy, the thesaurus terms for "companies" seem to 'overpower' the ones for "insolvent".</p><p>Sometimes, this strategy is prone to errors as topic 143 proves. The words looked up in the EVM are "smoking" and "giving", which is misleading. The absolute rank strategy performs better in this case. For German with its compounds ("Unternehmensinsolvenzen" instead of "Insolvent Companies" for topic 138), the round robin strategy sometimes only adds two instead of five thesaurus terms to the query, the ranking otherwise being equal to the absolute rank strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Retrieval Technique</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Document Ranking</head><p>In all its CLEF submissions, the Berkeley 2 group used a document ranking algorithm based on logistic regression first used in the TREC-2 conference <ref type="bibr" coords="4,263.12,296.43,15.33,8.74" target="#b12">[13]</ref>. The logodds of relevance of document D to query Q is given by ( ) ( )</p><formula xml:id="formula_0" coords="4,70.56,333.14,408.33,56.03">4 * 0929 . 0 3 * 1937 . 0 2 * 33 . 0 1 * 4 . 37 51 . 3 , | log , | log ) , | ( log x x x x Q D R P Q D R P Q D R O + - + + - = = where ( ) Q D R P , | log</formula><p>is the probability of relevance of document D with respect to query Q and ( )</p><formula xml:id="formula_1" coords="4,72.24,398.31,57.63,8.73">Q D R P , | log</formula><p>is the probability of non-relevance of document D with respect to query Q. The regression variables are defined as follows:</p><formula xml:id="formula_2" coords="4,82.62,435.94,344.68,29.75">∑ = + + = n i ql i qtf n x 1 35 1 1 1 (1) ∑ = + = n i cl i ctf n x 1 log 1 1 3<label>(3)</label></formula><formula xml:id="formula_3" coords="4,82.68,480.46,340.72,29.75">∑ = + + = n i dl i dtf n x 1 80 log 1 1 2 (2) n x = 4 (<label>4</label></formula><formula xml:id="formula_4" coords="4,423.40,490.70,3.89,8.74">)</formula><p>where n is the number of terms common to both a document and a query, qtf i / dtf i represent the frequency of term i within the query and document respectively, ctf i is the frequency of term i in the collection, ql / dl represent the number of terms in the query and document respectively and cl is the collection length, i.e. the number of terms in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Collection and Query Processing</head><p>For all runs, we used a stopword list to remove very common words from the English and German collections and queries as well as an implementation of the Muscat stemmer for both English and German.</p><p>For German runs, we used a decompounding procedure developed and described by Aitao Chen [14,15], which has been shown to improve retrieval results. The decompounding procedure looks up document and query words in a base dictionary and splits compounds when found.</p><p>As a general procedure, we also use Aitao Chen's blind feedback algorithm <ref type="bibr" coords="4,375.12,685.40,16.71,8.74">[14,</ref><ref type="bibr" coords="4,391.83,685.40,12.53,8.74" target="#b13">15]</ref> in every run. It selects the top 30 ranked terms from the top 20 ranked documents from the initial search to merge with the original query.</p><p>query stopword removal (decompounding) stemming ranking blind feedback All query expansion and reformulation experiments described apply to the original query before submission to those processing steps and remain the same otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Monolingual Retrieval</head><p>For monolingual retrieval, we experimented with three query expansion strategies: adding five thesaurus terms retrieved with the EVM absolute rank merging from query title words; adding five thesaurus terms from the absolute rank merging strategy (using only query title words) but removing all thesaurus terms from the dictionary that occurred more than a 1,000 times in the document collection, thereby hoping to remove thesaurus terms that would not discriminate effectively; adding two thesaurus terms retrieved from the EVM for each query title word using the round robin merging strategy.</p><p>Last year, we experienced an improvement in precision when we weighted the expanded part of the query (the thesaurus terms) half as much as the original query words. This is also true for our other expansion mechanism (blind feedback), where new terms are added with half the weight as compared to the original query terms. For every expansion strategy, we analyze one run where the thesaurus terms are downweighted and one where they are treated as equally important part of the query.</p><p>We also experimented with submitting only the title of the query to the retrieval system, assuming that the shortness of the queries will simulate real user queries better than a title+description query. Since the EVMs don't need more information than the title words, we can also use the technique for these sparse queries.</p><p>For every run, we not only compared the overall average precision but also the precision scores on a query-byquery basis. This shows more clearly where the strengths and weaknesses of the individual strategies are but also reveals that sometimes just one word can influence precision scores dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">German</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Title + Description Runs</head><p>As the following table <ref type="table" coords="5,162.48,420.44,5.01,8.74" target="#tab_1">1</ref> shows, query expansion always improves over the baseline run of title+description if the expanded part is downweighted. If the thesaurus terms are not downweighted, only the round robin strategy improves over the baseline run. However, this case is also the dominating strategy, not only improving the baseline by 13% but also improving on the downweighted strategy and on the other merging strategies. Comparing precision on a query-by-query basis, it becomes clear that downweighting clearly dominates for the absolute rank strategies, whereas not downweighting equally dominates for the round robin strategy although the average precision scores are much closer. In 18 of 25 queries, absolute rank merging with downweighting had a better precision than the not downweighted absolute rank strategy, for the absolute rank -1000 strategy, downweighting achieved a better result in 20 cases. For round robin, not downweighting turned out to be better in 17 of 25 cases compared to downweighting.</p><p>Comparing all seven runs with each other shows that the best run (RR) dominates in 11 cases, the baseline run in 6 cases, ABS HW in 3 cases, RR HW in 3 cases and ABS -1000 HW in 2 cases, changing the ranking order compared to average precision scores.</p><p>However, it makes more sense to compare strategies pair wise to see which one is stronger. We will look at the absolute rank and round robin strategies more closely to see how expanding a query by just a few words can change the results. Although downweighting works better for absolute rank merging (16 queries better than baseline) than not downweighting (9 queries better than baseline), we will use the not downweighted strategy to control for the effects of the weighting schemes.  The table shows the thesaurus terms that were appended to the query. Even though all of them seem relevant, the double occurrence of the word "Erziehung" in the thesaurus terms might skew the results too much towards documents dealing with education (Erziehung) alone and less with the bilingual aspect of it. Indeed, deleting the word "Erziehung" from the thesaurus terms in the RR strategy raises the precision from 0.43 to 0.55 (+28%), proving that sometimes one word can cause a huge improvement. Query 139 serves as example where the expansion strategy works much better than the baseline: &lt;DE-title&gt; Gesundheitsökonomie &lt;/DE-title&gt; &lt;DE-desc&gt; Finde Dokumente, die die Versorgung der Bevölkerung mit medizinischen und ärztlichen Dienstleistungen aus ökonomischer Sicht diskutieren. &lt;/DE-desc&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABS RR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gesundheitswesen</head><p>Gesundheitswesen Ökonomie Ökonomie Kostentheorie Wirtschaftskreislauf Gesundheitspolitik In this case, the words "Gesundheitswesen" and "Ökonomie" help most in improving the precision, but even leaving these terms out, the other three suggested thesaurus terms from the ABS strategy still raise the precision from 0.2812 to 0.5049 (+79%)! Finally, query 148 is an interesting case showing how query expansion can be both advantageous and disadvantageous -depending on the terms expanded. The absolute rank strategy adds thesaurus terms that are too general for the query, decreasing precision by 62%. However, just adding the term "Spätaussiedler" from the round robin strategy improves precision by 44%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Title only Runs</head><p>For title only runs we only experimented with the best strategy: round robin merging. As table <ref type="table" coords="7,457.63,382.64,5.01,8.74">2</ref> shows, queries expanded with thesaurus terms clearly improve precision over the baseline run (19%). For title only runs, downweighting thesaurus terms works better, improving the precision over the baseline by 30% and even more so, slightly improving on the baseline of the title+description run! run T baseline RR RR HW average precision 0.3643 0.4339 0.4748 Table <ref type="table" coords="7,206.50,479.90,3.77,8.74">2</ref>. Title only runs for German Monolingual retrieval Comparing these runs on a query-by-query basis shows the dominance of the query expansion strategy even clearer: in 18 of 25 cases, RR is better than the baseline, and in 22 out of 25 cases RR HW is better than the baseline. RR HW is better than a title+description run in 14 cases.</p><p>One more experiment gives food for thought: instead of submitting the original query text, we only submitted the suggested EVM thesaurus terms from the round robin strategy, therefore reformulating the query instead of expanding it. Although the precision compared to the baseline decreases to 0.3075, substituting the thesaurus terms for the original query text works better in 12 of the 25 cases, showing that free-text does not dominate a controlled vocabulary strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">English</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Title + Description Runs</head><p>As table <ref type="table" coords="7,110.85,652.34,5.01,8.74" target="#tab_4">3</ref> shows, query expansion with EVM suggested thesaurus terms is not as successful for English monolingual retrieval. However, the trend remains the same as in German monolingual retrieval. The round robin strategy without downweighting is still the dominating strategy, improving on the baseline by 6%. For the absolute rank strategies, downweighting works better, although they don't improve on the baseline. The difference between downweighting or not is more pronounced when looking at the results on a query-byquery basis: in 21 out of 25 cases downweighting is better for the absolute rank strategy and in 20 of 25 cases for the absolute rank -1000 strategy. Not downweighting works better for round robin merging in 14 out of the 25 cases.</p><p>Comparing all seven runs shows that the best run (RR) only dominates in 9 cases, the baseline in 5, RR HW in 4, ABS HW in 3, ABS -1000 HW in 2 cases and ABS in 1 case demonstrating a weaker trend than in German monolingual retrieval.</p><p>Once again, graph 2 shows a comparison of precision scores for the baseline, the absolute rank and the round robin strategy. The absolute strategy works better than the baseline in 8 cases, but round robin is clearly better in 16 cases. Looking at graph 2 reveals two things: First, the absolute strategy seems to make things much much worse in some cases <ref type="bibr" coords="8,117.84,601.64,20.91,8.74">(131,</ref><ref type="bibr" coords="8,141.25,601.64,17.54,8.74">138,</ref><ref type="bibr" coords="8,161.30,601.64,16.72,8.74">141)</ref>. This is because it adds thesaurus terms that are too general. But even the round robin strategy doesn't seem to improve precision as much as in German monolingual retrieval. Ironically, it seems that the unique characteristics of the German language (compounds) help in suggesting thesaurus terms that are not only more on the mark but are also compounds themselves retrieving more relevant documents. For example, the thesaurus term *way of life* translates to *Lebensweise* in German. Whereas for English, the retrieval system will look for documents containing "way" and "life" (very general!), the retrieval system will look for "Lebensweise" in German, which is much more precise.</p><p>However, it also cannot be overlooked that the English collection contains less text (fewer abstracts) than the German collection to search on. It might be that the added thesaurus terms skew search results in that they take away weight from the free-text search terms ranking documents containing the thesaurus terms (more likely) higher than ones containing the free-text search terms. This would explain the greater improvement of the downweighting strategies for absolute rank merging as compared to German (precision increases by 20% and 33% for ABS and ABS -1000 in English, whereas only by 8% and 19% in German) and the smaller improvement of not downweighting for round robin (2.5% in English vs. 4% in German).</p><p>Nevertheless, one query can serve as an example that one word can make a difference in English also: just adding the EVM suggested thesaurus term *morals* to query 142 (Advertising and Ethics) will improve precision by 31%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Title only Runs</head><p>For title only runs, query expansion seems to improve on the baseline (+7%), although not as much as in German (19%). Downweighting again works better, improving the baseline by 14%. run T baseline RR RR HW average precision 0.3972 0.4242 0.4542 Table <ref type="table" coords="9,207.30,239.36,3.77,8.74">4</ref>. Title only runs for English Monolingual retrieval Looking at the results on a query-by-query basis shows the dominance of the expansion strategies a little better: in 16 cases out of 25 RR dominates over the baseline, whereas RR HW is better in 18 cases. The best strategy for title only runs can compete with the baseline title+description run, with similar average precision and a better performance in 12 out of 25 cases.</p><p>However, replacing the title words with EVM suggested thesaurus terms works less well than in German: in only 5 cases this strategy performs better, decreasing the overall average precision to 0.2983 (-25%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">EVM Query Expansion vs. Blind Feedback</head><p>Although it has been shown that query expansion with EVM suggested thesaurus terms will improve monolingual retrieval in general, it might be of interest to compare this automatic technique of query expansion to another one -blind feedback. We have used blind feedback with success in previous years and now use it in all our retrieval experiments. Although EVM and blind feedback query expansion are quite different in nature -EVM works from the query title text, blind feedback from the result set document text -they are used to enhance the query to achieve better results.  <ref type="table" coords="9,97.25,612.86,3.77,8.74">5</ref>. Comparing blind feedback and EVM query expansion with pair-wise comparison for the blind feedback and EVM technique. The numbers represent the numbers of queries where this strategy achieved a higher precision score than the other (e.g. for German, the EVM technique achieved a higher precision in 18 cases).</p><p>The combination of both techniques outperforms the baseline and the individual query expansion techniques. For German monolingual retrieval, only EVM suggested terms improve over the baseline (in 16 out of 25 cases). For English, however, EVM terms improve only slightly over the baseline (13 cases), whereas blind feedback improves over the baseline (16 cases) and outperforms EVM expansion (better in 15 cases).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Bilingual Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Translation Methods</head><p>For bilingual retrieval, we experimented with query expansion and query reformulation using EVMs in addition to query translation. Three translation techniques are compared:</p><p>1. Machine translation. We used a combination of the Systran translator (http://babelfish.altavista.com/) and the L &amp; H Power Translator. 2. Thesaurus matching. Words and phrases from the query are looked up in the thesaurus with a fuzzymatching algorithm and if a matching thesaurus term in the query language is found, the equivalent thesaurus term in the target language is used. See <ref type="bibr" coords="10,308.36,201.92,16.70,8.74" target="#b14">[16]</ref> for a more detailed description. 3. EVM. The query title words were submitted to the query language EVM and the round robin merging technique was used to retrieve thesaurus terms. The thesaurus terms in the query language were then replaced by the thesaurus terms in the target language. The query was then reformulated using only thesaurus terms. For bilingual retrieval, we will first compare these translation techniques separately and then in combination. In previous years, a combination of machine translation and thesaurus matching achieved the best results. For machine translation and thesaurus matching, both title and description of the query were submitted, for EVM only the suggested thesaurus terms were submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Translation</head><p>Table <ref type="table" coords="10,96.86,477.92,5.01,8.74">6</ref> shows the average precision scores for the three translation methods in comparison for both bilingual tracks from German to English and English to German. For more comparison, the table also shows the number of queries with the better precision in a pair-wise comparison.  <ref type="table" coords="10,103.18,640.64,3.77,8.74">6</ref>. Comparing 3 translation techniques for bilingual retrieval with pair wise comparison of strategy. The last 3 rows compare 2 strategies with each other, first machine translation vs. thesaurus matching, then machine translation vs. EVM terms and then thesaurus matching vs. EVM terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>German English English German</head><note type="other">Machine</note><p>This table demonstrates once again that although average precision scores might differ significantly, a query-byquery analysis shows differently. Although thesaurus matching seems to perform worse in German-English retrieval (-23%), machine translation is better in only little over half of the cases. And although machine translation and thesaurus matching seem to perform equally well in English-German retrieval, thesaurus matching performs better in 3/5th of the cases. The performance of the EVM suggested thesaurus terms compared to machine translation is astonishing: an automatically associated list of controlled vocabulary terms performs almost as well as the combined textual-based translations of two commercial machine translation programs!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Combining translation techniques</head><p>Combining translation techniques means submitting the translated output from the different methods in one and the same run. This increases the number of query words and the danger of introducing more non-discriminating search terms as well as favoring easy to translate terms (they most likely to occur in all methods), but for CLEF, this strategy has worked successfully in previous years. Combining translation methods helps with hard to translate words (higher chance of one method getting it right) and reduces the risk of mis-translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Official runs</head><p>The official runs for bilingual retrieval used the absolute rank merging technique when EVM suggested thesaurus terms were used. However, later experiments showed that round robin merging is also dominant for bilingual retrieval and so we report results for round robin merging. For documentation purposes we briefly state which official runs used which translation combinations below. Later sections will report combination runs with EVM round robin merging in more detail. As one can see, a combination of all three techniques is clearly the dominating strategy -it seems that adding more words describing the same concept generally improves the precision instead of adding too many nondiscriminating terms. It is also worth mentioning that all combination runs perform better than machine translation alone, even if one combines thesaurus matching and EVM terms only. In fact, even though lower in precision, this combination performs better in 13 out of 25 cases compared to both the machine translationthesaurus matching and the machine translation -EVM pairs; a worthy competitor to the commercial translation solutions.  Only the EVM round robin strategy manages to suggest the important word "Abfall" (waste) -whereas the other strategies either mistranslate "waste" or select the wrong thesaurus term due to incorrect fuzzy matching. The EVM words alone achieve a precision score of 0.6558, whereas the highest combination strategy achieves only 0.414 (thesaurus matching + EVM) -still better than the combination of machine translation and thesaurus matching (0.136), which is still better than machine translation (0.0295) or thesaurus matching (0.0236) alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">English-German Bilingual Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Query expansion techniques have been a topic of research in the IR field for decades. Automatic query expansion has been analyzed mostly in terms of blind feedback mechanisms based on a preliminary ranked list of documents. Query expansion based on thesauri or other controlled vocabularies is mostly a topic for manual query expansion or interactive modes of query expansion. This paper discusses an automatic query expansion strategy using controlled vocabulary terms.</p><p>Expanding a query with terms from a thesaurus is like asking an information expert to translate your search strategy into the search language of the database, hopefully providing better search terms than the original search statement. The information expert for this set of experiments is an association dictionary of thesaurus terms and free-text words from titles and abstracts from the collection. Based on title words from the query, thesaurus terms that are highly associated with those words are suggested. Two merging strategies have been tested: absolute rank merging, based on all title words as a set and round robin merging, which suggests two thesaurus terms for each individual query word.</p><p>For monolingual retrieval, query expansion with EVM suggested thesaurus terms improves over the baseline of title + description submission by 13% (German) and 6% (English), respectively. Downweighting the added terms performs better for absolute rank but not for the round robin merging. For German, submitting only thesaurus terms (replacing the original query) decreases the average precision over 25 cases, but achieves better precision in 12 individual cases.</p><p>Comparing EVM query expansion to blind feedback (terms are taken from ranked result set documents and downweighted when added to query) shows that EVM query expansion improves over blind feedback in German and similar in performance in English, and a combination of both dominates either strategy and the baseline.</p><p>For bilingual retrieval, using the thesaurus for translation works surprisingly well. Just using thesaurus terms for the query submission works almost as well as machine translation. Although average precision decreases (9% for English-German and 15% for German-English), EVM suggested thesaurus terms perform better in one third of the queries. A combination of two thesaurus techniques (EVM and thesaurus matching) outperforms machine translation. The combination of machine translation, thesaurus matching and EVM suggested terms outperforms all other strategies.</p><p>It has been shown that EVM suggested terms can provide the impact to raise precision for a query -if they are high quality search terms. High quality search terms are those that provide discriminating search power (they occur mostly in relevant documents), describe the information need exactly and, ideally, add new terms to the query. Added terms that are too vague will almost always degrade the performance. One word is all it takes to make the difference -now if we could only figure out which one!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgement</head><p>Thanks to Aitao Chen for implementing and permitting the use of the logistic regression formula for probabilistic information retrieval as well as German decompounding and blind feedback in his MULIR retrieval system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,70.56,84.63,120.68,8.74;4,198.30,99.15,179.38,8.74;4,198.30,113.19,34.47,8.74;4,291.54,113.19,35.03,8.74;4,198.30,126.63,153.54,8.74;4,198.30,140.13,31.15,8.74;4,291.57,140.13,34.46,8.74;4,198.30,153.63,178.57,8.74;4,198.30,167.13,67.55,8.74"><head>Title 143 :</head><label>143</label><figDesc>Giving up SmokingAbsolute rank merging Round robin merging smoking donation tobacco consumption social relations tobacco smoking behavior modification tobacco consumption behavior therapy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,125.22,400.82,4.59,7.67;6,118.50,379.82,11.31,7.67;6,118.50,359.53,11.31,7.67;6,118.50,338.53,11.31,7.67;6,118.50,317.53,11.31,7.67;6,118.50,297.31,11.31,7.67;6,118.50,276.31,11.31,7.67;6,118.50,255.31,11.31,7.67;6,118.50,234.31,11.31,7.67;6,118.50,214.03,11.31,7.67;6,125.22,193.03,4.59,7.67;6,129.00,412.81,13.58,7.67;6,193.50,412.81,13.58,7.67;6,257.21,412.81,13.58,7.67;6,321.71,412.81,13.58,7.67;6,385.48,412.81,13.58,7.67;6,449.98,412.81,13.58,7.67;6,424.50,201.32,42.08,7.67;6,424.50,215.54,16.72,7.67;6,424.50,229.82,11.18,7.67;6,131.82,431.90,331.96,8.74;6,70.56,454.88,454.42,8.74;6,70.56,466.40,454.37,8.74;6,70.56,477.86,369.55,8.74"><head>Graph 1 .</head><label>1</label><figDesc>Comparing precision scores per query for German Monolingual RetrievalGraph 1 shows that results can vary for each strategy and query, the most dramatic change being the improvement from 0.2812 in the baseline to 0.6003 for ABS in query 139 (an improvement of 113%!). Even more amazing, looking at individual queries shows how little it takes to improve or degrade.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,125.22,536.06,4.59,7.66;8,118.50,515.06,11.31,7.66;8,118.50,494.84,11.31,7.66;8,118.50,473.84,11.31,7.66;8,118.50,453.56,11.31,7.66;8,118.50,432.55,11.31,7.66;8,118.50,411.55,11.31,7.66;8,118.50,391.33,11.31,7.66;8,118.50,370.33,11.31,7.66;8,118.50,350.05,11.31,7.66;8,125.22,329.05,4.59,7.66;8,129.00,548.05,13.58,7.66;8,193.50,548.05,13.58,7.66;8,257.21,548.05,13.58,7.66;8,321.71,548.05,13.58,7.66;8,385.49,548.05,13.58,7.66;8,449.98,548.05,13.58,7.66;8,426.72,335.84,42.09,7.66;8,426.72,350.06,16.78,7.66;8,426.72,364.34,11.23,7.66;8,132.66,567.14,330.22,8.74"><head>Graph 2 .</head><label>2</label><figDesc>Comparing precision scores per query for English Monolingual Retrieval</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,90.48,481.70,402.93,165.94"><head>Table 1 .</head><label>1</label><figDesc>Average precision scores for title + description German Monolingual Runs</figDesc><table coords="5,90.48,481.70,402.93,154.42"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ABS</cell><cell>ABS</cell><cell></cell></row><row><cell>run</cell><cell cols="3">TD baseline ABS HW</cell><cell>ABS</cell><cell>-1000 HW</cell><cell>-1000</cell><cell>RR HW</cell><cell>RR</cell></row><row><cell></cell><cell>BK2G</cell><cell cols="2">BK2G</cell><cell></cell><cell>BK2G</cell><cell></cell><cell>BK2G</cell></row><row><cell>official run</cell><cell>MLGG1</cell><cell cols="2">MLGG2</cell><cell></cell><cell>MLGG3</cell><cell></cell><cell>MLGG4</cell></row><row><cell>average</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>precision</cell><cell>0.4547</cell><cell cols="2">0.4733</cell><cell>0.4369</cell><cell>0.4595</cell><cell>0.3866</cell><cell>0.4936</cell><cell>0.5144</cell></row><row><cell></cell><cell>ABS</cell><cell></cell><cell cols="3">absolute rank strategy</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">ABS -1000</cell><cell cols="5">absolute rank strategy omitting thesaurus terms that occur more than 1000times in the collection</cell></row><row><cell></cell><cell>RR</cell><cell></cell><cell cols="3">round robin merging</cell><cell></cell><cell></cell></row><row><cell></cell><cell>HW</cell><cell></cell><cell cols="5">expanded thesaurus terms are downweighted by half in this run</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,70.56,222.14,454.45,111.22"><head></head><label></label><figDesc>&lt;DE-title&gt; Russlanddeutsche und Sprache &lt;/DE-title&gt; &lt;DE-desc&gt; Finde Dokumente, die die sprachliche Integrität von Russlanddeutschen der ehemaligen Sowjetunion in Deutschland oder Russland diskutieren. &lt;/DE-desc&gt;</figDesc><table coords="7,212.10,258.38,154.90,74.98"><row><cell>ABS</cell><cell>RR</cell></row><row><cell>Sprache</cell><cell>Auswanderung</cell></row><row><cell>Sprachgebrauch</cell><cell>Spätaussiedler</cell></row><row><cell>Linguistik</cell><cell>Sprache</cell></row><row><cell>Fachsprache</cell><cell>Sprachgebrauch</cell></row><row><cell>Kommunikation</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,105.96,76.16,373.47,84.22"><head>Table 3 .</head><label>3</label><figDesc>Average precision scores for title + description English Monolingual Runs</figDesc><table coords="8,105.96,76.16,373.47,72.28"><row><cell></cell><cell>TD</cell><cell></cell><cell></cell><cell>ABS -</cell><cell>ABS -</cell><cell></cell><cell></cell></row><row><cell>run</cell><cell cols="2">baseline ABS HW</cell><cell>ABS</cell><cell>1000 HW</cell><cell>1000</cell><cell>RR HW</cell><cell>RR</cell></row><row><cell></cell><cell>BK2G</cell><cell>BK2G</cell><cell></cell><cell>BK2G</cell><cell></cell><cell></cell><cell></cell></row><row><cell>official run</cell><cell>MLEE1</cell><cell>MLEE2</cell><cell></cell><cell>MLEE3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>average</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>precision</cell><cell>0.4531</cell><cell>0.4149</cell><cell>0.3462</cell><cell>0.4125</cell><cell>0.3092</cell><cell>0.4697</cell><cell>0.4818</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,71.94,434.90,435.85,186.70"><head></head><label></label><figDesc>Table 5 gives a quick overview of runs using either strategy, both or none.</figDesc><table coords="9,71.94,465.14,392.03,156.46"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>blind feedback +</cell></row><row><cell></cell><cell>Without query</cell><cell></cell><cell>EVM suggested</cell><cell>EVM suggested</cell></row><row><cell></cell><cell>expansion</cell><cell>blind feedback</cell><cell>terms</cell><cell>terms</cell></row><row><cell></cell><cell></cell><cell>German</cell><cell></cell><cell></cell></row><row><cell>avg precision</cell><cell>0.4622</cell><cell>0.4547</cell><cell>0.4902</cell><cell>0.5144</cell></row><row><cell># of best</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>queries</cell><cell></cell><cell>7</cell><cell>18</cell><cell></cell></row><row><cell></cell><cell></cell><cell>English</cell><cell></cell><cell></cell></row><row><cell>avg precision</cell><cell>0.4175</cell><cell>0.4531</cell><cell>0.4517</cell><cell>0.4818</cell></row><row><cell># of best</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>queries</cell><cell></cell><cell>15</cell><cell>10</cell><cell></cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,70.56,270.92,324.11,112.24"><head></head><label></label><figDesc>Query 144 serves as example for the different output of the translation strategies.</figDesc><table coords="10,70.56,293.96,283.84,89.20"><row><cell>German query title:</cell><cell>Radio und Internet</cell></row><row><cell>English query title:</cell><cell>Radio and Internet</cell></row><row><cell>Machine translation:</cell><cell>radio and internet (L &amp; H)</cell></row><row><cell></cell><cell>radio and InterNet (Systran)</cell></row><row><cell>Thesaurus matching:</cell><cell>tradition (inaccurate due to fuzzy matching)</cell></row><row><cell></cell><cell>internet</cell></row><row><cell>EVM suggestions:</cell><cell>radio / radio program</cell></row><row><cell></cell><cell>Internet / online service</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,70.56,280.16,444.33,236.86"><head>Table 7 .</head><label>7</label><figDesc>Combinations of translation techniques for German-English bilingual retrieval with pair-wise comparison of strategy</figDesc><table coords="11,70.56,280.16,444.33,212.76"><row><cell cols="5">BK2GBLEG1 / GE1 machine translation + thesaurus matching</cell></row><row><cell cols="6">BK2GBLEG2 / GE2 machine translation + thesaurus matching + EVM absolute rank</cell></row><row><cell cols="2">BK2GBLEG3 / GE3</cell><cell></cell><cell cols="3">thesaurus matching + EVM absolute rank</cell></row><row><cell cols="2">BK2GBLEG4</cell><cell cols="4">machine translation + thesaurus matching + EVM absolute rank (downweighted)</cell></row><row><cell cols="4">6.3.2 German-English Bilingual Retrieval</cell><cell></cell></row><row><cell cols="5">Table 7 compares combination runs for German-English retrieval.</cell></row><row><cell></cell><cell cols="2">Machine Translation +</cell><cell>Machine Translation +</cell><cell></cell><cell>Machine Translation +</cell></row><row><cell></cell><cell cols="2">Thesaurus Matching</cell><cell></cell><cell>Thesaurus Matching +</cell><cell>Thesaurus Matching +</cell></row><row><cell></cell><cell></cell><cell></cell><cell>EVM thesaurus terms</cell><cell>EVM thesaurus terms</cell><cell>EVM thesaurus terms</cell></row><row><cell>avg.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>precision</cell><cell>0.4514</cell><cell></cell><cell>0.4566</cell><cell>0.4346</cell><cell>0.4803</cell></row><row><cell></cell><cell>13</cell><cell></cell><cell>12</cell><cell></cell></row><row><cell></cell><cell>12</cell><cell></cell><cell></cell><cell>13</cell></row><row><cell># of best</cell><cell>7</cell><cell></cell><cell></cell><cell></cell><cell>18</cell></row><row><cell>queries</cell><cell></cell><cell></cell><cell>12</cell><cell>13</cell></row><row><cell></cell><cell></cell><cell></cell><cell>9</cell><cell></cell><cell>16</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell>18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="12,70.56,73.16,454.40,192.70"><head>Table 8 .</head><label>8</label><figDesc>Combinations of translation techniques for English-German bilingual retrieval with pair wise comparison of strategy For English-German retrieval, all combination runs seem to perform similarly. However, once again, they clearly outperform machine translation alone. Of course, not all combinations work equally well for each query and, sometimes, one translation technique alone works much better. Query 136 serves as example:</figDesc><table coords="12,70.56,153.62,311.85,112.24"><row><cell>English query title:</cell><cell>Ecological waste economics</cell></row><row><cell>German query title:</cell><cell>Ökologische Abfallwirtschaft</cell></row><row><cell>Machine translation:</cell><cell>Ökologische Überflüssige Wirtschaftswissenschaft</cell></row><row><cell></cell><cell>Ökologische Überschüssige Volkswirtschaft</cell></row><row><cell>Thesaurus matching:</cell><cell>Ökologische Partei</cell></row><row><cell></cell><cell>K a s t e</cell></row><row><cell></cell><cell>Volkswirtschaftslehre</cell></row><row><cell>EVM suggestions:</cell><cell>Ökologie / Umweltpolitik</cell></row><row><cell></cell><cell>Abfallwirtschaft / Abfall</cell></row><row><cell></cell><cell>Wirtschaft / Volkswirtschaftslehre</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,74.33,208.40,450.68,8.74;13,88.56,219.86,278.60,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,215.44,208.40,68.02,8.74">Query Expansion</title>
		<author>
			<persName coords=""><forename type="first">Efthimis</forename><forename type="middle">N</forename><surname>Efthimiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,301.80,208.40,223.22,8.74;13,88.56,219.86,30.86,8.74">Annual Review of Information Systems and Technology (ARIST)</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Williams</surname></persName>
		</editor>
		<meeting><address><addrLine>Medford, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Information Today</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,74.33,231.38,450.60,8.74;13,88.56,242.90,228.08,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,241.11,231.38,213.90,8.74">An expert system for automatic query reformation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gauch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,464.04,231.38,60.89,8.74;13,88.56,242.90,167.78,8.74">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="124" to="136" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,74.33,254.36,450.66,8.74;13,88.56,265.88,436.41,8.74;13,88.56,277.40,319.18,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,253.64,254.36,271.36,8.74;13,88.56,265.88,114.47,8.74">An Associative Semantic Network for Machine-Aided Indexing, Classification and Searching</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">E</forename><surname>Doszkocs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Sass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,222.42,265.88,147.08,8.74;13,408.07,265.88,116.90,8.74;13,88.56,277.40,169.93,8.74">Proceedings of the 3rd ASIS SIG/CR Classification Research Workshop</title>
		<meeting>the 3rd ASIS SIG/CR Classification Research Workshop<address><addrLine>Medford, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Learned Information</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>Advances in Classification Research</note>
</biblStruct>

<biblStruct coords="13,74.33,288.86,450.57,8.74;13,88.56,300.38,141.12,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,314.51,288.86,156.55,8.74">Thesaurus-enhanced search interfaces</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Shiri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Revie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,480.42,288.86,44.49,8.74;13,88.56,300.38,80.83,8.74">Journal of Information Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="122" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,74.33,311.90,450.53,8.74;13,88.56,323.36,131.12,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,152.38,311.90,224.72,8.74">Interactive thesaurus navigation: intelligence rules OK?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,380.10,311.90,144.77,8.74;13,88.56,323.36,80.83,8.74">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="59" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,74.33,334.88,450.58,8.74;13,88.56,346.40,233.07,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,261.79,334.88,263.12,8.74;13,88.56,346.40,55.70,8.74">Subject knowledge improves interactive query expansion assisted by a thesaurus</title>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Sihvonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pertti</forename><surname>Vakkari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,150.78,346.40,105.57,8.74">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="673" to="690" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,74.33,357.86,450.62,8.74;13,88.56,369.38,436.38,8.74;13,88.56,380.90,86.69,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,320.89,357.86,204.06,8.74;13,88.56,369.38,171.36,8.74">A study of user interaction with a concept-based interactive query expansion support tool</title>
		<author>
			<persName coords=""><forename type="first">Hideo</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,282.48,369.38,45.58,8.74">ECIR 2004</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Mcdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tait</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,74.33,392.36,450.68,8.74;13,88.56,403.88,436.34,8.74;13,88.56,415.40,184.61,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,284.28,392.36,240.74,8.74;13,88.56,403.88,208.01,8.74">Ontology as a search-tool: a study of real users&apos; query formulation with and without conceptual support</title>
		<author>
			<persName coords=""><forename type="first">Sari</forename><surname>Suomela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,319.26,403.88,45.47,8.74">ECIR 2005</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fernández-Luna</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,74.33,426.86,450.60,8.74;13,88.56,438.38,244.24,8.74" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="13,193.02,426.86,136.23,8.74">Thesaurus for the Social Sciences</title>
		<author>
			<persName coords=""><forename type="first">Hannelore</forename><surname>Schott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Informations-Zentrum Socialwissenschaften</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Bonn</pubPlace>
		</imprint>
	</monogr>
	<note>German -English. English -German</note>
</biblStruct>

<biblStruct coords="13,78.91,449.84,446.03,8.74;13,88.56,461.36,436.42,8.74;13,88.56,472.88,436.46,8.74;13,88.56,484.34,196.68,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,186.14,449.84,299.55,8.74">The GIRT Data in the Evaluation of CLIR Systems -from 1997 Until</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Kluck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,88.56,461.36,436.42,8.74;13,88.56,472.88,125.93,8.74">Comparative Evaluation of Multilingual Information Access Systems: 4th Workshop of the Cross-Language Evaluation Forum, CLEF 2003</title>
		<title level="s" coord="13,494.38,472.88,30.64,8.74;13,88.56,484.34,109.79,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003-08-21">2003. 2003. August 21-22, 2003. 2004</date>
			<biblScope unit="volume">3237</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,78.91,495.86,446.11,8.74;13,88.56,507.38,348.52,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,240.19,495.86,284.83,8.74;13,88.56,507.38,42.71,8.74">An association-based method for automatic indexing with a controlled vocabulary</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Plaunt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">A</forename><surname>Norgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,138.06,507.38,226.28,8.74">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="888" to="902" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,78.91,518.84,445.97,8.74;13,88.56,530.36,182.25,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,191.06,518.84,231.88,8.74">Advanced Search Technology for Unfamiliar Metadata</title>
		<author>
			<persName coords=""><forename type="first">Fred</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,432.42,518.84,92.46,8.74;13,88.56,530.36,44.18,8.74">Third IEEE Metadata Conference</title>
		<meeting><address><addrLine>Bethesda, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-04">1999. April 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,78.91,541.88,446.02,8.74;13,88.56,553.34,424.37,8.74;13,70.56,564.86,454.38,8.74;13,88.56,576.38,75.58,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,249.99,541.88,274.94,8.74;13,88.56,553.34,107.77,8.74">Full text retrieval based on probabilistic equations with coefficients fitted by logistic regression</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,213.54,553.34,193.58,8.74;13,174.36,564.86,205.40,8.74">The Second Text Retrieval Conference (TREC-2)</title>
		<title level="s" coord="13,453.96,564.86,70.99,8.74;13,88.56,576.38,71.45,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K 14</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1994">1994. 2003. 2002. 2785</date>
		</imprint>
	</monogr>
	<note>Cross-Language Retrieval Experiments at CLEF</note>
</biblStruct>

<biblStruct coords="13,78.91,587.84,446.14,8.74;13,88.56,599.36,285.05,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,212.81,587.84,312.24,8.74;13,88.56,599.36,122.08,8.74">Multilingual Information Retrieval Using Machine Translation, Relevance Feedback and Decompounding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,217.98,599.36,87.07,8.74">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="149" to="182" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,78.91,610.88,445.95,8.74;13,88.56,622.34,436.40,8.74;13,88.56,633.86,436.38,8.74;13,88.56,645.38,314.70,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,266.98,610.88,257.88,8.74;13,88.56,622.34,196.56,8.74">UC Berkeley at CLEF 2003 --Russian Language Experiments and Domain-Specific Cross-Language Retrieval</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Petras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,306.00,622.34,218.96,8.74;13,88.56,633.86,344.92,8.74">Comparative Evaluation of Multilingual Information Access Systems: 4th Workshop of the Cross-Language Evaluation Forum, CLEF 2003</title>
		<title level="s" coord="13,173.63,645.38,142.64,8.74">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003-08-21">2003. August 21-22, 2003. 2004</date>
			<biblScope unit="volume">3237</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
