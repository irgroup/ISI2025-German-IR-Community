<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,420.39,148.86,44.12,15.15;1,99.61,170.78,403.78,15.15;1,258.89,192.69,85.23,15.15">2005: Experiments with the ImageCLEF St Andrew&apos;s Collection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,210.50,226.59,81.65,8.74"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gjones@computing.dcu.ie</email>
						</author>
						<author>
							<persName coords="1,314.84,226.59,77.66,8.74"><forename type="first">Kieran</forename><surname>Mcdonald</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University at CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Centre for Digital Video Processing</orgName>
								<orgName type="department" key="dep2">School of Computing Dublin</orgName>
								<orgName type="institution">City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,420.39,148.86,44.12,15.15;1,99.61,170.78,403.78,15.15;1,258.89,192.69,85.23,15.15">2005: Experiments with the ImageCLEF St Andrew&apos;s Collection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5E8EF0A80EF83C1E302BD2E9D0B2B796</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Content-Based Image Retrieval, Pseudo Relevance Feedback, Document selection for feedback</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The aim of the Dublin City University participation in the CLEF 2005 ImageCLEF St Andrew's Collection task was to explore an alternative approach to exploiting text annotation and content-based retrieval in a novel combined way for pseudo relevance feedback (PRF). This method combines evidence from retrieved lists generated using text and content-based retrieval to determine which documents will be assumed relevant for the PRF process. Unfortunately the results show that while standard textbased PRF improves upon a no feedback text baseline, at present our new approach to combining evidence from text and content-based retrieval does not give further improve improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dublin City University's participation in the CLEF 2005 ImageCLEF St Andrew's collection task explored a novel approach to pseudo relevance feedback (PRF) combining evidence from separate text and content-based retrieval. The underlying text retrieval system is based on a standard Okapi model for document ranking and PRF <ref type="bibr" coords="1,328.29,670.38,9.97,8.74" target="#b0">[1]</ref>. Three sets of experiments are reported for the following topic languages: Chinese (simplified), Dutch, French, German, Greek, Italian, Japanese, Portuguese, Russian and Spanish (european), along with corresponding monolingual English results as a baseline for comparison. Topics were translated into English using the online Babelfish machine translation engine. The first set of experiments establish baseline retrieval performance without PRF, the second experiments incorporate a standard PRF stage, and finally the third set investigates our new combined method for PRF. This paper is organised as follows: Section 2 briefly outlines the details of our standard retrieval system and describes our novel PRF method, Section 3 gives results for our experiments, and finally Section 4 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Standard Retrieval Approach</head><p>Our basic experimental retrieval system is a local implementation of the standard Okapi retrieval model <ref type="bibr" coords="2,120.42,223.03,9.97,8.74" target="#b0">[1]</ref>. Documents and search topics are processed to remove stopwords from the standard SMART list, and suffix stripped using the Snowball implementation of Porter stemming <ref type="bibr" coords="2,485.27,234.99,10.52,8.74" target="#b1">[2]</ref>  <ref type="bibr" coords="2,499.72,234.99,9.96,8.74" target="#b2">[3]</ref>. The resulting terms are weighted using the standard BM25 weighting scheme with parameters selected using the CLEF 2004 ImageCLEF test data as a training set.</p><p>Standard PRF was carried out using query expansion. The top ranked documents from a baseline retrieval run were assumed relevant. Terms from these documents were ranked using the Robertson selection value (RSV), and the top ranked terms added to the original topic statement. The parameters of the PRF were again selected using the CLEF 2004 ImageCLEF test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Combining Text and Content-based Retrieval for PRF</head><p>The preceding text-based retrieval methods have been shown to work reasonably effectively for the St Andrew's ImageCLEF task in earlier workshops <ref type="bibr" coords="2,332.20,364.95,9.97,8.74" target="#b3">[4]</ref>. However, this approach makes no use of the document or topic images. In our participation in the CLEF 2004 ImageCLEF task we attempted to improve text only based retrieval by performing a standard data fusion summation combination of retrieved ranked lists from text retrieval and the provided context-based retrieval lists generated using the GIFT/Viper system. The results of these combined lists showed little difference from the text-only runs.</p><p>Analysis of the GIFT/Viper only runs showed them to have very poor recall, but reasonable precision at high cutoff levels. However, further investigation of this showed that this good high cutoff precision is largely attributable to a good match on the topic image which is part of the document collection. Our analysis suggest that there is little to be gained from data fusion in this way, certainly when content-based retrieval is based on low-level features. Indeed it is perhaps surprising that this method does not degrade the text only retrieval runs.</p><p>Nevertheless, we were interested to see if the evidence from content-based retrieval runs might be usefully combined with the text retrieval runs in a different way. We hypothesize that documents retrieved by both the text-based and content-based methods are more likely to be relevant than documents retrieved by only one system. We adapted the standard PRF method to incorporate this hypothesis as follows. Starting from the top of lists retrieved independently using textbased and content-based retrieval, we look for documents retrieved by both systems. Documents retrieved by both systems are assumed to be relevant and used as the relevant document set for the query expansion PRF method outlined in the previous section. The expanded query is then applied to the text only retrieval system.</p><p>For this investigation content-based retrieval used our own image retrieval system based on standard low-level colour, edge and texture features. The colour comparison was based on 5 × 5 regional colour with HSV histogram dimensions 16 × 4 × 4. Edge comparison used Canny edge with 5 × 5 regions quantized into 8 directions. Texture matching was based on the first 5 DCT co-efficients, each quantized into 3 values for 3 × 3 regions. The scores of the three components were then combined in a weighted sum and the overall scores used to rank the content-based retrieved list. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>The settings for Okapi model were optimized using the CLEF 2004 ImageCLEF English languages topics as follows k1 = 1.0 and b = 0.5. These parameters were used for all results reported in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline Retrieval</head><p>Table <ref type="table" coords="3,118.87,498.03,4.98,8.74" target="#tab_0">1</ref> shows baseline retrieval results for the Okapi model without application of feedback. Monolingual results for English topics are shown in the left side column for each row. Results for each translated topic language relative to English are then shown in the other columns. From these results we can see that cross-language performance is degraded relative to monolingual by between around 20% and 45% for the different topic languages with respect to MAP, and by between 150 and 450 for the total number of relevant documents retrieved. These results are in line with those are would be expected for short documents with cross-language topics translated using a standard commercial machine translation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Standard Pseudo Relevance Feedback</head><p>Results using the CLEF 2004 ImageCLEF data with the English language topics were shown to be optimized on average by assuming the top 15 documents retrieved to be relevant and by adding the resulting top 10 ranked terms to the original topic, with the original terms upweighted by a factor of 3.5 relative to the expansion terms. Performance for individual topic languages may be improved slightly by selecting the parameters separately, but this effect is likely to be small. Table <ref type="table" coords="3,132.77,687.77,4.98,8.74" target="#tab_1">2</ref> shows results for applying PRF with these settings. The form of the results table is the same as that in Table <ref type="table" coords="3,208.87,699.72,3.88,8.74" target="#tab_0">1</ref>. From this table we can see that PRF is effective for this task for all topic languages. Further the reduction relative to monolingual in each case is also generally reduced. Again this trend is commonly observed for the cross-language retrieval tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Text and Image Combined Pseudo Relevance Feedback</head><p>The combination of features for content-based image retrieval was also optimized using the CLEF 2004 ImageCLEF task using only the topic and document images. Based on this optimization the matching scores of the features were combined 0.5 × colour + 0.3 × edge + 0.2 × texture.</p><p>The selection depth of documents from the ranked retrieved text and image lists to form the assumed relevant set was also set using the CLEF 2004 ImageCLEF data. We carried our extensive investigation of the optimal search depth for a range of topic languages. There was no apparent reliable trend across the language pairs, and we could not be confident that values chosen for a particular pair on the training data would be suitable for a new topic set. Based on analysis of overall trends across the set of language pairs we decided to set the search to a depth of retrieved text list to 180 documents and the visual list to a rank of 20 documents. Documents occurring in both lists down to these rank positions were assumed to relevant and used for term selection in text based PRF. Results from these experiments are shown in Table <ref type="table" coords="4,394.04,555.91,3.88,8.74" target="#tab_2">3</ref>. Comparing these results to those using the standard PRF in Table <ref type="table" coords="4,272.10,567.87,4.98,8.74" target="#tab_1">2</ref> we observe very little change to the results. In general the results for our new method are marginally reduced in comparison to the standard method.</p><p>Based on our findings with exploration of the training set, it is likely that these results could be improved marginally by adjusting the search depth of the lists for the PRF stage, however post fitting to the test data in this way does not represent a realistic search scenario and it is unlikely to give any clear increase in results. The reasons for the observed behaviour reamin to be investigated. For example, we need to consider differences in selected assumed relevant documents using the two PRF methods. Also we need to better understand the extent to which relevant documents can be found in the lists. Certainly the results for CLEF 2004 ImageCLEF suggest that the low recall for image only retrieval means that we may be ignoring too many relevant documents that are retrieved successfully by the text list, and that possibly some form of hybrid selection method might be better. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Further Work</head><p>Results from our experiments for the CLEF 2005 St Andrew's ImageCLEF task show expected performance trends for baseline cross-language retrieval using the standard Okapi models. Our proposed new PRF approach combining retrieval lists from text and image retrieval for this task failed to improve on results obtained using a standard PRF method. Further work is required to explore the behaviour of this method with a view to reformulating or extending it to successfully integrate text and image methods for annotated image retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,116.29,118.94,370.42,251.98"><head>Table 1 :</head><label>1</label><figDesc>Baseline retrieval runs using Babelfish topic translation.</figDesc><table coords="3,116.29,140.60,370.42,230.32"><row><cell></cell><cell></cell><cell cols="7">English Chinese (s) Dutch French German Greek</cell></row><row><cell>Prec.</cell><cell>5 docs</cell><cell>0.557</cell><cell cols="2">0.264</cell><cell>0.471</cell><cell>0.393</cell><cell>0.486</cell><cell>0.379</cell></row><row><cell></cell><cell>10 docs</cell><cell>0.500</cell><cell cols="2">0.254</cell><cell>0.436</cell><cell>0.375</cell><cell>0.418</cell><cell>0.404</cell></row><row><cell></cell><cell>15 docs</cell><cell>0.460</cell><cell cols="2">0.250</cell><cell>0.402</cell><cell>0.355</cell><cell>0.374</cell><cell>0.386</cell></row><row><cell></cell><cell>20 docs</cell><cell>0.427</cell><cell cols="2">0.230</cell><cell>0.377</cell><cell>0.323</cell><cell>0.343</cell><cell>0.370</cell></row><row><cell cols="2">Av Precision</cell><cell>0.355</cell><cell cols="2">0.189</cell><cell>0.283</cell><cell>0.244</cell><cell>0.284</cell><cell>0.249</cell></row><row><cell cols="2">% chg.</cell><cell>-</cell><cell cols="2">-46.8%</cell><cell cols="4">-20.3% -31.3% -20.0% -29.9%</cell></row><row><cell cols="2">Rel. Ret.</cell><cell>1550</cell><cell cols="2">1168</cell><cell>1213</cell><cell>1405</cell><cell>1337</cell><cell>1107</cell></row><row><cell cols="2">chg. Rel. Ret.</cell><cell>-</cell><cell cols="2">-382</cell><cell>-337</cell><cell>-145</cell><cell>-213</cell><cell>-443</cell></row><row><cell></cell><cell cols="8">English Italian Japanese Portuguese Russian Spanish (e)</cell></row><row><cell>Prec.</cell><cell>5 docs</cell><cell>0.557</cell><cell>0.300</cell><cell>0.393</cell><cell></cell><cell>0.407</cell><cell>0.379</cell><cell>0.336</cell></row><row><cell cols="2">10 docs</cell><cell>0.500</cell><cell>0.296</cell><cell>0.368</cell><cell></cell><cell>0.368</cell><cell>0.354</cell><cell>0.325</cell></row><row><cell cols="2">15 docs</cell><cell>0.460</cell><cell>0.269</cell><cell>0.336</cell><cell></cell><cell>0.343</cell><cell>0.329</cell><cell>0.307</cell></row><row><cell cols="2">20 docs</cell><cell>0.427</cell><cell>0.266</cell><cell>0.311</cell><cell></cell><cell>0.323</cell><cell>0.314</cell><cell>0.280</cell></row><row><cell cols="2">Av Precision</cell><cell>0.355</cell><cell>0.216</cell><cell>0.259</cell><cell></cell><cell>0.243</cell><cell>0.247</cell><cell>0.207</cell></row><row><cell cols="2">% chg.</cell><cell>-</cell><cell>-39.2%</cell><cell cols="2">-27.0%</cell><cell>-31.5%</cell><cell>-30.4%</cell><cell>-41.7%</cell></row><row><cell cols="2">Rel. Ret.</cell><cell>1550</cell><cell>1181</cell><cell>1304</cell><cell></cell><cell>1263</cell><cell>1184</cell><cell>1227</cell></row><row><cell cols="2">chg. Rel. Ret.</cell><cell>-</cell><cell>-369</cell><cell>-246</cell><cell></cell><cell>-287</cell><cell>-366</cell><cell>-323</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,116.29,118.94,370.42,251.98"><head>Table 2 :</head><label>2</label><figDesc>Text only PRF retrieval runs using Babelfish topic translation.</figDesc><table coords="4,116.29,140.60,370.42,230.32"><row><cell></cell><cell></cell><cell cols="7">English Chinese (s) Dutch French German Greek</cell></row><row><cell>Prec.</cell><cell>5 docs</cell><cell>0.529</cell><cell cols="2">0.257</cell><cell>0.450</cell><cell>0.407</cell><cell>0.443</cell><cell>0.439</cell></row><row><cell></cell><cell>10 docs</cell><cell>0.500</cell><cell cols="2">0.275</cell><cell>0.425</cell><cell>0.407</cell><cell>0.407</cell><cell>0.432</cell></row><row><cell></cell><cell>15 docs</cell><cell>0.467</cell><cell cols="2">0.274</cell><cell>0.407</cell><cell>0.393</cell><cell>0.393</cell><cell>0.410</cell></row><row><cell></cell><cell>20 docs</cell><cell>0.432</cell><cell cols="2">0.261</cell><cell>0.382</cell><cell>0.373</cell><cell>0.375</cell><cell>0.396</cell></row><row><cell cols="2">Av Precision</cell><cell>0.364</cell><cell cols="2">0.213</cell><cell>0.308</cell><cell>0.283</cell><cell>0.308</cell><cell>0.302</cell></row><row><cell cols="2">% chg.</cell><cell>-</cell><cell cols="2">-41.5%</cell><cell cols="4">-15.4% -22.3% -15.4% -17.0%</cell></row><row><cell cols="2">Rel. Ret.</cell><cell>1648</cell><cell cols="2">1320</cell><cell>1405</cell><cell>1580</cell><cell>1427</cell><cell>1219</cell></row><row><cell cols="2">chg. Rel. Ret.</cell><cell>-</cell><cell cols="2">-328</cell><cell>-243</cell><cell>-68</cell><cell>-221</cell><cell>-429</cell></row><row><cell></cell><cell cols="8">English Italian Japanese Portuguese Russian Spanish (e)</cell></row><row><cell>Prec.</cell><cell>5 docs</cell><cell>0.529</cell><cell>0.264</cell><cell>0.350</cell><cell></cell><cell>0.379</cell><cell>0.371</cell><cell>0.336</cell></row><row><cell cols="2">10 docs</cell><cell>0.500</cell><cell>0.279</cell><cell>0.346</cell><cell></cell><cell>0.346</cell><cell>0.357</cell><cell>0.321</cell></row><row><cell cols="2">15 docs</cell><cell>0.467</cell><cell>0.255</cell><cell>0.326</cell><cell></cell><cell>0.324</cell><cell>0.350</cell><cell>0.295</cell></row><row><cell cols="2">20 docs</cell><cell>0.432</cell><cell>0.245</cell><cell>0.329</cell><cell></cell><cell>0.316</cell><cell>0.338</cell><cell>0.286</cell></row><row><cell cols="2">Av Precision</cell><cell>0.354</cell><cell>0.215</cell><cell>0.268</cell><cell></cell><cell>0.247</cell><cell>0.280</cell><cell>0.224</cell></row><row><cell cols="2">% chg.</cell><cell>-</cell><cell>-40.9%</cell><cell cols="2">-26.4%</cell><cell>-32.1%</cell><cell>-23.1%</cell><cell>-38.5%</cell></row><row><cell cols="2">Rel. Ret.</cell><cell>1648</cell><cell>1223</cell><cell>1331</cell><cell></cell><cell>1364</cell><cell>1335</cell><cell>1360</cell></row><row><cell cols="2">chg. Rel. Ret.</cell><cell>-</cell><cell>-425</cell><cell>-317</cell><cell></cell><cell>-284</cell><cell>-313</cell><cell>-288</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,90.00,118.94,423.00,262.00"><head>Table 3 :</head><label>3</label><figDesc>PRF retrieval runs incorporating text and image retrieval evidence using Babelfish topic translation.</figDesc><table coords="5,116.29,150.62,370.42,230.32"><row><cell></cell><cell></cell><cell cols="7">English Chinese (s) Dutch French German Greek</cell></row><row><cell>Prec.</cell><cell>5 docs</cell><cell>0.529</cell><cell cols="2">0.264</cell><cell>0.443</cell><cell>0.407</cell><cell>0.443</cell><cell>0.414</cell></row><row><cell></cell><cell>10 docs</cell><cell>0.504</cell><cell cols="2">0.268</cell><cell>0.432</cell><cell>0.411</cell><cell>0.414</cell><cell>0.429</cell></row><row><cell></cell><cell>15 docs</cell><cell>0.460</cell><cell cols="2">0.271</cell><cell>0.402</cell><cell>0.393</cell><cell>0.391</cell><cell>0.405</cell></row><row><cell></cell><cell>20 docs</cell><cell>0.432</cell><cell cols="2">0.259</cell><cell>0.375</cell><cell>0.373</cell><cell>0.371</cell><cell>0.393</cell></row><row><cell cols="2">Av Precision</cell><cell>0.365</cell><cell cols="2">0.210</cell><cell>0.306</cell><cell>0.282</cell><cell>0.308</cell><cell>0.298</cell></row><row><cell cols="2">% chg.</cell><cell>-</cell><cell cols="2">-42.7%</cell><cell cols="4">-16.2% -22.7% -15.6% -18.4%</cell></row><row><cell cols="2">Rel. Ret.</cell><cell>1652</cell><cell cols="2">1318</cell><cell>1405</cell><cell>1578</cell><cell>1428</cell><cell>1218</cell></row><row><cell cols="2">chg. Rel. Ret.</cell><cell>-</cell><cell cols="2">-334</cell><cell>-247</cell><cell>-74</cell><cell>-224</cell><cell>-434</cell></row><row><cell></cell><cell cols="8">English Italian Japanese Portuguese Russian Spanish (e)</cell></row><row><cell>Prec.</cell><cell>5 docs</cell><cell>0.529</cell><cell>0.264</cell><cell>0.343</cell><cell></cell><cell>0.371</cell><cell>0.371</cell><cell>0.343</cell></row><row><cell cols="2">10 docs</cell><cell>0.504</cell><cell>0.279</cell><cell>0.350</cell><cell></cell><cell>0.350</cell><cell>0.354</cell><cell>0.318</cell></row><row><cell cols="2">15 docs</cell><cell>0.460</cell><cell>0.248</cell><cell>0.321</cell><cell></cell><cell>0.319</cell><cell>0.350</cell><cell>0.291</cell></row><row><cell cols="2">20 docs</cell><cell>0.432</cell><cell>0.241</cell><cell>0.325</cell><cell></cell><cell>0.309</cell><cell>0.339</cell><cell>0.284</cell></row><row><cell cols="2">Av Precision</cell><cell>0.365</cell><cell>0.215</cell><cell>0.268</cell><cell></cell><cell>0.247</cell><cell>0.279</cell><cell>0.224</cell></row><row><cell cols="2">% chg.</cell><cell>-</cell><cell>-41.1%</cell><cell cols="2">-26.6%</cell><cell>-32.3%</cell><cell>-23.6%</cell><cell>-38.6%</cell></row><row><cell cols="2">Rel. Ret.</cell><cell>1652</cell><cell>1227</cell><cell>1336</cell><cell></cell><cell>1366</cell><cell>1331</cell><cell>1361</cell></row><row><cell cols="2">chg. Rel. Ret.</cell><cell>-</cell><cell>-425</cell><cell>-316</cell><cell></cell><cell>-286</cell><cell>-321</cell><cell>-291</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,105.50,552.40,407.50,8.74;5,105.50,564.35,407.50,8.74;5,105.50,576.31,169.70,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,487.26,552.40,25.73,8.74;5,105.50,564.35,47.29,8.74">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,277.81,564.35,235.19,8.74;5,105.50,576.31,42.07,8.74">Proceedings of the Third Text REtrieval Conference (TREC-3)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Third Text REtrieval Conference (TREC-3)</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,596.23,225.81,9.02" xml:id="b1">
	<monogr>
		<ptr target="http://snowball.tartarus.org/" />
		<title level="m" coord="5,105.50,596.23,70.81,8.74">Snowball toolkit</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,616.16,326.18,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,171.96,616.16,140.77,8.74">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,321.31,616.16,36.54,8.74">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="10" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,636.09,407.50,8.74;5,105.50,648.04,407.50,8.74;5,105.50,660.00,407.51,8.74;5,105.50,671.95,149.44,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,105.50,648.04,407.50,8.74;5,105.50,660.00,27.61,8.74">Dublin City University at CLEF 2004: Experiments with the ImageCLEF St Andrew&apos;s Collection</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Groves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khasin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Lam-Adesina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mellebeek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,140.90,660.00,372.11,8.74;5,105.50,671.95,64.54,8.74">Proceedings of the CLEF 2004: Workshop on Cross-Language Information Retrieval and Evaluation</title>
		<meeting>the CLEF 2004: Workshop on Cross-Language Information Retrieval and Evaluation<address><addrLine>Bath, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
