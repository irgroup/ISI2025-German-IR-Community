<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,95.28,100.07,412.13,14.17;1,106.80,122.03,389.18,14.17">Categorizing and Annotating Medical Images by Retrieving Terms Relevant to Visual Features</title>
				<funder ref="#_EeMhWsD">
					<orgName type="full">Clare Boothe Luce Program of the Henry Luce Foundation</orgName>
				</funder>
				<funder ref="#_82hfCmE">
					<orgName type="full">Howard Hughes Medical Institute Cascade Mentoring Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,215.40,154.90,77.48,9.14"><forename type="first">Desislava</forename><surname>Petkova</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mount Holyoke College</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.36,154.90,68.30,9.14;1,387.60,151.90,1.41,7.31"><forename type="first">Lisa</forename><surname>Ballesteros</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mount Holyoke College</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,95.28,100.07,412.13,14.17;1,106.80,122.03,389.18,14.17">Categorizing and Annotating Medical Images by Retrieving Terms Relevant to Visual Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BAA50F2294194D72BB616120F580E7F6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing Cross-language Retrieval</term>
					<term>Image Annotation</term>
					<term>Categorization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Images are difficult to classify and annotate but the availability of digital image databases creates a constant demand for tools that automatically analyze image content and describe it with either a category or a set of words. We develop two clusterbased cross-media relevance models that effectively categorize and annotate images by adapting a cross-lingual retrieval technique to choose the terms most likely associated with the visual features of an image. We also identify several important distinctions between assigning categories and assigning words.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The exponential growth of multi-media information has created a compelling need for innovative tools for managing, retrieving, presenting, and analyzing image collections. Medical databases, for example, continue to grow as hospitals and research institutes produce thousands of medical images daily. The design and development of image retrieval systems will support a variety of tasks, including image retrieval, auto-illustration of text documents, medical diagnostics, organizing image collections such as digital photo albums, and browsing Image retrieval techniques can be classified into two types, content based image retrieval (CBIR) and text-based image retrieval (TBIR). CBIR attempts to find images based on visual similarities such as shape or texture. TBIR techniques retrieve images based on semantic relationships rather than visual features and require that descriptive words or annotations have been previously assigned to each image. For collections of realistic size, it is impractical to rely exclusively on manual annotation because the process is both time-consuming and subjective. The task is even more challenging for special collections such as medical databases since they require expensively trained professionals to do the annotation. As a practical alternative, automatic annotation can either complement or substitute manual annotation.</p><p>The goal of automatic image annotation is to assign semantically descriptive words to unannotated images. As with most tasks involving natural language processing, we assume that a training collection of already annotated images is available, which the system can use to learn what correlations exist between words and visual components or visterms. We specify the task further by considering annotation to be a cross-lingual retrieval problem: Two languages -textual and visual -are both used to describe images, and we want to infer the textual representation of an image given its visual representation. Therefore, we can think of words being the target language and visterms being the source language. Of course, the language of visterms is entirely synthetic but a CLIR system does not require specialized linguistic theory and knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Other researchers have proposed methods for modeling the relationships between words and visual components. Mori et al divide images into regions and then use the co-occurrence of words and regions to make nonsmoothed maximum likelihood estimates <ref type="bibr" coords="2,389.17,200.73,9.90,9.14" target="#b8">[9]</ref>. Duygulu et al apply a segmentation algorithm to generate image blobs and then use a Machine Translation model to assign words as a form of multi-modal translation from blobs to words <ref type="bibr" coords="2,398.87,224.61,9.90,9.14" target="#b2">[3]</ref>. More recently, Jeon et al apply a Maximum Entropy model that treats annotation as a discrete stochastic process whose unknown parameters are word probabilities <ref type="bibr" coords="2,282.44,248.49,9.90,9.14" target="#b5">[6]</ref>.</p><p>Our approach is a modification of the Cross-media Relevance Model (CMRM) developed by Jeon et al <ref type="bibr" coords="2,137.71,272.49,9.90,9.14" target="#b4">[5]</ref>. In this case, the visterms of an image to be annotated constitute a query and all candidate words are ranked in terms of their relevance to the visual representation. An annotation of any length can be created by selecting the n highest ranked words. More precisely, using a collection T of training images J, the joint probability of observing a word w and the set of visterms derived from an unannotated image I = {v 1 , ..., v m } is computed as:</p><formula xml:id="formula_0" coords="2,201.12,339.52,200.67,28.80">P (w, v 1 , ..., v m ) = J∈T P (J)P (w|J) m i=1 P (v i |J)</formula><p>where P (w|J) and P (v|J) are maximum-likelihood estimates smoothed with collection frequencies.<ref type="foot" coords="2,108.36,388.43,3.97,5.86" target="#foot_0">1</ref> </p><formula xml:id="formula_1" coords="2,209.52,419.85,182.66,50.30">P (w|J) = (1 -α) #(w, J) |J| + α #(w, T ) |T | P (v|J) = (1 -β) #(v, J) |J| + β #(v, T ) |T |</formula><p>Therefore, CMRM uses word-visterm co-occurrences across training images to estimate the probability of associating words and visterms together. But since this method computes the word and visterm distributions P (•|J) of each image separately, it does not take into account global similarity patterns, i.e. how individual images and their representations are related to each other. This shortcoming can be compensated by extracting and incorporating information from groups of similar imagesclusters -created by examining the overall corpus structure.</p><p>Document clustering within the framework of full text retrieval has been investigated by Liu et al <ref type="bibr" coords="2,101.74,565.05,9.99,9.14" target="#b7">[8]</ref>. They define two cluster-based models: Cluster Query Likelihood (CQL) and Cluster-based Document Model (CBDM). Both explore across-document word co-occurrence patterns in addition to within-document occurrence patterns to improve the ranking of documents in response to user queries. CQL directly ranks clusters based on P (Q|C), the probability of a cluster C generating the query Q, while CBDM ranks documents but smooths their language models with the models of respective clusters. Liu et al show that clustering improves retrieval performance indicating that clusters provide more representative statistics of word distributions because they combine multiple related documents.</p><p>We adapt these techniques to annotate and categorize images by extending the Cross-media Relevance Model to take advantage of cluster statistics in addition to image statistics.</p><formula xml:id="formula_2" coords="3,105.00,75.72,411.29,71.97">P (w, v 1 , ..., vm) = X C∈T P (C)P (w|C) m Y i=1 P (v i |C) P (w|C) = (1 -γ) #(w, C) |C| + γ #(w, T ) |T | P (v|C) = (1 -δ) #(v, C) |C| + δ #(v, T ) |T | P (w, v 1 , ..., vm) = X J ∈T P (J)P (w|J) m Y i=1 P (v i |J) P (w|J) = (1 -α) #(w, J) |J| + α #(w, C J ) |C J | P (v|J) = (1 -β) #(v, J) |J| + β #(v, C J ) |C J |</formula><p>Mathematical definitions of CQL (left) and CBDM (right). Note that clusters C play two different roles -ranking in CQL and smoothing in CBDM.</p><p>The motivation is that by analyzing collection-wide co-occurrence patterns, a cluster-based approach to annotation can achieve a better estimation of word-visterm relationships. Clusters, viewed as large pseudo-images, have more words and visterms and therefore their language models P (•|C) can be approximated better than those of single images. In short, even if no prior knowledge about the collection is available, we can learn from its similarity structure by inferring wordvisterm co-occurrences from similar images. For example, indirect relationships between words and visterms that do not occur together can be identified when there exist intermediate visterms with which they co-occur independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology of categorization and annotation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">From categories to concepts</head><p>Textual representations provided for ImageCLEFmed 2005 are clearly categories rather than annotations. Training images are divided into disjoint groups by being put into one of 57 folders, and each folder is given a short description (multi-axial code). Since we are interested in both categorizing and annotating images, we first need to create more realistic annotations. We achieve this by breaking up categorical records into sets of individual concepts.</p><p>We define a "concept" to be a comma-separated string, creating a restricted vocabulary of 46 distinct concepts. Some of these are literal dictionary words (e.g. 'x-ray" and 'spine'), others are sequences of words (e.g. 'plain radiography' and 'radio carpal joint'), and they all identify a single distinctive image property. For example, the third concept in a categorical description indicates body orientation -the choices are 'coronal', 'sagittal', 'axial' and 'other orientation'. Clearly, it does not make sense to have 'other' as a concept on its own.  Thus we get two kinds of textual representations per image -a category and an annotation. We also recognize the first important difference between the two. Concepts do not point directly to objects in the images (there is one object per image anyway) but describe very high-level, specialized attributes which are not reflected directly by any visual feature. As a result, images that are apparently different can have very similar annotations, i.e. share many concepts (Figure <ref type="figure" coords="4,90.00,62.25,3.88,9.14" target="#fig_0">1</ref>). In contrast, all images classified in the same category are visually similar. In the rest of the paper, we refer to concepts and categories jointly as terms.</p><p>We also observe that concepts have an unusual distribution where the six most frequent ones account for more than 75% of the total number of occurrences (Figure <ref type="figure" coords="4,406.89,98.13,3.88,9.14" target="#fig_2">3</ref>). In fact, one concept -'x-ray' -appears in every single image. Both CQL and CBDM would likely be biased in favor of these very frequent concepts, tending to select them rather than rare ones. Since we set the models to generate fixed-length annotations of six concepts (this is the maximum length of training annotations), we would expect the same set of concepts to be assigned over and over.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Describing image content</head><p>Given the fact that we make a distinction between categories and concepts, we need to redefine the task. We are interested both in selecting the category to which an image belongs and in choosing concepts to describe it. (And of course the appropriate concepts and category are themselves related.) Therefore, we are going to evaluate each model on two tasks simultaneously -annotation and categorization.</p><p>Recall that both CQL and CBDM compute a set of probabilities P (w i |I), i = 1...|V |, based on the visterms of an image I. These probabilities are used to rank terms w according to their suitability to describe the content of I. The only restriction on the vocabulary V is that it is a finite set of discrete elements. Both categories and individual concepts satisfy this requirement therefore we can use the same implementation to assign either categories or concepts by only changing the input to the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Assigning categories</head><p>We consider each category to be an annotation of length 1. By learning relationships between categories and visterms we can categorize new images directly by assigning the term with the highest probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Assigning concepts</head><p>We divide categories into concepts and work with annotations of various lengths. By learning relationships between concepts and visterms we can annotate new images directly by assigning several of the highest probability concepts. Alternatively, we can categorize new images indirectly by representing categories as combinations of concepts:</p><formula xml:id="formula_3" coords="5,166.08,71.44,270.76,28.68">P (category) = P (concept 1 , ..., concept k ) = k i=1 P (concept i )</formula><p>4 Data processing and experimental setup Preliminary image processing involves extracting visual features and obtaining an image vocabulary of visterms. Briefly, our representations are generated in the following way. Each image is grid partitioned into regions and the complete set of image regions is partitioned into disjoint groups based on corresponding feature vectors. All regions in a group are given the same unique identifier or visterm. Once image processing is complete, our approach relies on a model of the correspondences between terms and visterms, inferred from a set of training images that have been previously annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feature extraction and visterm generation</head><p>The dataset consists of 10000 images, divided into a training set of 9000 and a test set of 1000. First, each of these images is scaled to 256×256 pixels (regardless of the original aspect ratio) and divided into a 5×5 square grid. This produces 250,000 regions to be discretized into visterms. Regions are clustered on the basis of visual similarities and each cluster is assigned a unique identifier. Since the ImageCLEFmed collection consists entirely of black-and-white images, we only consider visual features that analyze texture. More specifically, we apply two texture analysis features -Gabor and Tamura.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Gabor energy</head><p>The Gabor Energy method measures the similarity between image neighborhoods and specially defined masks to detect spatially local patterns such as oriented lines, edges and blobs <ref type="bibr" coords="5,479.08,388.06,9.99,9.14" target="#b3">[4]</ref>. We use a MATLAB implementation courtesy of Shaolei Feng at the Center for Intelligent Information Retrieval, University of Massachusetts at Amherst. This feature computes a 12-bin histogram per image region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Tamura texture</head><p>The Tamura features -Coarseness, Directionality and Contrast -are intended to reproduce human visual perception. They attempt to quantify intuitive information such as roughness, presence of orientation, and picture quality in terms of factors like sharpness of edges and period of repeating patterns. We use the FIRE Flexible Image Retrieval Engine to extract Tamura features <ref type="bibr" coords="5,469.56,504.10,9.99,9.14" target="#b1">[2]</ref>. Given an input image, FIRE creates three output partial images, one for each of the three features, which we convert into vectors. Each feature produces a 36-dimensional vector from every 6×6 partial image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Combining visual features</head><p>Visual features describe distinctive image properties. Even if two features both analyze texture, they do so using different calculations and therefore might recognize different characteristics of the texture. On the other hand, we do not want to waste time and resources to extract correlated features, which are equivalent rather than complimentary sources of information. However, Deselaers et al show that Gabor filters and the individual Tamura features are not correlated <ref type="bibr" coords="5,499.30,634.06,9.99,9.14" target="#b0">[1]</ref>. Therefore, we try to combine the four of them for a more comprehensive texture analysis. We investigate two alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Combining features at visterm generation</head><p>First we join feature vectors produced by each feature in one compound vector, and then we cluster to quantize the vectors into visterms. For example, the length of Gabor energy is 12 (representing 12 histogram bins) and the length of Coarseness is 36 (representing 36 pixels of a 6x6 partial image). The result is a 250000×48 matrix of feature vectors, which is partitioned into 500 visterms. These theoretically reflect similarity of regions based both on Gabor energy and Coarseness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Combining features at visterm representation</head><p>Rather than combining feature vectors prior to visterm assignment, we cluster the feature vectors produced by each feature separately. For example, we partition the regions into 500 visterms based on Gabor energy and then repartition them based on Coarseness. Use different cluster identifiers each time, e.g. integers from 1 to 500 for Gabor energy and integers from 501 to 1000 for Coarseness, and assign both types of visterms to individual images. So if an image is originally divided into 25 regions, it will end up with twice as many visterms. In this case, images can be similar in one respect, e.g. have some Gabor visterms in common, and dissimilar in another, e.g. share no Coarseness visterms. Also, their visual representations are longer, therefore probability estimates could be closer to the true underlying distribution.</p><p>The two approaches have different resource requirements. Combining at generation needs more memory (to fit a bigger matrix) while combining at representation needs more time (to group the regions separately for each feature). This fact should be taken into consideration, especially when working with large collections.</p><p>Our experiments show that combining features at generation is not very effective while two features combined at representation work better than either feature alone. Figure <ref type="figure" coords="6,457.12,351.57,4.98,9.14" target="#fig_4">4</ref> graphs the performance of CQL according to error rate, as the number of clusters increases. Figure <ref type="figure" coords="6,475.57,363.45,4.98,9.14" target="#fig_5">5</ref> graphs the same results for CBDM. It is likely that the combining features at generation fails because the weaker feature Coarseness is three times as long as the better feature Gabor energy. On the other hand, when combining at representation each feature accounts for 25 out of the 50 visterms per image, so in this respect the features are given equal weight.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Clustering techniques</head><p>The theoretical framework of cluster-based relevance modeling does not depend on the specific implementation of clustering. We investigate three different clustering techniques that partition an image collection into groups of similar images: groups based on manually assigned categories, K-means clustering, or K-nearest neighbors (KNN) clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Categories</head><p>Since ImageCLEFmed images are assigned to one particular category, we can assume that categories play the role of cluster labels. It becomes straightforward to partition the collection by putting all images of the same category in a separate cluster. The result is a set of 57 nonoverlapping clusters of various lengths, depending on how many training examples from each category are provided.</p><p>In general annotations are longer than a single word and therefore clustering could not be that simple. Moreover, visterms tend to identify lower-level visual properties and terms -higher-level semantic ones, making it advantageous to consider both the visual and textual representations of images for cluster computations. By combining terms and visterms a clustering technique can generate clusters with both visual and semantic coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">K-means</head><p>This clustering algorithm takes K, the desired number of clusters, as input and returns a list of indices indicating to which cluster each point in the partitioned dataset belongs. The procedure starts by randomly selecting K elements from the set as initial cluster centroids. Each remaining element is added to the cluster to which it is most similar, then the centroids are reevaluated. The algorithm refines the partitioning iteratively by repeatedly reevaluating and reassigning until no element changes assignment and the clustering converges.</p><p>K-means is a hard clustering which produces mutually exclusive clusters. Performance depends on the starting condition -both the predetermined value of K and the initial choice of centroids. The appropriate number of clusters is determined by the dataset configuration which is usually unknown. And even if the value of K is close to the natural number of groupings, given the starting centroid positions K-means can still get trapped in a local maximum and fail to find a good solution. The method is also sensitive to extreme points which lie notably far away from most points or outliers. Because K-means computes centroids as within-cluster averages, an outlier can pull away a centroid away from its true position. We select the value for K experimentally. We test values that range from 50 to 250 in 25-step increments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">K-nearest neighbors</head><p>Kurland et al propose a clustering method that takes the K-1 nearest neighbors of each training image to form a cluster of size K <ref type="bibr" coords="7,237.12,456.21,9.90,9.14" target="#b6">[7]</ref>. In contrast to K-means, KNN is a soft clustering technique that can assign an element to more than one cluster. If an image is a top ranked neighbor to several others, then it belongs to each of the corresponding clusters. KNN generates as many clusters as there are training images, and all of them have exactly the same size since each includes an image and its K-1 nearest neighbors.</p><p>To find the nearest neighbors of a training image J k , all images J m , m = 1...|T |, m = k, are first ranked according to their similarity to J k . In our work, language models are generated by smoothing image frequencies with collection frequencies. Then the similarity between J k and J m is estimated as sim(J k , J m ) =</p><formula xml:id="formula_4" coords="7,237.00,550.01,53.34,12.83">|J k | i=1 P (t i |J m )</formula><p>, where t i are the terms and visterms of J k . The ranking process is repeated |T | times -once for each one of the training images in the collection T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Parameter setting</head><p>The cluster-based models rely on several smoothing and clustering parameters. These include: α for smoothing terms in image models, β for visterms in image models, γ for terms in cluster models, δ for visterms in cluster models, K for the number of clusters with K-means, and K for the number of nearest neighbors with KNN clustering.</p><p>We apply 10-fold cross validation to set each parameter. We divide the 9000 training images into 10 subsets of equal size and optimize performance by minimizing the error rate. For each possible parameter value, we train the model 10 times using one of the folds for testing and the rest for training, and we average the accuracy of the 10 trails. This evaluation method is more reliable than the simpler holdout method because it uses every training image for validation exactly once.</p><p>We determine that CQL works best with γ = 0.1 and δ = 0.2 while CBDM works best with α = 0.5, β = 0.8, γ = 0.5 and δ = 0.3. We use these values throughout the rest of the experiments. On the other hand, cluster quality is closely linked to the visual feature, more precisely to its effectiveness to produce visterms with discriminative power. Since the value of K is featuredependent, we cross-validate it individually for each visual feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Feature effectiveness</head><p>To get a sense of the relative effectiveness of the extracted features, we compare Coarseness and Gabor energy. The former has highest performance at 100 clusters, the latter at 225, and Gabor energy is the more useful feature (Tables <ref type="table" coords="8,264.69,238.05,4.98,9.14" target="#tab_1">1</ref> and<ref type="table" coords="8,290.59,238.05,3.88,9.14">2</ref>). However, we cannot improve accuracy by simply setting K to an ever higher value. Larger K does not automatically imply better performanceit is cluster quality that matters, not the number of clusters.</p><p>Since images represented with Coarseness visterms are clustered into fewer groups, it is likely that dissimilar images will occasionally be contained in the same cluster. Perhaps Coarseness captures less information about content, yielding poorer discrimination between the visual representations of images. This would be true if the images are naturally structured into more groups, but the clustering algorithm fails to distinguish between some groups based on the Coarseness representations. However, even though Coarseness extracts less information than Gabor energy (or rather, less useful information), its texture analysis does not overlap with that of Gabor energy. Since they identify different image properties, combining the two features proves to be an advantage (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation measures</head><p>Possible evaluation measures do not necessarily suggest the same feature as most effective. Therefore, we need to decide which measure is most appropriate for either task. We compare four measures with respect to categorization using the CQL model: error rate, precision at 0.0 recall, average F-measure, and mean average precision. As discussed in Section 5.1, we set the smoothing parameters γ and δ to 0.1 and 0.2, respectively. The clustering parameter K is feature-dependent -we use 225 for Gabor energy, 100 for Coarseness, and 200 for Gabor energy combined with the three Tamura features (Coarseness, Directionality and Contrast). Results are reported in Tables <ref type="table" coords="8,173.10,499.53,4.98,9.14" target="#tab_1">1</ref> and<ref type="table" coords="8,200.68,499.53,3.90,9.14">2</ref>. The experiments show that Gabor energy is the best feature for assigning annotations. On the other hand, Gabor energy and Tamura combined is the optimal feature for assigning categories according to all but mean average precision, in which Gabor energy is best. This leads to the question of which evaluation measure should be used to optimize parameters. Table <ref type="table" coords="9,118.06,130.78,3.90,9.14">2</ref>: Ranking visual features according to their annotation effectiveness (CQL performance).</p><p>Perhaps the most important difference between categories and annotations is that a category consists of a single term and an annotation is constructed from multiple terms (not necessarily but in most cases). We select the evaluation measure based on this important distinction.</p><p>When assigning categories, only the highest ranked category is selected, so we need not be concerned about the tradeoff between recall and precision. On the other hand, when we assign annotations we select several concepts. In this case, we are interested in both recall and precision. These properties of categorization and annotation help us choose the appropriate evaluation measure. For categorization, an evaluation measure that reflects the precision of assigning categories should be selected -either error rate or precision at 0% recall. For the annotation task an evaluation measure that combines recall and precision should be selected -either F-measure or mean average precision. In the remaining experiments, effectiveness of categorization and annotation are measured via error rate and F-measure, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Weighting concept probabilities to assign categories</head><p>The method of splitting annotations into concepts and then multiplying concept probabilities to rank categories is, not surprisingly, very ineffective (Table <ref type="table" coords="9,345.23,352.90,3.88,9.14" target="#tab_3">3</ref>). Concepts by themselves have a very good annotation performance, so in theory they should contain enough information for categorization. By directly multiplying concept probabilities to estimate the probability of assigning a category, we treat all concepts as 'equal'. In general this is the right approach since we do not know in advance what the user would be interested in, so there are no 'significant' and 'insignificant' words. Therefore, we would want to assign both frequent (background) words and rare (foreground) words correctly. But in the very specific case of using concepts to categorize medical images, we can in fact make a distinction between concepts based on the number of times a concept appears in categorical definitions.</p><p>As Figure <ref type="figure" coords="9,150.56,628.65,4.98,9.14" target="#fig_6">6</ref> shows, some concepts are more important than others for defining a category -its a rare concept that defines a category best by distinguishing it from the rest. To take advantage of this information, we scale concepts using a TF×IDF weighting scheme. Concept probabilities are computed as: where S is the set of categorical definitions.</p><formula xml:id="formula_5" coords="9,233.40,683.85,136.11,22.35">P (c i |J) = 1 log(#(c i , S)) P (c i |J)</formula><p>Thus we emphasize concepts that appear in only a few categories and penalize concepts that appear in many categories since they are not very effective for differentiating between categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Clustering Effectiveness</head><p>Clustering is an important ingredient of our method and the choice of clustering technique can significantly affect performance. We look at three alternatives as described in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Categories</head><p>Our baseline method is clustering according to manually assigned category (Section 4.3.1). In this case there are no parameters to optimize because all cluster information is explicitly specified in the data.</p><p>Even though category-clusters have satisfactory performance (Table <ref type="table" coords="10,399.41,442.29,3.88,9.14" target="#tab_5">4</ref>), we should acknowledge that most collections are not processed, so this kind of 'naive' clustering would not always be an option. And even if there are categorized images available (for instance, if the user provides some manual examples to a browsing system which automatically organizes an image collection according to user preferences), these categories would be user-dependent and therefore not necessarily well-defined. They could be either too broad or too specific, i.e. generate clusters that are either too large and loose, or too small and with no real advantage over individual images.  Therefore, we would like to find an unsupervised clustering method which performs as well as or even better than manually labeled clusters. Then we could rely on the system to automatically find appropriate values for clustering parameters, so that generated clusters approximate the collection's natural configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">K-means</head><p>K-means is a general clustering technique described in Section 4.3.2. It has one input parameter, the number of clusters K, which is optimized separately for each visual feature. K-means gives CQL a statistically significant improvement but slightly hurts CBDM (Table <ref type="table" coords="11,442.17,142.41,3.88,9.14" target="#tab_5">4</ref>). The results indicate that the medical categories are relatively broad. For example, there might be a category which contains two visually different types of images, and the accuracy of CQL increases as a result of separating them into two different clusters. (We know that K-means breaks up some of the category clusters because the value of K is larger than 57 (Table <ref type="table" coords="11,372.32,190.29,3.88,9.14" target="#tab_7">5</ref>). In this way, the system deals with the issue of some clusters not being compact enough. On the other hand, cluster compactness has less influence on their usefulness as background models for smoothing and this could explain why the performance of CBDM does not improve. (With CBDM emphasis is on generalization and therefore recall, and with CQL -on correctness and therefore precision.)</p><p>For other collections manual categories can be too narrowly defined. In such situations we would expect K-means to generate fewer clusters than categories, thus increasing recall, which would have a positive effect both on CQL and CBDM.</p><p>However, it is not necessary to use CQL and CBDM with the same set of clusters. In fact, CBDM shows a consistent tendency to perform best with fewer but larger clusters as compared to CQL:  CQL and CBDM apply clusters in two conceptually different roles -on one hand, as training examples which are somewhat more general than images, and on the other hand, as background collections which are somewhat more specific than the entire collection. Implicitly, bigger clusters are more useful for generalizing patterns observed in individual images -if the clusters are too small, they would fail to capture some aspects of member images and their content. Therefore, with CBDM we are less concerned about the compactness of the clusters, and can allow some relatively dissimilar elements to join the same cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">K-nearest neighbors</head><p>KNN is a soft clustering technique described in Section 4.3.3. We optimize K separately for the two cluster-based models and establish empirically that K=25 for CQL and K=50 for CBDM.</p><p>First, this corroborates our previous conclusion that CQL works well with very compact clusters and CBDM works well with more general clusters. We also observe that categorization performance improves with a statistically significant difference as compared to K-means clustering (Table <ref type="table" coords="11,501.01,610.29,3.88,9.14" target="#tab_5">4</ref>). KNN clusters have more local coherence because they are defined with respect to particular image (i.e. locally). Since by generation a KNN cluster is specific to an image, it is better at describing its context. In addition, the KNN method does not reduce the number of training examples. It generates as many clusters as there are images. On the other hand, K-means creates considerably fewer clusters, which implies that there are fewer observations on which to base the model's probability estimations.</p><p>In this work, we analyzed a cluster-based cross-lingual retrieval approach to image annotation and categorization. We described two methods for incorporating cluster statistics into the general framework of cross-media relevance modeling and showed that both build effective probabilistic models of term-visterm relationships. We also discussed how different clustering techniques affect the quality and discriminative power of automatically generated clusters. Finally, we demonstrated an efficient method for combining visterms produced by several visual features.</p><p>We regard clustering as a kind of unsupervised classification that offers greater flexibility than manual classification. If the actual categories are too broad, then the system can break them into smaller clusters. If the actual categories are too specific, then it can redefine them by generating bigger clusters. If manually assigned categories are unavailable, the system can create them automatically. The only disadvantage is that automatic clusters do not have explicit textual descriptions, but the word distribution in clusters could be analyzed to build statistical language models.</p><p>In the future, we plan to investigate grouping by concept (similar to the method of grouping by category described here but based on annotation words) as an alternative version of soft clustering. We are also interested in analyzing the categorization performance of CQL and CBDM on a collection of true-color images to examine how visual properties influence accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,90.00,602.73,422.66,9.14;3,90.00,614.61,422.64,9.14;3,90.02,486.32,63.28,63.28"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Images from six different categories whose annotations have all but one concept in common. Even though the annotations are very similar, the images themselves look quite different.</figDesc><graphic coords="3,90.02,486.32,63.28,63.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,90.00,334.65,202.96,9.14;4,90.00,346.53,47.74,9.14"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Category distribution in Image-CLEFmed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,309.96,334.65,202.99,9.14;4,309.96,346.53,47.74,9.14"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Concept distribution in Image-CLEFmed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,90.00,591.69,194.44,9.14;6,90.00,603.69,135.73,9.14"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CQL performance with Gabor energy and Coarseness combined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,318.36,591.69,194.55,9.14;6,318.36,603.69,145.80,9.14"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: CBDM performance with Gabor energy and Coarseness combined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="10,90.00,236.25,422.69,9.14;10,90.00,248.13,102.97,9.14"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Concept distribution across the set of 57 categories. The most frequent concept appears is every single category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,90.00,521.74,422.68,100.69"><head>Table 1 :</head><label>1</label><figDesc>Ranking visual features according to their categorization effectiveness (CQL performance).</figDesc><table coords="8,96.00,521.74,415.93,66.59"><row><cell></cell><cell cols="2">Ranking according to</cell><cell cols="2">Ranking according to</cell><cell cols="2">Ranking according to</cell><cell cols="2">Ranking according to</cell></row><row><cell></cell><cell>error rate</cell><cell></cell><cell>highest precision</cell><cell></cell><cell>F-measure</cell><cell></cell><cell>mAP</cell><cell></cell></row><row><cell>I.</cell><cell>Gabor energy</cell><cell>.3178</cell><cell>Gabor energy</cell><cell>.6792</cell><cell>Gabor energy</cell><cell>.4125</cell><cell>Gabor energy</cell><cell>.3800</cell></row><row><cell></cell><cell>and Tamura</cell><cell></cell><cell>and Tamura</cell><cell></cell><cell>and Tamura</cell><cell></cell><cell></cell><cell></cell></row><row><cell>II.</cell><cell>Gabor energy</cell><cell>.3722</cell><cell>Gabor energy</cell><cell>.6527</cell><cell>Gabor energy</cell><cell>.3724</cell><cell>Gabor energy</cell><cell>.3195</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>and Tamura</cell><cell></cell></row><row><cell>III.</cell><cell>Coarseness</cell><cell>.5078</cell><cell>Coarseness</cell><cell>.5087</cell><cell>Coarseness</cell><cell>.2010</cell><cell>Coarseness</cell><cell>.2412</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,90.00,376.36,422.67,142.39"><head>Table 3 :</head><label>3</label><figDesc>Improving the categorization performance (measured in error rate) of concepts by scaling their probabilities.</figDesc><table coords="9,196.44,376.36,210.08,107.67"><row><cell></cell><cell>Categories</cell><cell>Concepts</cell><cell>Concepts</cell></row><row><cell></cell><cell></cell><cell>not scaled</cell><cell>scaled</cell></row><row><cell>CQL</cell><cell>.372222</cell><cell>.588889</cell><cell>.389999</cell></row><row><cell>Gabor energy</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CQL</cell><cell>.507778</cell><cell>.627778</cell><cell>.528889</cell></row><row><cell>Coarseness</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CBDM</cell><cell>.374444</cell><cell>.596667</cell><cell>.377778</cell></row><row><cell>Gabor energy</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CBDM</cell><cell>.468889</cell><cell>.624444</cell><cell>.483333</cell></row><row><cell>Coarseness</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,90.00,623.49,422.71,33.02"><head>Table 4 :</head><label>4</label><figDesc>Categorization performance of cluster-based CMRM improves with unsupervised clustering (K-means or KNN). 95%-confidence p-values according to the Wilcoxon signed-rank test are reported in parenthesis.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,90.00,410.61,422.88,21.14"><head>Table 5 :</head><label>5</label><figDesc>K, the number of clusters K-means generates, is a feature-dependent parameter. However, CBDM consistently is set to use smaller K, and hence bigger clusters, than CQL.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,691.54,391.15,7.31"><p>Throughout the paper I denotes an image to be annotated, and J -an already annotated training image.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>* This work was funded in part by the <rs type="funder">Howard Hughes Medical Institute Cascade Mentoring Program</rs> (<rs type="grantNumber">HHMI</rs> #<rs type="grantNumber">52005134</rs>) and the <rs type="funder">Clare Boothe Luce Program of the Henry Luce Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_82hfCmE">
					<idno type="grant-number">HHMI</idno>
				</org>
				<org type="funding" xml:id="_EeMhWsD">
					<idno type="grant-number">52005134</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,105.47,330.09,407.21,9.14;12,105.48,342.09,407.37,9.14" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,345.01,330.09,167.68,9.14;12,105.48,342.09,84.44,9.14">Features for Image Retrieval: A Quantitative Comparison</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,210.39,342.40,271.42,8.38">Proceedings of the 26th DAGM Pattern Recognition Symposium</title>
		<meeting>the 26th DAGM Pattern Recognition Symposium</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.47,362.01,407.27,9.14;12,105.48,374.01,353.51,9.14" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,340.69,362.01,172.06,9.14;12,105.48,374.01,124.71,9.14">FIRE -Flexible Image Retrieval Engine: ImageCLEF 2004 Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,251.50,374.32,176.05,8.38">Proceedings of the CLEF 2004 Workshop</title>
		<meeting>the CLEF 2004 Workshop</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.47,393.94,407.26,9.14;12,105.48,405.82,407.19,9.14;12,105.48,417.82,213.16,9.14" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,416.38,393.94,96.35,9.14;12,105.48,405.82,307.82,9.14">Object Recognition as Machine Translation: Leaning a Lexicon for a Fixed Image Vocabulary</title>
		<author>
			<persName coords=""><forename type="first">Pinar</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kobus</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,434.47,406.12,78.20,8.38;12,105.48,418.12,182.49,8.38">Proceedings of 7th European Conference on Computer Vision</title>
		<meeting>7th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.47,437.74,406.95,9.14;12,105.48,449.74,111.11,9.14" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,207.62,437.74,171.53,9.14">Gabor Filters as Texture Discriminator</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Fogel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dov</forename><surname>Sagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,389.38,438.04,123.05,8.38;12,105.48,450.04,23.95,8.38">Journal of Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="102" to="113" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.47,469.66,407.15,9.14;12,105.48,481.54,407.21,9.14;12,105.48,493.54,76.96,9.14" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,324.87,469.66,187.75,9.14;12,105.48,481.54,160.63,9.14">Automatic Image Annotation and Retrieval using Cross-Media Relevance Models</title>
		<author>
			<persName coords=""><forename type="first">Jiwoon</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,290.25,481.84,222.44,8.38;12,105.48,493.84,46.00,8.38">Proceedings of the 26th International ACM SIGIR Conference</title>
		<meeting>the 26th International ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.47,513.46,407.20,9.14;12,105.48,525.46,407.31,9.14;12,105.48,537.34,22.89,9.14" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,251.15,513.46,256.76,9.14">Using Maximum Entropy for Automatic Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">Jiwoon</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,117.36,525.76,333.97,8.38">Proceedings of the 3rd International Conference on Image and Video Retrieval</title>
		<meeting>the 3rd International Conference on Image and Video Retrieval</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="24" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.47,557.26,407.35,9.14;12,105.48,569.26,350.54,9.14" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,242.06,557.26,270.76,9.14;12,105.48,569.26,37.76,9.14">Corpus Structure, Language Models, and Ad Hoc Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Kurland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,164.23,569.56,260.95,8.38">Proceedings of the 27th Internation ACM SIGIR Conference</title>
		<meeting>the 27th Internation ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.47,589.18,407.14,9.14;12,105.48,601.18,284.47,9.14" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,263.38,589.18,207.50,9.14">Cluster-Based Retrieval using Language Models</title>
		<author>
			<persName coords=""><forename type="first">Xiaoyong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,493.58,589.48,19.03,8.38;12,105.48,601.48,253.51,8.38">Proceedings of the 27th International ACM SIGIR Conference</title>
		<meeting>the 27th International ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.47,621.10,407.31,9.14;12,105.48,632.98,407.24,9.14;12,105.48,644.98,132.57,9.14" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,350.43,621.10,162.34,9.14;12,105.48,632.98,246.09,9.14">Image-to-word Transformation based on Dividing and Vector Quantizing Images with Words</title>
		<author>
			<persName coords=""><forename type="first">Yasuhide</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hironobu</forename><surname>Takanashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryuichi</forename><surname>Oka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,377.31,633.28,135.41,8.38;12,105.48,645.28,101.01,8.38">Proceedings of the 1st MISRM International Workshop</title>
		<meeting>the 1st MISRM International Workshop</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
