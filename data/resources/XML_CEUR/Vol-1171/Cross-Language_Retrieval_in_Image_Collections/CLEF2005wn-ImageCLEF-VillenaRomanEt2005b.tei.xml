<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,116.10,74.75,363.19,12.58">MIRACLE&apos;s Naive Approach to Medical Images Annotation</title>
				<funder ref="#_7uJdeYQ">
					<orgName type="full">Spanish R+D National Plan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,182.10,102.18,83.60,9.02"><forename type="first">Julio</forename><surname>Villena-Román</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Decisions and Language</orgName>
								<orgName type="institution">DAEDALUS -Data</orgName>
								<address>
									<country>S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.20,102.18,126.09,9.02"><forename type="first">José</forename><forename type="middle">Carlos</forename><surname>González-Cristóbal</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Decisions and Language</orgName>
								<orgName type="institution">DAEDALUS -Data</orgName>
								<address>
									<country>S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,174.78,114.00,107.80,9.02"><forename type="first">José</forename><forename type="middle">Miguel</forename><surname>Goñi-Menoyo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.60,114.00,121.04,9.02"><forename type="first">José</forename><surname>Luís Martínez-Fernandez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Decisions and Language</orgName>
								<orgName type="institution">DAEDALUS -Data</orgName>
								<address>
									<country>S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,116.10,74.75,363.19,12.58">MIRACLE&apos;s Naive Approach to Medical Images Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">118D376834B0982BD4A813CCD2A1595B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.2 Information Storage</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software. E.1 [Data Structures]. E.2 [Data Storage Representations]. H.2 [Database Management] Linguistic Engineering, Information Retrieval, medical images, image annotation, learning algorithms, decision table, nearest-neighbour, Weka</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the proposed tasks of the ImageCLEF 2005 campaign has been an Automatic Annotation Task. The objective is to provide the classification of a given set of 1,000 previously unseen medical (radiological) images according to 57 predefined categories covering different medical pathologies. 9,000 classified training images are given which can be used in any way to train a classifier. The Automatic Annotation task uses no textual information, but image-content information only. This paper describes our participation in the automatic annotation task of ImageCLEF 2005.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF is the cross-language image retrieval track which was established in 2003 as part of the Cross Language Evaluation Forum (CLEF), a benchmarking event for multilingual information retrieval held annually since 2000. Originally, ImageCLEF focused specifically on evaluating the retrieval of relevant images of the collection using different query languages, therefore having to deal with monolingual and bilingual image retrieval (multilingual retrieval is not possible as the document collection is only in one language). Later, the scope of ImageCLEF widened and goals evolved to investigate the effectiveness of combining text and image for retrieval (text and content-based), collect and provide resources for benchmarking image retrieval systems and promote the exchange of ideas which will lead to improvements in the performance of retrieval systems in general.</p><p>In addition to the retrieval experiments, the 2005 campaign also included a new completely different task: an Automatic Annotation task. The objective is to provide the classification of a given set of 1,000 previously unseen medical (radiological) images according to 57 predefined categories covering different medical pathologies. 9,000 classified training images are given which can be used in any way to train a classifier. The Automatic Annotation task uses no textual information, but image-content information only.</p><p>The MIRACLE team is made up of three university research groups located in Madrid (UPM, UC3M and UAM) along with DAEDALUS, a company founded in 1998 as a spin-off of two of these groups. DAEDALUS is a leading company in linguistic technologies in Spain and is the coordinator of the MIRACLE team. This is the third participation in CLEF, after years 2003 and 2004 <ref type="bibr" coords="1,291.72,661.02,14.59,9.02" target="#b10">[11]</ref>, <ref type="bibr" coords="1,309.96,661.02,14.59,9.02" target="#b9">[10]</ref>, <ref type="bibr" coords="1,328.20,661.02,10.95,9.02" target="#b7">[8]</ref>, <ref type="bibr" coords="1,342.80,661.02,10.95,9.02" target="#b2">[3]</ref>, <ref type="bibr" coords="1,357.39,661.02,10.95,9.02" target="#b1">[2]</ref>, <ref type="bibr" coords="1,371.99,661.02,10.95,9.02" target="#b6">[7]</ref>. As well as bilingual, monolingual and cross lingual tasks, the team has participated in the ImageCLEF, Q&amp;A, WebCLEF and GeoCLEF tracks. This paper describes our participation in the automatic annotation task of ImageCLEF 2005. Although this task is clearly aimed at image analysis research groups and our areas of expertise don't include image analysis research, we decided to participate in this task adopting a naive approach which consists on isolating ourselves from the content-based analysis by using a publicly available content-based image retrieval system (GIFT <ref type="bibr" coords="1,509.57,725.88,11.26,9.02" target="#b0">[1]</ref>) and applying learning (mainly classification) techniques on the results. The main objective behind our effort to participate is to promote and encourage multidisciplinary participation in all aspects of information retrieval, no matter if it is text or content based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task goals</head><p>Automatic image classification or image annotation is an important step when searching for images from a database, as a way to limit the number of results or filter them to increase precision or as a starting point for a guided search.</p><p>In the specific context of medical images, the automatic image annotation may be used as part of a diagnosis support system <ref type="bibr" coords="2,138.91,184.86,10.64,9.02" target="#b3">[4]</ref>. This system ought to classify and register medical images, using methods of pattern recognition and structural analysis to describe the image content in a feature based, formal and generalized way. The formalized and normalized description of the images then would be used as a mean to compare images in the archive which allows a fast and reliable retrieval. In addition to the queries on an existing electronic archive, the automatic classification allows a simple insertion of conventional radiographs into the system without interaction and therefore costly editing of diagnostic findings.</p><p>Based on the IRMA (Image Retrieval in Medical Applications) project <ref type="bibr" coords="2,364.20,261.48,10.61,9.02" target="#b5">[6]</ref>, a database of 9,000 fully classified radiographs taken randomly from medical routine is made available and can be used to train a classification system. 1,000 radiographs for which classification labels are not available to the participants have to be classified, which is the objective of the Automatic Annotation task in ImageCLEF 2005.</p><p>The aim is to find out how well current techniques can identify image modality, body orientation, body region, and biological system examined based on the images. The results of the classification step can be used for multilingual image annotations as well as for DICOM (Digital Imaging and Communications in Medicine) header corrections.</p><p>The images are annotated with complete IRMA code, a multi-axial code for image annotation. The IRMA code is currently available in English and German. It is planned to use the results of such automatic image annotation tasks for further, textual image retrieval tasks in the future. However, to simplify the task, only 57 simple class numbers are provided for ImageCLEF 2005. The meaning of each class and the number of images belonging to it is shown in Table <ref type="table" coords="2,152.30,414.78,3.77,9.02" target="#tab_0">1</ref>. The distribution of images is not homogeneous among all classes, with a clear deviation to class 12 (chest) with more than 28% of the training images. This may be cause for concern when building the classifiers and should be taken into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Description of experiments</head><p>This task is clearly aimed at image analysis research groups and the areas of expertise of the MIRACLE group don't include image analysis research. However, as our group did have a strong expertise in automatic learning algorithms applied to different projects mainly in the fields of data, text and web mining, we decided to make the effort and participate in this task to promote and encourage multidisciplinary participation in all aspects of information retrieval, no matter if it is text or content based.</p><p>To isolate from the content-based retrieval part of the process, we resorted to GIFT (GNU Image Finding Tool) <ref type="bibr" coords="3,70.92,576.66,10.60,9.02" target="#b0">[1]</ref>, a publicly available content-based image retrieval system which was developed under the GNU license and allows to perform query by example on images, using an image as the starting point for the search process. GIFT relies entirely on the image contents and thus it doesn't require the collection to be annotated. It also provides a mechanism to improve query results by relevance feedback.</p><p>Our approach is based on the multidisciplinary combination of the usage of GIFT to perform content-based searches and the application of learning techniques over the retrieval results to build a classifier. Our system is divided in two parts: the content-based retrieval component (mainly GIFT) and the learning component, which makes calls to the retrieval component when necessary and uses the results to build the classifier. We think that this is a naive approach in the sense that we had to completely trust the results from the retrieval engine without no possibility or knowledge to change its behaviour. The only margin for improvement was on the learning component of the system, which in fact relied on the retrieval component.</p><p>We finally submitted two different runs to be evaluated by the task coordinators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieval Component</head><p>Unzipping the database with the 9,000 training images provided by the task coordinators results in a structure of 57 directories (Train01 to Train57), which allows to easily know the class of each image simply by parsing the path of the file. GIFT was then used to index the whole set of images, down-scaled to 32x32 pixels.</p><p>The retrieval component takes two parameters as inputs: a query image and a number of results. It internally makes calls to GIFT with the image as a query, gets the images that are more similar to the query image, and finally returns the given number of top results, each with the filename of the image and its relevance.</p><p>Although different search algorithms can be integrated as plug-ins in GIFT, only the provided separate normalisation algorithm has been used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decision Table Classifier</head><p>This run selects the classification by using a decision table majority classifier <ref type="bibr" coords="4,387.78,211.62,10.64,9.02" target="#b4">[5]</ref>. First, the retrieval component performs an initial search of the query image and returns a list of the top N images which their relevancies. Then, a weighting function is applied to the relevance of each result. Finally, as each result is associated to a particular class (which can be easily obtained just parsing the filename), the confidence of each class is calculated as the sum of the weighted relevancies of all the results which correspond to that class. The output is the confidence of each of the 57 classes, assuming that the class with the highest relevance is considered to be the class of the image.</p><p>After several tests using 10-fold cross-validation with the training images, the best results were obtained when assuming N=20 (taking the top 20 results to compute the aggregated class confidence) and using a factor of 1/n (n being the number of result, from 1 to 20) as the weighting function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nearest-Neighbour Classifier</head><p>This run is based on the previous experiment and applies a K-Nearest-Neighbour classifier <ref type="bibr" coords="4,453.43,367.38,11.70,9.02" target="#b4">[5]</ref> to predict the output class. The classifier is trained for all the training examples (images), using 58 input variables: the vector of confidences for the 57 classes (float values), calculated as explained before, and the class which corresponds to the maximum value (string value). The output variable (the one to model in training) is the class of each training image. This variable will be predicted later for each test image. Weka <ref type="bibr" coords="4,395.94,414.48,11.55,9.02" target="#b4">[5]</ref>, <ref type="bibr" coords="4,411.34,414.48,15.40,9.02" target="#b11">[12]</ref> was used to implement this classifier.</p><p>After several tests using 10-fold cross-validation with the training images, the best results were obtained when assuming K=8 (8 nearest examples) and enabling attribute normalization and no distance weighting. Although we were aware of the non homogeneous training examples among different classes, we didn't take this fact into account due to lack of time to carry out the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Figure <ref type="figure" coords="4,99.72,526.02,5.01,9.02">1</ref> compares the class distribution of training/test images (respectively, 9,000/1,000 images), analyzing the qrels file provided by the task coordinator after the submission deadline. The full data is shown in Table <ref type="table" coords="4,500.29,537.78,5.01,9.02" target="#tab_4">4</ref> (see appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Comparison between training/test class distribution</head><p>As in the training set, some sampling bias can be observed also in the test set, and, furthermore, differences in the relative distribution between them. We think that this differences may affect the building of learning models and the also the evaluation of the different groups, and it has to be taken into account for next years.</p><p>The results of the classifiers are shown in Table <ref type="table" coords="5,272.38,347.82,3.76,9.02">2</ref>, ranked by error rate (note that each 0.1% corresponds to 1 misclassification).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2: Evaluation of classifiers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Error rate mira20relp57.txt (1)  21.4 mira20relp58IB8.txt (2)  22.3 Euclidean distance, 32x32 images, 1-Nearest-Neighbour (3)  36.8 (1) mira20relp57.txt is the Decision table classifier (2) mira20relp58IB8.txt is the Nearest neighbour classifier (3) According to the track organizers, for a 1-Nearest-Neighbour classifier comparing the images downscaled to 32x32 pixels using Euclidean distance, the error rate is 36.8% (which means 368 images were misclassified).</p><p>As shown in Table <ref type="table" coords="5,155.61,527.34,3.77,9.02">2</ref>, the best result was obtained with the decision table classifier. This error rate greatly improves the baseline of a 1-nearest-neighbour classifier.</p><p>The differences between our two runs haven't still been analysed in detail at this moment, but a possible explanation for the performance loss with the nearest neighbour classifier may be imputed to model overtraining (when selecting the value of the parameters) or to the non homogeneous distribution of the training examples.</p><p>Figure <ref type="figure" coords="5,99.66,598.20,5.01,9.02">2</ref> shows the distribution of error rate for each class for the decision table classifier. The full data is shown in  Our best submission misclassifies only 88 images more than the best submission, from RWTH Aachen Computer Science. These results are very satisfactory for us, considering the simple techniques which have been employed in our experiment and also that we are not a group with expertise in image processing as the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>Our naive approach to this task has proved to be valuable and the results are good enough to be comparable to more sophisticated techniques used by other groups. The combination of a "black-box" search using a publicly accessible content-based retrieval engine with a simple classification algorithm based on a decision table with weighted relevance aggregation has turned to provide similar results to other "more complex" algorithms such as nearest-neighbour or not much worse than boosting. This simplicity may be a good starting point for the implementation of a real system.</p><p>Regarding the classifier, we think that there is still space for improvement with a more careful training of the model, probably having a better selection of the training set, or introducing extra parameters to model the sampling biases. We will study if another combination of parameter values would have led to better results, controlling the model overtraining.</p><p>In addition, there are other techniques which we also want to test, such as decision trees, neural networks or Bayesian algorithms. Another possibility which couldn't be tested due to lack of time is to apply clustering to filter results before the learning algorithm, which could discriminate noisy classes and thus increase precision.  The last column shows the number of different classes related to the test class, which can be used to study the noise level for the class or its discrimination ability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,102.42,70.86,390.48,209.64"><head></head><label></label><figDesc></figDesc><graphic coords="5,102.42,70.86,390.48,209.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,105.60,70.92,384.18,185.04"><head></head><label></label><figDesc></figDesc><graphic coords="6,105.60,70.92,384.18,185.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,92.58,432.81,410.37,315.48"><head>Table 1 : Annotation classes</head><label>1</label><figDesc></figDesc><table coords="2,92.58,450.78,410.37,297.52"><row><cell>Class</cell><cell></cell><cell></cell><cell>Description</cell><cell>Images</cell><cell>%</cell></row><row><cell>01</cell><cell>Radiography</cell><cell>Coronal</cell><cell>cranium, musculosceletal system</cell><cell>336</cell><cell>3.7%</cell></row><row><cell>02</cell><cell></cell><cell></cell><cell>cranium, facial cranium, musculosceletal system</cell><cell>32</cell><cell>0.4%</cell></row><row><cell>03</cell><cell></cell><cell></cell><cell>spine, cervical spine, musculosceletal system</cell><cell>215</cell><cell>2.4%</cell></row><row><cell>04</cell><cell></cell><cell></cell><cell>spine, thoracic spine, musculosceletal system</cell><cell>102</cell><cell>1.1%</cell></row><row><cell>05</cell><cell></cell><cell></cell><cell>spine, lumbar spine, musculosceletal system</cell><cell>225</cell><cell>2.5%</cell></row><row><cell>06</cell><cell></cell><cell></cell><cell>arm, hand, musculosceletal system</cell><cell>576</cell><cell>6.4%</cell></row><row><cell>07</cell><cell></cell><cell></cell><cell>arm, radio carpal joint, musculosceletal system</cell><cell>77</cell><cell>0.9%</cell></row><row><cell>08</cell><cell></cell><cell></cell><cell>arm, handforearm, musculosceletal system</cell><cell>48</cell><cell>0.5%</cell></row><row><cell>09</cell><cell></cell><cell></cell><cell>arm, elbow, musculosceletal system</cell><cell>69</cell><cell>0.8%</cell></row><row><cell>10</cell><cell></cell><cell></cell><cell>arm, upper arm, musculosceletal system</cell><cell>32</cell><cell>0.4%</cell></row><row><cell>11</cell><cell></cell><cell></cell><cell>arm, shoulder, musculosceletal system</cell><cell>108</cell><cell>1.2%</cell></row><row><cell>12</cell><cell></cell><cell></cell><cell>chest</cell><cell>2563</cell><cell>28.5%</cell></row><row><cell>13</cell><cell></cell><cell></cell><cell>chest, bones, musculosceletal system</cell><cell>93</cell><cell>1.0%</cell></row><row><cell>14</cell><cell></cell><cell></cell><cell>abdomen, gastrointestinal system</cell><cell>152</cell><cell>1.7%</cell></row><row><cell>15</cell><cell></cell><cell></cell><cell>abdomen, uropoietic system</cell><cell>15</cell><cell>0.2%</cell></row><row><cell>16</cell><cell></cell><cell></cell><cell>abdomen, upper abdomen, gastrointestinal system</cell><cell>23</cell><cell>0.3%</cell></row><row><cell>17</cell><cell></cell><cell></cell><cell>pelvis, musculosceletal system</cell><cell>217</cell><cell>2.4%</cell></row><row><cell>18</cell><cell></cell><cell></cell><cell>leg, foot, musculosceletal system</cell><cell>205</cell><cell>2.3%</cell></row><row><cell>19</cell><cell></cell><cell></cell><cell>leg, ankle joint, musculosceletal system</cell><cell>137</cell><cell>1.5%</cell></row><row><cell>20</cell><cell></cell><cell></cell><cell>leg, lower leg, musculosceletal system</cell><cell>31</cell><cell>0.3%</cell></row><row><cell>21</cell><cell></cell><cell></cell><cell>leg, knee, musculosceletal system</cell><cell>194</cell><cell>2.2%</cell></row><row><cell>22</cell><cell></cell><cell></cell><cell>leg, upper leg, musculosceletal system</cell><cell>48</cell><cell>0.5%</cell></row><row><cell>23</cell><cell></cell><cell></cell><cell>leg, hip, musculosceletal system</cell><cell>79</cell><cell>0.9%</cell></row><row><cell>24</cell><cell></cell><cell>Sagittal</cell><cell>cranium, facial cranium, musculosceletal system</cell><cell>17</cell><cell>0.2%</cell></row><row><cell>25</cell><cell></cell><cell></cell><cell>cranium, neuro cranium, musculosceletal system</cell><cell>284</cell><cell>3.2%</cell></row><row><cell>26</cell><cell></cell><cell></cell><cell>spine, cervical spine, musculosceletal system</cell><cell>170</cell><cell>1.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,70.92,609.96,453.68,20.78"><head>Table 5 (</head><label>5</label><figDesc>see appendix). Note the lower error rate for the classes with many training examples (such as 12 and 34). This may also lead to think that the model is overfitting the training set.</figDesc><table coords="6,234.78,264.69,39.19,8.74"><row><cell>Figure 2:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,70.92,264.69,453.60,38.03"><head>Error rate per class</head><label></label><figDesc>Apart from MIRACLE, other 12 groups participated in this year's evaluation, with more than one submission. Next table shows the results for each group's best submission.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,219.54,311.73,156.31,174.29"><head>Table 3 : Comparison among groups</head><label>3</label><figDesc></figDesc><table coords="6,219.54,329.70,156.31,156.32"><row><cell>Group</cell><cell cols="2">Error rate Difference</cell></row><row><cell>rwth-i6</cell><cell>12.6</cell><cell></cell></row><row><cell>rwth-mi</cell><cell>13.3</cell><cell>0.7</cell></row><row><cell>ulg.ac.be</cell><cell>14.1</cell><cell>1.5</cell></row><row><cell>geneva-gift</cell><cell>20.6</cell><cell>8.0</cell></row><row><cell>infocomm</cell><cell>20.6</cell><cell>8.0</cell></row><row><cell>miracle</cell><cell>21.4</cell><cell>8.8</cell></row><row><cell>ntu</cell><cell>21.7</cell><cell>9.1</cell></row><row><cell>nctu-dblab</cell><cell>24.7</cell><cell>12.1</cell></row><row><cell>cea</cell><cell>36.9</cell><cell>24.3</cell></row><row><cell>mtholyoke</cell><cell>37.8</cell><cell>25.2</cell></row><row><cell>cindi</cell><cell>43.3</cell><cell>30.7</cell></row><row><cell>montreal</cell><cell>55.7</cell><cell>43.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,197.10,217.59,201.19,537.37"><head>Table 4 : Image distribution in training/test sets Class % Training (1) % Test (2) Difference</head><label>4</label><figDesc></figDesc><table coords="7,204.66,247.53,192.90,507.42"><row><cell>01</cell><cell>3.70%</cell><cell>3.8%</cell><cell>0.1</cell></row><row><cell>02</cell><cell>0.40%</cell><cell>0.3%</cell><cell>-0.1</cell></row><row><cell>03</cell><cell>2.40%</cell><cell>2.4%</cell><cell>0.0</cell></row><row><cell>04</cell><cell>1.10%</cell><cell>1.2%</cell><cell>0.1</cell></row><row><cell>05</cell><cell>2.50%</cell><cell>2.5%</cell><cell>0.0</cell></row><row><cell>06</cell><cell>6.40%</cell><cell>6.7%</cell><cell>0.3</cell></row><row><cell>07</cell><cell>0.90%</cell><cell>0.8%</cell><cell>-0.1</cell></row><row><cell>08</cell><cell>0.50%</cell><cell>0.3%</cell><cell>-0.2</cell></row><row><cell>09</cell><cell>0.80%</cell><cell>1.0%</cell><cell>0.2</cell></row><row><cell>10</cell><cell>0.40%</cell><cell>0.7%</cell><cell>0.3</cell></row><row><cell>11</cell><cell>1.20%</cell><cell>1.2%</cell><cell>0.0</cell></row><row><cell>12</cell><cell>28.50%</cell><cell>29.7%</cell><cell>1.2</cell></row><row><cell>13</cell><cell>1.00%</cell><cell>0.7%</cell><cell>-0.3</cell></row><row><cell>14</cell><cell>1.70%</cell><cell>1.4%</cell><cell>-0.3</cell></row><row><cell>15</cell><cell>0.20%</cell><cell>0.3%</cell><cell>0.1</cell></row><row><cell>16</cell><cell>0.30%</cell><cell>0.1%</cell><cell>-0.2</cell></row><row><cell>17</cell><cell>2.40%</cell><cell>2.4%</cell><cell>0.0</cell></row><row><cell>18</cell><cell>2.30%</cell><cell>1.2%</cell><cell>-1.1</cell></row><row><cell>19</cell><cell>1.50%</cell><cell>1.7%</cell><cell>0.2</cell></row><row><cell>20</cell><cell>0.30%</cell><cell>0.2%</cell><cell>-0.1</cell></row><row><cell>21</cell><cell>2.20%</cell><cell>2.9%</cell><cell>0.7</cell></row><row><cell>22</cell><cell>0.50%</cell><cell>0.3%</cell><cell>-0.2</cell></row><row><cell>23</cell><cell>0.90%</cell><cell>1.0%</cell><cell>0.1</cell></row><row><cell>24</cell><cell>0.20%</cell><cell>0.4%</cell><cell>0.2</cell></row><row><cell>25</cell><cell>3.20%</cell><cell>3.6%</cell><cell>0.4</cell></row><row><cell>26</cell><cell>1.90%</cell><cell>2.3%</cell><cell>0.4</cell></row><row><cell>27</cell><cell>1.20%</cell><cell>1.3%</cell><cell>0.1</cell></row><row><cell>28</cell><cell>2.50%</cell><cell>1.6%</cell><cell>-0.9</cell></row><row><cell>29</cell><cell>1.00%</cell><cell>0.8%</cell><cell>-0.2</cell></row><row><cell>30</cell><cell>0.70%</cell><cell>0.7%</cell><cell>0.0</cell></row><row><cell>31</cell><cell>0.70%</cell><cell>0.8%</cell><cell>0.1</cell></row><row><cell>32</cell><cell>0.90%</cell><cell>0.1%</cell><cell>-0.8</cell></row><row><cell>33</cell><cell>0.70%</cell><cell>0.5%</cell><cell>-0.2</cell></row><row><cell>34</cell><cell>9.80%</cell><cell>7.9%</cell><cell>-1.9</cell></row><row><cell>35</cell><cell>0.20%</cell><cell>0.4%</cell><cell>0.2</cell></row><row><cell>36</cell><cell>1.00%</cell><cell>2.1%</cell><cell>1.1</cell></row><row><cell>37</cell><cell>0.20%</cell><cell>0.2%</cell><cell>0.0</cell></row><row><cell>38</cell><cell>1.30%</cell><cell>1.9%</cell><cell>0.6</cell></row><row><cell>39</cell><cell>0.40%</cell><cell>0.5%</cell><cell>0.1</cell></row><row><cell>40</cell><cell>0.60%</cell><cell>0.3%</cell><cell>-0.3</cell></row><row><cell>41</cell><cell>0.70%</cell><cell>1.5%</cell><cell>0.8</cell></row><row><cell>42</cell><cell>0.80%</cell><cell>1.3%</cell><cell>0.5</cell></row><row><cell>43</cell><cell>1.10%</cell><cell>0.8%</cell><cell>-0.3</cell></row><row><cell>44</cell><cell>2.10%</cell><cell>2.3%</cell><cell>0.2</cell></row><row><cell>45</cell><cell>0.40%</cell><cell>0.3%</cell><cell>-0.1</cell></row><row><cell>46</cell><cell>0.30%</cell><cell>0.1%</cell><cell>-0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,182.16,216.63,231.00,537.37"><head>Table 5 : Confusion matrix for both classifiers Class Right Wrong Error Rate Related classes</head><label>5</label><figDesc></figDesc><table coords="8,189.06,246.57,149.75,507.42"><row><cell>01</cell><cell>37</cell><cell>1</cell><cell>2.6%</cell></row><row><cell>02</cell><cell>2</cell><cell>1</cell><cell>33.3%</cell></row><row><cell>03</cell><cell>23</cell><cell>1</cell><cell>4.2%</cell></row><row><cell>04</cell><cell>10</cell><cell>2</cell><cell>16.7%</cell></row><row><cell>05</cell><cell>24</cell><cell>1</cell><cell>4.0%</cell></row><row><cell>06</cell><cell>53</cell><cell>14</cell><cell>20.9%</cell></row><row><cell>07</cell><cell>2</cell><cell>6</cell><cell>75.0%</cell></row><row><cell>08</cell><cell>0</cell><cell>3</cell><cell>100.0%</cell></row><row><cell>09</cell><cell>2</cell><cell>8</cell><cell>80.0%</cell></row><row><cell>10</cell><cell>0</cell><cell>7</cell><cell>100.0%</cell></row><row><cell>11</cell><cell>2</cell><cell>10</cell><cell>83.3%</cell></row><row><cell>12</cell><cell>290</cell><cell>7</cell><cell>2.4%</cell></row><row><cell>13</cell><cell>1</cell><cell>6</cell><cell>85.7%</cell></row><row><cell>14</cell><cell>10</cell><cell>4</cell><cell>28.6%</cell></row><row><cell>15</cell><cell>1</cell><cell>2</cell><cell>66.7%</cell></row><row><cell>16</cell><cell>0</cell><cell>1</cell><cell>100.0%</cell></row><row><cell>17</cell><cell>22</cell><cell>2</cell><cell>8.3%</cell></row><row><cell>18</cell><cell>8</cell><cell>4</cell><cell>33.3%</cell></row><row><cell>19</cell><cell>13</cell><cell>4</cell><cell>23.5%</cell></row><row><cell>20</cell><cell>0</cell><cell>2</cell><cell>100.0%</cell></row><row><cell>21</cell><cell>25</cell><cell>4</cell><cell>13.8%</cell></row><row><cell>22</cell><cell>2</cell><cell>1</cell><cell>33.3%</cell></row><row><cell>23</cell><cell>4</cell><cell>6</cell><cell>60.0%</cell></row><row><cell>24</cell><cell>3</cell><cell>1</cell><cell>25.0%</cell></row><row><cell>25</cell><cell>28</cell><cell>8</cell><cell>22.2%</cell></row><row><cell>26</cell><cell>15</cell><cell>8</cell><cell>34.8%</cell></row><row><cell>27</cell><cell>13</cell><cell>0</cell><cell>0.0%</cell></row><row><cell>28</cell><cell>13</cell><cell>3</cell><cell>18.8%</cell></row><row><cell>29</cell><cell>4</cell><cell>4</cell><cell>50.0%</cell></row><row><cell>30</cell><cell>2</cell><cell>5</cell><cell>71.4%</cell></row><row><cell>31</cell><cell>0</cell><cell>8</cell><cell>100.0%</cell></row><row><cell>32</cell><cell>0</cell><cell>1</cell><cell>100.0%</cell></row><row><cell>33</cell><cell>1</cell><cell>4</cell><cell>80.0%</cell></row><row><cell>34</cell><cell>78</cell><cell>1</cell><cell>1.3%</cell></row><row><cell>35</cell><cell>0</cell><cell>4</cell><cell>100.0%</cell></row><row><cell>36</cell><cell>9</cell><cell>12</cell><cell>57.1%</cell></row><row><cell>37</cell><cell>1</cell><cell>1</cell><cell>50.0%</cell></row><row><cell>38</cell><cell>9</cell><cell>10</cell><cell>52.6%</cell></row><row><cell>39</cell><cell>2</cell><cell>3</cell><cell>60.0%</cell></row><row><cell>40</cell><cell>0</cell><cell>3</cell><cell>100.0%</cell></row><row><cell>41</cell><cell>12</cell><cell>3</cell><cell>20.0%</cell></row><row><cell>42</cell><cell>12</cell><cell>1</cell><cell>7.7%</cell></row><row><cell>43</cell><cell>6</cell><cell>2</cell><cell>25.0%</cell></row><row><cell>44</cell><cell>13</cell><cell>10</cell><cell>43.5%</cell></row><row><cell>45</cell><cell>0</cell><cell>3</cell><cell>100.0%</cell></row><row><cell>46</cell><cell>0</cell><cell>1</cell><cell>100.0%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been partially supported by the <rs type="funder">Spanish R+D National Plan</rs>, by means of the project <rs type="projectName">RIMMEL (Multilingual and Multimedia Information Retrieval, and its Evaluation</rs>), <rs type="grantNumber">TIN2004-07588-C03-01</rs>. Special mention to our colleagues of the MIRACLE team should be done (in alphabetical order): <rs type="person">Ana María García-Serrano</rs>, <rs type="person">Ana González-Ledesma</rs>, <rs type="person">José Mª Guirao-Miras</rs>, <rs type="person">Sara Lana-Serrano</rs>, <rs type="person">Paloma Martínez-Fernández</rs>, <rs type="person">Ángel Martínez-González</rs>, <rs type="person">Antonio Moreno-Sandoval</rs> and <rs type="person">César de Pablo Sánchez</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_7uJdeYQ">
					<idno type="grant-number">TIN2004-07588-C03-01</idno>
					<orgName type="project" subtype="full">RIMMEL (Multilingual and Multimedia Information Retrieval, and its Evaluation</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Other tables</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,97.92,269.64,409.46,9.02" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,125.40,269.64,121.25,9.02">The GNU Image-Finding Tool</title>
		<author>
			<persName coords=""><surname>Gift</surname></persName>
		</author>
		<ptr target="http://www.gnu.org/software/gift/[Visited18/07/2005" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,287.46,426.53,9.02;9,97.92,299.22,400.09,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,472.22,287.46,52.24,9.02;9,97.92,299.22,284.36,9.02">MIRACLE&apos;s Hybrid Approach to Bilingual and Monolingual Information Retrieval</title>
		<author>
			<persName coords=""><surname>Goñi-Menoyo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M;</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><forename type="middle">L</forename><surname>Martínez-Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Villena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,391.80,299.22,99.25,9.02">CLEF 2004 proceedings</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,328.74,426.51,9.02;9,97.92,340.50,426.70,9.02;9,97.92,352.32,426.59,9.02;9,97.92,364.08,426.53,9.02;9,97.92,375.84,22.58,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,97.92,352.32,327.11,9.02">MIRACLE&apos;s hybrid approach to bilingual and monolingual Information Retrieval</title>
		<author>
			<persName coords=""><surname>Goñi-Menoyo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Martínez-Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><forename type="middle">;</forename><surname>Villena-Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><forename type="middle">;</forename><surname>García-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paloma</forename><forename type="middle">;</forename><surname>Martínez-Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>De Pablo-Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>César</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alonso-Sánchez</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javier</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,433.97,352.32,90.54,9.02;9,97.92,364.08,94.07,9.02">Working Notes for the CLEF 2004 Workshop</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Francesca</forename><surname>Borri</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,393.60,426.70,9.02;9,97.92,405.36,74.49,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,166.43,393.60,256.52,9.02">Image Information Retrieval: An Overview of Current Research</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Goodrum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,430.01,393.60,72.73,9.02">Informing Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="63" to="66" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,423.18,426.47,9.02;9,97.92,434.94,165.22,9.02" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,222.23,423.18,245.92,9.02">Data Mining: Practical machine learning tools and techniques</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
	<note>2nd Edition</note>
</biblStruct>

<biblStruct coords="9,97.92,452.70,426.56,9.02;9,97.92,464.46,48.93,9.02" xml:id="b5">
	<monogr>
		<ptr target="http://www.irma-project.org/[Visited18/07/2005" />
		<title level="m" coord="9,97.92,452.70,231.56,9.02">IRMA project: Image Retrieval in Medical Applications</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,482.22,426.68,9.02;9,97.92,494.04,426.59,9.02;9,97.92,505.80,3.78,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,440.33,482.22,84.27,9.02;9,97.92,494.04,311.95,9.02">MIRACLE approach to ImageCLEF 2004: merging textual and content-based Image Retrieval</title>
		<author>
			<persName coords=""><surname>Martínez-Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><forename type="middle">;</forename><surname>García-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Villena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Méndez-Sáez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,421.48,494.04,103.03,9.02">CLEF 2004 proceedings</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,523.56,426.57,9.02;9,97.92,535.32,426.72,9.02;9,97.92,547.14,426.58,9.02;9,97.92,558.90,426.61,9.02;9,97.92,570.66,61.99,9.02" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,177.15,535.32,347.49,9.02;9,97.92,547.14,327.86,9.02">MIRACLE Approaches to Multilingual Information Retrieval: A Baseline for Future Research. Comparative Evaluation of Multilingual Information Access Systems</title>
		<author>
			<persName coords=""><forename type="first">José</forename><forename type="middle">L</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><forename type="middle">;</forename><surname>Villena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><surname>Fombella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">G</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><forename type="middle">;</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paloma</forename><forename type="middle">;</forename><surname>Goñi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><forename type="middle">M</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>José</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,260.66,558.90,152.33,9.02">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C;</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Brascher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="210" to="219" />
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,588.42,426.63,9.02;9,97.92,600.18,426.45,9.02;9,97.92,612.00,332.79,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,270.83,600.18,225.97,9.02">Evaluation of MIRACLE approach results for CLEF</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Villena-Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fombella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Goñi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ed</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,97.92,612.00,181.55,9.02">Working Notes for the CLEF 2003 Workshop</title>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 21-22 August</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,629.76,426.63,9.02;9,97.92,641.52,426.52,9.02;9,97.92,653.28,426.59,9.02;9,97.92,665.04,348.00,9.02" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,274.10,641.52,250.34,9.02;9,97.92,653.28,225.19,9.02">Image Retrieval: The MIRACLE Approach. Comparative Evaluation of Multilingual Information Access Systems</title>
		<author>
			<persName coords=""><forename type="first">Julio</forename><forename type="middle">;</forename><surname>Villena</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><surname>Fombella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">G</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><forename type="middle">;</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><forename type="middle">;</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paloma</forename><forename type="middle">;</forename><surname>Goñi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><forename type="middle">M</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>José</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,140.13,665.04,141.06,9.02">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C;</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Brascher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="621" to="630" />
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,682.86,426.56,9.02;9,97.92,694.62,426.58,9.02;9,97.92,706.38,239.82,9.02" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,260.43,694.62,163.45,9.02">MIRACLE results for ImageCLEF 2003</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Villena-Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fombella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Goñi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ed</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,431.70,694.62,92.80,9.02;9,97.92,706.38,88.68,9.02">Working Notes for the CLEF 2003 Workshop</title>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date>21-22 August</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,724.14,425.92,9.02" xml:id="b11">
	<monogr>
		<ptr target="http://www.cs.waikato.ac.nz/ml/weka/[Visited18/07/2005" />
		<title level="m" coord="9,97.92,724.14,147.09,9.02">Weka: Data Mining Software in Java</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
