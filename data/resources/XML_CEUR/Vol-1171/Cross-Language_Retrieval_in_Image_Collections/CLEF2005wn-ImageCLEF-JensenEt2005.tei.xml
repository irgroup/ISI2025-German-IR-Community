<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,112.74,77.84,384.61,11.84;1,187.74,92.96,234.55,11.84">Manual Query Modification and Automated Translation to Improve Cross-Language Medical Image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,272.76,119.98,64.60,8.48"><forename type="first">Jeffery</forename><forename type="middle">R</forename><surname>Jensen</surname></persName>
							<email>jensejef@ohsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology Oregon Health &amp;</orgName>
								<orgName type="institution">Science University Portland</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.68,130.78,66.63,8.48"><forename type="first">William</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
							<email>hersh@ohsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology Oregon Health &amp;</orgName>
								<orgName type="institution">Science University Portland</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,112.74,77.84,384.61,11.84;1,187.74,92.96,234.55,11.84">Manual Query Modification and Automated Translation to Improve Cross-Language Medical Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F89984E79FC9AB2BFF3C8657DDF9EBE9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation Question answering, Questions beyond factoids</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image retrieval has great potential for a variety of tasks in medicine but is currently underdeveloped. For the ImageCLEF 2005 medical task, we developed a context-based retrieval system. We then conducted our experiment using an automatic query, a manual query, and a manual/visual query. The best results were obtained from manual modification of queries, while combining those results with data from content-based image retrieval had a detrimental effect.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the medical image retrieval task of ImageCLEF is to identify and develop methods to enhance the retrieval of images based on real-world topics that a user would bring to such an image retrieval system. A test collection of 40,000 images -annotated in English, French, and/or German -and 25 topics provided the basis for experiments.</p><p>There are two general approaches to image retrieval, context-based and content-based <ref type="bibr" coords="1,425.97,515.98,76.56,8.48;1,100.38,526.78,34.09,8.48" target="#b2">(Müller, Michoux et al., 2004)</ref>. Context-based (or semantic) image retrieval uses a textual source of information to determine an image's subject matter, such as an annotation or more structured metadata. Content-based (or visual) image retrieval, on the other hand, uses features from the image, such as color, texture, shapes, etc., to determine its subject matter. The latter has historically been a difficult task, especially in the medical domain <ref type="bibr" coords="1,463.92,559.24,31.17,8.48;1,100.38,570.04,65.46,8.48" target="#b0">(Antani, Long et al., 2002)</ref>. The most success has been for "more images like this one" types of queries. There has actually been little research in the types of techniques that would achieve good performance for queries more akin to those a user might enter into a text retrieval system, such as "images showing skin cancers." Some researchers have begun to investigate hybrid methods that combine both image context and content for indexing and retrieval <ref type="bibr" coords="1,185.34,613.30,98.87,8.48" target="#b0">(Antani, Long et al., 2002)</ref>.</p><p>Oregon Health &amp; Science University (OHSU) participated in the medical image retrieval task of ImageCLEF. Our experiments were based on a context-based image retrieval system we developed, although we also attempted to improve our performance by augmenting the image output with results made available from a content-based search. Our experimental runs included an automatic query, a manually modified query with automated translation, and a manual/visual query (the manual query refined with the results of a content-based search).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>Our web-based system was developed using open-source products from the Apache Software Foundation, such as Lucene and Tomcat. The core of the system is Java Servlet that uses Lucene and other tools to index and retrieve the text of annotations ("documents") associated with each image. The Servlet uses two main components, a Library and Lucene. The Library builds a data structure to manage the ImageCLEF data, whereas Lucene provides the functionality for the Indexer and Searcher objects. The Indexer indexes the textual annotations of the images, while the Searcher retrieves documents from the indexed data. Tomcat is used to host the system and administer basic levels of security for the ImageCLEF data. Figure <ref type="figure" coords="2,462.14,197.86,4.70,8.48">1</ref> shows a graphical overview of our system.</p><p>The system works as follows. The query is sent to the system as a search-request. The Servlet then forwards the search-request to the Searcher object. The Searcher object performs the search and builds a result set from the Library. The result set is then returned to the Servlet to be displayed to the user. The user can then navigate through the results or perform a new search, if desired.</p><p>In order to index the image annotations, we developed a library component to represent the data. The library consists of collections (CASImage, MIR, PathoPic and Peir), cases, images and their respective annotations. Each collection consists of one or more cases; each case contains one or more images, and one or more annotations; and each image contains one or more annotations. Each annotation is an XML document describing either the case or an image.</p><p>Figure <ref type="figure" coords="2,132.95,665.92,4.70,8.48">1</ref> -The dashed line represents the components that were developed from Lucene, and Tomcat, which is represented by the dotted line, hosts the entire system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Servlet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Library</head><p>Indexer Searcher</p><p>Apache Lucene is a freely available and fully featured search engine (lucene.apache.org), which we have used in other retrieval evaluation forums, such as the Text Retrieval Conference (TREC) Genomics Track <ref type="bibr" coords="3,100.38,98.32,100.52,8.48" target="#b1">(Cohen, Hersh et al., 2005)</ref>. Documents are indexed by parsing of individual words within them and weighting via inverse document frequency (IDF) and term frequency (TF) functions.</p><p>Lucene is distributed with a variety of Analyzers for textual indexing. We chose Lucene's Standard Analyzer, which supports acronyms, floating point numbers, as well as the lowercasing and stop word removal. The Standard Analyzer was chosen to bolster precision. Each annotation, within the library, was indexed with three data fields, which consisted of a collection name, a file name and the contents of the file to be indexed. However, we indexed each annotation without the use of an XML parser. Therefore, every element was indexed along with its corresponding value.</p><p>Lucene provides an application program interface (API) and robust query language that supports a variety of term modifiers and searching operators. It supports two different types of terms for searching, single and phrase. These terms can be searched over the entire Library, or over specific data fields. As our data contains specific fields (the collection name, file name, and the contents of the file to be indexed), users can search over a specific collection or retrieve a known case or image.</p><p>As with most information retrieval systems, Lucene uses a scoring algorithm that sums for each query term in each document the product of the TF, the IDF, the boost factor of the term, the normalization of the document, the fraction of query terms in the document, and the normalization of the weight of the query terms, for each term in the query. The score of document d for query q consisting of terms t is calculated as follows: 3 Retrieval OHSU submitted three official runs for ImageCLEF 2005 medical image retrieval track. These included two that were purely text-based, and one that employed a combination of textual and visual searching methods.</p><formula xml:id="formula_0" coords="3,407.76,327.94,45.03,10.14">) ( *</formula><p>Our first run (OHSUauto) was purely text-based. This run was a "baseline" run of sorts, just using the text in the topics as provided with the unmodified Lucene system. We used the translations into French and German that were also provided with the topics. We took all of the images associated with each retrieved annotation for our ranked image output.</p><p>For our second run (OHSUman), we carried out manual modification of the query for each topic. For some topics, the keywords were expanded or mapped to more specific terms. Therefore, the search statements for this run were more specific. For example, one topic focused on chest x-rays showing an enlarged heart, so we added a term like cardiomegaly. Since the manual modification resulted in no longer having accurate translations, we "expanded" the manual queries with translations that were obtained from Babelfish (http://babelfish.altavista.com). The newly translated terms were added to the query with the text of each language group (English, German, and French) connecting via a union (logical OR). Figure <ref type="figure" coords="3,449.19,603.82,4.70,8.48">2</ref> shows a sample query from our second run, OHSUman.</p><p>(AP^2 PA^2 anteroposterior^2 posteroanterior^2 thoracic thorax cardiomegaly^3 heart coeur)</p><p>Figure <ref type="figure" coords="3,127.32,680.50,4.70,8.48">2</ref> -Manual query for topic 12.</p><p>In addition to the minimal term mapping and/or expansion, we also increased the significance for a group of relevant terms using Lucene's "term boosting" function. For example, for the topic focusing on chest x-rays showing an enlarged heart; we increased the significance of documents that contained the terms, chest and xray and posteroanterior and cardiomegaly, while the default significance was used for documents that contained the terms, chest or x-ray or posteroanterior, or cardiomegaly. This strategy was designed to give a higher rank to the more relevant documents within a given search. Moreover, this approach attempted to improve the precision of the results from our first run. Similar to the OHSUauto run, we returned all images associated with the retrieved annotation.</p><p>Our third run (OHSUmanviz) used a combination of textual and visual searching methods. We took the image output from OHSUman and excluded all documents that were not retrieved by the University of Geneva content-based "baseline" run (GE_M_4g.txt). In other words, we performed an intersection (logical AND) between the OHSUman and GE_M_4g.txt runs as a "combined" context-based and content-based run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Our automatic query run (OHSUauto) had the largest number of images returned for each topic. The MAP for this run was extremely low at 0.0403, which fell below the median (0.11) of the 9 submissions in the "text-only automated" category.</p><p>The manually modified queries run (OHSUman) for the most part returned large numbers of images. However, there were some topics for which it returned fewer images than the OHSUauto run. Two of these topics were those that focused on Alzheimer's disease and hand-drawn images of a person. This run was placed in the "text-only manual" category and achieved an MAP of 0.2116. Despite being the only submission in this category, this technique managed to score above the "text-only automatic" category, and as such was the best text-only run.</p><p>When we incorporated visual retrieval data (OHSUmanviz), our queries returned the smallest number images for each topic. The intent was to improve precision of the results from the previous two techniques. This run was placed in the "text and visual manual" category, and achieved an MAP of 0.1601, which was the highest score in this category. We were surprised to see that this technique's performance was less than that of our manual query technique. Recall, that both, our manual and manual/visual, techniques used the same textual queries. Therefore the difference in the overall score was a result of the visual refinement.</p><p>Figure <ref type="figure" coords="4,127.31,468.34,4.70,8.48" target="#fig_0">3</ref> illustrates the number of images returned by each of the techniques, while Figure <ref type="figure" coords="4,441.61,468.34,4.70,8.48">4</ref> show MAP per query for each run. Even though the fully automatic query technique consistently returned the largest number of images on a per-query basis, this approach rarely outperformed the others. Whereas the manual query technique did not consistently return large numbers of images for each query, it did return a good proportion of relevant images for each query. The manual/visual query technique found a good proportion of relevant images but clearly eliminated some images that the text-only search found, resulting in decreased performance.</p><p>We also noticed that all of our techniques did not perform well for queries 11-17 and 20-22. The majority of these topics were seeking radiographs, ranging from x-rays, CT scans, and MRI scans. We hope to carry out further analysis to identify other patterns where our approaches failed and, more importantly, where other techniques may provide benefit. Our ImageCLEF medical track experiments showed that manual query modification and use of an automated translation tool provide substantial benefit in retrieving relevant images. Filtering the output with findings from a baseline content-based approach diminished performance. There are likely additional permutations of these techniques that would improve performance, and future plans include testing them to identify which ones do so. We also hope to explore other methods, such as more selective use of the textual annotations, use of more advanced retrieval weighting algorithms (e.g., Okapi weighting), and better integration with content-based imaging systems.</p><p>We also aim to build a more robust image retrieval system proper, which will both simplify further experiments as well as give us the capability to employ real users to better manually modify queries and/or provide translation. Additional work we aim to carry out includes better elucidating the needs of those who use image retrieval systems to better inform our own system as well as test collections to assess systems of others and ourselves.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,100.38,358.00,255.59,8.48"><head>Figure 3 -</head><label>3</label><figDesc>Figure 3 -Number of images returned per query, for each technique</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,100.38,275.74,408.06,8.48;6,134.22,286.60,368.76,8.48;6,134.22,297.46,327.50,8.48" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,230.72,275.74,277.72,8.48;6,134.22,286.60,189.76,8.48">A biomedical information system for combined content-based retrieval of spine x-ray images and associated text information</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,330.18,286.60,172.80,8.48;6,134.22,297.46,190.29,8.48">Proceedings of the 3rd Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the 3rd Indian Conference on Computer Vision, Graphics and Image Processing<address><addrLine>ICVGIP; Ahamdabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,100.38,308.20,406.89,8.48;6,134.22,319.06,267.44,8.48;6,134.22,329.92,187.46,8.48" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,236.96,308.20,270.31,8.48;6,134.22,319.06,152.55,8.48">Using co-occurrence network structure to extract synonymous gene and protein names from MEDLINE abstracts</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<ptr target="http://www.biomedcentral.com/1471-2105/6/103" />
	</analytic>
	<monogr>
		<title level="j" coord="6,292.56,319.06,76.67,8.48">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">103</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,100.38,340.66,379.62,8.48;6,134.22,351.52,365.57,8.48;6,134.22,362.32,34.04,8.48" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,247.38,340.66,232.61,8.48;6,134.22,351.52,187.51,8.48">A review of content-based image retrieval systems in medical applications-clinical benefits and future directions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Michoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,327.54,351.52,168.43,8.48">International Journal of Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
