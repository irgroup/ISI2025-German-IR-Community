<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,130.44,75.29,334.31,12.58">Combining Text and Image Queries at ImageCLEF2005</title>
				<funder ref="#_4zw3G4Z #_sAP5nxp">
					<orgName type="full">National Science Council, Taiwan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,180.12,113.91,72.58,8.74"><forename type="first">Yih-Cheng</forename><surname>Chang</surname></persName>
							<email>ycchang@nlg.csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University Taipei</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,260.68,113.91,63.93,8.74"><forename type="first">Wen-Cheng</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University Taipei</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Medical Informatics</orgName>
								<orgName type="institution">Tzu Chi University Hualien</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.20,113.91,59.75,8.74"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
							<email>hhchen@csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University Taipei</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,130.44,75.29,334.31,12.58">Combining Text and Image Queries at ImageCLEF2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">32EDC68A7F49E13A221C561C5664B7AA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ACM Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval---Retrieval models</term>
					<term>Relevance feedback Cross language image retrieval</term>
					<term>cross-media translation</term>
					<term>automatic image annotation</term>
					<term>classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our methods for the tasks of bilingual ad hoc retrieval and automatic annotation in ImageCLEF 2005. In ad hoc task, we propose a feedback method for cross-media translation in a visual run, and combine the results of visual and textual runs to generate the final result. Experimental results show that our feedback method performs well. Comparing to initial visual retrieval, average precision is increased from 8% to 34% after feedback. The performance is increased to 39% if we combine the results of textual run and visual run with pseudo relevance feedback. In automatic annotation task, we propose several methods to measure the similarity between a test image and a category, and a test image is classified to the most similar category. Experimental results show that the proposed approaches have good performance, but the simplest 1-NN method has the best performance. We will analyze these results in the paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While digital images have an explosive growth, cross-language image retrieval and automatic annotation become very important nowadays. An automatic annotation system can help us to annotate large amount of images, and a cross-language image retrieval system retrieves images that are annotated in different languages.</p><p>Two types of approaches, i.e., content-based and text-based approaches, are usually adopted in image retrieval <ref type="bibr" coords="1,70.92,537.69,10.64,8.74" target="#b0">[1]</ref>. Content-based image retrieval (CBIR) uses low-level visual features to retrieve images. In such a way, it is unnecessary to annotate images and translate users' queries. However, due to the semantic gap between image visual features and high-level concepts <ref type="bibr" coords="1,261.98,560.67,10.64,8.74" target="#b1">[2]</ref>, it's still hard to use a CBIR system to retrieve images with correct semantic meanings. Integrating textual information may help a CBIR system to cross the semantic gap and improve retrieval performance.</p><p>Recently many approaches tried to combine text-and content-based methods for image retrieval. A simple approach is conducting text-and content-based retrieval separately and merging the retrieval results of the two runs <ref type="bibr" coords="1,92.86,618.09,10.83,8.74" target="#b2">[3,</ref><ref type="bibr" coords="1,103.69,618.09,7.22,8.74" target="#b3">4]</ref>. In contrast to the parallel approach, a pipeline approach uses textual or visual information to perform initial retrieval, and then uses the other features to filter out irrelevant images <ref type="bibr" coords="1,447.12,629.61,10.63,8.74" target="#b4">[5]</ref>. In these two approaches, textual and visual queries are formulated by users and do not directly influence each other. Another approach, i.e., transformation-based approach, tries to mine the relations between images and text, and uses the mined relations to transform textual information into visual one, and vice versa <ref type="bibr" coords="1,434.18,664.05,10.64,8.74" target="#b5">[6]</ref>. In this paper we try another method to transform visual features to textual ones. We use a feedback method to transform a visual query into textual one. The text descriptions of the top retrieved images of the initial retrieval are used for feedback to conduct a second retrieval. The new textual information can help us cache the semantic meaning of a visual query, and thus improve retrieval performance.</p><p>The correlation between images and text can be used to annotate images. However, the training data of automatic annotation task has no textual information, thus we use only visual features to classify images. In automatic annotation task, we try several classification methods. A nearest neighbor (1-NN) method is considered as our baseline. We propose several methods to measure the similarity between a test image and a class, and a test image is classified to the most similar class. We propose a method that measures the similarity between an image and a class by averaging the similarity scores of the top n most similar images in the class. Besides, we also propose an approach that divides a class into several smaller classes and classifies a test image according to the similarities between the test image and the centroids of the smaller classes.</p><p>The rest of the paper is organized as fellows. Section 2 and 3 introduce the proposed approaches and experimental results of bilingual ad hoc retrieval task and automatic annotation task, respectively. Section 4 concludes the remark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Bilingual Ad Hoc Retrieval Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feedback Method for Cross-Media Translation</head><p>To do cross-media translation between visual and textual representations, several correlation-based approaches have been proposed in automatic annotation task. Those approaches model the correlation between text and visual representation, and use the mined relation to translate images to text descriptions. Mori, Takahashi and Oka <ref type="bibr" coords="2,90.54,286.47,11.71,8.74" target="#b6">[7]</ref> divided images into grids, and then the grids of all images were clustered. Co-occurrence information was used to estimate the probability of each word for each cluster. Duygulu, et al. <ref type="bibr" coords="2,415.98,297.99,11.61,8.74" target="#b7">[8]</ref> used blobs to represent images. First, images are segmented into regions using a segmentation algorithm like Normalized Cuts <ref type="bibr" coords="2,510.22,309.45,10.64,8.74" target="#b8">[9]</ref>. All regions are clustered and each cluster is assigned a unique label (blob token). EM algorithm is used to construct a probability table that links blob tokens with word tokens. Jeon, Lavrenko, and Manmatha <ref type="bibr" coords="2,507.86,332.43,16.67,8.74" target="#b9">[10]</ref> proposed a cross-media relevance model (CMRM) to learn the joint distribution of blobs and words. They further proposed continuous-space relevance model (CRM) that learned the joint probability of words and regions, rather than blobs <ref type="bibr" coords="2,175.11,366.93,14.99,8.74" target="#b10">[11]</ref>.</p><p>The above approaches use the relation between text and visual representation as a bridge to translate image to text. However, it is hard to learn all relations between all visual and textual features. In the experiments mentioned above, relations are learned from only hundreds of keywords in textual annotation. Another problem is that the degree of ambiguity of the relations is usually high. For example, visual feature "red circle" may have many meanings such as sun set, red flower, and red ball. Similarly, the word "flower" may look very different, e.g. different color and shape, in images.</p><p>In this paper we translate visual and textual features without learning correlations. We treat the retrieved images and their text descriptions as aligned documents, and a corpus-based method that uses pseudo relevance feedback is adopted to translate visual or textual features and generate a new query.</p><p>In cross-language image retrieval, giving a set of images I={i 1 ,i 2 ,…,i m } with text descriptions T I,L1 ={t 1 ,t 2 ,…,t m } in language L1, users use textual query Q L2 in language L2 (L2≠L1) and example images E={e 1 ,e 2 ,…,e p } to retrieve relevant images from I.</p><p>We use a feedback method in a visual run to translate the visual query into textual one as follows. We first use an example image e i as initial query, and use a CBIR system, i.e. VIPER <ref type="bibr" coords="2,395.37,527.79,15.36,8.74" target="#b11">[12]</ref>, to retrieve images from I. The retrieved images are R={r i1 ,r i2 ,…,r in } and their text descriptions are T R,L1 ={t ri1 ,t ri2 ,…,t ril } in language L1. Then we use the text descriptions of the top k retrieved images to construct a new textual query. The new textual query can be seen as a translation of initial visual query. In the feedback run, we submit the new textual query to a text-based retrieval system, i.e. Okapi <ref type="bibr" coords="2,267.42,573.75,15.33,8.74" target="#b12">[13]</ref>, to retrieve images from I.</p><p>In addition to the visual feedback run, we also conduct a text-based run using the textual query in the test set. We use the method we proposed last year <ref type="bibr" coords="2,242.19,596.73,16.67,8.74" target="#b13">[14]</ref> to translate textual query Q L2 into query Q L1 in language L1, and submit the translated query Q L1 to Okapi system to retrieve images. The results of textual run and visual feedback run can be combined. The similarity scores of images in the two runs are normalized and linearly combined using equal weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experimental Results</head><p>In the experiments, the text-based retrieval system used is Okapi IR system, and the content-based retrieval system used is VIPER system. For textual index in Okapi, the caption text, &lt;HEADLINE&gt; and &lt;CATEGORIES&gt; sections of English captions are used for indexing. The weighting function used is BM25. Chinese queries and example images are used as our source queries.</p><p>We submitted four Chinese-English cross-lingual runs, two English monolingual runs and one visual run in CLEF 2005 image track. In English monolingual runs, using narrative or not using narrative will be compared.</p><p>In the four cross-lingual runs, combining with visual run or not combing with visual run, and using narrative or not using narrative will be compared. The details of the cross-lingual runs and visual run are described as follows.</p><p>(1) NTU-adhoc05-CE-T-W This run use textual queries without narrative to retrieve images. We use query translation method we used last year to translate Chinese queries into English to retrieve images using textual index. (2) NTU-adhoc05-CE-TN-W-Ponly This run use textual queries with narrative. We only use the positive information in narrative. The sentences that contain phrase "不算相關 (are not relevant)" are removed.</p><p>(3) NTU-adhoc05-EX-prf This run is a visual run with pseudo relevance feedback (the query becomes textual one after feedback).</p><p>We use the retrieval results of VIPER system provided by ImageCLEF as our initial retrieval results, and use the text descriptions of the top 2 images to construct a new textual query in feedback run. The caption text in descriptions is used to construct a query. The textual query is submitted to Okapi IR system to retrieve images. This run merges the results of NTU-adhoc05-CE-TN-W-Ponly and NTU-adhoc05-EX-prf. From Table <ref type="table" coords="3,130.97,337.35,3.77,8.74" target="#tab_0">1</ref>, the average precision of monolingual retrieval using title field only is 0.3952. Comparing to the performance of last year (0.6304), this year's query set is much harder. After adding narrative information, average precision is increased slightly. The performance of Chinese-English cross-lingual textual run is about 60.7% of English monolingual run. It shows that there are still many errors in language translation. From Table <ref type="table" coords="3,98.46,383.31,3.75,8.74" target="#tab_1">2</ref>, the performance of initial visual run, i.e. VIPER, is not good enough. Text-based runs, even cross-lingual runs, perform much better than initial visual run. It shows that semantic information is very important for the queries of this year. After feedback, the performance is increased dramatically from 0.0829 to 0.3452. The result shows that the feedback method transforms visual information into textual one well. Combining textual and visual feedback runs further improves retrieval performance. The combined runs perform better than the individual runs. The results show that it needs more information to define users' information need. The feedback textual query has additional information and help user's textual query perform better.  For most queries, monolingual run has better performance than visual feedback run. We can say that there are translation errors in cross-media translation. There are ten topics in which the performance of visual feed back run is better than that of monolingual run. This is probably because that the user's information need is not detailed described in a textual query, i.e. some information is lost. Also, the words used in textual query and image descriptions may be inconsistency. Thus, it is hard to retrieve all relevant images by the textual queries formulated by users. We can use additional information that is not provided directly by users to retrieve more relevant images. The constructed query in feedback run has additional information that comes from example images. For example, when a user wants to find images that have aircraft on land, using query "aircraft in military air base" may be better than using "aircraft on the ground". This is because that the descriptions of images don't mention that aircraft is on the ground directly, but aircrafts in military air base are very likely to be parked and thus are on the ground. The additional information "military air base" is obtained because that it is mentioned in the descriptions of images retrieved by example images using a CBIR system. Comparing the performances of runs that combining visual feedback run or not, we can find that most topics perform better after combining. This is probably because that the additional information in feedback run helps our system retrieves images more precisely, and that queries constructed in feedback run could recover translation errors in a cross-lingual run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Automatic Annotation Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classification Approaches</head><p>The automatic annotate task in ImageCLEF 2005 can be seen as a classification task, since each image can only be annotated with one word (category). In classification task, k-nearest neighbor (k-NN) method is a usually adopted approach <ref type="bibr" coords="4,144.38,640.65,15.35,8.74" target="#b14">[15]</ref>. Performance for different categories in k-NN method usually depends on the number of training data in each category. Test images tend to be classified to the categories that have many training data (We will show this later). To solve this problem, computing several representative data is used to normalize the number of training data in each category. We can reduce the number of training data to 1 using a centroid to represent a category. But sometimes using only one centroid to represent a whole category is not sufficient if the images in the category are very different. For example, the images of the flank and the front of skull look very different. Using two centroids of two smaller classes to represent category "skull" is better than using only one centroid that is between the flank and front of skull. In this task, we use clustering to help us to find the representative data of each category. We assume that the images that belong to the same cluster and the same category are very similar, and can be represented by a centroid. The detail of our method is described as follows.</p><p>(1) First we use k-means algorithm to cluster all training data. The images in a cluster may belong to different categories. (2) After clustering, we compute the centroids of each category in each cluster.</p><p>(3) Given a test image, we compute the distances between it and each centroids, and the test image is classified to the category with the shortest distance. The second method we used is to compute the similarities between a test image and each category, and then classify the test image to the most similar category. The similarity between a test image and a class is measured by averaging the similarity values between the test image and the top 2 most similar images in the class. A test image is classified to the class that has the highest similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>In this task we submit three runs. The three runs use the same image features. The difference between them is the classification method used. The image features are extracted in the following way. First we resize images to 256 x 256 pixels and segment each image into 32 x 32 blocks (each block is 8 x 8 pixels). Then we compute the average gray value of each block to construct a vector with 1024 elements. We use this vector to represent an image, and the similarity between two images is measured by cosine formula. The details of each run are described as follows.</p><p>(1) NTU-annotate05-1NN This run is our based line. It uses 1-NN method to classify each image. (2) NTU-annotate05-Top2</p><p>This run uses the second method described in Section 3.1. We compute the similarity between a test image and category using the top 2 nearest images in each category, and classify the test image to the most similar category. (3) NTU-annotate05-SC This run uses the first method described in Section 3.1. Training data is clustered using k-means algorithm (k=1000). We compute the centroids of each category in each cluster, and classify a test image to the category of the nearest centroid. The results of official runs are shown in Table <ref type="table" coords="5,270.05,419.07,3.77,8.74" target="#tab_2">3</ref>. The results show that 1-NN method is very useful. 1-NN has the same performance as run NTU-annotate05-Top2, but it doesn't need to compute average similarity, thus it is faster than top2 method.</p><p>The performance of run NTU-annotate05-SC is worse than run NTU-annotate05-1NN. Normalizing the number of training data in each category may have a trade-off. Normalization may increase the performance of categories that have less training data, but decrease the performance of categories that have more training data. Table <ref type="table" coords="5,333.22,476.49,5.01,8.74" target="#tab_3">4</ref> shows the error rate of individual categories. The categories that have a lot of training data are listed in the upper part of Table <ref type="table" coords="5,401.80,488.01,3.76,8.74" target="#tab_3">4</ref>, and the categories that have a few training data are in the lower part. From Table <ref type="table" coords="5,298.80,499.47,3.76,8.74" target="#tab_3">4</ref>, the performances of categories with a lot of training data are better than that of categories with a few training data. For the categories with a lot of training data, 1-NN method performs better than normalization method (run SC). In contrast, normalization method performs much better than 1-NN method for the categories with a few training data. It shows that normalization method could reduce the problem that prefers classifying images to large categories. The reason that the overall performance of normalization method is worse than that of 1-NN method is that large categories have more test images and thus have more influence on the final result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In bilingual ad hoc retrieval task, we propose a simple and useful feedback method for cross-language image retrieval. We transform visual features into textual ones without learning correlations. Experimental results show that the proposed feedback approach performs well. Comparing to initial visual retrieval, average precision is increased from 8% to 34% after feedback. The feedback textual query has additional information that comes from example images, and help user's textual query perform better. After combining textual and visual feedback runs, average precision is increased from 0.2399 to 0.3977 and from 0.3952 to 0.5053 in cross-lingual and monolingual experiments, respectively. We will test our method in other image collections in the future.</p><p>In automatic annotation task, we propose a method that normalizes the number of training data of each category. The normalization approach may have a trade-off. It may increase the performance of categories that have less training data, but decrease the performance of categories that have more training data. We will try our method in different collections and study what is the suitable time to use normalization since using normalization may have a trade-off.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,82.92,270.33,130.97,8.74;3,100.92,284.91,423.47,8.74;3,100.92,296.37,351.15,8.74;3,82.92,311.31,165.78,8.74"><head></head><label></label><figDesc>(4) NTU-adhoc05-CE-T-WEprf This run merges the results of NTU-adhoc05-CE-T-W and NTU-adhoc05-EX-prf. The similarity scores of images in the two runs are normalized and linearly combined using equal weight 0.5. (5) NTU-adhoc05-CE-TN-WEprf-Ponly</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,213.00,76.74,169.33,9.02"><head>Figure 1 .Figure 1</head><label>11</label><figDesc>Figure 1. Average precision of each query</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,92.64,486.60,395.08,167.79"><head>Table 1 .</head><label>1</label><figDesc>Results of official runs</figDesc><table coords="3,92.64,506.74,395.08,147.65"><row><cell>Run</cell><cell cols="2">Features in Query Text Visual</cell><cell>Average Precision</cell></row><row><cell>NTU-adhoc05-CE-T-W</cell><cell>Chinese (Title)</cell><cell>None</cell><cell>0.2399</cell></row><row><cell>NTU-adhoc05-CE-TN-W-Ponly</cell><cell>Chinese (Title+ Positive Narrative)</cell><cell>None</cell><cell>0.2453</cell></row><row><cell>NTU-adhoc05-CE-T-WEprf</cell><cell>Chinese (Title)</cell><cell>Example image</cell><cell>0.3977</cell></row><row><cell>NTU-adhoc05-CE-TN-WEprf-Ponly</cell><cell>Chinese (Title+ Positive Narrative)</cell><cell>Example image</cell><cell>0.3993</cell></row><row><cell>NTU-adhoc05-EX-prf</cell><cell>English (feedback query)</cell><cell>Example image (initial query)</cell><cell>0.3425</cell></row><row><cell>NTU-adhoc05-EE-T-W</cell><cell>English</cell><cell>None</cell><cell>0.3952</cell></row><row><cell>NTU-adhoc05-EE-TN-W-Ponly</cell><cell>English (Title+ Positive Narrative)</cell><cell>None</cell><cell>0.4039</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,93.66,669.12,392.08,74.13"><head>Table 2 .</head><label>2</label><figDesc>Performances</figDesc><table coords="3,93.66,669.39,392.08,73.86"><row><cell></cell><cell cols="3">of unofficial runs (NTU-adhoc05-EE-T-WEprf merges the results of</cell></row><row><cell cols="3">NTU-adhoc05-EE-T-W and NTU-adhoc05-EX-prf)</cell><cell></cell></row><row><cell>Run</cell><cell cols="2">Features in Query Text Visual</cell><cell>Average Precision</cell></row><row><cell>NTU-adhoc05-EE-T-WEprf</cell><cell>English (Title)</cell><cell>Example image</cell><cell>0.5053</cell></row><row><cell>Initial Visual Run (VIPER)</cell><cell>None</cell><cell>Example image</cell><cell>0.0829</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,129.36,602.76,333.42,46.11"><head>Table 3 .</head><label>3</label><figDesc>Results of official runs</figDesc><table coords="5,129.36,624.22,333.42,24.65"><row><cell>Run</cell><cell>NTU-annotate05-1NN</cell><cell>NTU-annotate05-Top2</cell><cell>NTU-annotate05-SC</cell></row><row><cell>Error Rate</cell><cell>21.7 %</cell><cell>21.7 %</cell><cell>22.5 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,71.34,72.90,452.67,322.05"><head>Table 4 .</head><label>4</label><figDesc>Error rate of individual categories. The upper part shows the top 10 categories that have a lot of training data, and the lower part shows the categories that have a few training data</figDesc><table coords="6,104.04,105.28,384.64,289.67"><row><cell>Cat.</cell><cell>#Training image</cell><cell>#Test image</cell><cell>Error rate (1NN)</cell><cell>Error rate (Top2)</cell><cell>Error rate (SC)</cell></row><row><cell>12</cell><cell>2563</cell><cell>297</cell><cell>0.003367</cell><cell>0.003367</cell><cell>0.016835</cell></row><row><cell>34</cell><cell>880</cell><cell>79</cell><cell>0.012658</cell><cell>0.012658</cell><cell>0.000000</cell></row><row><cell>6</cell><cell>576</cell><cell>67</cell><cell>0.194030</cell><cell>0.223881</cell><cell>0.253731</cell></row><row><cell>1</cell><cell>336</cell><cell>38</cell><cell>0.000000</cell><cell>0.000000</cell><cell>0.078947</cell></row><row><cell>25</cell><cell>284</cell><cell>36</cell><cell>0.138889</cell><cell>0.166667</cell><cell>0.194444</cell></row><row><cell>28</cell><cell>228</cell><cell>16</cell><cell>0.312500</cell><cell>0.312500</cell><cell>0.250000</cell></row><row><cell>5</cell><cell>225</cell><cell>25</cell><cell>0.080000</cell><cell>0.080000</cell><cell>0.080000</cell></row><row><cell>17</cell><cell>217</cell><cell>24</cell><cell>0.125000</cell><cell>0.125000</cell><cell>0.208333</cell></row><row><cell>3</cell><cell>215</cell><cell>24</cell><cell>0.291667</cell><cell>0.291667</cell><cell>0.250000</cell></row><row><cell>18</cell><cell>205</cell><cell>12</cell><cell>0.416667</cell><cell>0.500000</cell><cell>0.416667</cell></row><row><cell>Avg.</cell><cell>572.9</cell><cell>61.8</cell><cell>0.157478</cell><cell>0.171574</cell><cell>0.174896</cell></row><row><cell>51</cell><cell>9</cell><cell>1</cell><cell>0.000000</cell><cell>0.000000</cell><cell>0.000000</cell></row><row><cell>52</cell><cell>9</cell><cell>1</cell><cell>0.000000</cell><cell>0.000000</cell><cell>0.000000</cell></row><row><cell>55</cell><cell>10</cell><cell>2</cell><cell>1.000000</cell><cell>1.000000</cell><cell>1.000000</cell></row><row><cell>53</cell><cell>15</cell><cell>3</cell><cell>0.333333</cell><cell>0.000000</cell><cell>0.333333</cell></row><row><cell>15</cell><cell>15</cell><cell>3</cell><cell>0.666667</cell><cell>0.666667</cell><cell>0.666667</cell></row><row><cell>24</cell><cell>17</cell><cell>4</cell><cell>1.000000</cell><cell>0.750000</cell><cell>0.750000</cell></row><row><cell>35</cell><cell>18</cell><cell>4</cell><cell>0.750000</cell><cell>1.000000</cell><cell>0.500000</cell></row><row><cell>37</cell><cell>22</cell><cell>2</cell><cell>1.000000</cell><cell>1.000000</cell><cell>1.000000</cell></row><row><cell>16</cell><cell>23</cell><cell>1</cell><cell>1.000000</cell><cell>1.000000</cell><cell>0.000000</cell></row><row><cell>46</cell><cell>30</cell><cell>1</cell><cell>1.000000</cell><cell>1.000000</cell><cell>0.000000</cell></row><row><cell>Avg.</cell><cell>16.8</cell><cell>2.2</cell><cell>0.675000</cell><cell>0.641667</cell><cell>0.425000</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>Research of this paper was partially supported by <rs type="funder">National Science Council, Taiwan</rs>, under the contracts <rs type="grantNumber">NSC93-2752-E-001-001-PAE</rs> and <rs type="grantNumber">NSC94-2752-E-001-001-PAE</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4zw3G4Z">
					<idno type="grant-number">NSC93-2752-E-001-001-PAE</idno>
				</org>
				<org type="funding" xml:id="_sAP5nxp">
					<idno type="grant-number">NSC94-2752-E-001-001-PAE</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,74.29,707.14,450.16,7.85;6,88.92,717.46,23.27,7.85" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,152.87,707.14,238.92,7.85">Image Information Retrieval: An Overview of Current Research</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Goodrum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,399.31,707.14,73.00,7.85">Information Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="63" to="66" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.30,727.78,450.18,7.85;6,88.92,738.16,435.47,7.85;6,88.92,748.48,26.29,7.85" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,229.16,727.78,295.32,7.85;6,88.92,738.16,86.28,7.85">Semantic Feature Layers in Content-based Image Retrieval: Implementation of Human World Features</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Eidenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Breiteneder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,197.09,738.16,194.59,7.85">Proceedings of International Conference on Control</title>
		<meeting>International Conference on Control</meeting>
		<imprint>
			<publisher>Automation, Robotic and Vision</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,74.30,73.18,450.17,7.85;7,88.92,83.50,435.50,7.85;7,88.92,93.82,432.47,7.85" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,297.72,73.18,226.74,7.85;7,88.92,83.50,153.65,7.85">Cross-Media Feedback Strategies: Merging Text and Image Information to Improve Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Besançon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hède</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">A</forename><surname>Moellic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fluhr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,265.51,83.50,258.91,7.85;7,88.92,93.82,233.49,7.85">Multilingual Information Access for Text, Speech and Images: 5th Workshop of the Cross-Language Evaluation Forum, CLEF 2004</title>
		<title level="s" coord="7,328.67,93.82,23.02,7.85">LNCS</title>
		<imprint>
			<publisher>Springer-Verlag GmbH</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="709" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,74.30,104.20,450.09,7.85;7,88.92,114.52,435.39,7.85;7,88.92,124.84,435.45,7.85;7,88.92,135.22,58.53,7.85" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,405.37,104.20,119.03,7.85;7,88.92,114.52,235.48,7.85">Dublin City University at CLEF 2004: Experiments with the ImageCLEF St. Andrew&apos;s Collection</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Groves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khasin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lam-Adesina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mellebeek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,343.22,114.52,181.09,7.85;7,88.92,124.84,296.25,7.85">Multilingual Information Access for Text, Speech and Images: 5th Workshop of the Cross-Language Evaluation Forum, CLEF 2004</title>
		<title level="s" coord="7,391.79,124.84,23.00,7.85">LNCS</title>
		<imprint>
			<publisher>Springer-Verlag GmbH</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="653" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,74.30,145.54,450.16,7.85;7,88.92,155.86,435.60,7.85;7,88.92,166.24,435.54,7.85;7,88.92,176.56,155.80,7.85" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,366.62,155.86,157.90,7.85;7,88.92,166.24,87.17,7.85">Lazy Users and Automatic Video Retrieval Tools in (the) Lowlands</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Baan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Ballegooij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusenbroek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Den Hartog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>List</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Raaijmakers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Todoran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vendrig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,196.81,166.24,245.99,7.85">Proceedings of the Tenth Text REtrieval Conference (TREC 2001)</title>
		<meeting>the Tenth Text REtrieval Conference (TREC 2001)</meeting>
		<imprint>
			<publisher>National Institute of Standards and Technology</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,74.30,186.94,450.11,7.85;7,88.92,197.26,331.68,7.85" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,233.75,186.94,287.14,7.85">Integrating Textual and Visual Information for Cross-Language Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,101.21,197.26,287.11,7.85">Proceedings of the Second Asia Information Retrieval Symposium (AIRS 2005)</title>
		<meeting>the Second Asia Information Retrieval Symposium (AIRS 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,74.30,207.58,450.19,7.85;7,88.92,217.96,435.55,7.85;7,88.92,228.28,77.76,7.85" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,226.43,207.58,298.06,7.85;7,88.92,217.96,41.66,7.85">Image-to-Word Transformation Based on Dividing and Vector Quantizing Images with Words</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Oka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,153.58,217.96,370.89,7.85;7,88.92,228.28,44.80,7.85">Proceedings of the First International Workshop on Multimedia Intelligent Storage and Retrieval Management</title>
		<meeting>the First International Workshop on Multimedia Intelligent Storage and Retrieval Management</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,74.30,238.60,450.14,7.85;7,88.92,248.98,435.43,7.85;7,88.92,259.30,27.41,7.85" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,288.51,238.60,235.93,7.85;7,88.92,248.98,109.47,7.85">Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,218.88,248.98,246.47,7.85">Proceedings of Seventh European Conference on Computer Vision</title>
		<meeting>Seventh European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="97" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,74.30,269.62,450.14,7.85;7,88.92,280.00,129.49,7.85" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,173.48,269.62,152.89,7.85">Normalized Cuts and Image Segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,333.01,269.62,191.44,7.85;7,88.92,280.00,41.34,7.85">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,78.43,290.32,445.98,7.85;7,88.92,300.64,435.58,7.85;7,88.92,311.02,243.67,7.85" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,249.27,290.32,275.15,7.85;7,88.92,300.64,25.11,7.85">Automatic Image Annotation and Retrieval using Cross-Media Relevance Models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,134.84,300.64,389.66,7.85;7,88.92,311.02,128.59,7.85">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2003)</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2003)</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,78.17,321.34,446.29,7.85;7,88.92,331.66,303.58,7.85" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,251.13,321.34,182.21,7.85">A Model for Learning the Semantics of Pictures</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,454.47,321.34,70.00,7.85;7,88.92,331.66,271.06,7.85">Proceedings of the Seventeenth Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Seventeenth Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,78.44,342.04,445.98,7.85;7,88.92,352.36,435.48,7.85;7,88.92,362.68,96.11,7.85" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,286.32,342.04,238.10,7.85;7,88.92,352.36,277.62,7.85">Content-based query of image databases, inspirations from text retrieval: Inverted files, frequency-based weights and relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Raki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,389.44,352.36,134.96,7.85;7,88.92,362.68,30.04,7.85">Scandinavian Conference on Image Analysis</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="143" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,78.44,373.06,445.90,7.85;7,88.92,383.38,438.59,7.85;7,88.92,393.76,32.28,7.85" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,257.44,373.06,251.11,7.85">Okapi at TREC-7: Automatic Ad Hoc, Filtering, VLC and Interactive</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,88.92,383.38,234.66,7.85">Proceedings of the Seventh Text REtrieval Conference (TREC-7)</title>
		<meeting>the Seventh Text REtrieval Conference (TREC-7)</meeting>
		<imprint>
			<publisher>National Institute of Standards and Technology</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="253" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,78.41,404.08,446.12,7.85;7,88.92,414.40,435.45,7.85;7,88.92,424.78,239.42,7.85" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,253.21,404.08,253.74,7.85">From Text to Image: Generating Visual Query for Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,88.92,414.40,435.45,7.85;7,88.92,424.78,40.52,7.85">Multilingual Information Access for Text, Speech and Images: 5th Workshop of the Cross-Language Evaluation Forum, CLEF 2004</title>
		<imprint>
			<publisher>Springer-Verlag GmbH</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="664" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,78.43,435.10,446.00,7.85;7,88.92,445.42,435.56,7.85;7,88.92,455.80,160.56,7.85" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,486.46,435.10,37.96,7.85;7,88.92,445.42,288.04,7.85">Automatic categorization of medical images for content-based retrieval and data mining</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">O</forename><surname>Güld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">B</forename><surname>Wein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,389.87,445.42,134.61,7.85;7,88.92,455.80,30.87,7.85">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="143" to="155" />
			<date type="published" when="2005">2005</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
