<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.60,149.76,391.52,12.93">The CLEF 2005 Cross-Language Image Retrieval Track</title>
				<funder ref="#_TqJXKsH">
					<orgName type="full">EU Sixth Framework Programme</orgName>
				</funder>
				<funder ref="#_MShPAkw">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_dw9eHUw">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_aCA7QbU">
					<orgName type="full">German Research Community DFG</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,93.96,187.01,54.20,9.96"><forename type="first">Paul</forename><surname>Clough</surname></persName>
							<email>p.d.clough@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Studies</orgName>
								<orgName type="institution">Sheffield University</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,158.64,187.01,72.54,9.96"><forename type="first">Henning</forename><surname>Müeller</surname></persName>
							<email>henning.mueller@sim.hcuge.ch</email>
							<affiliation key="aff1">
								<orgName type="department">Medical Informatics Service</orgName>
								<orgName type="institution">Geneva University and Hospitals</orgName>
								<address>
									<settlement>Geneva</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.68,187.01,78.76,9.96"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
							<email>deselaers@cs.rwth-aachen.de</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Lehrstuhl für Informatik VI</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>D-52056</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.96,187.01,81.54,9.96"><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
							<email>michael.grubinger@research.vu.edu.au</email>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Mathematics</orgName>
								<orgName type="institution">Victoria University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,423.00,187.01,78.79,9.96"><forename type="first">Thomas</forename><surname>Lehmann</surname></persName>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Medical Informatics</orgName>
								<orgName type="department" key="dep2">Medical Faculty</orgName>
								<orgName type="institution">Aachen University of Technology (RWTH)</orgName>
								<address>
									<addrLine>Pauwelsstr. 30</addrLine>
									<postCode>D-52057</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.44,199.01,61.25,9.96"><forename type="first">Jeffery</forename><surname>Jensen</surname></persName>
							<email>jensejef@ohsu.edu</email>
							<affiliation key="aff5">
								<orgName type="department">Biomedical Informatics</orgName>
								<orgName type="institution">Oregon Health and Science University</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>Oregon</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.87,199.01,63.20,9.96"><forename type="first">William</forename><surname>Hersh</surname></persName>
							<email>hersh@ohsu.edu</email>
							<affiliation key="aff5">
								<orgName type="department">Biomedical Informatics</orgName>
								<orgName type="institution">Oregon Health and Science University</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>Oregon</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="2,241.53,478.33,43.87,6.97"><forename type="first">William</forename><surname>Rev</surname></persName>
						</author>
						<author>
							<persName coords="2,287.92,478.33,16.39,6.97"><surname>Swan</surname></persName>
						</author>
						<author>
							<persName coords="2,206.27,558.13,69.78,6.97"><forename type="first">Karen</forename><forename type="middle">A</forename><surname>Johnstone</surname></persName>
						</author>
						<author>
							<persName coords="2,285.29,558.13,55.39,6.97"><forename type="first">Thomas</forename><surname>Rodger</surname></persName>
						</author>
						<title level="a" type="main" coord="1,105.60,149.76,391.52,12.93">The CLEF 2005 Cross-Language Image Retrieval Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0963E64DE07549E55509BB1A565A947D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ministers ][ identified male ][ dress -clerical ] coronal</term>
					<term>cranium</term>
					<term>musculosceletal system 02 plain radiography</term>
					<term>coronal</term>
					<term>facial cranium</term>
					<term>musculosceletal system 03 plain radiography</term>
					<term>coronal</term>
					<term>cervical spine</term>
					<term>musculosceletal system 04 plain radiography</term>
					<term>coronal</term>
					<term>thoracic spine</term>
					<term>musculosceletal system 05 plain radiography</term>
					<term>coronal</term>
					<term>lumbar spine</term>
					<term>musculosceletal system 06 plain radiography</term>
					<term>coronal</term>
					<term>hand</term>
					<term>musculosceletal system 07 plain radiography</term>
					<term>coronal</term>
					<term>radio carpal joint</term>
					<term>musculosceletal system 08 plain radiography</term>
					<term>coronal</term>
					<term>handforearm</term>
					<term>musculosceletal system 09 plain radiography</term>
					<term>coronal</term>
					<term>elbow</term>
					<term>musculosceletal system 10 plain radiography</term>
					<term>coronal</term>
					<term>upper arm</term>
					<term>musculosceletal system 11 plain radiography</term>
					<term>coronal</term>
					<term>shoulder</term>
					<term>musculosceletal system 12 plain radiography</term>
					<term>coronal</term>
					<term>chest</term>
					<term>unspecified 13 plain radiography</term>
					<term>coronal</term>
					<term>bones</term>
					<term>musculosceletal system 14 plain radiography</term>
					<term>coronal</term>
					<term>abdomen</term>
					<term>gastrointestinal system 15 plain radiography</term>
					<term>coronal</term>
					<term>abdomen</term>
					<term>uropoietic system 16 plain radiography</term>
					<term>coronal</term>
					<term>upper abdomen</term>
					<term>gastrointestinal system 17 plain radiography</term>
					<term>coronal</term>
					<term>pelvis</term>
					<term>musculosceletal system 18 plain radiography</term>
					<term>coronal</term>
					<term>foot</term>
					<term>musculosceletal system 19 plain radiography</term>
					<term>coronal</term>
					<term>ankle joint</term>
					<term>musculosceletal system 20 plain radiography</term>
					<term>coronal</term>
					<term>lower leg</term>
					<term>musculosceletal system 21 plain radiography</term>
					<term>coronal</term>
					<term>knee</term>
					<term>musculosceletal system 22 plain radiography</term>
					<term>coronal</term>
					<term>upper leg</term>
					<term>musculosceletal system 23 plain radiography</term>
					<term>coronal</term>
					<term>hip</term>
					<term>musculosceletal system 24 plain radiography</term>
					<term>sagittal</term>
					<term>facial cranium</term>
					<term>musculosceletal system 25 plain radiography</term>
					<term>sagittal</term>
					<term>neuro cranium</term>
					<term>musculosceletal system 26 plain radiography</term>
					<term>sagittal</term>
					<term>cervical spine</term>
					<term>musculosceletal system 27 plain radiography</term>
					<term>sagittal</term>
					<term>thoracic spine</term>
					<term>musculosceletal system 28 plain radiography</term>
					<term>sagittal</term>
					<term>lumbar spine</term>
					<term>musculosceletal system 29 plain radiography</term>
					<term>sagittal</term>
					<term>hand</term>
					<term>musculosceletal system 30 plain radiography</term>
					<term>sagittal</term>
					<term>radio carpal joint</term>
					<term>musculosceletal system 31 plain radiography</term>
					<term>sagittal</term>
					<term>handforearm</term>
					<term>musculosceletal system 32 plain radiography</term>
					<term>sagittal</term>
					<term>elbow</term>
					<term>musculosceletal system 33 plain radiography</term>
					<term>sagittal</term>
					<term>shoulder</term>
					<term>musculosceletal system 34 plain radiography</term>
					<term>sagittal</term>
					<term>chest</term>
					<term>unspecified 35 plain radiography</term>
					<term>sagittal</term>
					<term>foot</term>
					<term>musculosceletal system 36 plain radiography</term>
					<term>sagittal</term>
					<term>ankle joint</term>
					<term>musculosceletal system 37 plain radiography</term>
					<term>sagittal</term>
					<term>lower leg</term>
					<term>musculosceletal system 38 plain radiography</term>
					<term>sagittal</term>
					<term>knee</term>
					<term>musculosceletal system 39 plain radiography</term>
					<term>sagittal</term>
					<term>upper leg</term>
					<term>musculosceletal system 40 plain radiography</term>
					<term>sagittal</term>
					<term>hip</term>
					<term>musculosceletal system 41 plain radiography</term>
					<term>axial</term>
					<term>right breast</term>
					<term>reproductive system 42 plain radiography</term>
					<term>axial</term>
					<term>left breast</term>
					<term>reproductive system 43 plain radiography</term>
					<term>axial</term>
					<term>knee</term>
					<term>musculosceletal system 44 plain radiography</term>
					<term>other orientation</term>
					<term>facial cranium</term>
					<term>musculosceletal system 45 plain radiography</term>
					<term>other orientation</term>
					<term>neuro cranium</term>
					<term>musculosceletal system 46 plain radiography</term>
					<term>other orientation</term>
					<term>cervical spine</term>
					<term>musculosceletal system 47 plain radiography</term>
					<term>other orientation</term>
					<term>hand</term>
					<term>musculosceletal system 48 plain radiography</term>
					<term>other orientation</term>
					<term>right breast</term>
					<term>reproductive system 49 plain radiography</term>
					<term>other orientation</term>
					<term>left breast</term>
					<term>reproductive system 50 plain radiography</term>
					<term>other orientation</term>
					<term>foot</term>
					<term>musculosceletal system 51 fluoroscopy</term>
					<term>coronal</term>
					<term>hilum</term>
					<term>respiratory system 52 fluoroscopy</term>
					<term>coronal</term>
					<term>upper abdomen</term>
					<term>gastrointestinal system 53 fluoroscopy</term>
					<term>coronal</term>
					<term>pelvis</term>
					<term>cardiovascular system 54 fluoroscopy</term>
					<term>coronal</term>
					<term>lower leg</term>
					<term>cardiovascular system 55 fluoroscopy</term>
					<term>coronal</term>
					<term>knee</term>
					<term>cardiovascular system 56 fluoroscopy</term>
					<term>coronal</term>
					<term>upper leg</term>
					<term>cardiovascular system 57 angiography</term>
					<term>coronal</term>
					<term>pelvis</term>
					<term>cardiovascular system</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The purpose of this paper is to outline efforts from the 2005 CLEF crosslanguage image retrieval campaign (ImageCLEF). The aim of this CLEF track is to explore the use of both text and content-based retrieval methods for cross-language image retrieval. Four tasks were offered in the ImageCLEF track: a ad-hoc retrieval from an historic photographic collection, ad-hoc retrieval from a medical collection, an automatic image annotation task, and a user-centered (interactive) evaluation task that is explained in the iCLEF summary. 24 research groups from a variety of backgrounds and nationalities (14 countries) participated in ImageCLEF. In this paper we describe the ImageCLEF tasks, submissions from participating groups and summarise the main findings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF<ref type="foot" coords="1,143.16,539.17,3.95,6.97" target="#foot_0">7</ref> conducts evaluation of cross-language image retrieval and is run as part of the Cross Language Evaluation Forum (CLEF) campaign. The ImageCLEF retrieval benchmark was established in 2003 <ref type="bibr" coords="1,176.45,564.41,10.64,9.96" target="#b3">[1]</ref> and run again in 2004 <ref type="bibr" coords="1,290.80,564.41,10.52,9.96" target="#b4">[2]</ref> with the aim of evaluating image retrieval from multilingual document collections. Images by their very nature are language independent, but often they are accompanied by texts semantically related to the image (e.g. textual captions or metadata). Images can then be retrieved using primitive features based on pixels which form the contents of an image (e.g. using a visual exemplar), abstracted features expressed through text or a combination of both. The language used to express the associated texts or textual queries should not affect retrieval, i.e. an image with a caption written in English should be searchable in languages other than English.</p><p>ImageCLEF provides tasks for both system-centered and user-centered retrieval evaluation within two main areas: retrieval of images from photographic collections and retrieval of images from medical collections. These domains offer realistic scenarios in which to test the performance of image retrieval systems, offering different challenges and problems to participating research groups. A major goal of ImageCLEF is to investigate the effectiveness of combining text and image for retrieval and promote the exchange of ideas which may help improve the performance of future image retrieval systems.</p><p>ImageCLEF has already seen participation from both academic and commercial research groups worldwide from communities including: Cross-Language Information Retrieval (CLIR), Content-Based Image Retrieval (CBIR), medical information retrieval and user interaction. We provide participants with the following: image collections, representative search requests (expressed by both image and text) and relevance judgements indicating which images are relevant to each search request. Campaigns such as CLEF and TREC have proven invaluable in providing standardised resources for comparative evaluation for a range of retrieval tasks and ImageCLEF aims to provide the research community with similar resources for image retrieval. In the following sections of this paper we describe separately each search task: section 2 describes ad-hoc retrieval from historic photographs, section 3 ad-hoc retrieval from medical images, section sec:annotation the automatic annotation of medical images and. For each we briefly describe the test collections, the search tasks, participating research groups, results and a summary of the main findings.</p><p>2 Ad-hoc Retrieval from Historic Photographs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Aims and Objectives</head><p>This is a bilingual ad-hoc retrieval task in which a system is expected to match a user's one-time query against a more or less static collection (i.e. the set of documents to be searched is known prior to retrieval, but the search requests are not). Similar to the task run in previous years (see, e.g. <ref type="bibr" coords="2,107.75,359.09,10.23,9.96" target="#b4">[2]</ref>), the goal of this task is given multilingual text queries, retrieve as many relevant images as possible from the provided image collection (the St. Andrews collection of historic photographs). Queries for images based on abstract concepts rather than visual features are predominant in this task. This limits the effectiveness of using visual retrieval methods alone as either these concepts cannot be extracted using visual features and require extra external semantic knowledge (e.g. the name of the photographer), or images with different visual properties may be relevant to a search request (e.g. different views of Rome). However, based on feedback from participants in 2004, the search tasks for 2005 are aimed to reflect more visually-based queries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data and Search Tasks</head><p>The St. Andrews collection consists of 28,133 images, all of which have associated textual captions written in British English (the target language). The captions consist of 8 fields including title, photographer, location, date and one or more pre-defined categories (all manually assigned by domain experts). For example, see Fig. <ref type="figure" coords="2,272.64,695.21,3.84,9.96" target="#fig_0">1</ref>. Further examples can be found in [?] and the St. Andrews University Library<ref type="foot" coords="2,212.76,705.85,3.95,6.97" target="#foot_1">8</ref> . We provided participants with 28 topics (titles shown in Table <ref type="table" coords="2,503.04,707.21,10.06,9.96" target="#tab_8">11</ref> and an example image shown in Fig. <ref type="figure" coords="2,262.44,719.09,3.84,9.96" target="#fig_3">5</ref>), the main themes based on analysis of log files from a web server at St. Andrews university, knowledge of the image collection and discussions with maintainers of the image collection. After identifying these main themes, we modified queries to test various aspects of cross-language and visual search and used a custom-built IR system to identify suitable topics (in particular those topics with an estimated 20 and above relevant images). A complexity score was developed by the authors to categorise topics with respect to linguistic complexity <ref type="bibr" coords="3,183.28,164.33,9.96,9.96" target="#b6">[4]</ref>.</p><p>Each topic consists of a title (a short sentence or phrase describing the search request in a few words), and a narrative (a description of what constitutes a relevant or non-relevant image for that search request). In addition to the text description for each topic, we also provided two example images which we envisage could be used for relevance feedback (both manual and automatic) and query-by-example searches <ref type="foot" coords="3,230.40,222.85,3.95,6.97" target="#foot_2">9</ref> . Both topic title and narratives have been translated into the following languages: German, French, Italian, Spanish (European), Spanish (Latin American), Chinese (Simplified), Chinese (Traditional) and Japanese. Translations have also been produced for the titles only and these are available in 25 languages including: Russian, Croatian, Bulgarian, Hebrew and Norwegian. All translations have been provided by native speakers and verified by at least one other native speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Creating Relevance Assessments</head><p>Relevance assessments were performed by staff at the University of Sheffield (the majority unfamiliar with the St. Andrews collection but given training and access to the collection through our IR system). The top 50 results from all submitted runs (349) were used to create image pools giving an average of 1,376 (max: 2,193 and min: 760) images to judge per topic. The authors judged all topics to create a "gold standard" and at least two further assessments were obtained for each topic. Assessors used a custom-built tool to make judgements accessible on-line enabling them to log in when and where convenient. We asked assessors to judge every image in the topic pool, but also to use interactive search and judge: searching the collection using their own queries to supplement the image pools with further relevant.</p><p>The assessment of images in this ImageCLEF task is based on using a ternary classification scheme: (1) relevant, (2) partially relevant and (3) not relevant. The aim of the ternary scheme is to help assessors in making their relevance judgements more accurate (e.g. an image is definitely relevant in some way, but maybe the query object is not directly in the foreground: it is therefore considered partially relevant). Relevance assessment for the more general topics are based entirely on the visual content of images (e.g. "aircraft on the ground"). However, certain topics also require the use of the caption to make a confident decision (e.g. "pictures of North Street St Andrews"). What constitutes a relevant image is a subjective decision, but typically a relevant image will have the subject of the topic in the foreground, the image will not be too dark in contrast, and maybe the caption confirms the judge's decision.</p><p>Based on these judgements, various combinations are used to create the set of relevant images and as in previous years, we used the pisec-total set: those images judges as relevant or partiallyrelevant by the topic creator and at least one other assessor. These are then used to evaluate system performance and compare submissions. The size of pools and number of relevant images is shown in Table <ref type="table" coords="3,129.00,610.73,10.06,9.96" target="#tab_8">11</ref> (the %max indicating the pool size compared to the maximum possible pool size, i.e. if all top 50 images from each submission were unique).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Participating Groups</head><p>In total, 19 groups registered for this task and 11 ended up submitting (including 5 new groups compared to last year) a total of 349 runs (all of which were evaluated). Participants were given queries and relevance judgements from 2004 as training data and access to a default CBIR system (GIFT/Viper). Submissions from participants are briefly described in the following.</p><p>CEA: CEA from France, submitted 9 runs. Experimented with 4 languages, title and title+narrative, and merging between modalities (text and image). This is simply based on normalised scores obtained by each search and is conservative (results obtained using visual topics and CBIR system are used only to reorder results obtained using textual topics) NII: National Institute of Informatics from Japan, submitted 16 runs with 3 languages. These experiments were aimed to see if the inclusion of learned word association model -the model which represents how words are related -can help finding relevant images in adhoc CLIR setting. To do this, basic unigram language models were combined with differently estimated word association models that performs soft word-expansion. Also, combining simple keyword matching-like language models to above mentioned soft word-expansion language models at the model-output level. All runs were text only.</p><p>Alicante: University of Alicante (Computer Science) from Spain, submitted 62 runs (including 10 joint runs with UNED and Jaen). They experimented with 13 languages using title, automatic query expansion and text only. Their system combines probabilistic information with ontological information and a feedback technique. Several information streams are created using different sources: stems, words and stem bigrams, the final result obtained by combining them. An ontology has been created automatically from the St. Andrews collection to relate a query with several image categories. Four experiments were carried out to analyse how different features contribute to retrieval results. Moreover, a voting-based strategy was developed joining three different systems of participating universities: University of Alicante, University of Jaén and UNED.</p><p>CUHK: Chinese University of Hong Kong, submitted 36 runs for English and Chinese (simplified). CUHK experimented with title, title+narrative and using visual methods to rerank search results (visual features are composed of two parts: DCT coefficients and Colour moments with a dimension of 9). Various IR models used for retrieval (trained on 2004 data), together with query expansion. LDC Chinese segmentor is used to extract words from Chinese queries and translated into English using a dictionary. DCU: Dublin City University (Computer Science) from Ireland, submitted 33 runs for 11 languages.</p><p>All runs were automatic using title only. Standard OKAPI used incorporating stop word removal, suffix stripping and query expansion using pseudo relevance feedback. Their main focus of participation was to explore an alternative approach to combining text and image retrieval in an attempt to make use of information provided by the query image. Separate ranked lists returned using text retrieval without feedback and image retrieval based on standard low-level colour, edge and texture features, were investigated to find documents returned by both methods. These documents were then assumed to be relevant and used for text based pseudo relevance feedback and retrieval as in our standard method.</p><p>Geneva: University Hospitals Geneva from Switzerland, submitted 2 runs based on visual retrieval only (automatic and no feedback).</p><p>Indonesia: University of Indonesia (Computer Science), submitted 9 runs using Indonesian queries only.</p><p>They experimented with using title and title+narrative, with and without query expansion and combining text and image retrieval (all runs automatic).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIRACLE:</head><p>Daedalus and Madrid University from Spain, submitted 106 runs for 23 languages. All runs were automatic, using title only, no feedback and text-based only.</p><p>NTU: National Taiwan University from Taiwan, submitted 7 runs for Chinese (traditional) and English (also included a visual-only run). All runs are automatic and NTU experimented with using query expansion, using title and title+narrative and combining visual and text retrieval.</p><p>Jaen: University of Jaén (Intelligent Systems) from Spain, submitted 64 runs in 9 languages (all automatic). Jaen experimented with title and title+narrative, with and without feedback and combining both text and visual retrieval. Jaén experimented with both term weighting and the use of pseudo relevance feedback.</p><p>UNED: UNED from Spain, submitted 5 runs for Spanish (both Latin American and European) and English. All runs were automatic, title, text only and with feedback. UNED experimented with three different approaches: i) a naive baseline using a word by word translation of the title topics; ii) a strong baseline based on Pirkola's work; and iii) a structured query using the named entities with field search operators and Pirkola's approach.</p><p>Participants were asked to categorise their submissions by the following dimensions: query language, type (automatic or manual), use of feedback (typically relevance feedback is used for automatic query expansion), modality (text only, image only or combined) and the initial query (visual only, title only, narrative only or a combination). A summary of submissions by these dimensions is shown in Table <ref type="table" coords="5,220.68,200.21,3.84,9.96" target="#tab_0">1</ref>. No manual runs have been submitted this year, and a large proportion are text only using just the title. Together with 41% of submissions using query expansion, this co-incides with the large number of query languages offered this year and the focus on query translation by participating groups (although 6 groups submitted runs involving CBIR). An interesting submission this year was the combined efforts of Jaen, UNED and Alicante to create an approach based on voting for images. Table <ref type="table" coords="5,303.60,259.97,5.03,9.96" target="#tab_1">2</ref> provides a summary of submissions by query language. At least one group submitted for each language, the most popular (non-English) being French, German and Spanish (European). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Results</head><p>Results for submitted runs were computed using the latest version of trec eval<ref type="foot" coords="5,451.08,562.93,7.91,6.97" target="#foot_3">10</ref> from NIST (v7.3). From the scores output, four chosen to evaluate submissions are Mean Average Precision (MAP), precision at result 10 (P10), precision at result 100 (P100) and the number of relevant images retrieved (RelRet) from which we compute recall (the proportion of relevant retrieved). Table <ref type="table" coords="5,117.00,612.17,5.03,9.96">3</ref> summarises the top performing systems in the ad-hoc task based on MAP. Whether MAP is the best score to rank image retrieval systems is debatable, hence our inclusion of P10 and P100 scores. The highest English (monolingual) retrieval score is 0.4135, with a P10 of 0.5500 and P100 of 0.3197. On average recall is high (0.8434), but low MAP and P10 indicating that relevant images are likely retrieved at lower rank positions. The highest monolingual score is obtained using combined visual and text retrieval and relevance feedback.</p><p>The highest cross-language MAP is Chinese (traditional) for the NTU submission which is 97% of highest monolingual score. Retrieval performance is variable across language with some performing poorly, e.g. Romanian, Bulgarian, Czech, Croatian, Finnish and Hungarian. Although these languages did not have translated narratives available for retrieval, it is more likely low performance results from limited availability of translation and language processing resources and difficult language structure (e.g. results from CLEF2004 showed Finnish to be a very challenging language due to its complex morphology). Hungarian performs the worst at 23% of monolingual. However, it is encouraging to see participation at CLEF for these languages. On average, MAP Table <ref type="table" coords="6,147.21,502.99,4.50,8.98">3</ref>. Systems with highest MAP for each language in the ad-hoc retrieval task. Table <ref type="table" coords="7,133.32,416.57,5.03,9.96" target="#tab_2">4</ref> shows the average MAP score averaged across all submissions by query dimension. There is a wide variation in counts for each dimension and type, therefore results are only an indication of effects on performance for each dimension. On average, it would appear that submissions with feedback (e.g. query expansion) performed better than without, submissions based on a combination of image and text retrieval appear to give higher performance (modality, although the NTU visual-only runs also perform well giving this type a high MAP score) and using both the image and text for the initial query (title+image) gives highest average MAP score (although again small counts for this dimension type).</p><p>Table <ref type="table" coords="7,133.32,512.21,10.06,9.96" target="#tab_8">11</ref> shows the highest MAP, P10, P100 and RelRet scores obtained from submissions for each topic. Results vary across topic as expected; some topics are harder than others. In this initial evaluation, we find that 18 topics have a recall of 1, 18 topics a P10 of 1, and 12 topics with a maximum MAP score greater than 0.7. The highest performing topic (easiest) is 11 "Swiss mountain scenery" and the lowest is topic 18 "woman in white dress". In addition, 18 topics have a maximum Relretr=relevant (i.e. a recall of 1) indicating that all relevant images have been retrieved in the top 1000 results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Discussion</head><p>The variety of submissions in the ad-hoc task this year has been pleasing with a number of groups experimenting with both visual and text-based retrieval methods and combining the two (although the number of runs submitted as combined is much lower than 2004). As in 2004, the combination of text and visual retrieval appears to give highest retrieval effectiveness (based on MAP) indicating this is still an area for research. We aimed to offer a wider range of languages of which 13 have submissions from at least two groups (compared to 10 in 2004). It would seem that the focus for many groups in 2005 has been translation with more use made of both title and narrative than 2004. However, it is interesting to see languages such as Chinese (traditional) and Spanish (Latin American) perform above European languages such as French, German and Spanish (European) which performed best in 2004.</p><p>Although topics were designed to be more suited to visual retrieval methods (based on comments from participants in 2004), the topics are still dominated by semantics and background knowledge; pure visual similarity still plays a less significant role. The current ad-hoc task is not well-suited to purely visual retrieval because colour information, which typically plays an important role in CBIR, is ineffective due to the nature of the St. Andrews collection (historic photographs). Also unlike typical CBIR benchmarks, the images in the St. Andrews collection are very complex containing both objects in the foreground and background which prove indistinguishable to CBIR methods. Finally, the relevant image set is visually different for some queries (e.g. different views of a city) making visual retrieval methods ineffective. This highlights the importance of using either text-based IR methods on associated metadata alone, or combined with visual features. Relevance feedback (in the form of automatic query expansion) still plays an important role in retrieval as also demonstrated by submissions in 2004: a 17% increase in 2005 and 48% in 2004.</p><p>We are aware that research in the ad-hoc task using the St. Andrews collection has probably reached a plateau. There are obvious limitations with the existing collection: mainly black and white images, domain-specific vocabulary used in associated captions, restricted retrieval scenario (i.e. searches for historic photographs) and experiments with limited target language (English) are only possible (i.e. cannot test further bilingual pairs). To address these and widen the image collections available to ImageCLEF participants, we have been provided with access to a new collection of images from a personal photographic collection with associated textual descriptions in German and Spanish (as well as English). This is planned for use in the ImageCLEF 2006 ad-hoc task.</p><p>3 Ad-hoc Retrieval from Medical Image Collections</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Goals and objectives</head><p>Domain-specific information retrieval is getting increasingly important and this holds especially true for the medical field, where patients as well as clinicians and researchers have their particular information needs <ref type="bibr" coords="8,171.78,444.77,14.64,9.96" target="#b12">[10]</ref>. Whereas information needs and retrieval methods for textual documents have been well researched, there is only a small amount of information available on the need to search for images <ref type="bibr" coords="8,164.92,468.65,14.65,9.96" target="#b13">[11]</ref>, and even less so for the use of images in the medical domain. ImageCLEFmed is creating resources to evaluate information retrieval tasks on medical image collections. This process includes the creation of image collections, of query tasks, and the definition of correct retrieval results for these tasks for system evaluation. Part of the tasks have been based on surveys of medical professionals and how they use images <ref type="bibr" coords="8,308.22,516.41,10.04,9.96" target="#b11">[9]</ref>.</p><p>Much of the basic structure is similar to the non-medical ad-hoc task such as the general outline, the evaluation procedure and the relevance assessment tool used. These similarities will not be described in any detail in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data sets used and query topics</head><p>In 2004, only the Casimage<ref type="foot" coords="8,210.60,601.81,7.91,6.97" target="#foot_4">11</ref> dataset was made available to participants <ref type="bibr" coords="8,413.34,603.17,14.74,9.96" target="#b14">[12]</ref>, containing almost 9.000 images of 2.000 cases <ref type="bibr" coords="8,212.86,615.05,14.80,9.96" target="#b15">[13]</ref>, 26 query topics with relevance judgements of three medical experts. It is also part of the 2005 collection. Images present in the data set include mostly radiology modalities, but also photographs, powerpoint slides and illustrations. Cases are mainly in French, with around 20% being in English. We were also allowed to use the PEIR <ref type="foot" coords="8,406.68,649.57,7.91,6.97" target="#foot_5">12</ref> (Pathology Education Instructional Resource) database using annotation from the HEAL<ref type="foot" coords="8,385.08,661.57,7.91,6.97" target="#foot_6">13</ref> project (Health Education Assets Library, mainly Pathology images <ref type="bibr" coords="8,271.75,674.81,10.33,9.96" target="#b9">[7]</ref>). This dataset contains over 33.000 images with English annotation, with the annotation being in XML per image and not per case as casimage. The nuclear medicine database of MIR, the Mallinkrodt Institute of Radiology<ref type="foot" coords="8,441.72,697.45,7.91,6.97" target="#foot_7">14</ref>  <ref type="bibr" coords="8,454.08,698.81,14.59,9.96">[14]</ref>, was also made available to us for ImageCLEF. This dataset contains over 2.000 images mainly from nuclear medicine with annotations per case and in English. Finally, the PathoPic 15 collection (Pathology images <ref type="bibr" coords="9,123.93,152.33,10.73,9.96" target="#b10">[8]</ref>) was included into our dataset. It contains 9.000 images with an extensive annotation per image in German. Part of the German annotation is translated into English, but it is still incomplete. This means, that a total of more than 50.000 images was made available with annotations in three different languages. Two collections have case-based annotations whereas two collections have image image-based annotations. Only through the access to the data by the copyright holders, we were able to distribute these images to the participating research groups.</p><p>The image topics were based on a small survey at OHSU. Based on this survey, the topics were developed along the following main axes:</p><p>-Anatomic region shown in the image; -Image modality (x-ray, CT, MRI, gross pathology, ...); -Pathology or disease shown in the image; -abnormal visual observation (eg. enlarged heart);</p><p>As the goal was clearly to accommodate both visual and textual research groups we developed a set of 25 topics containing three different groups of queries: queries that are expected to be solvable with a visual retrieval system (topics 1-12), topics where both text and visual features are expected to perform well (topics 13-23) and semantic topics, where visual features are not expected to improve results. All query topics were of a higher semantic level than the 2004 topics because the automatic annotation task provides a testbed for purely visual retrieval/classification. All 25 topics contain one to three images, one query also an image as negative feedback. The query text was given out with the images in the three languages present in the collections: English, German, and French. An example for a visual query of the first category can be seen in Figure <ref type="figure" coords="9,465.60,402.77,3.84,9.96">2</ref>.</p><p>Show me chest CT images with emphysema. Zeige mir Lungen CTs mit einem Emphysem. Montre-moi des CTs pulmonaires avec un emphysème. Fig. <ref type="figure" coords="9,111.23,622.27,4.50,8.98">2</ref>. An example of a query that is at least partly solvable visually, using the image and the text as query. Still, use of annotation can augment retrieval quality. The query text is presented in three languages.</p><p>A query topic that will require more than purely visual features can be seen in Figure <ref type="figure" coords="9,483.72,679.49,3.90,9.96">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relevance judgements</head><p>The relevance assessments were performed at OHSU in Portland, Oregon. A simple interface was used from previous ImageCLEF relevance assessments. 9 judges, mainly medical doctors and one image processing specialist performed the relevance judgements. Due to a lack of resources, only part of the topics could be judged by more than one person.</p><p>To create the image pools for the judgements, the first 40 images of each submitted run were taken into account to create pools with an average size of 892 images. The largest pool size was Show me all x-ray images showing fractures.</p><p>Zeige mir Röntgenbilder mit Brüchen. Montres-moi des radiographies avec des fractures. Fig. <ref type="figure" coords="10,111.23,344.47,4.50,8.98">3</ref>. A query that requires more than visual retrieval but visual features can deliver some hints to good results as well.</p><p>1167 and the smallest one 470. It took the judges an average of roughly three hours to judge the images for a single topic. Compared to the purely visual topics from 2004 (around one hour judgement per topic containing an average of 950 images) the judgement process took much longer per image as the semantic queries required to verify the text and often an enlarged version of the images. The longer time might also be due to the fact that in 2004 all images were pre-marked as irrelevant, and only relevant images required a change, whereas this year we did not have anything pre-marked. Still, this process is significantly faster than most text research judgements, as a large number of irrelevant images could be sorted out very quickly.</p><p>We use a ternary judgement scheme including relevant, partially-relevant, and non-relevant. For the official qrels, we only used images marked as relevant. We also had several topics judged by two persons, but still took only the first judgements for the evaluations. Further analysis will follow in the final conference proceedings when more knowledge is available on the used techniques as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Participants</head><p>The number of registered participants of ImageCLEF has multiplied over the last three years. ImageCLEF started with 4 participants in 2003, then in 2004 a total of 18 groups participated and in 2005 we have 36 registered groups. The medical retrieval task had 12 participants in 2004 when it was purely visual and 13 in 2005 as a mixture of visual and non-visual retrieval. A surprisingly small number of groups (13 of 28 registered groups) finally submitted results, which can be due to the short time span between delivery of the images and the deadline for results submission. Another point was the fact that several groups only registered very late as they had not had information about ImageCLEF beforehand, but they were still interested in the datasets also for future participations. As the registration to the task is free, they could simply register to get this access.</p><p>The following groups registered but were finally not able to submit results for a variety of reasons:</p><p>-University of Alicante, Spain -National Library of Medicine, Bethesda, MD, USA -University of Montreal, Canada -University of Science and Medical Informatics, Innsbruck, Austria -University of Amsterdam, Informatics department, The Netherlands -UNED, LSI, Valencia, Spain -Central University, Caracas, Venezuela -Temple University, computer science, USA -Imperial College, computing lab, UK -Dublin City university, computer science, Ireland -CLIPS Grenoble, France -University of Sheffield, UK -Chinese University of Honk Kong, China Finally 13 groups (two of them from the same laboratory but different groups in Singapore) submitted results for the medical retrieval task, including a total of 134 runs. Only 6 manual runs were submitted. Here is a short list of their participation including a short description of the submitted runs:</p><p>-National Chiao Tuna University, Taiwan: submitted 16 runs in total, all automatic. 6 runs were visual only and 10 mixed runs. They use simple visual features (color histogram, coherence matrix, layout features) as well as text retrieval using a vector-space model with word expansion using wordnet. -State university of New York (SUNY), Buffalo, USA: submitted a total of 6 runs, one visual and five mixed runs. GIFT was used as visual retrieval system and SMART as textual retrieval system, while mapping the text to UMLS. -University and Hospitals of Geneva, Switzerland: submitted a total of 19 runs, all automatic runs. This includes two textual and two visual runs plus 15 mixed runs. The retrieval relied mainly on the GIFT and easyIR retrieval systems. -RWTH Aachen, computer science, Germany: submitted 10 runs, two being manual mixed retrieval, two automatic textual retrieval, three automatic visual retrieval and three automatic mixed retrieval. The Fire retrieval engine was used with varied visual features and a text search engine using English and mixed-language retrieval. -Daedalus and Madrid University, Spain: submitted 14 runs, all automatic. 4 runs were visual only and 10 were mixed runs; They mainly used semantic word expansions with EuroWordNet. -Oregon Health and Science University, Portland, OR, USA: submitted three runs in total, two manual runs, one for visual and one for textual retrieval and one automatic textual run. As retrieval engines GIFT and Lucene are being used. -University of Jaen, Spain: had a total of 42 runs, all automatic. 6 runs were textual, only, and 36 were mixed. GIFT is used as a visual query system and the LEMUR system is used for text in a variety of configurations to achieve multilingual retrieval. -Institute for Infocomm research, Singapore: submitted 7 runs, all of them automatic visual runs; For their runs they first manually selected visually similar images to train the features, which should rather be classified as a manual run, then. Then, they use a two-step approach for visual retrieval. -Institute for Infocomm research -second group , Singapore: submitted a total of 3 runs, all visual with one being automatic and two manual runs The main technique applied is the connection of medical terms and concepts to visual appearances. -RWTH Aachen -medical informatics, Germany: submitted two visual only runs with several visual features and classification methods of the IRMA project. -CEA, France: submitted five runs, all automatic with two being visual, only and three mixed runs. The techniques used include the the PIRIA visual retrieval system and a simple frequencybased text retrieval system. -IPAL CNRS/ I2R, France/Singapore: submitted a total of 6 runs, all automatic with two being text only and the other a combination of textual and visual features. For textual retrieval they map the text onto single axes of the MeSH ontology. They also use negative weight query expansion and mix visual and textual results for optimal results. -University of Concordia, Canada: submitted one visual run containing a query only for the first image of every topic using only visual features. The technique applied is an association model between low-level visual features and high-level concepts mainly relying on texture, edge and shape features.</p><p>In Table <ref type="table" coords="12,144.84,152.33,5.03,9.96">5</ref> an overview of the submitted runs can be seen including the query dimensions.</p><p>Table <ref type="table" coords="12,162.81,183.79,4.50,8.98">5</ref>. Query dimensions of the submissions for the medical retrieval task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results</head><p>This section will give an overview of the best results of the various categories and will also do some more in depth analysis on a topic basis. More needs to follow based on the submissions of the papers from the participants.</p><p>Table <ref type="table" coords="12,132.00,420.77,5.03,9.96">6</ref> shows all the manual runs that were submitted with a classification into the technique used for the retrieval Table <ref type="table" coords="12,220.41,462.19,4.50,8.98">6</ref>. Overview of the manual retrieval results. In Table <ref type="table" coords="12,145.56,595.01,5.03,9.96" target="#tab_5">7</ref> are the best 5 results for textual retrieval only and the best ten results for visual and for mixed retrieval.</p><p>If we are looking at single topics it becomes clear that the systems vary extremely over the topics. If we calculate the average over the best system for each query we would be much closer to 0.5 than to what the best system actually achieved, 0.2821. So far, non of the systems optimised the feature selection based on the query input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Discussion</head><p>The results show a few clear trends. Very few groups performed manual submissions using relevance judgements, which is most likely due to the need of resources for such evaluations. Still, relevance feedback has shown to be extremely useful in many retrieval tasks and the evaluation of it seems extremely necessary, as well. Surprisingly, in the submitted results, relevance feedback does not seem to have a much superior performance compared to the automatic runs. In the 2004 tasks the relevance feedback runs were often significantly better than without feedback.</p><p>It also becomes clear that the topics developed were much more geared towards textual retrieval than visual retrieval. The best results for textual retrieval are much higher than for visual retrieval only, and a few of the bad textual runs seem simply to have indexing problems. When analysing the topics in more details a clear division becomes clear between the developed visual and textual topics, but also some of the topics marked as visual had actually better results using a textual system. Some systems actually perform extremely well on a few topics but then extremely bad on other topics. No system is actually the best system for more than two of the topics. The best results were clearly obtained when combining textual and visual features most likely due to the fact that there were queries for that either one of the feature sets would work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Automatic Annotation Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Introduction, Idea, and Objectives</head><p>Automatic image annotation is a classification task, where an image is assigned to its correspondent class from a given set of pre-defined classes. As such, it is an important step for content-based image retrieval (CBIR) and data mining <ref type="bibr" coords="13,277.77,702.29,14.71,9.96" target="#b17">[15]</ref>. The aim of the Automatic Annotation Task in ImageCLEFmed 2005 was to compare state-of-the-art approaches to automatic image annotation and to quantify their improvements for image retrieval. In particular, the task aims at finding out how well current techniques for image content analysis can identify the medical image modality, body orientation, body region, and biological system examined. Such an automatic classification can be used for multilingual image annotations as well as for annotation verification, e.g., to detect false information held in the header streams according to Digital Imaging and Communications in Medicine (DICOM) standard <ref type="bibr" coords="13,220.22,786.05,14.67,9.96" target="#b18">[16]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Database</head><p>The database consisted of 9,000 fully classified radiographs taken randomly from medical routine at the Aachen University Hospital. 1,000 additional radiographs for which classification labels were unavailable to the participants had to be classified into one of the 57 classes, the 9,000 database images come from. Although only 57 simple class numbers were provided for ImageCLEFmed 2005.</p><p>The images are annotated with complete IRMA code, a multi-axial code for image annotation.</p><p>The code is currently available in English and German. It is planned to use the results of such automatic image annotation tasks for further, textual image retrieval tasks in the future.</p><p>Example images together with their class number are given in Figure <ref type="figure" coords="14,423.24,717.17,3.90,9.96" target="#fig_2">4</ref>. Table <ref type="table" coords="14,464.04,717.17,5.03,9.96">8</ref> gives the English textual description for each of the classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Participating Groups</head><p>In total 26 groups registered for participation in the automatic annotation task. All groups have downloaded the data but only 12 groups submitted runs. Each group had at least two differ-ent submissions. The maximum number of submissions per group was 7. In total, 41 runs were submitted which are briefly described in the following.</p><p>CEA: CEA from France, submitted three runs. In each run different feature vectors were used and classified using a k-Nearest Neighbour classifier (k was either 3 or 9). In the run labelled cea/pj-3.txt the images were projected along horizontal and vertical axes to obtain a feature histogram. For cea/tlep-9.txt histogram of local edge patterns features and colour features were created, and for cea/cime-9.txt quantified colours were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CINDI:</head><p>The CINDI group from Concordia University in Montreal, Canada used multi-class SVMs (one-vs-one) and a 170 dimensional feature vector consisting of colour moments, colour histograms, cooccurence texture features, shape moment, and edge histograms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geneva:</head><p>The medGIFT group from Geneva, Switzerland used various different settings for graylevels, and Gabor filters in their medGIFT image retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Infocomm:</head><p>The group from Infocomm Institute, Singapore used three kinds of 16x16 low-resolutionmap-features: initial gray values, anisotropy and contrast. To avoid over-fitting, for each of 57 classes, a separate training set was selected and about 6,800 training images were chosen out of the given 9,000 images. Support Vector Machines with RBF (radial basis functions) kernels were applied to train the classifiers which were then employed to classify the test images.</p><p>Miracle: The Miracle Group from UPM Madrid, Spain uses GIFT and a decision table majority classifier to calculate the relevance of each individual result in miracle/mira20relp57.txt. In mira20relp58IB8.txt additionally a k-nearest neighbour classifier with k = 8 and attribute normalisation is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Montreal:</head><p>The group from University of Montreal, Canada submitted 7 runs, which differ in the used features used. They to estimated, which classes are best represented by which features and combined appropriate features. mtholyoke: For the submission from Mount Holyoke College, MA, USA, Gabor energy features were extracted from the images and two different cross-media relevance models were used to classify the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>nctu-dblab:</head><p>The NCTU-DBLAB group from National Chiao Tung University, Taiwan used a support vector machine (SVM) to learn image feature characteristics. Based on the SVM model, several image features were used to predict the class of the test images. ntu: The Group from National Taiwan University used mean gray values of blocks as features and different classifiers for their submissions.</p><p>rwth-i6: The Human language technology and pattern recognition group from RWTH Aachen University, Germany had two submissions. One used a simple zero-order image distortion model taking into account local context. The other submission used a maximum entropy classifier and histograms of patches as features.</p><p>rwth-mi: The IRMA group from Aachen, Germany used features proposed by TAMURA et al to capture global texture properties and two distance measures for down-scaled representations, which preserve spatial information and are robust w.r.t. global transformations like translation, intensity variations, and local deformations. The weighing parameters for combining the single classifiers were guessed for the first submission and trained on a random 8,000 to 1,000 partitioning of the training set for the second submission. ulg.ac.be: The ULg method is based on random sub-windows and decision trees. During the training phase, a large number of multi-size sub-windows are randomly extracted from training images. Then, a decision tree model is automatically built (using Extra-Trees and/or Tree Boosting), based on size-normalised versions of the sub-windows, and operating directly on their pixel values. Classification of a new image similarly entails the random extraction of sub-windows, the application of the model to these, and the aggregation of sub-window predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>The error rates ranges between 12.6 % and 73.3 % (Table <ref type="table" coords="17,339.84,253.97,3.88,9.96">9</ref>). Based on the training data, a system guessing the most frequent group for all 1,000 test images would result with 70.3 % error rate, since 297 radiographs of the test set were from class 12 (Table <ref type="table" coords="17,357.24,277.97,8.28,9.96" target="#tab_7">10</ref>). A more realistic baseline of 36.8 % error rate is computed from an 1-nearest-neighbour classifier comparing down-scaled 32 × 32 versions of the images using the Euclidean distance.</p><p>For each class, a more detailed analysis including the number of training and test images as well as with respect to all 41 submitted runs, the average classification accuracy, the class most frequently misclassified, and the average percentage over all submitted runs of images being assigned to this class is given in Table <ref type="table" coords="17,255.24,349.61,8.48,9.96" target="#tab_7">10</ref>. Obviously, the difficulty of the 57 classes diversifies. The average classification accuracy range from 6.3 % to 90.7 %, and there is a tendency that classes with less training images are more difficult. For instance for class 32, 78 images were contained in the training but only one image in the test data. In 23 runs, this test image was misclassified (43.9 %). Five times, it was labelled to be from class 25 (12.2 %). Also, it can be seen that many images of the classes 7 and 8 have been classified to be of class 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion</head><p>Similar experiments have been described in literature. However, previous experiments have been restricted to a small number of categories. For instance, several algorithms have been proposed for orientation detection of chest radiographs, where lateral and frontal orientation are distinguished by means of image content analysis <ref type="bibr" coords="17,254.86,487.13,15.61,9.96" target="#b20">[18,</ref><ref type="bibr" coords="17,270.47,487.13,11.71,9.96" target="#b21">19]</ref>. For this two-class experiment, the error rates are below 1 % <ref type="bibr" coords="17,138.33,499.13,14.62,9.96" target="#b22">[20]</ref>. In a recent investigation, Pinhas and Greenspan report error rates below 1 % for automatic categorisation of 851 medical images into 8 classes <ref type="bibr" coords="17,365.85,511.01,14.81,9.96" target="#b23">[21]</ref>. In previous investigations of the IRMA group, error rates between 5.3% and 15% were reported for experiments with 1617 of 6 <ref type="bibr" coords="17,98.27,535.01,15.49,9.96" target="#b24">[22]</ref> and 6,231 of 81 classes <ref type="bibr" coords="17,218.53,535.01,14.68,9.96" target="#b25">[23]</ref>, respectively. Hence, error rates of 12 % for 10,000 of 57 classes are plausible.</p><p>As mentioned before, classes 6, 7, and 8 were frequently confused. All show parts of the arms and thus look extremely similar (Fig. <ref type="figure" coords="17,258.48,570.77,3.84,9.96" target="#fig_2">4</ref>). However, a reason for the common misclassification in favour of class 6 might be that there are by a factor of 5 more training images from class 6 than from classes 7 and 8 together.</p><p>Given the confidence files from all runs, classifier combination was tested using the sum-and the product rule in such a manner that first the two best confidence files were combined, then the three best confidence files, and so forth. Unfortunately, the best results was 12.9%. Thus, no improvement over the current best submission was possible using simple classifier combination techniques.</p><p>Having some results close to 10% error rate, classification and annotation of images might open interesting vistas for CBIR systems. Although the task considered here is more restricted than the Medical Retrieval Task and thus can be considered easier, techniques applied here will most probably be apt to be used in future CBIR applications, too. Therefore, it is planned to use the results of such automatic image annotation tasks for further, textual image retrieval tasks. queries based on realistic information needs of medical professionals. The ad-hoc task has continued to attract interest and this year has seen an increase in the number of translated topics and those with translated narratives. The addition of the IRMA annotation task has provided a further challenge to the medical side of ImageCLEF and proven a popular task for participants, covering mainly the visual retrieval community. The user-centered retrieval task, however, remains with low participation, mainly due to the high level of resources required to run an interactive task. We will continue to improve tasks for ImageCLEF 2006 mainly based on feedback from participants.</p><p>A large number of participants only registered but finally did not submit results. This means that the resources are very valuable and already access to the resources is a readon to register. Still, only if we have participants submitting results with different techiques, there is really the possibility to compare retrieval systems and developed better retrieval for the future. So for 2006 we hope to receive much feedback for tasks and many people who register, submit results and participate at the CLEF workshop to discuss the presented techniques.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,143.04,587.21,316.82,9.96;2,102.72,483.74,75.24,93.06"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example image and caption from the St. Andrews collection.</figDesc><graphic coords="2,102.72,483.74,75.24,93.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="14,156.60,157.28,9.11,8.97"><head></head><label></label><figDesc>02</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="14,90.00,573.89,422.80,9.96;14,90.00,585.89,20.85,9.96"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example images from the IRMA database which was used for the automatic annotation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="22,104.52,705.65,393.73,9.96"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Example images given to participants for the ad-hoc retrieval task (1 of 2 images).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,159.24,315.89,268.16,175.07"><head>Table 1 .</head><label>1</label><figDesc>Ad hoc experiments listed by query dimension.</figDesc><table coords="5,159.24,340.52,231.16,150.45"><row><cell>Dimension</cell><cell>type</cell><cell>#Runs (%)</cell></row><row><cell>Language</cell><cell>non-English</cell><cell>230 (66%)</cell></row><row><cell>Run type</cell><cell>Automatic</cell><cell>349 (100%)</cell></row><row><cell>Feedback (QE)</cell><cell>yes</cell><cell>142 (41%)</cell></row><row><cell>Modality</cell><cell>image</cell><cell>4 (1%)</cell></row><row><cell></cell><cell>text</cell><cell>318 (91%)</cell></row><row><cell></cell><cell>text+image</cell><cell>27 (8%)</cell></row><row><cell>Initial Query</cell><cell>image only</cell><cell>4 (1%)</cell></row><row><cell></cell><cell>title only</cell><cell>274 (79%)</cell></row><row><cell></cell><cell>narr only</cell><cell>6 (2%)</cell></row><row><cell></cell><cell>title+narr</cell><cell>57 (16%)</cell></row><row><cell></cell><cell>title+image</cell><cell>4 (1%)</cell></row><row><cell></cell><cell>title+narr+image</cell><cell>4 (1%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,178.32,178.73,246.42,246.57"><head>Table 2 .</head><label>2</label><figDesc>Ad hoc experiments listed by query language.</figDesc><table coords="6,219.24,202.81,164.63,222.49"><row><cell>Query Language</cell><cell cols="2">#Runs #Participants</cell></row><row><cell>English</cell><cell>70</cell><cell>9</cell></row><row><cell cols="2">Spanish (Latinamerican) 36</cell><cell>4</cell></row><row><cell>German</cell><cell>29</cell><cell>5</cell></row><row><cell>Spanish (European)</cell><cell>28</cell><cell>6</cell></row><row><cell>Chinese (simplified)</cell><cell>21</cell><cell>4</cell></row><row><cell>Italian</cell><cell>19</cell><cell>4</cell></row><row><cell>French</cell><cell>17</cell><cell>5</cell></row><row><cell>Japanese</cell><cell>16</cell><cell>4</cell></row><row><cell>Dutch</cell><cell>15</cell><cell>4</cell></row><row><cell>Russian</cell><cell>15</cell><cell>4</cell></row><row><cell>Portuguese</cell><cell>12</cell><cell>3</cell></row><row><cell>Greek</cell><cell>9</cell><cell>3</cell></row><row><cell>Indonesian</cell><cell>9</cell><cell>1</cell></row><row><cell>Chinese (traditional)</cell><cell>8</cell><cell>2</cell></row><row><cell>Swedish</cell><cell>7</cell><cell>2</cell></row><row><cell>Filipino</cell><cell>5</cell><cell>1</cell></row><row><cell>Norwegian</cell><cell>5</cell><cell>1</cell></row><row><cell>Polish</cell><cell>5</cell><cell>1</cell></row><row><cell>Romanian</cell><cell>5</cell><cell>1</cell></row><row><cell>Turkish</cell><cell>5</cell><cell>1</cell></row><row><cell>Visual</cell><cell>3</cell><cell>2</cell></row><row><cell>Bulgarian</cell><cell>2</cell><cell>1</cell></row><row><cell>Croatian</cell><cell>2</cell><cell>1</cell></row><row><cell>Czech</cell><cell>2</cell><cell>1</cell></row><row><cell>Finnish</cell><cell>2</cell><cell>1</cell></row><row><cell>Hungarian</cell><cell>2</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,91.44,535.57,440.17,217.57"><head>Table 4 .</head><label>4</label><figDesc>MAP for ad-hoc averaged across all submissions by query dimension.</figDesc><table coords="6,91.44,535.57,440.17,217.57"><row><cell cols="2">Query Language MAP P10 P100 Recall Group</cell><cell>Run ID</cell><cell cols="3">Initial Query Feedback Modality</cell></row><row><cell>English</cell><cell>0.4135 0.5500 0.3197 0.8434 CUHK</cell><cell>CUHK-ad-eng-tv-kl-jm2</cell><cell>title+img</cell><cell>with</cell><cell>text+img</cell></row><row><cell cols="2">Chinese (trad.) 0.3993 0.5893 0.3211 0.7526 NTU</cell><cell>NTU-CE-TN-WEprf-Ponly</cell><cell>title+narr</cell><cell>with</cell><cell>text+img</cell></row><row><cell>Spanish (LA)</cell><cell cols="2">0.3447 0.4857 0.2839 0.7891 Alicante, Jaen R2D2vot2SpL</cell><cell>title</cell><cell>with</cell><cell>text</cell></row><row><cell>Dutch</cell><cell cols="2">0.3435 0.4821 0.2575 0.7891 Alicante, Jaen R2D2vot2Du</cell><cell>title</cell><cell>with</cell><cell>text</cell></row><row><cell>Visual</cell><cell>0.3425 0.5821 0.2650 0.7009 NTU</cell><cell>NTU-adhoc05-EX-prf</cell><cell>visual</cell><cell>with</cell><cell>image</cell></row><row><cell>German</cell><cell cols="2">0.3375 0.4929 0.2514 0.6383 Alicante, Jaen R2D2vot2Ge</cell><cell>title</cell><cell>with</cell><cell>text</cell></row><row><cell cols="2">Spanish (Euro) 0.3175 0.4536 0.2804 0.8048 UNED</cell><cell>unedESENent</cell><cell>title</cell><cell>with</cell><cell>text</cell></row><row><cell>Portuguese</cell><cell>0.3073 0.4250 0.2436 0.7542 Miracle</cell><cell>imirt0attrpt</cell><cell>title</cell><cell cols="2">without text</cell></row><row><cell>Greek</cell><cell>0.3024 0.4321 0.2389 0.6383 DCU</cell><cell>DCUFbTGR</cell><cell>title</cell><cell>with</cell><cell>text</cell></row><row><cell>French</cell><cell>0.2864 0.4036 0.2582 0.7322 Jaen</cell><cell>SinaiFrTitleNarrFBSystran</cell><cell>title+narr</cell><cell>with</cell><cell>text</cell></row><row><cell>Japanese</cell><cell>0.2811 0.3679 0.2086 0.7333 Alicante</cell><cell>AlCimg05Exp3Jp</cell><cell>title</cell><cell>with</cell><cell>text</cell></row><row><cell>Russian</cell><cell>0.2798 0.3571 0.2136 0.6879 DCU</cell><cell>DCUFbTRU</cell><cell>title</cell><cell>with</cell><cell>text</cell></row><row><cell>Italian</cell><cell>0.2468 0.3536 0.2054 0.6227 Miracle</cell><cell>imirt0attrit</cell><cell>title</cell><cell cols="2">without text</cell></row><row><cell cols="2">Chinese (simpl.) 0.2305 0.3179 0.1732 0.6153 Alicante</cell><cell>AlCimg05Exp3ChS</cell><cell>title</cell><cell>with</cell><cell>text</cell></row><row><cell>Indonesian</cell><cell>0.2290 0.4179 0.2068 0.6566 Indonesia</cell><cell>UI-T-IMG</cell><cell>title</cell><cell cols="2">without text+img</cell></row><row><cell>Turkish</cell><cell>0.2225 0.3036 0.1929 0.6320 Miracle</cell><cell>imirt0allftk</cell><cell>title</cell><cell cols="2">without text</cell></row><row><cell>Swedish</cell><cell>0.2074 0.3393 0.1664 0.5647 Jaen</cell><cell cols="2">SinaiSweTitleNarrFBWordlingo title</cell><cell cols="2">without text</cell></row><row><cell>Norwegian</cell><cell>0.1610 0.1964 0.1425 0.4530 Miracle</cell><cell>imirt0attrno</cell><cell>title</cell><cell cols="2">without text</cell></row><row><cell>Filipino</cell><cell>0.1486 0.1571 0.1229 0.3695 Miracle</cell><cell>imirt0allffl</cell><cell>title</cell><cell cols="2">without text</cell></row><row><cell>Polish</cell><cell>0.1558 0.2643 0.1239 0.5073 Miracle</cell><cell>imirt0attrpo</cell><cell>title</cell><cell cols="2">without text</cell></row><row><cell>Romanian</cell><cell>0.1429 0.2214 0.1218 0.3747 Miracle</cell><cell>imirt0attrro</cell><cell>title</cell><cell cols="2">without text</cell></row><row><cell>Bulgarian</cell><cell>0.1293 0.2250 0.1196 0.5694 Miracle</cell><cell>imirt0allfbu</cell><cell>title</cell><cell cols="2">without text</cell></row><row><cell>Czech</cell><cell>0.1219 0.1929 0.1343 0.5310 Miracle</cell><cell>imirt0allfcz</cell><cell>title</cell><cell cols="2">without text</cell></row><row><cell>Croatian</cell><cell>0.1187 0.1679 0.1075 0.4362 Miracle</cell><cell>imirt0attrcr</cell><cell>title</cell><cell cols="2">without text</cell></row><row><cell>Finnish</cell><cell>0.1114 0.1321 0.1211 0.3257 Miracle</cell><cell>imirt0attrfi</cell><cell>title</cell><cell cols="2">without text</cell></row><row><cell>Hungarian</cell><cell>0.0968 0.1321 0.0768 0.3789 Miracle</cell><cell>imirt0allfhu</cell><cell>title</cell><cell cols="2">without text</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="13,178.44,149.33,245.93,303.47"><head>Table 7 .</head><label>7</label><figDesc>Overview of the best manual retrieval results.</figDesc><table coords="13,196.68,168.68,185.89,284.13"><row><cell>Run identifier</cell><cell cols="3">visual textual results</cell></row><row><cell>IPALI2R Tn</cell><cell></cell><cell>x</cell><cell>0.2084</cell></row><row><cell>IPALI2R T</cell><cell></cell><cell>x</cell><cell>0.2075</cell></row><row><cell>i6-En.clef</cell><cell></cell><cell>x</cell><cell>0.2065</cell></row><row><cell>UBimed en-fr.T.BI2</cell><cell></cell><cell>x</cell><cell>0.1746</cell></row><row><cell>SinaiEn okapi nofb</cell><cell></cell><cell>x</cell><cell>0.091</cell></row><row><cell>I2Rfus.txt</cell><cell>x</cell><cell></cell><cell>0.1455</cell></row><row><cell>I2RcPBcf.txt</cell><cell>x</cell><cell></cell><cell>0.1188</cell></row><row><cell>I2RcPBnf.txt</cell><cell>x</cell><cell></cell><cell>0.1114</cell></row><row><cell>I2RbPBcf.txt</cell><cell>x</cell><cell></cell><cell>0.1068</cell></row><row><cell>I2RbPBnf.txt</cell><cell>x</cell><cell></cell><cell>0.1067</cell></row><row><cell>mirabase.qtop(GIFT)</cell><cell>x</cell><cell></cell><cell>0.0942</cell></row><row><cell>mirarf5.1.qtop</cell><cell>x</cell><cell></cell><cell>0.0942</cell></row><row><cell>GE M 4g.txt</cell><cell>x</cell><cell></cell><cell>0.0941</cell></row><row><cell>mirarf5.qtop</cell><cell>x</cell><cell></cell><cell>0.0941</cell></row><row><cell>mirarf5.2.qtop</cell><cell>x</cell><cell></cell><cell>0.0934</cell></row><row><cell>IPALI2R TIan</cell><cell>x</cell><cell>x</cell><cell>0.2821</cell></row><row><cell>IPALI2R TIa</cell><cell>x</cell><cell>x</cell><cell>0.2819</cell></row><row><cell>nctu visual+text auto 4</cell><cell>x</cell><cell>x</cell><cell>0.2389</cell></row><row><cell>UBimed en-fr.TI.1</cell><cell>x</cell><cell>x</cell><cell>0.2358</cell></row><row><cell>IPALI2R TImn</cell><cell>x</cell><cell>x</cell><cell>0.2325</cell></row><row><cell>nctu visual+text auto 8</cell><cell>x</cell><cell>x</cell><cell>0.2324</cell></row><row><cell>nctu visual+text auto 6</cell><cell>x</cell><cell>x</cell><cell>0.2318</cell></row><row><cell>IPALI2R TIm</cell><cell>x</cell><cell>x</cell><cell>0.2312</cell></row><row><cell>nctu visual+text auto 3</cell><cell>x</cell><cell>x</cell><cell>0.2286</cell></row><row><cell>nctu visual+text auto 1</cell><cell>x</cell><cell>x</cell><cell>0.2276</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="19,90.00,149.33,422.74,685.19"><head>Table 10 .</head><label>10</label><figDesc>The number of training and test images in the classes, the average, minimum, and maximum error rates.</figDesc><table coords="19,135.84,179.48,328.26,655.05"><row><cell>class</cell><cell>train im-ages</cell><cell>test im-ages</cell><cell>avg. classification accuracy [%]</cell><cell>most mistaken class</cell><cell>avg. images that were classified to the most mistaken class [%]</cell></row><row><cell>1</cell><cell>336</cell><cell>38</cell><cell>84.0</cell><cell>25</cell><cell>3.9</cell></row><row><cell>2</cell><cell>32</cell><cell>3</cell><cell>18.7</cell><cell>44</cell><cell>46.3</cell></row><row><cell>3</cell><cell>215</cell><cell>24</cell><cell>69.6</cell><cell>5</cell><cell>3.0</cell></row><row><cell>4</cell><cell>102</cell><cell>12</cell><cell>57.3</cell><cell>3</cell><cell>5.9</cell></row><row><cell>5</cell><cell>225</cell><cell>25</cell><cell>75.6</cell><cell>3</cell><cell>2.6</cell></row><row><cell>6</cell><cell>576</cell><cell>67</cell><cell>66.0</cell><cell>12</cell><cell>4.4</cell></row><row><cell>7</cell><cell>77</cell><cell>8</cell><cell>27.7</cell><cell>6</cell><cell>21.0</cell></row><row><cell>8</cell><cell>48</cell><cell>3</cell><cell>6.5</cell><cell>6</cell><cell>38.2</cell></row><row><cell>9</cell><cell>69</cell><cell>10</cell><cell>21.0</cell><cell>21</cell><cell>19.8</cell></row><row><cell>10</cell><cell>32</cell><cell>7</cell><cell>6.3</cell><cell>6</cell><cell>10.8</cell></row><row><cell>11</cell><cell>108</cell><cell>12</cell><cell>26.0</cell><cell>6</cell><cell>9.8</cell></row><row><cell cols="2">12 2563</cell><cell>297</cell><cell>90.7</cell><cell>34</cell><cell>1.5</cell></row><row><cell>13</cell><cell>93</cell><cell>7</cell><cell>17.1</cell><cell>12</cell><cell>18.5</cell></row><row><cell>14</cell><cell>152</cell><cell>14</cell><cell>57.1</cell><cell>12</cell><cell>9.9</cell></row><row><cell>15</cell><cell>15</cell><cell>3</cell><cell>26.8</cell><cell>5</cell><cell>18.7</cell></row><row><cell>16</cell><cell>23</cell><cell>1</cell><cell>9.8</cell><cell>6</cell><cell>31.7</cell></row><row><cell>17</cell><cell>217</cell><cell>24</cell><cell>71.3</cell><cell>34</cell><cell>5.1</cell></row><row><cell>18</cell><cell>205</cell><cell>12</cell><cell>43.5</cell><cell>6</cell><cell>19.5</cell></row><row><cell>19</cell><cell>137</cell><cell>17</cell><cell>62.1</cell><cell>6</cell><cell>4.7</cell></row><row><cell>20</cell><cell>31</cell><cell>2</cell><cell>13.4</cell><cell>21</cell><cell>24.4</cell></row><row><cell>21</cell><cell>194</cell><cell>29</cell><cell>66.6</cell><cell>6</cell><cell>4.3</cell></row><row><cell>22</cell><cell>48</cell><cell>3</cell><cell>25.2</cell><cell>19</cell><cell>9.8</cell></row><row><cell>23</cell><cell>79</cell><cell>10</cell><cell>29.5</cell><cell>21</cell><cell>8.0</cell></row><row><cell>24</cell><cell>17</cell><cell>4</cell><cell>32.3</cell><cell>6</cell><cell>16.5</cell></row><row><cell>25</cell><cell>284</cell><cell>36</cell><cell>71.0</cell><cell>1</cell><cell>10.4</cell></row><row><cell>26</cell><cell>170</cell><cell>23</cell><cell>61.3</cell><cell>3</cell><cell>5.3</cell></row><row><cell>27</cell><cell>109</cell><cell>13</cell><cell>62.3</cell><cell>12</cell><cell>5.6</cell></row><row><cell>28</cell><cell>228</cell><cell>16</cell><cell>63.0</cell><cell>12</cell><cell>7.5</cell></row><row><cell>29</cell><cell>86</cell><cell>8</cell><cell>18.6</cell><cell>6</cell><cell>26.2</cell></row><row><cell>30</cell><cell>59</cell><cell>7</cell><cell>26.1</cell><cell>21</cell><cell>11.5</cell></row><row><cell>31</cell><cell>60</cell><cell>8</cell><cell>8.2</cell><cell>6</cell><cell>19.8</cell></row><row><cell>32</cell><cell>78</cell><cell>1</cell><cell>43.9</cell><cell>25</cell><cell>12.2</cell></row><row><cell>33</cell><cell>62</cell><cell>5</cell><cell>22.9</cell><cell>6</cell><cell>12.7</cell></row><row><cell>34</cell><cell>880</cell><cell>79</cell><cell>88.5</cell><cell>12</cell><cell>5.5</cell></row><row><cell>35</cell><cell>18</cell><cell>4</cell><cell>25.6</cell><cell>6</cell><cell>9.8</cell></row><row><cell>36</cell><cell>94</cell><cell>21</cell><cell>40.7</cell><cell>6</cell><cell>5.9</cell></row><row><cell>37</cell><cell>22</cell><cell>2</cell><cell>6.1</cell><cell>36</cell><cell>17.1</cell></row><row><cell>38</cell><cell>116</cell><cell>19</cell><cell>37.6</cell><cell>21</cell><cell>13.5</cell></row><row><cell>39</cell><cell>38</cell><cell>5</cell><cell>7.8</cell><cell>22</cell><cell>12.2</cell></row><row><cell>40</cell><cell>51</cell><cell>3</cell><cell>12.2</cell><cell>23</cell><cell>19.5</cell></row><row><cell>41</cell><cell>65</cell><cell>15</cell><cell>59.3</cell><cell>48</cell><cell>24.4</cell></row><row><cell>42</cell><cell>74</cell><cell>13</cell><cell>66.0</cell><cell>49</cell><cell>21.2</cell></row><row><cell>43</cell><cell>98</cell><cell>8</cell><cell>56.1</cell><cell>6</cell><cell>9.1</cell></row><row><cell>44</cell><cell>193</cell><cell>23</cell><cell>39.1</cell><cell>12</cell><cell>7.2</cell></row><row><cell>45</cell><cell>35</cell><cell>3</cell><cell>26.0</cell><cell>1</cell><cell>19.5</cell></row><row><cell>46</cell><cell>30</cell><cell>1</cell><cell>17.1</cell><cell>28</cell><cell>26.8</cell></row><row><cell>47</cell><cell>147</cell><cell>15</cell><cell>42.4</cell><cell>6</cell><cell>33.2</cell></row><row><cell>48</cell><cell>79</cell><cell>6</cell><cell>66.7</cell><cell>41</cell><cell>20.3</cell></row><row><cell>49</cell><cell>78</cell><cell>9</cell><cell>54.2</cell><cell>42</cell><cell>35.2</cell></row><row><cell>50</cell><cell>91</cell><cell>8</cell><cell>33.5</cell><cell>6</cell><cell>25.6</cell></row><row><cell>51</cell><cell>9</cell><cell>1</cell><cell>43.9</cell><cell>12</cell><cell>17.1</cell></row><row><cell>52</cell><cell>9</cell><cell>1</cell><cell>51.2</cell><cell>26</cell><cell>9.8</cell></row><row><cell>53</cell><cell>15</cell><cell>3</cell><cell>16.3</cell><cell>5</cell><cell>29.3</cell></row><row><cell>54</cell><cell>46</cell><cell>3</cell><cell>57.7</cell><cell>21</cell><cell>11.4</cell></row><row><cell>55</cell><cell>10</cell><cell>2</cell><cell>11.0</cell><cell>54</cell><cell>23.2</cell></row><row><cell>56</cell><cell>15</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>57</cell><cell>57</cell><cell>7</cell><cell>81.5</cell><cell>12</cell><cell>5.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="21,91.44,467.69,427.13,266.62"><head>Table 11 .</head><label>11</label><figDesc>Topics used in ImageCLEF and maximum MAP, P10, P100 and RelRetr scores.</figDesc><table coords="21,91.44,500.77,427.13,233.53"><row><cell cols="2">Number Title</cell><cell cols="4">Pool size (% max) Relevant MAP P10 P100 Rel retr</cell></row><row><cell>1</cell><cell>Aircraft on the ground</cell><cell>1690 (9.7%)</cell><cell>85</cell><cell>0.8259 1.0000 0.7600</cell><cell>83</cell></row><row><cell>2</cell><cell>People gathered at bandstand</cell><cell>2420 (13.9%)</cell><cell>27</cell><cell>0.6899 0.9000 0.2600</cell><cell>27</cell></row><row><cell>3</cell><cell>Dog in sitting position</cell><cell>763 (4.4%)</cell><cell>34</cell><cell>0.5723 1.0000 0.2700</cell><cell>34</cell></row><row><cell>4</cell><cell>Steam ship docked</cell><cell>1797 (10.3%)</cell><cell>76</cell><cell>0.4316 0.9000 0.4000</cell><cell>72</cell></row><row><cell>5</cell><cell>Animal statue</cell><cell>861 (4.9%)</cell><cell>37</cell><cell>0.8349 1.0000 0.3400</cell><cell>37</cell></row><row><cell>6</cell><cell>Small sailing boat</cell><cell>1447 (8.3%)</cell><cell>122</cell><cell cols="2">0.6975 1.0000 0.7200 118</cell></row><row><cell>7</cell><cell>Fishermen in boat</cell><cell>1182 (6.8%)</cell><cell>32</cell><cell>0.5151 0.9000 0.3000</cell><cell>32</cell></row><row><cell>8</cell><cell>Building covered in snow</cell><cell>1329 (7.6%)</cell><cell>38</cell><cell>0.4177 0.7000 0.2800</cell><cell>35</cell></row><row><cell>9</cell><cell>Horse pulling cart or carriage</cell><cell>1435 (8.2%)</cell><cell>108</cell><cell cols="2">0.5972 1.0000 0.6200 108</cell></row><row><cell>10</cell><cell>Sun pictures &amp; Scotland</cell><cell>1553 (8.9%)</cell><cell>203</cell><cell cols="2">0.7139 1.0000 0.9300 197</cell></row><row><cell>11</cell><cell>Swiss mountain scenery</cell><cell>1460 (8.4%)</cell><cell>83</cell><cell>0.9660 1.0000 0.8000</cell><cell>83</cell></row><row><cell>12</cell><cell>Postcards from Iona &amp; Scotland</cell><cell>1665 (9.5%)</cell><cell>34</cell><cell>0.7493 1.0000 0.3400</cell><cell>34</cell></row><row><cell>13</cell><cell>Stone viaduct with several arches</cell><cell>1567 (9.0%)</cell><cell>184</cell><cell cols="2">0.5587 1.0000 0.7000 174</cell></row><row><cell>14</cell><cell>People at the marketplace</cell><cell>1203 (6.9%)</cell><cell>55</cell><cell>0.8207 1.0000 0.5100</cell><cell>55</cell></row><row><cell>15</cell><cell>Golfer putting on green</cell><cell>1367 (7.8%)</cell><cell>48</cell><cell>0.5652 0.9000 0.3700</cell><cell>48</cell></row><row><cell>16</cell><cell>Waves breaking on beach</cell><cell>1544 (8.8%)</cell><cell>71</cell><cell>0.5281 1.0000 0.4100</cell><cell>68</cell></row><row><cell>17</cell><cell>Man or woman reading</cell><cell>1074 (6.2%)</cell><cell>13</cell><cell>0.8156 0.8000 0.1300</cell><cell>13</cell></row><row><cell>18</cell><cell>Woman in white dress</cell><cell>1112 (6.4%)</cell><cell>40</cell><cell>0.2696 0.5000 0.2600</cell><cell>39</cell></row><row><cell>19</cell><cell>Composite postcards of Northern Ireland</cell><cell>1943 (11.1%)</cell><cell>50</cell><cell>0.5017 1.0000 0.5000</cell><cell>50</cell></row><row><cell>20</cell><cell>Royal visit to Scotland (not Fife)</cell><cell>1359 (7.8%)</cell><cell>13</cell><cell>0.7820 0.9000 0.1300</cell><cell>13</cell></row><row><cell>21</cell><cell>Monument to poet Robert Burns</cell><cell>875 (5.0%)</cell><cell>35</cell><cell>0.7349 1.0000 0.3300</cell><cell>35</cell></row><row><cell>22</cell><cell>Building with waving flag</cell><cell>1221 (7.0%)</cell><cell>56</cell><cell>0.6475 1.0000 0.4800</cell><cell>56</cell></row><row><cell>23</cell><cell>Tomb inside church or cathedral</cell><cell>1706 (9.8%)</cell><cell>62</cell><cell>0.7653 1.0000 0.5500</cell><cell>62</cell></row><row><cell>24</cell><cell>Close-up picture of bird</cell><cell>1414 (8.1%)</cell><cell>33</cell><cell>0.6353 1.0000 0.2700</cell><cell>29</cell></row><row><cell>25</cell><cell>Arched gateway</cell><cell>2037 (11.7%)</cell><cell>235</cell><cell cols="2">0.5857 1.0000 0.8700 208</cell></row><row><cell>26</cell><cell>Portrait pictures of mixed sex group</cell><cell>1410 (8.1%)</cell><cell>30</cell><cell>0.7618 0.9000 0.2900</cell><cell>30</cell></row><row><cell>27</cell><cell>Woman or girl carrying basket</cell><cell>1000 (5.7%)</cell><cell>14</cell><cell>0.5011 0.6000 0.1400</cell><cell>14</cell></row><row><cell>28</cell><cell cols="2">Colour pictures of woodland scenes around St Andrews 2263 (13.0%)</cell><cell>98</cell><cell>0.8200 1.0000 0.6700</cell><cell>98</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_0" coords="1,99.96,786.80,161.25,8.97"><p>See http://ir.shef.ac.uk/imageclef/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_1" coords="2,99.96,787.30,168.41,8.27"><p>http://www-library.st-andrews.ac.uk/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_2" coords="3,99.96,786.80,286.01,8.97"><p>See http://ir.shef.ac.uk/imageclef2005/adhoc.htm for an example</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_3" coords="5,99.96,786.80,195.45,8.97"><p>http://trec.nist.gov/trec eval/trec eval.7.3.tar.gz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_4" coords="8,99.96,743.38,112.12,8.27"><p>http://www.casimage.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_5" coords="8,99.96,754.42,116.79,8.27"><p>http://peir.path.uab.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_6" coords="8,99.96,765.34,126.13,8.27"><p>http://www.healcentral.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_7" coords="8,99.96,776.26,149.61,8.27"><p>http://gamma.wustl.edu/home.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_8" coords="8,99.96,787.30,206.03,8.27"><p>http://alf3.urz.unibas.ch/pathopic/intro.htm</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>This work has been funded in part by the <rs type="funder">EU Sixth Framework Programme</rs> (FP6) within the <rs type="projectName">Bricks project (IST</rs> contract number <rs type="grantNumber">507457</rs>) as well as the <rs type="projectName">SemanticMining</rs> project (<rs type="grantNumber">IST NoE 507505</rs>). The establishment of the IRMA database was funded by the <rs type="funder">German Research Community DFG</rs> under grand <rs type="grantNumber">Le 1108/4</rs>. We also acknowledge the generous support of <rs type="funder">National Science Foundation (NSF)</rs> grant <rs type="grantNumber">ITR-0325160</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_TqJXKsH">
					<idno type="grant-number">507457</idno>
					<orgName type="project" subtype="full">Bricks project (IST</orgName>
				</org>
				<org type="funded-project" xml:id="_aCA7QbU">
					<idno type="grant-number">IST NoE 507505</idno>
					<orgName type="project" subtype="full">SemanticMining</orgName>
				</org>
				<org type="funding" xml:id="_MShPAkw">
					<idno type="grant-number">Le 1108/4</idno>
				</org>
				<org type="funding" xml:id="_dw9eHUw">
					<idno type="grant-number">ITR-0325160</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="17,90.00,762.05,422.61,9.96;17,90.00,774.05,422.61,9.96;17,90.00,786.05,422.74,9.96;18,183.96,224.81,235.04,9.96;18,149.04,246.56,43.52,8.97;18,396.56,246.56,54.20,8.97;18,149.04,258.34,98.10,8.27;18,434.34,257.84,16.18,8.97;18,149.04,269.38,182.43,8.27;18,434.19,268.88,16.18,8.97;18,149.04,280.30,93.43,8.27" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="17,90.00,762.05,422.61,9.96;17,90.00,774.05,316.08,9.96;17,320.07,786.05,192.66,9.96;18,183.96,225.31,35.67,8.98">ImageCLEF has continued to attract researchers from a variety of global communities interested image retrieval using both low-level image features and associated texts</title>
		<idno>tamura.06.confidence 13.3 rwth-i6/MESUBMISSION</idno>
		<imprint/>
	</monogr>
	<note>image collection and creating more semantic Table 9. Resulting error rates for the submitted runs submission error rate. rwth-i6/IDMSUBMISSION 12.6 rwth_mi-ccf_idm.03</note>
</biblStruct>

<biblStruct coords="18,442.38,301.76,8.09,8.97;18,149.04,313.18,229.50,8.27;18,434.10,312.68,16.18,8.97;18,149.04,324.10,121.46,8.27;18,434.30,323.60,16.18,8.97;18,149.04,335.14,177.76,8.27;18,434.20,334.64,16.18,8.97;18,149.04,346.06,126.13,8.27;18,434.29,345.56,16.18,8.97;18,149.04,356.98,177.76,8.27;18,434.20,356.48,16.18,8.97;18,149.04,368.02,177.76,8.27;18,434.20,367.52,16.18,8.97;18,149.04,378.94,121.46,8.27;18,434.30,378.44,16.18,8.97;18,149.04,389.86,130.80,8.27;18,434.28,389.36,16.18,8.97;18,149.04,400.90,112.11,8.27;18,434.32,400.40,16.18,8.97;18,149.04,411.82,126.13,8.27;18,434.29,411.32,16.18,8.97;18,149.04,422.74,177.76,8.27;18,434.20,422.24,16.18,8.97;18,149.04,433.78,135.47,8.27;18,434.27,433.28,16.18,8.97;18,149.04,444.70,140.14,8.27;18,434.26,444.20,16.18,8.97;18,149.04,455.62,107.44,8.27;18,434.32,455.12,16.18,8.97;18,149.04,466.66,107.44,8.27;18,434.32,466.16,16.18,8.97;18,149.04,477.58,126.13,8.27;18,434.29,477.08,16.18,8.97;18,149.04,488.50,130.80,8.27;18,434.28,488.00,16.18,8.97;18,149.04,499.54,144.82,8.27;18,434.26,499.04,8.09,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="18,219.30,313.18,140.51,8.27">-random-subwindows-extra-trees</title>
		<idno>NTU-annotate05-1NN.result 21.7 ntu/NTU-annotate05-Top2</idno>
		<ptr target="mira20relp58IB8.txt22.3ntu/NTU-annotate05-SC.result22.5nctu-dblab/nctu_mc_result_1.txt24" />
		<imprint>
			<date type="published" when="2007">3_I2R_sg.dat 21.7 ntu</date>
		</imprint>
	</monogr>
	<note>res 14.7 geneva-gift/GIFT5NN_8g.txt 20.6 infocomm/Annotation_result4_I2R_sg.dat 20.6 geneva. .txt 20.9 infocomm/Annotation_result1_I2R_sg.dat 20.9 infocomm/Annotation_result2_I2R_sg.dat 21.0 geneva-gift/GIFT1NN_8g.txt 21.2 geneva. .txt 21.3 miracle. 21.7 infocomm</note>
</biblStruct>

<biblStruct coords="18,442.23,696.20,8.09,8.97;18,149.04,708.10,243.79,8.27;18,434.11,707.60,16.18,8.97;20,90.00,426.95,62.80,10.77" xml:id="b2">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="18,446.27,696.20,4.05,8.97;18,149.04,708.10,84.33,8.27">3 Euclidean Distance</title>
		<title level="s" coord="18,318.09,708.10,74.74,8.27">Nearest-Neighbor</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,98.10,449.96,414.73,8.97;20,106.80,461.00,388.14,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="20,275.27,449.96,220.65,8.97">The CLEF 2003 cross language image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,106.80,461.00,298.48,8.97">Proceedings of Cross Language Evaluation Forum (CLEF) 2003 Workshop</title>
		<meeting>Cross Language Evaluation Forum (CLEF) 2003 Workshop<address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,98.10,471.56,414.73,8.97;20,106.80,482.60,405.77,8.97;20,106.80,493.52,405.92,8.97;20,106.80,504.44,405.81,8.97;20,106.80,515.48,16.18,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="20,276.12,471.56,236.71,8.97;20,106.80,482.60,405.77,8.97;20,106.80,493.52,38.48,8.97">The CLEF 2004 Cross Language Image Retrieval Track, In Multilingual Information Access for Text, Speech and Images: Results of the Fifth CLEF Evaluation Campaign</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,482.38,493.52,30.34,8.97;20,106.80,504.44,143.60,8.97">Lecture Notes in Computer Science (LNCS)</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,98.10,526.16,414.49,8.97;20,106.80,537.08,405.40,8.97;20,106.80,548.00,20.74,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="20,345.20,526.16,167.39,8.97;20,106.80,537.08,58.33,8.97">Pichunter: Bayesian relevance feedback for image retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Omohundro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Yianilos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,171.68,537.08,291.87,8.97">Proceedings of the 13th International Conference on Pattern Recognition</title>
		<meeting>the 13th International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,98.10,558.68,414.36,8.97;20,106.80,569.60,405.87,8.97;20,106.80,580.64,29.97,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="20,282.12,558.68,230.34,8.97;20,106.80,569.60,61.23,8.97">Towards a Topic Complexity Measure for Cross-Language Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,184.50,569.60,290.44,8.97">Proceedings of Cross Language Evaluation Forum (CLEF) 2005 Workshop</title>
		<meeting>Cross Language Evaluation Forum (CLEF) 2005 Workshop<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="20,98.10,591.32,414.62,8.97;20,106.80,602.24,405.88,8.97;20,106.80,613.16,63.92,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="20,230.92,591.32,281.79,8.97;20,106.80,602.24,74.74,8.97">Concept Hierarchy across Languages in Text-Based Image Retrieval: A User Evaluation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Petrelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,202.13,602.24,305.80,8.97">Proceedings of Cross Language Evaluation Forum (CLEF) 2005 Workshop</title>
		<meeting>Cross Language Evaluation Forum (CLEF) 2005 Workshop<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="20,98.10,623.84,414.68,8.97;20,106.80,634.76,405.89,8.97;20,106.80,645.80,29.97,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="20,399.36,623.84,113.42,8.97;20,106.80,634.76,51.16,8.97">Boolean Operators in Interactive Search</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Villena-Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Crespo-García</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>González-Cristóbal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,176.47,634.76,297.52,8.97">Proceedings of Cross Language Evaluation Forum (CLEF) 2005 Workshop</title>
		<meeting>Cross Language Evaluation Forum (CLEF) 2005 Workshop<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="20,98.10,656.48,414.52,8.97;20,106.80,667.40,198.06,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="20,319.07,656.48,193.54,8.97;20,106.80,667.40,25.07,8.97">Introducing HEAL: The health education assets library</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Candler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Uijtdehaage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Dennis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,139.53,667.40,75.81,8.96">Academic Medicine</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="253" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,98.10,678.08,414.62,8.97;20,106.80,689.00,360.15,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="20,399.99,678.08,112.73,8.97;20,106.80,689.00,237.46,8.97">Webbasierte Lernwerkzeuge für die Pathologie -web-based learning tools for pathology</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Glatz-Krieger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Glatz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gysel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dittler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Mihatsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,352.60,689.00,36.66,8.96">Pathologe</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="394" to="399" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,98.10,699.68,414.59,8.97;20,106.80,710.60,405.90,8.97;20,106.80,721.64,257.98,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="20,304.90,699.68,207.79,8.97;20,106.80,710.60,209.75,8.97">Task analysis for evaluating image retrieval systems in the ImageCLEF biomedical image retrieval task</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,340.77,710.60,171.93,8.96;20,106.80,721.64,122.77,8.96">Slice of Life conference on Multimedia in Medical Education (SOL 2005)</title>
		<meeting><address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,97.75,732.32,414.85,8.97;20,106.80,743.24,287.70,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="20,238.84,732.32,269.35,8.97">How well do physicians use electronic information retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Hickam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,106.80,743.24,179.84,8.96">Journal of the American Medical Association</title>
		<imprint>
			<biblScope unit="volume">280</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1347" to="1352" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,97.75,753.92,415.05,8.97;20,106.80,764.84,405.74,8.97;20,106.80,775.76,405.77,8.97;20,106.80,786.80,187.07,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="20,247.81,753.92,244.45,8.97">Searching for photos -journalists&apos; practices in pictorial IR</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Markkula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sormunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,314.37,764.84,198.17,8.96;20,106.80,775.76,291.89,8.97">The Challenge of Image Retrieval, A Workshop and Symposium on Image Retrieval, Electronic Workshops in Computing</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Eakins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jose</surname></persName>
		</editor>
		<meeting><address><addrLine>Newcastle upon Tyne</addrLine></address></meeting>
		<imprint>
			<publisher>The British Computer Society</publisher>
			<date type="published" when="1998-02-06">5-6 February 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,97.75,153.08,414.95,8.97;21,106.80,164.12,401.04,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="21,363.66,153.08,149.05,8.97;21,106.80,164.12,136.34,8.97">A reference data set for the evaluation of medical image retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-P</forename><surname>Vallée</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Terrier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,251.54,164.12,178.37,8.96">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="295" to="305" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,97.75,175.04,414.82,8.97;21,106.80,185.96,338.35,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="21,399.86,175.04,112.72,8.97;21,106.80,185.96,145.15,8.97">Casimage project -a digital teaching files authoring environment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dfouni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-P</forename><surname>Vallée</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ratib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,260.29,185.96,113.74,8.96">Journal of Thoracic Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,97.75,197.00,415.03,8.97;21,106.80,207.92,266.52,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="21,367.07,197.00,145.71,8.97;21,106.80,207.92,48.00,8.97">An internet-based nuclear medicine teaching file</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Vreeland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,161.73,207.92,112.84,8.96">Journal of Nuclear Medicine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1520" to="1527" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,97.75,218.84,414.99,8.97;21,106.80,229.88,405.78,8.97;21,106.80,240.80,176.00,8.97" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="21,470.05,218.84,42.69,8.97;21,106.80,229.88,307.46,8.97">Automatic categorisation of medical images for content-based retrieval and data mining</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">O</forename><surname>Gld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">B</forename><surname>Wein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,421.80,229.88,90.79,8.97;21,106.80,240.80,89.20,8.97">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="155" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,97.75,251.72,414.94,8.97;21,106.80,262.76,312.81,8.97" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="21,436.45,251.72,76.25,8.97;21,106.80,262.76,174.88,8.97">Quality of DICOM header information for image categorization</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">O</forename><surname>Güld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohnen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bredno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,288.94,262.76,46.51,8.97">Procs SPIE</title>
		<imprint>
			<biblScope unit="volume">4685</biblScope>
			<biblScope unit="page" from="280" to="287" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,97.75,273.68,414.72,8.97;21,106.80,284.60,207.66,8.97" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="21,351.67,273.68,160.80,8.97;21,106.80,284.60,69.54,8.97">The IRMA code for unique classification of medical images</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohnen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">B</forename><surname>Wein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,183.79,284.60,46.51,8.97">Procs SPIE</title>
		<imprint>
			<biblScope unit="volume">5033</biblScope>
			<biblScope unit="page" from="440" to="451" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,97.75,295.64,414.79,8.97;21,106.80,306.56,32.96,8.97" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="21,196.48,295.64,155.38,8.97">Orientation correction for chest images</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pietka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,359.44,295.64,106.94,8.97">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="189" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,97.75,317.48,414.92,8.97;21,106.80,328.52,405.76,8.97;21,106.80,339.44,32.96,8.97" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="21,254.16,317.48,258.51,8.97;21,106.80,328.52,241.46,8.97">Recognition of chest radiograph orientation for picture archiving and communications systems display using neural networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Boone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Seshagiri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Steiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,356.21,328.52,108.98,8.97">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="190" to="193" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,97.75,350.36,414.84,8.97;21,106.80,361.40,280.18,8.97" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="21,392.55,350.36,120.04,8.97;21,106.80,361.40,79.20,8.97">Determining the view position of chest radiographs</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">O</forename><surname>Güld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohnen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">B</forename><surname>Wein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,193.37,361.40,106.82,8.97">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="280" to="291" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,97.75,372.32,414.82,8.97;21,106.80,383.24,210.41,8.97" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="21,208.24,372.32,304.33,8.97;21,106.80,383.24,72.48,8.97">A continuous and probabilistic framework for medical image representation and categorization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,186.54,383.24,46.51,8.97">Procs SPIE</title>
		<imprint>
			<biblScope unit="volume">5371</biblScope>
			<biblScope unit="page" from="230" to="238" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,97.75,394.28,414.71,8.97;21,106.80,405.20,212.11,8.97" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="21,232.48,394.28,275.55,8.97">Classification of Medical Images using Non-linear Distortion Models</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gollan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,106.80,405.20,151.45,8.97">Proc. Bildverarbeitung fr die Medizin</title>
		<imprint>
			<biblScope unit="page" from="366" to="370" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,97.75,416.12,414.93,8.97;21,106.80,427.16,250.58,8.97" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="21,355.86,416.12,156.82,8.97;21,106.80,427.16,112.46,8.97">Comparison of global features for categorization of medical images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">O</forename><surname>Güld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Leisten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,226.84,427.16,46.51,8.97">Procs SPIE</title>
		<imprint>
			<biblScope unit="volume">5371</biblScope>
			<biblScope unit="page" from="211" to="222" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
