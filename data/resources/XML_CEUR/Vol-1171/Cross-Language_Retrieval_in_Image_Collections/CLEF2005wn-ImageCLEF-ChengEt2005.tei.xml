<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,84.12,75.59,427.16,12.58">NCTU_DBLAB@ImageCLEFmed 2005: Medical Image Retrieval Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,149.52,112.68,70.84,9.02"><forename type="first">Pei-Cheng</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Science</orgName>
								<orgName type="institution">National Chiao Tung University</orgName>
								<address>
									<addrLine>1001 Ta Hsueh Rd</addrLine>
									<postCode>30050</postCode>
									<settlement>Hsinchu</settlement>
									<region>O.C</region>
									<country>TAIWAN, R</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,228.64,112.68,74.19,9.02"><forename type="first">Been-Chian</forename><surname>Chien</surname></persName>
							<email>bcchien@mail.nutn.edu.tw</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National University of Tainan</orgName>
								<address>
									<addrLine>33, Sec. 2, Su-Lin Street</addrLine>
									<postCode>700</postCode>
									<settlement>Tainan</settlement>
									<country>Taiwan, R.O</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.05,112.68,50.29,9.02"><forename type="first">Hao-Ren</forename><surname>Ke</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Information Management</orgName>
								<orgName type="institution" key="instit1">University Library</orgName>
								<orgName type="institution" key="instit2">National Chiao Tung University</orgName>
								<address>
									<addrLine>1001 Ta Hsueh Rd</addrLine>
									<postCode>30050</postCode>
									<settlement>Hsinchu</settlement>
									<region>O.C</region>
									<country>TAIWAN, R</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,386.42,112.68,64.19,9.02"><forename type="first">Wei-Pang</forename><surname>Yang</surname></persName>
							<email>wpyang@cis.nctu.edu.tw</email>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Information Management</orgName>
								<orgName type="institution">National Dong Hwa University</orgName>
								<address>
									<addrLine>1, Sec. 2, Da Hsueh Rd., Shou-Feng</addrLine>
									<settlement>Hualien</settlement>
									<country>Taiwan, R.O</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,84.12,75.59,427.16,12.58">NCTU_DBLAB@ImageCLEFmed 2005: Medical Image Retrieval Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DA020CA78AE54D0B70FA7C373B7D5227</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ACM Categories and Subject Descriptors: Information Storage and Retrieval</term>
					<term>Artificial Intelligence</term>
					<term>Free Medical Image retrieval</term>
					<term>content based image retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this article, we describe the used technologies and experimental results for the medical retrieval task at ImageCLEF 2005. The topics of competition this year contain both semantic queries and visual queries. The content-based approach containing four image features and the text-based approach using word expansion are developed to accomplish the mission. The experimental results show that the text-based approach has higher precision rate than content-based approach. Further, the results of combining both the content-based and text-based approaches are better than those using only one of the approaches. We summarize that the consideration on the image of visual queries can provide more human semantic perception and improve the efficiency for medical image retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper, we present the research experience of the NCTU group at ImageCLEFmed 2005. The dataset of medical image retrieval task contains about 50,000 images totally from the Casimage, MIR, PEIR, and PathoPIC datasets. Each image of collection contains annotations in XML format. The majority of the annotations are in English but a significant number is also in French and German, with a few cases that do not contain any annotation at all. The queries of this task were formulated with example images and a short textual description explaining the research goal. The participants were requested to accomplish the task in either fully automatic retrieval or retrieval with manual feedback. The used query information can consider the example images or the textual description only, or combine the images and textual together. The main purpose is to evaluate the retrieval of medical images from heterogeneous and multilingual document collections containing images as well as text. For handling the medical image retrieval task, the NCTU group gives two primitives: content-based approach and text-based approach. The combination of the two approaches using similarity weight was also discussed. The content-based approach in this work uses four image features, Facade scale image feature, Gray Histogram layout, Coherence Moment and Color histogram, extracted from the images directly. The text-based approach processes the annotations based on the vector space model and word expansion using Wordnet <ref type="bibr" coords="1,457.45,643.09,43.65,9.02">[Miller95]</ref>. The mixed retrieval of visual and textual is done by combining the content-based approach and the text-approach with different weights adjusting.</p><p>In Section 2, the image features for the content-based approach are described. Section 3 illustrates the text-based approach that processes the multilingual annotations and translation. Section 4 describes our submissions at ImageCLEF 2005 and the ranking results. Here, we give an explanation and a discussion on our experimental results. Finally, section 5 provides concluding remarks and future direction are for medical image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Image features</head><p>This section describes the features used in the paper for the ImageCLEF 2005 evaluation. In an image retrieval system, image features are extracted from pixels of image. For fast response time, image features used must be concise, and for precision, image features is used must contain meaningful information to represent the image itself. Image feature can retrieval the image in visual. In this paper we adopt several image features we have proposed <ref type="bibr" coords="2,110.08,130.75,40.85,9.02">[Cheng 04</ref>] which image features have good performance in medical image application. In designing the image features, to emphasize the contrast of an image and handle images with less illuminative influence, we normalize the value of a pixel before quantization. In <ref type="bibr" coords="2,372.82,153.72,43.59,9.02">[Cheng 04</ref>] we proposed a relative normalization method. First, we cluster a whole image into four clusters by the K-means clustering method <ref type="bibr" coords="2,70.92,176.76,31.35,9.02" target="#b6">[Han01]</ref>. We sort the four clusters ascendant according to their mean values. After clustering, we shift the mean of the first cluster to value 50 and the fourth cluster to 200; then, each pixel in a cluster is multiplied by a relative weight to normalize. Let m c1 be the mean value of cluster 1 and m c4 is the mean value of cluster 4. The normalization formula of pixel p(x,y) is defined in Eq. (1).</p><formula xml:id="formula_0" coords="2,169.01,229.29,198.82,24.77">) ( 200 )) 50 ( ) , ( ( ) , ( 1 4 1 c c c normal m m m y x p y x p - × - - =</formula><p>.</p><p>(1)</p><p>After normalization, we scale an image into common 128*128 pixels and extract image features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Facade scale image feature</head><p>The pixel values of an image are trivial and straight-forward features. For computational efficiency, images are always scaled to a common small size and compared using the Euclidean distance. <ref type="bibr" coords="2,411.72,309.66,48.92,9.02" target="#b7">[Keysers04]</ref> has shown that in the optical character recognition and medical image retrieval based on facade image features have obtained excellent results. In this work we scale down an image into 8×8 pixels to form a 64 feature vectors as facade scale image feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gray Histogram layout</head><p>Histogram <ref type="bibr" coords="2,115.92,367.92,41.73,9.02" target="#b8">[Swain91]</ref> is a prime image feature for image information retrieval. Histogram method is invariant in image rotation, it is easy to implement and have good result in color image indexing. Because radiotherapy medical image only consists of gray level, the spatial relationship becomes very important. Medical images always contain particular anatomic regions (lung, liver, head, and so on); therefore, similar images have similar spatial structures. We divide an image into nine sections and calculate their histogram respectively. After normalization, the gray values are quantized into 16 levels for computational efficiency.</p><p>In the gray histogram, the gray value may be quantized into several bins to improve the similarity between adjacent bins. We set an interval range δ to extend the similarity of each gray value. The histogram layout feature estimates the probability of each gray level that appears in a particular area. The probability equation is defined in Eq. (2), where δ is set to 10, where p j is a pixel of the image, and m is the total number of pixels. The gray histogram layout of an image has a total of 144 bins.</p><formula xml:id="formula_1" coords="2,202.97,498.75,140.16,52.66">m c p p I h m j i j j c i ∑ = ∩ + - = 1 ] 2 , 2 [ ) ( δ δ δ .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Coherence Moment</head><p>One of the problems to design an image representation is the semantic gap. The state-of-the-art technology still cannot reliably identify objects. The coherence moment feature attempts to describe the features from the human's viewpoint in order to reduce the semantic gap. We cluster an image into four classes by the K-means algorithm. After clustering an image into four classes, we calculate the number of pixels (COH κ ), mean value of gray value (COH μ ) and standard variance of gray value (COH ρ ) in each class. For each class, we group connected pixels into eight directions as an object. If an object is bigger than 5% of the whole image, we denote it as a big object; otherwise it is a small object. We count how many big objects (COH ο ) and small objects (COH ν ) are in each class, and use COH ο and COH ν as parts of image features. Since we intend to know how the reciprocal effects among pixels, so we use the smooth method on the image. If the spatial distribution of pixels of two images is similar, they will also be similar after smoothing. If their spatial distributions are quite different, they may have a different result after smoothing. After smoothing, we cluster an image into four classes and calculate the number of big objects (COH τ ) and small objects (COH ω ). Each pixel will be influenced by its neighboring pixels. Two close objects of the same class may be merged into one object. Then, we can analyze the variation between the two images before and after smoothing. The coherence moment of each class form a seven-feature vector, (COH κ , COH μ , COH ρ , COH ο , COH ν , COH τ , COH ω ). The coherence moment of an image is a 56-feature vector that combines the coherence moments of the four classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Color histogram Features</head><p>Color histogram <ref type="bibr" coords="3,141.08,96.24,41.78,9.02" target="#b8">[Swain91]</ref> is a basic method and has good performance for representing image content. The color histogram method gathers statistics about the proportion of each color as the signature of an image. In our work, the colors of an image are represented in the HSV (Hue/ Saturation/ Value) space, which is believed closer to human perception than other models, such as RGB (Red/ Green/ Blue) or CMY (Cyan/ Magenta/ Yellow). We quantize the HSV space into 18 hues, 2 saturations, and 4 values, with additional 4 levels of gray values; as a result, there are a total of 148 (i.e., 18×2×4+4) bins. Let C (|C| = m) a set of colors (i.e., 148 bins), P I (Q I ) is represented as Eq. (3), which models the color histogram H(P I ) (H(Q I )) as a vector, in which each bucket counts the ratio of pixels of</p><formula xml:id="formula_2" coords="3,95.76,166.76,424.84,39.46">P i c h I (Q I ) in color c i . &gt; =&lt; ) ( ),..., ( 1 I c I c I P h P h P m , &gt; =&lt; ) ( ),..., ( 1 I c I c I Q h Q h Q m (3)</formula><p>In many previous studies, each pixel is only assigned a single color. Consider the following situation: I 1 , I 2 are two images, all pixels of I 1 and I 2 fall into c i and c i+1 respectively; I 1 and I 2 are indeed similar to each other, but the similarity computed by the color histogram will regard them as different images. To address the problem, we set an interval range δ to extend the color of each pixel and introduce the idea of a partial pixel as shown in Eq.( <ref type="formula" coords="3,86.86,256.89,3.98,9.05">4</ref> </p><formula xml:id="formula_3" coords="3,453.36,347.79,71.03,19.91">| ) 2 ( | p p - - ,</formula><p>respectively. It is clear that a pixel has its contributions not only to c i but also to its neighboring bins.</p><p>Using the modified color histogram, the similarity of two color images q and d is defined in Eq. ( <ref type="formula" coords="3,460.06,401.86,3.62,9.05">5</ref>):</p><p>. ) (</p><formula xml:id="formula_4" coords="3,155.63,423.59,221.82,48.78">)) ( ), ( min( | H(q) | H(d) H(q) H(d)) (q), SIMcolor(H 1 1 ∑ ∑ = = = ∩ = n i i n i i i q h d h q h</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">color/gray feature</head><p>The medical image collection of the ImageCLEF 2005 evaluation contains gray and color images. In color images, users are usually attracted by the change of colors more than the positions of objects. Thus, the effective feature in query a color image is different from query a gray image. The image have an obvious feature is whether the image is color or gray value image. When the user queries an image by example, the system first determines whether the example is color or grayscale. We calculate the color histogram, if the four bins of gray values occupy more than 80% of the whole image, we decide that the query image is gray; otherwise it is color. If the input is a color image, then we set the weight parameter denoted by "C"; if the query image is detected gray valued image, we use the weight parameter denote by "G" as show in Table <ref type="table" coords="3,396.12,573.26,5.03,9.05" target="#tab_1">1</ref> and<ref type="table" coords="3,420.39,573.26,28.30,9.04" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Textual Vector Representation</head><p>In the ImageCLEFmed collections of annotations are in English, French and German. The overall multilingual search process is show in Fig. <ref type="figure" coords="3,194.89,619.29,3.76,9.05" target="#fig_1">1</ref>. Given an initial query Q, the system performs the cross-language retrieval, and returns a set of relevant documents to user. We use the representation expressing a query as a vector in the vector space model <ref type="bibr" coords="3,123.34,642.27,40.76,9.04" target="#b11">[Salton83]</ref>. The Textual Vector Representation is defined as following. Let W (|W| = n) be the set of significant keywords in the corpus. For a document D, its textual vector representation (i.e., D T ) is defined as Eq. (6), </p><formula xml:id="formula_5" coords="4,226.61,243.70,269.63,15.32">&gt; =&lt; ) ( ),..., ( 1 T t T t T D w D w D n (6)</formula><p>where the n dimensions indicate the weighting of a keyword t i in D T , which is measured by TF-IDF <ref type="bibr" coords="4,479.78,263.02,40.65,9.05" target="#b11">[Salton83]</ref>, as computed in Eq.(7);</p><formula xml:id="formula_6" coords="4,229.75,287.41,266.44,28.50">i T i i t D t T t n N tf tf D w log max ) ( , × =<label>(7)</label></formula><p>In Eq.( <ref type="formula" coords="4,100.53,329.74,3.97,9.05" target="#formula_6">7</ref>), tf tf T i D t max , stands for the normalized frequency of t i in D T , max tf is the maximum number of occurrences of any keyword in D T , N indicates the number of documents in the corpus, and denotes the number of documents in whose caption t i t n i appears. In the above, we introduce the textual vector representation for documents. As for q query Q, one problem is that since Q T is given in English, it is necessary to translate Q T into French and German, which is the languages used in the document collection. A short query usually cannot cover as many useful search terms as possible because of the lack of sufficient words. We perform the query expansion process to add new terms to the original query. The additional search terms is taken from a thesaurus -WordNet <ref type="bibr" coords="4,245.75,437.97,40.13,9.05">[Miller95]</ref>. For each expansion English term, it is then translated into one or several corresponding French and German words by looking it up in a dictionary 1 . Assume is the set of all English words obtained after query expansion and query translation, it is obvious that may contain a lot of words which are not correct translations or useful search terms. To resolve the translation ambiguity problem, we define word co-occurrence relationships to determine final query terms. </p><p>After giving the definition of , for a query Q, its textual vector representation (i.e., Q</p><formula xml:id="formula_8" coords="4,78.12,583.17,420.64,47.56">) ( T Q biguity AfterDisam T ) is defined in Eq. (9), &gt; =&lt; ) ( ),..., ( 1 T t T t T Q w Q w Q n (9)</formula><p>where is the weighting of a keyword t ) ( T t Q w i i in Q T , which is measured as Eq.</p><p>(10), indicates whether there exists an .</p><formula xml:id="formula_9" coords="4,97.08,677.42,290.00,15.34">) ( T c Q w i ) ( T j Q biguity AfterDisam e ∈</formula><p>In Eq. 1 http://www.freelang.net/ (10), W is the set of significant keywords as defined before,</p><formula xml:id="formula_10" coords="5,324.96,74.07,24.03,24.76">tf tf T i Q t max ,</formula><p>stands for the normalized frequency of t i in , maxtf is the maximum number of occurrences of any keyword in , N indicates the number of images in the corpus, and denotes the number of images in whose caption t</p><formula xml:id="formula_11" coords="5,73.56,104.13,430.23,75.78">) ( T Q biguity AfterDisam ) ( T Q biguity AfterDisam i t n i appears. ⎪ ⎩ ⎪ ⎨ ⎧ × = i T i i t Q t T t n N tf tf Q w log max ) ( ,<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Submissions to the ImageCLEF 2005 Evaluation</head><p>The entire ImageCLEFmed library consists of multiple collections (e.g., Casimage, PEIR, MIR, PathoPIC). Each collection is organized into cases that represent a group of related images and annotations. Each case consists of a group of images and an optional annotation. Each image is part of a case and has optional associated annotations, which consist metadata (e.g., HEAL tagging), and/or a textual annotation. In the practice for medical images retrieval system, the doctor query by semantic sentence that is more convenience. After textual query, the system retrieves related images by annotation for user to browse. The user can further more rank the images in visually. Combine the textual and visually features will help the user find the desire image more precise. In ImageCLEF 2005, medical image retrieval task contains 25 queries for evaluation, the queries mixed visual image and semantic textual to retrieve desire images. The visually queries use image example to find similar images, each topic contain at least one image example. The semantic textual queries allow user query by a sentence, which some semantic concept are hard to derive from images directly. The goal of this task is to examine how the visual feature can improve the query result. All submissions of participants in this task were classified into automatic runs and manual runs. The automatic runs means that the system at the query process without human manual intervened. In the automatic category, the methods can be classified into three sub-categories: Text only, Visual only and Mixed retrieval (visual and textual) according to the feature used. The category "Text only" means that systems use textual feature only to retrieve relevant images. "Visual only" category means that systems only use visual image feature without combine textual annotation to retrieve similar images. Mixed retrieval means the systems combine the visual and textual feature to retrieve images. In this task, we have submitted ten runs for the mixed retrieval of automatic runs and six runs for the visual only of automatic runs. In the content-based approach, we combine four proposed image features by weighted adjusting to retrieve related images. The weight of features we set at the system initial and do not have any further user intervention while query is processing. Table <ref type="table" coords="5,313.31,475.73,5.01,9.02" target="#tab_1">1</ref> lists the query result of visual only runs and the setting weight of four image features. Table <ref type="table" coords="5,254.66,487.25,5.01,9.02" target="#tab_2">2</ref> lists the result of mixed retrieval runs and the setting weight of image features and textual features. The different of each runs is the weighted setting of features. The query topics contain color and gray images. We first examine the queries image is color or gray image by color/gray feature. According to the image is color or gray set different weight for image features. In the Table <ref type="table" coords="5,70.93,533.22,3.76,9.02" target="#tab_1">1</ref>, "C" denotes that query image is color image and "G" denotes that query image is gray image. We submit six runs for visual only category. The run, "nctu_visual_auto_a8", has the better result in our experiment. The weight of each feature are set equal, it means that four image features have the same importance. The result also shows that visual only approach has a bottleneck because the query topics contain semantic queries. The setting weights of mixed runs and results are listed in the table 2. The result of run8, run9 and run10 illustrate that combine the visual and textual feature will get better results than single features. Run8 assume that the significant of visual and textual feature are equal. Run9 emphasizes the weight of visual features and Run10 emphasizes the weight of textual features. The result shows that text-based approach is better than content-based approach, but the content-based approach can improve the textual result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and future work</head><p>ImageCLEF 2005 medical image retrieval task offers a good test platform to evaluate the ability of image retrieval technologies. There are totally 112 runs submitted for this task. The results of the evaluation show that the method we proposed is excellent. Our best result rank by MAP is 3, there are one better system than us.</p><p>In the experiment result, we find that content-based approach retrieving similar images rely on visual feature, which has less semantic expansion. The text-based approach has better performance than content-based approach.</p><p>Combine the textual and visual features will get best result.</p><p>The results in the medical retrieval task show that weighted setting between the features is very important. The variation between different settings of weight is extreme. Suitable weight adjusting will improves the results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,90.84,256.89,7.96,9.05;3,305.22,297.19,2.00,9.00;3,290.40,297.19,2.00,9.00;3,321.78,269.16,2.00,9.00;3,284.70,269.16,2.00,9.00;3,258.60,289.26,3.33,9.00;3,246.06,289.26,3.33,9.00;3,299.88,303.06,1.67,4.50;3,279.60,287.46,3.06,4.50;3,274.14,287.46,2.50,4.50;3,316.02,275.10,2.50,4.50;3,296.04,275.10,2.50,4.50;3,255.00,295.20,1.67,4.50;3,241.20,295.20,2.22,4.50;3,294.84,297.19,6.11,9.00;3,249.96,289.26,6.11,9.00;3,236.46,289.26,5.00,9.00;3,282.00,290.09,1.00,2.70;3,243.30,297.83,0.83,2.70;3,274.38,272.69,8.55,14.69;3,276.30,285.85,3.56,6.12;3,301.68,265.95,5.48,12.25;3,264.42,286.05,5.48,12.25;3,300.84,280.49,4.94,12.94;3,309.06,265.37,5.48,12.94;3,288.06,265.37,6.30,12.94;3,490.38,279.88,11.68,9.05;3,70.92,311.44,453.51,9.77;3,127.50,330.73,3.33,9.00;3,121.02,338.65,5.00,9.00;3,100.08,330.73,2.50,9.00;3,92.88,338.65,5.00,9.00;3,72.00,330.73,3.33,9.00;3,119.94,320.58,4.94,12.94;3,91.80,320.58,4.94,12.94;3,111.96,327.51,5.48,12.24;3,84.00,327.51,5.48,12.24;3,104.82,330.73,5.00,9.00;3,76.86,330.73,5.00,9.00;3,137.04,327.46,190.27,12.30;3,362.40,330.72,3.33,9.01;3,347.70,330.72,2.50,9.01;3,333.30,330.72,3.33,9.01;3,358.44,336.66,2.50,4.51;3,343.92,336.66,2.50,4.51;3,351.60,326.92,5.49,12.95;3,336.06,326.92,6.31,12.95;3,366.90,330.70,91.46,9.05;3,519.90,330.72,3.33,9.01;3,513.42,338.64,5.01,9.01;3,492.49,330.72,2.50,9.01;3,485.29,338.64,5.01,9.01;3,464.41,330.72,3.33,9.01;3,512.34,320.56,4.95,12.95;3,484.19,320.56,4.95,12.95;3,504.36,327.51,5.48,12.24;3,476.40,327.51,5.48,12.24;3,497.22,330.73,5.00,9.00;3,469.26,330.73,5.00,9.00;3,70.92,358.65,305.29,9.77;3,399.96,362.79,4.93,12.90;3,408.11,347.37,5.46,12.90;3,387.35,347.37,6.28,12.90;3,420.72,351.12,2.00,9.01;3,384.06,351.12,2.00,9.01;3,415.02,357.07,2.49,4.49;3,395.16,357.07,2.49,4.49;3,400.74,347.91,5.48,12.24;3,431.16,358.66,14.47,9.05;3,482.28,362.64,5.00,13.08;3,503.16,347.22,6.37,13.08;3,475.98,347.22,5.00,13.08"><head></head><label></label><figDesc>c i-1 , c i , and c i+1 stand for a color bin, a solid line indicate the boundary of c i , p is the value of a pixel, c i . The contributions of the pixel to c i and c i-1 are computed as δ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,70.92,223.00,243.86,9.04"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Text based multilingual query translation flowchart</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,70.92,458.60,453.59,110.03"><head></head><label></label><figDesc>If the co-occurrence frequency of e and e j in the corpus is greater than a predefined threshold, both e i and e j are regarded as useful search terms.</figDesc><table coords="4,70.92,458.60,366.99,110.03"><row><cell>sion AfterExpan</cell><cell>( Q T</cell><cell>)</cell><cell>=</cell><cell>{ e 1</cell><cell cols="2">,...,</cell><cell cols="2">h e</cell><cell>}</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">sion AfterExpan</cell><cell>( T Q</cell><cell>)</cell></row><row><cell cols="6">So far, we have a set of search terms,</cell><cell cols="8">AfterDisam</cell><cell cols="2">biguity</cell><cell>( T Q</cell><cell>)</cell><cell>, which is presented as Eq.(8),</cell></row><row><cell cols="8">( Q T biguity AfterDisam</cell><cell>)</cell><cell>=</cell><cell>{ e i</cell><cell>,</cell><cell>e</cell><cell>j</cell><cell>|</cell><cell>i e</cell><cell>,</cell><cell>e</cell><cell>j</cell><cell>∈</cell><cell>( Q T lation AfterTrans</cell><cell>)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">&amp;</cell><cell>i e</cell><cell>,</cell><cell>e</cell><cell>j</cell><cell>have</cell><cell>a</cell><cell>t significan</cell><cell>co</cell><cell>-</cell><cell>} occurrence</cell></row></table><note coords="4,399.00,506.41,1.80,5.83"><p>i</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,95.94,592.25,390.53,147.56"><head>Table 1 :</head><label>1</label><figDesc>The query result of visual only runs and the weight of visual image features</figDesc><table coords="5,103.20,603.87,383.27,135.94"><row><cell>Submission runs</cell><cell></cell><cell></cell><cell cols="4">The weight of Image features</cell><cell></cell><cell></cell><cell cols="2">Result</cell></row><row><cell>Image features</cell><cell cols="2">Coherence</cell><cell cols="2">Gray HIS</cell><cell cols="2">Color HIS</cell><cell cols="2">Facade</cell><cell>MAP</cell><cell>Rank of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>runs</cell></row><row><cell>detected color or gray</cell><cell>C</cell><cell>G</cell><cell>C</cell><cell>G</cell><cell>C</cell><cell>G</cell><cell>C</cell><cell>G</cell><cell></cell><cell></cell></row><row><cell>nctu_visual_auto_a1</cell><cell>0.3</cell><cell>0.2</cell><cell>0.3</cell><cell>0.5</cell><cell>1</cell><cell>0.2</cell><cell>1</cell><cell>1</cell><cell>0.0628</cell><cell>14</cell></row><row><cell>nctu_visual_auto_a2</cell><cell>0.3</cell><cell>0.2</cell><cell>0.5</cell><cell>0.3</cell><cell>0.3</cell><cell>0.5</cell><cell>1</cell><cell>1</cell><cell>0.0649</cell><cell>10</cell></row><row><cell>nctu_visual_auto_a3</cell><cell>0.5</cell><cell>1</cell><cell>0.5</cell><cell>1</cell><cell>1</cell><cell>0.5</cell><cell>1</cell><cell>1</cell><cell>0.0661</cell><cell>8</cell></row><row><cell>nctu_visual_auto_a5</cell><cell>0.1</cell><cell>0.2</cell><cell>0.1</cell><cell>0.5</cell><cell>1</cell><cell>0.5</cell><cell>0.5</cell><cell>1</cell><cell>0.0631</cell><cell>13</cell></row><row><cell>nctu_visual_auto_a7</cell><cell>0.3</cell><cell>0.2</cell><cell>0.3</cell><cell>0.5</cell><cell>1</cell><cell>0.2</cell><cell>1</cell><cell>0.5</cell><cell>0.0644</cell><cell>11</cell></row><row><cell>nctu_visual_auto_a8</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0.0672</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,70.92,119.21,441.58,171.33"><head>Table 2 :</head><label>2</label><figDesc>The result of mixed retrieval runs and the weight of visual image features and textual features</figDesc><table coords="6,78.06,130.69,434.44,159.86"><row><cell>Submission runs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">The weight of Image features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Image features</cell><cell cols="2">Coherenc</cell><cell cols="2">Gray HIS</cell><cell cols="2">Color HIS</cell><cell cols="2">Facade</cell><cell cols="2">visual</cell><cell cols="2">textual</cell><cell>MAP</cell><cell>Rank</cell></row><row><cell></cell><cell>e</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Detected color or gray</cell><cell>C</cell><cell>G</cell><cell>C</cell><cell>G</cell><cell>C</cell><cell>G</cell><cell>C</cell><cell>G</cell><cell>C</cell><cell>G</cell><cell>C</cell><cell>G</cell><cell></cell><cell></cell></row><row><cell>nctu_visual+Text_auto_1</cell><cell cols="3">0.3 0.2 0.3</cell><cell>0.5</cell><cell>1</cell><cell>0.2</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell cols="2">0.8 0.1</cell><cell>0.2276</cell><cell>10</cell></row><row><cell>nctu_visual+Text_auto_2</cell><cell cols="3">0.3 0.2 0.5</cell><cell>0.3</cell><cell>0.3</cell><cell>0.5</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell cols="2">0.8 0.1</cell><cell>0.2127</cell><cell>14</cell></row><row><cell>nctu_visual+Text_auto_3</cell><cell>0.5</cell><cell>1</cell><cell>0.5</cell><cell>1</cell><cell>1</cell><cell>0.5</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell cols="2">0.8 0.1</cell><cell>0.2286</cell><cell>9</cell></row><row><cell>nctu_visual+Text_auto_4</cell><cell cols="3">0.3 0.2 0.3</cell><cell>0.5</cell><cell>1</cell><cell>0.2</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0.2</cell><cell>0.2389</cell><cell>3</cell></row><row><cell>nctu_visual+Text_auto_5</cell><cell cols="3">0.1 0.2 0.1</cell><cell>0.5</cell><cell>1</cell><cell>0.5</cell><cell>0.5</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell cols="2">0.8 0.1</cell><cell>0.2246</cell><cell>12</cell></row><row><cell>nctu_visual+Text_auto_6</cell><cell cols="3">0.3 0.2 0.3</cell><cell>0.5</cell><cell>1</cell><cell>0.2</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0.2318</cell><cell>7</cell></row><row><cell>nctu_visual+Text_auto_7</cell><cell cols="3">0.3 0.2 0.3</cell><cell>0.5</cell><cell>1</cell><cell>0.2</cell><cell>1</cell><cell>0.5</cell><cell>1</cell><cell>1</cell><cell cols="2">0.8 0.1</cell><cell>0.2265</cell><cell>11</cell></row><row><cell>nctu_visual+Text_auto_8</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0.2324</cell><cell>6</cell></row><row><cell>nctu_visual+Text_auto_9</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell cols="2">0.1 0.1</cell><cell>0.0906</cell><cell>22</cell></row><row><cell>nctu_visual+Text_auto_10</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0.1</cell><cell>0.1</cell><cell>1</cell><cell>1</cell><cell>0.1941</cell><cell>15</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,133.49,432.60,390.92,9.02;6,70.92,444.06,453.41,9.02;6,70.92,455.57,123.85,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,268.27,432.60,251.92,9.02">Texture features for browsing and retrieval of large image data</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,70.92,444.06,415.63,9.02">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="837" to="842" />
			<date type="published" when="1996-08">August 1996</date>
		</imprint>
	</monogr>
	<note>Special Issue on Digital Libraries)</note>
</biblStruct>

<biblStruct coords="6,123.56,467.09,403.48,9.02;6,70.92,478.56,456.88,9.02;6,70.92,490.07,65.91,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,210.46,478.56,219.22,9.02">Query by Image and Video Content: The QBIC system</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ashley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gorkani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Petkovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Steele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,436.62,478.56,64.32,9.02">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="23" to="32" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,121.70,501.59,402.73,9.02;6,70.92,513.06,453.52,9.02;6,70.93,524.57,453.48,9.02;6,70.93,536.09,270.37,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,415.02,501.59,109.41,9.02;6,70.92,513.06,168.85,9.02">Blobworld: A system for region-based image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,446.59,513.06,77.85,9.02;6,70.93,524.57,241.44,9.02">Third International Conference On Visual Information Systems (VISUAL&apos; 99)</title>
		<title level="s" coord="6,372.90,524.57,147.31,9.02">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huijsmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</editor>
		<meeting><address><addrLine>Springer{Verlag, Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="509" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,126.38,547.55,398.11,9.02;6,70.92,559.07,453.52,9.02;6,70.92,570.58,257.55,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,321.64,547.55,202.85,9.02;6,70.92,559.07,231.99,9.02">Color and texture based image segmentation using EM and its application to content-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,325.14,559.07,199.30,9.02;6,70.92,570.58,110.24,9.02">Proceedings of the International Conference on Computer Vision (ICCV&apos;98)</title>
		<meeting>the International Conference on Computer Vision (ICCV&apos;98)<address><addrLine>Bombay, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="675" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,118.91,582.05,405.50,9.02;6,70.92,593.57,453.51,9.02;6,70.92,605.03,394.68,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,343.17,582.05,181.24,9.02;6,70.92,593.57,434.81,9.02">Content-Based Query of Image Databases, Inspirations from Text Retrieval: Inverted Files, Frequency-Based Weights and Relevance Feedback</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Raki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,70.92,605.03,177.87,9.02">Scandinavian Conference on Image Analysis</title>
		<meeting><address><addrLine>Kangerlussuaq, Greenland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="143" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,116.00,616.55,408.39,9.02;6,70.92,628.07,453.51,9.02;6,70.92,639.53,35.91,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,284.41,616.55,239.99,9.02;6,70.92,628.07,66.21,9.02">SIMPLIcity: Semantics-Sensitive Integrated Matching for Picture Libraries</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wiederhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,144.18,628.07,261.84,9.02">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="947" to="963" />
			<date type="published" when="2001-09">September 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,106.88,651.05,417.52,9.02;6,70.92,662.57,22.58,9.02" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,203.06,651.05,156.73,9.02">Data Mining: Concepts and Techniques</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Academic Press</publisher>
			<pubPlace>San Diego, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,123.61,674.03,400.82,9.02;6,70.92,685.55,453.51,9.02;6,70.92,697.06,50.90,9.02" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,340.53,674.03,183.90,9.02;6,70.92,685.55,88.33,9.02">Adaptation in Statistical Pattern Recognition using Tangent Vectors</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dahmen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,166.39,685.55,262.30,9.02">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="269" to="274" />
			<date type="published" when="2004-02">February 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,115.85,708.53,408.53,9.02;6,70.92,720.05,63.41,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,248.54,708.53,59.95,9.02">Color Indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,320.22,708.53,168.59,9.02">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,118.75,731.57,405.73,9.02;6,70.92,743.04,418.75,9.02" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,324.20,731.57,200.28,9.02;6,70.92,743.04,80.91,9.02">KIDS&apos;s evaluation in medical image retrieval task at ImageCLEF 2004</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,163.68,743.04,181.99,9.02">Working Notes for the CLEF 2004 Workshop</title>
		<meeting><address><addrLine>Bath,UK</addrLine></address></meeting>
		<imprint>
			<date>September</date>
			<biblScope unit="page" from="585" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,113.51,73.26,411.00,9.02;7,70.92,84.71,291.40,9.02" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,255.47,73.26,201.98,9.02">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,474.42,73.26,50.09,9.02;7,70.92,84.71,262.05,9.02">Proceedings of the Fifth Annual Workshop on Computational Learning Theory</title>
		<meeting>the Fifth Annual Workshop on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,115.78,96.23,408.65,9.02;7,70.92,107.75,22.58,9.02" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="7,230.46,96.23,183.01,9.02">Introduction to Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
