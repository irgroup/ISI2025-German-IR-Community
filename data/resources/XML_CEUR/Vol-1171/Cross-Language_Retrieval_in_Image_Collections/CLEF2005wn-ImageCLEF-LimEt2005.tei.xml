<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.68,149.24,311.82,15.08;1,138.96,171.32,325.34,15.08">A Structured Learning Approach for Medical Image Indexing and Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,207.60,205.03,62.77,8.78"><forename type="first">Joo-Hwee</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,297.37,205.03,93.64,8.78"><forename type="first">Jean-Pierre</forename><surname>Chevallet</surname></persName>
							<email>jean-pierre.chevallet@imag.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">IPAL-CNRS</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.68,149.24,311.82,15.08;1,138.96,171.32,325.34,15.08">A Structured Learning Approach for Medical Image Indexing and Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">994A696C65D56791650236A24F050B94</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2 [Database Managment]: H.2.3 Languages-Query Languages</term>
					<term>I.4 [Image Processing and Computer Vision]: I.4.10 Image Representation-Statistical</term>
					<term>J.3 [Life and Medical Sciences]: Medical Information Systems Algorithms, Design, Experimentation, Languages, Performance Medical Images, Visual Ontology, Similarity-Based Retrieval, Semantics-Based Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical images are critical assets for medical diagnosis, research, and teaching. To facilitate automatic indexing and retrieval of large medical image databases, we propose a structured framework for designing and learning vocabularies of meaningful medical terms with associated visual appearance from image samples. These VisMed terms span a new feature space to represent medical image contents. After a multi-scale detection process, a medical image is indexed as compact spatial distributions of VisMed terms.</p><p>When queries are in the form of example images, both a query image and a database image can be matched based on their distributions of VisMed terms, much like the matching of feature-based histograms though the bins refer to semantic medical terms. In addition, a flexible tiling (FlexiTile) matching scheme has been proposed to compare the similarity between two medical images of arbitrary aspect ratios. This matching scheme supports similarity-based retrieval with visual queries. The ranked list of such retrieval is denoted as "i2r-vk-sim.txt" in our submission to ImageCLEF 2005.</p><p>When a query is expressed as a text description that involves modality, anatomy, and pathology etc, it can be translated into a visual query representation that chains the presences of VisMed terms with spatial significance via logical operators (AND, OR, NOT) and spatial quantifiers for automatic query processing based on the VisMed image indexes. This query formulation and processing scheme allows semantics-based retrieval with text queries. The ranked list of such retrieval is denoted as "i2r-vksem.txt" in our submission to ImageCLEF 2005.</p><p>By fusing the ranked lists from both the similarity-based and semantics-based retrievals, we can leverage on the information expressed in both visual and text queries respectively. The ranked list of such retrieval is denoted as "i2r-vk-avg.txt" in our submission to ImageCLEF 2005.</p><p>We apply the VisMed approach on the Medical Image Retrieval task of the Image-CLEF track under CLEF 2005. Based on 0.3% (i.e. 158 images) of the 50, 026 images from 4 collections plus 96 images obtained from the web, we cropped 1460 image regions to train and validate 39 VisMed terms using support vector machines. The Mean Average Precisions (MAP) over 25 query topics for the submissions "i2r-vk-sim.txt", "i2rvk-sem.txt", and "i2r-vk-avg.txt" are 0.0721, 0.06, and 0.0921 respectively, according to the evaluation results released by the ImageCLEF 2005 organizers. The submission "i2r-vk-avg.txt" is also combined with text-only submissions "IPALI2R Tn" and "IPALI2R T" to form submissions for mixed retrieval. The best MAP among these submissions for mixed retrieval is 0.2821 from submission "IPALI2R TIan".</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical images are an integral part in medical diagnosis, research, and teaching. Medical image analysis research has focused on image registration, measurement, and visualization. Although large amounts of medical images are produced in hospitals every day, there is relatively less research in medical content-based image retrieval (CBIR) <ref type="bibr" coords="2,303.60,361.75,14.70,8.78" target="#b15">[16]</ref>. Besides being valuable for medical research and training, medical CBIR systems also have a role to play in clinical diagnosis <ref type="bibr" coords="2,435.14,373.75,14.70,8.78" target="#b12">[13]</ref>. For instance, for less experienced radiologists, a common practice is to use a reference text to find images that are similar to the query image <ref type="bibr" coords="2,226.81,397.74,10.00,8.78" target="#b2">[3]</ref>. Hence, medical CBIR systems can assist doctors in diagnosis by retrieving images with known pathologies that are similar to a patient's image(s).</p><p>Among the limited research efforts of medical CBIR, classification or clustering driven feature selection and weighting has received much attention as general visual cues often fail to be discriminative enough to deal with more subtle, domain-specific differences and more objective ground truth in the form of disease categories is usually available <ref type="bibr" coords="2,344.18,457.50,10.45,8.78" target="#b2">[3,</ref><ref type="bibr" coords="2,358.11,457.50,11.62,8.78" target="#b10">11]</ref>.</p><p>In reality, pathology bearing regions tend to be highly localized <ref type="bibr" coords="2,383.58,469.50,10.00,8.78" target="#b2">[3]</ref>. Hence, local features such as those extracted from segmented dominant image regions approximated by best fitting ellipses have been proposed <ref type="bibr" coords="2,177.85,493.26,10.00,8.78" target="#b5">[6]</ref>. A hierarchical graph-based representation and matching scheme has been suggested to deal with multi-scale image decomposition and their spatial relationships <ref type="bibr" coords="2,471.85,505.26,10.00,8.78" target="#b5">[6]</ref>. However, it has been recognized that pathology bearing regions cannot be segmented out automatically for many medical domains <ref type="bibr" coords="2,206.40,529.26,14.71,8.78" target="#b15">[16]</ref>. As an alternative, a comprehensive set of 15 perceptual categories related to pathology bearing regions and their discriminative features are carefully designed and tuned for high-resolution CT lung images to achieve superior precision rates over a brute-force feature selection approach <ref type="bibr" coords="2,206.89,565.02,14.70,8.78" target="#b15">[16]</ref>.</p><p>Hence, it is desirable to have a medical CBIR system that represents images in terms of semantic local features, that can be learned from examples (rather than handcrafted with a lot of expert input) and do not rely on robust region segmentation. In order to manage large and complex set of visual entities (i.e. high content diversity) in the medical domain, we propose a structured learning framework to facilitate modular design and extraction of medical visual semantics, VisMed terms, in building content-based medical image retrieval systems (Section 2). VisMed terms are image regions that exhibit semantic meanings to medical practitioners and that can be learned statistically to span a new indexing space (Section 2.1). During image indexing, they are detected in image content, reconciled across multiple resolutions, and aggregated spatially to form local semantic histograms (Section 2.2).</p><p>The resulting compact and abstract VisMed image indexes can support both similarity-based query and semantics-based query efficiently, as we will describe how they are applied to Image-CLEF 2005 datasets in Section 3. When queries are in the form of example images, both a query image and a database image can be matched based on their distributions of VisMed terms, much like the matching of feature-based histograms though the bins refer to semantic medical terms. In addition, a flexible tiling (FlexiTile) matching scheme has been proposed to compare the similarity between two medical images of arbitrary aspect ratios (Section 3.1).</p><p>When a query is expressed as a text description that involves modality, anatomy, and pathology etc, they can be translated into a visual query representation that chains the presences of VisMed terms with spatial significance via logical operators (AND, OR, NOT) and spatial quantifiers for automatic query processing based on the VisMed image indexes. This query formulation and processing scheme allows semantics-based retrieval with text queries (Section 3.2). By fusing the ranked lists from both the similarity-based and semantics-based retrievals, we can leverage on the information expressed in both visual and text queries respectively (Section 3.3). The relevant ImageCLEF 2005 evaluation results will be discussed (Section 3.4) before conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning VisMed Terms for Image Indexing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning of VisMed Terms</head><p>VisMed terms are typical semantic tokens with visual appearance in medical images (e.g. Xraybone-fracture, CT-abdomen-liver, MRI-head-brain, photo-skin). They are defined using image region instances cropped from sample images and modeled and built based on statistical learning. In this paper, we have adopted color and texture features as well as support vector machines (SVMs) <ref type="bibr" coords="3,127.68,342.79,15.49,8.78" target="#b17">[18]</ref> for VisMed term representation and learning respectively though the framework is not dependent on a particular feature and classifier. The notion of using a visual vocabulary to represent and index image contents for more effective (i.e. semantic) query and retrieval has been proposed and applied to consumer images <ref type="bibr" coords="3,276.00,378.54,10.45,8.78" target="#b6">[7,</ref><ref type="bibr" coords="3,289.68,378.54,11.81,8.78" target="#b9">10]</ref>.</p><p>To compute VisMed terms from training instances, we use SVMs on color and texture features for an image region and denote this feature vector as z. A SVM S k is a detector for VisMed term k on z. The classification vector T for region z is computed via the softmax function <ref type="bibr" coords="3,463.72,414.31,10.69,8.78" target="#b0">[1]</ref> as</p><formula xml:id="formula_0" coords="3,254.88,441.67,34.15,9.92">T k (z) =</formula><p>exp Sk (z) j exp Sj(z) .</p><p>(</p><p>i.e. T k (z) corresponds to a VisMed entry in the 39-dimensional vector T adopted in this paper.</p><p>In our experiments, we use the YIQ color space over other color spaces (e.g. RGB, HSV, LUV) as it performed better in our experiments. For the texture feature, we adopted the Gabor coefficients which have been shown to provide excellent pattern retrieval results <ref type="bibr" coords="3,440.42,505.75,14.70,8.78" target="#b11">[12]</ref>.</p><p>A feature vector z has two parts, namely, a color feature vector z c and a texture feature vector z t . We compute the mean and standard deviation of each YIQ color channel and the Gabor coefficients (5 scales, 6 orientations) respectively <ref type="bibr" coords="3,310.55,541.75,14.70,8.78" target="#b9">[10]</ref>. Hence the color feature vector z c has 6 dimensions and the texture feature vector z t has 60 dimensions. Zero-mean normalization <ref type="bibr" coords="3,497.50,553.51,15.50,8.78" target="#b14">[15]</ref> was applied to both the color and texture features. In our evaluation described below, we adopted RBF kernels with modified city-block distance between feature vectors y and z,</p><formula xml:id="formula_2" coords="3,229.20,594.37,283.72,25.69">|y -z| = 1 2 ( |y c -z c | N c + |y t -z t | N t )<label>( 2 )</label></formula><p>where N c and N t are the numbers of dimensions of the color and texture feature vectors (i.e. 6 and 60) respectively. This just-in-time feature fusion within the kernel combines the contribution of color and texture features equally. It is simpler and more effective than other feature fusion methods that we have attempted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Indexing based on VisMed Terms</head><p>After learning, the VisMed terms are detected during image indexing from multi-scale block-based image patches without region segmentation to form semantic local histograms as described below.</p><p>Conceptually, the indexing is realized in a three-layer visual information processing architecture (Figure <ref type="figure" coords="3,125.32,745.99,3.89,8.78" target="#fig_1">1</ref>). The bottom layer denotes the pixel-feature maps computed for feature extraction. In our experiments, there are 3 color maps (i.e. YIQ channels) and 30 texture maps (i.e. Gabor coefficients of 5 scales and 6 orientations). From these maps, feature vectors z c and z t compatible with those adopted for VisMed term learning (Equation ( <ref type="formula" coords="4,341.85,136.39,4.15,8.78" target="#formula_2">2</ref>)) are extracted.  To detect VisMed terms with translation and scale invariance in an image to be indexed, the image is scanned with windows of different scales, similar to the strategy in view-based object detection <ref type="bibr" coords="4,133.92,411.19,15.73,8.78" target="#b16">[17,</ref><ref type="bibr" coords="4,153.60,411.19,11.80,8.78" target="#b13">14]</ref>. More precisely, given an image I with resolution M × N , the middle layer, Reconciled Detection Map (RDM), has a lower resolution of P × Q, P ≤ M, Q ≤ N . Each pixel (p, q) in RDM corresponds to a two-dimensional region of size r x × r y in I. We further allow tessellation displacements d x , d y &gt; 0 in X, Y directions respectively such that adjacent pixels in RDM along X direction (along Y direction) have receptive fields in I which are displaced by d x pixels along X direction (d y pixels along Y direction) in I. At the end of scanning an image, each pixel (p, q) that covers a region z in the pixel-feature layer will consolidate the classification vector T k (z) (Equation ( <ref type="formula" coords="4,167.57,494.95,3.87,8.78" target="#formula_1">1</ref>)).</p><p>In our experiments, we progressively increase the window size r x × r y from 20 × 20 to 60 × 60 at a displacement (d x , d y ) of (10, 10) pixels, on an image whose longer side is fixed at 360 pixels after a size normalization step that preserves the aspect ratio. After the detection step, we have 5 maps of detection of dimensions 23 × 35 to 19 × 31, which are reconciled into a common RDM as explained below.</p><p>To reconcile the detection maps across different resolutions onto a common basis, we adopt the following principle: If the most confident classification of a region at resolution r is less than that of a larger region (at resolution r + 1) that subsumes the region, then the classification output of the region should be replaced by those of the larger region at resolution r + 1. For instance, if the detection of a face is more confident than that of a building at the nose region (assuming that both face and building (but not nose) are in the visual vocabulary designed for a particular application), then the entire region covered by the face, which subsumes the nose region, should be labeled as face.</p><p>Using this principle, we compare detection maps of two consecutive resolutions at a time, in descending window sizes (i.e. from windows of 60 × 60 and 50 × 50 to windows of 30 × 30 and 20 × 20). After 4 cycles of reconciliation, the detection map that is based on the smallest scan window (20 × 20) would have consolidated the detection decisions obtained at other resolutions for further spatial aggregation.</p><p>The purpose of spatial aggregation is to summarize the reconciled detection outcome in a larger spatial region. Suppose a region Z comprises of n small equal regions with feature vectors z 1 , z 2 , • • • , z n respectively. To account for the size of detected VisMed terms in the spatial area Z, the classification vectors of the reconciled detection map are aggregated as</p><formula xml:id="formula_3" coords="5,254.16,130.87,258.76,26.96">T k (Z) = 1 n i T k (z i ).<label>(3)</label></formula><p>This is the top layer in our three-layer visual information processing architecture where a Spatial Aggregation Map (SAM) further tessellates over RDM with A × B, A ≤ P, B ≤ Q pixels. This form of spatial aggregation does not encode spatial relation explicity. But the design flexibility of s x , s y in SAM on RDM (the equivalent of r x , r y in RDM on I) allows us to specify the location and extent in the content to be focused and indexed. We can choose to ignore unimportant areas (e.g. margins) and emphasize certain areas with overlapping tessellation. We can even have different weights attached to the areas during similarity matching.</p><p>To facilitate spatial aggregation and matching of image with different aspect ratios ρ, we design 5 tiling templates for Eq. ( <ref type="formula" coords="5,205.74,262.86,3.87,8.78" target="#formula_3">3</ref>), namely 3 × 3 × 2, 3 × 3, 2 × 3, and 1 × 3 grids resulting in 3, 6, 9, 6, and 3 T k (Z) vectors per image respectively. Since the tiling templates have aspect ratios of 3, 1.5, and 1, the decision thresholds to assign a template for an image are set to their mid-points (2.25 and 1.25) as ρ &gt; 2.25, 1.25 &lt; ρ ≤ 2.25, and ρ ≤ 1.25 respectively based on ρ = L S where L and S refer to the longer and shorter sides of an image respectively. For more details on detection-based indexing, readers are referred to <ref type="bibr" coords="5,233.07,322.63,14.70,8.78" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Medical Image Retrieval using VisMed Terms</head><p>As part of the Cross Language Evaluation Forum (CLEF), the ImageCLEF 2005 track <ref type="bibr" coords="5,480.29,377.35,10.45,8.78" target="#b1">[2]</ref> that promotes cross language image retrieval has a Medical Image Retrieval (MedIR) task in 2005, organized by Henning Mueller and William Hersh. The test collection contains images from the Casimage, MIR, PEIR, and PathoPIC datasets with a total of 50, 026 images. The collection contains annotations in XML format. The majority of the annotations are in English but a significant number is also in French and German, with a few cases that do not contain any annotation at all. The 25 queries for the MedIR task have been formulated with example images and short textual descriptions. The organizers evaluate retrieval performance in terms of uninterpolated Mean Average Precision (MAP) computed across all topics using trec eval.</p><p>We have applied the VisMed approach on the MedIR task. We set out to designed VisMed terms that correspond to typical semantic regions in the medical images. However due to time constraints, we only designed 39 VisMed terms relevant to the query topics. Table <ref type="table" coords="5,468.25,508.87,4.98,8.78" target="#tab_0">1</ref> lists the 39 VisMed terms (00-38) and Figure <ref type="figure" coords="5,254.41,520.87,4.98,8.78" target="#fig_2">2</ref> illustrates one visual example each for the VisMed terms from top-left (00) to bottom-right (38) in row-wise order. The last two VisMed terms in Table <ref type="table" coords="5,90.00,544.62,3.91,8.78" target="#tab_0">1</ref>, "image-region-bright" and "image-region-dark", refer to bright and dark patches in an image respectively. With a uniform VisMed framework, dark background in the scan images (e.g. CT, MRI) and bright (i.e. empty) areas in drawing etc are simply modeled as dummy terms instead of using image preprocessing to detect them separately.  using SVMs. As we would like to minimize the number of images selected from the test collection for VisMed term learning, we include relevant images available from the web. For a given VisMed term, the negative samples are the union of the positive samples of all the other 38 VisMed terms. We ensure that they do not contain any of the positive and negative query images given by the 25 query topics. The odd and even entries of the cropped regions are used as training and validation sets respectively (i.e. 730 each) to optimize the RBF kernel parameter of support vector machines. The best generalization performance with mean error 1.01% on the validation set was obtained with C = 100, α = 1.0 <ref type="bibr" coords="6,199.20,523.26,10.00,8.78" target="#b3">[4]</ref>. Both the training and validation sets are then combined to form a larger training set to retrain the 39 VisMed detectors. Both query and database images are indexed using the framework as described in the previous section (Eq. ( <ref type="formula" coords="6,404.35,547.26,4.25,8.78" target="#formula_1">1</ref>) to (3)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Similarity-Based Retrieval with Visual Query</head><p>Given two images represented as different grid patterns, we propose a flexible tiling (FlexiTile) matching scheme to cover all possible matches. For instance, given a query image Q of 3 × 1 grid and an image Z of 3 × 3 grid, intuitively Q should be compared to each of the 3 columns in Z and the highest similarity will be treated as the final matching score. As another example, consider matching a 3 × 2 grid with 2 × 3 grid. The 4 possible tiling and matching choices are shown in Figure <ref type="figure" coords="6,121.45,652.86,3.90,8.78" target="#fig_3">3</ref>.</p><p>The FlexiTile matching scheme is formalized as follows. Suppose a query image Q and a database image Z are represented as M 1 × N 1 and M 2 × N 2 grids respectively. The overlaping grid M × N where M = min(M 1 , M 2 ) and N = min(N 1 , N 2 ) is the maximal matching area. The similarity λ between Q and Z is the maximum matching among all possible M × N tilings,</p><formula xml:id="formula_4" coords="6,185.28,716.77,327.64,19.93">λ(Q, Z) = m1=u1,n1=v1 max m1=1,n1=1 m2=u2,n2=v2 max m2=1,n2=1 λ(Q m1,n1 , Z m2,n2 ),<label>(4)</label></formula><p>where </p><formula xml:id="formula_5" coords="6,118.80,745.25,313.38,10.65">u 1 = M 1 -M + 1, v 1 = N 1 -N + 1, u 2 = M 2 -M + 1, v 2 = N 2 -N + 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and the similarity</head><formula xml:id="formula_6" coords="7,194.88,249.19,313.78,24.35">λ(Q m1,n1 , Z m2,n2 ) = i j λ ij (Q m1,n1 , Z m2,n2 ) M × N , (<label>5</label></formula><formula xml:id="formula_7" coords="7,508.66,257.35,4.25,8.78">)</formula><p>and finally the similarity λ ij (Q m1,n1 , Z m2,n2 ) between two image blocks is computed based on L 1 distance measure (city block distance) as,</p><formula xml:id="formula_8" coords="7,178.08,314.95,330.58,27.20">λ ij (Q m1,n1 , Z m2,n2 ) = 1 - 1 2 k |T k (Q p1,q1 ) -T k (Z p2,q2 )| (<label>6</label></formula><formula xml:id="formula_9" coords="7,508.66,321.67,4.25,8.78">)</formula><p>where</p><formula xml:id="formula_10" coords="7,119.28,352.39,190.39,9.92">p 1 = m 1 + i, q 1 = n 1 + j, p 2 = m 2 + i, q 2 =</formula><p>n 2 + j and it is equivalent to color histogram intersection except that the bins have semantic interpretation as VisMed terms. However, to avoid spurious matching between very different grids (e.g. 3 × 1 and 1 × 3), we set the similarity to zero if the difference in a grid dimension between two image indexes is more than one. That is, two images are considered dissimilar if they exhibit very different aspect ratios.</p><p>There is a trade-off between content symmetry and spatial specificity. If we want images of similar semantics with different spatial arrangement (e.g. mirror images) to be treated as similar, we can have larger tessellated block in SAM (i.e. the extreme case is a global histogram). However in applications such as medical images where there is usually very small variance in views and spatial locations are considered differentiating across images, local histograms will provide good sensitivity to spatial specificity. Furthermore, we can attach different weights to the blocks to emphasize the focus of attention (e.g. center) if necessary. In this paper, we report experimental results based on even weights as grid tessellation is used. Now we extend the similarity matching for multiple query images. Let us denote</p><formula xml:id="formula_11" coords="7,90.00,505.81,422.95,25.21">Q + = {Q + 1 , Q + 2 , • • • , Q + p } and Q -= {Q - 1 , Q - 2 , • • • , Q - n }</formula><p>as the sets of positive and negative query images respectively and Q = Q + ∪ Q -. We define the similarity between a set of query images and a database image Z as the maximum similarity among similarities between each query image and Z i.e.</p><formula xml:id="formula_12" coords="7,235.20,574.69,277.72,17.53">λ(Q + , Z) = max i λ(Q + i , Z),<label>(7)</label></formula><formula xml:id="formula_13" coords="7,235.20,594.61,273.46,17.53">λ(Q -, Z) = max i λ(Q - i , Z). (<label>8</label></formula><formula xml:id="formula_14" coords="7,90.00,597.19,422.91,35.39">) If Q -= ∅, then λ(Q, Z) = λ(Q + , Z). Conversely, if Q + = ∅, then λ(Q, Z) = 1 -λ(Q -, Z).</formula><p>If Z is exactly one of the positive query images or negative query images, then λ(Q, Z) should be 1 or 0 respectively i.e. λ(Q</p><formula xml:id="formula_15" coords="7,187.44,645.01,325.48,44.23">+ , Z) = 1 or λ(Q -, Z) = 1 respectively. Otherwise, λ(Q, Z) = 1 2 (λ(Q + , Z) + (1 -λ(Q -, Z))) (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantics-Based Retrieval with Text Query</head><p>A new visual query language, Query by Spatial Icons (QBSI), has been proposed to combine pattern matching and logical inference <ref type="bibr" coords="7,261.16,733.99,14.70,8.78" target="#b9">[10]</ref>. A QBSI query is composed as a spatial arrangement of visual semantics. A Visual Query Term (VQT) P specifies a region R where a VisMed i should appear and a query formulus chains these terms up via logical operators. The truth value µ(P, Z) of a VQT P for any image Z is simply defined as</p><formula xml:id="formula_16" coords="8,266.40,143.59,246.76,9.92">µ(P, Z) = T i (R)<label>(10)</label></formula><p>where T i (R) is defined in Equation (3). As described in Section 2.2, the medical images are indexed as 3 ×1, 3 ×2, 3 ×3, 2 ×3, and 1 ×3 grids, depending on their aspect ratios. When a query involves the presence of a VisMed term in a region larger than a single block in a grid and its semantics prefers a larger area of presence of the VisMed term to have a good match (e.g. entire kidney, skin lesion, chest x-ray images with tuberculosis), Equation <ref type="bibr" coords="8,194.92,222.78,17.80,8.78" target="#b9">(10)</ref> will become</p><formula xml:id="formula_17" coords="8,246.96,241.03,261.75,24.83">µ(P, Z) = Zj∈R T i (Z j ) |R| (<label>11</label></formula><formula xml:id="formula_18" coords="8,508.71,249.91,4.45,8.78">)</formula><p>where Z j are the blocks in a grid that cover R and |R| denotes the number of such blocks. This corresponds to a spatial universal quantifier (∀).</p><p>On the other hand, if a query only requires the presence of a VisMed term within a region regardless of the area of the presence (e.g. presence of a bone fracture, presence of micro nodules), then the semantics is equivalent to the spatial existential quantifier (∃) and Equation (10) will be computed as</p><formula xml:id="formula_19" coords="8,253.68,346.62,259.48,14.96">µ(P, Z) = max Zj∈R T i (Z j )<label>(12)</label></formula><p>A QBSI query P can be specified as a disjunctive normal form of VQT (with or without negation),</p><formula xml:id="formula_20" coords="8,203.28,392.21,309.88,10.65">P = (P 11 ∧ P 12 ∧ • • •) ∨ • • • ∨ (P c1 ∧ P c2 ∧ • • •) (13)</formula><p>Then the query processing of query P for any image Z is to compute the truth value µ(P, Z) using appropriate logical operators. As uncertainty values are involved in VisMed term detection and indexing, we adopt fuzzy operations <ref type="bibr" coords="8,250.37,433.02,10.45,8.78" target="#b4">[5]</ref> as follows:</p><p>µ( P , Z) = 1µ(P, Z), (14) µ(P i ∧ P j , Z) = min(µ(P i , Z), µ(P j , Z)), (15) µ(P i ∨ P j , Z) = max(µ(P i , Z), µ(P j , Z)).</p><p>(</p><formula xml:id="formula_21" coords="8,499.81,482.23,13.35,8.78">)<label>16</label></formula><p>For the query processing of the query topics in ImageCLEF 2005, a query text description is manually translated into a QBSI query with the help of a visual query interface <ref type="bibr" coords="8,468.08,513.67,15.48,8.78" target="#b9">[10]</ref> which outputs an XML format to state the VisMed terms, the spatial regions, the Boolean operators, and the spatial quantifiers. As an illustration, query 02 "Show me x-ray images with fractures of the femur" is translated as "∀ xray-bone ∈ whole ∧ ∀ xray-pelvis ∈ upper ∧ ∃ xray-bone-fracture ∈ whole " where "whole" and "upper" refer to the whole image and upper part of an image respectively.</p><p>In fact, the VisMed terms can be further structured into an abstraction hierarchy, namely, IS-A hierarchy and Part-Whole hierarchy, to support more complex queries. Some possible examples of IS-A hierarchies are: a skin lesion can be either benign or malignant; different specific types of bone fracture belong to a common "bone fracture". A Part-Whole hierarchy allows us to detect (and query) a complex object in terms of its constituent parts. This is especially useful when a 3D object has no consistent shape representation in a 2D image. For more details about QBSI, please refer to <ref type="bibr" coords="8,154.56,656.94,14.70,8.78" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining Similarity-and Semantics-Based Retrieval</head><p>If a query topic is represented with both query images and text description, we can combine the similarities resulting from query processing using Equations ( <ref type="formula" coords="8,352.30,714.79,4.25,8.78" target="#formula_4">4</ref>) to ( <ref type="formula" coords="8,378.95,714.79,4.25,8.78">9</ref>) and ( <ref type="formula" coords="8,412.75,714.79,8.90,8.78" target="#formula_16">10</ref>) to ( <ref type="formula" coords="8,444.44,714.79,8.90,8.78" target="#formula_21">16</ref>) respectively. A simple scheme would be a linear combination of λ(Q, Z) and µ(P, Z) with ω ∈ [0, 1]</p><formula xml:id="formula_22" coords="8,206.40,745.25,302.31,9.96">ρ(Q, P, Z) = ω • λ(Q, Z) + (1 -ω) • µ(P, Z) (<label>17</label></formula><formula xml:id="formula_23" coords="8,508.71,745.99,4.45,8.78">)</formula><p>where ρ is the overall similarity and the optimal ω can be determined empirically using even sampling at 0.1 intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation from ImageCLEF 2005 Organizers</head><p>According to the ImageCLEF 2005 organizers, the MAP over 25 query topics for the submissions on similarity-based retrieval (Section 3.1, labeled as "i2r-vk-sim.txt"), semantics-based retrieval (Section 3.2, labeled as "i2r-vk-sem.txt"), and their fusion (Section 3.3, denoted as "i2r-vk-avg.txt") are 0.0721, 0.06, and 0.0921 respectively. The submission "i2r-vk-avg.txt" is also combined with textonly submissions "IPALI2R Tn" and "IPALI2R T" to form submissions for mixed retrieval. The best MAP among these submissions for mixed retrieval is 0.2821 by submission "IPALI2R TIan". More details can be found at the website http://trec.ohsu.edu/image/. The performance of the current VisMed implementation can be further improved. First of all, only two features, one each for color and texture, have been used to train the VisMed term detectors. More domain-specific features can be incorporated to enhance detection accuracies.</p><p>Secondly, some VisMed terms have high variations in visual appearances, it may be necessary to divide them into subclasses to ease the learning task. For example, 09-path-kidney may appear in different colors, 21-photo-stomach-ulcer has to cover both endoscopic and pathological images, etc. As we wanted to minimize the number of images from the test collection used for learning VisMed terms, we tried to look for additional images from the web. However, towards the end of the experiments of the VisMed approach, we realized that the web images, which were supposed to complement the very small training set selected from the test collection, consist of visual samples that are atypical (or even rather different) from those found in the medical test collection (i.e. over-generalization). For instance, as shown in Figure <ref type="figure" coords="9,215.80,520.14,3.90,8.78" target="#fig_4">4</ref>, the visual samples used to train VisMed terms related to face <ref type="bibr" coords="9,495.88,520.14,12.93,8.78" target="#b10">(11)</ref><ref type="bibr" coords="9,508.81,520.14,4.31,8.78" target="#b11">(12)</ref><ref type="bibr" coords="9,90.00,532.14,12.51,8.78" target="#b12">(13)</ref>, hand osteoarthritis <ref type="bibr" coords="9,195.13,532.14,16.42,8.78" target="#b14">(15)</ref>, skin and lesion (18-20), kidney pathologies <ref type="bibr" coords="9,406.63,532.14,16.42,8.78" target="#b16">(17)</ref>, and sketch (26-27), are not easily found (if not irrelevant) in the given test collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Medical CBIR is an emerging and challenging research area. We have proposed a structured framework for designing image semantics from statistical learning. Our adaptive framework is scalable to different image domains <ref type="bibr" coords="9,244.77,622.63,15.49,8.78" target="#b9">[10,</ref><ref type="bibr" coords="9,263.26,622.63,7.81,8.78" target="#b7">8]</ref> and embraces other design choices such as better visual features, learning algorithms, object detectors, spatial aggregation and matching schemes when they become available.</p><p>We reckon that a limitation of the current VisMed approach is the need to design the VisMed terms manually with labeled image patches as training samples. We have begun some work in a semi-supervised approach to discover meaningful visual vocabularies from minimally labeled image samples <ref type="bibr" coords="9,127.67,694.38,10.00,8.78" target="#b8">[9]</ref>. In the near future, we would also explore the integration with inter-class semantics <ref type="bibr" coords="9,90.00,706.38,10.00,8.78" target="#b7">[8]</ref>. Last but not least, we would also work with medical experts to design a more comprehensive set of VisMed terms to cover all the essential semantics in medical images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,190.08,365.59,222.92,8.78"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A 3-layer architecture for image indexing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,177.36,710.47,248.05,8.78;5,104.88,733.99,408.32,8.78;5,90.00,745.98,423.02,8.78;5,88.80,600.87,425.20,94.49"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: One visual example each for the VisMed terms Based on 0.3% (i.e. 158 images) of the 50, 026 images from the 4 collections plus 96 images obtained from the web, we cropped 1460 image regions to train and validate 39 VisMed terms</figDesc><graphic coords="5,88.80,600.87,425.20,94.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,190.56,196.39,221.97,8.78;7,131.52,109.10,340.10,71.94"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example to illustrate FlexiTile matching</figDesc><graphic coords="7,131.52,109.10,340.10,71.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,99.84,410.71,403.35,8.78;9,88.80,344.67,425.31,50.69"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Some training images that unlikely or irrelevant for the ImageCLEF 2005 datasets</figDesc><graphic coords="9,88.80,344.67,425.31,50.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,163.92,119.35,275.22,275.18"><head>Table 1 :</head><label>1</label><figDesc>VisMed terms and numbers of region samples</figDesc><table coords="6,163.92,136.63,275.22,257.90"><row><cell>VisMed Terms</cell><cell># VisMed Terms</cell><cell>#</cell></row><row><cell>00-angio-aorta-artery</cell><cell>30 01-angio-aorta-kidney</cell><cell>30</cell></row><row><cell>02-ct-abdomen-bone</cell><cell>40 03-ct-abdomen-liver</cell><cell>20</cell></row><row><cell>04-ct-abdomen-vessel</cell><cell>30 05-ct-chest-bone</cell><cell>30</cell></row><row><cell>06-ct-chest-emphysema</cell><cell>30 07-ct-chest-nodule</cell><cell>20</cell></row><row><cell>08-path-alzheimer</cell><cell>40 09-path-kidney</cell><cell>50</cell></row><row><cell>10-path-leukemia</cell><cell>30 11-photo-face-eye</cell><cell>60</cell></row><row><cell>12-photo-face-mouth</cell><cell>30 13-photo-face-nose</cell><cell>30</cell></row><row><cell>14-photo-fetus</cell><cell>50 15-photo-finger-osteo</cell><cell>60</cell></row><row><cell>16-photo-heart-attack</cell><cell>40 17-photo-kidney</cell><cell>30</cell></row><row><cell>18-photo-skin</cell><cell>60 19-photo-skin-benign</cell><cell>30</cell></row><row><cell cols="2">20-photo-skin-malignant 30 21-photo-stomach-ulcer</cell><cell>60</cell></row><row><cell>22-mri-head-face</cell><cell>50 23-mri-head-bone</cell><cell>40</cell></row><row><cell>24-mri-head-brain</cell><cell>50 25-sctg-body</cell><cell>60</cell></row><row><cell>26-sketch-human-head</cell><cell>40 27-sketch-human-limb</cell><cell>40</cell></row><row><cell>28-ultrasound-grey</cell><cell>30 29-ultrasound-color</cell><cell>20</cell></row><row><cell>30-xray-bone</cell><cell>40 31-xray-bone-fracture</cell><cell>60</cell></row><row><cell>32-xray-bone-joint</cell><cell>40 33-xray-chest-heart</cell><cell>20</cell></row><row><cell cols="3">34-xray-chest-lung-clear 30 35-xray-chest-lung-opaque 40</cell></row><row><cell>36-xray-pelvis</cell><cell>40 37-image-region-bright</cell><cell>20</cell></row><row><cell>38-image-region-dark</cell><cell>20</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,110.40,134.23,383.91,8.78" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,172.56,134.23,176.24,8.77">Neural Networks for Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Clarendon Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.40,154.15,402.88,8.78;10,110.40,166.15,182.54,8.78" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,232.60,154.15,253.45,8.78">The clef cross language image retrieval track (imageclef)</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<ptr target="http://ir.shef.ac.uk/imageclef2005/" />
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.40,186.07,402.67,8.78;10,110.40,198.07,402.91,8.78;10,110.40,210.07,44.05,8.78" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,420.07,186.07,93.00,8.78;10,110.40,198.07,246.34,8.78">Unsupervised feature selection applied to content-based retrieval of lung images</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Broderick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Aisen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,364.56,198.07,94.66,8.77">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="373" to="378" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.40,229.99,402.80,8.78;10,110.40,241.75,403.09,8.78;10,110.40,253.75,75.24,8.78" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,170.36,229.99,179.03,8.78">Making large-scale svm learning practical</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,194.64,241.75,244.01,8.77">Advances in Kernel Methods -Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT-Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="169" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.40,273.67,393.50,8.78" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,229.68,273.67,179.17,8.77">Fuzzy Sets, Uncertainty, and Information</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J</forename><surname>Klir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Folger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.40,293.59,402.40,8.78;10,110.40,305.59,100.94,8.78" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,211.70,293.59,237.58,8.78">Content-based image retrieval in medical applications</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,460.32,293.59,52.48,8.77;10,110.40,305.59,15.58,8.77">Methods Inf Med</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="354" to="361" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.40,325.51,402.80,8.78;10,110.40,337.51,203.42,8.78" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,158.17,325.51,312.31,8.78">Building visual vocabulary for image indexation and query formulation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,480.24,325.51,32.95,8.77;10,110.40,337.51,111.48,8.77">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="139" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.40,357.43,402.87,8.78;10,110.40,369.43,224.78,8.78" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,212.42,357.43,300.85,8.78;10,110.40,369.43,34.68,8.78">Combining intra-image and inter-class semantics for consumer image retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,153.36,369.43,84.56,8.77">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="847" to="864" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.40,389.35,402.89,8.78;10,110.40,401.35,285.75,8.78" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,222.26,389.35,286.68,8.78">Discovering recurrent image semantics from class discrimination</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,110.40,401.35,206.50,8.77">EURASIP Journal of Applied Signal Processing</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="10,110.41,421.27,402.76,8.78;10,110.40,433.03,297.74,8.78" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,217.41,421.27,295.75,8.78;10,110.40,433.03,69.93,8.78">A structured learning framework for content-based image indexing and visual query</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,189.12,433.03,121.89,8.77">Multimedia Systems Journal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="317" to="331" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.41,452.95,402.91,8.78;10,110.40,464.95,402.74,8.78;10,110.40,476.95,110.55,8.78" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,171.60,452.95,246.84,8.78">Semantic based biomedical image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,263.52,464.95,249.62,8.77;10,110.40,476.95,37.48,8.77">Trends and Advances in Content-Based Image and Video Retrieval</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Veltkamp</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.40,496.87,402.70,8.78;10,110.40,508.87,195.02,8.78" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,255.40,496.87,253.25,8.78">Texture features for browsing and retrieval of image data</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,110.40,508.87,96.59,8.77">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="837" to="842" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.41,528.79,402.87,8.78;10,110.40,540.79,402.67,8.78;10,110.40,552.55,167.18,8.78" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,368.21,528.79,145.07,8.78;10,110.40,540.79,350.82,8.78">A review of content-based image retrieval systems in medical applications -clinical benefits and future directions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Michoux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bandon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,470.65,540.79,42.43,8.77;10,110.40,552.55,85.00,8.77">Intl. J. of Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.41,572.71,402.55,8.78;10,110.40,584.47,160.69,8.78" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,313.54,572.71,180.18,8.78">A general framework for object detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">C</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.40,584.47,60.68,8.77">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="555" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.42,604.39,402.50,8.78;10,110.40,616.39,236.77,8.78" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,278.23,604.39,234.69,8.78;10,110.40,616.39,30.87,8.78">Content-based image retrieval with relevance feedback in mars</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,162.97,616.39,85.03,8.77">Proc. of IEEE ICIP</title>
		<meeting>of IEEE ICIP</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="815" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.41,636.31,402.68,8.78;10,110.41,648.31,402.54,8.78;10,110.41,660.31,146.06,8.78" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,361.55,636.31,151.54,8.78;10,110.41,648.31,261.26,8.78">Using human perceptual categories for content-based retrieval from a medical image database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pavlopoulou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,384.97,648.31,127.98,8.77;10,110.41,660.31,61.18,8.77">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="119" to="151" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.35,680.23,402.52,8.78;10,110.41,692.23,185.18,8.78" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,236.92,680.23,271.72,8.78">Example-based learning for view-based human face detection</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,110.41,692.23,96.59,8.77">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="51" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.41,712.15,289.81,8.78" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m" coord="10,172.09,712.15,116.10,8.77">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
