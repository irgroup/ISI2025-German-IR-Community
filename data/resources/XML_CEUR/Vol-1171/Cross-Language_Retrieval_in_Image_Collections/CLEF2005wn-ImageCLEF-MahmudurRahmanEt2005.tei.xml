<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,112.32,98.63,378.48,15.61;1,164.16,120.59,274.74,15.61">Supervised Machine Learning based Medical Image Annotation and Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,163.44,154.07,107.20,9.96"><forename type="first">Md</forename><forename type="middle">Mahmudur</forename><surname>Rahman</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CINDI Group</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455, De Maisonneuve Blvd. West</addrLine>
									<postCode>H3G 1M8</postCode>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,279.52,154.07,62.43,9.96"><forename type="first">Bipin</forename><forename type="middle">C</forename><surname>Desai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CINDI Group</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455, De Maisonneuve Blvd. West</addrLine>
									<postCode>H3G 1M8</postCode>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.60,154.07,90.33,9.96"><forename type="first">Prabir</forename><surname>Bhattacharya</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CINDI Group</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455, De Maisonneuve Blvd. West</addrLine>
									<postCode>H3G 1M8</postCode>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,112.32,98.63,378.48,15.61;1,164.16,120.59,274.74,15.61">Supervised Machine Learning based Medical Image Annotation and Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CA4E22291B7AF2BB1A616DB60BA1ABF8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.7 Digital Libraries</term>
					<term>I.4.8 [Image Processing and Computer Vision]: Scene Analysis-Object Recognition Algorithms, Machine learning, Performance, Experimentation Image annotation, Classification, Content based image retrieval, Support vector machine, Statistical distance measure</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the approaches and experimental results of image annotation and retrieval in our first participation of ImageCLEFmed 2005. In this work, we investigate a supervised learning approach to associate low-level global image features with their high level visual and/or semantic categories for image annotation and retrieval. For automatic image annotation, we represent input images through a large dimensional feature vector of texture, edge and shape features. A multi-class classification system based on pairwise coupling of several binary support vector machine (SVM) is trained on this input to predict the categories of test images, which will be effective for later annotation. For visual only retrieval, we utilize a low dimensional feature vector of color, texture and edge features based on principal component analysis (PCA) and category specific feature distribution information in a statistical similarity measure function. Based on the online category prediction of query and database images by the multi-class SVM classifier, pre-computed category specific first and second order statistical parameters are utilized in Bhattacharyya distance measure on the assumption that distributions are multivariate Gaussian. Experimental results of both image annotation and retrieval are reported in this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>During the last decade, there have been an overwhelming research interests in medical image retrieval and classification from different communities <ref type="bibr" coords="1,316.46,683.15,15.49,9.96" target="#b10">[11,</ref><ref type="bibr" coords="1,334.82,683.15,11.60,9.96" target="#b9">10]</ref>. Medical images of various modalities (X-ray, CT, MRI, Ultrasound etc.) constitute an important source of anatomical and functional information for the diagnosis of diseases, medical research and education. Effectively and efficiently searching and annotating these large image collections poses significant technical challenges.</p><p>ImageCLEFmed is part of the Cross Language Evaluation Forum (CLEF), a benchmarking event for multilingual information retrieval. The main goal of ImageCLEFmed is to improve the retrieval of medical images from heterogeneous and multilingual document collections containing images as well as text. This year (2005) in ImageCLEFmed, there are mainly two task: image annotation and retrieval to be performed.</p><p>In automatic image annotation task, the main aim is to find out how well current techniques can identify image modality, body orientation, body region, and biological system examined based on the images. Here, we utilized a database of 9,000 fully classified images taken randomly from medical routine to train a classification system. 1,000 images for which classification labels are not available are used as test images and have been classified by a multi-class classification system. The results of the classification step can be used for multilingual image annotations as well as for DICOM header corrections.</p><p>In image retrieval task, we have experimented with a visual only approach, where an example image is used to perform a search against a medical image database to find similar images based on visual attributes (color, texture, etc.). Each medical image or a group of images represents an illness, and case notes in English or French are associated with each illness to be used for diagnosis. This task is based on the Casimage, MIR, PEIR, and PathoPIC datasets, containing about 50,000 images of different modalities (CT, MRI, X-ray etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Image annotation</head><p>Automatic image annotation or image classification is an area of active research in the field of machine learning and pattern recognition. Retrieval systems have traditionally used manual image annotation for indexing and then later retrieving their image collections. However, manual image annotation is an expensive and labor intensive procedure <ref type="bibr" coords="2,344.69,379.19,9.87,9.96" target="#b6">[7]</ref>. Here, we investigate an automatic approach to categorize or annotate images based on a supervised learning technique. A classification task usually involves with training and testing data which consist of some data instances. In supervised classification, we are given a collection of labeled images (or priori knowledge), and the problem is to label a newly encountered, yet unlabeled image. Each instance in the training set contains category or class specific labels and several image feature descriptors in the form of a combined feature vector.</p><p>In this work, we present effective texture, edge and shape descriptors to represent image content at global level and input in to a multi-class classifaction system based on several binary support vector machine (SVM) classifier. The goal of SVM is to produce a model which will predict target value or category of images with highetst probability or confidence in the testing set which are given as input feature vectors to the classification system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Selection &amp; Extraction</head><p>This section describes how our system characterizes images for efficient representation. The collection contained mainly monochrome or gray level medical images with specific layout. Hence, we characterize images by texture, edge and shape features, whereby ignoring color information totally. We know that only one type of low-level features does not work well. So, we incorporate these features in a combined feature vector and use it as input to the classification system. The accuracy of our classification system depends greatly on the representation of these low-level visual features. These image features help propagate annotation from trained images to unlabeled images. The more discriminative the low-level features, the more accurate the content-based classification or annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Texture Feature</head><p>Medical images of different categories can be distinguished via their homogeneousness or texture characteristics. Therefore, it may be useful to extract texture features for image classfication.</p><p>We include a texture feature set approach to extract spatially localized texture information from the gray level co-occurrence matrix <ref type="bibr" coords="3,278.28,115.67,10.00,9.96" target="#b0">[1]</ref>. A gray level co-occurrence matrix is defined as a sample of the joint probability density of the gray levels of two pixels separated by a given displacement d and angle θ. The G × G gray level co-occurrence matrix p for a displacement vector d = (dx, dy) is defined as <ref type="bibr" coords="3,231.72,151.55,14.60,9.96" target="#b15">[16]</ref>:</p><formula xml:id="formula_0" coords="3,199.20,173.51,313.84,9.96">p(i, j) = |((r, s), (t, v)) : (I(r, s) = i, I(t, v) = j|,<label>(1)</label></formula><p>where (r, s), (t, v) ∈ N × N, (t, v) = (r + dx)(s + dy).</p><p>Likewise image histograms, which can be thought of as an estimate of the probability distribution of gray values or color levels in an image, a co-occurrence matrix is an estimate of the joint probability distribution of pairs of gray-levels at some fixed relative position in the image. Typically, the information stored in a co-occurrence matrix is sparse. Also, it is often useful to consider a number of co-occurrence matrices, one for each relative position of interest, in order to grasp different textual cues or the same textual cur at different scale. In order to obtain efficient descriptors, the information contained in co-occurrence matrices is traditionally condensed in a few statistical features. We obtained four co-occurrence matrices for four different orientations (horizontal 0 • ,vertical 90 • , and two diagonals 45 • and 135 • ) and normalize the entries [0,1] by dividing each entry by total number of pixels.</p><p>The co-occurrence matrix reveals certain properties about the spatial distribution of the gray levels in the texture image. For example, if most of the entries in the co-occurrence matrix are concentrated along the diagonals, then the texture is coarse with respect to the displacement vector d. Haralick has proposed a number of useful texture features that can be computed from the co-occurrence matrix. Higher order features, such as energy, entropy, contrast, homogeneity and maximum probability are measured based on each gray level co-occurrence matrix to form a five dimensional feature vector as follows <ref type="bibr" coords="3,271.45,398.63,14.60,9.96" target="#b15">[16]</ref>:</p><formula xml:id="formula_1" coords="3,247.08,420.67,265.96,22.56">Energy = i j p 2 (i, j)<label>(2)</label></formula><p>Entropy =i j p(i, j)logp(i, j)</p><formula xml:id="formula_2" coords="3,230.52,458.03,282.52,52.28">Contrast = i j (i -j) 2 p(i, j)<label>(3)</label></formula><formula xml:id="formula_3" coords="3,226.08,489.59,286.96,56.00">Homogeneity = i j p(i, j) 1 + |i -j|<label>(4)</label></formula><p>M aximum probability = max p(i, j)</p><p>Finally, we obtained a twenty dimensional feature vector by concatenating the feature vectors of each co-occurrence matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Edge Feature</head><p>To represent the edge feature on a global level, a histogram of edge direction is constructed. The edge information contained in the images is processed and generated by using the Canny edge detection (with σ = 1, Gaussian masks of size = 9, low threshold = 1, and high threshold = 255) algorithm <ref type="bibr" coords="3,157.57,663.47,13.06,9.96" target="#b3">[4]</ref>.The corresponding edge directions are quantized into 72 bins of 5 • each. Scale invariance is achieved by normalizing this histograms with respect to the number of edge points in the image. Hence, the dimension of the edge feature vector is 72.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Shape Feature</head><p>Image classification for medical images with gary scale images have to use shape feature for efficient representation. We represent the global shape of an image in terms of seven invariant moments <ref type="bibr" coords="4,90.00,103.79,10.00,9.96" target="#b7">[8]</ref>. These features are invariant under rotation, scale, translation, and reflection of images and have been widely used in a number of applications due to their invariance properties. For a 2-D image, f (x, y), the central moment of order (p + q) is given by</p><formula xml:id="formula_6" coords="4,221.16,149.93,291.88,22.11">µ pq = x y (x -x) p (y -y) q f (x, y).<label>(7)</label></formula><p>Seven moment invariants (M 1 -M 7 ) based on the second and third order moments are given as follows <ref type="bibr" coords="4,122.76,195.83,9.91,9.96" target="#b7">[8]</ref>:</p><formula xml:id="formula_7" coords="4,130.80,229.67,382.24,130.17">M 1 = (µ 20 + µ 02 ) M 2 = (µ 20 -µ 02 ) 2 + µ 2 11 M 3 = (µ 30 -3µ 12 ) 2 + (3µ 21 -µ 03 ) 2 M 4 = (µ 30 + µ 12 ) 2 + (µ 21 + µ 03 ) 2 M 5 = (µ 30 + µ 12 ) + (µ 30 -3µ 12 )[(µ 30 + µ 12 ) 2 -3(µ 21 + µ 03 ) 2 ] +(3µ 21 -µ 03 ) × (µ 21 + 3µ 03 )[3(µ 03 + µ 21 ) 2 -(µ 21 -µ03) 2 ] M 6 = (µ 20 + µ 02 )[(µ 30 + µ 12 ) 2 -(µ 21 + µ 03 ) 2 + 4µ 11 (µ 30 + µ 12 )(µ 21 + µ 03 ) M 7 = (3µ 21 -µ 03 )(µ 30 + µ 12 )[(µ 30 + µ 12 ) 2 -3(µ 21 + µ 03 ) 2 ] -(µ 30 -3µ 12 )(µ 21 + µ 03 )[3(µ 03 + µ 21 ) 2 -(µ 21 -µ03) 2 ]<label>(8)</label></formula><p>M 1 -M 6 are invariant under rotation and reflection. M 7 is invariant only in its absolute magnitude under a reflection. Now, let us consider f t , f e , and f s be the texture, edge and shape feature vector respectively of an image. Now the composite feature vector is formed by simple concatenation of each individual feature vector as F combined = (f t + f e + f s ), where the dimension R d of F combined is the sum of individual feature vector dimension and d = (20 + 72 + 7) = 99. We use this vector to represent the images. Thus, the input space for our SVM classifiers is a 99-dimensional space, and each image in our database corresponds to a point in this space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Category prediction with multi-class SVM</head><p>Support vector machine (SVM) is an emerging machine learning technology which has been successfully used in content based image retrieval <ref type="bibr" coords="4,294.85,512.99,9.91,9.96" target="#b4">[5]</ref>.</p><p>Given training data ( x 1 , . . . , x n ) that are vectors in some space x i ∈ R n and their labels (y 1 , . . . , y n ) where y i ∈ (+1, -1) n , the general form of the binary linear classification function is</p><formula xml:id="formula_8" coords="4,267.24,558.83,245.80,9.96">g(x) = w • x + b (9)</formula><p>which corresponds to a separating hyper plane</p><formula xml:id="formula_9" coords="4,273.96,602.75,234.63,9.96">w • x + b = 0 (<label>10</label></formula><formula xml:id="formula_10" coords="4,508.59,602.75,4.45,9.96">)</formula><p>where x is an input vector, w is a weight vector, and b is a bias. The goal of SVM is to find the parameters w and b for the optimal hyper plane to maximize the geometric margin 2 || w|| between the hyper planes, subject to the solution of the following optimization problem <ref type="bibr" coords="4,437.53,650.27,10.00,9.96" target="#b2">[3]</ref>:</p><formula xml:id="formula_11" coords="4,241.92,670.13,271.12,30.75">min w, b, ξ 1 2 w T w + C n i=1 ξ i<label>(11)</label></formula><p>subject to</p><formula xml:id="formula_12" coords="5,248.04,71.81,265.00,12.03">y i ( w T φ( x i ) + b) ≥ 1 -ξ i<label>(12)</label></formula><p>where ξ i ≥ 0 and C &gt; 0 is the penalty parameter of the error term. Here training vectors x i are mapped into a high dimensional space by the non linear mapping function φ : R n → R f , where f &gt; n or f could even be infinite. Optimization problem and its solution can be represented by the inner product. Hence,</p><formula xml:id="formula_13" coords="5,229.92,137.45,278.67,12.15">x i . x j → φ( x i ) T φ( x j ) = K( x i , x j ) (<label>13</label></formula><formula xml:id="formula_14" coords="5,508.59,139.19,4.45,9.96">)</formula><p>where K is a kernel function. The SVM classification function is given by <ref type="bibr" coords="5,413.53,157.07,9.91,9.96" target="#b4">[5]</ref>:</p><formula xml:id="formula_15" coords="5,223.68,178.01,289.36,30.75">f ( x) = sign n i=1 α i y i K( x i , x) + b<label>(14)</label></formula><p>A number of methods have been proposed for extension to multi-class problem to separate L mutually exclusive classes essentially by solving many two-class problems and combining their predictions in various ways <ref type="bibr" coords="5,213.74,242.51,9.91,9.96" target="#b4">[5]</ref>. One technique, commonly known as Pairwise Coupling (PWC) or "one-vs.-one"is to construct SVMs between all possible pairs of classes. This method trains L * (L -1)/2 binary classifiers, each of which provides a partial decision for classifying a data point. PWC then combines the output of all classifiers to form a class prediction. During testing, each of the L * (L -1)/2 classifier votes for one class. The winning class is the one with the largest number of accumulated votes. We use this technique for the implementation of our multi-class SVM by using the LIBSVM software package <ref type="bibr" coords="5,291.02,314.27,10.00,9.96" target="#b5">[6]</ref>.</p><p>Multi-class image classification systems work by classifying an image into one of many predefined categories. There are 57 categories of images provided in the training set for the image annotation task. So, we define a set of 57 labels where each label characterizes the representative semantics of an image category. Class labels along with feature vectors are generated from all images at the training stage. In testing stage, each unannotated image is classified against the 57 categories using PWC or "one-vs.-one" technique. This produces a ranking of the 57 categories, with each category assigned a confidence or probability score to each image. The labels of these 57 categories, along with their probabilities, become the annotation for the testing images as the following format:</p><formula xml:id="formula_16" coords="5,96.72,435.11,403.15,9.96">&lt; imageno &gt;&lt; conf idence class1 &gt;&lt; conf idence class2 &gt; • • • &lt; conf idence class57 &gt;</formula><p>The confidence represents the weight of a label or category in the overall description of an image and the class with the highest confidence is considered to be the class of the image. This ranking of categories based on confidence score can be very useful in future annotation or search purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Image retrieval</head><p>Currently, most content based image retrieval (CBIR) systems are similarity-based, where similarity between query and target images in a database is measured by some form of distance metrics in feature space <ref type="bibr" coords="5,149.04,562.91,14.60,9.96" target="#b14">[15]</ref>. However, retrieval systems generally conduct this similarity matching on a very high-dimensional feature space without any semantic interpretation or paying enough attention to the underlying distribution of the feature space <ref type="bibr" coords="5,303.05,586.79,14.60,9.96" target="#b13">[14]</ref>. However, high dimensional feature vectors not only increase the computational complexity in similarity matching and indexing, but also increase the logical database size. For many frequently used visual features in medical images, often their category specific distributions are also available. In this case, it is possible to extract a set of low-level features (e.g., color, texture, shape, etc.) to predict semantic categories of each image by identifying its class assignment using a classifier. Thus, an image can be best characterized by exploiting the information of feature distribution from its semantic category.</p><p>In the image retrieval task, we have investigated a category based adaptive statistical similarity measure technique on the low-dimensional feature space. For this, we utilized principal component analysis (PCA) for dimension reduction and multi-class support vector machine (SVM) for online category prediction of query and database images. Hence, category specific statistical parameters in low-dimensional feature space can be exploited by the statistical distance measure in real time similarity matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature extraction &amp; representation in PCA sub-space</head><p>The performance of a CBIR system mainly depends on the particular image representation and similarity matching function employed. Images in four datasets contain both color and gray level images for retrieval evaluation. Hence, we have extracted color, texture and edge features for our image representation at global level.</p><p>Color is the most useful low-level feature and its histogram based representation is one of the earliest descriptors, which is widely used in CBIR <ref type="bibr" coords="6,309.72,191.39,14.60,9.96" target="#b14">[15]</ref>. For color feature, a 108 dimensional color histogram is created in vector form on HSV (Hue, Saturation, Value) color space. In HSV space, the colors correlates well and can be matched in a way that is consistent with human perception. In this work, we uniformly quantized HSV space into 12 bins for hue(each bin consisting of a range of 30 • ), 3 bins for saturation and 3 bins for value, which results in 108 bins for color histogram. Many medical images of different categories can be distinguished via their homogeneous texture and global edge characteristics. Hence, we have extracted the same global texture and edge features as measured for image annotation.</p><p>As the dimensions of color, texture and edge (108+20+72 = 200) feature vectors are high, we need to apply some dimension reduction technique to reduce the computational complexity and logical database size. Moreover, if the training samples used to estimate the statistical parameters are smaller compare to the size of feature dimension, then inaccuracy or singularity may arise for second order (covariance matrix) parameter estimation. The problem of selecting most representative feature attributes commonly known as dimension reduction, has been examined by principal component analysis (PCA) <ref type="bibr" coords="6,253.23,358.79,15.49,9.96" target="#b11">[12]</ref> in our experiment. The basic idea of PCA is to find m linearly transformed components so that they explain the maximum amount of variances in the input data and mathmetical steps used to describe the method is as follows: given a set of N feature vectors (training samples)</p><formula xml:id="formula_17" coords="6,307.08,393.53,89.23,11.55">x i ∈ R d |i = (1 • • • N )</formula><p>, where the mean vector(µ) and covariance matrix (C) is estimated as</p><formula xml:id="formula_18" coords="6,196.56,428.33,316.48,30.63">µ = 1 N N i=1 x i &amp; C = 1 N N i=1 ( x i -µ)( x i -µ) T<label>(15)</label></formula><p>Let ν i and λ i be the eigenvectors and the eigenvalues of C, then they satisfy the following:</p><formula xml:id="formula_19" coords="6,253.56,490.01,259.48,30.75">λ i = N i=1 (ν T i ( x i -µ)) 2<label>(16)</label></formula><p>Here, N i=1 λ i accounts for the total variance of the original feature vectors set. Now, PCA method tries to approximate the original feature space using an m dimensional feature vector, that is using m largest eigenvalues account for a large percentage of variance, where typically m &lt;&lt; min(d, N ). These m eigenvectors span a subspace, where</p><formula xml:id="formula_20" coords="6,287.64,570.59,86.05,10.65">V = [ v 1 , v 2 , • • • , v m ]</formula><p>is the d × m-dimensional matrix that contains orthogonal basis vectors of the feature space in its columns. The m×d transformation V T transforms the original feature vector from R d → R m ones. That is</p><formula xml:id="formula_21" coords="6,241.08,614.69,271.96,12.03">V T ( x i -µ) = y i , i = 1 • • • N (17)</formula><p>where y i ∈ R m and kth component of the y i vector is called the kth principal component (PC) of the original feature vector x i . So, the feature vector in the original R d space for query and database images can be projected on to the R m space via the transformation of V T <ref type="bibr" coords="6,453.96,662.15,14.60,9.96" target="#b11">[12]</ref>. We have applied this PCA technique to our composite feature vector and reduced the feature dimension for subsequent SVM training and similarity matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive statistical distance measure</head><p>Statistical distance measure, defined as the distances between two probability distributions, finds its uses in many research areas in pattern recognition, information theory, and communication. It captures correlations or variations between attributes of the feature vectors and provides bounds for probability of retrieval error of a two way classification problem. Recently, CBIR community also adopted statistical distance measures for similarity matching <ref type="bibr" coords="7,383.88,127.67,10.57,9.96" target="#b1">[2,</ref><ref type="bibr" coords="7,398.53,127.67,11.62,9.96" target="#b13">14]</ref>. In this scheme query image q and target image t are assumed to be in different classes and their respective density as p q ( x) and p t ( x), both defined on R d . When these densities are multivariate normal, they can be approximated by mean vector µ and covariance matrix C as p q ( x) = N ( x; µ q , C q ) &amp; p t ( x) = N ( x; µ t , C t ) where,</p><formula xml:id="formula_22" coords="7,200.40,185.39,312.64,24.96">N ( x; µ, C) = 1 (2π)d|C| exp -1 2 ( x-µ) T C -1 ( x-µ)<label>(18)</label></formula><p>here, x ∈ R d and | • | is matrix determinant <ref type="bibr" coords="7,291.85,216.95,10.00,9.96" target="#b8">[9]</ref>. A popular measure of similarity between two Gaussian distributions is the Bhattacharyya distance, which is equivalent to an upper bound of the optimal Bayesian classification error probability <ref type="bibr" coords="7,326.32,240.83,29.69,9.96">[9] [13]</ref>. Bhattacharyya distance (D Bhatt ) between query image q and target image t in the database is given by:</p><formula xml:id="formula_23" coords="7,149.76,271.15,363.28,31.93">D Bhatt (q, t) = 1 8 (µ q -µ t ) T (C q + C t ) 2 -1 (µ q -µ t ) + 1 2 ln (Cq+Ct) 2 |C q ||C t |<label>(19)</label></formula><p>where µ q and µ t are the mean vectors, and C q and C t are the covariance matrices of query image q and target image t respectively. Equation ( <ref type="formula" coords="7,293.91,322.31,8.90,9.96" target="#formula_23">19</ref>) is composed of two terms, the first one being the distance between mean vectors of images, while the second term gives the class separability due to the difference between class covariance matrices. In the retrieval experiment, we used the Bhattacharyya distance measure for similarity matching. Here, it is called adaptive due to the nature of online selection of mean (µ) and covariance matrix (C) by the multi-class SVM as discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Category prediction &amp; parameter estimation</head><p>To utilize category specific distribution information in similarity matching, we utilize the multiclass SVM classifer as described in section 2.2 to predict the category of query and databse images.</p><p>Based on the online prediction, distance measure functions will be adjusted to accommodate category specific parameters for query and reference images of database. To estimate the parameters of the category specific distributions, feature vectors are extracted and reduced in dimension as mentioned in section 3.1, from N selected training image samples. It is assumed that feature of each category will have distinguishable normal distribution. Computing the statistical distance measures between two multivariate normal distributions requires first and second order statistics in the form of mean (µ) and covariance matrix (C) or parameter vector θ = (µ, C) as described in previous section. Suppose that there are L different semantic categories in the database, each assumed to have a multivariate normal distribution with mean vector µ i and covariance matrix C i , for i ∈ L. However, the true values of µ and C of each category usually are not known in advance and must be estimated from a set of training samples N <ref type="bibr" coords="7,340.56,571.31,9.91,9.96" target="#b8">[9]</ref>. The µ i and C i of each category are estimated as</p><formula xml:id="formula_24" coords="7,170.28,592.49,342.75,30.87">µ i = 1 N i Ni j=1 x i,j &amp; C i = 1 N i -1 Ni j=1 ( x i,j -µ i )( x i,j -µ i ) T<label>(20)</label></formula><p>where x i,j is sample j from category i, N i is the number of training samples from category i and N = (N 1 + N 2 + . . . + N L ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimentation &amp; Results</head><p>For annotation experiment, we perform the following procedure at the training stage: For image classification, recent work <ref type="bibr" coords="8,273.36,359.15,10.45,9.96" target="#b4">[5]</ref> shows that the RBF kernel works well with highdimensional image data and can handle the case when the relation between class labels and attributes is nonlinear. Therefore, we use RBF kernel as a reasonable first choice. There are two tunable parameters while using RBF kernels and soft-margin SVMs in the version space: C and γ. The γ in the RBF kernel controls the shape of the kernel and C controls the trade-offs between margin maximization and error minimization. Increasing C may decrease training error, but it can also lead to poor generalization <ref type="bibr" coords="8,248.42,430.79,10.00,9.96" target="#b6">[7]</ref>.</p><p>It is not known beforehand which C and γ are the best for our classification problem. In the training stage, the goal is to identify good (C and γ ), so that the classifier can accurately predict testing data. It may not be useful to achieve high training accuracy (i.e., classifiers accurately predict training data whose class labels are indeed known). Therefore, we use a 10 fold crossvalidation with various combinations of γ and C to measure the classification accuracy.</p><p>In 10-fold cross-validation, we first divide the training set into 10 subsets of equal size. Sequentially one subset is tested using the classifier trained on the remaining 9 (10-1) subsets. Thus, each instance of the whole training set is predicted once so the cross-validation accuracy is the percentage of data which are correctly classified. We perform a grid-search on C and γ and using cross-validation. Basically pairs of (C , γ ) are tried and the one with the best cross-validation accuracy is picked. We find the best (C , γ ) is (200, 0.01) with the cross-validation rate 54.65%. After the best (C , γ) is found, the whole training set of 9,000 images is trained again to generate the final classifiers. Finally, the generated classifiers are tested on the 1000 image testing set of unknown labels to generate the image annotation.</p><p>In ImageCLEFmed 2005 automatic image annotation experiment, we have submitted only one run with the above parameters and the classification error rate is 43.3%. Which means, 433 images were misclassified out of 1000 or accuracy of our system is 56.7% at this moment.</p><p>In image retrieval experiment, for statistical parameter estimation and SVM training, we observed the images of four datasets closely and selected 33 different categories based on perceptual and modality specific differences, each with 100 images for generating the training samples. However, for actual evaluation of similarity measure functions, we conducted our experiments on the entire database (Around 50,000 images from four different collections). For SVM training, we used the reduced feature vector with radial basis kernel (RBF) function. After 10 fold cross validation, we found the best parameters C = 100 and γ = .03 with an accuracy of 72.96% in our current setting and finally trained the whole training set with these parameters. The dimensionality of the feature vector is reduced to R d → R m , where d=200 and m=25 and accounted for 90.0% of the total variances. In the ImageCLEFmed 2005 evaluation, we have submitted only one run with the above parameters and achieved a mean average precision of 0.0072 across all queries, which is very low at this moment compare to other systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and future work</head><p>This is the first year for the CINDI group taking part in the ImageCLEF campaign and specially in the ImageCLEFmed track. This year, our main goal was to participate and do some initial experiment with the databases provided by the organizer.</p><p>This report presents our approaches to automatic image annotation and retrieval based on global image feature contents and multi-class SVM classifier. Despite having 57 categories for annotation, many of them closely-related, our classification system is still able to provide moderate classification performance. In future, we will investigate with region based local image features and statistical methods that can deal with the challenges of an unbalanced training set, as provided in the current experimental setting.</p><p>The retrieval performance of our system is not good enough due to the following main reasons. First of all, it is very difficult to select a reasonable training set of images with predifined categories from four different datasets with a huge amount of variability in image size, resolution, modality etc. The performance of our system is critical to the appropriate training of multi-class SVM as parameter selection of statistical distance measure is depended on the online category prediction. Secondly, we have only used global image feature, which may not suitable for medical images as large unwanted background dominated in many of these images. In future we will try to resolve these issues and incorporate text based search approach in addition with the visual based approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,191.88,267.47,219.26,9.96;8,193.44,59.02,216.09,194.08"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Block diagram of the retrieval technique</figDesc><graphic coords="8,193.44,59.02,216.09,194.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,139.44,58.93,324.00,195.37"><head></head><label></label><figDesc></figDesc><graphic coords="9,139.44,58.93,324.00,195.37" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,110.52,83.27,402.48,9.96;10,110.52,95.27,402.52,9.96;10,110.52,107.15,100.13,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,239.31,83.27,157.94,9.96">Texture Analysis in Machine Vision</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,410.17,83.27,102.84,9.96;10,110.52,95.27,394.68,9.96">Chapter Using Texture in Image Similarity and Retrieval,Series on Machine Perception and Artificial Intelligence</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,127.07,402.61,9.96;10,110.52,139.07,402.78,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,226.35,127.07,279.25,9.96">Probabilistic vs. geometric similarity measures for image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.52,139.07,328.34,9.96">Proceedings. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,158.99,402.54,9.96;10,110.52,170.99,175.87,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,163.20,158.99,264.37,9.96">A tutorial on support vector machines for pattern recognition</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,438.74,158.99,74.32,9.96;10,110.52,170.99,90.39,9.96">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="121" to="167" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,190.91,402.62,9.96;10,110.52,202.79,106.74,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,160.46,190.91,187.26,9.96">A computational approach to edge detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,358.56,190.91,154.59,9.96;10,110.52,202.79,24.69,9.96">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,222.71,402.78,9.96;10,110.52,234.71,172.49,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,280.13,222.71,207.78,9.96">SVMs for histogram-based image classification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,496.37,222.71,16.94,9.96;10,110.52,234.71,141.46,9.96">IEEE Transaction on Neural Networks</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,254.63,402.60,9.96;10,110.52,266.63,233.94,9.96" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,222.03,254.63,209.57,9.96">LIBSVM : a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,286.55,402.57,9.96;10,110.52,298.43,402.47,9.96;10,110.52,310.43,226.26,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,324.67,286.55,188.43,9.96;10,110.52,298.43,252.05,9.96">CBSA: Content-based Soft Annotation for Multimodal Image Retrieval Using Bayes Point Machines</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kingshy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sychay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,376.10,298.43,136.89,9.96;10,110.52,310.43,146.56,9.96">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="26" to="38" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,330.35,402.53,9.96;10,110.52,342.35,233.82,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,361.62,330.35,151.44,9.96;10,110.52,342.35,40.12,9.96">Aircraft identification by moment invariants</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Dudani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Breeding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Mcghee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,161.78,342.35,111.38,9.96">IEEE Trans. Comput. C</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="39" to="45" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,362.27,402.74,9.96;10,110.52,374.15,27.88,9.96" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,174.27,362.27,203.72,9.96">Introduction to Statistical Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
	<note>Second ed</note>
</biblStruct>

<biblStruct coords="10,110.52,394.07,402.55,9.96;10,110.52,406.07,390.18,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,269.35,394.07,243.73,9.96;10,110.52,406.07,26.11,9.96">Medical image databases: A content-based retrieval approach</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">D</forename><surname>Tagare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jafe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,148.45,406.07,246.82,9.96">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="184" to="198" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,425.99,402.54,9.96;10,110.52,437.99,402.53,9.96;10,110.52,449.87,126.90,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,352.34,425.99,160.73,9.96;10,110.52,437.99,249.46,9.96">A review of content-based image retrieval applications -clinical benefits and future directions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Michoux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bandon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,371.40,437.99,141.65,9.96;10,110.52,449.87,47.86,9.96">International Journal of Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,469.79,402.50,9.96;10,110.52,481.79,275.46,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,259.48,469.79,253.54,9.96;10,110.52,481.79,83.46,9.96">Dimensionality and sample size considerations in pattern recognition practice</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bhandrasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,205.21,481.79,98.38,9.96">Handbook of Statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="835" to="855" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,501.71,402.66,9.96;10,110.52,513.71,215.46,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,164.30,501.71,322.17,9.96">The divergence and Bhattacharyya distance measures in signal selection</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,495.37,501.71,17.81,9.96;10,110.52,513.71,139.97,9.96">IEEE Trans. Commun. Technol, COM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="52" to="60" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,533.63,402.60,9.96;10,110.52,545.51,323.68,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,350.93,533.63,162.19,9.96;10,110.52,545.51,130.43,9.96">Empirical evaluation of dissimilarity measures for color and texture</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,252.37,545.51,146.41,9.96">Intern. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,565.43,402.47,9.96;10,110.52,577.43,402.64,9.96;10,110.52,589.43,45.30,9.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,376.69,565.43,136.31,9.96;10,110.52,577.43,124.20,9.96">Content-Based Image Retrieval at the End of the Early Years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,246.03,577.43,215.71,9.96">IEEE Trans. on Pattern Anal. and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,609.35,402.52,9.96;10,110.52,621.23,402.58,9.96;10,110.52,633.23,104.36,9.96" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,241.12,609.35,271.93,9.96;10,110.52,621.23,58.24,9.96">The Handbook of Pattern Recognition and Computer Vision (2nd Edition)</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tuceryan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<editor>C. H. Chen, L. F. Pau, P. S. P. Wang</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>World Scientific Publishing Co</publisher>
			<biblScope unit="page" from="207" to="248" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
