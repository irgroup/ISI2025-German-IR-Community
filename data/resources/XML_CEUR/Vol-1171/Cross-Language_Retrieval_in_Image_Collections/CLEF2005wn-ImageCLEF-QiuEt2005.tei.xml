<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,119.16,82.36,357.04,14.42">Report for Annotation task in ImageCLEFmed 2005</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.06,123.00,32.30,9.88"><forename type="first">Bo</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm (I2R)</orgName>
								<address>
									<postCode>119613</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.52,123.00,50.88,9.88"><forename type="first">Wei</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm (I2R)</orgName>
								<address>
									<postCode>119613</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,286.32,123.00,36.78,9.88"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm (I2R)</orgName>
								<address>
									<postCode>119613</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,331.66,123.00,77.58,9.88"><forename type="first">Chang</forename><forename type="middle">Sheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm (I2R)</orgName>
								<address>
									<postCode>119613</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,119.16,82.36,357.04,14.42">Report for Annotation task in ImageCLEFmed 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">756AA958438571B33FB3C2BDD316F45E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H3.1 Content Analysis and Indexing</term>
					<term>H3.3 Information Search and Retrieval</term>
					<term>Measurement, Performance, Experimentation automatic medical image annotation, SVM, low resolution map, multi-class classification, unbalance, over-fitting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the medical image annotation task we have mainly explored ways to use different image features to achieve robust classification performance, including both global features and regional blob features. Experimental results show that using a combination of the blob region feature and three low resolution pixel maps (gray level, texture and contrast) can achieve the highest recognition accuracy. All these features are normalized and stacked to form a one-dimension feature vector as inputs of classifiers. In our experiments Supporting Vector Machines (SVM) with RBF (radial basis functions) kernels are used for the classification task, trained over a subset of 9000 given medical training images. Our proposed method has achieved a recognition rate of 89% over a subset of the training images which were not used in the SVM training. According to the evaluation result from the imageCLEF05 organizers, our method has achieved a recognition rate of about 80% over the 1000 testing images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the fast development of modern medical devices, more and more medical images are generated, so that the demand becomes more and more urgent for automatically indexing, comparing, analyzing and annotating the huge volume of medical images. As a benchmark work, ImageCLEFmed gets more and more well-known owing to its open data platform.</p><p>Two tasks are published in ImageCLEFmed 2005: retrieval and annotation. We target at the automatic annotation task, which is the first time to be published. In this task, 9,000 radiographs are classified into 57 classes (see Figure <ref type="figure" coords="1,242.78,594.36,3.61,9.02">1</ref>), which can be taken as training datasets; and 1,000 unlabeled radiographs are taken as the test dataset. As the first stage, the annotation task is very simple: automatic labeling the 1,000 images is the whole object of the annotation system. Evaluation of the system will base on the 'error rate', which means the percent of how many images are wrongly classified.</p><p>Medical image annotation can be regarded as an interpretation to medical evidence, while in this research, evidences are images. Generally it is a doctor who uses the specialist vocabulary and natural language phrases to interpret those medical evidences, and relates them to some specific cases. For automatic machine-based reasoning based on the evidence gathered, additional interpretive semantics must be attached to the data. Some methods have been explored in special domains, like the diagnosis of breast cancer <ref type="bibr" coords="1,156.29,702.36,10.60,9.02" target="#b0">[1]</ref>. In the annotation task of ImageCLEFmed 2005, the annotation has been simplified to mark a class label for each medical image as one of 57 given classes. But in fact the images have been annotated with complete IRMA codes (both in English and in German), which are multi-axial codes for image annotation. In future the results of current annotation task will be used for further textual image retrieval tasks.</p><p>So by now, the annotation task can be taken as a multi-class classification problem, which is a great challenge for medical images with 57 classes. Compared with other classification problems, there are some particular difficulties for medical images:</p><p>Great unbalance between 57 classes; See Figure <ref type="figure" coords="2,155.13,122.76,3.76,9.02">2</ref>. In 57 training sets, class 6 has more than 500 samples (images), class 12 has more than 2,500 samples, class 34 has near 1,000 samples, while all the others are much less. 20 classes occupies near 80% of the whole training sets. This unbalance causes many common classification methods unavailable.</p><p>Visual similarities between some classes; See Figure <ref type="figure" coords="2,153.81,182.76,3.74,9.02" target="#fig_1">3</ref>. For people who are not experts of radiographs, it is impossible to find the differences between some classes visually. Variety in one class and difficulty in defining visual features. See Figure <ref type="figure" coords="2,150.70,219.45,3.51,8.10">4</ref>. Too many modalities vary in one class. To find a general visual feature for one class is often very difficult. It's more like an experimental work, where many features and methods have been tested based on simulation experiments.</p><p>In Section 2 feature extraction techniques are described; Section 3 overviews SVM; Section 4 gives the results of experiments; at last Section 5 gives the conclusion and future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Feature sets</head><p>Feature extraction is a basic problem in image processing field. In <ref type="bibr" coords="2,356.68,318.36,11.68,9.02" target="#b1">[2]</ref> there are 56 CBIR (content-based image retrieval) systems reviewed, and a summary of low-level features are listed in 3 main categories: color, texture, and shape, plus 2 single features: layout, face detection. In <ref type="bibr" coords="2,389.23,342.36,23.39,9.02">[3][4]</ref> there are some similar classifications of features.</p><p>'Color' includes dominant colors, region histogram, color coherence vector, color moments, correlation histogram, global/ sub-image histogram, eigen image, etc.</p><p>'Texture' includes edge statistics (edge image histogram, edge orientation histogram), local binary patterns, random field decomposition, atomic texture features (contrast, anisotropy, density, randomness, directionality, softness, uniformity, often variations of Tamura features, and often derived from a concurrence matrix of pixel values). The results of wavelet decomposition, Gabor filter and Fourier filters, etc., are also taken as texture features in <ref type="bibr" coords="2,312.38,438.36,10.64,9.02" target="#b1">[2]</ref>.</p><p>'Shape' includes elementary descriptors (centroid, area, orientation, length of major and minor axes, eccentricity, circularity, and features derived from algebraic moments), bounding box/ellipse, curvatures scale space, elastic models, Fourier descriptors, template matching, edge direction histogram, etc.</p><p>The feature 'layout' is the absolute or relative spatial position of the pixels. It may include low-resolution-pixel-map (LRPM), which is used in our method.</p><p>In the annotation task of ImageCLEFmed 2005, all provided images are black and white. So texture seems to be a more powerful feature than color. To judge the influence of noises, texture maps are calculated on both initial images and filtered images. Moreover, texture histogram is calculated on those texture maps. In Table <ref type="table" coords="2,207.72,558.36,3.76,9.02" target="#tab_0">1</ref>, three texture features are considered: contrast, anisotropy, and polarity, where 'h' means the height of an image and 'w' means its width. A~F gives different sets of features. In Figure <ref type="figure" coords="2,118.61,582.36,3.76,9.02">5</ref>, there is a small example of textures and LRPM.</p><p>As we can see from Table <ref type="table" coords="2,220.77,594.36,3.77,9.02" target="#tab_0">1</ref>, if images' sizes are different, the lengths of the feature vectors will also be different. In this case, LRPMs are used to unify all the images to the size 16x16. On the other hand, LRPMs can reduce the feature vectors' length. We didn't try other sizes except 16x16 because in <ref type="bibr" coords="2,90.00,630.36,11.71,9.02" target="#b2">[3]</ref> it shows that the sizes of LRPM have no obvious influence to the results. Besides of texture, regional features are also considered, such as Blob <ref type="bibr" coords="3,403.91,74.76,10.62,9.02" target="#b5">[6]</ref>. A Blob's parameters include: color, texture, area, length of long axis and short axis, rotation angle, Fourier decomposition parameters, etc. It has been applied successfully in medical image retrieval in our past work <ref type="bibr" coords="3,459.55,98.76,10.63,9.02" target="#b6">[7]</ref>.</p><p>Facing all kinds of features, it is really very difficult to find out which is more valuable than the others. And it is impossible to use all the features at the same time. So feature selection becomes a key problem. The 'best' features should be the most distinguishing features, and be invariant to different transformations of the input. To choose the best features is a very difficult theoretical problem. A practical way is to do simulation experiments, so as to select suitable features from the best result of classification.</p><p>Through simulation experiments, three kinds of LRPMs are finally chosen as features used in our work. One is from the initial images; the other two are from the maps of texture features: contrast and anisotropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Classification methods</head><p>According to <ref type="bibr" coords="3,146.43,246.36,10.61,9.02" target="#b7">[8]</ref>, roughly speaking, classification methods can be divided into the parametric and the nonparametric. Parametric methods include Bayesian estimation (Maximum-Likelihood, Hidden Markov models, Expectation-Maximization, Fisher Linear Discriminant, Multiple Discriminant Analysis, etc), Linear Discriminant functions (Perceptron Criterion Function, Relaxation Procedures, Minimum Squared-Error Procedures, Principle Component Analysis, Support Vector Machines, Ho-Kashyap Procedures, etc), Multi-layer Neural Networks, Stochastic methods (Simulated Annealing, Boltzmann learning, Evolutionary methods, etc).</p><p>SVM is chosen in our program. It is a method widely used for statistical learning, and classifiers and regression models designing. Primarily SVM tackles the binary classification problem. The objective is to find an optimal separating hyper-plane (OSH) that correctly classifies feature data points as much as possible and separates the points of two classes as far as possible. The approach is to map the training data into a higher dimensional (possibly infinite) space and formulate a constrained quadratic programming for the optimization. Different mappings construct different SVMs <ref type="bibr" coords="3,455.49,390.36,10.61,9.02" target="#b8">[9]</ref>.</p><p>SVM for multiple-classes classification is still under development. Generally there are two types of approaches. One type has been to incorporate multiple class labels directly into the quadratic solving algorithm. Another more popular type is to combine several binary classifiers. We used SVM Torch , which belongs to the latter.</p><p>Kernel selection is a crucial issue for SVM. Kernels introduce different nonlinearities into the SVM problem by mapping input data into Hilbert space via a mapping function where it may then be linearly separable. Different kernels will accommodate different nonlinear mappings and the performance of the resulting SVM will often hinge on the appropriate choice of the kernel <ref type="bibr" coords="3,486.16,486.36,15.36,9.02" target="#b9">[10]</ref>. Generally it is impossible to judge in advance which kernel is the best for classification, and trial-and-error method is a common way to select kernels. There are 4 kernels in SVM Torch : linear, polynomial, radial basis function (RBF), sigmoid tanh.</p><p>( , ) (2) <ref type="figure" coords="3,254.88,583.57,14.24,9.00">x y</ref> x y i (3) Eq. ( <ref type="formula" coords="3,131.29,595.56,3.90,9.02" target="#formula_0">1</ref>) results in a classifier that is a polynomial of degree p in the data; Eq. ( <ref type="formula" coords="3,426.91,595.56,3.90,9.02">2</ref>) gives a Gaussian radial basis function classifier, and Eq. ( <ref type="formula" coords="3,265.76,607.56,3.89,9.02">3</ref>) gives a particular kind of two-layer sigmoidal neural network. For the RBF case, the number of centers, the centers themselves, the weights, and the threshold are all produced automatically by the SVM training and give excellent results compared to classical RBFs, for the case of Gaussian RBFs. For the neural network case, the first layer consists of N sets of weights, each set consisting of d L (the dimension of the data) weights, and the second layer consists of N weights, so that an evaluation simply requires taking a weighted sum of sigmoids, themselves evaluated on dot products of the test data with the support vectors. Thus for the neural network case, the architecture (number of weights) is determined by SVM training. Note that the hyperbolic tangent kernel only satisfies Mercer's condition for certain values of its parameters <ref type="bibr" coords="3,469.52,703.56,15.03,9.02" target="#b10">[11]</ref>.</p><formula xml:id="formula_0" coords="3,243.18,531.15,79.00,12.24">K = + x y x y i<label>( 1) p</label></formula><formula xml:id="formula_2" coords="3,243.18,579.78,101.68,12.94">( , ) tanh( ) K κ δ = -</formula><p>RBF is chosen in our program, and the standard variance σ is its parameter. Another parameter c is the trade-off between training error and the margin.</p><p>PCA (Principle Component Analysis) <ref type="bibr" coords="3,264.42,739.56,11.70,9.02" target="#b4">[5]</ref> is also tried in our experiments. But owing to the serious unbalance of the training dataset, it can not overcome the over-fitting problem. In Section 4 more details about this will be shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and result analysis</head><p>Starting from 9,000 labeled training images (see Figure <ref type="figure" coords="4,316.10,114.36,5.01,9.02">2</ref> and Table <ref type="table" coords="4,365.96,114.36,3.61,9.02" target="#tab_1">2</ref>), the goal is to classify the 1,000 given unlabeled images. In this process, there are 3 key points: feature selection, method selection, parameter selection. All of these can be decided by simulation experiments.</p><p>• What is the simulation experiment?</p><p>In 'simulation experiments', both the training images and test images are coming from the given training dataset. In this case, the ground-truth of the test dataset is known. Then it is possible to evaluate the result of an experiment. From the feedback of simulation experiments, we can choose the best features, suitable method, and best parameters.</p><p>Of course different partitions of the 9,000 training images will cause different results. Thus the simulation experiments should be preceded under various partitions, various features, various methods, and various parameters. Some ratios are assigned to the partitions. Each ratio is applied in each of 57 training sets separately. For example, if defined the ratio to 4:1, in each of 57 training sets, 80% images will be taken as training data and the left 20% will be regarded as test data. Ensuring each class has some training data and corresponding test data, is obviously better than dividing the whole training dataset randomly, in which way, the effect of the classification method cannot be seen clearly because there may be some classes disappearing at the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Simulation experiments with PCA</head><p>First of all, the PCA method is tested in simulation experiments. This is owing to in our past work <ref type="bibr" coords="4,90.00,354.36,10.65,9.02" target="#b4">[5]</ref>, PCA plus Blob features made a good result in image retrieval. In Figure <ref type="figure" coords="4,412.95,354.36,3.75,9.02">6</ref>, it is shown the best result of the simulated experiment on PCA plus Blob. The average accuracy (AA) of the classification is 0.5026.</p><p>The AA mentioned here is equal to correctness rate, which is calculated according to the following formula:</p><formula xml:id="formula_3" coords="4,222.06,417.93,283.28,19.75">_ _ _ _ _ _ _ number of images classified correctly AA size of test dataset = (4)</formula><p>For example, in our case, the size of test dataset is 3,512, and there are 1,765 images classified correctly, so the AA is 0.5026.</p><p>Figure <ref type="figure" coords="4,139.80,469.56,5.01,9.02">7</ref> shows the best result of the simulation experiment on PCA plus texture features. The AA is 0.6977. But using either Blob or texture features, PCA results in almost all the images trapped into a few 'attractive' classes, like 6, 12, 34. This is owing to their big sizes of corresponding training dataset, which are 576, 2563, 880 separately (see Table <ref type="table" coords="4,287.10,505.56,3.63,9.02" target="#tab_1">2</ref>), and the 3 highest peaks in Figure <ref type="figure" coords="4,441.91,505.56,3.77,9.02">2</ref>. They occupy 44.65% of all the 9,000 training images. This obvious unbalanced distribution between different classes in training dataset can explain why the AA is not so low even in such a bad classification distribution (nearly all of the images are classified into the 3 classes).</p><p>If purely from the viewpoint of AA to evaluate the result of PCA for annotation, it's not too bad. But in Figure <ref type="figure" coords="4,147.75,565.56,5.01,9.02">6</ref> and Figure <ref type="figure" coords="4,202.79,565.56,3.74,9.02">7</ref>, PCA is proved to be inoperative in solving such a seriously unbalanced problem, because there are too many empty classes. Like in Figure <ref type="figure" coords="4,383.72,577.56,3.76,9.02">7</ref>, only 3 classes have some classified samples but all the other classes have none.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Simulation experiment with SVM</head><p>Too much training data existing in class 6, 12, 34 cause the risk of over-fitting. However, this risk can be controlled at least for simple noise models, e.g. models with constant noise levels, using soft margin SVM with specific sequences of regularization parameters <ref type="bibr" coords="4,360.27,649.56,15.33,9.02" target="#b11">[12]</ref>. In SVM Torch , the parameter for the soft margin is C, and the parameter for the standard variance is std.</p><p>In our experiment, SVM plus different features is tested: SVM+ Blob (Figure <ref type="figure" coords="4,191.96,686.25,3.88,8.10">8</ref>) SVM+ LRPM (Figure <ref type="figure" coords="4,199.70,698.25,3.88,8.10">9</ref>) SVM+ texture (Figure <ref type="figure" coords="4,203.17,710.25,8.18,8.10">10</ref>) SVM+ LRPM + texture (Figure <ref type="figure" coords="4,238.09,722.25,7.98,8.10">11</ref>) SVM+ Blob + LRPM + texture (Figure <ref type="figure" coords="4,268.13,734.25,8.23,8.10">12</ref>) As shown in the above figures as well as Figure <ref type="figure" coords="4,306.37,746.25,7.66,8.10">14</ref>, 'SVM + all (=Blob+LRPM+texture)' reaches the highest AA (0.8890).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison between methods:</head><p>From those figures we can see that SVM are better than PCA because SVM can 'recognize' more classes. Further more, SVM can reach higher AA compared with PCA.</p><p>Comparison between features: Both by SVM, LRPM features (AA: 0.7725) seems to be better than Blob features (AA: 0.6311). This shows that in this task low-level features are more powerful than middle-level features, although Blob plays well in image retrieval field. Besides texture feature seems to be the best among the three with its AA 0.8318. Further more, the combination of the three kinds of features can reach a highest AA=0.8890. This is owing to different features reflecting different attributes of images, so that their combination can make a better result.</p><p>Influence of training dataset Generally speaking, different training datasets will cause different classification results, and the larger the training dataset is, the better the result will be. But in our case, larger training dataset may make the result worse owing to over-fitting problem. E.g., Figure <ref type="figure" coords="5,354.10,230.76,10.05,9.02">12</ref> uses only 600 samples out of 2563 samples from the training dataset of class 12, and its AA is 0.8890; if using 1000 samples to train the classifier, its AA only can reach 0.8727; when using 1500 samples, the AA lowers down to 0.8413.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• The real experiments and results</head><p>With the conclusions from above simulation experiments, the 'SVM+Blob+LRPM +texture' method is chosen at last, which seems to be the best combination of method and features. But unfortunately, in our submission result to ImageCLEFmed 2005, Blob hasn't been included, which resulted in an AA 0.7940.</p><p>Our latest result is 0.8070 (Figure <ref type="figure" coords="5,247.77,339.45,7.41,8.10" target="#fig_1">13</ref>), in which, AAs of 11 classes are higher than 0.9, containing 505 correctly classified images in all 515 images; AAs of 23 classes are from 0.5 to 0.9, containing 270 right classified images in all 356 images; in the last 23 classes with the AAs below 0.5, there are 129 images in total, and only 32 images are right classified. This shows that, with the features above, SVM is not good at classifying small classes with few samples.</p><p>In the following we discuss some factors influencing the AAs. Threshold of sizes of training datasets According to Figure <ref type="figure" coords="5,194.48,423.45,4.50,8.10">2</ref> and Table <ref type="table" coords="5,243.25,423.45,3.50,8.10" target="#tab_1">2</ref>, there is a serious unbalance among the training datasets. The largest class contains 2563 samples (class 12), while the smallest class has only 9 samples (class 51, 52). In many cases, too many training samples will cause the over-fitting problem. One of the solutions is to define a threshold to limit the numbers of training samples. For example, the threshold is set to 300. Then each of 57 training datasets is to be limited to 300 training samples.</p><p>But as shown in Figure <ref type="figure" coords="5,206.79,482.76,8.35,9.02">15</ref>, using soft margin SVM, when the threshold increases from 500, it will do little influence to AA. If using PCA, the threshold will do great influence to AA. So on this point, the performance of SVM is much better than PCA.</p><p>Parameters selection of SVM For SVM's kernel RBF, there are two parameters: variance std and margin C. In Figure <ref type="figure" coords="5,151.34,542.76,12.52,9.02">16,</ref><ref type="figure" coords="5,166.83,542.76,11.06,9.02">(a)</ref> shows the influence of std, and (b) shows the influence of C. It can be seen that std does more influence to AA while C does little.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Error analysis</head><p>In the information retrieval field, the most common used item to evaluate the retrieval results is PR curve (precision recall curve). Though in our case it is a classification problem, we keep on using them as evaluation figures. The difference is there will be no curves but graphs. A 'curve' is used to describe the effect of different parameters of a retrieval method, and one curve only corresponds to one class. A 'graph' can show all the classes' position in the same coordinates simultaneously, and describe the effect of the classification method. It is more suitable to illustrate the results of a multi-class classifier.</p><p>An example of this kind of graph is shown in Figure <ref type="figure" coords="5,324.69,674.76,8.53,9.02">17</ref>: G is the best region because the points in this region have high recall and precision; B is the worst region because its points have low recall and precision. For a multi-class problem, a convincible result should let its most classes fall in region G. As for ours, related to Figure <ref type="figure" coords="5,196.60,710.76,8.35,9.02" target="#fig_1">13</ref>, in Figure <ref type="figure" coords="5,251.49,710.76,10.00,9.02">17</ref> there are more than half of the classes (around 53%) falling in G.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,197.46,198.32,200.49,8.32;8,306.00,115.80,117.00,78.00"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Visual similarities between some classes</figDesc><graphic coords="8,306.00,115.80,117.00,78.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,165.06,311.84,265.29,8.32;8,198.00,215.22,216.00,86.46"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. Variety in one class and difficulty to define visual features</figDesc><graphic coords="8,198.00,215.22,216.00,86.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,184.86,579.50,225.60,8.33;8,171.66,419.40,252.00,155.16"><head>Figure 6 .Figure 7 .Figure 8 .Figure 9 .Figure 12 .</head><label>678912</label><figDesc>Figure 6. Classification result of PCA using Blob features</figDesc><graphic coords="8,171.66,419.40,252.00,155.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,127.32,616.47,130.08,8.10"><head>Figure 14 .Figure 15 .Figure 17 .</head><label>141517</label><figDesc>Figure 14. Different features vs. AA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,127.14,642.74,329.06,119.46"><head>Table 1 .</head><label>1</label><figDesc>Texture feature sets</figDesc><table coords="2,127.14,656.58,329.06,105.62"><row><cell></cell><cell>contrast</cell><cell>anisotropy</cell><cell>polarity</cell></row><row><cell>Original Image (A)</cell><cell>h x w</cell><cell>h x w</cell><cell>h x w</cell></row><row><cell>Filtered Image (B)</cell><cell>h x w</cell><cell>h x w</cell><cell>h x w</cell></row><row><cell>Histogram of A (C)</cell><cell>256 x 1</cell><cell>256 x 1</cell><cell>256 x 1</cell></row><row><cell>Histogram of B (D)</cell><cell>256 x 1</cell><cell>256 x 1</cell><cell>256 x 1</cell></row><row><cell>Filtered C (E)</cell><cell>256 x 1</cell><cell>256 x 1</cell><cell>256 x 1</cell></row><row><cell>Filtered D (F)</cell><cell>256 x 1</cell><cell>256 x 1</cell><cell>256 x 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,162.12,458.54,271.11,220.68"><head>Table 2 .</head><label>2</label><figDesc>Sample numbers of 57 given classes (training datasets)</figDesc><table coords="11,162.12,480.36,271.11,198.86"><row><cell>Class</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell cols="3">number 336 32</cell><cell cols="5">215 102 225 576 77</cell><cell>48</cell><cell>69</cell></row><row><cell>Class</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell></row><row><cell cols="7">number 32 108 2563 93 152 15</cell><cell cols="3">23 217 205</cell></row><row><cell>Class</cell><cell>19</cell><cell>20</cell><cell>21</cell><cell>22</cell><cell>23</cell><cell>24</cell><cell>25</cell><cell>26</cell><cell>27</cell></row><row><cell cols="3">number 137 31</cell><cell>194</cell><cell>48</cell><cell>79</cell><cell cols="4">17 284 170 109</cell></row><row><cell>Class</cell><cell>28</cell><cell>29</cell><cell>30</cell><cell>31</cell><cell>32</cell><cell>33</cell><cell>34</cell><cell>35</cell><cell>36</cell></row><row><cell cols="3">number 228 86</cell><cell>59</cell><cell>60</cell><cell>78</cell><cell cols="3">62 880 18</cell><cell>94</cell></row><row><cell>Class</cell><cell>37</cell><cell>38</cell><cell>39</cell><cell>40</cell><cell>41</cell><cell>42</cell><cell>43</cell><cell>44</cell><cell>45</cell></row><row><cell cols="3">number 22 116</cell><cell>38</cell><cell>51</cell><cell>65</cell><cell>74</cell><cell cols="3">98 193 35</cell></row><row><cell>Class</cell><cell>46</cell><cell>47</cell><cell>48</cell><cell>49</cell><cell>50</cell><cell>51</cell><cell>52</cell><cell>53</cell><cell>54</cell></row><row><cell cols="3">number 30 147</cell><cell>79</cell><cell>78</cell><cell>91</cell><cell>9</cell><cell>9</cell><cell>15</cell><cell>46</cell></row><row><cell>Class</cell><cell>55</cell><cell>56</cell><cell>57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">number 10</cell><cell>15</cell><cell>57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">Conclusions and future work</head><p>SVM plus some texture features and blob features reaches an AA of 80.7%, while according to the published results of the annotation task in a link to ImageCLEF's website, the highest AA of ImageCLEFmed is 87.4%. It means 67 right-classified samples more than ours.</p><p>For SVM's parameter's tuning, it is proven in our experiments that only std makes sense for better result. Other kernels except RBF are tried but no better.</p><p>PR graph is helpful to judge the effects of classification algorithms.</p><p>As we can see in our work, features are the most important factor for image classification. In the future, new features should be mined. As for methods, neural network like HMM should be tried.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,111.00,214.65,394.41,8.10;6,111.00,226.65,394.39,8.10;6,111.00,238.65,170.30,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,313.02,214.65,192.39,8.10;6,111.00,226.65,70.00,8.10">Ontology-based Medical Image Annotation with Description Logics</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dasmahapatra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shadbolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,189.78,226.90,315.61,7.85;6,111.00,238.90,41.29,7.85">Proceedings of The 15th IEEE International Conference on Tools with Artificial Intelligence</title>
		<meeting>The 15th IEEE International Conference on Tools with Artificial Intelligence<address><addrLine>Sacramento, CA, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,111.00,250.65,394.48,8.10;6,111.00,262.65,230.97,8.10" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,248.80,250.65,186.95,8.10">Content-Based Image Retrieval Systems: A Survey</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Remco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mirela</forename><surname>Veltkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tanase</surname></persName>
		</author>
		<idno>UU-CS-2000-34</idno>
		<ptr target="http://give-lab.cs.uu.nl/cbirsurvey/" />
		<imprint>
			<date type="published" when="2000-10">Oct. 2000</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="6,111.00,274.65,394.34,8.10;6,111.00,286.65,394.35,8.10;6,111.00,298.65,183.83,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,467.33,274.65,38.01,8.10;6,111.00,286.65,302.18,8.10">Automatic Categorization of Medical Images for Content-based Retrieval and Data Mining</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Güld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,421.26,286.90,84.09,7.85;6,111.00,298.90,79.82,7.85">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="143" to="155" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,111.00,310.65,394.36,8.10;6,111.00,322.65,394.30,8.10;6,111.00,334.65,242.97,8.10" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,177.66,310.65,213.24,8.10">A Survey on: Contents Based Search in Image Databases</title>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Johansson</surname></persName>
		</author>
		<idno>LiTH-ISY-R-2215</idno>
		<ptr target="http://www.cvl.isy.liu.se/ScOut/TechRep/" />
		<imprint>
			<date type="published" when="2000-02">Feb., 2000</date>
			<pubPlace>SWEDEN</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Technical Reports from the Computer Vision Laboratory, Dept. of Electrical Engineering, Linköping University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,111.00,346.65,394.34,8.10;6,111.00,358.65,256.96,8.10" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,396.29,346.65,109.05,8.10;6,111.00,358.65,211.89,8.10">Content-based Medical Image Retrieval Using Dynamically Optimized Regional Features</title>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sim</forename><forename type="middle">Heng</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kelvin</forename><surname>Foong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,111.00,370.65,394.29,8.10;6,111.00,382.65,394.29,8.10;6,111.00,394.65,20.28,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,364.97,370.65,140.33,8.10;6,111.00,382.65,104.09,8.10">Blobworld: A system for region-based image indexingand retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,222.66,382.89,278.78,7.85">Proceeding of Third International Conference Visual Information Systems</title>
		<meeting>eeding of Third International Conference Visual Information Systems</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,111.00,406.65,394.34,8.10;6,111.00,418.65,394.37,8.10;6,111.00,430.65,87.24,8.10" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,361.72,406.65,143.63,8.10;6,111.00,418.65,279.79,8.10">A novel content-based medical image retrieval method based on query topic dependent image features (QTDIF)</title>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>SPIE Medical Imaging</publisher>
			<pubPlace>San Diego, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,111.00,442.65,388.40,8.10;6,499.38,440.68,6.00,5.40;6,111.00,454.65,116.75,8.10" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,284.59,442.65,76.97,8.10">Pattern Classification</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>A Wiley-Interscience Publication</publisher>
		</imprint>
	</monogr>
	<note>2 nd ed.</note>
</biblStruct>

<biblStruct coords="6,111.00,466.65,394.33,8.10;6,111.00,478.65,394.40,8.10;6,111.00,490.65,350.51,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,248.83,466.65,256.50,8.10;6,111.00,478.65,71.88,8.10">Support vector machine pairwise classifiers with error reduction for image classification</title>
		<author>
			<persName coords=""><forename type="first">K.-S</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,189.54,478.89,315.86,7.85;6,111.00,490.65,64.45,8.10">Proceedings of the ACM Multimedia Workshop on Multimedia Information Retrieval (ACM MIR 2001)</title>
		<meeting>the ACM Multimedia Workshop on Multimedia Information Retrieval (ACM MIR 2001)<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computing Machinery</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="32" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,111.00,502.65,394.34,8.10;6,111.00,514.65,195.67,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,159.90,502.65,176.48,8.10">Multi-task feature and kernel selection for SVMs</title>
		<author>
			<persName coords=""><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,344.16,502.90,161.19,7.85;6,111.00,514.90,152.56,7.85">Proceedings of the twenty-first international conference on Machine learning ICML &apos;04</title>
		<meeting>the twenty-first international conference on Machine learning ICML &apos;04</meeting>
		<imprint>
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,111.00,526.65,394.33,8.10;6,111.00,538.65,108.79,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,169.53,526.65,234.58,8.10">A tutorial on support vector machines for pattern recognition</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,412.08,526.90,93.25,7.85;6,111.00,538.89,34.91,7.85">Data Mining Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="121" to="167" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,111.00,550.65,394.33,8.10;6,111.00,562.65,159.42,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,169.91,550.65,290.46,8.10">On the Influence of the Kernel on the Consistency of Support Vector Machines</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Steinwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,467.40,550.89,37.93,7.85;6,111.00,562.89,100.50,7.85">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="93" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
