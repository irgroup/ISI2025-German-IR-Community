<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,90.00,146.04,423.11,18.08;1,128.30,167.95,346.47,18.08">Combining Multilevel Visual Features for Medical Image Retrieval in ImageCLEFmed 2005</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,123.82,203.01,45.66,10.46"><forename type="first">Wei</forename><surname>Xiong</surname></persName>
							<email>wxiong@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,180.04,203.01,31.41,10.46"><forename type="first">Bo</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.01,203.01,34.32,10.46"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.88,203.01,68.97,10.46"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,346.40,203.01,40.14,10.46"><forename type="first">S</forename><forename type="middle">H</forename><surname>Ong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>117576</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,416.46,203.01,58.27,10.46"><forename type="first">Kelvin</forename><surname>Foong</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Preventive Dentistry</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>119074</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,90.00,146.04,423.11,18.08;1,128.30,167.95,346.47,18.08">Combining Multilevel Visual Features for Medical Image Retrieval in ImageCLEFmed 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B88620B3985B2F10B95BD5F697D2C3CA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>Measurement, Performance, Experimentation medical, image retrieval, ImageCLEFmed, hierarchical,multiple, feature, fusion, performance, mean average precision, experiments</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we report our work on the fully automatic medical image retrieval task in ImageCLEFmed 2005. First, we manually identify visually similar sample images by visual perception for each query topic. These help us understand the variations of the query topic and form templates for similarity measure. To achieve higher performance, two similarity measuring channels are used with each using different sets of features and operating in parallel. Their results are then combined to form a final score for similarity ranking. To improve efficiency, a pre-filtering process using other features is utilized to act as a coarse topic image filtering before the two similarity measures for fine topic retrieval. During retrieval, no relevance feedback is used. Only visual features are used in our experiments for all the topics including visually possible queries (topics 1-11), mixed visual/semantic queries (topics 12-22) and semantic (rather textual) queries (topics 23-25). Over 50,000 medical images our approach achieved a mean average precision of 14.6% for all 25 topics, ranked as the best-performance run for the automatic medical image retrieval task in the ImageCLEFmed 2005.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEFmed is a subtask of ImageCLEF. In 2005, it offers tasks for both system-centered and user-centered evaluation of image retrieval systems. One of the four tasks offered is the Image-CLEFmed which contains two medical-related sub tasks: medical image retrieval and automatic annotation for medical images. This paper reports our work in the sub-task of medical image retrieval. We confine our efforts in automatic runs using visual features only. The test data for this sub-task consist of four components: Casimage (containing 8725 radiology pathology images), MIR (containing 1177 nuclear medicine images from Mallinckrodt Institute of Radiology), PEIR (containing 32319 pathology and radiology images from Pathology Education Instructional Resource) and PathoPIC (containing 7805 pathology images). Besides the different imaging modalities and anatomic regions, these 50k+ images are of various sizes and image qualities. Some of them use 1color channel while others 3-color channels. Results of ImageCLEFmed 2004 <ref type="bibr" coords="2,427.50,194.04,10.52,10.46" target="#b3">[4]</ref> with 26 retrieval topics on the Casimage collection are provided as training data for ImageCLEFmed 2005. 25 query topics are provided in this track. Each of them contains topic statements in English, French and German, and a collection of images for each topic. Normally one or two example images for the desired result for the topic are supplied. One query also contains a negative example as a test. These queries are divided into visually possible queries (topics 1-11), mixed visual/semantic queries (topics 12-22) and semantic (rather textual) queries (topics 23-25). Since we will use visual characteristics alone, it would be very hard for us to handle topics 12-25. This has been proved by the submitted results of this forum: the best run of the mixtures of textual and visual retrievals is almost twice good as that of runs using visual-only retrievals.</p><p>Content-based image retrieval (CBIR) retrieves images in terms of their visual contents <ref type="bibr" coords="2,488.76,313.59,10.52,10.46" target="#b7">[8,</ref><ref type="bibr" coords="2,502.49,313.59,7.01,10.46" target="#b8">9]</ref>. We apply it into this medical image retrieval campaign. The low-level raw features we used include pixel-level features such as color (which are very local), regional-level features such as regional color, shape and texture (which combine local and global information), and image-level features such as color and texture statistical property (which are global). Before retrieval, we manually identify more visually similar sample images by visual perception for each query topic. We can also generate some synthetic images from these chosen images with reasonable visual varieties. This helps us understand the visual variations of the query topic. Take the example of topic 3 provided in this track which asks to show pathology images of an entire kidney. Figure <ref type="figure" coords="2,479.11,409.24,4.98,10.46">1</ref> shows the example image provided for this query topic together with three of many other instances of the query. They all contain an entire kidney but they differ in almost all visual properties. In fact, there are a lot of variations of an entire kidney in size, shape, color, and/or skin texture. It shows that one should not choose visual features from a single example alone. It also shows that the retrieval task in this campaign is rather challenging. In some sense, retrieval of each topic is like a task of face recognition which has been proven to be very difficult <ref type="bibr" coords="2,407.38,480.97,14.62,10.46" target="#b13">[14]</ref>.</p><p>Figure <ref type="figure" coords="2,122.14,589.22,7.75,10.46">1:</ref> A query topic has many instances visually different. The leftmost is provided as the query example while others are the instances chosen.</p><p>The sample images chosen for each topic are used as training data to design similarity measuring functions. We have used three functions η 1 , η 2 and η 3 for each topic with three different sets of features, respectively. The first function η 1 serves as a filter to remove those dissimilar images. This improves efficiency as many images can be excluded in the next comparison stage. Next, η 2 and η 3 operates in parallel <ref type="bibr" coords="2,224.27,672.25,15.50,10.46" target="#b11">[12]</ref> to yield two similarity measures. Then they are combined to produce a final score for image ranking. During retrieval, no relevance feedback is used anymore.</p><p>We submitted seven runs of experiments. All use visual features only but cover all the 25 topics including the visually possible queries (topics 1-11), mixed visual/semantic queries (topics 12-22) and semantic (rather textual) queries (topics 23-25). Five runs achieved top performance in this sub-task. We covered the top five runs in this subtask.</p><p>In the following sections, we introduce our work in more detail. The features used are explained in Section 2, followed by the retrieval methodology presented in Section 3. Section 4 explains our experiments. Conclusions are then drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multiple Feature Descriptions</head><p>In this section, we describe the visual features used in our work. A survey of visual features useful for general CBIR can be found in <ref type="bibr" coords="3,239.55,189.03,14.62,10.46" target="#b12">[13]</ref>. We have employed color, shape, texture characteristics at the pixel level, the region level and the entire image level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Global Color and Layout Property</head><p>We have noticed that some of the query examples provided are colorful while others are gray. Those color images use three color channels. Most of gray images use one channel only. However there are some that still employ three channels. This channel information can be used directly to classify images. Furthermore, the layouts of images are different consistently. For example, the ultrasound images are almost triangles. These features form description set F 1 . They look more similar visually than their original versions. These so-called "icons" are extensively used in face recognition and have proven to be effective <ref type="bibr" coords="3,300.68,437.01,9.97,10.46" target="#b1">[2]</ref>. We have also applied them to medical image retrieval <ref type="bibr" coords="3,129.07,448.96,14.62,10.46" target="#b11">[12]</ref>. We call them description set F 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Low Resolution Pixel Map at Pixel Level</head><p>Figure <ref type="figure" coords="3,160.33,571.73,3.88,10.46">2</ref>: Examples of original images and their respective low-resolution maps</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Blob Feature at Object/Image Level</head><p>We consider both regional and image-wide color, texture and shape features. Besides the pixel color, local contrast, anisotropy and polarity are captured to form a joint color-texture-spatial feature vector space. Gaussian-mixture models are built locally. The purpose is to define homogeneous regions in the feature space quantitatively. Numbering less than 5, the number of Gaussians for each region may be different from each other. All the model parameters are found using the EM algorithm. These regions are extracted by a region merging process. The merged regions are segmented and referred to as meaningful local objects. The color, texture and shape properties of the regions are computed. The largest 10 regions are identified and obtained separately. Mean values and statistical properties of color, texture features and area are counted. The contour of each region is represented by elliptical Fourier expansion descriptors. We use the first 20 coefficients. Each region can be represented by the ellipse reconstructed from the first order of the expansion. These are the so-called blob representation <ref type="bibr" coords="4,337.78,134.27,9.97,10.46" target="#b2">[3]</ref>. Three pairs of examples are shown in Figure <ref type="figure" coords="4,134.89,146.22,4.98,10.46" target="#fig_1">3</ref> where for each pair the left one is the original image and the right one is its blob representation. We have also included the global color histogram and texture histogram over the whole image. The above regional features and the global features form feature set F 3 , called as "blob" in this paper. The feature vector is of 352-dimension. For a more detailed description of the specific usage, see our previous work <ref type="bibr" coords="4,269.63,194.04,15.50,10.46" target="#b10">[11,</ref><ref type="bibr" coords="4,288.45,194.04,11.63,10.46" target="#b11">12]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Retrieval Methodology</head><p>The section presents the retrieval methodology used in this work. The basic processing flow of our approach is illustrated in Figure <ref type="figure" coords="4,236.68,394.93,3.88,10.46" target="#fig_2">4</ref>. Above the dashed horizontal line are processing procedures for any given query and below the line are procedures for the images in the test databases.</p><p>Before retrieval, we browse the four databases provided especially the CASImage database used for training. For j-th query topic, j=1,. . . ,25, some more semantically similar images (which may be visually different) are chosen to form a set Q j n of n training images. For each image, some raw features, such as color, geometrical and texture properties, are extracted to form a p × 1 feature vector</p><formula xml:id="formula_0" coords="4,119.92,466.66,149.31,11.36">x i = (x i1 , x i2 , • • • , x ip ) for image i.</formula><p>Principal components are analyzed upon a set of such features. (We will provide more details later.) An eigenspace E j is set up for each query topic j, j = 1, . . . , 25. Feature dimension may also be reduced, which is illustrated as a dashed box. The feature vector of a test image x i = (x i1 , x i2 , . . . , x ip ) is then projected to E j . The similarity is measured in E j .</p><p>This procedure is repeated for all test images to generate a similarity ranked list for them. For the test image, a pre-filtering is introduced using F 1 . Those images that are impossible to be similar are excluded earlier (denoted by "N"). Only those which can pass (indicated by "Y") will go to the final comparison stage. In this final stage, two parallel engines are introduced for similarity measures. They use independent sets of features "icon" and "blob", representing local and global characteristics, respectively.  More specifically, we analysis principal components and utilize them in two ways. The first is for feature dimension reduction. The second is used to design similarity measuring functions <ref type="bibr" coords="5,90.00,134.27,15.50,10.46" target="#b9">[10,</ref><ref type="bibr" coords="5,108.23,134.27,11.63,10.46" target="#b11">12]</ref>. Given a training dataset Q j n with n images: X = (x 1 , x 2 , • • • , x n ) T , the generating matrix can be constructed as <ref type="bibr" coords="5,187.83,146.22,15.50,10.46" target="#b9">[10,</ref><ref type="bibr" coords="5,206.65,146.22,12.73,10.46" target="#b11">12]</ref> </p><formula xml:id="formula_1" coords="5,232.03,166.31,280.98,13.19">C 1 = XX T , or, C 2 = X T X.<label>(1)</label></formula><p>Here C 1 is of n×n and C 2 is of p×p. As mentioned before, n is the number of images/vectors and p is the number of features. C 1 is used to generate templates of this dataset and C 2 is used to reduce the dimension of the feature space when necessary. Supposing m out of n eigenvalues (λ i ) and their eigenvectors (u i ) are chosen based on C 1 . From the eigenvector matrix</p><formula xml:id="formula_2" coords="5,90.00,224.85,423.00,24.39">U = (u 1 , u 2 , • • • , u m ) T , the template vectors V = (v 1 , v 2 , • • • , v m )</formula><p>T are given by using</p><formula xml:id="formula_3" coords="5,230.59,257.75,278.16,25.54">v i = 1 √ λ i X T u i , i = 1, ..., m. (<label>2</label></formula><formula xml:id="formula_4" coords="5,508.76,264.49,4.24,10.46">)</formula><p>Given a test image set, its feature matrix Y is reconstructed by</p><formula xml:id="formula_5" coords="5,271.25,311.72,241.75,12.29">Y = VV T Y,<label>(3)</label></formula><p>with a least square error</p><formula xml:id="formula_6" coords="5,268.89,347.42,239.86,10.46">s = Y -Y . (<label>4</label></formula><formula xml:id="formula_7" coords="5,508.76,347.42,4.24,10.46">)</formula><p>The similarity-measuring functions η 2 and η 3 have the same form of this error but with different feature sets as parameters.</p><p>The weighted summation rule <ref type="bibr" coords="5,238.42,389.26,10.52,10.46" target="#b0">[1,</ref><ref type="bibr" coords="5,252.27,389.26,7.75,10.46" target="#b5">6]</ref> is used to fuse the two similarity measures:</p><formula xml:id="formula_8" coords="5,264.05,411.18,244.71,11.36">d = w 1 s 1 + w 2 s 2 , (<label>5</label></formula><formula xml:id="formula_9" coords="5,508.76,411.18,4.24,10.46">)</formula><p>where s 1 and s 2 are the similarity computed above using different feature sets F 1 and F 2 whereas w 1 and w 2 are weighted coefficients subject to 0 ≤ w 1 , w 2 ≤ 1, w 1 +w 2 = 1. The resulting distance d serves as the final score for ranking: the larger the score is, the less similar the query and the test image are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this campaign, we use visual features alone for all topic retrieval tasks including those that require text information. Experiments start from choosing training data. For each topic, we manually choose some images in the test database to represent the visual varieties of the query topic. Three undergraduate engineering students without medical background select these images. The only criteria are the visual appearance of the images. Consequently, it is no doubt there are many images wrongly chosen and the numbers of images are larger. The more correct the visual varieties of the query topic we can collect into the training set, the better the representation is semantically. This is done offline and before the retrieval.</p><p>We have also referred to the results from the baseline work from medGIFT <ref type="bibr" coords="5,440.59,619.37,9.97,10.46" target="#b6">[7]</ref>. Table <ref type="table" coords="5,487.10,619.37,4.98,10.46" target="#tab_2">2</ref> lists the number of images collected for each query topic. Here "q", "a" and "b" refers to the query topic and the number of images for two sets of training data, respectively. Total number of images in the training set "a" is 4789 with a mean 191.56 for each topic. In other words, 9.573% of the 50026 test images are used for training, which is a small portion. For training set "b", total there are 3874 images (i.e., 7.744% of the 50026 test images) with a mean 154.96 for each topic.</p><p>Next, we compute all the three feature sets F 1 , F 2 ("icon") and F 3 ("blob") for all images including those for training and testing. The similarity measuring function η 1 is a unit function for binary classification in terms of F 1 . Design η 2 and η 3 according to Equations (1) to (4) using F 2 and F 3 respectively. We combine their results according to Equation ( <ref type="formula" coords="5,391.84,726.97,4.24,10.46" target="#formula_8">5</ref>) with the same coefficients (w 1 = w 2 = 0.5). We submitted seven runs of retrievals. Table <ref type="table" coords="6,300.28,244.54,4.98,10.46" target="#tab_2">2</ref> lists their labels and their performance in terms of mean average precision (MAP). They are divided into 3 groups as shown in Table <ref type="table" coords="6,476.67,256.50,3.88,10.46" target="#tab_2">2</ref>. Only Group 3 "I2Rfus" utilizes all techniques mentioned above. Other runs in Groups 1 and 2 use parts of the techniques for comparison. In Group 1, two subgroups are further divided in terms of the feature sets used. Subgroup 1 uses "blob" and Subgroup 2 uses "icon". In each subgroup, we have two members, one using pre-filtering ("I2RbPBcf" and "I2RcPBcf") while others ("I2RbPBnf" and "I2RcPBnf") do not. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head><p>Run MAP Group Run MAP 1 (set "a") I2RbPBnf 0.1067 2 (set "a") I2Rfus 0.1455 1 (set "a") I2RcPBnf 0.1114 3 (set "b") I2RbP1nf 0.0928 1 (set "a") I2RbPBcf 0.1068 3 (set "b") I2RcP1nf 0.0934 1 (set "a") I2RcPBcf 0.1188 We observe that using the "icon" feature set gives normally slightly higher MAP than using the "blob" feature set. This is clear by comparing "I2RbPBnf" (0.1067) against "I2RcPBnf" (0.1068), and "I2RbPBcf" (0.1114) against "I2RcPBcf" (0.1188), respectively. The binary classifier in Stage 1 improves the entire system performance. To see this effect, we can compare "I2RbPBnf" (0.1067) against "I2RbPBcf" (0.1114) and "I2RcPBnf" (0.1068) against "I2RcPBcf" (0.1188), respectively. The improvement is more significant when using the "icon" feature set (11.24%) than using the "blob" set (with 4.4%). Group 2 is the fusion of "I2RcPBnf" and "I2RcPBcf" where the weights are equal. It achieves the best results (MAP=14.55%).</p><p>It is important to select more examples to form a training set for each query topic before retrieval. To have a comparison, some of images (the underlined numbers in Table <ref type="table" coords="6,448.65,561.64,4.43,10.46" target="#tab_1">1</ref>) are removed from the representation sets of some topics. We repeat experiments "I2RbPBnf" (0.1067) against "I2RcPBnf" (0.1068) but using these new training sets (Set "b"). This results in "I2RbP1nf" (using "blob" with MAP=0.0928) and "I2RcP1nf" (using "icon" with MAP=0.0934) in Group 3. Again, "icon" features have slightly better precision performance. Comparing experiments vertically using the two training sets, one finds that performance of Group 3 drops down using either feature set. This shows that the representation of the query topic using the training set is indeed important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and conclusion</head><p>We have reported our efforts to the medical image retrieval task in the ImageCLEFmed 2005. We analyzed the contents of images and employ three sets of visual features at different levels to represent each image. We start from manual selecting some training images for each topic before retrieval. These images construct a training set for us to span an eigenspace for the topic and to define similarity metrics for it. A pre-filtering process is used to act as a coarse topic image filtering before the two similarity measures for fine topic retrieval. The features are simple and the comparison is easy and fast. Many test images can be simply classified into impossible class reliably. To achieve higher performance, two similarity measuring channels are used. They use different sets of features and operate in parallel. Their results are then combined to form a final score for similarity ranking. We have not used relevance feedback during the retrieval.</p><p>In our experiments, only visual features are applied to not only the 11 visual-retrieval-possible topics, but also those 13 topics needing rather textual information. We have submitted seven runs in this track. Our best approach utilizes multiple sets of features with pre-filtering and fusing strategies, which enables us to achieve a very good performance in the visual-only group.</p><p>It should be noted that our work is based on the pre-selection of more example images for the query topic. The training data were chosen offline and it may be inconvenient for online applications. Both the qualities and the number of these images influence the retrieval performance. In our current efforts, they are still large for some topics and yet we have not refined them. Our future efforts would refine the sets and combine some machine learning techniques to facilitate the selection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,90.00,341.36,423.01,10.46;3,90.00,353.32,423.09,10.46;3,90.00,365.27,423.02,10.46;3,90.00,377.23,423.08,10.46;3,90.00,389.19,423.05,10.46;3,90.00,401.14,423.03,10.46;3,90.00,413.10,370.45,10.46"><head></head><label></label><figDesc>Images in the database, even in the same class, vary in size and may have translations. Resizing them into a thumbnail<ref type="bibr" coords="3,193.98,353.32,10.52,10.46" target="#b1">[2,</ref><ref type="bibr" coords="3,208.42,353.32,7.75,10.46" target="#b4">5]</ref> of a fixed size, through introducing distortions, may overcome the above difficulties in representing the same class of images in the database. It is a reduced and low-resolution version from the original image in the database ignoring its original size. Thus this can also remove noise in the high-frequency band. A 16-by-16 image pixel map, called an "icon", is used. Examples are shown in Figure 2. Here the left three images are original ones in the same class and the right three images are their respective reduced versions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,128.32,316.81,346.42,10.46;4,170.37,218.06,70.41,83.84"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of original images and their respective blob representations</figDesc><graphic coords="4,170.37,218.06,70.41,83.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,207.93,716.95,187.16,10.46"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A diagram of the processing flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,126.38,117.27,350.24,93.89"><head>Table 1 :</head><label>1</label><figDesc>Number of images in the training set for each of 25 query topics</figDesc><table coords="6,126.38,138.94,350.24,72.23"><row><cell>q</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell></row><row><cell cols="8">a 460 161 79 142 146 194 19</cell><cell>9</cell><cell cols="5">107 257 33 418 382</cell></row><row><cell cols="6">b 457 161 79 117 96</cell><cell>24</cell><cell>19</cell><cell>9</cell><cell cols="5">107 257 39 360 371</cell></row><row><cell cols="2">q 14</cell><cell cols="3">15 16 17</cell><cell>18</cell><cell>19</cell><cell>20</cell><cell>21</cell><cell>22</cell><cell>23</cell><cell>24</cell><cell>25</cell><cell></cell></row><row><cell cols="10">a 420 105 40 316 161 181 190 149 44</cell><cell cols="3">23 571 179</cell><cell></cell></row><row><cell cols="9">b 140 167 26 286 141 176 150 56</cell><cell>44</cell><cell>10</cell><cell cols="2">468 117</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,159.41,347.60,284.19,10.46"><head>Table 2 :</head><label>2</label><figDesc>MAPs of seven runs submitted to ImageCLEFmed 2005</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,110.48,344.45,402.52,10.46;7,110.48,356.41,151.51,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,221.42,344.45,219.86,10.46">Experimental evaluation of expert fusion strategies</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alkoot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,449.19,344.45,63.81,10.46;7,110.48,356.41,57.78,10.46">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1361" to="1369" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,376.33,402.57,10.46;7,110.48,388.29,402.52,10.46;7,110.48,400.24,183.55,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,399.89,376.33,113.16,10.46;7,110.48,388.29,207.27,10.46">Eigenfaces vs. fisherfaces: Recognition using class specific linear projection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">J</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,325.75,388.29,187.25,10.46;7,110.48,400.24,87.16,10.46">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,420.17,402.57,10.46;7,110.48,432.12,402.52,10.46;7,110.48,444.08,99.08,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,338.59,420.17,174.46,10.46;7,110.48,432.12,102.18,10.46">Recognition of images in large databases using color and texture</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,223.81,432.12,285.23,10.46">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1026" to="1038" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,464.00,402.59,10.46;7,110.48,475.96,402.61,10.46;7,110.48,487.91,402.55,10.46;7,110.48,499.87,402.58,10.46;7,110.48,511.82,150.78,10.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,353.89,464.00,159.19,10.46;7,110.48,475.96,60.66,10.46">The clef 2004 cross language image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,147.73,487.91,365.30,10.46;7,110.48,499.87,342.94,10.46">Lecture Notes in Computer Science (LNCS), Multilingual Information Access for Text, Speech and Images: Results of the Fifth CLEF Evaluation Campaign</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>in print</note>
</biblStruct>

<biblStruct coords="7,110.48,531.75,402.58,10.46;7,110.48,543.70,402.52,10.46;7,110.48,555.66,71.54,10.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,335.63,531.75,177.43,10.46;7,110.48,543.70,62.66,10.46">Visual features for content-based medical image retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Howarth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yavlinsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Heesch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,191.76,543.70,316.62,10.46">Proceedings of Cross Language Evaluation Forum (CLEF) Workshop 2004</title>
		<meeting>Cross Language Evaluation Forum (CLEF) Workshop 2004<address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,575.58,402.52,10.46;7,110.48,587.54,317.63,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,182.06,575.58,225.21,10.46">A theoretical study on six classifier fusion strategies</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,415.55,575.58,97.45,10.46;7,110.48,587.54,178.69,10.46">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="281" to="286" />
			<date type="published" when="2002">Feburary 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,607.46,402.61,10.46;7,110.48,619.42,402.58,10.46;7,110.48,631.37,402.52,10.46;7,110.48,643.33,402.57,10.46;7,110.48,655.28,304.02,10.46" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,324.19,607.46,188.91,10.46;7,110.48,619.42,246.29,10.46">Report on the clef experiments: Combining image and multi-lingual search for medical image retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Henning Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Geissbmühler</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,315.95,631.37,197.05,10.46;7,110.48,643.33,402.57,10.46;7,110.48,655.28,92.07,10.46">Lecture Notes in Computer Science (LNCS), Multilingual Information Access for Text, Speech and Images: Results of the Fifth CLEF Evaluation Campaign</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>in print</note>
</biblStruct>

<biblStruct coords="7,110.48,675.21,402.61,10.46;7,110.48,687.17,402.52,10.46;7,110.48,699.12,202.39,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,396.98,675.21,116.12,10.46;7,110.48,687.17,334.36,10.46">A review of content-based image retrieval systems in medicine -clinical benefits and future directions</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Henning Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Michoux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bandon</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,456.08,687.17,56.93,10.46;7,110.48,699.12,133.23,10.46">International Journal of Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,719.05,402.63,10.46;7,110.48,731.00,402.51,10.46;7,110.48,742.96,159.56,10.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,421.46,719.05,91.65,10.46;7,110.48,731.00,160.69,10.46">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,279.60,731.00,233.39,10.46;7,110.48,742.96,48.17,10.46">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,110.36,402.62,10.46;8,110.48,122.31,402.51,10.46;8,110.48,134.27,402.55,10.46" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,474.90,110.36,38.21,10.46;8,110.48,122.31,334.60,10.46">Contentbased medical image retrieval using dynamically optimized regional features</title>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Changsheng</forename><surname>Qi Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sim</forename><forename type="middle">Heng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kelvin</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Foong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,468.05,122.31,44.94,10.46;8,110.48,134.27,219.27,10.46">The IEEE International Conference on Image Processing 2005</title>
		<meeting><address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="8,110.48,154.19,402.61,10.46;8,110.48,166.15,402.55,10.46;8,110.48,178.10,180.33,10.46" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,415.86,154.19,97.23,10.46;8,110.48,166.15,397.16,10.46">A novel content-based medical image retrieval method based on query topic dependent image features (QTDIF)</title>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,110.48,178.10,85.51,10.46">Proceedings of SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5748</biblScope>
			<biblScope unit="page" from="123" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,198.03,402.61,10.46;8,110.48,209.98,402.60,10.46;8,110.48,221.94,402.55,10.46;8,110.48,233.89,32.41,10.46" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,158.18,209.98,354.90,10.46;8,110.48,221.94,93.71,10.46">Multipre : A novel framework with multiple parallel retrieval engines for contentbased image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sim</forename><forename type="middle">Heng</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kelvin</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Pierre</forename><surname>Chevallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,229.93,221.94,99.83,10.46">ACM Multimedia 2005</title>
		<meeting><address><addrLine>Hilton, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">November 2005</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="8,110.48,253.82,402.61,10.46;8,110.48,265.77,402.53,10.46;8,110.48,277.73,43.73,10.46" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,246.05,253.82,267.05,10.46;8,110.48,265.77,149.77,10.46">Survey on image content analysis, indexing, and retrieval techniques and status report of mepg-7</title>
		<author>
			<persName coords=""><forename type="first">Zijun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,269.29,265.77,195.58,10.46">Tamkang Journal of Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="101" to="118" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,297.65,402.62,10.46;8,110.48,309.61,22.70,10.46" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="8,350.47,297.65,158.35,10.46">Face recognition: A literature survey</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
