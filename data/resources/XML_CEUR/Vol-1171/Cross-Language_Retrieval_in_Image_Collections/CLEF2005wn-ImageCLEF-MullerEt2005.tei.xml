<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,95.52,148.73,411.70,15.51;1,211.32,170.69,180.20,15.51">Using medGIFT and easyIR for the ImageCLEF 2005 evaluation tasks</title>
				<funder ref="#_qNM7xXY">
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,121.44,204.07,66.28,9.96"><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<email>henning.mueller@sim.hcuge.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Service of Medical Informatics</orgName>
								<orgName type="institution">University and University Hospitals of Geneva</orgName>
								<address>
									<addrLine>24 Rue Micheli-du-Crest</addrLine>
									<postCode>CH-1211</postCode>
									<settlement>Geneva 4</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,195.55,204.07,86.56,9.96"><forename type="first">Antoine</forename><surname>Geissbühler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Service of Medical Informatics</orgName>
								<orgName type="institution">University and University Hospitals of Geneva</orgName>
								<address>
									<addrLine>24 Rue Micheli-du-Crest</addrLine>
									<postCode>CH-1211</postCode>
									<settlement>Geneva 4</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.79,204.07,53.44,9.96"><forename type="first">Johan</forename><surname>Marty</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Service of Medical Informatics</orgName>
								<orgName type="institution">University and University Hospitals of Geneva</orgName>
								<address>
									<addrLine>24 Rue Micheli-du-Crest</addrLine>
									<postCode>CH-1211</postCode>
									<settlement>Geneva 4</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.29,204.07,65.24,9.96"><forename type="first">Christian</forename><surname>Lovis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Service of Medical Informatics</orgName>
								<orgName type="institution">University and University Hospitals of Geneva</orgName>
								<address>
									<addrLine>24 Rue Micheli-du-Crest</addrLine>
									<postCode>CH-1211</postCode>
									<settlement>Geneva 4</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,424.07,204.07,57.18,9.96"><forename type="first">Patrick</forename><surname>Ruch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Service of Medical Informatics</orgName>
								<orgName type="institution">University and University Hospitals of Geneva</orgName>
								<address>
									<addrLine>24 Rue Micheli-du-Crest</addrLine>
									<postCode>CH-1211</postCode>
									<settlement>Geneva 4</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,95.52,148.73,411.70,15.51;1,211.32,170.69,180.20,15.51">Using medGIFT and easyIR for the ImageCLEF 2005 evaluation tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B01FDDCBE72BE806E80AF954B4228F8A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Image retrieval, evaluation, visual retrieval GIFT, easyIR, visual/textual retrieval, image retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the use of the medGIFT retrieval system for three of the four ImageCLEF 2005 retrieval tasks. We participated in the ad-hoc retrieval task that was similar to the 2004 ad-hoc task, the new medical retrieval task that required much more semantic analysis of the textual annotation than in 2004 and the new automatic annotation task. The techniques used in 2005 are fairly similar to the 2004 techniques for the two retrieval tasks. For the automatic annotation task, scripts were optimised to allow classification with a retrieval system. Unfortunately, an error in the text retrieval system corrupted part of our runs and led to relatively bad results for all runs including text. This error should be fixed before the final proceedings are printed, so correct figures are expected for this.</p><p>All retrieval results rely heavily on two retrieval systems: for visual retrieval we use the GNU Image Finding Tool (GIFT ), and for textual retrieval the EasyIR retrieval system. For the ad-hoc retrieval task, two runs were submitted with different configurations of grey levels and the Gabor filters. No textual retrieval was attempted, but only purely visual retrieval, resulting in generally lower scores than text retrieval. For the medical retrieval task, visual retrieval was performed with several configurations of Gabor filters and grey level and color quantisations as well as several variations of combining text and visual features. Unfortunately, all these runs are broken as the textual retrieval results are almost random. Due to a lack of resources no relevance feedback runs were submitted, which is where medGIFT performed best in 2004. For the classification task, a retrieval with the image to classify was performed and the first N = 1, 5, 10 resulting images were used to calculate scores for the classes by simply adding up the score of the N-images for each class. No machine learning was performed on the data of the known classes, so the results are surprisingly good and were only topped by systems with sophisticated learning strategies optimised for the used data set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image retrieval is an increasingly important domain in the field of information retrieval. Compared to text retrieval little is known about how to search for images, although it has been an extremely active domain as well in the field of computer vision as in information retrieval <ref type="bibr" coords="2,456.32,157.15,10.55,9.96" target="#b7">[8,</ref><ref type="bibr" coords="2,468.43,157.15,12.82,9.96" target="#b11">12,</ref><ref type="bibr" coords="2,482.82,157.15,12.82,9.96" target="#b12">13,</ref><ref type="bibr" coords="2,497.20,157.15,11.69,9.96" target="#b16">16]</ref>. Benchmarks such as ImageCLEF <ref type="bibr" coords="2,238.31,169.15,10.55,9.96" target="#b1">[2,</ref><ref type="bibr" coords="2,250.54,169.15,7.79,9.96" target="#b2">3]</ref> allow us to actually evaluate our algorithms compared to other systems and deliver us an insight into the techniques that perform well and those that do not perform as good. Thus, new developments can be directed towards these goals and techniques of other well-performing systems can be adapted to our needs.</p><p>In 2005, the ad-hoc retrieval task created topics that were better adapted for visual systems using the same database as in 2004. The tasks made available contained three images, so more visual information. We submitted two configurations of our system to this task using visual information only.</p><p>The medical retrieval task was performed on a much larger database than in 2004 containing a total of more than 50.000 images <ref type="bibr" coords="2,252.08,276.79,9.98,9.96" target="#b3">[4]</ref>. The annotation was also more varied, ranging from a few words in a very structured form to completely unstructured paragraphs. This made it hard to preprocess any of the information, so finally only free-text retrieval was used for our results submission including all XML tags. Also, the tasks were much harder and mainly semantic query tasks, which made the retrieval by visual means more difficult. Due to a lack of resources we could only submit partial results that did not include any relevance feedback or automatic query expansion.</p><p>The automatic annotation task was very interesting and challenging at the same time <ref type="bibr" coords="2,481.10,360.43,9.98,9.96" target="#b5">[6]</ref>. We did not take into account any of the training data and simply used the retrieval system GIFT and a nearest neighbour technique to classify the results. Still, the results were surprisingly good (6th best submission, 2nd best group) and when taking into account the learning data using an approach as described in <ref type="bibr" coords="2,200.79,408.19,14.67,9.96" target="#b9">[10]</ref>, these results are expected to get better.</p><p>ImageCLEF gave us the opportunity to compare our system with other techniques which is invaluable and will provide us with directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Basic Technologies Used</head><p>For our ImageCLEF participation, we aim at combining content-based retrieval of images with cross-language retrieval applied on the textual annotation of the images. Based on the results from last year (2004), we used parameters that were expected to lead to good results, plus some new combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Retrieval</head><p>The technology used for the content-based retrieval of images is mainly taken from the Viper<ref type="foot" coords="2,508.44,568.16,3.95,4.84" target="#foot_0">1</ref> project of the University of Geneva. Much information about this system is available <ref type="bibr" coords="2,451.00,580.99,14.67,9.96" target="#b13">[14]</ref>. Outcome of the Viper project is the GNU Image Finding Tool, GIFT<ref type="foot" coords="2,356.52,592.16,3.95,4.84" target="#foot_1">2</ref> . This software tool is open source and can in consequence also be used by other participants of ImageCLEF. A ranked list of visually similar images for every query topic was made available for participants and will serve as a baseline to measure the quality of submissions. Demonstration versions with a web-accessible interface of GIFT were also made available for participants to query visually as with feedback in an interactive way as not everybody can be expected to install an entire Linux tool for such a benchmark to use GIFT. The feature sets that are used by GIFT are:</p><p>• Local color features at different scales by partitioning the images successively into four equally sized regions (four times) and taking the mode color of each region as a descriptor;</p><p>• global color features in the form of a color histogram, compared by a simple histogram intersection;</p><p>• local texture features by partitioning the image and applying Gabor filters in various scales and directions. Gabor responses are quantised into 10 strengths;</p><p>• global texture features represented as a simple histogram of responses of the local Gabor filters in various directions and scales.</p><p>A particularity of GIFT is that it uses many techniques well-known from text retrieval. Visual features are quantised and the feature space is very similar to the distribution of words in texts, corresponding roughly to a Zipf distribution. A simple tf/idf weighting is used and the query weights are normalised by the results of the query itself. The histogram features are compared based on a histogram intersection <ref type="bibr" coords="3,239.75,254.95,14.67,9.96" target="#b15">[15]</ref>. The medical version of the GIFT is called medGIFT<ref type="foot" coords="3,336.96,266.00,3.95,4.84" target="#foot_2">3</ref>  <ref type="bibr" coords="3,345.00,266.83,9.98,9.96" target="#b8">[9]</ref>. It is also accessible as open source and adaptations concern mainly visual features and the user interface that shows the diagnosis on screen and is linked with a radiologic teaching file so the MD can not only browse images but also get the textual data and other images of the same case. Grey levels play a more important role for medical images and their numbers are raised, especially for relevance feedback (RF) queries. The number of the Gabor filter responses also has an impact on the performance and these are changed with respect to directions and scales. We used in total 4, 8 and 16 grey levels and for the Gabor filters we used 4 and 8 directions. Other techniques in medGIFT such as a pre-treatment of images <ref type="bibr" coords="3,134.00,362.47,10.55,9.96" target="#b6">[7]</ref> were not used for this competition due to a lack of resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Textual Search</head><p>The basic granularity of the Casimage and MIR collections is the case. A case gathers a textual report, and a set of images. For the PathoPic and Peir databases annotation exists for every image. The queries contain one to three images and text in three languages. We used all languages as a single query and also indexed all documents in a single index. Case-based annotation was expanded to all images of the case after the retrieval step, so for us the final unit of retrieval is the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Indexes</head><p>Textual experiments were conducted with the easyIR engine <ref type="foot" coords="3,349.44,512.00,3.95,4.84" target="#foot_3">4</ref> . As a single report is able to contain written parts in several languages mixed, it would have been necessary to detect the boundaries of each language segment. Ideally, French, German and English textual segments would be stored in different indexes. Each index could have been translated into the other language using a general translation method, or more appropriately using a domain-adapted method <ref type="bibr" coords="3,420.22,560.71,14.67,9.96" target="#b10">[11]</ref>. However, such a complex architecture would require to store different segments of the same document in separate indexes. Considering the lack of data to tune the system, we decided to index all collections using a unique index using an English stemmer, For simplicity reasons, the XML tags were also indexed and not separately treated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Weighting Schema</head><p>We chose a generally good weighting schema of the term frequency -inverse document frequency family. Following weighting convention of the SMART engine, cf. Table <ref type="table" coords="3,402.71,664.75,3.90,9.96">1</ref>, we used atc-ltn parameters, with α = β = 0.5 in the augmented term frequency.</p><formula xml:id="formula_0" coords="4,173.64,110.04,255.52,149.88">Term Frequency First Letter f (tf ) n (natural) tf l (logarithmic) 1 + log(tf ) a (augmented) α + β × ( tf max(tf ) ), where α = 1 -β and 0 &lt; α &lt; 1 Inverse Document Frequency Second Letter f ( 1 df ) n(no) 1 t(full) log( N df ) Normalisation Third Letter f (length) n(no) 1 c(cosine) t i=1 w 2 i,j × t j=1 w 2 j,q</formula><p>Table 1: Usual tf-idf weight; for the cosine normalisation factor, the formula is given for Euclidean space: w i,j is the document term weight, w j,q is the query term weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Combining the Two</head><p>Combinations of visual and textual features for retrieval are rather scarce in the literature <ref type="bibr" coords="4,487.07,345.79,9.98,9.96" target="#b4">[5]</ref>, so many of the mechanism and fine tuning of the combinations will still need more work, especially when the optimisation is based on the actual query. For the visual query we used all images that are present for a query, including one query containing negative feedback. For the text part, the text of all three languages was used as a combined query together with the combined index that includes the documents in all languages. Results list of the first 1000 documents were taken into account for both the visual and the textual search. Both result lists were normalised to deliver results within the range [0; 1]. The visual result is normalised by the result of the query itself whereas the text was normalised by the document with the highest score. This leads to visual results that are usually slightly lower than the textual results.</p><p>To combine the two lists, two different methods were chosen. The first one simply combines the list with different percentages for visual and textual results (textual= 50, 33, 25, 10%). In a second form of combination the list of the first 1000 visual results was taken, and then, all those that were in the first 200 textual documents were multiplied with N-times the value of the textual results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The ad hoc retrieval task</head><p>For the ad-hoc retrieval task we submitted results using fairly similar techniques as those in 2004. The 2005 topics were actually more adapted to the possibilities of visual retrieval systems as more visual attributes were taken into account for the topic creation. Still, textual retrieval stays very necessary for good results. It is not so much a problem of the queries but rather a problem of the database containing mostly grey or brown scale images of varying quality where automatic treatment such as color indexing is difficult. This should change in 2006 with a new database using mostly consumer pictures of vacation destinations. Such a database could be better analysed automatically using the available color information We used the GIFT system in two configurations, once the normal GIFT engine with 4 grey levels and the full HSV space using the Gabor filter responses in four directions and at three scales. The second configuration took into account 8 grey levels as the 2004 results for 16 grey levels were actually much worse than expected. We also raised the number of directions of the Gabor filters to 8 instead of four. The results of the basic GIFT system were made available to all participants and used by several. Surprisingly the results of the basic GIFT system remain the best in the test with a MAP of 0.0829, being at the same time the best purely visual system participating.</p><p>The system with eight grey levels and eight directions for the Gabor filters performed slightly worse and a MAP of 0.0819 was reached. Other visual systems performed slightly lower. The best mono-lingual text systems performed at a MAP of 0.41. Several text retrieval systems performed worse than the visual system for a variety of languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The automatic annotation task</head><p>We were new to the automatic annotation task as almost everyone and had mainly used our system for retrieval, so far. Due to a lack of resources no optimisation using the available training data was performed. Still, the tf/idf weighting is automatically weighting rare features higher which leads to a discriminative analysis.</p><p>As techniques we performed a query with each of the 1000 images to classify and took into account the first N = 1, 5, 10 retrieval results. For each of these results images from the training set the correct class was determined and this class was thus augmented with the similarity score of the image. The class with the highest final score became automatically the final class selected for the image. For retrieval we used three different settings of the features using 4, 8, and 16 grey levels. The runs with 8 and 16 grey levels also had eight directions of the Gabor filters for indexation. Best results obtained in the competition were from the Aachen groups (best run at 12.6% error rate) that have been working on very similar data for several years, now.</p><p>The best results for our system were retrieved when using 5NN and eight grey levels (error rate 20.6%), and the next best results using 5NN and 16 grey levels (20.9). Interestingly, the worst results were obtained with 5NN and 4 grey levels (22.1). Using 10NN led to slightly worse results (21.3) and 1NN was rather in the middle (4 grey levels 21.8; 8 grey levels: 21.1; 16 grey levels 21.7).</p><p>As a result we can say that all results are extremely close together 20.6-22.1 %, so the differences do not seem statistically significant. 5NN seems to be the best but this might also be linked to the fact that some classes have a very small population and 10NN would simply retrieve too many image of other classes to be competitive. 8 levels of grey and 8 directions of the Gabor filters seem to perform best, but the differences are still very small.</p><p>In the future it is planned to train the system with the available training data using the algorithm described in <ref type="bibr" coords="5,191.09,477.07,14.67,9.96" target="#b9">[10]</ref>. This technique is similar to the market basket analysis <ref type="bibr" coords="5,455.58,477.07,9.98,9.96" target="#b0">[1]</ref>. A proper strategy for the training needs to be developed to especially help smaller classes to be well classified. These classes cause normally most of the classification problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The medical retrieval task</head><p>Unfortunately, our textual retrieval results submitted contained an indexation error, and so the textual results were almost random. We have not identified the error yet, but hope to have it found before the final proceedings are printed. Thus the only textual run that we submitted had only a MAP of 0.0226, whereas the best textual retrieval systems were at 0.2084 (IPAL/I2R). Due to a limitation of resources, we were not able to submit relevance feedback runs, which is the discipline where GIFT usually is strongest. The best feedback system was OHSU with a map of 0.2116 for only textual retrieval.</p><p>The best visual system is I2R with a map of 0.1455. Our GIFT retrieval system was made available to participants and was widely used. Again, the basic GIFT system obtained the best results among the various combinations in feature space (map 0.0941), with only i2r having actually better results. The second indexation using 8 grey levels and eight directions of the Gabor filters performs slightly worse at 0.0872.</p><p>For mixed textual/visual retrieval, the best results were obtained by IPAL/I2R with map 0.2821. Our best result in this category is using 10% textual part and 90% visual part and obtains 0.0981. These results should be much better when using a properly indexed text base. The following results were obtained for other combinations: 20% visual: 0.0934, 25%: 0.0929, 33%: 0.0834, 50%: 0.044. When using eight grey levels and 8 Gabor directions: 10% visual: 0.0891, 20%: 0.084, 33%: 0.075, 50%: 0.0407. The results could lead to the assumption that visual retrieval is better than textual retrieval in our case, but this holds only true because of our indexation error. We will try to fix the error and deliver proper results as soon as possible to have a correct comparison with the other groups.</p><p>A second combination technique that we applied used as a basis the results from textual retrieval and then added the visual retrieval results multiplied with a factor N = 2, 3, 4 to the first 1000 results of textual retrieval. This strategy proved fruitful in 2004 the other way round by taking first the visual results and then augmenting only the first N=1000 results. The results for the main GIFT system were: 3 times visual: 0.0471, 4 times visual 0.0458, 2 times visual 0.0358. For the system with 8 grey levels, the respective results are: 3 times visual 0.0436, 4 times visual 0.0431, 2 times visual 0.0237. A reverse order of taking the visual results first and then augment the textually similar would have led to better results in this case but when having correct results for text as well as for visual retrieval, this needs to be proven.</p><p>We cannot really deduct extremely much of our current submission as several errors prevented better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Although we did not have any resources for an optimised submission we still learned from the 2005 tasks that the GIFT system delivers a good baseline for image retrieval and that it is widely usable for a large number of tasks and different images.</p><p>More detailed results show that the ad-hoc task is hard for visual retrieval even with a more visually-friendly set of queries as the image set does not contain enough color information or clear objects, which is crucial for fully visual information retrieval.</p><p>The automatic annotation or classification task proved that our system delivers good results even without learning and shows that information retrieval can also be used well for document classification. When taking into account the available training data these results will surely improve significantly.</p><p>From the medical retrieval task not much can be deduced for now as we need to work on our textual indexation and retrieval to find the error responsible for the mediocre results. Still, we can say that GIFT is well suited and among the best systems for general visual retrieval. It will need to be analysed which features were used by other systems, especially the few runs that performed better.</p><p>For next year we will definitely have to take into account the available training data and we hope as well to use more complex algorithms for example to extract objects form the medical images and limit retrieval to theses objects. Another strong point of GIFT is the good relevance feedback and this can surely improve results significantly as well. Already the fact to have a similar databases for two years in a row would help as such large databases need a large time to be indexed and require human resources for optimisation as well.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,716.61,92.23,7.35"><p>http://viper.unige.ch/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,726.21,138.71,7.35"><p>http://www.gnu.org/software/gift/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,105.24,696.21,134.51,7.35"><p>http://www.sim.hcuge.ch/medgift/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,105.24,705.69,189.49,7.35"><p>http://lithwww.epfl.ch/~ruch/softs/softs.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>Part of this research was supported by the <rs type="funder">Swiss National Science Foundation</rs> with grant <rs type="grantNumber">632-066041</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qNM7xXY">
					<idno type="grant-number">632-066041</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,110.51,706.15,402.13,9.96;6,110.52,718.15,402.29,9.96;6,110.52,730.03,22.88,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,306.77,706.15,188.78,9.96">Fast algorithms for mining association rules</title>
		<author>
			<persName coords=""><forename type="first">Rakesh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramakrishnan</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,110.52,718.15,180.48,9.96">Proceedings of the 20th VLDB Conference</title>
		<meeting>the 20th VLDB Conference<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">September 12-15 1994</date>
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,111.43,402.61,9.96;7,110.52,123.43,402.11,9.96;7,110.52,135.31,402.26,9.96;7,110.52,147.31,402.09,9.96;7,110.52,159.31,252.62,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,344.82,111.43,168.30,9.96;7,110.52,123.43,179.90,9.96">Overview of the CLEF cross-language image retrieval track (ImageCLEF) 2004</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,372.16,135.31,140.62,9.96;7,110.52,147.31,322.28,9.96">Multilingual Information Access for Text, Speech and Images: Result of the fifth CLEF evaluation campaign</title>
		<title level="s" coord="7,440.79,147.31,71.81,9.96;7,110.52,159.31,77.01,9.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, England</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,178.63,402.38,9.96;7,110.52,190.51,401.92,9.96;7,110.52,202.51,276.83,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,339.18,178.63,173.71,9.96;7,110.52,190.51,178.94,9.96">A proposal for the CLEF cross language image retrieval track (ImageCLEF) 2004</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,314.08,190.51,198.36,9.96;7,110.52,202.51,55.67,9.96">The Challenge of Image and Video Retrieval (CIVR 2004)</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2004-07">July 2004</date>
			<biblScope unit="page">3115</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,221.83,402.26,9.96;7,110.52,233.71,401.99,9.96;7,110.52,245.71,402.12,9.96;7,110.52,257.71,22.88,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,409.57,221.83,103.21,9.96;7,110.52,233.71,349.50,9.96">Task analysis for evaluating image retrieval systems in the ImageCLEF biomedical image retrieval task</title>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffery</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,481.11,233.71,31.40,9.96;7,110.52,245.71,281.19,9.96">Slice of Life conference on Multimedia in Medical Education (SOL 2005)</title>
		<meeting><address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,277.03,402.28,9.96;7,110.52,288.91,401.98,9.96;7,110.52,300.91,402.31,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,347.08,277.03,165.71,9.96;7,110.52,288.91,229.24,9.96">Combining textual and visual cues for content-based image retrieval on the world wide web</title>
		<author>
			<persName coords=""><forename type="first">La</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saratendu</forename><surname>Cascia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stan</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,361.83,288.91,150.67,9.96;7,110.52,300.91,218.67,9.96">IEEE Workshop on Content-based Access of Image and Video Libraries (CBAIVL&apos;98)</title>
		<meeting><address><addrLine>Santa Barbara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-06-21">June 21 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,320.23,402.18,9.96;7,110.52,332.11,402.23,9.96;7,110.52,344.11,402.55,9.96;7,110.52,356.11,22.88,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,272.64,332.11,240.12,9.96;7,110.52,344.11,138.27,9.96">Automatic categorization of medical images for contentbased retrieval and data mining</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">O</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Güld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Berthold</forename><forename type="middle">B</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,259.09,344.11,194.69,9.96">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="143" to="155" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,375.43,402.13,9.96;7,110.52,387.31,401.80,9.96;7,110.52,399.31,223.95,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,367.34,375.43,145.29,9.96;7,110.52,387.31,62.79,9.96">Logo and text removal for medical image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joris</forename><surname>Heuberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,192.16,387.31,320.15,9.96;7,110.52,399.31,64.26,9.96">Springer Informatik aktuell: Proceedings of the Workshop Bildverarbeitung für die Medizin</title>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-03">March 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,418.63,402.17,9.96;7,110.52,430.51,402.28,9.96;7,110.52,442.51,262.74,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,460.17,418.63,52.51,9.96;7,110.52,430.51,398.19,9.96">A review of content-based image retrieval systems in medicine -clinical benefits and future directions</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Michoux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Bandon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,110.52,442.51,193.40,9.96">International Journal of Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,461.83,402.27,9.96;7,110.52,473.83,402.11,9.96;7,110.52,485.71,356.28,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,464.47,461.83,48.31,9.96;7,110.52,473.83,295.64,9.96">Integrating content-based visual access methods into a medical case database</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Paul</forename><surname>Vallée</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,432.44,473.83,80.19,9.96;7,110.52,485.71,224.59,9.96">Proceedings of the Medical Informatics Europe Conference (MIE 2003)</title>
		<meeting>the Medical Informatics Europe Conference (MIE 2003)<address><addrLine>St. Malo, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05">May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.50,505.03,402.29,9.96;7,110.52,517.03,402.10,9.96;7,110.52,528.91,348.59,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,351.16,505.03,161.63,9.96;7,110.52,517.03,233.51,9.96">Learning from user behavior in image retrieval: Application of the market basket analysis</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">Mcg</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thierry</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,356.90,517.03,155.72,9.96;7,110.52,528.91,26.51,9.96">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="65" to="77" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Special Issue on Content-Based Image Retrieval</note>
</biblStruct>

<biblStruct coords="7,110.50,548.23,402.07,9.96;7,110.52,560.23,348.38,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,175.24,548.23,175.19,9.96">Query translation by text categorization</title>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,371.73,548.23,140.84,9.96;7,110.52,560.23,187.92,9.96">Proceedings of the conference on Computational Linguistics (COLING 2004)</title>
		<meeting>the conference on Computational Linguistics (COLING 2004)<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08">August 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.50,579.55,402.13,9.96;7,110.52,591.55,401.96,9.96;7,110.52,603.43,402.18,9.96;7,110.52,615.43,198.90,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,414.58,579.55,98.06,9.96;7,110.52,591.55,240.45,9.96">Relevance feedback: A power tool for interactive content-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharad</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,358.78,591.55,153.70,9.96;7,110.52,603.43,125.04,9.96">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="644" to="655" />
			<date type="published" when="1998-09">September 1998</date>
		</imprint>
	</monogr>
	<note>Special Issue on Segmentation, Description, and Retrieval of Video Content</note>
</biblStruct>

<biblStruct coords="7,110.50,634.75,402.21,9.96;7,110.52,646.63,402.11,9.96;7,110.52,658.63,306.95,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,138.34,646.63,264.59,9.96">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">M</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcel</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Armarnath</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramesh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,413.72,646.63,98.91,9.96;7,110.52,658.63,184.08,9.96">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.50,677.95,402.41,9.96;7,110.52,689.95,402.10,9.96;7,110.52,701.83,402.29,9.96;7,110.52,713.83,90.22,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,448.53,677.95,64.38,9.96;7,110.52,689.95,251.74,9.96">Content-based query of image databases: inspirations from text retrieval</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Mcg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wolfgang</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thierry</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,499.22,689.95,13.40,9.96;7,110.52,701.83,367.39,9.96">Selected Papers from The 11th Scandinavian Conference on Image Analysis SCIA &apos;99)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1193" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,204.94,713.83,137.90,9.96" xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">K</forename><surname>Ersboll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Johansen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.50,733.15,402.13,9.96;7,110.52,745.03,108.59,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,288.33,733.15,62.50,9.96">Color indexing</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dana</forename><forename type="middle">H</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,360.39,733.15,152.24,9.96;7,110.52,745.03,26.51,9.96">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.50,111.43,402.36,9.96;8,110.52,123.43,402.52,9.96;8,110.52,135.31,22.88,9.96" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,325.72,111.43,187.15,9.96;8,110.52,123.43,77.02,9.96">Medical image databases: A content-based retrieval approach</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hemant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tagare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Jaffe</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,197.07,123.43,249.40,9.96">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="184" to="198" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
