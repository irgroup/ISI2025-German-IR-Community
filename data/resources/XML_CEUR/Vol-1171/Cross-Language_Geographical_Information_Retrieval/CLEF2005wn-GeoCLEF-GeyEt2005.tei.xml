<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,70.56,73.02,408.58,9.02">GeoCLEF: the CLEF 2005 Cross-Language Geographic Information Retrieval Track Overview</title>
				<funder ref="#_MsbaDfG">
					<orgName type="full">EU</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,70.56,95.88,48.08,9.02"><forename type="first">Fredric</forename><surname>Gey</surname></persName>
							<email>gey@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,126.81,95.88,46.43,9.02"><forename type="first">Ray</forename><surname>Larson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.45,95.88,65.80,9.02"><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
							<email>m.sanderson@sheffield.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Studies</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.55,95.88,45.86,9.02"><forename type="first">Hideo</forename><surname>Joho</surname></persName>
							<email>h.joho@sheffield.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Studies</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.64,95.88,49.75,9.02"><forename type="first">Paul</forename><surname>Clough</surname></persName>
							<email>p.d.clough@sheffield.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Studies</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,381.19,95.88,54.13,9.02"><forename type="first">Vivien</forename><surname>Petras</surname></persName>
							<email>vivienp@sims.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,70.56,73.02,408.58,9.02">GeoCLEF: the CLEF 2005 Cross-Language Geographic Information Retrieval Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">96329DBE3ACAD06C8BC46E501D3AF736</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval] effectiveness); H.3.7 Digital Libraries Measurement</term>
					<term>Performance</term>
					<term>Experimentation Geographic Information Retrieval</term>
					<term>Cross-language Information Retrieval</term>
				</keywords>
			</textClass>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>GeoCLEF is a new track for CLEF 2005. GeoCLEF was run as a pilot track to evaluate retrieval of multilingual documents with an emphasis on geographic search. Existing evaluation campaigns such as TREC and CLEF do not explicitly evaluate geographical relevance. The aim of GeoCLEF is to provide the necessary framework in which to evaluate GIR systems for search tasks involving both spatial and multilingual aspects. Participants were offered a TREC style ad hoc retrieval task based on existing CLEF collections. GeoCLEF was a collaborative effort by research groups at the University of California, Berkeley and the University of Sheffield. Twelve research groups from a variety of backgrounds and nationalities submitted 117 runs to GeoCLEF.</p><p>Geographical Information Retrieval (GIR) concerns the retrieval of information involving some kind of spatial awareness. Given that many documents contain some kind of spatial reference, there are examples where geographical references (geo-references) may be important for IR. For example, to retrieve, re-rank and visualize search results based on a spatial dimension (e.g. "find me news stories about riots near Dublin City"). In addition to this, many documents contain geo-references expressed in multiple languages which may or may not be the same as the query language. This would require an additional translation step to enable successful retrieval.</p><p>For this pilot track 2 languages, German and English, were chosen to be the document languages, while topics were developed in English with topic translations provided for German, Portuguese and Spanish. There were two Geographic Information Retrieval tasks: monolingual (English to English or German to German) and bilingual (language X to English or language X to German, where X was one of English, German, Portuguese or Spanish).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document collections used in GeoCLEF</head><p>The document collections for this year's GeoCLEF experiments are all newswire stories from the years 1994 and 1995 used in previous CLEF competitions. Both the English and German collections contain stories covering international and national news events, therefore representing a wide variety of geographical regions and places. The English document collection consists of 169,477 documents and was composed of stories from the British newspaper The Glasgow Herald (1995) and the American newspaper The Los Angeles Times <ref type="bibr" coords="1,475.59,606.42,25.05,9.02">(1994)</ref>. The German document collection consists of 294,809 documents from the German news magazine Der Spiegel (1994/95), the German newspaper Frankfurter Rundschau (1994) and the Swiss news agency SDA <ref type="bibr" coords="1,483.07,629.46,37.72,9.02">(1994/95)</ref>. Although there are more documents in the German collection, the average document length (in terms of words in the actual text) is much larger for the English collection. In both collections, the documents have a common structure: newspaper-specific information like date, page, issue, special filing numbers and usually one or more titles, a byline and the actual text. The document collections were not geographically tagged or contained any other location-specific information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generating Search Topics</head><p>A total of 25 topics were generated for this year's GeoCLEF. Ten of them were extended from the past CLEF topics and 15 of them were newly created. This section will discuss the processes taken to create the spatiallyaware topics for the track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Format of topic description</head><p>We used the format to describe the search topics, which we proposed in the introductory presentation of Geo Track in CLEF 2004. The format was designed to highlight the geographic aspect of the topics so that the participants can exploit the information in the retrieval process without extracting the geographic references from the description. A sample topic was shown in Figure <ref type="figure" coords="2,283.37,130.38,3.75,9.02">1</ref>.</p><p>As can be seen, after the standard data such as the title, description, and narrative, the information about the main concept, locations, and spatial relation which were manually extracted from the title were added to the topics. The above example has the original topic ID of CLEF since it was created based on the past topic. The process of selecting the past CLEF topics for this year's GeoCLEF will be described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of past CLEF topics</head><p>Creating a subset of topics from the past CLEF topics had several advantages for us. First of all, it would reduce the amount of effort required to create new topics. Similarly, it would save the resource required to carry out the relevance assessment of the topics. The idea was to revisit the past relevant documents with a greater weight on the geographical aspect. Finally, it was anticipated that the distribution of relevant documents across the collections would be ensured to some extent.</p><p>The process of selecting the past CLEF topics for our track was as follows. Firstly, two of the authors went through the topics of the past Ad-Hoc tracks (except Topic 1-40 due to the limited coverage of document collections) and identified those which either contained one or more geographical references in the topic description or asked a geographical question (i.e., Which countries are …?). A total of 72 topics were found from this analysis.</p><p>The next stage involved examining the distribution of relevant documents across the collections chosen for this year's track. A cross tabulation was run on the qrel files for the document collections to identify the topics that covered our collections. A total of 10 topics were then chosen based on the above analysis as well as the additional manual examination of the suitability for the track.</p><p>One of the characteristics we found from the chosen past CLEF topics was a relatively low granularity of geographical references used in the descriptions. Many referred to countries. This is not surprising given that a requirement of CLEF topics is that they are likely to retrieve relevant documents from as many of the CLEF collections as possible (which are predominately newspaper articles from different countries). Consequently, the geographic references in topics were likely to be to well-known locations, i.e. countries.</p><p>However, we felt that the topics with a finer granularity should also be devised to make the track geographically more interesting. Therefore, we decided to create the rest of topics by focusing on each of the chosen collections. 7 topics were created based on the articles of LA Times, and 8 topics were created based on Glasgow Herald. The new topics were then translated into other languages by one of the organisers and the volunteers from the participants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geospatial processing of document collections</head><p>Geographical references found in the document collections were automatically tagged. This was done for two reasons: firstly, it was thought that highlighting the geographic references in the documents would facilitate the topic generation process; secondly, it would help assessors identify relevant documents more quickly if such references were highlighted. In the end though only some assessments were conducted using such information.</p><p>Tagging was conducted using a geo-parsing system developed in the Spatially-Aware Information Retrieval on the Internet (SPIRIT) project (http://www.geospirit.org/). The implementation of the system was built using the information extraction component from the General Architecture for Text Engineering (GATE) system <ref type="bibr" coords="3,70.56,176.40,84.29,9.02" target="#b4">(Cunningham, 2002)</ref> with the additional contextual rules especially designed for the geographical entities. The system used several gazetteers such as the SABE (Seamless Administrative Boundaries of Europe) dataset, the Ordnance Survey 1:50,000 Scale Gazetteer for the UK, and the Getty Thesaurus of Geographic Names (TGN). The detail of the geo-parsing system can be found in <ref type="bibr" coords="3,283.42,210.90,57.01,9.02" target="#b2">Clough (2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relevance assessment</head><p>Assessment was shared by Berkeley and Sheffield Universities. Sheffield was assigned topics 18-25 for the English collections (LA Times, Glasgow Herald); Berkeley assessed topics 1-17 for English and topics 1-25 for the German collections. Assessment resources were restricted for both groups, which influenced the manner in which assessments were conducted. Berkeley used the conventional approach of judging documents taken from the pool formed by the top-n documents from participants' submissions. In TREC the tradition is to set n to 100. However, due to a limited number of assessors, Berkeley set n to 60, consistent with the ad-hoc CLEF cutoff. English judgments were conducted by Berkeley authors of this paper, and half of the German judgments were conducted by an external assessor paid €1000 (from CLEF funds). Although restricting the number of documents assessed by so much appears to be a somewhat drastic measure, it was observed at last year's TRECVID that reducing pool depth to as little as 10 had little effect on the relative ordering of runs submitted to that evaluation exercise (Kraaji, Smeaton, Over and Arlandis, 2004). More recently <ref type="bibr" coords="3,281.90,383.34,115.79,9.02" target="#b8">Sanderson and Zobel (2005)</ref> conducted a large study of the levels of error in effectiveness measures based on shallow pools and again showed that error levels were little different from those based on much deeper pools.</p><p>Sheffield was able to secure some funding to pay students to conduct relevance assessments, but the money had to be spent before geoCLEF participants were due to submit their results. Assessments had to be conducted before the submission date; therefore, Sheffield used the Interactive Searching and Judging (ISJ) method described by <ref type="bibr" coords="3,125.69,463.85,148.83,9.02" target="#b3">Cormack, Palmer and Clarke (1998)</ref> and broadly tested by <ref type="bibr" coords="3,369.67,463.85,110.10,9.02" target="#b7">Sanderson and Joho (2004)</ref>. With this approach to building a set of relevance judgments, assessors for a topic become searchers, who were encouraged to search the topic in as broad and diverse a way as possible, marking any relevant documents found. To this end, an ISJ system was previously built for the SPIRIT project was modified for GeoCLEF (see Figure <ref type="figure" coords="3,465.65,498.35,3.61,9.02">4</ref>). Sheffield employed 17 searchers (mostly University students), paying each of them (£40) for a half-day session; one searcher worked for three sessions. In each session, two topics were covered. Before starting, searchers were given a short introduction to the system. The authors of the paper also contributed to the assessing process. As so many searchers were found, Sheffield moved beyond the eight topics assigned to it and contributed judgments to the rest of the English topics, overlapping with Berkeley's judgments. For the judgments used in the GeoCLEF exercise, if two documents were found to judged by both Sheffield and Berkeley, Berkeley's judgment was used. The reason for producing such an overlap is the plan to compare judgment quality between the ISJ process and the more conventional pooling approach, which will be forthcoming. The participants used a wide variety of approaches to the GeoCLEF tasks, ranging from basic IR approaches (with no attempts at spatial or geographic reasoning or indexing) to deep NLP processing to extract place and topological clues from the texts and queries. As Table <ref type="table" coords="4,291.14,656.76,5.01,9.02">1</ref> shows, all of the participating groups submitted runs for the Monolingual English task. (Note that Linguateca did not submit runs, but worked with the organizers to translate the GeoCLEF queries to Portuguese, which were then used by other groups). The bilingual X-&gt;EN task actually represents 3 separate tasks, depending on whether the German, Spanish, or Portuguese query sets were used (and likewise for X-&gt;DE from English, Spanish or Portuguese). The University of Alicante is the only group that submitted runs for all possible Monolingual and Bilingual tasks including Spanish and Portuguese to both English and German. The least participation was for the Bilingual X-&gt;DE task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GeoCLEF Performance</head><p>Monolingual performance: Since the largest number of runs (57) were submitted for monolingual English, it is not surprising that that evalution is represented by the largest number of groups (11). Monolingual German was carried out by 6 groups submitting 25 runs. The following is a ranked list of performance and results by overall mean average precision using the TREC_Eval software, displaying best English against best German. We choose only the single best run from each participating group (independent of method used to produce the best run): One immediately apparent observation is that German performance is substantially below that of English performance. This derives from two sources: Many of the topics were "English" news story-oriented and had few, if any, relevant documents in the German language. Four topics (1, 20, 22, and 25) had no relevant German documents. Topics 18 and 23 had 1 and 2 relevant documents, respectively. By contrast, no English version of the topic had less than 3 relevant documents. The German task seems to have been inherently more difficult, with fewer geographic resources available in the German language to work with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Comparison on Mandatory Tasks:</head><p>A fairer comparison (one usually used in CLEF, TREC and NTCIR) is to compare system performance on identical tasks. The two runs expected from each participating group were a Title-Description run which used only these fields and a Title-Description-Geotags run which utilized the geographic tag triples (Concept-Location-Operator-Location). The precision scores for best Title-Description runs for monolingual English are as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recall</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual performance</head><p>Fewer groups accepted the challenge of bilingual retrieval. There were a total of 22 bilingual X to English runs submitted by 5 groups and 17 bilingual X to German runs submitted by 3 groups. The table below shows the performance of bilingual best runs by each group for both English and German, independent of method used to produce the run. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Future Work</head><p>While the results of the GeoCLEF 2005 pilot track were encouraging, both in terms of number of groups/runs participating, but also in terms of interest, there is some question as to whether we have truly identified what constitutes the proper evaluation of geographic information retrieval. One participant has remarked that "The geographic scope of most queries had the granularity of Continents or groups of countries. It should include queries with domain of interest restricted to much smaller areas, at least to the level of cities with 50000 people."</p><p>In addition, the best performance was achieved by groups using standard keyword search techniques. If we believe that GIR ≠ Keyword Search, then we must find a path which distinguishes between the two. GeoCLEF will probably continue in 2006 and expand the number of document languages (likely Portuguese and perhaps Spanish) as well as the scope of the task (i.e. consider more difficult topics such as "find stories about places within 125 kilometers of [Vienna, Viena, Wien]").</p><p>Possible directions which we might foresee for 2006 are: 1. Additional languages: which and how many? Since Portuguese was suggested this year, it seems a natural for next year. Spanish was also considered this year and would be fairly easy to integrate? The inclusion of another language assumes that some group will be willing to do the relevance judgments. 2. Multilinguality? Currently the tasks are monolingual and bilingual. Should we have a multilingual task where the documents are ranked independent of language? 3. Task difficulty: Should we increase the challenge of GeoCLEF 2006? One possible direction to increase task difficulty is to include geospatial distance or locale in the topic, i.e. "find stories about places within 125 kilometers of Vienna" or "Find stories about wine-making along the Mosel River" or "what rivers pass through Koblenz Germany?". Should the task become more of a named entity extraction task (see the next point on evaluation)? 4. Evaluation: Do we stick with the relative looseness of ranking documents according to subject and geographic reference? Or should we make the task more of an entity extraction task, like the shared task of the Conference on Computational Natural Language <ref type="bibr" coords="7,353.94,222.36,61.03,9.02">Learning 2002</ref><ref type="bibr" coords="7,414.97,222.36,67.47,9.02">/2003 (CoNLL)</ref> found at http://www.cnts.ua.ac.be/conll2003/ner/ . This task had a definite geographic component. See also the background lecture by Marti Hearst at http://www.sims.berkeley.edu/courses/is290-2/f04/lectures/lecture15.ppt. In this instance we might have the evaluation be to extract a list of unique geographic names and the recall/precision measures are on the completeness of the list (how many relevant found) and (I guess) how many are found at rank x (precision) as well as the F measure. I'm not sure if this measure is also used for the list task in TREC question answering. Paul Clough and Mark Sanderson have proposed a MUC style evaluation for GIR <ref type="bibr" coords="7,342.94,302.88,120.50,9.02" target="#b1">(Clough and Sanderson, 2004)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,70.56,338.04,284.10,9.02"><head></head><label></label><figDesc>Figure 1 Topic GC001: Shark Attacks off Australia and California</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,73.74,576.42,448.05,9.02;4,80.52,587.88,434.44,9.02;4,74.52,599.40,446.42,9.02;4,177.42,610.92,240.64,9.02;4,100.92,367.62,393.60,206.34"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Interactive Search and Judging system. Queries are entered in the top panel with search results displayed below, where documents can be marked as relevant/not relevant. A history of the searcher's queries is listed in the right-hand panel. The user can display the full text of a document (loaded in a new window) with query words and place names highlighted.</figDesc><graphic coords="4,100.92,367.62,393.60,206.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,70.56,73.02,454.48,245.90"><head></head><label></label><figDesc>ParticipantsTwelve groups participated in the GeoCLEF task this year, the following table shows the group names and the sub-tasks in which they submitted runs:</figDesc><table coords="5,70.56,107.40,453.69,211.52"><row><cell></cell><cell>Mono</cell><cell>Mono</cell><cell>Bi</cell><cell>Bi</cell><cell>Total</cell></row><row><cell>Group Name</cell><cell>EN</cell><cell>DE</cell><cell>-&gt;EN</cell><cell>-&gt;DE</cell><cell>Runs</cell></row><row><cell>California State University, San Marcos</cell><cell>2</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>4</cell></row><row><cell>Grupo XLDB (Universidade de Lisboa)</cell><cell>6</cell><cell>4</cell><cell>4</cell><cell>0</cell><cell>14</cell></row><row><cell>Linguateca (Portugal and Norway)</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Linguit GmbH. (Germany)</cell><cell>16</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>16</cell></row><row><cell>MetaCarta Inc.</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell></row><row><cell>MIRACLE (Universidad Politécnica de Madrid)</cell><cell>5</cell><cell>5</cell><cell>0</cell><cell>0</cell><cell>10</cell></row><row><cell>NICTA, University of Melbourne</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>4</cell></row><row><cell>TALP Research Center (Universitat Politècnica de Catalunya)</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>4</cell></row><row><cell>Universidad Politécnica de Valencia</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell></row><row><cell>University of Alicante</cell><cell>5</cell><cell>4</cell><cell>12</cell><cell>13</cell><cell>34</cell></row><row><cell>University of California, Berkeley (Berkeley 1)</cell><cell>3</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>10</cell></row><row><cell>University of California, Berkeley (Berkeley 2)</cell><cell>4</cell><cell>4</cell><cell>2</cell><cell>2</cell><cell>12</cell></row><row><cell>University of Hagen (FernUniversität in Hagen)</cell><cell>0</cell><cell>5</cell><cell>0</cell><cell>0</cell><cell>5</cell></row><row><cell>Total Submitted Runs</cell><cell>53</cell><cell>25</cell><cell>22</cell><cell>17</cell><cell>117</cell></row><row><cell>Number of Groups Participating in Task</cell><cell>11</cell><cell>6</cell><cell>5</cell><cell>3</cell><cell>12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,70.56,72.90,454.50,378.98"><head></head><label></label><figDesc>CSUSM run is a statistically significant improvement over this run using a paired t-test at 5% probability levelThe next mandatory run was to also include (in addition to Title and Description) the contents of the Geographic tags in the topic description. The next table provides performance comparison for the best 5 runs with TD+GeoTags:</figDesc><table coords="6,70.56,72.90,454.50,378.98"><row><cell></cell><cell>CSUSM</cell><cell>Berkeley2</cell><cell>Alicante</cell><cell>Berkeley</cell><cell>NICTA</cell></row><row><cell>0.0</cell><cell>0.7634</cell><cell>0.7899</cell><cell>0.7889</cell><cell>0.6976</cell><cell>0.6680</cell></row><row><cell>0.1</cell><cell>0.6514</cell><cell>0.6545</cell><cell>0.6341</cell><cell>0.5222</cell><cell>0.5628</cell></row><row><cell>0.2</cell><cell>0.5348</cell><cell>0.5185</cell><cell>0.4972</cell><cell>0.4321</cell><cell>0.4209</cell></row><row><cell>0.3</cell><cell>0.4883</cell><cell>0.4584</cell><cell>0.4315</cell><cell>0.3884</cell><cell>0.3456</cell></row><row><cell>0.4</cell><cell>0.4549</cell><cell>0.3884</cell><cell>0.3776</cell><cell>0.3435</cell><cell>0.2747</cell></row><row><cell>0.5</cell><cell>0.3669</cell><cell>0.3562</cell><cell>0.3258</cell><cell>0.2783</cell><cell>0.2217</cell></row><row><cell>0.6</cell><cell>0.3039</cell><cell>0.2967</cell><cell>0.2728</cell><cell>0.2221</cell><cell>0.1715</cell></row><row><cell>0.7</cell><cell>0.2439</cell><cell>0.2563</cell><cell>0.2072</cell><cell>0.1877</cell><cell>0.1338</cell></row><row><cell>0.8</cell><cell>0.1834</cell><cell>0.1963</cell><cell>0.1591</cell><cell>0.1168</cell><cell>0.0908</cell></row><row><cell>0.9</cell><cell>0.1040</cell><cell>0.1169</cell><cell>0.0701</cell><cell>0.0525</cell><cell>0.0624</cell></row><row><cell>1.0</cell><cell>0.0484</cell><cell>0.0603</cell><cell>0.0314</cell><cell>0.0194</cell><cell>0.0272</cell></row><row><cell>MAP</cell><cell>0.3613</cell><cell>0.3528</cell><cell>0.3255*</cell><cell>0.2794*</cell><cell>0.2514*</cell></row><row><cell>*Recall</cell><cell>Berkeley2</cell><cell>Alicante</cell><cell>CSUSM</cell><cell>Berkeley</cell><cell>Miracle</cell></row><row><cell>0.0</cell><cell>0.8049</cell><cell>0.7856</cell><cell>0.7017</cell><cell>0.6981</cell><cell>0.5792</cell></row><row><cell>0.1</cell><cell>0.7144</cell><cell>0.6594</cell><cell>0.5822</cell><cell>0.5627</cell><cell>0.4932</cell></row><row><cell>0.2</cell><cell>0.5971</cell><cell>0.5318</cell><cell>0.4612</cell><cell>0.4804</cell><cell>0.4266</cell></row><row><cell>0.3</cell><cell>0.5256</cell><cell>0.4675</cell><cell>0.4204</cell><cell>0.4149</cell><cell>0.3516</cell></row><row><cell>0.4</cell><cell>0.4534</cell><cell>0.4138</cell><cell>0.3803</cell><cell>0.3460</cell><cell>0.3184</cell></row><row><cell>0.5</cell><cell>0.3868</cell><cell>0.3580</cell><cell>0.2937</cell><cell>0.2960</cell><cell>0.2815</cell></row><row><cell>0.6</cell><cell>0.3464</cell><cell>0.2924</cell><cell>0.2293</cell><cell>0.2257</cell><cell>0.2231</cell></row><row><cell>0.7</cell><cell>0.2913</cell><cell>0.2342</cell><cell>0.1974</cell><cell>0.1869</cell><cell>0.1889</cell></row><row><cell>0.8</cell><cell>0.2301</cell><cell>0.1779</cell><cell>0.1451</cell><cell>0.1198</cell><cell>0.1450</cell></row><row><cell>0.9</cell><cell>0.1318</cell><cell>0.0823</cell><cell>0.1084</cell><cell>0.0534</cell><cell>0.0928</cell></row><row><cell>1.0</cell><cell>0.0647</cell><cell>0.0317</cell><cell>0.0281</cell><cell>0.0243</cell><cell>0.0344</cell></row><row><cell>MAP</cell><cell>0.3937</cell><cell>0.3471</cell><cell>0.3032*</cell><cell>0.2924*</cell><cell>0.2653*</cell></row><row><cell cols="6">*Berkeley2 run is a statistically significant improvement over this run using a paired t-test 1% probability level</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments:</head><p>All effort done by the GeoCLEF organizers both in Sheffield and Berkeley was volunteer labo(u)r -none of us has funding for GeoCLEF work. The English assessment was evenly divided with <rs type="person">Ray Larson</rs> and myself at <rs type="institution">University of California</rs> taking half and the Sheffield group taking the other half. <rs type="person">Vivien Petras</rs> did half the German assessments. At the last minute <rs type="person">Carol Peters</rs> found some funding for part of the German assessment, which might not have been completed otherwise. Similarly groups volunteered the topic translations into Portuguese (thanks to <rs type="person">Diana Santos</rs> and <rs type="person">Paulo Rocha</rs> of <rs type="affiliation">Linguateca</rs>) and Spanish (thanks <rs type="person">Andres Montoyo</rs> of <rs type="affiliation">U. Alicante</rs>). In addition a tremendous amount of work was done above and beyond the call of duty by the <rs type="institution">Padua group</rs> (thanks <rs type="person">Giorgio Di Nunzio</rs> and <rs type="person">Nicola Ferro</rs>) -we owe them a great debt. Funding to help pay for assessor effort and travel came from the <rs type="funder">EU</rs> projects, <rs type="projectName">SPIRIT</rs> and BRICKS. The future direction and scope of GeoCLEF will be heavily influenced by funding and the amount of volunteer effort available.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_MsbaDfG">
					<orgName type="project" subtype="full">SPIRIT</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,70.56,475.38,454.47,9.02;7,70.56,486.84,454.41,9.02;7,70.56,498.36,32.52,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,409.65,475.38,115.38,9.02;7,70.56,486.84,52.49,9.02">Corpora for Topic Detection and Tracking</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Martey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Rennert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,206.17,486.84,285.14,9.02">Topic Detection and Tracking: Event-based Information Organization</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</editor>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="33" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.56,521.33,454.54,9.02;7,70.56,532.85,419.69,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,222.32,521.33,302.78,9.02;7,70.56,532.85,87.82,9.02">A Proposal for Comparative Evaluation of Automatic Annotation for Georeferenced Documents</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,176.56,532.85,283.92,9.02">Proceedings of Workshop on Geographic Information Retrieval, SIGIR</title>
		<meeting>Workshop on Geographic Information Retrieval, SIGIR</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.56,555.83,454.35,9.02;7,70.56,567.35,339.69,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,167.16,555.83,339.82,9.02">Extracting Metadata for Spatially-Aware Information Retrieval on the Internet</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,70.56,567.35,192.40,9.02">Proceedings of GIR&apos;05 Workshop at CIKM2005</title>
		<meeting>GIR&apos;05 Workshop at CIKM2005<address><addrLine>Bremen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11-04">2005. Nov 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.56,590.33,454.43,9.02;7,70.56,601.85,454.40,9.02;7,70.56,613.31,127.81,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,310.80,590.33,198.20,9.02">Efficient Construction of Large Test Collections</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,70.56,601.85,454.40,9.02;7,70.56,613.31,85.57,9.02">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.56,636.35,454.40,9.02;7,70.56,647.81,438.14,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,371.39,636.35,153.57,9.02;7,70.56,647.81,269.17,9.02">GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Tablan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,357.19,647.81,91.63,9.02">Proceedings of ACL&apos;02</title>
		<meeting>ACL&apos;02<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.56,670.85,454.28,9.02;7,70.56,682.31,405.72,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,315.16,670.85,133.66,9.02">TRECVID 2004 -An Overview</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Arlandis</surname></persName>
		</author>
		<ptr target="http://www-nlpir.nist.gov/projects/tvpubs/tv.pubs.org.html" />
	</analytic>
	<monogr>
		<title level="m" coord="7,470.57,670.85,54.28,9.02;7,70.56,682.31,161.79,9.02">TREC Video Retrieval Evaluation Online Proceedings</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.56,705.35,454.40,9.02;7,70.56,716.81,403.35,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,330.16,705.35,194.80,9.02;7,70.56,716.81,254.03,9.02">Pooling for a Large-Scale Test Collection: An Analysis of the Search Results from the First NTCIR Workshop</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kuriyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nozue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Eguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,331.98,716.81,85.58,9.02">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="59" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,70.56,72.90,454.43,9.02;8,70.56,84.36,454.32,9.02;8,70.56,95.88,301.71,9.02" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,216.90,72.90,202.11,9.02">Forming Test Collections with No System Pooling</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,220.76,84.36,304.12,9.02;8,70.56,95.88,208.37,9.02">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bruza</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,70.56,118.86,454.45,9.02;8,70.56,130.38,208.43,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,215.51,118.86,305.77,9.02">Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,81.42,130.38,193.35,9.02">Proceedings of the 28th ACM SIGIR conference</title>
		<meeting>the 28th ACM SIGIR conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
