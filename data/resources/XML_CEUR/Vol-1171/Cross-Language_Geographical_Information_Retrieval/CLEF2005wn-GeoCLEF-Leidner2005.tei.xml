<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,97.65,148.91,407.70,15.49;1,233.82,170.83,135.36,15.49">Preliminary Experiments with Geo-Filtering Predicates for Geographic IR</title>
				<funder ref="#_3VtTBX3">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,257.58,204.67,72.21,8.97"><forename type="first">Jochen</forename><forename type="middle">L</forename><surname>Leidner</surname></persName>
							<email>leidner@linguit.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Linguit GmbH</orgName>
								<address>
									<addrLine>Friedensstraße 10</addrLine>
									<postCode>76887</postCode>
									<settlement>Bad, Bergzabern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of the Saarland</orgName>
								<address>
									<country>FR</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">-Speech Signal Processing</orgName>
								<address>
									<addrLine>Building 17.1, Office 0.17</addrLine>
									<postCode>66041</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>2 Buccleuch Place</addrLine>
									<postCode>EH8 9LW</postCode>
									<settlement>Edinburgh</settlement>
									<country>Scotland UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,97.65,148.91,407.70,15.49;1,233.82,170.83,135.36,15.49">Preliminary Experiments with Geo-Filtering Predicates for Geographic IR</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F3F52FBEA197905F1F4B21753C80C8DB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a set of experiments for monolingual English retrieval at GEO-CLEF 2005. We evaluate a technique for spatial retrieval based on named entity tagging, toponym resolution, and re-ranking by means of geographic filtering. To this end, we present a series of systematic experiments in the Vector Space paradigm. We investigate plain bag-of-word versus a kind of phrasal retrieval, the potential of meronymic query expansion as a recall-enhancing device, and compare three alternative geo-spatial filtering techniques based on spatial clipping. We evaluate these on 25 monolingual English queries.</p><p>Our prelimiary results show that always choosing toponym referents based on a simple "maximum population" heuristic to approximate the salience of a referent fails to outperform TF*IDF baselines with the GEO-CLEF 2005 dataset when combined with three geo-filtering predicates. Conservative geo-filtering outperforms more aggresive predicates. The evidence further seems to suggest that query expansion with WordNet meronyms is not effective in combination with the method described.</p><p>A cursory post-hoc analysis indicates that responsible factors for the low performance include sparseness of available population data, gaps in the gazetteer that associates Minumum Bounding Rectangles with geo-terms in the query, and the composition of the GEO-CLEF 2005 dataset itself.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since all human activity relates to places, a large number of information needs also contain a geographic or otherwise spatial aspect. People want to know about the nearest restaurant, about the outcome of the match football match in Manchester, or about how many died in a flood in in Thailand. Traditional IR however, does not accomodate this spatial aspect enough: place names or geographic expressions are merely treated as strings, just like other query terms. This paper presents a general technique to accomodate geographic space in IR, and presents an evaluation of a particular instance of it carried out withn CLEF 2005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The CLEF Evaluation</head><p>The Cross-Language Evaluation Forum (CLEF) 1 is an initiative funded by the European Union, and part of the DELOS Network of Excellence for Digital Libraries (EU FW-6). It aims to study competing IR methods across a variety of languages and tasks in annual international evaluations. &lt;top&gt; &lt;num&gt; GC001 &lt;/num&gt; &lt;orignum&gt; C084 &lt;/orignum&gt; &lt;EN-title&gt;Shark Attacks off Australia and California&lt;/EN-title&gt; &lt;EN-desc&gt; Documents will report any information relating to shark attacks on humans. &lt;/EN-desc&gt; &lt;EN-narr&gt; Identify instances where a human was attacked by a shark, including where the attack took place and the circumstances surrounding the attack. Only documents concerning specific attacks are relevant; unconfirmed shark attacks or suspected bites are not relevant. &lt;/EN-narr&gt; &lt;!--NOTE: This topic has added tags for GeoCLEF --&gt; &lt;EN-concept&gt; Shark attacks &lt;/EN-concept&gt; &lt;EN-spatialrelation&gt;near&lt;/EN-spatialrelation&gt; &lt;EN-location&gt; Australia &lt;/EN-location&gt; &lt;EN-location&gt; California &lt;/EN-location&gt; &lt;/top&gt; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">The GEO-CLEF Track</head><p>In 2005, CLEF for the first time incorporated a track to study the performance of information retrieval strategies that take into account the notion of geographic space. This GEO-CLEF track, organized by the universities Berkeley and Sheffield, have the objective: "to compare methods of query translation, query expansion, translation of geographical references, use of text and spatial retrieval methods separately or combined, retrieval models and indexing methods." (from the CLEF homepage)</p><p>In the first GEO-CLEF track, the languages English (monolingual), German (monolingual and crosslingual), Portuguese and Spanish (cross-lingual) were offered. GEO-CLEF queries contain a geographic aspect (cf. Figure <ref type="figure" coords="2,164.96,498.48,4.15,8.97" target="#fig_0">1</ref>) that express spatial relevance contraints. Figure <ref type="figure" coords="2,371.83,498.48,4.98,8.97">2</ref> lists the topic titles of the 25 English test queries used for GEO-CLEF in 2005. Each of the queries is run against a corpus which is a sub-collection of 56,472 Glasgow Herald documents and 113,005 documents from the LA Times. Obviously, this gives the corpus a distinct Scottish-Californian geographic bias, as we shall se later.</p><p>Paper plan. The remainder of this paper is organized as follows. Section 2 describes the method used to enhance IR with spatial knowledge. We present the experimental results obtained in the GEO-CLEF 2005 evaluation in Section 3, and summarize and conclude in Section 4 with some suggestion for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>This section describes the method used in this study. Figure <ref type="figure" coords="2,339.09,650.00,4.98,8.97" target="#fig_1">3</ref> shows the experimental setup. There are four essential processing steps. A document retrieval engine (IR) retrieves a set of documents relevant to the queries and groups them in a ranked list. A named entity tagging phase (NERC) then identifies all toponyms. Afterwards a toponym resolution (TR) module looks up all candidate referents for each toponym (i.e, the locations that the place name may be referring to) and tries to disambiguate the toponyms based on a heuristic. If successful, it also assigns the latitude/longitude of the centroid of the location to the resolved toponym. For each document-query pair a geo-filtering module (CLIP) then discards all locations outside a Minimum Bounding Rectangle (MBR) that is the denotation of the spatial expression in the query. Finally, based on a so-called geo-filtering predicate, it is decided whether or not the document under investigation is to be discarded, propagating up subsequent documents in the ranking. We now describe each phase in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Retrieval (IR)</head><p>The document retrieval engine provides access to the indexed GEO-CLEF collection. No stop-word filtering or stemming was used at index time, and index access is case-insensitive. The IR engine is used to retrieve the top 1,000 documents for each evaluation query from the collection using the Vector Space Model with a plain vanilla TF*IDF ranking function:</p><formula xml:id="formula_0" coords="4,199.95,455.56,313.05,24.30">score(d, q) = ∑ ∀tinq t f (t, d) id f (t) lengthNorm(t, d)<label>(1)</label></formula><p>([GH05] p. 78 f.). We used the Lucene 1.4.3 search API for vector space retrieval <ref type="bibr" coords="4,414.49,493.69,30.17,8.97" target="#b3">[Cut05,</ref><ref type="bibr" coords="4,446.99,493.69,25.13,8.97" target="#b5">GH05]</ref>. <ref type="foot" coords="4,477.15,491.84,3.69,6.63" target="#foot_1">2</ref> We use Lucene's document analysis functionality for English text without modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Named Entity Tagging (NERC)</head><p>For named entity tagging, we use a Maximum Entropy classifier trained on MUC-7 data <ref type="bibr" coords="4,444.87,552.83,27.75,8.97" target="#b2">[CC03]</ref>. Tagging 1,000 retrieved document is a very expensive procedure; in a production system, this step would probably be carried out at indexing time. Therefore, the retrieved documents are actually pooled across runs to speed up the processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Toponym Resolution (TR)</head><p>For looking up the candidate referents, we use the large-scale gazetteer described in <ref type="bibr" coords="4,445.35,635.88,19.92,8.97">[Lei]</ref> as primary gazetteer, supplemented by the World Gazetteer<ref type="foot" coords="4,280.63,645.99,3.69,6.63" target="#foot_2">3</ref> for population information (as secondary gazetteer). The algorithm used to resolve toponyms to referents works as follows: first, we look up the potential referents with associated latitude/longitude from the primary gazetteer. Then we look up population information for candidate referents from the secondary gazetteer. In order to relate the population entries from the World Gazetteer to corresponding entries of the main gazetteer, we defined a custom equality operator ( . =) between two candidate referents for a toponym T R i such that R 1 . = R 2 holds iff there is a string equality between their toponyms (T R 1 = T R 2 ) and the latitude and longitude of the candidate referents are in the same 1-degree grid (i.e., if and only if</p><formula xml:id="formula_1" coords="5,222.94,388.44,138.94,11.09">[R 1 lat ] = [R 2 lat ] ∧ [R 1 long ] = [R 2 long ]).</formula><p>If there is no population information available, the toponym remains unresolved (partial algorithm). If there is exactly one population entry, the toponym is resolved to that entry. If more than one candidate has population information available, the referent with the largest population is selected. Figure <ref type="figure" coords="5,325.17,424.31,4.98,8.97" target="#fig_2">4</ref> shows the algorithm at work. In the example at the top a case is shown where only population information (prefixed by an asterisk) for one referent is available. This is used as evidence for that referent being the most salient candidate, and consequently it is selected. In the example shown at the bottom, population numbers for two candidate referents are available; the place with the larger number of inhabitant is selected. Note that the coordinates in the two gazetteers need to be rounded in order to ensure the matching of corresponding entries is successful. Out of the 41,360 toponym types<ref type="foot" coords="5,240.89,494.19,3.69,6.63" target="#foot_3">4</ref> , population information was available in the World Gazetteer for some (i.e., more than zero) candidate referents only for 4,085 toponyms. This means that using only the population heuristics, the upper bound for system recall is R = 9.88%, and for F-Score F β=1 = 9.41%, assuming perfect resolution precision. Once the best toponym resolution strategy has become more apparent, it would be preferable to compute it offline and store the results in a spatially-enabled database management system such as PostgreSQL <ref type="bibr" coords="5,255.17,555.82,33.21,8.97" target="#b11">[vOV91]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Geographic Filtering (CLIP)</head><p>In principle, we can introduce a notion of geographic relevance in an existing approach to information retrieval in at least two ways: in a ranking-based approach the relevance metric gets directly modified to take locations in the document and query into account, i.e. instead of, say, using</p><formula xml:id="formula_2" coords="5,252.72,646.02,260.28,8.97">SCORE(d) = TFIDF(d) (2)</formula><p>we need a geographic relevance measure, GEO-SCORE(d), which we may combine with our term-based score using linear interpolation:</p><formula xml:id="formula_3" coords="5,194.29,693.77,318.71,12.98">SCORE (d) = λ SCORE(d) + (1 -λ) GEO-SCORE(d).<label>(3)</label></formula><p>Alternatively, we may use a filtering-based approach such as the one attempted here, in which we apply traditional IR and then identify locations by means of toponym recognition and toponym resolution. We   can then filter out documents or parts of documents that do not fall within our geographic area of interest. Given a polygon P described in a query, and a set of locations L = 1 . . . N mentioned in a document. Be ∆ i an N-dimensional vector of geographic distances on the geoid between the N locations in a text document d (mentioned with absolute frequencies f i ) and the centroid of P. Then we can use a filter predicate GEO-FILTER( f , ∆) to eliminate the document if its spatial "aboutness" is not high enough:</p><formula xml:id="formula_4" coords="8,189.97,293.13,323.03,26.57">SCORE (d, P) = SCORE(d) GEO-FILTER( f d , ∆ d , P) 0 otherwise (4)</formula><p>In filtering the decision is simply between passing through the original IR score or setting it to 0, thus effectively discarding the document from the ranking. Here are the definitions of three simple GEO-FILTER predicates:</p><p>1. ANY-INSIDE. This filter is most conservative and tries to avoid discarding true positives at the risk of under-utilizing the discriminative power of geographic space for IR. It only filters out documents that mention no location in the query polygon P:</p><formula xml:id="formula_5" coords="8,102.45,424.88,410.55,49.95">ANY-INSIDE( f d , ∆ d , P) = true ∃ ∈d : ∈ P f alse otherwise (5) 2. MOST-INSIDE.</formula><p>This filter is slightly more agressive than ANY-INSIDE, but still allows for some noise (locations mentioned that do not fall into the geographic area of interest as described by the query polygon P). It discards all documents that mention more locations that fall outside the query polygon than inside:</p><formula xml:id="formula_6" coords="8,102.45,523.44,410.55,49.95">MOST-INSIDE( f d , ∆ d , P) = true |{ ∈ d| ∈ P}| &gt; |{ ∈ d| / ∈ P}| f alse otherwise (6) 3. ALL-INSIDE.</formula><p>This filter is perhaps too agressive for most purposes; it discards all documents that mention even a single location that fall outside the query polygon P, i.e. all locations must be in the geographic space under consideration:</p><formula xml:id="formula_7" coords="8,218.54,612.08,294.47,23.31">ALL-INSIDE( f d , ∆ d , P) = true ∀ ∈d : ∈ P f alse otherwise (7)</formula><p>Figure <ref type="figure" coords="8,118.50,653.06,4.98,8.97" target="#fig_4">6</ref> summarizes the semantics of the three geo-filtering predicates used.</p><p>In practice, we use Minimal Bounding Rectangles (MBRs) to approximate the polygons described by the locations in the query, which trades runtime performance against retrieval performance. More specifically, we computed the union of the Alexandria Digital Library and ESRI gazetteers (Table <ref type="table" coords="8,493.57,688.93,4.15,8.97" target="#tab_1">2</ref>) to look up MBRs for geographic terms in the GEO-CLEF queries. <ref type="foot" coords="8,347.24,699.03,3.69,6.63" target="#foot_4">5</ref> In cases of multiple candidate referents (e.g. for California), the MBR for the largest feature type was chosen (i.e. in the case of California, the U.S. membership state interpretation). Latin America was not found in the Alexandria Gazetteer. A It would of course have been beneficial for the retrieval performance if the MBRs that were not available in the ESRI and Alexandria gazetteers had been gathered from elsewhere, as there are plenty of sources scattered across the Internet. However, then the experimental outcome would perhaps no longer reflect a typical automatic system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Query Expansion with Meronyms</head><p>Query expansion is typically used as a Recall-enhancing device, because by adding terms to the original query that are related to the original terms, additional relevant documents are retrieved that would not have been covered by the original query, possibly at the expense of Precision. Here, we experimented with meronym query expansion, i.e. with geographic terms that stand in a spatial "part-of" relation (as in "Germany is part of Europe"). We used WordNet 2.0 to retrieve toponyms that stand in a meronym relationship with any geographic term from the query. The choice of WordNet was motivated by the excessive size of both gazetteers used in the toponym resolution step. For each query, we transitively added all constituent geographic entites, e.g. for California we added Orange County as well as Los Angeles. 6  Figure <ref type="figure" coords="9,119.09,708.34,4.98,8.97" target="#fig_5">7</ref> shows the number of terms that are added for each query. For queries 2, 6, 7, 8, 12 and 19 the number of meronyms in WordNet was actually higher than 1,000; however, an analysis revealed that an 6 Apostrophies (""') were eliminated for technical reasons. implementation limit of Lucene was hit that caused a cutoff after 1,000 terms.</p><p>The next section describes the evaluation results. Please consult Appendix A for a description of each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Method</head><p>The GEO-CLEF 2005 evaluation was very similar to previous TREC and CLEF evaluations: for each run, 11-Point-Average Precision against interpolated Recall and R-Precision against retrieved documents are plotted. In addition, difference from median across participants for each topic is reported.</p><p>Traditionally, the relevance judgments in TREC-style evaluations are binary, i.e. a document either meets the information need expressed in a TREC topic (1) or not (0). Intrinsically fuzzy queries (e.g. "shark attacks near Australia") introduce the problem that a strict yes/no decision might no longer be appropriate; there is no "crisp cut-off point. In the same way that the ranking has to be modified to account for geographic distance, a modification of the evaluation procedure ought to be considered. However, for GEO-CLEF 2005, binary relevance assessments were used. Nota Bene. For organizational reasons, this series of experiments did not contribute any documents to the judgment pool for the relevance assessments, which results in a negative bias of the performance results measured compared to the true performance of the experiments and other GEO-CLEF 2005 participants. This is because all relevant documents found by the methods described herein but not returned by any other participants will be have been wrongly assessed as "not relevant". Therefore, a discussion of the relative performance compared to other participants is not included in this paper. On the other hand, this makes the results comparable to future experiments with GEO-CLEF data outside the annual evaluation, which will of course likewise not be able to influence the pooling a posteriori.</p><p>We describe the results obtained and present (a very preliminary) discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Retrieval Performance. In our experiments, the baseline run LTITLE that uses only the topic title and no spatial processing performs surprisingly well (Figure <ref type="figure" coords="10,304.67,741.11,3.60,8.97">8</ref>), with an Average Precision averaged over queries of 23.62% and a Precision at 10 documents of just 36%. Table <ref type="table" coords="12,129.90,381.04,4.98,8.97" target="#tab_2">3</ref> gives a summary of the averaged results for each run. As for the terminology, all run names start the letter L followed by an indicator of how the query was formed. CONC means using the content of the &lt;CONCEPT&gt; tag and posing a phrasal query to the IR engine, CONCPHRSPAT means using the content of both &lt;CONCEPT&gt; and &lt;SPATIAL&gt; tags, and &lt;TITLE&gt; uses the title tag. PHR refers to runs using the IR engine's phrasal query mechanism rather than bag-of-terms. For these runs, queries look as follows:</p><p>( ("Shark Attacks"ˆ2.0) (("shark attack"˜8)ˆ1.5) (Shark Attacks) )</p><p>This combined way of querying takes into account the phrase shark attacks (as subsequent terms in the document only) with twice the weight of the "normal" bag-of-words query (last sub-query). The middle line searches for the lemmatized words shark and attack within an 8-term window and weights this subquery with 1.5. Runs containing ANY, MOST, or ALL as part of their name indicate that geo-filtering with the ANY-INSIDE, MOST-INSIDE or ALL-INSIDE filtering predicates, respectively, was used. Finally, WN as part of a run name indicates that query expansion with WordNet meronyms was applied. Appendix A contains an description of the meaning of the run names. Runtime Performance. The eximental setup for this study was not optimized for runtime performance. The indexing of the GEO-CLEF document collection took 38 minutes (100 minutes wallclock time on a single machine with Network File System). The execution time for the title-only baseline (run LTITLE) was 12 s (runtime averaged over 3 runs). <ref type="foot" coords="12,255.46,609.61,3.69,6.63" target="#foot_5">7</ref> The most expensive operation was the named entity taggin of the result document pool 3,549 min (ca. 60 hours). Gazetteer lookup amounted to 400 min for all pooled result types, and toponym resolution time took 7:30 min, again for all pooled result types. The re-ranking by geographic filtering itself was fast and took only 8 seconds per run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>Applying the "maximum population" heuristic alone to achieve toponym resolution together with geofiltering in general performed poorly and in none of the four series of experiments outperformed a baseline that applied no dedicated spatial processing.  Interestingly, a plain vanilla Vector Space Model with TF-IDF and the obligatory run using title-only queries (LTITLE) performs better than the median across all participant entries for 19 out of 25 (or 76%) of the queries in GEO-CLEF 2005.</p><p>For three geo-filtering predicates tested, a consistent relative pattern could be observed across all runs: The ANY-INSIDE filter almost consistently outperformed (in one case it was en par with) the MOST-INSIDE filter, which in turn always outperformed the ALL-INSIDE filter. While it was expected that MOST-INSIDE would not perform all well as the other two filter types, it is interesting that the conservative ANY-INSIDE outperformed MOST-INSIDE on average.</p><p>The evidence seems to suggest further than geographic query expansion with WordNet meronyms is not effective as a recall-enhancing device, independent on whether or which geo-filter is applied afterwards: average precision at. Note however, that this is true only on average, not for all individual queries. Furthermore two queries were actually not executed by the Lucene engine because the query expansion caused the query to exceed implementation limits (too many query terms).</p><p>Geo-CLEF Methodology. Regarding the modus operandi of GEO-CLEF, future evaluations would benefit from a separation of training/development and test set regarding the queries.</p><p>Furthermore, alternative relevance assessments based on geographic distance rather than binary decisions (document relevant/document not relevant) might be attempted. For instance, Root-Mean-Square Distance (RMSD, Equation <ref type="formula" coords="14,202.69,315.26,4.15,8.97" target="#formula_8">8</ref>) could be used to indicate the (geo-)distance between a query centroid q and a set of location centroids d 1 , . . . , d N in a document:</p><formula xml:id="formula_8" coords="14,235.41,349.09,277.59,27.55">RMSD(d, q) = 1 N N ∑ i=1 (d i -q) 2<label>(8)</label></formula><p>Such an measure could be used to compute a continuous-scale geographic relevance measure once the assessors annotated the test queries and the toponyms in the pooled result documents with their "ground truth" coordinates. Geo-CLEF Dataset. To better understand the low performance of the experiment, we performed a manual analysis of topic GC0001. To find a large pool of potentially relevant documents, we retrieved the results for the queries (shark OR sharks) AND (attack OR attacked OR attacks) (shark OR sharks) AND (kill OR killed OR kills)</p><p>i.e. we are looking for documents in which sharks get mentioned together with the verbs attack (61 hits) or kill (63 hits), respectively, no matter where the potential attack event described happens. Since shark does not have well-known synonyms, and since we use several forms of to attack and the even stronger to kill (in case the attack itself is not focused on but rather its outcome), we expect these two queries to cover most relevant documents for the first topic (together resulting in a pool of size 107 documents). The aim of our microscopic analysis is to find out whether the mechanisms applied are at all meaningful, given the dataset. For example, the GIR method proposed in this paper could be worthless if all mentions of Perth, actually conincided with mentions of Australia, because then the query term Australia would then capture relevant documents directly. Indeed, in document GH951219-000021, we find PERTH : A severed human arm wrapped in a torn piece of wetsuit has washed up on a beach more than three months after a shark killed a 29-year-old scuba diver , police in Australia said .</p><p>We went through the retrieved document set and carried out a relevance assessment, bearing in mind the geography. <ref type="foot" coords="14,133.85,664.31,3.69,6.63" target="#foot_6">8</ref> Table <ref type="table" coords="14,167.05,666.16,4.98,8.97" target="#tab_5">4</ref> shows the result. Only 11 (or less than 10.28%) of retrieved documents are actually about shark attack events, of which 4 contain California and 1 contains Australia. Only one document was dealing with a story outside the geographic scopy of the first GEO-CLEF query (in bold type), whereas  5 documents report about shark attacks in Calfornia while not explicitly mentioning California; these are the interesting cases where geo-filtering or any other dedicated GIR technique could have helped, and this statistics shows that it would have helped recall rather than precision since more relevant documents unretrievable by the geographic search terms in the query would have been retrieved in addition than nonrelevant documents excluded on grounds of geographic irrelevance. Geo-filtering as proposed here could achieve this recall-enhancing function, most likely because of the limited population gazetteer used here, and its precision-enhancing ability could not be demonstrated on the GEO-CLEF 2005 corpus, perhaps because of this unfortunate ratio between out-of-geographic-scope documents of 1/107 at least not for this first query studied. Another problem is that Schotland and California are both used for corpus sampling and as regions in the query. Since many articles contain the place of publication in headers or footers. Since in these experiments, no dedicated position-dependent document analysis was carried out this could have introduced noise. The fact that the second half of the queries performs much lower is due to the fact that despite merging two gazetteer sources, our gazetteer used is still not dense enough to cover e.g. the Scottish Trossachs, the Scottish Highlands or even Siberia. Finally, the number of queries in GEO-CLEF 2005 was quite small (only 25 queries). As a result, the problems mentioned before can in combination easily overshadow any algorithm's performance, which a more detailed analysis would have to show.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Conclusions</head><p>We have described a method for geographic information retrieval based on named entity tagging to identify place names (or toponym recognition, geo-parsing), toponym resolution (or geo-coding, place name disambiguation) and geographic filtering (or clipping).</p><p>First results show that a very simple method for toponym resolution based on a "maximum population" heuristic is not effective when combined with three point-in-MBR geo-filtering predicates in the setting used. We conjecture this may be due to the lack of available population data. In addition, we discovered that geographic query expansion with WordNet meronyms appears not to improve retrieval performance. However, a deeper analysis of the results will be necessary before drawing any definite conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Future Work</head><p>For future work, several opportunities for further study should be given consideration:</p><p>1. The results presented here should be compared the with different, more sophisticated clipping criteria that take the amount of spatial overlap into account. For example, instead of using MBRs computed from sets of centroid points <ref type="bibr" coords="16,229.20,174.54,33.75,8.97" target="#b0">[AJT01]</ref> proposes a Dynamic Spatial Approximation Method (DSAM), which uses Vonoroi approximation to compute more precise polygons from sets of points. Once polygons are available, spatial overlap metrics can be applied to improve retrieval <ref type="bibr" coords="16,442.34,198.45,26.80,8.97" target="#b8">[RF04]</ref>.</p><p>2. It is vital to discover methods to determine a good balance when weighting the spatial influence and the term influence in the query against each other in a principled way, probably even dependent on the query type.</p><p>3. On the query side, the specific spatial relations should be taking into account. However, this requires defining how users and/or CLEF assessors actually judge different relations beforehand (how near does something have to be to be considered "near"?).</p><p>4. On the document side, text-local relationships from the toponym context should be taken into account. Right now, all toponyms (LOC) are considered equal, which does not utilize knowledge from the context of their occurrence. For instance, a document collection that has one mention of New York in every document footer because the news agency resides in New York can pose a problem.</p><p>5. The impact of the particular gazetteer used for query expansion and toponym resolution ought to be studied with respect to the dimensions size/density (UN-LOCODE/WordNet versuss NGA GeoNames) and local/global (e.g. EDINA DIGIMAP versus NGA GeoNames).</p><p>6. Last but perhaps most importantly, more sophisticated toponym resolution strategies (e.g. <ref type="bibr" coords="16,472.05,405.67,35.83,8.97" target="#b7">[LSW03]</ref>) should be compared against the simple population heuristic used in this study. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Descriptions of Runs</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,185.02,336.28,232.95,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Anatomy of a GEO-CLEF Query (GC001).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,203.56,336.40,195.88,8.97"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Experimental Setup Used in this Study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,153.62,651.56,295.77,8.97;6,90.00,388.63,422.90,247.81"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Toponym Resolution Using the Maximum-Population Heuristic.</figDesc><graphic coords="6,90.00,388.63,422.90,247.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,126.34,592.18,350.33,8.97"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Number of Toponyms Resolved in Document Against Number of Documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,90.00,170.52,422.75,8.97;8,90.25,182.48,59.26,8.97"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Three Simple Geo-Filtering Predicates: ANY-INSIDE (left), MOST-INSIDE (middle) and ALL-INSIDE (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,158.21,324.28,286.59,8.97"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Geographic Query Expansion with Meronyms from WordNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="13,123.36,706.61,356.28,8.97"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: The run LCONCPHRSPATANY shows that robustness across queries is an issue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,158.51,268.84,285.98,327.59"><head>Table 1 :</head><label>1</label><figDesc>List of the Most Frequent Toponyms in the GEO-CLEF corpus. Toponyms in bold type are artifacts of the Glasgow/California bias of the corpus.</figDesc><table coords="3,158.51,268.84,285.98,327.59"><row><cell>GC001 Shark Attacks off Australia and California</cell></row><row><cell>GC002 Vegetable Exporters of Europe</cell></row><row><cell>GC003 AI in Latin America</cell></row><row><cell>GC004 Actions against the fur industry in Europe and the U.S.A.</cell></row><row><cell>GC005 Japanese Rice Imports</cell></row><row><cell>GC006 Oil Accidents and Birds in Europe</cell></row><row><cell>GC007 Trade Unions in Europe</cell></row><row><cell>GC008 Milk Consumption in Europe</cell></row><row><cell>GC009 Child Labor in Asia</cell></row><row><cell>GC010 Flooding in Holland and Germany</cell></row><row><cell>GC011 Roman cities in the UK and Germany</cell></row><row><cell>GC012 Cathedrals in Europe</cell></row><row><cell>GC013 Visits of the American president to Germany</cell></row><row><cell>GC014 Environmentally hazardous Incidents in the North Sea</cell></row><row><cell>GC015 Consequences of the genocide in Rwanda</cell></row><row><cell>GC016 Oil prospecting and ecological problems in Siberia</cell></row><row><cell>GC017 American Troops in Sarajevo, Bosnia-Herzegovina</cell></row><row><cell>GC018 Walking holidays in Scotland</cell></row><row><cell>GC019 Golf tournaments in Europe</cell></row><row><cell>GC020 Wind power in the Scottish Islands</cell></row><row><cell>GC021 Sea rescue in North Sea</cell></row><row><cell>GC022 Restored buildings in Southern Scotland</cell></row><row><cell>GC023 Murders and violence in South-West Scotland</cell></row><row><cell>GC024 Factors influencing tourist industry in Scottish Highlands</cell></row><row><cell>GC025 Environmental concerns in and around the Scottish Trossachs</cell></row><row><cell>Figure 2: Test Queries Used in GEO-CLEF 2005.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,90.00,112.82,423.00,413.83"><head>Table 2 :</head><label>2</label><figDesc>Minimal bounding rectangles (MBRs) from the Alexandria and ESRI gazetteers. MBRs are given as pairs of points, each with lat/long in degrees. A dash means that no result was found or that a centroid point was available only. manual search for South America also did not retrieve the continent, but found several other hits, e.g. South America Island in Alaska. Holland was recognized by the Alexandria Gazetteer as a synonym for the Netherlands. While this corresponds to typical usage, formally speaking Holland refers to a part of the Netherlands. The ESRI server returned two entries for Caspian Sea, one as given in the table, another with MBR (41.81; 50.54), (42.21; 50.94)-since they share the same feature type they could not otherwise be distinguished. Finally, the software module CLIP performs geographic filtering of a document given an MBR, very much like the clipping operation found in typical GIS packages, albeit on unstructured documents.</figDesc><table coords="9,119.52,112.82,363.96,250.46"><row><cell>Expression</cell><cell>Alexandria MBR</cell><cell>ESRI MBR</cell></row><row><cell>Asia</cell><cell>(0; 0), (90; 180)</cell><cell>-</cell></row><row><cell>Australia</cell><cell cols="2">(-45.73; 111.22), (-8.88; 155.72) (-47.5; 92.2), (10.8; 179.9)</cell></row><row><cell>Europe</cell><cell>(35.0; -30.0), (70.0; 50.0)</cell><cell>(35.3; -11.5), (81.4; 43.2)</cell></row><row><cell>Latin America</cell><cell>-</cell><cell>(-55.4; -117), (32.7; -33.8)</cell></row><row><cell cols="2">Bosnia-Herzegovina (42.38; 15.76), (45.45; 20.02)</cell><cell>-</cell></row><row><cell>Germany</cell><cell>(46.86; 5.68), (55.41; 15.68)</cell><cell>(47.27; 5.86), (55.057; 15.03)</cell></row><row><cell>Holland</cell><cell>(50.56; 3.54), (53.59; 7.62)</cell><cell>(51.29; 5.08), (51.44; 5.23)</cell></row><row><cell>Japan</cell><cell>(30.1; 128.74), (46.26; 146.46)</cell><cell>(24.25; 123.68), (45.49; 145.81)</cell></row><row><cell>Rwanda</cell><cell>(-3.01; 28.9), (-1.03; 31.2)</cell><cell>(-2.83; 28.85), (-1.05; 30.89)</cell></row><row><cell>UK</cell><cell>(49.49; -8.41), (59.07; 2.39)</cell><cell>(49.96; -8.17), (60.84; 1.75)</cell></row><row><cell>United States</cell><cell>(13.71; -177.1), (76.63; -61.48)</cell><cell>(18.93; -178.22), (71.35;-68)</cell></row><row><cell>California</cell><cell cols="2">(32.02; -124.9), (42.51; -113.61) -</cell></row><row><cell>Scotland</cell><cell>-(56.0; -4.0)</cell><cell>(54.63; -8.62), (60.84; -0.76)</cell></row><row><cell>Siberia</cell><cell>-(60.0; 100.0)</cell><cell>-</cell></row><row><cell>Scottish Islands</cell><cell>-</cell><cell>-</cell></row><row><cell>Scottish Trossachs</cell><cell>-(49.63; -104.22)</cell><cell>-</cell></row><row><cell>Scottish Highlands</cell><cell>-(57.5; -4.5)</cell><cell>-</cell></row><row><cell>Sarajevo</cell><cell>-(43.86; 18.39)</cell><cell>(43.65; 18.18), (44.05; 18.58)</cell></row><row><cell>Caspian Sea</cell><cell>-(42.0; 50.0)</cell><cell>(45; 48.41), (42.40; 48.81)</cell></row><row><cell>North Sea</cell><cell>-(55.33; 3.0)</cell><cell>(58.04; 1.02), (58.44; 1.42)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,90.00,213.76,423.00,507.80"><head>Table 3 :</head><label>3</label><figDesc>GEO-CLEF 2005 result summary.</figDesc><table coords="11,90.00,213.76,423.00,507.80"><row><cell></cell><cell></cell><cell></cell><cell cols="11">CLEF 2005 GC-Monolingual-EN -Interpolated Recall vs Average Precision</cell></row><row><cell cols="2">100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LTITLE</cell></row><row><cell></cell><cell>90%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average Precision</cell><cell>40% 50% 60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0% 0%</cell><cell></cell><cell>10%</cell><cell>20%</cell><cell cols="2">30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell cols="2">70%</cell><cell>80%</cell><cell cols="2">90%</cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Interpolated Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">CLEF 2005 GC-Monolingual-EN -Comparison to median by topic (topics 1 to 13)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LTITLE</cell></row><row><cell>Difference (average precision)</cell><cell>-0.5 0 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-1</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Topic Number</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">CLEF 2005 GC-Monolingual-EN -Comparison to median by topic (topics 14 to 25)</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LTITLE</cell></row><row><cell>Difference (average precision)</cell><cell>-0.5 0 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-1</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell><cell>19</cell><cell>20</cell><cell>21</cell><cell>22</cell><cell cols="2">23</cell><cell>24</cell><cell>25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Topic Number</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="15">Figure 8: The automatic topic title run (LTITLE): Average Precision (left) and performance relative to the</cell></row><row><cell cols="4">median across participants (right).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="15,204.14,337.18,194.73,8.97"><head>Table 4 :</head><label>4</label><figDesc>Documents About Shark Attack Events.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,104.35,741.22,138.68,6.26"><p>http://www.clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,104.35,727.73,408.66,7.17;4,90.00,737.20,52.03,7.17"><p>There is also CLucene, a faster C implementation[van05], but at the time of writing it is slightly less mature than the Java implementation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,104.35,747.00,83.78,7.17"><p>http://worldgazetteer.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,104.35,747.00,312.81,7.17"><p>This number excludes "toponyms" that start with a digit (false positives caused by the NE tagger).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="8,104.35,744.25,186.52,7.17"><p>On the query side, manual disambiguation was performed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="12,104.35,737.54,408.65,7.17;12,90.00,747.00,293.33,7.17"><p>Runtime performance is based on a 1-CPU Fujitsu-Siemens SCENIC W600-i865G with Intel Pentium 4 processor (2994 MHz, 1 MB cache, 5931 BogoMIPS) running Novel SuSE Linux 9.1 kernel version 2.6.8-24-smp.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="14,104.35,709.15,298.74,7.17;14,90.00,718.61,423.00,7.17;14,90.00,728.08,423.00,7.17;14,90.00,737.54,423.00,7.17;14,90.00,747.00,39.96,7.17"><p>Funnily enough, documents LA030994-0075, LA053094-0133, LA121594-0181 and LA121594-0267 contain stories of type "man bites dog": they report about a large initiative of people killing sharks, not vice versa, which means we have to judge them not relevant. We do not count LA103094-0316 as relevant, which reports that former Australian prime minister Holt was believed to have been eaten by sharks, since it is not actually a report on an established shark attack event.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The author is grateful to <rs type="person">Prof. Dr. Dietrich Klakow</rs> for discussions of the results and for hosting him in his lab during a visit in 2005. This work was supported in part by the <rs type="programName">SOCRATES program</rs> of the <rs type="funder">European Union</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3VtTBX3">
					<orgName type="program" subtype="full">SOCRATES program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="16,140.73,553.31,372.27,8.97;16,140.73,565.27,372.27,8.97;16,140.73,577.23,213.20,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,388.94,553.31,124.06,8.97;16,140.73,565.27,243.94,8.97">Voronoi-based region approximation for geographical information retrieval with gazetteers</title>
		<author>
			<persName coords=""><forename type="first">Harith</forename><surname>Alani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><surname>Tudhope</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,393.35,565.27,119.65,8.97;16,140.73,577.23,119.98,8.97">International Journal of Geographical Information Science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="287" to="306" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,140.73,597.15,372.27,8.97;16,140.73,609.11,353.68,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="16,302.10,597.15,210.90,8.97;16,140.73,609.11,212.24,8.97">Practical similarity ranking. EU Project Deliverable, Spatially-Aware Information Retrieval on the Internet</title>
		<author>
			<persName coords=""><forename type="first">Avi</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Van Kreveld</surname></persName>
		</author>
		<idno>IST-2001-35047 D18</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">5302</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,140.73,629.03,372.27,8.97;16,140.73,640.99,372.27,8.97;16,140.73,652.94,168.79,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,291.55,629.03,221.45,8.97;16,140.73,640.99,23.01,8.97">Language independent NER using a maximum entropy tagger</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,180.44,640.99,327.86,8.97">Proceedings of the Seventh Conference on Natural Language Learning (CoNLL-03)</title>
		<meeting>the Seventh Conference on Natural Language Learning (CoNLL-03)<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="164" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,140.73,672.87,259.34,8.97" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="16,201.34,672.87,27.26,8.97">Lucene</title>
		<author>
			<persName coords=""><forename type="first">Doug</forename><surname>Cutting</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org/" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,140.73,692.79,372.27,8.97;16,140.73,704.75,372.27,8.97;16,140.73,716.70,266.22,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,316.83,692.79,196.17,8.97;16,140.73,704.75,98.63,8.97">Building a geographical ontology for intelligent spatial search on the Web</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">I</forename><surname>Abdelmoty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,258.48,704.75,254.51,8.97;16,140.73,716.70,117.10,8.97">Proceedings of IASTED International Conference on Databases and Applications (DBA-2005)</title>
		<meeting>IASTED International Conference on Databases and Applications (DBA-2005)<address><addrLine>Innsbruck, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>IASTED</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,140.73,736.63,372.27,8.97" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Otis</forename><surname>Gospodnetić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Hatcher</surname></persName>
		</author>
		<title level="m" coord="16,285.91,736.63,65.82,8.97">Lucene in Action</title>
		<meeting><address><addrLine>Greenwich, CT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Manning</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,140.73,112.02,372.27,8.97;17,140.73,123.98,367.16,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,220.16,112.02,214.31,8.97">An evaluation dataset for the toponym resolution task</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jochen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Leidner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,442.81,112.02,70.20,8.97;17,140.73,123.98,110.96,8.97">Computers, Environment and Urban Systems</title>
		<imprint/>
	</monogr>
	<note>Special Issue on Geographic Information Retrieval. in press</note>
</biblStruct>

<biblStruct coords="17,140.73,143.90,372.27,8.97;17,140.73,155.86,372.27,8.97;17,140.73,167.81,372.27,8.97;17,140.73,179.77,372.27,8.97;17,140.73,191.72,338.14,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="17,364.25,143.90,148.75,8.97;17,140.73,155.86,184.40,8.97">Grounding spatial named entities for information extraction and question answering</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jochen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gail</forename><surname>Leidner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bonnie</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,344.33,155.86,168.67,8.97;17,140.73,167.81,372.27,8.97;17,140.73,179.77,372.27,8.97;17,140.73,191.72,137.07,8.97">Proceedings of the Workshop on the Analysis of Geographic References held at the Joint Conference for Human Language Technology and the Annual Meeting of the Noth American Chapter of the Association for Computational Linguistics 2003 (HLT/NAACL&apos;03)</title>
		<meeting>the Workshop on the Analysis of Geographic References held at the Joint Conference for Human Language Technology and the Annual Meeting of the Noth American Chapter of the Association for Computational Linguistics 2003 (HLT/NAACL&apos;03)<address><addrLine>Edmonton, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="31" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,140.73,211.65,372.27,8.97;17,140.73,223.60,372.27,8.97;17,140.73,235.56,372.27,8.97;17,140.73,247.51,314.96,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="17,292.71,211.65,220.29,8.97;17,140.73,223.60,123.68,8.97">Spatial ranking methods for geographic information retrieval (GIR) in digital libraries</title>
		<author>
			<persName coords=""><forename type="first">Larson</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patricia</forename><surname>Frontiera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,282.46,223.60,230.54,8.97;17,140.73,235.56,153.54,8.97">Research and Advanced Technology for Digital Libraries, 8th European Conference, ECDL 2004</title>
		<title level="s" coord="17,193.58,247.51,139.13,8.97">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">September 12-17, 2004. 2004</date>
			<biblScope unit="volume">3232</biblScope>
			<biblScope unit="page" from="45" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,140.73,267.44,372.27,8.97;17,140.73,279.39,372.27,8.97;17,140.73,291.35,365.10,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="17,289.19,267.44,223.81,8.97;17,140.73,279.39,24.85,8.97">Disambiguating geographic names in a historical digital library</title>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Crane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,184.42,279.39,328.58,8.97;17,140.73,291.35,147.59,8.97">Proceedings of the 5th European Conference on Research and Advanced Technology for Digital Libraries (ECDL &apos;01)</title>
		<meeting>the 5th European Conference on Research and Advanced Technology for Digital Libraries (ECDL &apos;01)<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,140.73,311.27,335.89,8.97" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><surname>Ben Van Klinken</surname></persName>
		</author>
		<ptr target="http://sourceforge.net/projects/clucene/" />
		<title level="m" coord="17,213.98,311.27,33.64,8.97">CLucene</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,140.73,331.20,372.27,8.97;17,140.73,343.15,372.27,8.97;17,140.73,355.11,62.27,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="17,283.90,331.20,209.93,8.97">Building a GIS on top of the open DBMS Postgres</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Van Oosterom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vijlbrief</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,140.73,343.15,342.32,8.97">Proceedings of the European Conference on Geographic Information Systems (EGIS)</title>
		<meeting>the European Conference on Geographic Information Systems (EGIS)</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="775" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,140.73,375.03,372.27,8.97;17,140.73,386.99,372.27,8.97;17,140.73,398.94,22.42,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="17,438.83,375.03,74.16,8.97;17,140.73,386.99,249.51,8.97">Multi-dimensional scattered ranking methods for geographic information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Iris</forename><surname>Marc Van Kreveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Avi</forename><surname>Reinbacher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roelof</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,397.54,386.99,61.72,8.97">GeoInformatica</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="84" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
