<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,215.93,74.47,302.91,12.19;1,202.80,90.55,189.75,12.19">at CLEF 2005: Cross-Language Spoken Document Retrieval (CL-SR) Experiments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,207.12,135.27,102.54,8.74"><forename type="first">Adenike</forename><forename type="middle">M</forename><surname>Lam-Adesina</surname></persName>
							<email>adenike@computing.dcu.ie</email>
						</author>
						<author>
							<persName coords="1,316.88,135.27,71.40,8.74"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gjones@computing.dcu.ie</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,215.93,74.47,302.91,12.19;1,202.80,90.55,189.75,12.19">at CLEF 2005: Cross-Language Spoken Document Retrieval (CL-SR) Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7E7EB698460DE6D92BBE3D3089FAF0F9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval -Relevance Feedback</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation Cross-language spoken document retrieval, Transcription segmentation, Pseudo relevance feedback, Metadata combination</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Dublin City University participation in the CLEF CL-SR 2005 task concentrated on exploring the application of our existing information retrieval methods based on the Okapi model to the conversational speech data set. This required an approach to determining approximate sentence boundaries within the free-flowing automatic transcription provided. We also performed exploratory experiments on the use of the metadata provided with the document transcriptions. Topics were translated into English using Systran V3.0 machine translation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Dublin City University participation in the CLEF CL-SR 2005 task concentrated on exploring the application of our existing information retrieval methods based on the Okapi model for this data set, and exploratory experiments on the use of the provided document metadata. Our official submissions included both the English monolingual and French bilingual runs. This paper reports additional results for German and Spanish bilingual runs. Topics were translated into English using Systran V3.0 machine translation system. The resulting English topics were used for retrieving from the English document collection.</p><p>Our standard Okapi retrieval system incorporates a summary-based pseudo relevance feedback (PRF) stage. This PRF system operates by selecting topic expansion terms from document summaries, full details are described in <ref type="bibr" coords="1,124.38,562.71,10.61,8.74" target="#b0">[1]</ref>. However, since the automated transcriptions of the conversational speech documents do not contain punctuation, we needed to develop a method of selecting significant document segments. Details of this method are described in Section 2.1.</p><p>The document transcriptions are provided with a rich set of metadata. It is not immediately clear how best to exploit this most effectively in retrieval. This paper reports our initial exploratory experiments in making use of this additional information.</p><p>The remainder of this paper is structured as follows: Section 2 overviews our retrieval system and describes our sentence boundary creation technique, Section 3 presents the results of our experimental investigations, and Section 4 concludes the paper with a discussion of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Setup</head><p>The basis of our experimental system is the City University research distribution version of the Okapi system <ref type="bibr" coords="1,510.66,696.51,10.64,8.74" target="#b1">[2]</ref>. The documents and search topics were processed to remove stopwords from a list of about 260 words, suffix stripped using the Okapi implementation of Porter stemming <ref type="bibr" coords="1,319.95,719.49,11.67,8.74" target="#b2">[3]</ref> and terms were indexed using a small standard set of synonyms. None of these procedures were adapted for the new CL-SR document set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Term Weighting</head><p>Document terms were weighted using the Okapi BM25 weighting scheme developed in <ref type="bibr" coords="2,423.47,103.47,11.69,8.74" target="#b1">[2]</ref> calculated as follows,</p><formula xml:id="formula_0" coords="2,165.61,123.27,295.27,33.78">) , ( ))) ( ( ) 1 (( * 1 ) 1 1 ( ) , ( ) ( ) , ( j i tf j ndl b b K K j i tf i cfw j i cw + × + - + × × = (1)</formula><p>where cw(i,j) represents the weight of term i in document j, cfw(i) is the standard collection frequency weight, tf(i,j) is the document term frequency, and ndl(j) is the normalized document length. ndl(j) is calculated as ndl(j) = dl(j)/avdl where dl(j) is the length of j and avdl is the average document length for all documents. k1 and b are empirically selected tuning constants for a particular collection. k1 is designed to modify the degree of effect of tf(i,j), while constant b modifies the effect of document length. High values of b imply that documents are long because they are verbose, while low values imply that they are long because they are multi-topic. Our values are tuned based on the training topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pseudo-Relevance Feedback</head><p>We apply PRF for query expansion using a summary-based method described in <ref type="bibr" coords="2,397.05,284.10,11.68,8.74" target="#b0">[1]</ref> which has been shown to be effective in our previous submissions to CLEF, including <ref type="bibr" coords="2,317.43,295.62,11.68,8.74" target="#b4">[4]</ref> and elsewhere. The main challenge for query expansion is the selection of appropriate terms from the assumed relevant documents. Our query expansion method selects terms from summaries of the top ranked relevant document. All non-stopwords in the summaries are ranked using a slightly modified version of the Robertson selection value (rsv</p><formula xml:id="formula_1" coords="2,279.85,330.12,221.23,34.89">) [] shown in equation (2). rsv ) ( ) ( ) ( i rw i r i × = ( 2 )</formula><p>where r(i) = number of relevant documents containing term i, and rw(i) is the standard Robertson/Sparck Jones relevance weight</p><formula xml:id="formula_2" coords="2,141.16,393.47,288.26,51.32">[2], ) 5 . 0 ) ( )( 5 . 0 ) ( ) ( ( ) 5 . 0 ) ( ) ( )( 5 . 0 ) ( ( log ) ( + - + - + + - - + = rw i r R i r i n i r R i n N i r i</formula><p>where n(i) = the total number of documents containing term i, r(i) = the total number of relevant documents term i occurs in, R = the total number of relevant documents for this query, and N = the total number of documents</p><p>The top ranked terms are then added to the topic. In our modified version of the rsv(i), potential expansion terms are selected from the summaries of the top ranked documents, but ranked using statistics from top a large number of assumed relevant ranked documents from the initial run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Sentence Selection</head><p>Our standard process for summary generation is to select representative sentences from the document. Since the document transcriptions do not contain punctuation marking we needed an alternative approach to identifying significant units in the transcription. We approached this using a method derived from Luhn's word cluster hypothesis. Luhn's hypothesis states that significant words separated by not more than 5 non-significant words are strongly related. Strongly related word clusters were identified in the running document transcription by searching for word groups separated by not more than 5 insignificant words, as shown in Figure <ref type="figure" coords="2,475.60,606.22,3.75,8.74">1</ref>. Note that words appearing between clusters are not included in clusters, but can be ignored for the purposes of query expansion since they are by definition stop words. Luhn's Keyword Cluster Method Luhn's method assigns a sentence score for highest scoring cluster within a sentence. We adopted this method to assign a cluster score as follows:</p><formula xml:id="formula_3" coords="3,272.64,97.05,45.44,25.41">TW SW SS 2 1 =</formula><p>where SS1 = the sentence score SW = the number of bracketed significant words (in this case 6) TW = the total number of bracketed words (in this case 14)</p><p>Query-Bias Method This method assigns a score to each sentence based on the number of query terms in the sentence as follows:</p><formula xml:id="formula_4" coords="3,274.14,217.89,44.42,25.71">NQ TQ SS 2 2 =</formula><p>where SS2 = the sentence score TQ = the number of query terms present in the sentence NQ = the number of terms in a query</p><p>The overall score for each sentence (cluster) was then form by summing these two measures for each sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Investigation</head><p>This section describes the establishment of the parameters of our experimental system and gives results from our investigations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Selection of System Parameters</head><p>In order to set the appropriate parameters for our feedback runs, we carried out development runs using the CLSR 2005 training topics. The Okapi parameters were set as follows k1=1.4 b=0.8. For all our PRF runs, 5 documents were assumed relevant for term selection and document summaries comprised the best scoring 4 clusters. The rsv values to rank the potential expansion terms were estimated based on the top 20 or 40 ranked assumed relevant documents. The top 20 ranked expansion terms taken from the clusters were added to the original query in each case. Based on results from our previous experiments in CLEF, the original topic terms are up-weighted by a factor of 3.5 relative to terms introduced by PRF. For our runs we used either the Title section (dcu*tit) or the Title and Description (dcu*desc) section of each topic. Our official runs are marked *. Initial baseline monolingual results using English without query expansion are also given for comparison.</p><p>For our experiments the document fields were combined as follows, dcua2 -combination of ASRTEXT2004A and AUTOKEYWORDA1 dcua1a2 -combination of ASRTEXT2004A, AUTOKEYWORDA1 and AUTOKEYWORDA2 dcusum -combination of ASRTEXT2004A, AUTOKEYWORDA1 and AUTOKEYWORDA2 and the SUMMARY dcuall -combination of NAME, MANUALKEYWORD, SUMMARY and ASRTEXT2004A section of each documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>Tables 1-4 show results of our experiments using these different data combinations for the 25 test topics released for the CLEF 2005 CL-SR task. Results shown are Mean Average Precision (MAP), total relevant retrieved (Rr), and precision at cutoffs of 10 and 30 documents. Topic languages used are English, French, German and Spanish. Topics were translated into English using the Systran V3.0 machine translation system. The upper set of results in each table shows combined Title and Description topic queries and the lower set Title only topic queries.  <ref type="table" coords="4,95.79,217.17,3.91,8.74">1</ref>: Results using a combination of ASRTEXT2004A and AUTOKEYWORDA1, the Title or Title and Description topic fields. Expansion terms ranked for selection using statistics of 40 top ranked documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Results in Table <ref type="table" coords="4,138.85,251.67,5.01,8.74">1</ref> show results for combination of ASRTEXT2004A with AUTOKEYWORDA1. It can be seen that the PSF method improves results for the English topics. Also that the results using Title only topics is better than using the combined Title and Description topics with respect to MAP. This result is perhaps a little surprising since the latter are generally found to be perform better and we are investigating the reasons for the results observed here. However, the number of relevant documents retrieved is generally higher when using the combined topics. Cross language results using French topics are shown to perform better than monolingual English for both MAP and relevant retrieved. This is again unusual, but not unprecedented. Results for translated German and Spanish show a reduction on the monolingual results.   <ref type="table" coords="5,95.89,263.13,3.91,8.74">4</ref>: Results using a combination of NAME, MANUALKEYWORD, SUMMARY and ASRTEXT2004A section of each document, the Title or Title and Description topic fields. Expansion terms ranked for selection using statistics of 40 top ranked documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Table <ref type="table" coords="5,96.05,309.15,5.01,8.74">4</ref> shows a final set of experiments combining the NAME, MANUALKEYWORD and SUMMARY fields with ASRTEXT2004A. These results show large improvements over the results shown in previous tables. Performance for Title only and Title and Description combined topics is now similar with neither clearly showing an advantage. Monolingual English performance is clearly better than results for translated French topics, while our PRF method is still shown to be effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Further Work</head><p>Our initial experiments with the CLEF 2005 CL-SR task illustrate a number of points: PRF can be successfully applied to this data set, that the different fields of the document set make varying contributions to information retrieval effectiveness. In general in can be seen that manual assigned fields are more usefully than the automatically generated ones.</p><p>These experiments only represent a small subset of those that are possible with this dataset. In order to better understand the usefulness of document fields and retrieval methods more detailed analysis of these existing results and further experiments are planned.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,223.63,709.17,148.09,8.74"><head>…</head><label></label><figDesc>Fig 1. Example of Sentence creation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,70.56,355.59,454.40,364.72"><head>Table 2 :</head><label>2</label><figDesc>Results using a combination of ASRTEXT2004A, AUTOKEYWORDA1 and AUTOKEYWORDA2, the Title or Title and Description topic fields. Expansion terms ranked for selection using statistics of 40 top ranked documents.Table2shows results for the same experiments to those in Table1with the addition of AUTOKEYWORDA2 to the documents. Results here are generally show similar trends to those in Table1with small absolute increases in performance in most cases. In this case the performance advantage of French topics over English topics with PRF has largely disappeared.</figDesc><table coords="4,106.56,355.59,376.23,364.72"><row><cell>-id</cell><cell>Topic Lang.</cell><cell>MAP</cell><cell>Rr</cell><cell>P10</cell><cell>P30</cell></row><row><cell>dcua1a2desc40f</cell><cell>Baseline</cell><cell>0.0464</cell><cell>500</cell><cell>0.1880</cell><cell>0.1053</cell></row><row><cell></cell><cell>English</cell><cell>0.0670</cell><cell>784</cell><cell>0.1840</cell><cell>0.1480</cell></row><row><cell></cell><cell>French</cell><cell>0.0943</cell><cell>773</cell><cell>0.2160</cell><cell>0.1707</cell></row><row><cell></cell><cell>German</cell><cell>0.0455</cell><cell>611</cell><cell>0.0960</cell><cell>0.0920</cell></row><row><cell></cell><cell>Spanish</cell><cell>0.0640</cell><cell>765</cell><cell>0.1640</cell><cell>0.1280</cell></row><row><cell>dcua1a2tit40f</cell><cell>Baseline</cell><cell>0.0796</cell><cell>472</cell><cell>0.2280</cell><cell>0.1600</cell></row><row><cell></cell><cell>English</cell><cell>0.1101*</cell><cell>727</cell><cell>0.2520</cell><cell>0.1960</cell></row><row><cell></cell><cell>French</cell><cell>0.1064*</cell><cell>768</cell><cell>0.2600</cell><cell>0.1907</cell></row><row><cell></cell><cell>German</cell><cell>0.0740</cell><cell>691</cell><cell>0.1720</cell><cell>0.1493</cell></row><row><cell></cell><cell>Spanish</cell><cell>0.0912</cell><cell>679</cell><cell>0.2200</cell><cell>0.1560</cell></row><row><cell>Run-id</cell><cell>Topic Lang.</cell><cell>MAP</cell><cell>Rr</cell><cell>P10</cell><cell>P30</cell></row><row><cell>dcusumdesc40f</cell><cell>Baseline</cell><cell>0.1047</cell><cell>598</cell><cell>0.2240</cell><cell>0.1707</cell></row><row><cell></cell><cell>English</cell><cell>0.1472</cell><cell>889</cell><cell>0.2720</cell><cell>0.2173</cell></row><row><cell></cell><cell>French</cell><cell>0.1544</cell><cell>856</cell><cell>0.2600</cell><cell>0.2160</cell></row><row><cell></cell><cell>German</cell><cell>0.1083</cell><cell>696</cell><cell>0.1640</cell><cell>0.1373</cell></row><row><cell></cell><cell>Spanish</cell><cell>0.1074</cell><cell>860</cell><cell>0.1680</cell><cell>0.1520</cell></row><row><cell>Dcusumtit40f</cell><cell>Baseline</cell><cell>0.1407</cell><cell>618</cell><cell>0.2840</cell><cell>0.2160</cell></row><row><cell></cell><cell>English</cell><cell>0.1672</cell><cell>770</cell><cell>0.2920</cell><cell>0.2427</cell></row><row><cell></cell><cell>French</cell><cell>0.1654*</cell><cell>837</cell><cell>0.3080</cell><cell>0.2507</cell></row><row><cell></cell><cell>German</cell><cell>0.1100</cell><cell>738</cell><cell>0.2200</cell><cell>0.1600</cell></row><row><cell></cell><cell>Spanish</cell><cell>0.1542</cell><cell>736</cell><cell>0.2840</cell><cell>0.1296</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,70.56,723.63,454.25,31.72"><head>Table 3 :</head><label>3</label><figDesc>Results using a combination of ASRTEXT2004A, AUTOKEYWORDA1 and AUTOKEYWORDA2 and the SUMMARY section of each document, the Title or Title and Description topic fields. Expansion terms ranked for selection using statistics of 40 top ranked documents.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,70.56,84.63,454.36,187.24"><head>Table 3</head><label>3</label><figDesc>shows results for a further set of experiments with the SUMMARY field added to the document descriptions. Results here show large increases compared to those in Table2, indicating that the contents of the SUMMARY field are useful descriptions of the documents.</figDesc><table coords="5,70.56,131.13,412.19,140.74"><row><cell>Run-id</cell><cell>Topic Lang.</cell><cell>MAP</cell><cell>Rr</cell><cell>P10</cell><cell>P30</cell></row><row><cell>Dcualldesc40f</cell><cell>Baseline</cell><cell>0.2213</cell><cell>1031</cell><cell>0.3680</cell><cell>0.2707</cell></row><row><cell></cell><cell>English</cell><cell>0.3073</cell><cell>1257</cell><cell>0.4880</cell><cell>0.3773</cell></row><row><cell></cell><cell>French</cell><cell>0.2762</cell><cell>1122</cell><cell>0.4960</cell><cell>0.3600</cell></row><row><cell></cell><cell>German</cell><cell>0.2045</cell><cell>1001</cell><cell>0.3600</cell><cell>0.2760</cell></row><row><cell></cell><cell>Spanish</cell><cell>0.2320</cell><cell>1160</cell><cell>0.3600</cell><cell>0.2680</cell></row><row><cell>Dcualltit40f</cell><cell>Baseline</cell><cell>0.2421</cell><cell>736</cell><cell>0.4120</cell><cell>0.3107</cell></row><row><cell></cell><cell>English</cell><cell>0.2833</cell><cell>1009</cell><cell>0.4320</cell><cell>0.3373</cell></row><row><cell></cell><cell>French</cell><cell>0.2569</cell><cell>1136</cell><cell>0.4240</cell><cell>0.3027</cell></row><row><cell></cell><cell>German</cell><cell>0.2288</cell><cell>962</cell><cell>0.3280</cell><cell>0.2720</cell></row><row><cell></cell><cell>Spanish</cell><cell>0.2468</cell><cell>908</cell><cell>0.3800</cell><cell>0.2973</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,95.73,507.75,429.16,8.74;5,95.76,519.21,204.32,8.74;5,300.12,517.03,5.04,5.65;5,310.86,519.21,213.91,8.74;5,95.76,530.73,366.10,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,265.16,507.75,259.73,8.74;5,95.76,519.21,83.60,8.74">Applying Summarization Techniques for Term Selection in Relevance Feedback</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Lam-Adesina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,203.64,519.21,96.44,8.74;5,300.12,517.03,5.04,5.65;5,310.86,519.21,213.91,8.74;5,95.76,530.73,208.30,8.74">Proceedings of the 24 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,95.74,542.25,429.21,8.74;5,95.76,553.71,429.11,8.74;5,95.76,565.23,22.58,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,417.52,542.25,68.64,8.74">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,164.29,553.71,260.51,8.74">Proceedings of the Third Text REtrieval Conference (TREC-3)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Third Text REtrieval Conference (TREC-3)</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,95.74,576.69,312.59,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,150.86,576.69,137.68,8.74">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,296.69,576.69,32.23,8.74">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="10" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,412.67,576.69,112.12,8.74;5,95.76,588.21,391.85,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,462.79,576.69,62.01,8.74;5,95.76,588.21,125.87,8.74">The Automatic Creation of Literature Abstracts</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">P</forename><surname>Luhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,228.15,588.21,172.01,8.74">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,95.81,599.73,429.02,8.74;5,95.76,611.19,429.16,8.74;5,95.76,622.71,350.21,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,396.45,599.73,128.38,8.74">Dublin City University at CLEF</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Judge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khasin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lam-Adesina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,121.59,611.19,403.33,8.74;5,95.76,622.71,272.79,8.74">Experiments in Monolingual, Bilingual and Multilingual Retrieval, Proceedings of the CLEF 2004: Workshop on Cross-Language Information Retrieval and Evaluation</title>
		<meeting><address><addrLine>Bath, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,95.73,634.23,429.20,8.74;5,95.76,645.69,93.35,8.74;5,189.12,643.51,4.32,5.65;5,198.18,645.69,326.72,8.74;5,95.76,657.21,239.59,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,226.43,634.23,283.31,8.74">The Advantages of Query-Biased Summaries in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tombros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,95.76,645.69,93.35,8.74;5,189.12,643.51,4.32,5.65;5,198.18,645.69,326.72,8.74;5,95.76,657.21,85.50,8.74">proceedings of the 21 st Annual International ACM SIGIR Conference Research and Development in Information Retrieval</title>
		<meeting>the 21 st Annual International ACM SIGIR Conference Research and Development in Information Retrieval<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="2" to="10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
