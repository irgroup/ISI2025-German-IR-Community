<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,100.51,148.86,401.99,15.15;1,128.37,170.78,346.26,15.15;1,240.88,192.69,121.23,15.15">UNED at CL-SR CLEF 2005: Mixing Different Strategies to Retrieve Automatic Speech Transcriptions</title>
				<funder>
					<orgName type="full">Universidad Nacional de Educación a Distancia)</orgName>
				</funder>
				<funder>
					<orgName type="full">UNED</orgName>
				</funder>
				<funder ref="#_trteUYT">
					<orgName type="full">Spanish Government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,166.73,226.59,109.96,8.74"><forename type="first">Fernando</forename><surname>López-Ostenero</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ETSI Informática</orgName>
								<orgName type="laboratory">NLP Group</orgName>
								<orgName type="institution">UNED c</orgName>
								<address>
									<addrLine>Juan del Rosal, 16</addrLine>
									<postCode>E-28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,284.73,226.59,65.48,8.74"><forename type="first">Víctor</forename><surname>Peinado</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ETSI Informática</orgName>
								<orgName type="laboratory">NLP Group</orgName>
								<orgName type="institution">UNED c</orgName>
								<address>
									<addrLine>Juan del Rosal, 16</addrLine>
									<postCode>E-28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,372.90,226.59,63.37,8.74"><forename type="first">Valentín</forename><surname>Sama</surname></persName>
							<email>vsama@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">ETSI Informática</orgName>
								<orgName type="laboratory">NLP Group</orgName>
								<orgName type="institution">UNED c</orgName>
								<address>
									<addrLine>Juan del Rosal, 16</addrLine>
									<postCode>E-28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,100.51,148.86,401.99,15.15;1,128.37,170.78,346.26,15.15;1,240.88,192.69,121.23,15.15">UNED at CL-SR CLEF 2005: Mixing Different Strategies to Retrieve Automatic Speech Transcriptions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">352A0D3FF9844687035AE6380880786B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval speech recognition</term>
					<term>pseudo-relevance feedback speech recognition</term>
					<term>named entities recognition</term>
					<term>cross-language information retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe the UNED's participation in the CLEF CL-SR 2005 track. We have tested several strategies to clean the automatic transcriptions and we have performed 84 different runs mixing these strategies with a proper noun recognition and different pseudo-relevant feedback approaches, in order to study the influence of each method in the retrieval process both in monolingual and cross-lingual environments.</p><p>We noticed that the influence of proper noun recognition is higher on the crosslingual environment, where MAP scores double when we use our entity recognizer. The best pseudo-relevance feedback approach was the one using the MANUALKEY-WORDS field. The effects of the different cleaning strategies were very similar, except for character trigrams, which obtained poor scores compared with the full word approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the CLEF CL-SR 2005 track is to develop and evaluate systems for ranked retrieval of spontaneous conversational speech, over a collection of 8104 segments of interviews with different people.</p><p>Our participation in the track is focused on testing and mixing different techniques to improve the retrieval effectiveness: strategies to clean the documents, proper-noun recognition and different pseudo-relevance feedback approaches.</p><p>The effects of the cleaning strategies are very similar for all methods using full words (morpho, pos and clean). The manual keywords show to be the best field to use in a pseudo-relevance feedback approach. When using our entity recognizer, we also improve the MAP scores in both monolingual and cross-lingual environments. However, we also noticed that the influence of proper noun detection is bigger on the cross-lingual environment, because we use the entity recognizer to detect nouns that should not be translated.</p><p>The remaining sections of this paper are divided as follows: in Section 2 we describe out testbed, the design of our submitted runs, and the new strategies used in our additional experiments. In Section 3 we present the results of our 84 runs and we analyze the influence of: proper noun recognition in monolingual and cross-lingual environments (3.1), cleaning strategies (3.2) and the different pseudo-relevance feedback methods used <ref type="bibr" coords="2,314.24,207.66,19.38,8.74">(3.3)</ref>. Finally, in Section 4 we present some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiment design</head><p>Following the CL-SR CLEF 2005 guidelines we submitted five different runs, and we perform several other experiments after receive the results and the relevance assessments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Testbed</head><p>The test collection consists of 8104 segments from interviews of Holocaust survivors. Each document has several fields with different pieces of information about the segment. In our experiments we have used the following information:</p><p>• We compared the two different automatic transcriptions (ASR2003A and ASR2004A) of the segment, and we find that there are no big differences between both transcriptions. So, we decide to use only the transcription field ASR2004A.</p><p>• The human written summary of the segment (SUMMARY) to detect Proper Nouns which not need to be translated.</p><p>• Three different sets of keywords: one set of manually selected keywords (MANUALKEY-WORD) and two different automatic sets of keywords (AUTOKEYWORD2004A1 and AUTOKEYWORD2004A2).</p><p>We have tried different strategies to clean the automatic transcriptions of the documents:</p><p>1. We noticed that several words that are not recognized by the automatic transcription, in particular proper nouns, appear split in their characters, like "l i e b b a c h a r d ". We have searched all the occurrences of a list of single characters into the documents, and we have joined them assuming that these characters form a whole unrecognized word.</p><p>2. When one speaks, it is usual to repeat some words to emphasize a specific part of the conversation or to show to the other person a thinking process ("let me think, yes, yes, yes..."). Automatic Speech Recognizers transcribe these duplicated words and when performing a retrieval process, the results can be affected by these words. So we have removed all extra occurrences of the duplicated words.</p><p>The resulting documents after these two first steps have been indexed in a collection that we will refer as clean.</p><p>3. In Information Retrieval, the most informative words seems to be nouns, adjectives and verbs. Our next step was to clean the documents, removing all words except nouns, adjectives and verbs.</p><p>We used two different approaches to perform this cleaning:</p><p>• We used the FreeLing <ref type="bibr" coords="3,235.84,112.02,10.52,8.74" target="#b5">[6]</ref> set of linguistic tools to perform a morphological analysis of the documents from the clean collection. The output of this analysis indicates the possible Part of Speech for a given word. If, according to this analysis, a word can act as a noun, an adjective or a verb, the word remains in the document and was removed otherwise. This technique was used successfully to extract noun-phrases from a Spanish collection <ref type="bibr" coords="3,229.81,171.80,10.51,8.74" target="#b2">[3]</ref> and it proved to be very useful in an Information Retrieval environment. We built a new collection called morpho with the documents obtained after this process.</p><p>• We perform a full Part of Speech tagging using freeling again. This process is more complex than the previous one, and includes a PoS Disambiguation phase in order to select only one of the possible Part of Speech for a given word. Like in the previous process, only words that act as nouns, adjectives or verbs remain in the resulting documents, which have been indexed in a collection that we will refer as pos.</p><p>4. Additionally, we split the cleaned documents in character 3-grams to compare the performance of this simple approach with the performance of more complex cleaning process. This collection will be referred as 3grams.</p><p>We have used the English and the Spanish topics, provided by the organizers, from those we have only used TITLE and DESCRIPTION fields in all our runs. For each topic we have removed the usual stopwords and, in the 3-grams runs, we have split each word into character 3-grams individually.</p><p>For our cross-lingual runs, we used a query translation approach following Pirkola's proposal <ref type="bibr" coords="3,499.72,373.86,9.96,8.74" target="#b3">[4]</ref>, where alternative translations for a term were taken as synonyms, giving them equal weights.</p><p>Finally we have used INQUERY <ref type="bibr" coords="3,249.60,397.77,10.52,8.74" target="#b1">[2]</ref> as a search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Submitted runs</head><p>Following the CL-SR CLEF guidelines, we submit five different runs:</p><p>1. A monolingual run using the 3grams collection and the English topics expressed as 3-grams (mono-3grams).</p><p>2. A monolingual run using the morpho collection (mono-morpho).</p><p>3. A cross-lingual run using the morpho collection and the Spanish topics translated into English (trans-morpho).</p><p>4. A monolingual run using the pos collection (mono-pos).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>A cross-lingual run using the pos collection and the Spanish topics translated into English (trans-pos).</p><p>The results of the submitted runs are shown in Table <ref type="table" coords="3,335.28,601.24,3.88,8.74" target="#tab_0">1</ref>, where we can compare our results with the best monolingual and Spanish cross-lingual runs.</p><p>Regarding these numbers we can draw some preliminary conclusions:</p><p>• Our runs are far from the best monolingual and Spanish cross-lingual runs, so there is room for improvement.</p><p>• MAP scores of morpho and pos runs are very similar in both monolingual and crosslingual environments. As expected pos scores slightly better than morpho, but with only two different runs we don't have enough data to conclude that the PoS Disambiguation helps to clean these documents.</p><p>• Character 3-grams run scores worse than full word retrieval (75.6%). Again we have too few data to conclude that it's better to use full word retrieval than a 3-grams approach. • Just using a bilingual dictionary and a Pirkola's approach, our cross-lingual runs reach 40% MAP of their respective monolingual counterparts. In some cases (see section 3.1) a bad translation of some proper nouns difficult the cross-lingual search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking MAP</head><note type="other">Run</note><p>With only five different runs it's very difficult to obtain clear conclusions. According the suggestion of CL-SR CLEF organizers, we have run more experiments after the official submission. These experiments are described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Additional experiments</head><p>With these additional experiments, our intention was to test the effects of two new strategies (proper noun identification and pseudo-relevance feedback) and compare all possible combination of all our different approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proper noun identification</head><p>We have used our entity recognizer <ref type="bibr" coords="4,243.76,432.33,10.52,8.74" target="#b4">[5]</ref> in order to improve the query structure identifying possible proper nouns in the topics. These entities have been used in different ways in our monolingual and cross-lingual experiments:</p><p>• Monolingual: we just identify the proper nouns contained in the topics and, using the #phrase operator of INQUERY, we have structured the query.</p><p>• Cross-lingual: if we use the same strategy as above, the identified proper nouns in Spanish topics maybe will not appear in the English documents. For instance, in the topic #1133 ("The story of Varian Fry"), we identify these proper nouns:</p><p>-Varian Fry -Comité de Rescates de Emergencia -Marsella</p><p>We used the recognizer in order to identify possible proper nouns in the SUMMARY field of the documents too. Only proper nouns that appear in both lists have been used to structure the query.</p><p>In the given example, the only proper noun that appear in the SUMMARY field of the documents was "Varian Fry". So our strategy was to translate the topic, except these names.</p><p>Once we have the proper nouns identified, we use the INQUERY's operator #phrase with each one to structure the query. Below we can see the topic #2012 unstructured and structured using the proper noun identified on it.</p><p>• Topic #2012 unstructured: #sum( collaboration local population information collaboration local population german authorities east central europe holocaust );</p><p>• Topic #2012 structured: #sum( #phrase(German Authorities) #phrase(East Central Europe) #phrase(Holocaust) collaboration local population information collaboration local population );</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relevance Feedback</head><p>We decide to test a Pseudo-Relevance Feedback <ref type="bibr" coords="5,310.54,179.94,10.52,8.74" target="#b0">[1]</ref> (prf) approach to check the utility of the keyword fields of the documents. We built five different relevance feedback methods:</p><p>• A collection (AK1) using the AUTOKEYWORD2004A1 field.</p><p>• A collection (AK2) using the AUTOKEYWORD2004A2 field.</p><p>• A collection (AK12) mixing the keywords present in both autokeyword fields.</p><p>• A collection (MK) using the MANUALKEYWORD field.</p><p>• A collection (MKAK12) mixing the keywords from the three keyword fields.</p><p>In order to mix the keywords from different fields, we used the following process:</p><p>1. Each automatic keyword was scored according to his order of appearance in the field: the first keyword obtains a score of 20 (there are 20 keywords in each autokeyword field), and the last one just 1.</p><p>2. If a keyword appears in both autokeyword fields, his final score was the sum of the scores obtained in each field.</p><p>3. In order to create the AK12 collection, we selected the 20 keywords with higher score.</p><p>4. When building the MKAK12 collection, we first selected the n manual keywords, and then, we added the 20 -n first keywords from AK12 collection.</p><p>When searching, we select the top 10 ranked documents and we combine their keywords using the same method as described above to mix the keywords from different fields. Finally we refine the query adding the top 20 keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full set of runs</head><p>Our intention was to test all possible combinations of each feature: topic language, proper noun identification, cleaning method and relevance feedback. However, we notice that the meaning of cross-lingual runs using 3-grams might be confusing. The translated query is structured with the #syn operator of INQUERY. If we split the translated query into 3-grams, we can't use the #syn operator to structure it, because all 3-grams would be considered as synonyms even if they came from the same word. So we have not used 3-grams on our cross-lingual runs.</p><formula xml:id="formula_0" coords="5,133.75,603.70,338.05,93.99">mono trans × noent ent ×     3grams clean morpho pos     ×         NO AK1 AK2 AK12 MK MKAK12         language</formula><p>proper noun cleaning method relevance feedback</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Combination of all features</head><p>Each run is named with the labels of the different features used on it. For instance "mononoent-morpho-AK2 " represents a monolingual run without proper noun identification, over the morpho collection and using the AUTOKEYWORD2004A2 field for the Pseudo-Relevance Feedback process.</p><p>On figure <ref type="figure" coords="6,149.76,135.93,4.98,8.74">1</ref> we can see all possible values for each feature: 96 different combinations. But, if we exclude the cross-lingual runs when using 3-grams, there are 84 different runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and discussions</head><p>The results of all our runs are shown in Table <ref type="table" coords="6,293.19,201.90,3.88,8.74" target="#tab_2">2</ref>.</p><p>We can draw some preliminary points:</p><p>• Our best run, mono-ent-morpho-MK, scores a 25.95% MAP, a 82.9% of the best submitted monolingual run, from the University of Maryland. We have obtained a 277.8% of improvement respect our best submitted monolingual run.</p><p>• Our best cross-lingual run trans-ent-pos-MK scores about a 131.2% MAP respect the best submitted Spanish run from the University of Ottowa. In this case, the improvement respect our best submitted cross-lingual run is 545.8%.</p><p>• The best strategy seems to be pseudo-relevance feedback using the MANUALKEYWORDS field, followed by the combination of the three keywords fields.</p><p>• The monolingual 3-grams runs score poorly, reaching only a 30% MAP of our best run.</p><p>Let analyze more carefully the influence of the different approaches:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language and Proper noun effects</head><p>On table <ref type="table" coords="6,133.62,408.80,4.98,8.74" target="#tab_3">3</ref> we can compare the effects of the proper noun detection in both monolingual and cross-lingual runs. The numbers on the ent and noent columns show the percentage of the MAP of the cross-lingual runs compared with the MAP of the monolingual. The numbers on the mono and trans represent the increment of the MAP when using proper noun detection technique.</p><p>Regarding these numbers we can infer some interesting points:</p><p>• Using proper nouns, the MAP of cross-lingual runs reach 75% of the monolingual runs. Without proper nouns, cross-lingual runs reach only 35 -40% MAP of monolingual runs.</p><p>• Influence of proper nouns is higher on cross-lingual runs, increasing MAP more than twice respect to noent runs. On monolingual runs the increment is worthless and, probably, statistically not relevant.</p><p>• The influence of proper nouns is also worthless on 3-grams runs, even the best 3-grams run is mono-noent-3grams-MK that not uses proper noun detection.</p><p>For instance, on topic #1113 ("The story of Varian Fry"), the influence of proper noun detection is very important, because in Spanish the word "Varian" can be identified as a verbal form of "Variar" (to vary, change) and is wrongly translated:   </p><formula xml:id="formula_1" coords="6,104.94,617.35,73.42,8.77">• trans-noent-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cleaning effects</head><p>Regarding the influence of the cleaning method we can conclude that the best cleaning strategy seems to be morpho, but the differences respect pos and clean are minimal and, probably statistically not relevant.</p><p>Using just a morphological analyzer to identify possible part of speech for a given word, proves to be a very useful strategy in information retrieval if we don't have a full part of speech tagger.</p><p>Again, character 3-grams show to be a bad cleaning strategy when compared with full words approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relevance feedback</head><p>On table <ref type="table" coords="8,129.52,515.70,4.98,8.74" target="#tab_5">4</ref> we can compare the differences between the different pseudo-relevant feedback strategies tested. Each column represent the MAP percentage of one prf method respect to another. For instance MKAK12/MK column represent the percentage of MKAK12 prf respect MK prf.  • The best prf method is MK (average increment of MAP using the prf over manual keywords field respect no relevance feedback is about 271.6%), nearly followed by MKAK12 (an average MAP of 90.5% respect MK).</p><p>• There are no big differences between the use of each automatic keyword field, but prf using AUTOKEYWORD2004A1 field seems to obtain high MAP score. And, when combining both fields (AK12), MAP scores 41.51% of MKAK12 on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and future work</head><p>In this paper, we have shown different techniques to improve retrieval of automatic speech transcriptions in both monolingual and cross-lingual environments.</p><p>• We have tested four different cleaning techniques. Differences between full word techniques (clean, morpho and pos) are worthless, but a character 3-grams approach seems to be worse.</p><p>• Pseudo-relevance feedback using manually generated keywords shows to be the best option to increment the performance of the retrieval, with an average percentage of 271.6% respect no relevance feedback.</p><p>• The use of an entity recognizer to identify proper nouns, proves to be very useful, specially on a cross-lingual environment, where the MAP scores twice when using them.</p><p>Our intention is to perform further analysis over the results, including statistical relevance tests to determine the influence of the different methods we have tried.</p><p>We also want to test a different approach to identify proper nouns in the automatic transcriptions, or in the automatic keyword fields, instead of using the manual summary of the documents. Maybe the big improvement detecting proper nouns in the cross-lingual environment is due to the use of a manually generated field, similarly to the best scores obtained when using MANU-ALKEYWORDS field in pseudo-relevance feedback.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,127.42,110.79,364.19,117.11"><head>Table 1 :</head><label>1</label><figDesc>Comparison of results of submitted runs</figDesc><table coords="4,418.52,110.79,47.46,8.77"><row><cell>Language</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,178.12,678.45,246.77,8.74"><head>Table 2 :</head><label>2</label><figDesc>Results of all runs (submitted runs in boldface)</figDesc><table coords="8,183.89,110.81,235.22,223.65"><row><cell></cell><cell cols="2">trans/mono</cell><cell cols="2">ent/noent</cell></row><row><cell>Experiment</cell><cell>ent</cell><cell>noent</cell><cell>mono</cell><cell>trans</cell></row><row><cell>clean-NO</cell><cell cols="4">77.4% 40.1% 101.83% 196.50%</cell></row><row><cell>clean-AK1</cell><cell cols="4">88.3% 40.7% 101.30% 219.88%</cell></row><row><cell>clean-AK2</cell><cell cols="2">86.6% 34.1%</cell><cell>99.75%</cell><cell>253.54%</cell></row><row><cell>clean-AK12</cell><cell cols="2">84.2% 34.2%</cell><cell>98.32%</cell><cell>242.02%</cell></row><row><cell>clean-MK</cell><cell cols="4">78.2% 39.6% 103.85% 204.91%</cell></row><row><cell>clean-MKAK12</cell><cell cols="4">80.6% 38.5% 103.37% 216.21%</cell></row><row><cell>pos-NO</cell><cell cols="4">76.9% 39.9% 101.71% 195.97%</cell></row><row><cell>pos-AK1</cell><cell cols="4">88.8% 40.2% 100.69% 222.25%</cell></row><row><cell>pos-AK2</cell><cell cols="4">83.0% 33.1% 100.34% 251.73%</cell></row><row><cell>pos-AK12</cell><cell cols="2">84.5% 34.5%</cell><cell>99.69%</cell><cell>244.02%</cell></row><row><cell>pos-MK</cell><cell cols="4">78.8% 40.7% 103.40% 200.39%</cell></row><row><cell>pos-MKAK12</cell><cell cols="4">80.6% 37.9% 104.90% 223.23%</cell></row><row><cell>morpho-NO</cell><cell cols="4">78.6% 40.3% 101.85% 198.64%</cell></row><row><cell>morpho-AK1</cell><cell cols="4">89.1% 40.9% 101.65% 221.38%</cell></row><row><cell>morpho-AK2</cell><cell cols="4">82.6% 34.0% 101.75% 247.24%</cell></row><row><cell>morpho-AK12</cell><cell cols="2">85.3% 34.1%</cell><cell>99.79%</cell><cell>249.84%</cell></row><row><cell>morpho-MK</cell><cell cols="4">76.4% 41.0% 103.84% 193.36%</cell></row><row><cell cols="5">morpho-MKAK12 79.9% 39.2% 104.81% 213.87%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,95.56,347.48,411.87,8.74"><head>Table 3 :</head><label>3</label><figDesc>Influence of proper noun detection, comparison of monolingual and cross-lingual runs</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,194.64,719.98,213.73,8.74"><head>Table 4 :</head><label>4</label><figDesc>Influence of relevance feedback methods</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been partially supported by the <rs type="funder">Spanish Government</rs> under project <rs type="projectName">R2D2-Syembra</rs> (<rs type="grantNumber">TIC2003-07158-C04-02</rs>). <rs type="person">Víctor Peinado</rs> holds a PhD grant by <rs type="funder">UNED</rs> (<rs type="funder">Universidad Nacional de Educación a Distancia)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_trteUYT">
					<idno type="grant-number">TIC2003-07158-C04-02</idno>
					<orgName type="project" subtype="full">R2D2-Syembra</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,105.50,583.17,407.51,8.74;9,105.50,595.12,407.50,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,358.18,583.17,154.83,8.74;9,105.50,595.12,76.31,8.74">Automatic Query Expansion Using SMART: TREC 3</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,200.80,595.12,249.69,8.74">Proceedings of the 3rd Text Retrieval Conference (TREC3)</title>
		<meeting>the 3rd Text Retrieval Conference (TREC3)</meeting>
		<imprint>
			<biblScope unit="page" from="69" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,627.01,407.50,8.74;9,105.50,638.96,407.50,8.74;9,105.50,650.92,127.39,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,313.16,627.01,128.72,8.74">The Inquery Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Harding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,463.34,627.01,49.66,8.74;9,105.50,638.96,374.77,8.74">Proceedings of the Third International Conference on Database and Expert Systems Applications</title>
		<meeting>the Third International Conference on Database and Expert Systems Applications</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,670.84,407.50,8.74;9,105.50,682.80,407.50,8.74;9,105.50,694.75,296.34,8.74" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<title level="m" coord="9,157.48,670.84,355.52,8.74;9,105.50,682.80,156.66,8.74">Website Term Browser: Un sistema interactivo y multilingüe de búsqueda textual basado en técnicas lingüísticas</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Departamento de Lenguajes y Sistemas Informáticos, Universidad Nacional de Educación a Distancia</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="9,105.50,714.68,407.50,8.74;9,105.50,726.63,407.51,8.74;9,105.50,738.59,361.60,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,157.02,714.68,355.98,8.74;9,105.50,726.63,138.58,8.74">The Effects of Query Structure and Dictionary Setups in Dictionary-Based Cross-Language Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pirkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,267.20,726.63,245.81,8.74;9,105.50,738.59,273.26,8.74">Proceedings of SIGIR&apos;98, 21st ACM International Conference on Research and Development in Information Retrieval</title>
		<meeting>SIGIR&apos;98, 21st ACM International Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,112.02,407.51,8.74;10,105.50,123.98,407.50,8.74;10,105.50,135.93,227.22,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,325.90,112.02,187.10,8.74;10,105.50,123.98,236.20,8.74">UNED at ImageCLEF 2005: Automatically Structured Queries with Named Entities over Metadata</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Peinado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>López-Ostenero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,361.59,123.98,151.41,8.74;10,105.50,135.93,196.10,8.74">Cross Language Evaluation Forum, Working Notes for the CLEF 2005 Workshop</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,155.86,407.51,8.74;10,105.50,167.81,168.80,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,354.68,155.86,158.32,8.74;10,105.50,167.81,85.90,8.74">FreeLing: An Open-Source Suite of Language Analyzers</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Padró</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Padró</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
