<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.80,151.73,297.02,12.58;1,184.68,169.19,225.89,12.58">Experiences at ImageCLEF 2010 using CBIR and TBIR mixing information approaches</title>
				<funder ref="#_ZG6NxN4">
					<orgName type="full">Spanish government</orgName>
				</funder>
				<funder ref="#_JP523fU #_u9J5YYc">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.64,207.81,46.72,8.74"><forename type="first">J</forename><surname>Benavent</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universidad de Valencia</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,207.55,207.81,50.07,8.74"><forename type="first">X</forename><surname>Benavent</surname></persName>
							<email>xaro.benavent@uv.es</email>
						</author>
						<author>
							<persName coords="1,265.80,207.81,38.65,8.74"><forename type="first">E</forename><surname>De Ves</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universidad de Valencia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.72,207.81,49.97,8.74"><forename type="first">R</forename><surname>Granados</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidad Nacional de Educación a Distancia</orgName>
								<orgName type="institution" key="instit2">UNED</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universidad de Valencia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.62,207.81,80.34,8.74"><forename type="first">Ana</forename><surname>García-Serrano</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidad Nacional de Educación a Distancia</orgName>
								<orgName type="institution" key="instit2">UNED</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.80,151.73,297.02,12.58;1,184.68,169.19,225.89,12.58">Experiences at ImageCLEF 2010 using CBIR and TBIR mixing information approaches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">16BA0E663DEA51C68A0B4249DEB863CF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Retrieval, Textual-based Retrieval, Content-Based Image Retrieval, Relevance feedback, Merge Results Lists, Fusion, Indexing. Categories and subject descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.2 Information Storage</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital libraries. H.2 [Database Management]: H.2.5 Heterogeneous Databases</term>
					<term>E.2 [Data Storage Representations]</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The main goal of this paper it is to present our experiments in ImageCLEF 2010 Campaign (Wikipedia retrieval task). This edition we present a different way of using textual and visual information based on the assumption that the textual module better captures the meaning of a topic. So that, the TBIR module works firstly and acts as a filter, and the CBIR system reorder the textual result list. The CBIR system presents three different algorithms: the automatic, the query expansion and a logistic regression relevance feedback algorithm. We have submitted nine textual and eleven mixed runs. Our best run, at the 34 th position (25% at the first result list), is a textual run using our own implemented algorithm based on a VSM approach and TF-IDF weights (included in the IDRA tool) and all languages for annotation and for the topics. Our best mixed run (51th position is at 60% first result list) is using the textual list and the logistic regression relevance algorithm at the CBIR module. Most of our runs are above the average of its own modality for the different measures. The new system architecture with the IDRA tool for the textual module and the logistic regression relevance algorithm for the visual module are the right track to maintain in our research lines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The UNED-UV is a research group formed by researchers from two different universities in Spain, the Universidad Nacional de Educación a Distancia (UNED) and the Valencia University (UV). This research group is working together <ref type="bibr" coords="1,443.33,651.75,11.74,8.74" target="#b0">[1]</ref>  <ref type="bibr" coords="1,458.94,651.75,11.74,8.74" target="#b1">[2]</ref> since ImageCLEF08 edition.</p><p>The main goal of this paper it is to present our experiments in ImageCLEF 2010 Campaign (Wikipedia retrieval task) <ref type="bibr" coords="2,274.97,161.49,10.64,8.74" target="#b2">[3]</ref>. This ImageCLEF edition our group presents a different way of working using the information of the Content Based Image Retrieval (CBIR) system and the information of the Textual Based Image Retrieval (TBIR) system. The global system is based on the assumption that the conceptual meaning of a topic is initially better captured by the text module itself than by the visual module. Therefore, the TBIR system works firstly over the whole database working as a filter, and then the CBIR system reorders the filtered textual result list. In this way, the CBIR system acts also as a merging module.</p><p>The TBIR subsystem includes the UNED own implemented tool IDRA (InDexing and Retrieving Automatically) <ref type="bibr" coords="2,253.05,264.99,11.69,8.74" target="#b3">[4]</ref> that includes several functionalities, including an algorithm based on the Vector Space Model (VSM) approach using TF-IDF weighted vectors. The CBIR subsystem includes three different algorithms: the automatic, the query expansion and UV own logistic regression relevance feedback algorithm <ref type="bibr" coords="2,442.99,299.49,10.64,8.74" target="#b4">[5]</ref>.</p><p>A more detailed presentation of the system, the submitted experiments, and the obtained results is included in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>The global system (shown at Fig. <ref type="figure" coords="2,266.49,387.81,4.17,8.74" target="#fig_0">1</ref>) includes three main subsystems: the TBIR, the CBIR and the merging module. The TBIR subsystem uses the UNED own implemented tool IDRA <ref type="bibr" coords="2,226.52,410.79,10.65,8.74" target="#b4">[5]</ref>, in charge of indexing and retrieving textual annotations from images. The Valencia University CBIR system implements for this ImageCLEF edition three different algorithms: the automatic, the query expansion and the relevance feedback algorithm based on logistic regression <ref type="bibr" coords="2,365.87,445.29,10.63,8.74" target="#b4">[5]</ref>. The TBIR subsystem acts firstly over the whole images of the database, acting as a filter to the CBIR system selecting the relevant images for a certain query. In a second step, the CBIR system works over the set of filtered images reordering this list taking into account the visual information of the image. The CBIR system generates different visual result lists depending on the number of query images (for the automatic and the query expansion algorithm). These lists are merged by the merging module by an OWA operator <ref type="bibr" coords="2,160.57,525.81,10.63,8.74" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text-based Index and Retrieval</head><p>This module is in charge of the textual image retrieval using the metadata supplied for the images in the collection. IDRA tool <ref type="bibr" coords="2,286.39,593.25,11.72,8.74" target="#b3">[4]</ref> extracts, selects, preprocesses and indexes the metadata information, for later search and retrieve the most relevant images for the queries. After this process, a ranked results list is obtained for each textual experiment.</p><p>The textual retrieval task architecture can be seen in the Fig. <ref type="figure" coords="2,393.17,639.27,3.76,8.74" target="#fig_0">1</ref>. Each one of the components takes care of a specific task. These tasks will be sequentially executed: Preprocess. This component process the text in two ways: 1) special characters deletion: characters with no statistical meaning, like punctuation marks, are eliminated; and 2) stopwords detection: exclusion of semantic empty words from specifics lists for each language. When processing multilingual text, a manually join of these lists is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metadata Selection.</head><p>With this component the system selects the text we want to index, depending on the chosen language "Index Lang" (EN, FR, DE or ALL). Therefore, 4 different indexations will be generated: one multilingual and three monolingual.</p><p>In the case of the monolingual indexations the selected text for the chosen language L= {EN, FR, DE}, from images metadata files, will be: 1) &lt;name&gt; whenever there is specific metadata for language L, or when there is not for any language; 2) &lt;description&gt; and &lt;caption&gt; whenever there is specific metadata for L; 3) &lt;comment&gt; when the text in this tag is not contained in &lt;description&gt; or &lt;caption&gt; and therefore will add new information. This time we did not use the text from the corresponding Wikipedia articles indicated in the &lt;caption&gt; attribute "article".</p><p>When carrying out the multilingual indexation (ALL), the selected text will be the concatenation of the corresponding text for each of the three languages (EN+FR+DE), in the same way as explained for the monolingual cases. Queries File. 4 different queries files are constructed for the experiments: one for each language (EN, FR, DE), and another for the multilingual case, which is indicated in "Queries Lang". The strategy to select the text for each query is just to extract the information in the &lt;TITLE&gt; tag for the chosen language, and the concatenation of the three languages for the multilingual experiment.</p><p>IDRA Index. This component indexes the selected text associated with each image.</p><p>The indexation is based on the VSM approach using TF-IDF (term frequencyinverse document frequency) weighted vectors. This approach consists in calculating the weights vectors for each one of the images selected texts. Each vector is compounded by the TF-IDF weights values of the different words in the collection. TF-IDF weight is a statistical measure used to evaluate how important a word is to a text in a concrete collection, and is calculated as shown in <ref type="bibr" coords="4,359.21,282.51,10.64,8.74" target="#b0">(1)</ref>.</p><formula xml:id="formula_0" coords="4,129.96,298.39,110.23,27.94">⎟ ⎠ ⎞ ⎜ ⎝ ⎛ = - i j i n N t IDF TF 2 log *</formula><p>, t i,j : number of occurrences of the word t j in caption text T i . N: total number of images captions in the collection. n i : number of captions in which appears the word t i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(1)</head><p>All weights values for each vector will be then normalized using the Euclidean distance. Therefore, for each one of the words appearing in the collection, the IDRA Index process updates and stores the following values: n i , t i,j , N (described in (1)), T i : unique identifier of the image, idf j : inverse document frequency ( log 2 (N/n i ) ) in T i , E i : Euclidean distance used to normalize, and w j,i : weight of word t j in T i .</p><p>IDRA Search. Is in charge of launching the queries against a concrete indexation for the experiment, and it obtains the corresponding "TXT Results List". For each one of the queries, IDRA calculates its corresponding weights vector in the same way as in indexation. Then, the similarity between the query and an image text will depend on the proximity of their associated vectors calculated by the cosine measure:</p><formula xml:id="formula_1" coords="4,186.12,470.18,233.92,31.88">∑ ∑ ∑ = = i q i q i j i j i j i j i w w w w w w q T sim , * , , * , , , * * ) Ö cos( ) , (<label>(2)</label></formula><p>This similarity value will be calculated between the query and all the images metadata indexed. Images are ranked in descending order in the "TXT Results List".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Content-Based Information and Visual Retrieval</head><p>The VISION-Team at the Computer Science Department of the University of Valencia has its own CBIR system, and that has also been used in previous ImageCLEF editions (Photo-retrieval task in 2008 and 2009 <ref type="bibr" coords="4,376.96,601.95,11.69,8.74" target="#b0">[1]</ref> [2]). The low-level features of the CBIR system have been adapted for the images of the new image collection (WikipediaMM 2010) taking into account the results of the previous editions.</p><p>As in most CBIR systems, a feature vector represents each image. The first step at the Visual Retrieval system is extracting these features for all the images on the database as for each of the cluster query topic images for each question. Instead of using the low-level features provided by the organization, we have used our own features. We use different low-level features describing color and texture to build a vector of features. The number of low-level features has been increased from the 114 components at ImageCLEF09 up to 296 components at the current edition. This increment is mainly due to the use of local HS histogram (10x3 bins) instead of local H histograms (10 bins) descriptors in previous editions.</p><p>• Color information: Color information has been extracted calculating both local and global histograms of the images using 10x3 bins on the HS color system. Local histograms have been calculated dividing the images in four fragments of the same size. A bidimensional HS histogram with 10x3 bins is computed for each patch. Therefore, a feature vector of 30 components for the global histogram, and 192 components for the local histograms represent the color information of the image. • Texture information: Two types of texture features are computed: The granulometric distribution function, using the coefficients that result of fitting the distribution function with a B-spline basis. And, the Spatial Size Distribution. We have used two different versions of it by using as the structuring elements for the morphological operation that get size both a horizontal and a vertical segment <ref type="bibr" coords="5,296.06,346.95,10.62,8.74" target="#b0">[1]</ref>.</p><p>At this edition, the vision team has focus his work in testing three different visual algorithms applied to the results retrieved by the text module: the automatic, the relevance feedback and the query expansion. We assume that the conceptual meaning of a question is better captured by the text module than by a visual module when they work individually. Therefore, the task of the visual module is to re-order the textual result list taking into account the information of the query images given at each topic.</p><p>Automatic algorithm. This is the typical algorithm in a CBIR system. The first step is to calculate the feature vector that describes each image of the database as it has been explained at the previous paragraph. The second step is to calculate the similarity measurement between the feature vectors of each image on the database and the N query images. The distance metric applied in our experiments is the Mahalanobis distance that gives better results than the Euclidean one ( <ref type="bibr" coords="5,432.65,502.41,10.42,8.74" target="#b0">[1]</ref>). The Mahalanobis distance gives better results than the Euclidean due to the fact that this measure takes into account the correlations of the data set and is scale-invariant being this characteristic very useful because the broad differences between the different low-level feature values. The Mahalanobis distance needs to pre-calculate the covariance matrix of the sample data. Since, the size of the database is too huge we have chosen a different approach: a covariance matrix is computed for each textual result list given for each topic. Thus, we have managed to cope with the problem of computing the metric for the Mahalanobis distance in a large database.</p><p>As we have N query images, we will obtain N visual result lists, one for each query image in the topic. These N result lists are passed to the merging module to fuse them in one result list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query expansion algorithm.</head><p>The query expansion algorithm works in the same way that the automatic algorithm, being the only difference that this algorithm expands the N query images to a wider set of images M. Thus, the M query images set is composed of the N images given by the topic and the N' expanded images being M=N+N'. The N' images set are the 3 first images of the textual result list. The M result lists are passed to the merging module.</p><p>Relevance feedback algorithm based on logistic regression. This algorithm works differently to the two previous ones. Therefore, we will explain the concept of relevance feedback and the adjustments made to get a good performance of the algorithm for the proposed tasks <ref type="bibr" coords="6,261.72,225.03,10.61,8.74" target="#b4">[5]</ref>. Relevance feedback is a term used to describe the actions performed by a user to interactively improve the results of a query by reformulating it. An initial query formulated by a user may not fully capture his/her wishes. Users then typically change the query manually and re-execute the search until they are satisfied. By using relevance feedback, the system learns a new query that better captures the user's need for information. The user enters his/her preferences at each iteration through the selection of relevant and non-relevant images.</p><p>We will explain the way the logistic regression relevance feedback algorithm works. Let us consider the (random) variable Y giving the user evaluation where Y=1 means that the image is positively evaluated and Y=0 means a negative evaluation. Each image in the database has been previously described by using low-level features in such a way that the j-th image has the k-dimensional feature vector xj associated. Our data will consist of (xj, yj), with j=1,…,n, where n is the total number of images, xj is the feature vector and yj the user evaluation (1=positive and 0=negative). The image feature vector x is known for any image and we intend to predict the associated value of Y. In this work, we have used a logistic regression where P(Y=1|x) i.e. the probability that Y=1 (the user evaluates the image positively) given the feature vector x, is related with the systematic part of the model (a linear combination of the feature vector) by means of the logit function. For a binary response variable Y and p explanatory variables X1,…,Xp, the model for π(x)=P(Y=1|x) at values x=(x1,…,xp) of predictors is logit[π(x)]=α+β1x1+…+βpxp, where logit[π(x)]=ln(π(x)/(1-π(x))). The model parameters are obtained by maximizing the likelihood function given by:</p><formula xml:id="formula_2" coords="6,224.28,495.18,246.27,19.25">∏ - - = n y i y i i i x x l 1 )] ( 1 [ ) ( ) ( π π β<label>(3)</label></formula><p>The maximum likelihood estimators (MLE) of the parameter vector β are calculated by using an iterative method.</p><p>We have a major difficulty when having to adjust a global regression model in which we take the whole set of variables into account, because the number of selected images (the number of positive plus negative images) is typically smaller than the number of characteristics. In this case, the regression model adjusted has as many parameters as the number of data and many relevant variables could be not considered. In order to solve this problem, our proposal is to adjust different smaller regression models: each model considers only a subset of variables consisting of semantically related characteristics of the image. Consequently, each sub-model will associate a different relevance probability to a given image x, and we face the question of how to combine them in order to rank the database according to the user's preferences. This problem has been solved by means of an ordered averaged weighted operator (OWA) <ref type="bibr" coords="6,193.62,679.89,10.61,8.74" target="#b5">[6]</ref>.</p><p>In our case, we have adapted the manual relevance feedback to an automatic performance. The examples and the counter-examples (positive and negative images) are automatically selected for each topic. The examples are the query images of the topic plus N images taken from the first positions of the textual result list. The counter-examples are the M latest positions of the textual result list. The relevance feedback algorithm is executed once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Merging Algorithms</head><p>Two merging algorithms are used in different steps with different purposes.</p><p>OWA Fusion. In the modality for textual and visual retrieval the approach that follows this edition is based on the assumption that the conceptual meaning of a topic is initially better captured by the text module itself than by the visual module. Thus, the textual module works as a filter for the visual module, and the work of the visual module is to re-order the textual results list. In this way, there has not been used an explicit fusion algorithm to merge the textual result list and the visual result list. However, the visual module generates N result visual lists depending on the number of query images for the automatic and query expansion algorithms. These N lists are merged in one result final list by using the Mathematical aggregation operators OWA <ref type="bibr" coords="7,124.74,384.51,10.64,8.74" target="#b5">[6]</ref>. The OWA transform a finite number of inputs into a single output and play an important role in image retrieval. With the OWA operator no weight is associated with any particular input; instead, the relative magnitude of the input decides which weight corresponds to each input. In our application, the inputs are similarity distances to each of the N query images and this property is very interesting because we do not know, a priori, which image of the N images will provide us with the best information. The aggregation weights used for these experiments are the weights which correspond to the maximum, that is an OR operator.</p><p>MAXmerge. This algorithm is used to fuse together different results lists in order to carry out some experiments related to multilingualism (UNED-UV8, UNED-UV9 described in next section). MAXmerge algorithm is included in IDRA tool and consists on, for each query, to select the results from the different lists which have a higher relevance/similarity value for the corresponding query, independently of the list the results appears in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments (submitted runs)</head><p>We have participated in two modalities: textual and mixed retrieval (visual and textual). Finally, 20 runs were submitted (9 textual, 11 mixed). A schematic description of these runs is shown in Table <ref type="table" coords="7,298.61,628.29,3.74,8.74" target="#tab_0">1</ref>.</p><p>For textual modality, we present 9 runs. As it is explained in previous sections, 4 different indexations and 4 queries files were generated. From all possible combinations, we were interested in evaluate experiments with the 4 queries files against the multilingual indexation, obtaining 4 runs: UNED-UV1 (with multilingual queries), UNED-UV2 (with English queries), UNED-UV4 (with French queries), and UNED-UV6 (with Dutch queries). UNED-UV3, UNED-UV5 and UNED-UV7 correspond to monolingual experiments in which the language for the indexation is the same of the queries: English, French or Dutch, respectively. Finally, 2 more textual runs were submitted using the MAXmerge fusion algorithm: UNED-UV8 merging results lists from UNED-UV2, UNED-UV4 and UNED-UV6; and UNED-UV9 merging results from UNED-UV3, UNED-UV5 and UNED-UV7.</p><p>In the mixed modality, the textual module has passed through the visual module four different kinds of its basic textual algorithms corresponding to the UNED-UV1, UNED-UV2, UNED-UV3 and UNED-UV9 runs. The visual module has applied its three different algorithms (automatic, relevance feedback and query expansion) to the textual result lists in order to test the performance of these three algorithms over the different kind of text algorithms retrieval.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>After the evaluation by the task organizers, our results for each of the submitted experiments are presented in Table <ref type="table" coords="9,267.77,189.33,3.77,8.74" target="#tab_1">2</ref>. The table shows that our two best results are for the textual runs UNED-UV1 and UNED-UV9 (at the 34th and 40th position of the global result list, this is at the 25% first results). For the mixed modality, the best result is the UNED-UV11 at the 51th position (at the 60% first results). It is worth pointing out that the ranking position is computed by using the MAP measure (with a maximum MAP value of 0.1927 for our best run and a minimum MAP value of 0.1502 for our worst one). It can also be observed at Table <ref type="table" coords="9,363.81,258.33,5.01,8.74" target="#tab_1">2</ref> that most of our runs are above the average for each own modality (textual and mixed runs). These above results are marked in bold at the table. With textual experiments this campaign we aimed to analyze multilingual issues. Comparing UNED-UV1 results with UNED-UV2, UNED-UV4 and UNED-UV6 ones, we can observe that best retrieval with the multilingual indexation is performed when we use the queries file constructed with the concatenation of all languages (MAP=0.1927). Launching English queries obtains better results (0.1627) that French (0.0920) or Dutch (0.09936), surely due to the metadata information for that language. Analyzing results for UNED-UV8 an UNED-UV9 runs, we observe that both of them obtain a good performance (only UNED-UV1 obtains higher MAP than them). Slightly higher results are for UNED-UV9 (0. 1865 &gt; 0. 1790), so it is early to conclude when merging results from different languages, if it is better to launch the queries against monolingual indexations than against multilingual. At this moment, the effort in preprocessing has to be taken into account to decide.</p><p>The process to analyze our mixed modality results is by the comparison of the basic textual algorithms (UNED-UV1, UNED-UV2, UNED-UV3 and UNED-UV9) and their corresponding mixed runs (UNED-UV10-12 for the UNED-UV1, UNED-UV13-15 for UNED-UV2, and so on). We have improved the precision values at 10 and at 20 with the mixed runs, i.e. the UNED-UV11-Feedback (Prec@10 0.3914 and Prec@20 0.3629) improves its basic textual run UNED-UV1 (Prec@10 0.3914 and Prec@20 0.3564). The same improvement can be observed for the other mixed runs being compared with their corresponding textual runs. This result points out that visual algorithms can improve the textual result lists by back forward to the end of the list non relevant images retrieved by the textual module. However, MAP values are still lower than their corresponding textual runs. This could be due to the fact that more query images would be needed to get better results for higher precision measures (P@30, P@40 and so on), improving in that way the medium of the precision values (MAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks and Future Work</head><p>Our best result is for the textual modality and it is at the position 34th, at the first 25% of the best results of the contest; and, our best result for the mixed modality is at the 51th position, at the first 60% of the global contest. Most of our runs in ImageCLEF10 are above the average for its own modality. These results mean that our main algorithms for textual and visual modules have got good marks, and they can be tuned to improve the current results.</p><p>Regarding multilinguality, the multilingual run it is our best (multilingual query launched to the multilingual index). It defeats the runs using monolingual queries (also on multilingual index). When using monolingual indexes and merging the results lists according to the query, only a slightly difference it is obtained (MAP value 0.1927 &gt; 0. 1865 for UNED_UV9). It is early to conclude about, but the effort in preprocessing has to be taken into account to it.</p><p>The best result for mixed runs has been obtained with the logistic regression relevance feedback algorithm (UNED-UV11 at position 51), followed by the query expansion and the automatic one. Our new algorithms (logistic regression relevance feedback and query expansion) have markedly improved the results in comparison with the automatic algorithm used in previous editions. It is also important to notice that the best results of the contest are also achieved with a feedback algorithm. This reinforces our idea that the feedback algorithms are the right track to maintain in our future research lines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,252.36,385.41,90.53,8.10;3,124.74,408.06,345.93,9.02;3,124.74,419.85,345.84,8.74;3,124.74,431.37,44.53,8.74"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. System overview.Text Extraction. Extracts the text from the files which contains the associated metadata. It uses the JDOM Java API to identify the content of each of the tags of the XML files.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,311.91,287.97,158.72,8.74;8,124.74,299.49,345.99,8.74;8,124.74,311.01,345.91,8.74;8,124.74,322.47,345.78,8.74;8,124.74,333.99,345.82,8.74;8,124.74,345.51,345.85,8.74;8,124.74,356.97,345.89,8.74;8,124.74,368.49,282.16,8.74"><head></head><label></label><figDesc>For the [UNED-UV1] basic line the automatic, relevance feedback and query expansion algorithms have been applied getting the three corresponding runs [UNED-UV10], [UNED-UV11] and [UNED-UV12]. Following the same structure applying the three different visual algorithms to the [UNED-UV2] run the [UNED-UV13], [UNED-UV14] and [UNED-UV15] runs are obtained. From the [UNED-UV3] run the [UNED-UV16], [UNED-UV17] and [UNED-UV18]; and, from the [UNED-UV9] the [UNED-UV19], [UNED-UV20] and [UNED-UV21]. The last one was out of the maximum runs submitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,121.14,391.41,320.88,277.32"><head>Table 1 .</head><label>1</label><figDesc>Submitted textual and mixed experiments.</figDesc><table coords="8,121.14,416.17,320.88,252.56"><row><cell></cell><cell cols="2">CBIR</cell><cell>TBIR</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Annotation</cell><cell>Topic</cell></row><row><cell>Run</cell><cell cols="3">Mod Algor Algorithm</cell><cell>language</cell><cell>language</cell></row><row><cell cols="2">UNED-UV1 Text</cell><cell>-</cell><cell>VSM</cell><cell cols="2">EN+FR+DE EN+FR+DE</cell></row><row><cell cols="2">UNED-UV2 Text</cell><cell>-</cell><cell>VSM</cell><cell cols="2">EN+FR+DE EN</cell></row><row><cell cols="2">UNED-UV3 Text</cell><cell>-</cell><cell>VSM</cell><cell>EN</cell><cell>EN</cell></row><row><cell cols="2">UNED-UV4 Text</cell><cell>-</cell><cell>VSM</cell><cell cols="2">EN+FR+DE FR</cell></row><row><cell cols="2">UNED-UV5 Text</cell><cell>-</cell><cell>VSM</cell><cell>FR</cell><cell>FR</cell></row><row><cell cols="2">UNED-UV6 Text</cell><cell>-</cell><cell>VSM</cell><cell cols="2">EN+FR+DE DE</cell></row><row><cell cols="2">UNED-UV7 Text</cell><cell>-</cell><cell>VSM</cell><cell>DE</cell><cell>DE</cell></row><row><cell cols="2">UNED-UV8 Text</cell><cell>-</cell><cell cols="3">VSM (EN+FR+DE) + MAXmerge EN+FR+DE EN+FR+DE</cell></row><row><cell cols="2">UNED-UV9 Text</cell><cell>-</cell><cell cols="3">VSM (EN|FR|DE) + MAXmerge EN+FR+DE EN+FR+DE</cell></row><row><cell cols="4">UNED-UV10 Mixed AUTO [UNED-UV1]</cell><cell cols="2">EN+FR+DE EN+FR+DE</cell></row><row><cell cols="2">UNED-UV11 Mixed FB</cell><cell></cell><cell>[UNED-UV1]</cell><cell cols="2">EN+FR+DE EN+FR+DE</cell></row><row><cell cols="3">UNED-UV12 Mixed QE</cell><cell>[UNED-UV1]</cell><cell cols="2">EN+FR+DE EN+FR+DE</cell></row><row><cell cols="4">UNED-UV13 Mixed AUTO [UNED-UV2]</cell><cell cols="2">EN+FR+DE EN</cell></row><row><cell cols="2">UNED-UV14 Mixed FB</cell><cell></cell><cell>[UNED-UV2]</cell><cell cols="2">EN+FR+DE EN</cell></row><row><cell cols="3">UNED-UV15 Mixed QE</cell><cell>[UNED-UV2]</cell><cell cols="2">EN+FR+DE EN</cell></row><row><cell cols="4">UNED-UV16 Mixed AUTO [UNED-UV3]</cell><cell>EN</cell><cell>EN</cell></row><row><cell cols="2">UNED-UV17 Mixed FB</cell><cell></cell><cell>[UNED-UV3]</cell><cell>EN</cell><cell>EN</cell></row><row><cell cols="3">UNED-UV18 Mixed QE</cell><cell>[UNED-UV3]</cell><cell>EN</cell><cell>EN</cell></row><row><cell cols="4">UNED-UV19 Mixed AUTO [UNED-UV9]</cell><cell cols="2">EN+FR+DE EN+FR+DE</cell></row><row><cell cols="2">UNED-UV20 Mixed FB</cell><cell></cell><cell>[UNED-UV9]</cell><cell cols="2">EN+FR+DE EN+FR+DE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,124.74,304.17,345.98,309.24"><head>Table 2 .</head><label>2</label><figDesc>Results for the submitted experiments (The results in bold are above the average for the modality).</figDesc><table coords="9,148.32,339.31,298.63,274.09"><row><cell>Po Run</cell><cell cols="2">Mode MAP</cell><cell>P@10 P@20 R-prec. Bpref NDCG</cell></row><row><cell>34 UNED-UV1</cell><cell>Text</cell><cell cols="2">0.1927 0.3914 0.3564 0.2663 0.2282 0.4092</cell></row><row><cell>40 UNED-UV9</cell><cell>Text</cell><cell cols="2">0.1865 0.4200 0.3636 0.2638 0.2253 0.4012</cell></row><row><cell cols="4">51 UNED-UV11 Mixed 0.1792 0.3914 0.3629 0.2514 0.2175 0.3887</cell></row><row><cell>52 UNED-UV8</cell><cell>Text</cell><cell cols="2">0.1790 0.3914 0.3350 0.2533 0.2150 0.4006</cell></row><row><cell cols="4">59 UNED-UV20 Mixed 0.1717 0.4071 0.3571 0.2499 0.2133 0.3803</cell></row><row><cell>61 UNED-UV2</cell><cell>Text</cell><cell cols="2">0.1627 0.3657 0.3293 0.2340 0.2002 0.3582</cell></row><row><cell cols="4">68 UNED-UV12 Mixed 0.1525 0.3943 0.3621 0.2236 0.1939 0.3341</cell></row><row><cell cols="4">69 UNED-UV10 Mixed 0.1502 0.3971 0.3607 0.2204 0.1920 0.3318</cell></row><row><cell cols="4">70 UNED-UV14 Mixed 0.1498 0.3543 0.3250 0.2203 0.1902 0.3387</cell></row><row><cell cols="4">72 UNED-UV19 Mixed 0.1427 0.4171 0.3671 0.2166 0.1872 0.3219</cell></row><row><cell>76 UNED-UV3</cell><cell>Text</cell><cell cols="2">0.1370 0.3871 0.3336 0.2146 0.1787 0.3168</cell></row><row><cell cols="4">77 UNED-UV15 Mixed 0.1286 0.3829 0.3386 0.1947 0.1687 0.2935</cell></row><row><cell cols="4">78 UNED-UV17 Mixed 0.1285 0.3614 0.3379 0.2047 0.1723 0.3049</cell></row><row><cell cols="4">79 UNED-UV13 Mixed 0.1261 0.3857 0.3307 0.1879 0.1650 0.2909</cell></row><row><cell cols="4">83 UNED-UV16 Mixed 0.1089 0.4043 0.3357 0.1728 0.1491 0.2588</cell></row><row><cell cols="4">84 UNED-UV18 Mixed 0.1077 0.3886 0.3307 0.1729 0.1492 0.2571</cell></row><row><cell>88 UNED-UV6</cell><cell>Text</cell><cell cols="2">0.0936 0.2671 0.2314 0.1312 0.1151 0.1885</cell></row><row><cell>89 UNED-UV4</cell><cell>Text</cell><cell cols="2">0.0920 0.2829 0.2536 0.1492 0.1301 0.2128</cell></row><row><cell>97 UNED-UV5</cell><cell>Text</cell><cell cols="2">0.0661 0.2943 0.2650 0.1156 0.1017 0.1703</cell></row><row><cell>102 UNED-UV7</cell><cell>Text</cell><cell cols="2">0.0603 0.2586 0.2221 0.0994 0.0851 0.1378</cell></row><row><cell>Average</cell><cell>Text</cell><cell cols="2">0,1579 0,3961 0,3519 0,2277 0,1992 0,3622</cell></row><row><cell>Best (pos 12)</cell><cell>Text</cell><cell cols="2">0,2361 0,4871 0,4393 0,3077 0,2694 0,5217</cell></row><row><cell>Average</cell><cell cols="3">Mixed 0,1387 0,3701 0,3293 0,1982 0,1759 0,3319</cell></row><row><cell>Best (pos 1)</cell><cell cols="3">Mixed 0,2630 0,6110 0,5410 0,3289 0,2970 0,5360</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work has been partially supported by projects <rs type="grantNumber">TIN2007-67407-C03-03</rs>, <rs type="grantNumber">TIN2007-67587</rs> and <rs type="grantNumber">TEC2009-12980</rs> from <rs type="funder">Spanish government</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JP523fU">
					<idno type="grant-number">TIN2007-67407-C03-03</idno>
				</org>
				<org type="funding" xml:id="_u9J5YYc">
					<idno type="grant-number">TIN2007-67587</idno>
				</org>
				<org type="funding" xml:id="_ZG6NxN4">
					<idno type="grant-number">TEC2009-12980</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,128.12,226.48,342.38,7.85;11,136.08,236.80,334.46,7.85;11,136.08,247.18,334.52,7.85;11,136.08,257.50,302.85,7.85" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,449.99,226.48,20.50,7.85;11,136.08,236.80,334.46,7.85;11,136.08,247.18,34.34,7.85">Some results using different approaches to merge visual and text-based features in CLEF&apos;08 photo collection</title>
		<author>
			<persName coords=""><forename type="first">Ana</forename><surname>García-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xaro</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rubén</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miguel</forename><surname>Goñi-Menoyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,317.33,257.50,15.41,7.85">Págs</title>
		<idno type="ISSN">0302-9743</idno>
		<imprint>
			<biblScope unit="volume">5706</biblScope>
			<biblScope unit="page" from="568" to="571" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.12,267.88,342.40,7.85;11,136.08,278.20,334.57,7.85;11,136.08,288.58,301.69,7.85" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="11,210.25,278.20,134.06,7.85;11,369.93,278.20,100.72,7.85;11,136.08,288.58,48.28,7.85">Cross-Language Evaluation Forum CLEF</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Agerri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Goñi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gomar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Domingo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ayala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-09">2009. 2009. September 2009</date>
			<pubPlace>Corfu (Grecia)</pubPlace>
		</imprint>
	</monogr>
	<note>MIRACLE (FI) at ImageCLEFphoto. Working Notes for the CLEF</note>
</biblStruct>

<biblStruct coords="11,128.12,298.90,342.47,7.85;11,136.08,309.22,304.05,7.85" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,335.46,298.90,135.13,7.85;11,136.08,309.22,88.53,7.85">Overview of the Wikipedia Retrieval task at ImageCLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jana</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,240.67,309.22,120.89,7.85">the Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.12,319.60,342.51,7.85;11,136.08,329.92,334.47,7.85;11,136.08,340.30,334.49,7.85;11,136.08,350.62,190.02,7.85" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,391.24,319.60,79.39,7.85;11,136.08,329.92,154.48,7.85">La herramienta IDRA (Indexing and Retrieving Automatically)</title>
		<author>
			<persName coords=""><forename type="first">Rubén</forename><surname>Granados Muñoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><forename type="middle">García</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><forename type="middle">M Goñi</forename><surname>Menoyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,214.38,340.30,256.19,7.85;11,136.08,350.62,108.43,7.85">XXV Conferencia de la Sociedad Española para el Procesamiento del Lenguaje Natural (SEPLN&apos;09)</title>
		<meeting><address><addrLine>San Sebastián</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">Septiembre de 2009. 2009</date>
			<biblScope unit="volume">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.12,361.00,342.45,7.85;11,136.08,371.32,334.55,7.85;11,136.08,381.70,26.29,7.85" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,358.62,361.00,111.95,7.85;11,136.08,371.32,164.56,7.85">Applying logistic regression to relevance feedback in image retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zuccarello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ayala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>De Ves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Domingo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,306.83,371.32,70.12,7.85">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2621" to="2632" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.12,392.02,342.47,7.85;11,136.08,402.40,325.72,7.85" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,175.44,392.02,295.15,7.85;11,136.08,402.40,25.09,7.85">On ordered weighted averaging aggregation operators in multi criteria decision making</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Yager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,167.64,402.40,179.19,7.85">IEEE Transactions Systems Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="183" to="190" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
