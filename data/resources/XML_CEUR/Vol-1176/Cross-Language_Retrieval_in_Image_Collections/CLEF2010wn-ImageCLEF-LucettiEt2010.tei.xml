<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,127.70,89.31,339.91,12.64;1,181.58,109.50,232.06,8.96">Combination of Classifiers for Indoor Room Recognition CGS participation at ImageCLEF2010 Robot Vision Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,205.25,145.50,57.92,8.96"><forename type="first">Walter</forename><surname>Lucetti</surname></persName>
							<email>w.lucetti@sssup.ite.luchetti@sssup.it</email>
							<affiliation key="aff0">
								<orgName type="department">Advanced Robotics Research Center Scuola Superiore di Studi e Perfezionamento Sant&apos;Anna</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.67,145.50,71.25,8.96"><forename type="first">Emanuel</forename><surname>Luchetti</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Robotics Research Center Scuola Superiore di Studi e Perfezionamento Sant&apos;Anna</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,198.05,169.31,64.73,8.10"><forename type="first">Gustavo</forename><surname>Stefanini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Robotics Research Center Scuola Superiore di Studi e Perfezionamento Sant&apos;Anna</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,127.70,89.31,339.91,12.64;1,181.58,109.50,232.06,8.96">Combination of Classifiers for Indoor Room Recognition CGS participation at ImageCLEF2010 Robot Vision Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6535D859805D5171059E753042A869AE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>Visual Place Classification</term>
					<term>Features Extraction</term>
					<term>Support Vector Machines</term>
					<term>Bayes Classifier</term>
					<term>Artificial Neural Network</term>
					<term>Committees of Experts</term>
					<term>Stacked Regression</term>
					<term>Stacked Generalization</term>
					<term>Stereo Vision</term>
					<term>Hough Transform</term>
					<term>Discrete Fourier Transform</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper represents a description of our approach to the problem of topological localization of a mobile robot using visual information. Our method has been developed for ImageCLEF 2010 Robot Vision Task challenge. The challenge was focused on the problem of visual place classification, with a special focus on generalization. The goal was to recognize rooms by the images captured with a stereo camera mounted on a mobile robot within an office environment. Algorithms should be able to reply to question "Where are you?", saying "I don't know" if the room analyzed was not presented during training phase. For the challenge three sequences were given: Training Set, Validation Set and Test Set acquired on three different floors of the same building. We chose to approach the challenge realizing a multi-Level machine learning architecture, made of a first "weak" classifiers Level based on visual features extracted from images and of a second Level performing fusion of first Level outputs. We developed four configurations to determine the best approach to problem solving: Committees of Experts, Stacked Regression with Support Vector Machines stage, Stacked Regression with Artificial Neural Network stage, Weighted Linear Combination of all the three previous methods. Finally the result of twenty RUNs, five RUNs for each of the four different system configurations, were submitted at ImageCLEF challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="841.92" lry="595.32"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF <ref type="foot" coords="1,162.86,538.37,3.00,5.40" target="#foot_0">1</ref> hosted in 2010 the third edition of Robot Vision Challenge. The task addressed the problem of Visual Place Classification, this edition with a special focus on generalization <ref type="bibr" coords="1,172.46,562.67,10.69,8.96" target="#b0">[1]</ref>. We chose to approach the challenge using Classifiers Combination method <ref type="bibr" coords="1,431.47,574.67,10.69,8.96" target="#b1">[2]</ref>, after the analysis of the provided training set images. Together to Training Set a Validation Set was released, Validation Set consisted of 2069 image couples acquired on a different floor of the same building used to create the training set, but only seven known rooms was visited and three "Unknown" rooms were added to be able to test "Unknown" recognition techniques.</p><p>Finally after about one month from Training Set release, a final Test Set was released. It consisted of 2741 unlabeled image couples acquired to a third floor of the same building. Analogue rooms were visited and other "Unknown" rooms were presented. We performed twenty different RUNs on test set: five RUNs for each of the four different Classifiers Combination methods we developed.</p><p>For each submitted RUN was given a score as described in <ref type="bibr" coords="1,362.74,706.70,10.66,8.96" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Architecture</head><p>The approach we chose to follow is illustrated in Fig. <ref type="figure" coords="2,351.19,113.82,3.90,8.96">1</ref>: each stereo image couple is processed to extract different kind of visual features that are analyzed by a first stage of classifier (Level-0) to produce a set of label. The set of label produced by Level-0 becomes the input for the second stage (Level-1) that produces a set of values indicating the Confidence Level of every possible output class. A final stage analyzes Level-1 outputs and gives in output the final reply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Combination of Classifiers scheme</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation</head><p>Many method of Image Classification using machine learning techniques were implemented using a single type of features extracted from single image or Stereo Images couple. The classification made on one type of features works well only on a well stated kind of environment (i.e. texture features extraction is right for highly repetitive images, but is not informative for homogeneous images). Combining different kind of Level-0 classifiers allows to choose the classifier that gives the correct reply according to the image presented as input at the system. Level-1 is capable of taking several classifier outputs as input and to learn from training data how well they perform and how their outputs should be combined. Final results confirmed the strength of the method (Par. 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Level-0: a Pool of Experts</head><p>The Level-0 stage that we realized is composed of a total of ten classifiers, five set of two kind of classifiers working in couple: Support Vector Machines <ref type="bibr" coords="2,383.53,560.15,16.93,8.96" target="#b9">[10]</ref> and Normal Bayes Classifier <ref type="bibr" coords="2,154.22,572.15,15.42,8.96" target="#b10">[11]</ref>.</p><p>Five different sets of visual features are extracted from every pair of stereo image (Fig. <ref type="figure" coords="2,473.62,584.15,4.18,8.96">2</ref>) of the dataset:</p><formula xml:id="formula_0" coords="2,137.54,614.05,86.12,57.09"> Color Features  Texture Features  Segment Features  Depth Features  3D Space Features</formula><p>Every couple of classifiers is trained on the same set of visual features such to obtain two different replies of the same type, such to increase the probability of obtaining a correct classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Stereo Couple Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Color Features</head><p>Left image of every frame stereo couple is converted from RGB to Luv color mode (Fig. <ref type="figure" coords="3,473.86,301.17,4.12,8.96">3</ref>) and divided in nine sub-image. From every sub-image are extracted mean and standard deviation of each of the three image channels for a total of 54 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3. Color Features Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Texture Features</head><p>Left image of every frame stereo couple is divided in nine sub-image that are processed with Discrete Fourier Transform to extract frequency components (Fig. <ref type="figure" coords="3,423.19,609.23,3.63,8.96">4</ref>). Magnitude spectrum of each sub-image is calculated and are extracted frequency and phase of the ten higher power components, for a total of 180 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 4. Texture Features Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Segment Features</head><p>Left image from every frame is filtered using Canny Edge Detector algorithm <ref type="bibr" coords="4,446.74,336.21,16.72,8.96" target="#b11">[12]</ref> and from result image are extracted 30 line segments (Fig. <ref type="figure" coords="4,357.07,348.21,4.18,8.96">5</ref>) using Probabilistic Hough Transform <ref type="bibr" coords="4,158.06,360.21,10.68,8.96" target="#b7">[8]</ref>. For every segment are extracted length and angle for a total of 60 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5. Segment Features Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Depth Features</head><p>From every frame stereo couple disparity map is calculated (Fig. <ref type="figure" coords="4,393.07,623.39,4.12,8.96">6</ref>) using Semi-Global Block Matching Stereo Correspondence algorithm <ref type="bibr" coords="4,326.23,635.39,10.68,8.96" target="#b8">[9]</ref>. Disparity map is divided in nine grayscale sub-images, and from each sub-image are extracted mean and standard deviation for a total of 18 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 6. Depth Features Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">3D Space Features</head><p>Using stereo system Intrinsic Parameters every pixel of Disparity Map is projected in 3D space coordinates system (Fig. <ref type="figure" coords="5,239.33,332.97,3.63,8.96">7</ref>). From 3D Space Information are extracted seven features:</p><p> mean and standard deviation of distance of each 3D point from robot reference system origin;  mean and standard deviation of height of every 3D point;  mean, standard deviation and maximum of depth (Z coordinate) values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 7. 3D Features Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Level-1: Regression Stage</head><p>Classifiers labels obtained at Level-0 stage are combined to produce a set of Confidence Values. To choose the best approach we analyzed four different algorithms for Level-1 stage:</p><p> Committee of Experts  Stacked Regression using Support Vector Machines  Stacked Regression using Artificial Neural Network  Linear Weighted Combination of the previous three</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Committee of Experts</head><p>Committee of Experts (CoE) was initially described as a method to improve regression estimates in <ref type="bibr" coords="6,165.46,121.86,11.76,8.96" target="#b3">[4]</ref> and <ref type="bibr" coords="6,198.81,121.86,10.65,8.96" target="#b4">[5]</ref>, but can be used for both regression and classification. We use a modified version of CoE that has multiple output values instead of single output label. The algorithm is represented in Fig. <ref type="figure" coords="6,242.69,145.86,3.90,8.96" target="#fig_0">8</ref>: at Level-0 a pool of N experts estimates a target function giving N replies , with , where M is the number of the classes and is the Indicator Function. The reply of each j-th Level-0 expert ( ) indicates one (i-th) of the M available classes. The N outputs coming from Level-0 are linearly combined using weights:</p><p>The weights are manually assigned to each expert, according to the accuracy evaluated in classifying Validation Set images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Stacked Regression</head><p>Stacked Regression (SR) <ref type="bibr" coords="6,219.05,666.98,11.72,8.96" target="#b5">[6]</ref> is based on Stacked Generalization (SG) method, introduced by Wolpert in 1990 <ref type="bibr" coords="6,194.40,678.98,11.88,8.96" target="#b2">[3]</ref> and was initially presented as a method to combine multiple models for classification. Next SG was also used for regression and even unsupervised learning <ref type="bibr" coords="6,467.01,690.98,10.78,8.96" target="#b6">[7]</ref>.</p><p>Like in CoE method we have a pool of experts estimating a target function : the pool composes the so called "Level-0 generalizer" and it is trained in the first stage of SR. The second stage consists in training a Regression Level that takes as inputs the outputs of Level-0 generalizers and try to estimate the Confidence Values of every possible output class. This stage is called "Level-1 Regression" and its purpose is to learn the biases of Level-0 generalizers.</p><p>It is very important that Level-1 and Level-0 machine learning networks are trained using different dataset: in this way generalization capabilities are granted, and over fitting probabilities is decreased. The training approach chosen will be detailed in Par. 6. We chose to evaluate two kinds of Level-1 approach:  Array of Support Vector Machines Regressors (SVM-R), one for every possible output class (Fig. <ref type="figure" coords="7,168.98,177.90,3.63,8.96">9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 9. Stacked Regression with SVM-R array</head><p> Artificial Neural Network -Multi Layer Perceptron (ANN-MLP) with an output for every possible class (Fig. <ref type="figure" coords="7,229.01,461.85,7.97,8.96">10</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 10. Stacked Regression with ANN-MLP</head><p>SVM-R approach is different from ANN-MLP since every Level-1 output is independent from each other. Every output is obtained using a different SVM-R network trained to recognized its own class and to regret all the other.</p><p>ANN-MLP outputs are instead obtained by a single network with multiple outputs, where each output is influenced by the full connection between units of hidden-outputs layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Linear Combination of methods</head><p>The three methods previously analyzed can be linearly combined using weights such to obtain a unique more precise reply; the combination scheme is illustrated in Fig. <ref type="figure" coords="8,450.46,210.90,8.38,8.96" target="#fig_1">11</ref>. The linear combination is weighted and weights have been chosen according to generalization capabilities evaluated during Validation Phase on Validation Set images, where  With this configuration scheme we obtained a remarkable improvement in performances two different training sets for Level-0 and Level-1, each composed of 4800 examples (just compare RUN #2 and RUN #10 scores in Table <ref type="table" coords="9,309.17,472.31,3.63,8.96" target="#tab_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Final Results and Future Works</head><p>For ImageCLEF 2010 Robot Vision Task we submitted twenty RUNs (Table <ref type="table" coords="9,432.31,534.35,4.18,8.96" target="#tab_0">1</ref>) on a total of 42 RUNs submitted by all the team participating. Our best score has been 253 (the winner totalized 677), obtained in RUN #14 and we placed 4 th . The twenty submitted RUNs can be subdivided in five groups of four RUNs. Each group was composed of four tests on all the four methods previously illustrated and was different from the other for at least one feature. The first group was an initial test made training Level-0 Classifiers and Level-1 Regressors on the original unbalanced Training Set, for the other groups the same Balanced Training Set was used (see Par. 6). The second group was the first test carried on recognition of "Unknown Rooms"; "Unknown Rooms" recognition has been disabled in the third group where in case of not dominant Level-1 reply we chose to not give a reply label for the analyzed frame. In the last two groups Level-1 Regressors was trained again on the same Level-0 outputs of second and third groups, with Stereo Vision disabled such to determinate its real contribute to final results, since we noticed that Disparity Map and 3D Space Classifiers performances were very poor.</p><p>Analyzing result table is evident the strength of Combining of Classifiers method. Looking at RUNs from #9 to #12 we can notice that the best classifier in Level-0 (columns from 7 to 16) obtained a score of -725, while the worst result (column 2) in Level-1 configurations is -342. This is evident also looking at RUNs from #13 to #16.</p><p>Our best RUN, the #14, was obtained using SR with SVM approach, choosing to not classify unknown rooms (giving indeed no reply) and deactivating Stereo Vision Features that was not very performing (as highlighted in columns of runs from #5 to #12).</p><p>The weakness of our method lies in unknown rooms recognizing phase. To improve this phase we developed a method that introduces a new Level of classification that, taking "Level-1" outputs as input, would make distinction between "known" and "unknown" rooms in the case that there is not a dominant Level-1 output (Par. 5). The short time available did not allow us to fully complete this enhancement, that is scheduled to realized for our future works on Image Classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Color Features</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,244.97,616.72,105.31,8.10;6,114.90,401.05,368.25,207.00"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Committee of Experts</figDesc><graphic coords="6,114.90,401.05,368.25,207.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,201.65,519.88,191.78,8.10;8,114.90,316.05,368.50,195.13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Linear Weighted Combination of Regressors</figDesc><graphic coords="8,114.90,316.05,368.50,195.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,222.29,207.11,150.54,8.10;9,240.57,85.05,115.70,113.39"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Level-1 output activation scheme</figDesc><graphic coords="9,240.57,85.05,115.70,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,114.90,211.05,368.25,207.00"><head></head><label></label><figDesc></figDesc><graphic coords="7,114.90,211.05,368.25,207.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,114.90,495.05,368.25,207.00"><head></head><label></label><figDesc></figDesc><graphic coords="7,114.90,495.05,368.25,207.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,66.00,95.58,709.29,371.36"><head>Table 1 .</head><label>1</label><figDesc>Submitted RUNs Scores and System Configurations (n.a. = used, but scores not available / n.u. = not used)</figDesc><table coords="11,473.71,95.58,288.08,20.48"><row><cell>Texture Features</cell><cell>Hough Features</cell><cell>Disparity Map Features</cell><cell>3D Space Features</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,118.70,741.43,141.99,8.10"><p>ImageCLEF: http://www.imageclef.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,120.50,89.24,389.93,9.06;12,107.78,101.24,371.88,9.05" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,466.12,89.24,44.32,9.05;12,107.78,101.24,132.21,9.05">The Robot Vision Task at ImageCLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">Andrzej</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Fornoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henrik</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,258.21,101.24,134.26,9.05">the Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,120.50,113.24,389.77,9.06;12,107.78,125.34,332.38,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,291.21,113.24,199.21,9.06">Classifier Fusion for Outdoor Obstacle Detection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Dima</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Martial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,107.78,125.34,217.83,8.96">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="665" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,120.50,137.24,384.79,9.06" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,189.78,137.24,92.99,9.05">Stacked Generalization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<idno>LA-UR-90-3460</idno>
		<imprint>
			<date type="published" when="1990">1990</date>
			<pubPlace>Los Alamos, NM,</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct coords="12,120.50,149.24,389.25,9.05;12,107.78,161.24,392.93,9.06" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,177.26,149.24,332.48,9.05;12,107.78,161.24,203.13,9.05">Improving regression estimation: Averaging methods for variance reduction with extensions to general convex measure optimization</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Brown Universitiy</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct coords="12,120.50,173.24,389.15,9.05;12,107.78,185.24,402.52,9.06;12,107.78,197.34,90.64,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,254.93,173.24,254.72,9.05;12,107.78,185.24,38.36,9.05">When networks disagree: Ensemble methods for Hybrid Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Perrone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">N</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,165.38,185.34,202.75,8.96">Neural Network for speech and Image Processing</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mammone</surname></persName>
		</editor>
		<imprint>
			<publisher>Chapmann-Hallpp</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="126" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,120.50,209.24,302.66,9.06" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,175.44,209.24,76.90,9.05">Stacked Regression</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,263.57,209.34,72.43,8.96">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,120.50,221.24,389.18,9.05;12,107.78,233.34,399.29,8.96" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,232.53,221.24,269.12,9.05">An evaluation of linearly combining density estimators via stacking</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wolpert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Information and Computer Science Department, University of California, Irvine, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,120.50,245.24,389.79,9.06;12,107.78,257.34,64.77,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,322.89,245.24,178.43,9.06">Progressive Probabilistic Hough Transform</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Galambos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">V</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,107.78,257.34,33.99,8.96">BMVC</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,120.50,269.24,389.81,9.05;12,107.78,281.27,189.92,9.06" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,202.82,269.24,307.49,9.05;12,107.78,281.27,36.04,9.05">Stereo Vision in Structured Environments by Consistent Semi-Global Matching</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,155.54,281.37,24.90,8.96">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2386" to="2393" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,120.50,293.27,389.73,9.06;12,107.78,305.37,184.56,8.96" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<title level="m" coord="12,264.48,293.27,245.75,9.06;12,107.78,305.37,32.35,8.96">Support Vector Machines and other kernel-based learning methods</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,120.50,317.27,389.82,9.06;12,107.78,329.37,48.61,8.96" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,181.33,317.27,182.30,9.05">Introduction to Statistical Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct coords="12,120.50,341.27,307.58,9.06" xml:id="b11">
	<monogr>
		<ptr target="http://en.wikipedia.org/wiki/Canny_edge_detector" />
		<title level="m" coord="12,125.59,341.27,85.41,9.05">Canny Edge Detector</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
