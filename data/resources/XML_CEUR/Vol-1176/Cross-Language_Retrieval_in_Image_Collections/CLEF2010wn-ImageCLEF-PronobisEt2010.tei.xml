<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.04,115.96,319.05,12.62">The Robot Vision Track at ImageCLEF 2010</title>
				<funder ref="#_JeBCUfw">
					<orgName type="full">Hasler foundation</orgName>
				</funder>
				<funder ref="#_D2xP9y2">
					<orgName type="full">EU</orgName>
				</funder>
				<funder ref="#_ZqTeTqa">
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,147.67,156.27,76.07,8.74"><forename type="first">Andrzej</forename><surname>Pronobis</surname></persName>
							<email>pronobis@kth.se</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Autonomous Systems</orgName>
								<orgName type="institution">The Royal Institute of Technology</orgName>
								<address>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,234.30,156.27,64.12,8.74"><forename type="first">Marco</forename><surname>Fornoni</surname></persName>
							<email>mfornoni@idiap.ch</email>
							<affiliation key="aff2">
								<orgName type="institution">Idiap Research Institute</orgName>
								<address>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.98,156.27,93.43,8.74"><forename type="first">Henrik</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,432.35,156.27,35.33,8.74;1,289.39,168.23,32.10,8.74"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
							<email>bcaputo@idiap.ch</email>
							<affiliation key="aff2">
								<orgName type="institution">Idiap Research Institute</orgName>
								<address>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.04,115.96,319.05,12.62">The Robot Vision Track at ImageCLEF 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C2DF793F37F5DDB13FD3893EB38A800B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Place recognition</term>
					<term>robot vision</term>
					<term>robot localization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the robot vision track that has been proposed to the ImageCLEF 2010 participants. The track addressed the problem of visual place classification, with a special focus on generalization. Participants were asked to classify rooms and areas of an office environment on the basis of image sequences captured by a stereo camera mounted on a mobile robot, under varying illumination conditions. The algorithms proposed by the participants had to answer the question "where are you?" (I am in the kitchen, in the corridor, etc) when presented with a test sequence, acquired within the same building but at a different floor than the training sequence. The test data contained images of rooms seen during training, or additional rooms that were not imaged in the training sequence. The participants were asked to solve the problem separately for each test image (obligatory task). Additionally, results could also be reported for algorithms exploiting the temporal continuity of the image sequences (optional task). A total of seven groups participated to the challenge, with 42 runs submitted to the obligatory task, and 13 submitted to the optional task. The best result in the obligatory task was obtained by the Computer Vision and Geometry Laboratory, ETHZ, Switzerland, with an overall score of 677. The best result in the optional task was obtained by the Idiap Research Institute, Martigny, Switzerland, with an overall score of 2052.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF<ref type="foot" coords="2,187.76,140.62,3.97,6.12" target="#foot_0">4</ref>  <ref type="bibr" coords="2,196.55,142.19,8.19,8.74" target="#b0">[1]</ref><ref type="bibr" coords="2,204.74,142.19,4.10,8.74" target="#b1">[2]</ref><ref type="bibr" coords="2,208.84,142.19,8.19,8.74" target="#b2">[3]</ref> started in 2003 as part of the Cross Language Evaluation Forum (CLEF<ref type="foot" coords="2,196.52,152.57,3.97,6.12" target="#foot_1">5</ref> , <ref type="bibr" coords="2,206.53,154.15,10.30,8.74" target="#b3">[4]</ref>). Its main goal has been to promote research on multi-modal data annotation and information retrieval, in various application fields. As such it has always contained visual, textual and other modalities, mixed tasks and several sub tracks.</p><p>The robot vision track has been proposed to the ImageCLEF participants for the first time in 2009. The track attracted a considerable attention, with 19 inscribed research groups, 7 groups eventually participating and a total of 27 submitted runs. The track addressed the problem of visual place recognition applied to robot topological localization. The second edition of the challenge was held in conjunction with ICPR 2010 and saw an increase in participation, with 9 participating groups and 34 submitted runs. As for previous events, in 2010 the challenge addressed the problem of visual place classification, this time with a special focus on generalization.</p><p>Participants were asked to classify rooms and functional areas on the basis of image sequences, captured by a stereo camera mounted on a mobile robot within an office environment. The test sequence was acquired within the same building but at a different floor than the training sequence. It contained rooms of the same categorical type (corridor, office, bathroom), and it also contained room categories not seen in the training sequence (meeting room, library). The system built by participants had to be able to answer the question 'where are you?' when presented with a test sequence imaging a room category seen during training, and it had to be able to answer 'I do not know this category' when presented with a new room category.</p><p>We received a total of 55 submissions, of which 42 were submitted to the obligatory task and 13 to the optional task. The best result in the obligatory task was obtained by the Computer Vision and Geometry Laboratory, ETHZ, Switzerland. The best result in the optional task was obtained by the Idiap Research Institute, Martigny, Switzerland.</p><p>This paper provides an overview of the robot vision track and reports on the runs submitted by the participants. First, details concerning the setup of the robot vision track are given in Section 2. Then, Section 3 presents the participants and Section 4 provides the ranking of the obtained results. Conclusions are drawn in Section 5. Additional information about the task and on how to participate in the future robot vision challenges can be found on the ImageCLEF web pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The RobotVision Track</head><p>This section describes the details concerning the setup of the robot vision track. Section 2.1 describes the dataset used. Section 2.2 gives details on the tasks proposed to the participants. Finally, section 2.3 describes briefly the algorithm used for obtaining a ground truth and the evaluation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset</head><p>The image sequences used for the contest are taken from the previously unreleased COLD-Stockholm database. The sequences were acquired using the MobileRobots PowerBot robot platform equipped with a stereo camera system consisting of two Prosilica GC1380C cameras. The acquisition was performed on three different floors of an office environment, consisting of 36 areas (usually corresponding to separate rooms) belonging to 12 different semantic and functional categories.</p><p>The robot was manually driven through the environment while continuously acquiring images at a rate of 5fps. Each data sample was then labeled as belonging to one of the areas according to the position of the robot during acquisition (rather than contents of the images).</p><p>Three sequences were selected for the contest: a training sequence, a sequence that should be used for validation and a sequence for testing:</p><p>training sequence: Sequence acquired in 11 areas, on the 6th floor of the office building, during the day, under cloudy weather. The robot was driven through the environment following a similar path as for the test and validation sequences and the environment was observed from many different viewpoints (the robot was positioned at multiple points and performed 360 degree turns). validation sequence: Sequence acquired in 11 areas, on the 5th floor of the office building, during the day, under cloudy weather. Similar path was followed as for the training sequence; however without making the 360 degree turns.</p><p>testing sequence -Sequence acquired in 14 areas, on the 7th floor of the office building, during the day, under cloudy weather. The robot followed similar path as in case of the validation sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Task</head><p>Participants were given training data consisting of a sequence of stereo images.</p><p>The training sequence was recorded using a mobile robot that was manually driven through several rooms of a typical indoor office environment. The acquisition was performed under fixed illumination conditions and at a given time.</p><p>Each image in the training sequence was labeled and assigned to an ID and a semantic category of the area (usually a room) in which it was acquired.</p><p>The challenge was to build a system able to answer the question 'where are you?' (I'm in the kitchen, in the corridor, etc.) when presented with test sequence containing images acquired in a different environment (different floor of the same building) containing areas belonging to the semantic categories observed previously (present in the training sequence) or to new semantic categories (not imaged in the training sequence). The test images were acquired under similar illumination settings as the training data, but in a different office environment. The system should assign each test image to one of the semantic categories of the areas that were present in the training sequence or indicate that the image belongs to an unknown semantic category not included during training. Moreover, the system could refrain from making a decision (e.g. in the case of lack of confidence).</p><p>We considered two separate tasks, task 1 (obligatory) and task 2 (optional). In task 1, the algorithm had to be able to provide information about the location of the robot separately for each test image, without relying on information contained in any other image (e.g. when only some of the images from the test sequences are available or the sequences are scrambled). In task 2, the algorithm was allowed to exploit the continuity of the sequences and rely on the test images acquired before the classified image (images acquired after the classified image could not be used). The same training, validation and testing sequences were used for both tasks. The reported results were compared separately.</p><p>The competition started with the release of annotated training and validation data. Moreover, the participants were given a tool for evaluating performance of their algorithms. The test image sequences were released later. The test sequences were acquired in a different environment than the training and validation sequences (one more floor of the same building), under similar conditions, and contained additional rooms belonging to semantic categories that were not imaged previously. The algorithms trained on the training sequence will be used to annotate each of the test images. The same tools and procedure as for the validation were used to evaluate and compare the performance of each method during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ground Truth and Evaluation</head><p>The image sequences used in the competition were annotated with ground truth. The annotations of the training and validation sequences were available to the participants, while the ground truth for the test sequence was released after the results were announced. Each image in the sequences was labelled according to the position of the robot during acquisition as belonging to one of the rooms used for training or as an unknown room. The ground truth was then used to calculate a score indicating the performance of an algorithm on the test sequence. The following rules were used when calculating the overall score for the whole test sequence:</p><p>-1 point was granted for each correctly classified image belonging to one of the known categories. -1 points was subtracted for each misclassified image belonging to one of the known or unknown categories. -No points were granted or subtracted if an image was not classified (the algorithm refrained from the decision). -2 points were granted for a correct detection of a sample belonging to an unknown category (true positive). -2.0 points were subtracted for an incorrect detection of a sample belonging to an unknown category (false positive).</p><p>A script was available to the participants that automatically calculated the score for a specified test sequence given the classification results produced by an algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participation</head><p>In 2010, 7 groups participated to the Robot Vision task, namely:</p><p>- A total of 55 runs were submitted, with 42 runs submitted to the obligatory task and 13 runs submitted to the optional task. In order to encourage participation, there was no limit to the number of runs that each group could submit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This section presents the results of the robot vision track of ImageCLEF 2010. Table <ref type="table" coords="5,161.93,656.12,4.98,8.74" target="#tab_0">1</ref> shows the results for the obligatory task, while Table <ref type="table" coords="5,402.75,656.12,4.98,8.74" target="#tab_1">2</ref> shows the result for the optional task. Scores are presented for each of the submitted runs that complied with the rules of the contest.</p><p>We see that the majority of runs were submitted to the obligatory task. A possible explanation is that the optional task requires a higher expertise in robotics that the obligatory task, which therefore represents a very good entry point. The same behavior was noted at the other editions of the robot vision task.</p><p>These results indicate quite clearly that the capability to recognize visually a place under different viewpoints is still an open challenge for mobile robots. This is a strong motivations towards proposing similar tasks to the community in the future editions of the robot vision task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,177.71,117.78,259.94,167.32"><head>Table 1 .</head><label>1</label><figDesc>Results obtained by each group in the obligatory task.</figDesc><table coords="5,239.15,117.78,137.07,167.32"><row><cell>#</cell><cell>Group</cell><cell>Score</cell></row><row><cell>1</cell><cell>CVG</cell><cell>677</cell></row><row><cell>2</cell><cell>Idiap MULTI</cell><cell>662</cell></row><row><cell>3</cell><cell>NUDT</cell><cell>638</cell></row><row><cell cols="3">4 Centro Gustavo Stefanini 253</cell></row><row><cell>5</cell><cell>CAOR</cell><cell>62</cell></row><row><cell>6</cell><cell>DYNILSIS</cell><cell>-20</cell></row><row><cell>7</cell><cell>UAIC2010</cell><cell>-77</cell></row><row><cell>#</cell><cell>Group</cell><cell>Score</cell></row><row><cell cols="3">1 Idiap MULTI 2052</cell></row><row><cell>2</cell><cell>CAOR</cell><cell>62</cell></row><row><cell cols="3">3 DYNILSIS -67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,181.69,288.12,251.98,7.89"><head>Table 2 .</head><label>2</label><figDesc>Results obtained by each group in the optional task.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,646.48,117.68,7.47"><p>http://www.imageclef.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="2,144.73,657.44,136.51,7.47"><p>http://www.clef-campaign.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Conclusions</head><p>The robot vision task at ImageCLEF@ICPR2010 attracted a considerable attention and proved an interesting complement to the existing tasks. The approach presented by the participating groups were diverse and original, offering a fresh take on the topological localization problem. We plan to continue the task in the next years, proposing new challenges to the participants. In particular, we plan to focus on the problem of place categorization and use objects as an important source of information about the environment.</p></div>
			</div>
			<div type="funding">
<div><p>We would like to thank the CLEF campaign for supporting the ImageCLEF initiative. <rs type="person">B. Caputo</rs> was supported by the <rs type="projectName">EMMA</rs> project, funded by the <rs type="funder">Hasler foundation</rs>. <rs type="person">M. Fornoni</rs> was supported by the <rs type="projectName">MULTI</rs> project, funded by the <rs type="funder">Swiss National Science Foundation</rs>. A. Pronobis was supported by the <rs type="funder">EU</rs> <rs type="programName">FP7</rs> project <rs type="grantNumber">ICT-215181-CogX</rs>. The support is gratefully acknowledged.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_JeBCUfw">
					<orgName type="project" subtype="full">EMMA</orgName>
				</org>
				<org type="funded-project" xml:id="_ZqTeTqa">
					<orgName type="project" subtype="full">MULTI</orgName>
				</org>
				<org type="funding" xml:id="_D2xP9y2">
					<idno type="grant-number">ICT-215181-CogX</idno>
					<orgName type="program" subtype="full">FP7</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,421.54,342.24,7.86;6,146.91,432.50,333.68,7.86;6,146.91,443.46,333.67,7.86;6,146.91,454.42,106.52,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,196.57,432.50,215.77,7.86">The CLEF 2005 cross-language image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,436.33,432.50,44.27,7.86;6,146.91,443.46,128.10,7.86">Cross Language Evaluation Forum (CLEF</title>
		<title level="s" coord="6,304.34,443.46,176.25,7.86">Springer Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2005-09">2005. September 2006</date>
			<biblScope unit="page" from="535" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,465.38,342.24,7.86;6,146.91,476.34,333.68,7.86;6,146.91,487.30,333.68,7.86;6,146.91,498.25,333.68,7.86;6,146.91,509.21,295.27,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,309.84,465.38,170.75,7.86;6,146.91,476.34,101.67,7.86">The CLEF cross-language image retrieval track (ImageCLEF) 2004</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,265.77,487.30,214.82,7.86;6,146.91,498.25,225.54,7.86">Multilingual Information Access for Text, Speech and Images: Result of the fifth CLEF evaluation campaign</title>
		<title level="s" coord="6,450.23,498.25,30.36,7.86;6,146.91,509.21,135.42,7.86">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,520.17,342.25,7.86;6,146.91,531.13,333.68,7.86;6,146.91,542.09,333.68,7.86;6,146.91,553.05,253.29,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,193.85,531.13,286.74,7.86;6,146.91,542.09,19.07,7.86">Overview of the ImageCLEFmed 2007 medical retrieval and annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Deserno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,192.20,542.09,97.81,7.86">CLEF 2007 Proceedings</title>
		<title level="s" coord="6,366.84,542.09,113.76,7.86;6,146.91,553.05,55.21,7.86">Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5152</biblScope>
			<biblScope unit="page" from="473" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,564.01,342.25,7.86;6,146.91,574.97,333.68,7.86;6,146.91,585.93,73.21,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,188.10,564.01,140.48,7.86">Report on CLEF-2001 experiments</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,350.09,564.01,130.51,7.86;6,146.91,574.97,169.92,7.86">Report on the CLEF Conference 2001 (Cross Language Evaluation Forum)</title>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="27" to="43" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
