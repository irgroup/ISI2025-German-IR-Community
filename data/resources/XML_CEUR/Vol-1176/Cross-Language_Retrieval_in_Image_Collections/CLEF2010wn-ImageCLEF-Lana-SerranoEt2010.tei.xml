<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,132.24,151.57,330.95,12.64;1,140.40,168.97,314.75,12.64">DAEDALUS at ImageCLEF Wikipedia Retrieval 2010: Expanding with Semantic Information from Context</title>
				<funder ref="#_NFeRh7B">
					<orgName type="full">Universidad Politécnica de Madrid</orgName>
				</funder>
				<funder ref="#_BmPjP3F">
					<orgName type="full">Spanish Center for Industry Technological Development (CDTI, Ministry of Industry, Tourism and Trade)</orgName>
				</funder>
				<funder ref="#_auxD7m9">
					<orgName type="full">Agencia EFE, Germinus XXI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.36,207.88,74.58,8.96"><forename type="first">Sara</forename><surname>Lana-Serrano</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Decisions and Language</orgName>
								<orgName type="institution">DAEDALUS -Data</orgName>
								<address>
									<country>S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.20,207.88,83.46,8.96"><forename type="first">Julio</forename><surname>Villena-Román</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Decisions and Language</orgName>
								<orgName type="institution">DAEDALUS -Data</orgName>
								<address>
									<country>S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.80,207.88,126.13,8.96"><forename type="first">José</forename><surname>Carlos González-Cristóbal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Decisions and Language</orgName>
								<orgName type="institution">DAEDALUS -Data</orgName>
								<address>
									<country>S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,132.24,151.57,330.95,12.64;1,140.40,168.97,314.75,12.64">DAEDALUS at ImageCLEF Wikipedia Retrieval 2010: Expanding with Semantic Information from Context</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4DF7703C6A345613BB3352CA11774B8C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image retrieval</term>
					<term>domain-specific vocabulary</term>
					<term>ontology</term>
					<term>semantic expansion</term>
					<term>information retrieval</term>
					<term>indexing</term>
					<term>topic expansion</term>
					<term>context</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of DAEDALUS at the ImageCLEF 2010 Wikipedia Retrieval task. The main focus of our experiments is to evaluate the impact in the image retrieval process of the incorporation of semantic information extracted only from the textual information provided as metadata of the image itself, as compared to expanding with contextual information gathered from the document where the image is referred. For the semantic annotation, DBpedia ontology and YAGO classification schema are used. As expected, the obtained results show that, in general, the textual information attached to a given image is not able to fully represent certain features of the image. Furthermore, the use of semantic information in the process of multimedia information extraction poses two hard challenges still to solve: how to automatically extract the high level features associated to a multimedia resource, and, once the resource has been semantically tagged, which features must be used in the retrieval process to best model the actual and complete meaning of the user query.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The basic goal of the ImageCLEF 2010 Wikipedia Retrieval task <ref type="bibr" coords="1,395.40,570.04,11.71,8.96" target="#b0">[1]</ref> was, similar to previous campaigns, given a textual query and/or sample images describing a user's multimedia information need, find as many relevant images as possible from the Wikipedia images collection. Each image in the collection is tagged with both its user-provided annotation consisting of unstructured and noisy textual annotations in English, French, and German, and also links to the article(s) that contain the image. This paper describes the participation of DAEDALUS team at the ImageCLEF 2010 Wikipedia Retrieval task. We are a research group led by and named after DAEDALUS, a small private company in the field of Information and Telecommunication Technologies and a leading provider of language-based solutions in Spain, and research groups of two universities, Universidad Politécnica de Madrid and Universidad Carlos III de Madrid. We have taken part in CLEF since 2003 in many different tracks and tasks, as part of the MIRACLE team till last year.</p><p>This year, the main objective of our experiments is to evaluate and compare the results achieved by the application of techniques that are based on the computational similarity between the metadata associated to the images and the query itself, as opposed to other techniques based on the semantic description of the image based on the contextual information provided by the Wikipedia article in which the image is referred. For this purpose, the DBpedia ontology <ref type="bibr" coords="2,324.60,241.60,11.71,8.96" target="#b1">[2]</ref> and the YAGO <ref type="bibr" coords="2,403.20,241.60,11.71,8.96" target="#b2">[3]</ref> classification schema have been used as the knowledge base to annotate the semantic content,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>Based on our experience in previous campaigns in CLEF and other forums, we designed a flexible system in order to be able to execute a large number of runs that exhaustively cover many combinations of different techniques. Our system is composed of a set of small components that are easily combined in different configurations and executed sequentially to build the final result set. Specifically, our system is composed of four modules:</p><p>• Linguistic processing module, which extract, parses and prepares the input text for subsequent modules. • Semantic module, which expands documents and/or topics with semantic information retrieved from knowledge base.</p><p>• Textual (text-based) retrieval module, which indexes image annotations in order to search and find the list of images that are most relevant to the text of the topic. • Result combination module, which uses the OR operator to combine, if necessary, two different result lists.</p><p>A common baseline algorithm was used in all experiments to process the collection, following these steps:</p><p>1. Text Extraction: Ad-hoc scripts are run on the files that contain image annotations, on the Wikipedia articles and on the topics. The purpose of this process is to generate the different collections or topics that set up the different specific features of each experiment. 2. Tokenization: This process extracts the basic textual components in the annotations. Some basic entities are also detected, such as numbers, initials, abbreviations, and years. So far, compounds, proper nouns, acronyms or other types of entity are not specifically considered. The outcomes of this process are single words, multi-words, years in numbers and tagged entities resulting from the application of the semantic module. 3. Conversion to lowercase: All document terms are normalized by changing all letters to lowercase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Filtering: All words recognized as stopwords are filtered out. Stopwords in the target languages were initially obtained from the University of Neuchatel's resources page <ref type="bibr" coords="3,197.76,174.16,11.71,8.96" target="#b3">[4]</ref> and afterwards extended using our own developed resources. 5. Stemming: This process is applied to each one of the words to be indexed or used for retrieval. Standard Porter stemmers <ref type="bibr" coords="3,295.44,198.16,11.71,8.96" target="#b4">[5]</ref> for each considered language have been used. 6. Indexing and retrieval: Lucene <ref type="bibr" coords="3,273.24,222.16,11.71,8.96" target="#b5">[6]</ref> was used as the information retrieval engine for the whole textual indexing and retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>The main idea behind our experiments is to evaluate and compare the results achieved by the application of techniques that are based on the computational similarity between the metadata associated to the images and the query itself, opposed to other techniques based on the semantic description of the image using the contextual information provided by the Wikipedia article in which the image is referred.</p><p>The following fields have been considered as contextual information: • the metadata associated to the image itself (C),</p><p>• the title of the article (T), and • the first paragraph in the article (S).</p><p>The core knowledge base for the semantic expansion is a subset of the DBpedia ontology <ref type="bibr" coords="3,165.00,428.20,10.69,8.96" target="#b1">[2]</ref>, conveniently adapted and formatted to our purposes, and using the YAGO classification schema. YAGO <ref type="bibr" coords="3,280.08,440.20,11.71,8.96" target="#b2">[3]</ref> is a huge semantic knowledge base, part of the YAGO-NAGA project at the Max-Planck Institute for Informatics in Saarbrücken (Germany). It currently, holds more than 2 million entities (persons, organizations, cities, etc.) with over 20 million facts about these entities. YAGO has a manually confirmed accuracy of 95%, unlike many other automatically assembled knowledge bases.</p><p>Our resulting knowledge base contains 1,651,225 entities, 226,087 hierarchically related classes by means of 225,781 subClassOf relations and 4,121,043 typeOf relations among entities and classes.</p><p>Afterwards, an entity identification process is run using the information contained in the knowledge base using a parser specifically developed for this task. Last, the semantic information generated as the output is built by adding up the information about the entity itself, the information about its class(es) and all the ancestors of its class(es).</p><p>Finally we submitted 6 experiments to be evaluated, described in Table <ref type="table" coords="3,425.52,608.20,3.76,8.96" target="#tab_0">1</ref> A first preliminary evaluation of these figures shows that, globally, the contextual expansion greatly helps to improve the retrieval results and the semantic expansion tends to make them worse. However, this conclusion is not completely true because of the fact that in the retrieval process, the semantic terms have been boosted with respect to the contextual terms by assigning the first ones a higher relevance factor. This was initially done to be able to better analyze the impact of the semantic expansion, but finally it turned out not to be a good idea. Actually, this issue causes that the final results for the semantic experiments don't exactly reflect the behavior of an actual retrieval system.</p><p>If a deeper analysis is done, it is interesting to notice that, independently of the experiment, the precision levels are quite low when the queries include any reference to primitive features of the image ("white house with garden", "red fruits", "yellow buses", "close up of antenna") or to high-level semantic features such as actions ("people playing guitar", "people laughing") or perceptions.</p><p>Moreover, we can also notice that, regardless of the precision level of the results, in general, the incorporation of semantic information for a given topic always produces similar effects (improvements or reductions) independently of the type of contextual information that has been applied. Considering the impact that the use of semantic information has produced in the retrieval process, the following groups of queries haven been identified: • Queries that couldn't be annotated with semantic information ("lightning in the sky").</p><p>• Queries in which the use of semantic information produces slight improvements in the results ("horseman", "civil airplane").</p><p>• Queries in which the use of semantic information significantly improves the results (see Table <ref type="table" coords="5,182.16,186.16,3.63,8.96">3</ref>). In those queries, the weighting of the semantic terms with respect to the contextual ones, has turned out to be successful to model the specific semantic features that better represent the full meaning of the original query.</p><p>• Queries in which the use of semantic information significantly reduces the precision of the results (see Table <ref type="table" coords="5,276.36,234.16,3.63,8.96" target="#tab_3">4</ref>). The semantic representation of the query has extracted one or more features that are too general (mainly associated to the first levels of the ontology in the knowledge base), thus contributing with search terms that are not very precise, which in turn produce a very high volume of relevant documents. This fact, combined to the semantic terms boosting described before, have caused a significant decrease in the precision of the results. Table <ref type="table" coords="5,155.40,329.97,3.40,8.10">3</ref>. Some examples of the improvements in precision (measured in percentage over the average value) when using semantic information in the experiment setting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>After the detailed analysis of the achieved results for each of the topics, we can point out that the text-based information retrieval techniques applied to image retrieval only provide good results when the formulated queries exactly make reference to the semantic or contextual content of the image (images including something o located somewhere), but tend to be of no application for the extraction of primitive features (such as color, brightness, texture, shapes, corner points or its spatial distribution) or high-level semantic features about the meaning and purpose of the objects or scenes depicted (sentiments, emotions, actions, perceptions). For the first case, the incorporation of semantic information, based on the contextual information of the article in which the image is referred, usually improves the results for those queries in which the semantic information contributes with specific terms that narrow the search. For instance, the semantic information corresponding to the "tennis player on court" topic may help to select images associated to the "tennis player" class; however, the semantic information corresponding to the "cities at night" topic broadens the search to all images that show any of the subclasses extending from "city", which turns out to be very noisy.</p><p>Consequently, it seems that our future efforts should be focused first to study how to better apply any content-based image retrieval technique that helps us to extract the semantics of the image itself, and, on the other hand, to try and find the answers to the following open issues: 1) Should the semantic information be taken into account for all queries during the retrieval process? 2) In any case, should it have a specific processing depending on the query type? 3) Would it be a good idea to assign the same weight during the retrieval process to the semantic information associated to a given entity, or is it better to make this value dependent on the information class and/or the query type?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,141.00,608.20,297.09,76.88"><head>Table 1 .</head><label>1</label><figDesc>. . Description of the experiment set.</figDesc><table coords="3,141.00,651.40,294.98,33.68"><row><cell>Run Identifier</cell><cell>Contextual Expansion</cell><cell>Semantic Expansion</cell></row><row><cell>DAEDALUS_Bas</cell><cell>NO</cell><cell>NO</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,124.80,233.68,215.01,8.96"><head>Table 2 .</head><label>2</label><figDesc>. The highest figures are highlighted in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,113.04,257.49,369.25,128.91"><head>Table 2 .</head><label>2</label><figDesc>Evaluation of experiments.</figDesc><table coords="4,113.04,276.16,369.25,110.24"><row><cell>Run</cell><cell>MAP P@10 P@20 R-prec NDCG</cell><cell>Relevant Retrieved</cell></row><row><cell>DAEDALUS_Bas</cell><cell>0.1492 0.3971 0.3529 0.2377 0.3556</cell><cell>6088</cell></row><row><cell>DAEDALUS_NER_Bas</cell><cell>0.1249 0.3643 0.3236 0.2115 0.3255</cell><cell>5628</cell></row><row><cell>DAEDALUS_W_CT</cell><cell>0.1820 0.4471 0.4029 0.2662 0.4055</cell><cell>6453</cell></row><row><cell>DAEDALUS_NER_W_CT</cell><cell>0.1610 0.4043 0.3736 0.2514 0.3801</cell><cell>6038</cell></row><row><cell>DAEDALUS_W_CTS</cell><cell>0.1737 0.3943 0.3521 0.2478 0.4315</cell><cell>6871</cell></row><row><cell cols="2">DAEDALUS_NER_W_CTS 0.1593 0.4029 0.3593 0.2342 0.4036</cell><cell>6105</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,162.48,377.68,259.93,248.96"><head>Table 4 .</head><label>4</label><figDesc>Some examples of the decrease in precision (measured in percentage over the average value) when using semantic information in the experiment setting.</figDesc><table coords="5,162.48,377.68,259.93,248.96"><row><cell></cell><cell>Bas</cell><cell>W_CT</cell><cell>W_CTS</cell></row><row><cell cols="2">Topic 8: tennis player on court</cell><cell></cell><cell></cell></row><row><cell>MAP</cell><cell>385.43%</cell><cell>590.53%</cell><cell>37.88%</cell></row><row><cell>R-prec.</cell><cell>297.59%</cell><cell>390.76%</cell><cell>12.81%</cell></row><row><cell>Topic 15: cyclist</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAP</cell><cell>370.50%</cell><cell>307.05%</cell><cell>50.42%</cell></row><row><cell>R-prec.</cell><cell>453.59%</cell><cell>329.05%</cell><cell>12.51%</cell></row><row><cell cols="2">Topic 16: spider with cobweb</cell><cell></cell><cell></cell></row><row><cell>MAP</cell><cell>708.47%</cell><cell>511.69%</cell><cell>140.14%</cell></row><row><cell>R-prec.</cell><cell>500.54%</cell><cell>400.54%</cell><cell>100.00%</cell></row><row><cell>Topic 55: building site</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAP</cell><cell>348.65%</cell><cell>352.29%</cell><cell>402.13%</cell></row><row><cell>R-prec.</cell><cell>150.00%</cell><cell>110.53%</cell><cell>100.00%</cell></row><row><cell cols="2">Topic 70: close up of trees</cell><cell></cell><cell></cell></row><row><cell>MAP</cell><cell>409.69%</cell><cell>429.28%</cell><cell>213.17%</cell></row><row><cell>R-prec.</cell><cell>164.25%</cell><cell>137.56%</cell><cell>63.20%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been partially supported by the <rs type="funder">Spanish Center for Industry Technological Development (CDTI, Ministry of Industry, Tourism and Trade)</rs>, through the <rs type="programName">CONTENIDOS A LA CARTA Project</rs>, <rs type="programName">INGENIO 2010 Programme</rs>, <rs type="projectName">AVANZA</rs> <rs type="grantNumber">I+D 2008</rs>. Other partners in the Project are <rs type="funder">Agencia EFE, Germinus XXI</rs>, <rs type="grantNumber">11870</rs>.com and <rs type="funder">Universidad Politécnica de Madrid</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BmPjP3F">
					<orgName type="program" subtype="full">CONTENIDOS A LA CARTA Project</orgName>
				</org>
				<org type="funded-project" xml:id="_auxD7m9">
					<idno type="grant-number">I+D 2008</idno>
					<orgName type="project" subtype="full">AVANZA</orgName>
					<orgName type="program" subtype="full">INGENIO 2010 Programme</orgName>
				</org>
				<org type="funding" xml:id="_NFeRh7B">
					<idno type="grant-number">11870</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,128.56,418.24,342.01,8.96;7,136.20,430.24,293.61,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,301.44,418.24,169.14,8.96;7,136.20,430.24,59.05,8.96">Overview of the Wikipedia Retrieval task at ImageCLEF</title>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">T</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,222.96,430.24,98.89,8.96">Working Notes of CLEF</title>
		<meeting><address><addrLine>Padova. Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.56,442.24,231.32,8.96" xml:id="b1">
	<monogr>
		<ptr target="http://wiki.dbpedia.org/" />
		<title level="m" coord="7,136.20,442.24,122.28,8.96">The DBpedia Knowledge Base</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.56,454.24,342.03,8.96;7,136.20,466.24,43.81,8.96" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><surname>Yago</surname></persName>
		</author>
		<ptr target="http://www.mpi-inf.mpg.de/yago-naga/yago/" />
		<title level="m" coord="7,175.56,454.24,146.55,8.96">A Core of Semantic Knowledge</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.56,478.24,341.92,8.96;7,136.20,490.24,219.37,8.96" xml:id="b3">
	<monogr>
		<ptr target="http://members.unine.ch/jacques.savoy/clef/index.html" />
		<title level="m" coord="7,273.36,478.24,191.99,8.96">IR Multilingual Resources at UniNE</title>
		<imprint/>
		<respStmt>
			<orgName>University of Neuchatel</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.56,502.24,34.72,8.96;7,187.68,502.24,11.37,8.96;7,223.44,502.24,37.69,8.96;7,285.48,502.24,38.31,8.96;7,348.12,502.24,14.46,8.96;7,387.12,502.24,37.71,8.96;7,449.16,502.24,21.45,8.96;7,138.72,514.24,132.54,8.96" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="middle">M</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://www.snowball.tartarus.org" />
		<title level="m" coord="7,223.44,502.24,37.69,8.96;7,285.48,502.24,38.31,8.96;7,348.12,502.24,14.46,8.96;7,387.12,502.24,37.71,8.96;7,449.16,502.24,17.16,8.96">Snowball stemmers and resources page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.56,526.24,201.49,8.96" xml:id="b5">
	<monogr>
		<ptr target="http://lucene.apache.org" />
		<title level="m" coord="7,136.20,526.24,91.56,8.96">Apache Lucene project</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
