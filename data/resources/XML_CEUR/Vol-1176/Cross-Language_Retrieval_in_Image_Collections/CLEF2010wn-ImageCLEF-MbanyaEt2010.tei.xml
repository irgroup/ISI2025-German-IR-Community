<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.51,115.96,330.34,12.62;1,191.61,133.89,232.13,12.62">Augmenting Bag-of-Words -Category Specific Features and Concept Reasoning</title>
				<funder ref="#_tTktmjB">
					<orgName type="full">Federal Ministry of Economics and Technology of Germany</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.23,171.73,69.88,8.74"><forename type="first">Eugene</forename><surname>Mbanya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Fraunhofer Institute for Telecommunications</orgName>
								<orgName type="institution" key="instit2">Heinrich Hertz Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.66,171.73,85.65,8.74"><forename type="first">Christian</forename><surname>Hentschel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Fraunhofer Institute for Telecommunications</orgName>
								<orgName type="institution" key="instit2">Heinrich Hertz Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.87,171.73,70.45,8.74"><forename type="first">Sebastian</forename><surname>Gerke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Fraunhofer Institute for Telecommunications</orgName>
								<orgName type="institution" key="instit2">Heinrich Hertz Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,408.87,171.73,48.02,8.74"><forename type="first">Mohan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Fraunhofer Institute for Telecommunications</orgName>
								<orgName type="institution" key="instit2">Heinrich Hertz Institute</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,204.94,183.68,88.72,8.74"><forename type="first">Andreas</forename><surname>Nürnberger</surname></persName>
							<email>andreas.nuernberger@ovgu.de</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Data &amp; Knowledge Engineering Group</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">Otto-von-Guericke-University Magdeburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.59,183.68,82.35,8.74"><forename type="first">Patrick</forename><surname>Ndjiki-Nya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Fraunhofer Institute for Telecommunications</orgName>
								<orgName type="institution" key="instit2">Heinrich Hertz Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.51,115.96,330.34,12.62;1,191.61,133.89,232.13,12.62">Augmenting Bag-of-Words -Category Specific Features and Concept Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8FD9459C233FCC0F367BC9B33D6C4A59</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present our approach to the 2010 ImageClef PhotoAnnotation task. Based on the well-known bag-of-words approach we suggest two extensions. First, we analyzed the impact of category specific features and classifiers. In order to classify quality-related image categories we implemented a sharpness measure and use this as additional feature in the classification process. Second, we propose a postclassification step, which is based on the observation that many of the categories should be considered as being related to each other: Some categories exclude or allow for inference to others. We incorporate inference and exclusion rules by refining the classification results. The results we obtain show that both extensions can provide a classification performance increase when compared the the standard BoW approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual data such as image and video represents the fastest growing data in the Internet today. Photo communities such as Flickr host more than 4 billion photos 3 . Efficient retrieval methods are thus required to provide access to this vast amount of information, which would otherwise be useless. The sheer amount of data, however, renders manual annotation impossible and demands for automatic approaches. The ImageClef Photo Annotation Task is an annual competition, which gathers researchers to meet this challenge and provide solutions for automatic classification of photos taken from the Flickr community into different categories. In this paper we describe our approach to the 2010 ImageClef Pho-toAnnotation Task <ref type="bibr" coords="1,220.33,588.89,9.96,8.74" target="#b3">[4]</ref>.</p><p>We follow the widely-used visual codebook approach <ref type="bibr" coords="1,389.56,601.01,9.96,8.74" target="#b4">[5]</ref>. We extract local image descriptors, which are an extension of the standard SIFT algorithm by Lowe <ref type="bibr" coords="1,152.54,624.92,13.33,8.74" target="#b2">[3]</ref>. As many of the visual concepts provided within the task show a uniform distribution in texture space, instead of extracting features at scale-space extrema the features are densely extracted by computing the descriptor at fixed grid of feature points. Following the visual codebook paradigm, we cluster similar features into groups, each represented by a visual (code)word. This is done by vector quantizing a subset of all features of all images in the training set. The codebook is the set of all visual words. It is used to describe an image in terms of the codeword frequency distribution by assigning features to codewords. Thus, for each image a simple histogram of codewords provides a compact feature representation. An extension of this approach was presented in <ref type="bibr" coords="2,428.56,202.68,9.96,8.74" target="#b1">[2]</ref>. As some concepts are more likely to be present in specific regions of an image (e.g. sky is more likely to be present in the upper image part) the authors suggest to split an image into fixed subregions. Different resolutions of subregions are aggregated into a so-called spatial pyramid. Histograms of codewords are computed for each of these regions and distances between images are computed region-wise.</p><p>While the described process has the strong advantage of being generic with respect to the extracted features, concept specific peculiarities can hardly be captured. We show that a weighted combination of generic and concept-specific features can increase the overall classification accuracy. As an example we extract a sharpness measure to handle quality-based image categories (such as image blur).</p><p>Category learning is done by training an kernel-based classifier. Support vector machines have been widely used for image classification tasks. We apply a χ 2 -metric for distance computation as this has shown to provide good results in histogram classification <ref type="bibr" coords="2,239.57,382.45,9.96,8.74" target="#b8">[9]</ref>.</p><p>Finally, we propose a classification post-processing step, which is motivated by the idea of category co-occurrences. An analysis of the different image categories in the training set has shown, that many of the categories interfere with each other in sense that the presence of one category gives evidence to or excludes the presence of another. Category reasoning is implemented as a postclassification refinement step.</p><p>In the following, we describe the aforementioned steps in more detail: Section 2 describes the feature extraction and codebook generation process in detail. In section 3 we describe the process of category learning and image classification while section 4 provides information on the aforementioned category postprocessing. Finally, section 5 summarizes this paper by giving some results and providing an outlook to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Feature Extraction</head><p>For classification, two types of features have been used. One is the so-called Op-ponentSIFT [8] feature, an extension of the Scale-Invariant-Feature Transform (SIFT, <ref type="bibr" coords="2,167.62,608.30,10.79,8.74" target="#b2">[3]</ref>) features to the opponent color space while the other one is a sharpness measure for an image to be used in sharpness and blur related categories. As shown in [8], the OpponentSIFT feature with dense sampling was the best performing single feature for an annotation task and therefore has been chosen as a baseline for our submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">OpponentSIFT</head><p>The general principle of the OpponentSIFT based feature extraction is depicted in figure <ref type="figure" coords="3,173.43,149.02,3.87,8.74" target="#fig_1">1</ref>. First, the image is converted to the opponent color space. The opponent color space is given by the following definition from the RGB color space:  On each of the channels of the opponent color space, SIFT features are extracted on a dense grid (we use a step size of 6 pixels) at a fix image scale over the image. SIFT features are extracted by first computing a gradient image on each of the color channels. Then, for each feature point, a region of 64×64 pixels around it is considered for its feature vector. This 64×64 pixel block is subdivided into 4×4 cells, each containing 16×16 pixels. For each cell, a histogram of its gradient directions, aligned to the main gradient direction, is computed. These 16 histograms are concatenated to build the feature vector for one point.</p><formula xml:id="formula_0" coords="3,258.92,180.73,213.19,42.82">  O 1 O 2 O 3   =    R-G √ 2 R+G-2B √ 6 R+G+B √ 3   <label>(</label></formula><p>After obtaining one 384-dimensional (3 color channels × 16 cells × 8 orientation histogram bins) feature vector for each point, those vectors are quantized using K-Means clustering of a random subset of 800.000 features from all 8000 training images (i.e. 100 random features per image) with 4000 cluster centers (visual words). Then each feature point on the dense grid is assigned to a visual word by using a nearest neighbor classifier. Now that each feature point is described by its visual word, a histogram over these visual words in an image is computed, resulting in a 4000-dimensional histogram for each image. Similar to <ref type="bibr" coords="4,147.38,190.72,9.96,8.74" target="#b1">[2]</ref>, in addition to this single histogram for the whole image, histograms for parts of the image are created. Therefore, two spatial partitions are defined, one (1×3) consisting of 3 vertically stacked regions and one (2×2) consisting of the four image quadrants. This yields a total of eight histograms: One for the whole image, three for 1×3 partition and four for the 2×2 partition. Each individual histogram is then L 1 normalized, i.e. divided by the sum of the bin population. To equally distribute the influence of different spatial partitions, the histograms of the 1×3 partition are weighted by 1  3 and the histograms of the 2×2 partition are weighted by 1  4 . Finally, the histograms for a spatial partition are concatenated, resulting in histograms of 4000, 12000 and 16000 bins respectively. These histograms are then used in the SVM for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sharpness Features</head><p>In addition to the OpponentSIFT based features, a sharpness measure is used as an example for category-specific features. Therefore, the no-reference objective image sharpness measure by Ferzli and Karam described in <ref type="bibr" coords="4,408.06,381.68,10.52,8.74" target="#b0">[1]</ref> is used. This metric is based on user studies on blur perception in the human visual system (HVS). It incorporates the fact that the perception of blur in an image part is dependent on the edge statistics in this image part. Thus, a Sobel filter is used to generate an edge image. The edge image is then divided into 64×64 pixel blocks and all blocks are classified as edge or smooth blocks. A block is defined as an edge block where more than 0.2% of the pixels are edge pixels. Only edge blocks are considered when calculating the sharpness measure. The sharpness measure D R b for an edge block R b is calculated by</p><formula xml:id="formula_1" coords="4,240.48,496.00,240.11,34.97">D R b = ei∈R b w(e i ) w JN B (e i ) β 1 β (2)</formula><p>where β is a constant fixed to 3.6, w(e i ) is the edge width around pixel e i in vertical or horizontal direction respectively and w JN B (e i ) is the just noticeable blur width corresponding to the contrast of the block R b . The sharpness measure for the complete image is then calculated by</p><formula xml:id="formula_2" coords="4,292.24,595.29,188.35,22.31">S = L D<label>(3)</label></formula><p>where</p><formula xml:id="formula_3" coords="4,262.67,632.91,217.93,34.96">D = R b |D R b | β 1 β<label>(4)</label></formula><p>and L being the total number of blocks in the image. This resulting sharpness metric S ∈ R + ,with higher values indicating sharper images, has been used as a scalar input to the classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Category Learning and Classification</head><p>Kernel-based Support Vector Machines (SVM) have been widely used in visual codebook-based image classification scenarios (see e.g. <ref type="bibr" coords="5,374.23,220.59,9.96,8.74" target="#b8">[9]</ref>,[8], <ref type="bibr" coords="5,404.12,220.59,10.30,8.74" target="#b5">[6]</ref>).</p><p>We use a two-class setting for binary classification, i.e. classifying images depicting a specific concept or not.</p><p>As in our case, training and testing samples are histograms of codeword distributions, we use the χ 2 distance, which has shown to provide good results for comparing distributions <ref type="bibr" coords="5,261.94,282.35,9.96,8.74" target="#b8">[9]</ref>. Given two histograms H = (h 1 , ..., h m ) and H = (h 1 , ..., h m ) the χ 2 distance is defined as:</p><formula xml:id="formula_4" coords="5,245.28,317.27,235.31,30.32">D(H, H ) = 1 2 m i=1 (h i -h i ) 2 |h i | + |h i |<label>(5)</label></formula><p>To incorporate this metric into a Support Vector Machine we use a Gaussian kernel:</p><formula xml:id="formula_5" coords="5,239.72,393.60,240.88,22.31">K(H, H ) = exp(- 1 γ D(H, H ))<label>(6)</label></formula><p>The normalization parameter γ can be optimized using grid search and crossvalidation. However, Zhang et al. <ref type="bibr" coords="5,283.63,438.94,10.52,8.74" target="#b8">[9]</ref> have shown, that setting this value to the average distance between all training image histograms gives comparable results and reduces the computational effort. The only parameter we optimize in a cross-validation is the cost parameter C of the support vector classification. We precompute the kernel matrix to to speed up subsequent training of the SVM. Additional speed is gained by computing the matrix in parallel on an 8-Core SMP-System. As described in section 2, we yield three different histograms per image -one per spatial pyramid resolution -which results in three different kernels.</p><p>In addition to the codeword histograms, we extracted a scalar quality measure (see sec. 2.2). The distance between to images is based on the absolute value of the difference value of both scalars. We use a linear kernel to compare for the quality measure as this has shown to provide better results than a Gaussian Kernel in our experiments.</p><p>Finally, we combine these four kernels by averaging their output into a single kernel, which is then used to train the Support Vector Machine. Here we use the implementation provided by the SHOGUN machine learning toolbox <ref type="bibr" coords="5,435.93,632.21,9.96,8.74" target="#b6">[7]</ref>. As the SHOGUN LibSVM implementation does not provide a probabilistic output we simply compute the sigmoid function in order to obtain values in [0..1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Category Post-processing</head><p>In its current form, each category is treated independently from all other categories, assuming that dependencies between categories do not exist. This assumption actually does not hold in reality. There are even strong dependencies between different categories, i.e. categories excluding each other (such as day and night) or categories inferring other categories (e.g. plant can be inferred from flower ). These dependencies have been accounted for by post-processing the classification decisions and confidences from the SVMs. Therefore, dependencies have been extracted from the training set. For exclusion of categories, partitions of categories have been identified, i.e. sets of categories P that do not co-occur and exactly one category of this set appears in the training annotations for each image:</p><formula xml:id="formula_6" coords="6,283.95,279.11,196.64,20.06">p∈P C p = I<label>(7)</label></formula><p>p∈P</p><formula xml:id="formula_7" coords="6,301.32,313.24,179.27,9.65">C p = ∅<label>(8)</label></formula><p>In these definitions, I is the set of all images, P is the set of category indices belonging to a partition and C p is the set of images annotated with category p.</p><p>For category inference, pairs of categories are determined such that the statement "If an image is annotated with category a, it is also annotated with category b" holds. Formally, for these category pairs holds:</p><formula xml:id="formula_8" coords="6,289.51,413.16,191.09,9.65">C a ⊇ C b<label>(9)</label></formula><p>After determining the sets of excluding categories and the inference rules from the training set, these rules are used to post-process the output of the SVMs. The new confidence value c (i, p) for a category p is set to 0 if a higher confidence value for a different category within partition P exists. Only the largest confidence value for a category in a partition is kept.</p><formula xml:id="formula_9" coords="6,199.23,505.80,281.37,20.69">c (i, p) = c(i, p) if c(i, p) &gt; c(i, q) ∀q ∈ P \ p 0 else<label>(10)</label></formula><p>If there are no other categories in an exclusion set with higher confidence values, the confidence value is kept. The confidence value of an image for a category q that is on the right side of an inference rule p ⇒ q is updated if the confidence value of the left-hand side category p is greater than 0.5 and greater than the confidence value for category q:</p><formula xml:id="formula_10" coords="6,195.66,609.93,284.93,20.69">c (i, q) = c(i, p) if c(i, p) &gt; c(i, q) ∧ c(i, p) &gt; 0.5 c(i, q) else (11)</formula><p>This update rule can only increase the confidence value for a category, not decrease it.</p><p>We submitted 5 different runs. All runs use the OpponentSIFT histograms as baseline. The first run (OpSIFT ) uses the OpponentSIFT features alone. Another run (OpSIFT+Excl+Inf+Qual ) uses the quality measure as an additional feature and applies category inference and exclusion as post-processing step. A third run (OpSIFT+Qual ) does no post-processing but uses the quality feature.</p><p>The fourth run (OpSIFT+Inf+Qual ) uses the quality feature and applies only the category inference rule as we have noticed that exclusion often decreases the overall classification performance. A final fivth run (OpSIFT+Inf ) uses the Op-ponentSIFT features and applies the inference rule on the classification results. Three different evaluation measures have been computed. For evaluating the classification performance per concept the Mean Average Precision (MAP) was used. The evaluation per example was performed using the example-based F-Measure (F-Ex). Additionally an ontology score based on Flickr Context Similarity was computed (OS-fcs, for a detailed description see <ref type="bibr" coords="7,385.27,302.89,10.30,8.74" target="#b3">[4]</ref>). Table <ref type="table" coords="7,431.49,302.89,4.98,8.74" target="#tab_0">1</ref> shows the average scores achieved for each measure. For all evaluation measures, the runs with extensions to the baseline OpponentSIFT method gave the best results. Table <ref type="table" coords="7,178.05,476.03,4.98,8.74" target="#tab_1">2</ref> shows that using the additional category-specific features yields a gain of 0.00567 in terms of mean average precision for those categories where the sharpness feature has been used (i.e. motion blur, out of focus, partially blurred and no blur ). The reasons for this little gain are, in our opinion, two-fold: On one side, the weight of the sharpness feature has not been optimized yet, resulting in a potential over-or under-estimation of the importance of the category-specific feature. On the other side, the sharpness measure disregards the orientation of edges, making it harder for the classifiers to distinguish between different kinds of blur as to be expected in the categories motion blur and out of focus.</p><p>Table <ref type="table" coords="7,177.43,584.39,4.98,8.74" target="#tab_2">3</ref> shows the detailed average precision for each category individually. As not all extensions to the baseline method are applied to all categories, the results for those categories do not differ between runs. For those categories where the results differ, the best performing run is highlighted in the table. In terms of MAP per category, the exclusion of categories often performed worse than the other runs. Especially for categories where the average precision is already low, the exclusion method even diminishes the results. We found that is due to a lack of reliability of the confidence output of the SVMs. The SVM classifier of bad performing categories have a very small output range, e.g. values ranging from 0.94 to 0.96. In those cases, the number of support vectors used for these categories is usually near the total number of training samples. Using these categories for inference or exclusion of other categories yields a significant performance decrease, propagating the error introduced by one categories' classifier to other categories. In future, a check for the reliability of a classifier should be added to avoid this error propagation. This holds also for the category inference post-processing method, where this can happen as well. Error propagation also led to the decision that only inference rules with a confidence of 1.0 were used, missing opportunities were category inference could be used as well but evaluations have shown that in those cases, performance can drop significantly. In terms of the exemplar-based ontology score, the run containing all extensions performed best. We think that is due to the fact that the post-processing methods introduce a consistency between labels of an image which is actually evaluated by the ontology score.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,472.10,197.57,8.49,8.74;3,134.77,228.74,345.83,9.65;3,134.77,240.70,345.82,8.74;3,134.77,252.65,311.92,8.74"><head>1 )Channel O 3</head><label>13</label><figDesc>represents the intensity channel, while O 1 and O 2 represent the color components. Due to the subtraction in the first two channels, these are shift-invariant with respect to light intensity but not scale-invariant [8].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,209.14,516.22,197.07,7.89;3,221.22,283.70,172.91,217.75"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. OpponentSIFT-based Feature Extraction</figDesc><graphic coords="3,221.22,283.70,172.91,217.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.77,360.88,345.82,77.76"><head>Table 1 .</head><label>1</label><figDesc>Average evaluation scores for all submitted runs. Highlighted values show the run, which obtained the best score for a specific evaluation measure.</figDesc><table coords="7,223.66,360.88,168.03,46.44"><row><cell>Run-Configuration</cell><cell cols="2">MAP Avg. F-Ex OS-fcs</cell></row><row><cell>OpSIFT</cell><cell>0.3492</cell><cell>0.6283 0.6318</cell></row><row><cell cols="2">OpSIFT+Excl+Inf+Qual 0.3331</cell><cell>0.6341 0.6401</cell></row><row><cell>OpSIFT+Qual</cell><cell>0.3495</cell><cell>0.6275 0.6362</cell></row><row><cell>OpSIFT+Inf+Qual</cell><cell>0.3495</cell><cell>0.6278 0.6364</cell></row><row><cell>OpSIFT+Inf</cell><cell>0.3493</cell><cell>0.6285 0.6319</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,116.97,345.83,86.53"><head>Table 2 .</head><label>2</label><figDesc>Average Precision for categories where the sharpness measure has been used. Best scores are highlighted.</figDesc><table coords="8,232.14,116.97,151.08,55.21"><row><cell>Category</cell><cell cols="2">OpSIFT OpSIFT+Q</cell></row><row><cell>Motion Blur</cell><cell>0.2173</cell><cell>0.2096</cell></row><row><cell>Out of Focus</cell><cell>0.1715</cell><cell>0.1689</cell></row><row><cell>Partly Blurred</cell><cell>0.7009</cell><cell>0.7229</cell></row><row><cell>No Blur</cell><cell>0.8913</cell><cell>0.9002</cell></row><row><cell cols="2">Mean Average Precision 0.4953</cell><cell>0.5005</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,174.56,402.61,263.58,261.63"><head>Table 3 :</head><label>3</label><figDesc>Average Precision per category. Scores are only highlighted, when any of the extensions provided a performance increase or decrease.</figDesc><table coords="8,174.56,402.61,263.58,261.63"><row><cell>Category</cell><cell>OpSIFT</cell><cell>OpSIFT+</cell><cell>OpSIFT+</cell><cell>OpSIFT+</cell><cell>OpSIFT+</cell></row><row><cell></cell><cell></cell><cell>E+I+Q</cell><cell>Q</cell><cell>I+Q</cell><cell>I</cell></row><row><cell>Partylife</cell><cell cols="5">0.241183 0.241183 0.241183 0.241183 0.241183</cell></row><row><cell>Family Friends</cell><cell cols="5">0.476125 0.476125 0.476125 0.476125 0.476125</cell></row><row><cell>Beach Holidays</cell><cell cols="5">0.369467 0.369467 0.369467 0.369467 0.369467</cell></row><row><cell>Building Sights</cell><cell cols="5">0.518642 0.518642 0.518642 0.518642 0.518642</cell></row><row><cell>Snow</cell><cell cols="5">0.126314 0.126314 0.126314 0.126314 0.126314</cell></row><row><cell>Citylife</cell><cell cols="5">0.502739 0.502739 0.502739 0.502739 0.502739</cell></row><row><cell>Landscape Nature</cell><cell cols="5">0.769700 0.769700 0.769700 0.769700 0.769700</cell></row><row><cell>Sports</cell><cell cols="5">0.140956 0.140956 0.140956 0.140956 0.140956</cell></row><row><cell>Desert</cell><cell cols="5">0.039890 0.039890 0.039890 0.039890 0.039890</cell></row><row><cell>Spring</cell><cell cols="5">0.119698 0.106550 0.119698 0.119698 0.119698</cell></row><row><cell>Summer</cell><cell cols="5">0.226241 0.226859 0.226241 0.226241 0.226241</cell></row><row><cell>Autumn</cell><cell cols="5">0.293647 0.018332 0.293647 0.293647 0.293647</cell></row><row><cell>Winter</cell><cell cols="5">0.181106 0.020955 0.181106 0.181106 0.181106</cell></row><row><cell>No Visual Season</cell><cell cols="5">0.954757 0.947659 0.954757 0.954757 0.954757</cell></row><row><cell>Indoor</cell><cell cols="5">0.573047 0.517216 0.573047 0.573047 0.573047</cell></row><row><cell>Outdoor</cell><cell cols="5">0.875141 0.868957 0.875141 0.875141 0.875141</cell></row><row><cell>No Visual Place</cell><cell cols="5">0.578996 0.563815 0.578996 0.578996 0.578996</cell></row><row><cell>Plants</cell><cell cols="5">0.695391 0.697349 0.695391 0.697349 0.697349</cell></row><row><cell>Flowers</cell><cell cols="5">0.383228 0.383228 0.383228 0.383228 0.383228</cell></row><row><cell>Trees</cell><cell cols="5">0.601623 0.601623 0.601623 0.601623 0.601623</cell></row><row><cell>Sky</cell><cell cols="5">0.863894 0.865421 0.863894 0.865421 0.865421</cell></row><row><cell>Clouds</cell><cell cols="5">0.802869 0.802869 0.802869 0.802869 0.802869</cell></row><row><cell>Water</cell><cell cols="5">0.600010 0.600010 0.600010 0.600010 0.600010</cell></row><row><cell>Lake</cell><cell cols="5">0.265445 0.265445 0.265445 0.265445 0.265445</cell></row><row><cell>River</cell><cell cols="5">0.210948 0.210948 0.210948 0.210948 0.210948</cell></row><row><cell>Sea</cell><cell cols="5">0.486808 0.486808 0.486808 0.486808 0.486808</cell></row><row><cell>Mountains</cell><cell cols="5">0.493615 0.493615 0.493615 0.493615 0.493615</cell></row><row><cell>Day</cell><cell cols="5">0.837934 0.820454 0.837934 0.837934 0.837934</cell></row><row><cell>Night</cell><cell cols="5">0.519675 0.410528 0.519675 0.519675 0.519675</cell></row><row><cell>No Visual Time</cell><cell cols="5">0.757272 0.756426 0.757272 0.757272 0.757272</cell></row><row><cell>Sunny</cell><cell cols="5">0.426355 0.426355 0.426355 0.426355 0.426355</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements This work was supported in part by <rs type="funder">Federal Ministry of Economics and Technology of Germany</rs> under the project <rs type="projectName">THESEUS</rs> (<rs type="grantNumber">01MQ07018</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_tTktmjB">
					<idno type="grant-number">01MQ07018</idno>
					<orgName type="project" subtype="full">THESEUS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.35,274.09,342.25,7.86;10,146.91,285.05,333.68,7.86;10,146.91,296.01,124.84,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,252.07,274.09,228.53,7.86;10,146.91,285.05,250.43,7.86">A No-Reference Objective Image Sharpness Metric Based on the Notion of Just Noticeable Blur (JNB). Image Processing</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ferzli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,404.15,285.05,76.44,7.86;10,146.91,296.01,8.29,7.86">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="717" to="728" />
			<date type="published" when="2009-04">April 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,306.97,342.24,7.86;10,146.91,317.93,333.68,7.86;10,146.91,328.89,333.68,7.86;10,146.91,339.85,180.78,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,315.69,306.97,164.90,7.86;10,146.91,317.93,234.78,7.86">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,431.42,317.93,49.17,7.86;10,146.91,328.89,292.39,7.86;10,159.78,339.85,38.97,7.86">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
	<note>CVPR&apos;06)</note>
</biblStruct>

<biblStruct coords="10,138.35,350.81,342.24,7.86;10,146.91,361.77,266.87,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,201.49,350.81,236.53,7.86">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,447.52,350.81,33.07,7.86;10,146.91,361.77,138.34,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004-11">November 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,372.72,342.25,7.86;10,146.91,383.68,333.68,7.86;10,146.91,394.64,79.48,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,258.70,372.72,221.90,7.86;10,146.91,383.68,183.09,7.86">New Strategies for Image Annotation: Overview of the Photo Annotation Task at ImageCLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Huiskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,353.58,383.68,122.74,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,405.60,342.24,7.86;10,146.91,416.56,333.68,7.86;10,146.91,427.52,180.20,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,255.67,405.60,224.93,7.86;10,146.91,416.56,51.09,7.86">Video google: a text retrieval approach to object matching in videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,220.42,416.56,68.42,7.86;10,322.53,416.56,158.06,7.86;10,146.91,427.52,55.93,7.86">Proceedings. Ninth IEEE International Conference on</title>
		<meeting>Ninth IEEE International Conference on</meeting>
		<imprint>
			<date type="published" when="2003-04">2003. April 2003</date>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct coords="10,138.35,438.48,342.24,7.86;10,146.91,449.44,212.39,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,282.38,438.48,124.09,7.86">Concept-Based Video Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,413.94,438.48,66.65,7.86;10,146.91,449.44,127.66,7.86">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="215" to="322" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,460.40,342.24,7.86;10,146.91,471.35,333.68,7.86;10,146.91,482.31,259.08,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,301.99,471.35,174.22,7.86">The SHOGUN Machine Learning Toolbox</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Raetsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Widmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Behr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>De Bona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gehl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Franc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,146.91,482.31,153.92,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1799" to="1802" />
			<date type="published" when="2010-06">Jun 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.91,493.27,333.68,7.86;10,146.91,504.23,333.67,7.86;10,146.91,515.19,290.28,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,359.35,493.27,121.24,7.86;10,146.91,504.23,126.33,7.86">A comparison of color features for visual concept classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,283.10,504.23,197.49,7.86;10,146.91,515.19,221.56,7.86">Proceedings of the 2008 international conference on Content-based image and video retrieval -CIVR &apos;08</title>
		<meeting>the 2008 international conference on Content-based image and video retrieval -CIVR &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">141</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,526.15,342.25,7.86;10,146.91,537.11,333.67,7.86;10,146.91,548.07,303.30,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,379.36,526.15,101.23,7.86;10,146.91,537.11,329.28,7.86">Local Features and Kernels for Classification of Texture and Object Categories: A Comprehensive Study</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marsza Lek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,146.91,548.07,168.11,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="238" />
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
