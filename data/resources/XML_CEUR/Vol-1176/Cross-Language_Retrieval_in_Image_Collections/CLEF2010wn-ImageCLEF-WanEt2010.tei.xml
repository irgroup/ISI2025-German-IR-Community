<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,73.03,105.69,465.94,18.14">I2R AT IMAGECLEF WIKIPEDIA RETRIEVAL 2010</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,157.12,145.49,98.60,12.58"><forename type="first">Kong-Wah</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision and Image Understanding</orgName>
								<orgName type="institution">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>1 Fusionopolis Way</addrLine>
									<postCode>138632</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.86,145.49,101.67,12.58"><forename type="first">Yan-Tao</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision and Image Understanding</orgName>
								<orgName type="institution">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>1 Fusionopolis Way</addrLine>
									<postCode>138632</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,385.23,145.49,69.65,12.58"><forename type="first">Sujoy</forename><surname>Roy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision and Image Understanding</orgName>
								<orgName type="institution">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>1 Fusionopolis Way</addrLine>
									<postCode>138632</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,73.03,105.69,465.94,18.14">I2R AT IMAGECLEF WIKIPEDIA RETRIEVAL 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0313AFAFD75C65BA14828A5EA37E860A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multimodal Retrieval</term>
					<term>Visual re-ranking</term>
					<term>Multilingual fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report on our approaches and methods for the ImageCLEF 2010 Wikipedia image retrieval task. A distinctive feature of this year's image collection is that images are associated with unstructured and noisy textual annoations in three languages: English, French and German. Hence, besides following conventional text-based and multimodal approaches, we also focus some effort into investigating multilingual methods. We submitted a total of six runs along the following three directions: 1. augmenting basic text-based indexing with feature selection (three runs), 2. multimodal retrieval that re-ranks text-based results using visualnear-duplicates (VND), (one run), and 3. multilingual fusion that combines results from the three language resources indexed separately (two runs). Our best result (i2rcviu MONOLIN-GUAL, MAP of 0.2126) comes from the latter multilingual fusion approach, indicating the promise of exploiting multilingual resources. For our multimodal re-ranking run, we adopt a pseudo-relevance-feedback approach that builds a visual prototype model of each query without the need for any labeled example images. Essentially, we assume that the top-ranked image results from a text baseline retrieval are correct, and proceed to re-rank the result list such that images that are visually similar images to the top-ranked images are pushed up the ranks. This VND-based re-ranking is applied on the results of a text baseline (RUN i2rcviu I2R.baseline, MAP of 0.1847) that indexed images using all available annotations. This visual re-ranking run (i2rcviu I2R.VISUAL.NDK) achieves a MAP of 0.1984, a 7% improvement. Led by this encouraging result, we apply our VND re-ranking on the results from the multilingual run, and obtain our best retrieval result (not submitted) of 0.2338.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We present our approach and methods in the Wikipedia-MM task of ImageCLEF 2010 <ref type="bibr" coords="1,524.00,630.12,11.71,10.48" target="#b8">[9]</ref>. In this year, a key distinctive feature of the benchmark image collection is that images are annotated with unstructured and noisy text in three languages: English (EN), French (FR), and German (DE). Hence, apart from conventional text-based and multimodal (visual+text) approaches, we investigated into ways to exploit the multilingual nature of the image corpus. We submitted a total of six runs, focusing our effort along the following three main directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Text-based Retrieval</head><p>Firstly, we explore feature selection and relevance feedback techniques to enhance text-based retrieval. Of our six submission runs, three are from this line of research. Our motivation is that text methods would continue to be the main contributor to accurate image retrieval. Hence, attempting to improve text-based methods would naturally form the bulk of our effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Multimodal Methods</head><p>Our second focus is on multimodal approach. Specifically, we take the return results of a textbased baseline, makes the assumption that most of these return results are relevant and correct, and proceed to analyse the top images for visual-near-duplicates (VND). VND images are then clustered and re-ranked so that they become closer in ranks. This has the effect of improving the ranks of lower-ranked images that are similar to higher-ranked images. We note instead of making the weak assumption that the top images are relevant, we could have used the example images that accompany the queries. Nonetheless we adopt our present method for the following reasons:</p><p>1. several reseachers <ref type="bibr" coords="2,175.77,296.73,12.35,10.48" target="#b0">[1]</ref> have already explored the use of the example query images to build a visual prototype model for the query topic; 2. in real world scenarios, users are unlikely to provide example query images, and 3. we aim to explore fully automatic methods.</p><p>Our approach is also closer to the spirit of pseudo relevance feedback (PRF) commonly used in the text community. Because of the lack of time, we have only submitted one run for this line of research. In this run, we apply the VND re-ranking method on the results from a simple text baseline. This text baseline indexes the images based on whatever textual annotations that are present in the XML metadata, ignoring whether those annotations are in EN, DE or FR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Exploiting Multilingual Resources</head><p>Finally, our third focus, which we believe to be the most novel, is on exploiting multilingual cues. This year's benchmark image collection offers the unique opportunity for us to examine the comparative advantage of using multiple language resources on image retrieval.</p><p>Specifically, we build several image indexes based on the various combinations of annotation languages. For example, we build three image indexes based on a single annotation language, i.e. EN-only, DE-only and FR-only. For this case, if an image only has one annotation language, say, EN, then its DE and FR index will be empty, and a query issued to the DE index or FR index will not return that image. We also build three other image indexes based on two annotation languages, i.e. EN-DE, EN-FR, DE-FR. For this case, if an image has only EN annotation, then to build the EN-DE dual-lingual index, we will perform machine translation of EN to DE. For lack of time and compute resource, we stop short of building a triple-lingual index <ref type="foot" coords="2,389.72,623.93,4.23,6.99" target="#foot_0">1</ref> . Because all queries are described using all three languages, for each query, we issue to an image index using the query text in the appropriate language. For example, we issue to the EN-only-index with the EN-only query text, to the DE-only-index the DE-only query text, and so on. Similarly for the dual-lingual index, we form a concatenated new query text comprising description text in the respective languages. For example, we issue to the EN-DE-index the concatenated English query description and German query description.</p><p>Results from multiple image indexes can be fused by taking the maximum retrieval confidence. Clearly, there are configurable options to decide which image index to fuse from. We submitted a total of two runs: <ref type="bibr" coords="3,150.91,135.55,13.66,10.48" target="#b0">(1)</ref>. one that fuses results from the three mono-lingual image index, and (2). one that fuses results from the three dual-lingual image index. The official evaluation results show that fusing results from the mono-lingual image index produces better results than that from fusing results from the dual-lingual image index.</p><p>The rest of the paper is organized as follows. Section 2 provides details of our methods and runs, with results from the official evaluation. Section 3 discusses the results from our submitted runs and presents some post-evaluation results from our further experimentations. Section 4 concludes the paper with some future outlook.</p><p>2 Our Methods and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text-based retrievals</head><p>In all our experiments, we use the Lucene toolkit <ref type="bibr" coords="3,312.21,327.93,12.36,10.48" target="#b1">[2]</ref> as the retrieval system. In parsing the XML metadata files, we removed all mark-up tags, and retained the main textual data after stopword removal as the annotation content. To build the language-specific index, the annotation content is further splitted into its respective languages. The actual query text that is issued to a retrieval system is a concatenation of the query in its respective languages. Except for the run where we are exploring with query expansion, there is no other manipulation of the query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Baseline Text Retrieval System -RUN: I2R.baseline</head><p>As mentioned earlier, the index for our baseline text retrieval system is built by utilizing whatever textual annotations that are present in the XML metadata files. It ignores the language of the annotations. A query is composed by concatenating the query description text in all three languages. This submitted run is called i2rcviu I2R.baseline. It obtained a MAP of 0.1847. Figure <ref type="figure" coords="3,501.41,496.23,5.86,10.48" target="#fig_0">1</ref> shows the official result for this run. Compared to the best obtained results, this MAP value is not impressive. One likely reason for the suboptimal retrieval lies in the way we build this text baseline, and the way we issue concatenated queries of all three languages. Because only 10% of images have annotations in all three languages, for 90% of the images, there will be some terms in the concatenated queries that are non-informative. In other words, for the huge majority of images, there is a language mismatch. This results in noisy retrieval. However, we also note that this situation may be the norm in practical real-world scenarios, where there may not be enough resource to create separate indexes for each of the annotation language. Hence this baseline result can serve as a reference for the multilingual retrieval community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Feature Selection -RUN: I2R.Feat.selection</head><p>Feature selection is a process wherein a subset of the text features (words) are selected for the final text vector representation. The main idea is to discard unimportant, non-informative words, and to retain a smaller subset of words that contribute the most to accurate retrieval <ref type="bibr" coords="4,455.21,114.38,12.36,10.48" target="#b2">[3,</ref><ref type="bibr" coords="4,471.46,114.38,8.24,10.48" target="#b3">4]</ref>.</p><p>Our feature selection proceeds as follow. First, we build a new corpus by issuing to our baseline triple-lingual index the fully concatenated query description. For each query, we collate the top 1000 results. This means that we have a new corpus that is of maximum size 70K. We then perform feature selection on this new corpus. We apply a ad-hoc combination of feature selection techniques from <ref type="bibr" coords="4,79.31,186.61,12.36,10.48" target="#b2">[3,</ref><ref type="bibr" coords="4,96.23,186.61,8.24,10.48" target="#b3">4]</ref>. Specifically, we use the Term Contribution and Document Frequency metric to weight each unique term in the new corpus. We then sort the weighted terms and remove the top 15% and bottom 20% terms. As an extra check, for these words that are earmarked for removal, we further compute their ESA semantic relatedness <ref type="bibr" coords="4,264.30,229.94,12.36,10.48" target="#b4">[5]</ref> with every word in the query text of all 70 queries. If the ESA value of a word is above a threshold, we shall retain it. The idea is to avoid removing words that turn out to be individually relevant to a particular query. Using the new terms in the final list after removal, we create a new index. This submitted run is called i2rcviu I2R.Feat.selection. It obtained a MAP of 0.1945. Figure <ref type="figure" coords="4,231.48,287.73,5.86,10.48" target="#fig_1">2</ref> shows the official result for this run. Compared to the I2R.Feat.selection baseline run, there is a marginal improvement of 5%. This shows the utility of feature selection methods in text retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Query Expansion -RUN: I2R.PRF</head><p>We next experimented with a query expansion approach. We adopt Rocchio's pseudo-relevance feedback <ref type="bibr" coords="4,99.23,488.64,12.35,10.48" target="#b5">[6]</ref> as our query expansion model. By assuming the top return results to be relevant, the query expansion model reformulates the query by augmenting the original query with new feedback words selected from the top return results. For the sake of comparison, we also use the same Term Contribution, Document Frequency and ESA metrics to weight words in the top return documents. Amongst the top Term-Contribution and Document-Frequency words, we take the top K words with high ESA values, where K is the length of the original issued query. Hence the expanded query is of length 2K. The submitted run is called I2R.PRF. It obtained a MAP of 0.1840. Figure <ref type="figure" coords="4,501.78,575.31,5.86,10.48" target="#fig_2">3</ref> shows the official result for this run. This is a disappointing result. Not only is this worse than the Feature Selection run (I2R.Feat.selection), it is even worse than the baseline run (I2R.baseline). While the utility of query expansion has been proven in many information retrieval tasks, we did not see its success generalize onto the present Wikipedia image retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multimodal Approach -RUN : I2R.VISUAL.NDK</head><p>Our multimodal strategy to image retrieval continues on the trend of combining visual processing with the results from text analysis. Specifically, we adopt a re-ranking approach that reorders images according to their visual similarity with a set of visual prototype models constructed from the topranked images. The main intuition of our method is that for a given query, and a visual model of that query, images that are visually closer to the visual model of the query, are likely to be more relevant to the query.</p><p>Given a visual model of a query, we use a visual-near-duplicate (VND) approach to compute visual similarity between an image and the visual model. Near-duplicate images denote a group of images that depict the same or duplicate scene in the whole or part of the image but with slightly varying visual appearance. The reason for visual difference is due to geometric, photometric and scale changes caused by the variance of camera shooting angle, lighting condition, camera sensor or photo editing process. If a group of near-duplicate images are returned for certain queries, this indicates a good probability that all of them are positive answers.  There are many possible methods to construct a visual model of the query. A common strategy is to learn the model from a set of relevant images returned from an image search engine. For the set of official queries in WikipediaMM 2010, each query comes with three example images to visually illustrate the query intent. We note that some researchers have already applied learning a visual model of the query from these images <ref type="bibr" coords="5,251.01,647.67,11.70,10.48" target="#b0">[1]</ref>. However, the downside of this approach is that because the results of an image search could be noisy, there needs to be some manual effort involved in ascertaining the relevance of the returned images. This is especially problematic for non-object type of queries, where returned images tend to be even noiser.</p><p>In this paper, we experimented with a pseudo relevance feedback approach to build a visual model of the query by using the top-ranked images. The implicit assumption is that these top-ranked images are likely to be relevant. We choose the text baseline run (I2R.baseline) on which we will apply the visual re-ranking model. We follow the image-cluster-matching approach in <ref type="bibr" coords="6,534.22,77.76,12.35,10.48" target="#b6">[7]</ref> to build a visual model of each query from a set of V pos images, comprising the top-20-ranked images by the text baseline run I2R.baseline.</p><p>We use a local-feature-based representation for images. Each image is first computed for a number of key-points and their descriptors. The similarity between two images is then determined by matching their keypoint descriptors. Specifically, we utilize Difference of Gaussian as keypoint detector and Scale Invariant Feature Transform (SIFT) as local descriptor. After identifying the group of near-duplicate images in the retrieval list, we take a simple heuristic to rank them at the top as follow. For each image i, we sum up its distance D i to the each image in V pos as D i = ( jâˆˆVpos,j =i distance(i, j))/|V pos -{i}|. We implement the distance(.) function as a variant of the keypoint-based matching method in <ref type="bibr" coords="6,301.86,222.22,11.71,10.48" target="#b7">[8]</ref>. Our submitted run for this method is called I2R.VISUAL.NDK. It obtained a MAP of 0.1984. Figure <ref type="figure" coords="6,348.48,236.67,5.86,10.48" target="#fig_5">5</ref> shows the official result for this run. From the MAP values of 0.1984, we see that there is an improvement of 7% on the text baseline (MAP 0.1847). This is a healthy sign, and points to the effectiveness of using our visual re-ranking model. Note that this improvement comes about without the need for any training images or manual labeling effort. We report in section 3 further experiments that confirms the ability of our visual re-ranking model to improve results from other text retrieval baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Exploiting Multilingual Cues</head><p>In this year, we are provided with an image collection that is annotated with text in three languages: English, German and French. This offers us a good opportunity to explore the utility of these multilingual resources for improved image retrieval. Intuitively, the additional text annotations should help in retrieval since the information are highly related, but not in a redundant way. For example, if an "Volcano" image has the English word "Volcano", then obviously it would match well to the English query with the word "Volcano". If the same image does not have an English annotation, but rather it has a German equivalent translation, then it would still likely match well to the original English query translated to the German "Vulkan". Further more, if the image has both English and German annotations, then a query issued with both "Volcano" and "Vulkan" would have a more confident hit on this image.</p><p>Of course in a real-world setting, not all images would come with multilingual annotations. In fact, the statistics of the 2010 WikipediaMM collection is such that 60% of the images have annotations in only a single language, 25% of images have annotations in two languages, and only 10% of images have annotations in all three language. This means that the huge majority of images are singlelanguage-annotated. In terms of medium of language in the annotations, 60% of the images have annotations in English, followed by 46% of images having annotations in German, and 30% of images having annotations in French. In such a situation, we explore the following questions:</p><p>1. Will we do better if we create a separate mono-lingual index, and issue to it with the appropriate query text in the corresponding language? 2. Will we do better if we create a separate dual-lingual index, and issue to it with the appropriately concatenated query text in the corresponding languages?</p><p>Each of the above is compared against the default text baseline used in our paper: the I2R.baseline system, which builds a generic index that uses all the text annotations that are present in the XML metadata, disregarding which language the text are in. We report our exploration of both question and their results in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Mono-lingual -RUN: MONOLINGUAL</head><p>The main idea is simple. Since the bulk of images would have annotations in at least one language, and since all our queries have the equivalent translations in all three languages, we can build a new retrieval system by the following:</p><p>1. build a mono-lingual index for all images 2. issue the appropriate translated-queries to the corresponding mono-lingual index 3. fuse the three result lists using maximum confidence Note that in building the language-specific index, we do not require that all images must have all three annotation languages. In contrast to the dual-lingual run in the next subsection, we do not perform any machine translation here. In other words, if an image has only EN annotation, but no annotation in DE nor in FR, then only the EN-index contains this image, and the DE-index and the FR-index will not contain this image. During query time, only the English query description text will be issued to the EN-index.</p><p>Figure <ref type="figure" coords="7,162.27,498.02,4.55,10.48">6</ref>: Results for our mono-lingual RUN:i2rcviu MONOLINGUAL Our submitted run for this method is called MONOLINGUAL. It obtained a MAP of 0.2126. Out of our total six submitted runs, this run has the best result. Compared to the MAP of 0.1847 from the baseline run (I2R.baseline), this is a big improvement of 15%. Note that because the baseline I2R.baseline run builds a generic index that combines all available annotations in all languages, the better result from MONOLINGUAL tells us that it is better to be language-specific. Figure <ref type="figure" coords="7,521.82,586.52,5.86,10.48">6</ref> shows the official result for this run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Dual-lingual -RUN: DUAL LINGUAL</head><p>Encouraged by the promising result from our MONOLINGUAL run, we turn to the question of whether we can do better with dual-lingual index. We build a new retrieval system following similar ideas in MONOLINGUAL:</p><p>1. build a dual-lingual index for all images. Perform machine translation if neccessary.</p><p>2. issue the concatenation of the appropriate translated-queries to the corresponding dual-lingual index 3. fuse the three result lists using maximum confidence Note that in building the dual-lingual index, we now mandate that all images must have all three annotation languages. In other words, if an image has only EN annotation, but no annotation in DE nor in FR, then we perform machine translation of the En-text to both DE-text and FRtext. We used the Google AJAX Translation API for the machine translation. Limited by the constraints in the AJAX web services, we were forced to throttle our calls, and only performed translation of the headline snippets of the Wikipedia text. Our submitted run for this method is called DUAL LINGUAL. It obtained a MAP of 0.1742. Figure <ref type="figure" coords="8,385.91,204.29,5.86,10.48" target="#fig_6">7</ref> shows the official result for this run. The result is disappointing. It is nowhere near to the MAP value of 0.2126 of the MONOLIN-GUAL run. Compared to the MAP of 0.1847 from the baseline run (I2R.baseline), there is even a drop of 6%. We attribute the bad results to the heavily subdued translation effort by our throttled AJAX call to Google Translation service. It is also likely that the automatically detected headline snippets submitted for translation is not meaningful and representative of the main content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Further Experimentations</head><p>Of the six submitted runs, there are two promising results, namely our visual re-ranking I2R.VISUAL.NDK run and the mono-lingual MONOLINGUAL run. The visual re-ranking has improved retrieval over the baseline result by 7%, while using the mono-lingual index approach, retrieval improved by 15% over the baseline. As part of our post-evaluation effort, we apply the visual re-ranking method onto the best run from the official evalaution, namely the MONOLINGUAL run. We obtained a MAP result of 0.2338, an improvement of about 10%. This shows that the improvement from our visual re-ranking is robust and generalizable. Had we decided to submit this additional run, this result would have been ranked 13 th amongst all submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>For our participation in the ImageCLEF 2010 Wikipedia retrieval task, we submitted a total of six runs, exploring the following directions: <ref type="bibr" coords="8,264.58,643.21,13.66,10.48" target="#b0">(1)</ref>. Feature selection and feedback strategies text-based methods, <ref type="bibr" coords="8,101.40,657.66,13.66,10.48" target="#b1">(2)</ref>. Visual re-ranking without the need for any labeled images, (3). Fusion of mutilingual resources. From the official evaluation results, we see that both our visual re-ranking method and the fusion of mono-lingual resources can significantly improve retrieval. Both strategies will now form an integral part of our future effort in image retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,141.02,594.83,329.95,10.48"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Results for our text baseline RUN:i2rcviu I2R.baseline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,105.34,378.01,401.34,10.48"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results for our text feature selection RUN:i2rcviu I2R.Feat.selection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,55.07,676.27,501.87,10.48"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results for our pseudo relevance feedback-based qeury expansion RUN:i2rcviu I2R.PRF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,352.35,307.94,208.62,10.48;5,51.02,322.39,390.19,10.48"><head></head><label></label><figDesc>Figure 4 below shows a few examples of near-duplicate examples that are both positive answers to the given queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,217.32,561.49,177.37,10.48;5,205.43,346.83,91.67,198.42"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example VND matching</figDesc><graphic coords="5,205.43,346.83,91.67,198.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,92.13,327.43,427.73,10.48"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results for our visual re-ranking method RUN:i2rcviu I2R.VISUAL.NDK</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,125.21,306.71,361.57,10.48"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Results for our mono-lingual RUN:i2rcviu DUAL LINGUAL</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,68.96,691.69,492.02,8.74;2,51.02,703.65,476.41,8.74"><p>Note also that our default baseline text system is actually NOT a triple-lingual index system. It merely uses whatever annotations that is present. About 60% of images have annotations in only a single language, and</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="2,531.06,703.65,29.92,8.74;2,51.02,715.60,446.40,8.74"><p>25% of images have annotations in two languages. Only 10% of images have annotations in all three language</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,69.23,89.60,491.75,10.48;9,63.38,104.05,451.76,10.48" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,472.81,89.60,88.18,10.48;9,63.38,104.05,238.22,10.48">Visual Reranking for Image Retrieval over the Wikipedia Corpus</title>
		<author>
			<persName coords=""><forename type="first">Debora</forename><surname>Myoupo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescuy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Herve</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Le</forename><surname>Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre-Alain</forename><surname>Moellic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,316.90,104.05,162.69,10.48">ImageCLEF 2009 working notes</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,69.23,128.46,233.04,10.48" xml:id="b1">
	<monogr>
		<ptr target="http://lucene.apache.org/java/docs/" />
		<title level="m" coord="9,69.23,128.46,33.58,10.48">Lucene</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,69.23,152.86,491.74,10.48;9,63.38,167.31,198.66,10.48" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,291.27,152.86,269.70,10.48;9,63.38,167.31,14.83,10.48">An Evaluation on Feature Selection for Text Clustering</title>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,106.62,167.31,54.98,10.48">Proc ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,69.23,191.72,491.74,10.48;9,63.38,206.17,168.75,10.48" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,212.27,191.72,323.66,10.48">A comparative study on feature selection in text categorization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,63.38,206.17,68.31,10.48">Proc of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,69.23,230.57,491.74,10.48;9,63.38,245.02,497.60,10.48;9,63.38,259.47,101.45,10.48" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,304.41,230.57,256.56,10.48;9,63.38,245.02,159.84,10.48">Computing semantic relatedness using Wikipediabased explicit semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,250.43,245.02,305.85,10.48">Proc International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,69.23,283.87,491.74,10.48;9,63.38,298.32,447.67,10.48" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,134.64,283.87,218.99,10.48">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,495.40,283.87,65.57,10.48;9,63.38,298.32,348.83,10.48">The SMART Retrieval System -Experiments in Automatic Document Processing</title>
		<editor>
			<persName><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,69.23,322.73,491.74,10.48;9,63.38,337.17,154.28,10.48" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,287.45,322.73,273.52,10.48;9,63.38,337.17,30.24,10.48">In Defense of Nearest-Neighbor Based Image Classification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Boiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,122.24,337.17,57.44,10.48">Proc CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,69.23,361.58,491.74,10.48;9,63.38,376.03,323.48,10.48" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,282.34,361.58,278.64,10.48;9,63.38,376.03,78.87,10.48">An efficient parts-based near-duplicate and sub-image retrieval system</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Huston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,171.79,376.03,115.78,10.48">Proc ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="869" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,69.23,400.44,491.75,10.48;9,63.38,414.88,337.55,10.48" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,351.75,400.44,209.23,10.48;9,63.38,414.88,102.15,10.48">Overview of the Wikipedia Retrieval task at ImageCLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jana</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,194.90,414.88,170.14,10.48">the Working Notes of CLEF 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
